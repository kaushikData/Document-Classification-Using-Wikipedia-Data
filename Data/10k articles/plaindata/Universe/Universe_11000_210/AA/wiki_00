{"id": "40250704", "url": "https://en.wikipedia.org/wiki?curid=40250704", "title": "Ambient backscatter", "text": "Ambient backscatter\n\nAmbient backscatter uses existing radio frequency signals, such as radio, television and mobile telephony, to transmit data without a battery or power grid connection. Each such device uses an antenna to pick up an existing signal and convert it into tens to hundreds of microwatts of electricity. It uses that power to modify and reflect the signal with encoded data. Antennas on other devices, in turn, detect that signal and can respond accordingly.\n\nInitial implementations can communicate over several feet of distance, even with transmission towers up to away. Transmission rates were 1k bits per second between devices situated apart inside and apart outside, sufficient to handle text messages or other small data sets. Circuit sizes can be as small as 1 sq. mm.\n\nThis approach would let mobile and other devices communicate without being turned on. It would also allow unpowered sensors to communicate, allowing them to function in places where external power cannot be conveniently supplied.\n\n\n"}
{"id": "202899", "url": "https://en.wikipedia.org/wiki?curid=202899", "title": "Atmosphere", "text": "Atmosphere\n\nAn atmosphere (from Modern Greek ἀτμός \"(atmos)\", meaning 'vapour', and σφαῖρα \"(sphaira)\", meaning 'sphere') is a layer or a set of layers of gases surrounding a planet or other material body, that is held in place by the gravity of that body. An atmosphere is more likely to be retained if the gravity it is subject to is high and the temperature of the atmosphere is low.\n\nThe atmosphere of Earth is composed of nitrogen (about 78%), oxygen (about 21%), argon (about 0.9%) , carbon dioxide (0.03%) and other gases in trace amounts. Oxygen is used by most organisms for respiration; nitrogen is fixed by bacteria and lightning to produce ammonia used in the construction of nucleotides and amino acids; and carbon dioxide is used by plants, algae and cyanobacteria for photosynthesis. The atmosphere helps to protect living organisms from genetic damage by solar ultraviolet radiation, solar wind and cosmic rays. The current composition of the Earth's atmosphere is the product of billions of years of biochemical modification of the paleoatmosphere by living organisms.\n\nThe term stellar atmosphere describes the outer region of a star and typically includes the portion above the opaque photosphere. Stars with sufficiently low temperatures may have outer atmospheres with compound molecules.\n\nAtmospheric pressure at a particular location is the force per unit area perpendicular to a surface determined by the weight of the vertical column of atmosphere above that location. On Earth, units of air pressure are based on the internationally recognized standard atmosphere (atm), which is defined as 101.325 kPa (760 Torr or 14.696 psi). It is measured with a barometer.\n\nAtmospheric pressure decreases with increasing altitude due to the diminishing mass of gas above. The height at which the pressure from an atmosphere declines by a factor of \"e\" (an irrational number with a value of 2.71828...) is called the scale height and is denoted by \"H\". For an atmosphere with a uniform temperature, the scale height is proportional to the temperature and inversely proportional to the product of the mean molecular mass of dry air and the local acceleration of gravity at that location. For such a model atmosphere, the pressure declines exponentially with increasing altitude. However, atmospheres are not uniform in temperature, so estimation of the atmospheric pressure at any particular altitude is more complex.\n\nSurface gravity differs significantly among the planets. For example, the large gravitational force of the giant planet Jupiter retains light gases such as hydrogen and helium that escape from objects with lower gravity. Secondly, the distance from the Sun determines the energy available to heat atmospheric gas to the point where some fraction of its molecules' thermal motion exceed the planet's escape velocity, allowing those to escape a planet's gravitational grasp. Thus, distant and cold Titan, Triton, and Pluto are able to retain their atmospheres despite their relatively low gravities.\n\nSince a collection of gas molecules may be moving at a wide range of velocities, there will always be some fast enough to produce a slow leakage of gas into space. Lighter molecules move faster than heavier ones with the same thermal kinetic energy, and so gases of low molecular weight are lost more rapidly than those of high molecular weight. It is thought that Venus and Mars may have lost much of their water when, after being photo dissociated into hydrogen and oxygen by solar ultraviolet, the hydrogen escaped. Earth's magnetic field helps to prevent this, as, normally, the solar wind would greatly enhance the escape of hydrogen. However, over the past 3 billion years Earth may have lost gases through the magnetic polar regions due to auroral activity, including a net 2% of its atmospheric oxygen. The net effect, taking the most important escape processes into account, is that an intrinsic magnetic field does not protect a planet from atmospheric escape and that for some magnetizations the presence of a magnetic field works to increase the escape rate.\n\nOther mechanisms that can cause atmosphere depletion are solar wind-induced sputtering, impact erosion, weathering, and sequestration—sometimes referred to as \"freezing out\"—into the regolith and polar caps.\n\nAtmospheres have dramatic effects on the surfaces of rocky bodies. Objects that have no atmosphere, or that have only an exosphere, have terrain that is covered in craters. Without an atmosphere, the planet has no protection from meteoroids, and all of them collide with the surface as meteorites and create craters.\n\nMost meteoroids burn up as meteors before hitting a planet's surface. When meteoroids do impact, the effects are often erased by the action of wind. As a result, craters are rare on objects with atmospheres.\n\nWind erosion is a significant factor in shaping the terrain of rocky planets with atmospheres, and over time can erase the effects of both craters and volcanoes. In addition, since liquids can not exist without pressure, an atmosphere allows liquid to be present at the surface, resulting in lakes, rivers and oceans. Earth and Titan are known to have liquids at their surface and terrain on the planet suggests that Mars had liquid on its surface in the past.\n\nA planet's initial atmospheric composition is related to the chemistry and temperature of the local solar nebula during planetary formation and the subsequent escape of interior gases. The original atmospheres started with a rotating disc of gases that collapsed to form a series of spaced rings that condensed to form the planets. The planet's atmospheres were then modified over time by various complex factors, resulting in quite different outcomes.\n\nThe atmospheres of the planets Venus and Mars are primarily composed of carbon dioxide, with small quantities of nitrogen, argon, oxygen and traces of other gases.\n\nThe composition of Earth's atmosphere is largely governed by the by-products of the life that it sustains. Dry air from Earth's atmosphere contains 78.08% nitrogen, 20.95% oxygen, 0.93% argon, 0.04% carbon dioxide, and traces of hydrogen, helium, and other \"noble\" gases (by volume), but generally a variable amount of water vapor is also present, on average about 1% at sea level.\n\nThe low temperatures and higher gravity of the Solar System's giant planets—Jupiter, Saturn, Uranus and Neptune—allow them more readily to retain gases with low molecular masses. These planets have hydrogen–helium atmospheres, with trace amounts of more complex compounds.\n\nTwo satellites of the outer planets possess significant atmospheres. Titan, a moon of Saturn, and Triton, a moon of Neptune, have atmospheres mainly of nitrogen. When in the part of its orbit closest to the Sun, Pluto has an atmosphere of nitrogen and methane similar to Triton's, but these gases are frozen when it is farther from the Sun.\n\nOther bodies within the Solar System have extremely thin atmospheres not in equilibrium. These include the Moon (sodium gas), Mercury (sodium gas), Europa (oxygen), Io (sulfur), and Enceladus (water vapor).\n\nThe first exoplanet whose atmospheric composition was determined is HD 209458b, a gas giant with a close orbit around a star in the constellation Pegasus. Its atmosphere is heated to temperatures over 1,000 K, and is steadily escaping into space. Hydrogen, oxygen, carbon and sulfur have been detected in the planet's inflated atmosphere.\n\nEarth's atmosphere consists of a number of layers that differ in properties such as composition, temperature and pressure. The lowest layer is the troposphere, which extends from the surface to the bottom of the stratosphere. Three quarters of the atmosphere's mass resides within the troposphere, and is the layer within which the Earth's terrestrial weather develops. The depth of this layer varies between 17 km at the equator to 7 km at the poles. The stratosphere, extending from the top of the troposphere to the bottom of the mesosphere, contains the ozone layer. The ozone layer ranges in altitude between 15 and 35 km, and is where most of the ultraviolet radiation from the Sun is absorbed. The top of the mesosphere, ranges from 35 to 85 km, and is the layer wherein most meteors burn up. The thermosphere extends from 85 km to the base of the exosphere at 690 km and contains the ionosphere, a region where the atmosphere is ionised by incoming solar radiation. The ionosphere increases in thickness and moves closer to the Earth during daylight and rises at night allowing certain frequencies of radio communication a greater range. The Kármán line, located within the thermosphere at an altitude of 100 km, is commonly used to define the boundary between Earth's atmosphere and outer space. The exosphere begins variously from about 690 to 1,000 km above the surface, where it interacts with the planet's magnetosphere. Each of the layers has a different lapse rate, defining the rate of change in temperature with height.\n\nOther astronomical bodies such as these listed have known atmospheres.\n\n\n\nThe circulation of the atmosphere occurs due to thermal differences when convection becomes a more efficient transporter of heat than thermal radiation. On planets where the primary heat source is solar radiation, excess heat in the tropics is transported to higher latitudes. When a planet generates a significant amount of heat internally, such as is the case for Jupiter, convection in the atmosphere can transport thermal energy from the higher temperature interior up to the surface.\n\nFrom the perspective of a planetary geologist, the atmosphere acts to shape a planetary surface. Wind picks up dust and other particles which, when they collide with the terrain, erode the relief and leave deposits (eolian processes). Frost and precipitations, which depend on the atmospheric composition, also influence the relief. Climate changes can influence a planet's geological history. Conversely, studying the surface of the Earth leads to an understanding of the atmosphere and climate of other planets.\n\nFor a meteorologist, the composition of the Earth's atmosphere is a factor affecting the climate and its variations.\n\nFor a biologist or paleontologist, the Earth's atmospheric composition is closely dependent on the appearance of the life and its evolution.\n\n\n"}
{"id": "38296930", "url": "https://en.wikipedia.org/wiki?curid=38296930", "title": "Blade element momentum theory", "text": "Blade element momentum theory\n\nBlade element momentum theory is a theory that combines both blade element theory and momentum theory. It is used to calculate the local forces on a propeller or wind-turbine blade. Blade element theory is combined with momentum theory to alleviate some of the difficulties in calculating the induced velocities at the rotor.\n\nThis article emphasizes application of BEM to ground-based wind turbines, but the principles apply as well to propellers. Whereas the streamtube area is reduced by a propeller, it is expanded by a wind turbine. For either application, a highly simplified but useful approximation is the Rankine–Froude \"momentum\" or \"actuator disk\" model (1865,1889). This article explains the application of the \"Betz limit\" to the efficiency of a ground-based wind turbine.\n\nA development came in the form of Froude's blade element momentum theory (1878), later refined by Glauert (1926). Betz (1921) provided an approximate correction to momentum \"Rankine–Froude actuator-disk\" theory to account for the sudden rotation imparted to the flow by the actuator disk (NACA TN 83, \"The Theory of the Screw Propeller\" and NACA TM 491, \"Propeller Problems\"). In blade element momentum theory, angular momentum is included in the model, meaning that the wake (the air after interaction with the rotor) has angular momentum. That is, the air begins to rotate about the z-axis immediately upon interaction with the rotor (see diagram below). Angular momentum must be taken into account since the rotor, which is the device that extracts the energy from the wind, is rotating as a result of the interaction with the wind.\n\nThe \"Betz limit,\" not yet taking advantage of Betz' contribution to account for rotational flow with emphasis on propellers, applies the Rankine–Froude \"actuator disk\" theory to obtain the maximum efficiency of a stationary wind turbine. The following analysis is restricted to axial motion of the air:\n\nIn our streamtube we have fluid flowing from left to right, and an actuator disk that represents the rotor. We will assume that the rotor is infinitesimally thin. From above, we can see that at the start of the streamtube, fluid flow is normal to the actuator disk. The fluid interacts with the rotor, thus transferring energy from the fluid to the rotor. The fluid then continues to flow downstream. Thus we can break our system/streamtube into two sections: pre-acuator disk, and post-actuator disk. Before interaction with the rotor, the total energy in the fluid is constant. Furthermore, after interacting with the rotor, the total energy in the fluid is constant.\n\nBernoulli's equation describes the different forms of energy that are present in fluid flow where the net energy is constant i.e. when a fluid is not transferring any energy to some other entity such as a rotor. The energy consists of static pressure, gravitational potential energy, and kinetic energy. Mathematically, we have the following expression:\n\nwhere formula_2 is the density of the fluid, formula_3 is the velocity of the fluid along a streamline, formula_4 is the static pressure energy, formula_5 is the acceleration due to gravity, and formula_6 is the height above the ground. For the purposes of this analysis, we will assume that gravitational potential energy is unchanging during fluid flow from left to right such that we have the following:\n\nThus, if we have two points on a streamline, point 1 and point 2, and at point 1 the velocity of the fluid along the streamline is formula_8 and the pressure at 1 is formula_9, and at point 2 the velocity of the fluid along the streamline is formula_10 and the pressure at 2 is formula_11, and no energy has been extracted from the fluid between points 1 and 2, then we have the following expression:\n\nNow let us return to our initial diagram. Consider pre-actuator flow. Far upstream, the fluid velocity is formula_13; the fluid then expands as it approaches the rotor. In accordance with mass conservation, the mass flow rate must be constant. The mass flow rate, formula_14, through a surface of area formula_15 is given by the following expression:\n\nwhere formula_2 is the density and formula_3 is the velocity of the fluid along a streamline. Thus, if mass flow rate is constant, increases in area must result in decreases in fluid velocity along a streamline. This means the kinetic energy of the fluid is decreasing. If the flow is expanding but not transferring energy, then Bernoulli applies. Thus the reduction in kinetic energy is countered by an increase in static pressure energy. Why a streamtube expands as it approaches an object is not explained in this document.\n\nSo we have the following situation pre-rotor: far upstream, fluid pressure is the same as atmospheric, formula_19; just before interaction with the rotor, fluid pressure has increased and so kinetic energy has decreased. This can be described mathematically using Bernoulli's equation:\n\nwhere we have written the fluid velocity at the rotor as formula_21, where formula_22 is the axial induction factor. The pressure of the fluid on the upstream side of the actuator disk is formula_23. We are treating the rotor as an actuator disk that is infinitely thin. Thus we will assume no change in fluid velocity across the actuator disk. Since energy has been extracted from the fluid, the pressure must have decreased.\n\nNow let us consider post-rotor: immediately after interacting with the rotor, the fluid velocity is still formula_21, but the pressure has dropped to a value formula_25; far downstream, pressure of the fluid has reached equilibrium with the atmosphere; this has been accomplished in the natural and dynamically slow process of decreasing the velocity of flow in the stream tube in order to maintain dynamic equilibrium ( i.e. formula_26 far downstream. Assuming no further energy transfer, we can apply Bernoulli for downstream:\n\nwhere \n\nThus we can obtain an expression for the pressure difference between fore and aft of the rotor:\n\nIf we have a pressure difference across the area of the actuator disc, there is a force acting on the actuator disk, which can be determined from formula_30:\n\nwhere formula_32 is the area of the actuator disk. If the rotor is the only thing absorbing energy from the fluid, the rate of change in axial momentum of the fluid is the force that is acting on the rotor. The rate of change of axial momentum can be expressed as the difference between the initial and final axial velocities of the fluid, multiplied by the mass flow rate:\n\nThus we can arrive at an expression for the fluid velocity far downstream:\n\nThis force is acting at the rotor. The power taken from the fluid is the force acting on the fluid multiplied by the velocity of the fluid at the point of power extraction:\n\nSuppose we are interested in finding the maximum power that can be extracted from the fluid. The power in the fluid is given by the following expression:\n\nwhere formula_2 is the fluid density as before, formula_3 is the fluid velocity, and formula_15 is the area of an imaginary surface through which the fluid is flowing. The power extracted from the fluid by a rotor in the scenario described above is some fraction of this power expression. We will call the fraction the power co-efficient, formula_40. Thus the power extracted, formula_41 is given by the following expression:\n\nOur question is this: what is the maximum value of formula_40 using the Betz model?\n\nLet us return to our derived expression for the power transferred from the fluid to the rotor (formula_41). We can see that the power extracted is dependent on the axial induction factor. If we differentiate formula_41 with respect to formula_22, we get the following result:\n\nIf we have maximised our power extraction, we can set the above to zero. This allows us to determine the value of formula_22 which yields maximum power extraction. This value is a formula_49. Thus we are able to find that formula_50. In other words, the rotor cannot extract more than 59 per cent of the power in the fluid.\n\nCompared to the Rankine–Froude model, Blade element momentum theory accounts for the angular momentum of the rotor. Consider the left hand side of the figure below. We have a streamtube, in which there is the fluid and the rotor. We will assume that there is no interaction between the contents of the streamtube and everything outside of it. That is, we are dealing with an isolated system. In physics, isolated systems must obey conservation laws. An example of such is the conservation of angular momentum. Thus, the angular momentum within the streamtube must be conserved. Consequently, if the rotor acquires angular momentum through its interaction with the fluid, something else must acquire equal and opposite angular momentum. As already mentioned, the system consists of just the fluid and the rotor, the fluid must acquire angular momentum in the wake. As we related the change in axial momentum with some induction factor formula_22, we will relate the change in angular momentum of the fluid with the tangential induction factor, formula_52.\n\nLet us consider the following setup.\n\nWe will break the rotor area up into annular rings of infinitesimally small thickness. We are doing this so that we can assume that axial induction factors and tangential induction factors are constant throughout the annular ring. An assumption of this approach is that annular rings are independent of one another i.e. there is no interaction between the fluids of neighboring annular rings.\n\nLet us now go back to Bernoulli:\n\nThe velocity is the velocity of the fluid along a streamline. The streamline may not necessarily run parallel to a particular co-ordinate axis, such as the z-axis. Thus the velocity may consist of components in the axes that make up the co-ordinate system. For this analysis, we will use cylindrical polar co-ordinates formula_54. Thus formula_55.\n\nNOTE: We will in fact, be working in cylindrical co-ordinates for all aspects e.g. formula_56\n\nNow consider the setup shown above. As before, we can break the setup into two components: upstream and downstream.\n\nwhere formula_58 is the velocity of the fluid along a streamline far upstream, and formula_59 is the velocity of the fluid just prior to the rotor. Written in cylindrical polar co-ordinates, we have the following expression:\n\nwhere formula_13 and formula_21 are the z-components of the velocity far upstream and just prior to the rotor respectively. This is exactly the same as the upstream equation from the Betz model.\n\nIt should be noted that, as can be seen from the figure above, the flow expands as it approaches the rotor, a consequence of the increase in static pressure and the conservation of mass. This would imply that formula_63 upstream. However, for the purpose of this analysis, that effect will be neglected.\n\nwhere formula_65 is the velocity of the fluid just after interacting with the rotor. This can be written as formula_66. The radial component of the velocity will be zero; this must be true if we are to use the annular ring approach; to assume otherwise would suggest interference between annular rings at some point downstream. Since we assume that there is no change in axial velocity across the disc, formula_67. Angular momentum must be conserved in an isolated system. Thus the rotation of the wake must not die away. Thus formula_68 in the downstream section is constant. Thus Bernoulli simplifies in the downstream section:\n\nIn other words, the Bernoulli equations up and downstream of the rotor are the same as the Bernoulli expressions in the Betz model. Therefore, we can use results such as power extraction and wake speed that were derived in the Betz model i.e.\n\nThis allows us to calculate maximum power extraction for a system that includes a rotating wake. This can be shown to give the same value as that of the Betz model i.e. 0.59. This method involves recognising that the torque generated in the rotor is given by the following expression:\n\nwith the necessary terms defined immediately below.\n\nConsider fluid flow around an airfoil. The flow of the fluid around the airfoil gives rise to lift and drag forces. By definition, lift is the force that acts on the airfoil normal to the apparent fluid flow speed seen by the airfoil. Drag is the forces that acts tangential to the apparent fluid flow speed seen by the airfoil. What do we mean by an apparent speed? Consider the diagram below:\n\nThe speed seen by the rotor blade is dependent on three things: the axial velocity of the fluid, formula_21; the tangential velocity of the fluid due to the acceleration round an airfoil, formula_74; and the rotor motion itself, formula_75. That is, the apparent fluid velocity is given as below:\n\nThus the apparent wind speed is just the magnitude of this vector i.e.:\n\nWe can also work out the angle formula_78 from the above figure:\n\nSupposing we know the angle formula_80, we can then work out formula_81 simply by using the relation formula_82; we can then work out the lift co-efficient, formula_83, and the drag co-efficient formula_84, from which we can work out the lift and drag forces acting on the blade.\n\nConsider the annular ring, which is partially occupied by blade elements. The length of each blade section occupying the annular ring is formula_85 (see figure below).\n\nThe lift acting on those parts of the blades/airfoils each with chord formula_86 is given by the following expression:\n\nwhere formula_83 is the lift co-efficient, which is a function of the angle of attack, and formula_89 is the number of blades. Additionally, the drag acting on that part of the blades/airfoils with chord formula_86 is given by the following expression:\n\nRemember that these forces calculated are normal and tangential to the apparent speed. We are interested in forces in the formula_92 and formula_93 axes. Thus we need to consider the diagram below:\n\nThus we can see the following:\n\nformula_96 is the force that is responsible for the rotation of the rotor blades; formula_97 is the force that is responsible for the bending of the blades.\n\nRecall that for an isolated system the net angular momentum of the system is conserved. If the rotor acquired angular momentum, so must the fluid in the wake. Let us suppose that the fluid in the wake acquires a tangential velocity formula_98. Thus the torque in the air is given by\nBy the conservation of angular momentum, this balances the torque in the blades of the rotor; thus,\nFurthermore, the rate of change of linear momentum in the air is balanced by the out-of-plane bending force acting on the blades, formula_102. From momentum theory, the rate of change of linear momentum in the air is as follows:\nwhich may be expressed as\nBalancing this with the out-of-plane bending force gives\n\nLet us now make the following definitions:\n\nSo we have the following equations:\n\nLet us make reference to the following equation which can be seen from analysis of the above figure:\n\nThus, with these three equations, it is possible to get the following result through some algebraic manipulation:\n\nWe can derive an expression for formula_52 in a similar manner. This allows us to understand what is going on with the rotor and the fluid. Equations of this sort are then solved by iterative techniques.\n\n"}
{"id": "1573118", "url": "https://en.wikipedia.org/wiki?curid=1573118", "title": "Bushveld", "text": "Bushveld\n\nThe Bushveld is a sub-tropical woodland ecoregion of Southern Africa named after the term \"veld\". It encompasses most of Limpopo Province and a small part of the North West Province of South Africa, the Central and North-East Districts of Botswana and the Matabeleland South and part of the Matabeleland North provinces of Zimbabwe. Kruger National Park in South Africa has a number of 'Bushveld' camps.\n\nThe elevation of this region varies from 750 to 1,400 m and the annual rainfall from 350 mm in the west to 600 mm in parts of the northeast. There are four significant mountain ranges in this region:the Magaliesberg which runs from Rustenburg in the west to Bronkhorstspruit in the east and forms the southern border of the Bushveld; the Drakensberg escarpment that forms the eastern border of the Bushveld and runs from Tzaneen in the north to Belfast in the south; the Waterberg range that is in the middle of the Bushveld and the Soutpansberg range just north of Louis Trichardt. The latter is the northernmost mountain range in South Africa.\n\nAs implied by the region's name, the Bushveld's well-grassed plains are dotted by dense clusters of trees and tall shrubs. The grasses found here are generally tall and turn brown or pale in winter, which is the dry season throughout most of Southern Africa. The undisturbed portions of this habitat, such as much of the Waterberg Biosphere, are home to many large mammal species including white rhino, black rhino, giraffe, blue wildebeest, kudu, impala and a variety of further antelope species and other game.\n\nThe Bushveld is one of the most mineral-rich regions of the world. This is due to the Bushveld igneous complex, an extremely rich saucer-shaped geological formation that stretches over more than 50,000 square kilometers. This formation contains most of the world's reserves of minerals such as andalusite, chromium, fluorspar, platinum and vanadium. The complex includes the Merensky Reef, which is the world's biggest source of platinum as well as platinum-group metals.\n\nAs most of the region tends to be dry, the Bushveld is mostly beef cattle and game farming country, with only a few drought-resistant crops such as sorghum and millet being farmed, usually under irrigation.\n\nThe term \"Middleveld\" is sometimes used to describe land lying between an altitude of 600m and 1200m and has been synonymous with the term \"Bushveld\".\n\nTowns and cities in the region include:\n\n"}
{"id": "6172103", "url": "https://en.wikipedia.org/wiki?curid=6172103", "title": "Cable logging", "text": "Cable logging\n\nCable logging, also referred to as skyline logging, is a logging method primarily used on the West Coast of North America with yarder, loaders, and grapple yarders, but also in Europe (Austria, Switzerland, Czech Republic, France, Italy).\n\nThe cables can be rigged in several configurations.\n\nThere are two classes;\n\nThere are other varieties of loading systems as well. \n\nWhile skyline logging requires additional setup , the vertical lift of the skyline allows faster yarding, which can outweigh the additional labor costs, especially on larger harvest units. \n\nSince the 1980s Grapple Yarders have become popular.\n\nSkyline and grapple yarding, however, require more complex, and expensive equipment. A traditional highlead or gravity system will function with just two cables, a skidding line. As the names imply the skidding line is used to drag the logs in, and the haulback line is used to drag the skidding line back out for the next turn (or group of logs).\n\nA skyline system will add a third line---the skyline whose function is to hold the skidding line and the haulback line off the ground or 'in the sky'.\n\nThe yarder itself is located on a landing, a flat area on top of the ridge that is being logged. After the trees are retrieved by the yarder, the limbs are bucked (removed) and the logs are then placed in piles awaiting transport.\n\n\n\n"}
{"id": "28687028", "url": "https://en.wikipedia.org/wiki?curid=28687028", "title": "Calasparra Photovoltaic Power Plant", "text": "Calasparra Photovoltaic Power Plant\n\nCalasparra Photovoltaic Power Plant () is a photovoltaic power station in Calasparra, Murcia in Spain. The project consists of different production units. Calasparra II is a 6.67 MW ground-mounted unit with estimated annual output of 11.82 GWh. Calasparra III is a 6.6 MW units with estimated annual output of 11.7 GWh. The project was developed by FRV and constructed by Gestamp Solar.\n\n"}
{"id": "45428589", "url": "https://en.wikipedia.org/wiki?curid=45428589", "title": "Carbon fiber testing", "text": "Carbon fiber testing\n\nCarbon fiber testing is a material science test involving the testing of all carbon fiber containing materials. The results for the testing are used to select and design material composites, manufacturing processes and to ensured safety and integrity. Particularly safety-critical carbon fiber parts, such as structural parts in machines, vehicles, aircraft or architectural elements are subject to testing.\n\nTesting sciences classify three main disciplines for material testing that especially apply to carbon fiber materials. Most common are destructive tests, such as stress, fatigue and micro sectioning tests. There are also methods that allow non-destructive testing (NDT), so the material can be still be used after testing. Common methods are ultrasonic, X-ray, HF Eddy Current, Radio Wave testing or thermography. Additionally, Structural Health Monitoring (SHM) methods allow the testing during application.\n\nCarbon (carbon fiber reinforced plastic and reinforced polymers) are gaining importance as light-weight material. Particularly safety-critical carbon fiber parts, such as aircraft frames, need to be tested destructively (e.g. stress, fatigue) and non-destructively (e.g. fiber orientation, delamination, bonding). Typically, destructive tests are carried out to validate the mechanical properties, whereas NDT are used to monitor and control the manufacturing process of the CFRP parts. As carbon fiber composites are highly individual in shape and material composition novel NDT are emerging and sought for application. Applicable technologies are radio wave testing and high frequency eddy current testing, thermography, shearography, air coupled laser ultrasonics, terahertz scanning\n\nThe specifications for integrity of structurally relevant parts depend on the individual manufacturer. However, typically relevant quality criteria of the texture are fiber orientation, gaps, wrinkles, overlaps, distortion, undulation, uniformity as well as defects in the matrix delamination, inclusion, cracks, curing, void, debonding. Furthermore, basis weight or carbon fiber volume content are important properties. \nGenerally, defects and effects in carbon fiber materials are classified according to their location as structural defects (carbon fiber related) and matrix defects (resin related). Carbon fiber related effects are tested with X-ray and high frequency testing methods whereas matrix effects are commonly tested with ultrasonic and thermographic methods.\n\n"}
{"id": "28215750", "url": "https://en.wikipedia.org/wiki?curid=28215750", "title": "Colony (film)", "text": "Colony (film)\n\nColony is an Irish documentary film about colony collapse disorder, directed by Carter Gunn and Ross McDonnell. The film was produced by Morgan Bushe and Macdara Kelleher. It opened theatrically in Los Angeles on July 30, 2010 and New York City on August 13, 2010 at the 14th Annual DocuWeeks.\n\n"}
{"id": "2120812", "url": "https://en.wikipedia.org/wiki?curid=2120812", "title": "Corrosion inhibitor", "text": "Corrosion inhibitor\n\nA corrosion inhibitor is a chemical compound that, when added to a liquid or gas, decreases the corrosion rate of a material, typically a metal or an alloy. The effectiveness of a corrosion inhibitor depends on fluid composition, quantity of water, and flow regime. A common mechanism for inhibiting corrosion involves formation of a coating, often a passivation layer, which prevents access of the corrosive substance to the metal. Permanent treatments such as chrome plating are not generally considered inhibitors, however. Instead corrosion inhibitors are \"additives\" to the fluids that surround the metal or related object.\n\nCorrosion inhibitors are common in industry, and also found in over-the-counter products, typically in spray form in combination with a lubricant and sometimes a penetrating oil.\n\nThe nature of the corrosive inhibitor depends on (i) the material being protected, which are most commonly metal objects, and (ii) on the corrosive agent(s) to be neutralized. The corrosive agents are generally oxygen, hydrogen sulfide, and carbon dioxide. Oxygen is generally removed by reductive inhibitors such as amines and hydrazines:\nIn this example, hydrazine converts oxygen, a common corrosive agent, to water, which is generally benign. Related inhibitors of oxygen corrosion are hexamine, phenylenediamine, and dimethylethanolamine, and their derivatives. Antioxidants such as sulfite and ascorbic acid are sometimes used. Some corrosion inhibitors form a passivating coating on the surface by chemisorption. Benzotriazole is one such species used to protect copper. For lubrication, zinc dithiophosphates are common - they deposit sulfide on surfaces.\n\nThe suitability of any given chemical for a task in hand depends on many factors, including their operating temperature.\n\nCorrosion inhibitors are commonly added to coolants, fuels, hydraulic fluids, boiler water, engine oil, and many other fluids used in industry. For fuels, various corrosion inhibitors can be used. Some components include zinc dithiophosphates.\n\n\n"}
{"id": "34247261", "url": "https://en.wikipedia.org/wiki?curid=34247261", "title": "Cryptocentrus leonis", "text": "Cryptocentrus leonis\n\nCryptocentrus leonis is a species of goby native to marine and brackish waters along the shores of the South China Sea.\n"}
{"id": "22832517", "url": "https://en.wikipedia.org/wiki?curid=22832517", "title": "Crystal structure prediction", "text": "Crystal structure prediction\n\nCrystal structure prediction (CSP) is the calculation of the crystal structures of solids from first principles. Reliable methods of predicting the crystal structure of a compound, based only on its composition, has been a goal of the physical sciences since the 1950s. Computational methods employed include simulated annealing, evolutionary algorithms, distributed multipole analysis, random sampling, basin-hopping, data mining, density functional theory and molecular mechanics.\n\nThe crystal structures of simple ionic solids have long been rationalised in terms of Pauling's rules, first set out in 1929 by Linus Pauling. For metals and semiconductors one has different rules involving valence electron concentration. However, prediction and rationalization are rather different things. Most commonly, the term crystal structure prediction means a search for the minimum-energy arrangement of its constituent atoms (or, for molecular crystals, of its molecules) in space. The problem has two facets: combinatorics (the \"search phase space\", in practice most acute for inorganic crystals), and energetics (or \"stability ranking\", most acute for molecular organic crystals).\nFor complex non-molecular crystals (where the \"search problem\" is most acute), major recent advances have been the development of the Martonak version of metadynamics, the Oganov-Glass evolutionary algorithm USPEX, and first principles random search. The latter are capable of solving the global optimization problem with up to around a hundred degrees of freedom, while the approach of metadynamics is to reduce all structural variables to a handful of \"slow\" collective variables (which often works).\n\nPredicting organic crystal structures is important in academic and industrial science, particularly for pharmaceuticals and pigments, where understanding polymorphism is beneficial. The crystal structures of molecular substances, particularly organic compounds, are very hard to predict and rank in order of stability. Intermolecular interactions are relatively weak and non-directional and long range. This results in typical lattice and free energy differences between polymorphs that are often only a few kJ/mol, very rarely exceeding 10 kJ/mol. Crystal structure prediction methods often locate many possible structures within this small energy range. These small energy differences are challenging to predict reliably without excessive computational effort.\n\nSince 2007, significant progress has been made in the CSP of small organic molecules, with several different methods proving effective. The most widely discussed method first ranks the energies of all possible crystal structures using a customised MM force field, and finishes by using a dispersion-corrected DFT step to estimate the lattice energy and stability of each short-listed candidate structure. More recent efforts to predict crystal structures have focused on estimating crystal free energy by including the effects of temperature and entropy in organic crystals using vibrational analysis or molecular dynamics.\n\nThe following codes can predict stable and metastable structures given chemical composition and external conditions (pressure, temperature):\n"}
{"id": "50946947", "url": "https://en.wikipedia.org/wiki?curid=50946947", "title": "First law of thermodynamics (fluid mechanics)", "text": "First law of thermodynamics (fluid mechanics)\n\nIn physics, the first law of thermodynamics is an expression of the conservation of total energy of a system. The increase of the energy of a system is equal to the sum of work done on the system and the heat added to that system:\n\nwhere\n\nIn fluid mechanics, the first law of thermodynamics takes the following form:\n\nwhere\n\nBecause it expresses conservation of total energy, this is sometimes referred to as the energy balance equation of continuous media. The first law is used to derive the non-conservation form of the Navier–Stokes equations.\n\nWhere\n\nThat is, pulling is positive stress and pushing is negative stress.\n\nFor a compressible fluid the left hand side of equation becomes:\n\nbecause in general\n\nThat is, the change in the internal energy of the substance within a volume is the negative of the amount carried out of the volume by the flow of material across the boundary plus the work done compressing the material on the boundary minus the flow of heat out through the boundary. More generally, it is possible to incorporate source terms.\n\nwhere formula_17 is specific enthalpy, formula_18 is dissipation function and formula_19 is temperature. And where\ni.e. internal energy per unit volume equals mass density times the sum of: proper energy per unit mass, kinetic energy per unit mass, and gravitational potential energy per unit mass.\n\ni.e. change in heat per unit volume (negative divergence of heat flow) equals the divergence of heat conductivity times the gradient of the temperature.\n\ni.e. divergence of work done against stress equals flow of material times divergence of stress plus stress times divergence of material flow.\n\ni.e. stress times divergence of material flow equals deviatoric stress tensor times divergence of material flow minus pressure times material flow.\n\ni.e. enthalpy per unit mass equals proper energy per unit mass plus pressure times volume per unit mass (reciprocal of mass density).\n\n\n\n"}
{"id": "930109", "url": "https://en.wikipedia.org/wiki?curid=930109", "title": "Floating production storage and offloading", "text": "Floating production storage and offloading\n\nA floating production storage and offloading (FPSO) unit is a floating vessel used by the offshore oil and gas industry for the production and processing of hydrocarbons, and for the storage of oil. A FPSO vessel is designed to receive hydrocarbons produced by itself or from nearby platforms or subsea template, process them, and store oil until it can be offloaded onto a tanker or, less frequently, transported through a pipeline. FPSOs are preferred in frontier offshore regions as they are easy to install, and do not require a local pipeline infrastructure to export oil. FPSOs can be a conversion of an oil tanker or can be a vessel built specially for the application. A vessel used only to store oil (without processing it) is referred to as a floating storage and offloading (FSO) vessel.\n\nRecent developments in LNG industry require relocation of conventional LNG processing trains into the sea to unlock remote, smaller gas fields that would not be economical to develop otherwise, reduce capital expenses, and impact to environment. Emerging new type of FLNG facilities will be used. Unlike FPSOs apart of gas production, storage and offloading, they will also allow full scale deep processing, same as onshore LNG plant has to offer but squeezed to 25% of its footprint.\nFirst 3 FLNG's are under construction (as at 2016): Prelude FLNG (Shell), PFLNG1 and PFLNG2 (Petronas).\n\nOil has been produced from offshore locations since the late 1940s. Originally, all oil platforms sat on the seabed, but as exploration moved to deeper waters and more distant locations in the 1970s, floating production systems came to be used.\n\nThe first oil FPSO was built in 1977 on the Shell Castellon field, located in the Spanish Mediterranean. Today, over 270 vessels are deployed worldwide as oil FPSOs.\n\nOn July 29, 2009, Shell and Samsung announced an agreement to build up to 10 LNG FPSOs, at same Samsung Yard Flex LNG appeared to construct smaller units.\n\nOn May 20, 2011, Royal Dutch Shell announced the planned development of a floating liquefied natural gas facility (FLNG), called Prelude with 488 m long and 74 m wide, which is to be situated 200 km off the coast of Western Australia and is due for completion in around 2016, the largest vessel man-made ever. Royal Dutch Shell (2013), LNG FPSO (Liquefied Natural Gas Floating production Storage and Offloading), Samsung Heavy Industries at a cost of $12 Billion.\n\nIn June 2012, Petronas made a contract of procurement engineering, construction, installation and commissioning, a project with the Technip and DSME consortium. The unit is destined for the Kanowit gas field off Sarawak, Malaysia. It is expected to be the World's First Floating Liquefaction Unit in operation when completed in 2015.\n\nAt the opposite (discharge and regasification) end of the LNG chain, the first ever conversion of an LNG carrier, Golar LNG owned Moss type LNG carrier into an LNG floating storage and regasification unit was carried out in 2007 by Keppel shipyard in Singapore.\n\nOil produced from offshore production platforms can be transported to the mainland either by pipeline or by tanker. When a tanker is chosen to transport the oil, it is necessary to accumulate oil in some form of storage tank, such that the oil tanker is not continuously occupied during oil production, and is only needed once sufficient oil has been produced to fill the tanker.\n\nFloating production, storage and offloading vessels are particularly effective in remote or deep water locations, where seabed pipelines are not cost effective. FPSOs eliminate the need to lay expensive long-distance pipelines from the processing facility to an onshore terminal. This can provide an economically attractive solution for smaller \"oil fields\", which can be exhausted in a few years and do not justify the expense of installing a pipeline. Furthermore, once the field is depleted, the FPSO can be moved to a new location.\n\nA Floating Storage and Offloading unit (FSO) is essentially a simplified FPSO, without the capability for oil or gas processing. Most FSOs are converted \"single hull\" supertankers. An example is \"Knock Nevis\", ex \"Seawise Giant\", which for many years was the world's largest ship. It was converted into an FSO for offshore use before being scrapped.\n\nAt the other end of the LNG logistics chain, where the natural gas is brought back to ambient temperature and pressure, specially modified ships may also be used as floating storage and regasification units (FSRUs). A LNG floating storage and regasification unit receives liquefied natural gas (LNG) from offloading LNG carriers, and the onboard regasification system provides natural gas exported to shore through risers and pipelines.\n\n\nIn addition to the historical records already mentioned, a few more are added in this section.\n\nThe FPSO operating in the deepest waters is the FPSO \"BW Pioneer\", built and operated by BW Offshore on behalf of Petrobras Americas INC. The FPSO is moored at a depth of 2,600 m in Block 249 Walker Ridge in the US Gulf of Mexico and is rated for . The EPCI contract was awarded in October 2007 and production started in early 2012. The FPSO conversion was carried out at \"MMHE Shipyard Pasir Gudang\" in Malaysia, while the topsides were fabricated in modules at various international vendor locations. The FPSO has a disconnectable turret (APL). The vessel can disconnect in advance of hurricanes and reconnect with minimal down time. A contract for an FPSO to operate in even deeper waters (2,900 m) for Shell's Stones field in the US Gulf of Mexico was awarded to SBM Offshore in July 2013.\n\nOne of the world's largest FPSO is the \"Kizomba A\", with a storage capacity of . Built at a cost of over US$ 800 million by Hyundai Heavy Industries in Ulsan, Korea, it is operated by Esso Exploration Angola (ExxonMobil). Located in 1200 meters (3,940 ft) of water at Deep water block 200 statute miles (320 km) offshore in the Atlantic Ocean from Angola, Central Africa, it weighs 81,000 tonnes and is 285 meters long, 63 meters wide, and 32 meters high (935 ft by by 105 ft).\n\nThe first FSO in the Gulf of Mexico, The FSO \"Ta'Kuntah\", has been in operation since August 1998. The FSO, owned and operated by MODEC, is under a service agreement with \"PEMEX\" Exploration and Production. The vessel was installed as part of the Cantarell Field Development. The field is located in the Bay of Campeche, offshore Mexico's Yucatán peninsula. It is a converted ULCC tanker with a SOFEC external turret mooring system, two flexible risers connected in a lazy-S configuration between the turret and a pipeline end manifold (PLEM) on the seabed, and a unique offloading system. The FSO is designed to handle with no allowance for downtime.\n\nThe Skarv FPSO, developed and engineered by \"Aker Solutions\" for \"BP Norge\", is one of the most advanced and largest FPSO deployed in the Norwegian Sea, offshore Mid Norway. \"Skarv\" is a gas condensate and oil field development. The development ties in five sub-sea templates, and the FPSO has capacity to include several smaller wells nearby in the future. The process plant on the vessel can handle about of gas and of oil. An 80 km gas export pipe ties into Åsgard transport system. \"Aker Solutions\" (formerly Aker Kvaerner) developed the front-end design for the floating production facility as well as the overall system design for the field and preparation for procurement and project management of the total field development. The hull is an Aker Solutions proprietary \"Tentech975\" design. BP also selected Aker Solutions to perform the detail engineering, procurement and construction management assistance (EPcma) for the Skarv field development. The \"EPcma\" contract covers detail engineering and procurement work for the FPSO topsides as well as construction management assistance to BP including hull and topside facilities. The production started in field on August 2011. BP awarded the contract for fabrication of the \"Skarv\" FPSO hull to \"Samsung Heavy Industries\" in South Korea and the Turret contract to SBM. The FPSO has a length of 292 m, beam of 50.6 m and is 29 m deep, accommodates about 100 people in single cabins. The hull is delivered in January 2010.\n\n, there are more than 277 Floating Production Units in use around the world, of which 62% are FPSOs. Largest User of them is Petrobrás as of January 2015.\n\n"}
{"id": "89480", "url": "https://en.wikipedia.org/wiki?curid=89480", "title": "Flywheel", "text": "Flywheel\n\nA flywheel is a mechanical device specifically designed to efficiently store rotational energy. Flywheels resist changes in rotational speed by their moment of inertia. The amount of energy stored in a flywheel is proportional to the square of its rotational speed. The way to change a flywheel's stored energy is by increasing or decreasing its rotational speed by applying a torque aligned with its axis of symmetry,\n\nCommon uses of a flywheel include:\n\nFlywheels are typically made of steel and rotate on conventional bearings; these are generally limited to a maximum revolution rate of a few thousand RPM. High energy density flywheels can be made of carbon fiber composites and employ magnetic bearings, enabling them to revolve at speeds up to 60,000 RPM (1 kHz).\n\nCarbon-composite flywheel batteries have recently been manufactured and are proving to be viable in real-world tests on mainstream cars. Additionally, their disposal is more eco-friendly than traditional lithium ion batteries.\n\nFlywheels are often used to provide continuous power output in systems where the energy source is not continuous. For example, a flywheel is used to smooth fast angular velocity fluctuations of the crankshaft in a reciprocating engine. In this case, a crankshaft flywheel stores energy when torque is exerted on it by a firing piston, and returns it to the piston to compress a fresh charge of air and fuel. Another example is the friction motor which powers devices such as toy cars. In unstressed and inexpensive cases, to save on cost, the bulk of the mass of the flywheel is toward the rim of the wheel. Pushing the mass away from the axis of rotation heightens rotational inertia for a given total mass.\nA flywheel may also be used to supply intermittent pulses of energy at power levels that exceed the abilities of its energy source. This is achieved by accumulating energy in the flywheel over a period of time, at a rate that is compatible with the energy source, and then releasing energy at a much higher rate over a relatively short time when it is needed. For example, flywheels are used in power hammers and riveting machines.\n\nFlywheels can be used to control direction and oppose unwanted motions, see gyroscope. Flywheels in this context have a wide range of applications from gyroscopes for instrumentation to ship stability and satellite stabilization (reaction wheel), to keep a toy spin spinning (friction motor), to stabilize magnetically levitated objects (Spin-stabilized magnetic levitation)\n\nThe principle of the flywheel is found in the Neolithic spindle and the potter's wheel, as well as circular sharpening stones in antiquity.\n\nThe use of the flywheel as a general mechanical device to equalize the speed of rotation is, according to the American medievalist Lynn White, recorded in the \"De diversibus artibus\" (On various arts) of the German artisan Theophilus Presbyter (ca. 1070–1125) who records applying the device in several of his machines.\n\nIn the Industrial Revolution, James Watt contributed to the development of the flywheel in the steam engine, and his contemporary James Pickard used a flywheel combined with a crank to transform reciprocating motion into rotary motion.\n\nA flywheel is a spinning wheel, or disc, or rotor, rotating around its symmetry axis. Energy is stored as kinetic energy, more specifically rotational energy, of the rotor :\n\n\nwhere:\n\n\nwhere formula_7 denotes mass, and formula_8 denotes a radius.\n\nWhen calculating with SI units, the units would be for mass, kilograms; for radius, meters; and for angular velocity, radians per second and the resulting energy would be in joules.\n\nIncreasing amounts of rotation energy can be stored in the flywheel until the rotor shatters.\nThis happens when the hoop stress within the rotor exceeds the ultimate tensile strength of the rotor material. \n\nwhere:\n\n\nFlywheels are made from many different materials; the application determines the choice of material. Small flywheels made of lead are found in children’s toys. Cast iron flywheels are used in old steam engines. Flywheels used in car engines are made of cast or nodular iron, steel or aluminum. Flywheels made from high-strength steel or composites have been proposed for use in vehicle energy storage and braking systems.\n\nThe efficiency of a flywheel is determined by the maximum amount of energy it can store per unit weight. As the flywheel’s rotational speed or angular velocity is increased, the stored energy increases; however, the stresses also increase. If the hoop stress surpass the tensile strength of the material, the flywheel will break apart. Thus, the tensile strength limits the amount of energy that a flywheel can store.\n\nIn this context, using lead for a flywheel in a child’s toy is not efficient; however, the flywheel velocity never approaches its burst velocity because the limit in this case is the pulling-power of the child. In other applications, such as an automobile, the flywheel operates at a specified angular velocity and is constrained by the space it must fit in, so the goal is to maximize the stored energy per unit volume. The material selection therefore depends on the application.\n\nThe table below contains calculated values for materials and comments on their viability for flywheel applications. CFRP stands for carbon-fiber-reinforced polymer, and GFRP stands for glass-fiber reinforced polymer.\nThe table below shows calculated values for mass, radius, and angular velocity for storing 500 J. The carbon-fiber flywheel is by far the most efficient; however, it also has the largest radius. In applications (like in an automobile) where the volume is constrained, a carbon-fiber flywheel might not be the best option.\nFor comparison, the energy density of petrol (gasoline) is 44.4 MJ/kg or 12.3 kWh/kg.\n\nFor a given flywheel design, the kinetic energy is proportional to the ratio of the hoop stress to the material density and to the mass:\n\n\nformula_15 could be called the specific tensile strength. The flywheel material with the highest specific tensile strength will yield the highest energy storage per unit mass. This is one reason why carbon fiber is a material of interest.\n\nFor a given design the stored energy is proportional to the hoop stress and the volume:\n\n\nA rimmed flywheel has a rim, a hub, and spokes. Calculation of the flywheel's moment of inertia can be more easily analysed by applying various simplifications. For example:\n\nFor example, if the moments of inertia of hub, spokes and shaft are deemed negligible, and the rim's thickness is very small compared to its mean radius (formula_18), the radius of rotation of the rim is equal to its mean radius and thus:\n\n\n\n"}
{"id": "19449848", "url": "https://en.wikipedia.org/wiki?curid=19449848", "title": "Freeze thaw resistance", "text": "Freeze thaw resistance\n\nFreeze thaw resistance, or freezing and thawing resistance, is the property of solids to resist cyclic freezing and melting.\n\n"}
{"id": "18981554", "url": "https://en.wikipedia.org/wiki?curid=18981554", "title": "Grenfell Cloth", "text": "Grenfell Cloth\n\nGrenfell Cloth is a densely-woven cotton gabardine material used to make luxury and outdoor clothing since its creation in 1923. It was named after Sir Wilfred Grenfell, a British medical missionary working extensively in Newfoundland. He required a cloth to be woven to protect himself from the snow, wind, wet and cold weather he encountered in his work.\n\nThe cloth is made from 600 thread-per-inch cotton originally by T. Haythornthwaite & Sons Ltd at Lodge Mill, Burnley, in the United Kingdom. It is similar to Byrd Cloth.\n\nAfter a spell under Japanese ownership in the 80's and 90's, Grenfell Cloth garments are once again manufactured in Britain. Grenfell is now based in London.\n\nGrenfell Cloth has been used on many expeditions.\n\n"}
{"id": "6953458", "url": "https://en.wikipedia.org/wiki?curid=6953458", "title": "Hadley cell", "text": "Hadley cell\n\nThe Hadley cell, named after George Hadley, is a global scale tropical atmospheric circulation that features air rising near the Equator, flowing poleward at 10–15 kilometers above the surface, descending in the subtropics, and then returning equatorward near the surface. This circulation creates the trade winds, tropical rain-belts and hurricanes, subtropical deserts and the jet streams.\n\nIn each hemisphere, there is one primary circulation cell known as a Hadley cell and two secondary circulation cells at higher latitudes; between 30° and 60° latitude known as the Ferrel cell and beyond 60° as the Polar cell.\n\nThe major driving force of atmospheric circulation is the uneven distribution of solar heating across the Earth, which is greatest near the equator and lesser at the poles. The atmospheric circulation transports energy polewards, thus reducing the resulting equator-to-pole temperature gradient. The mechanisms by which this is accomplished differ in tropical and extratropical latitudes.\n\nHadley cells exist on either side of the equator. Each cell encircles the globe latitudinally and acts to transport energy from the equator to about the 30th latitude. The circulation exhibits the following phenomena:\nThe Hadley circulation exhibits seasonal variation. During the solstitial seasons (DJF and JJA), the upward branch of the Hadley cell occurs not directly over the equator but rather in the summer hemisphere. In the annual mean, the upward branch is slightly offset into the northern hemisphere, making way for a stronger Hadley cell in the southern hemisphere. This evidences a small net energy transport from the northern to the southern hemisphere.\n\nThe Hadley system provides an example of a thermally direct circulation. The thermodynamic efficiency of the Hadley system, considered as a heat engine, has been relatively constant over the 1979–2010 period, averaging 2.6%. Over the same interval, the power generated by the Hadley regime has risen at an average rate of about 0.54 TW per year; this reflects an increase in energy input to the system consistent with the observed increasing of tropical sea surface temperatures.\n\nOverall, mean meridional circulation cells such as the Hadley circulation are not particularly efficient at reducing the equator-to-pole temperature gradient due to cancellation between transports of different types of energy. In the Hadley cell, both sensible and latent heat are transported equatorward near the surface, while potential energy is transported above in the opposite direction, poleward. The resulting net poleward transport is only about 10% of this potential energy transport. This is partly a result of the strong constraints imposed on atmospheric motions by the conservation of angular momentum.\n\nIn the early 18th century, George Hadley, an English lawyer and amateur meteorologist, was dissatisfied with the theory that the astronomer Edmond Halley had proposed for explaining the trade winds. What was no doubt correct in Halley's theory was that solar heating creates upward motion of equatorial air, and air mass from neighboring latitudes must flow in to replace the risen air mass. But for the westward component of the trade winds Halley had proposed that in moving across the sky the Sun heats the air mass differently over the course of the day. Hadley was not satisfied with that part of Halley's theory and rightly so. Hadley was the first to recognize that Earth's rotation plays a role in the direction taken by an air mass as it moves relative to the Earth. Hadley's theory, published in 1735, remained unknown, but it was rediscovered independently several times. Among the re-discoverers was John Dalton, who later learned of Hadley's priority. Over time the mechanism proposed by Hadley became accepted, and over time his name was increasingly attached to it. By the end of the 19th century it was shown that Hadley's theory was deficient in several respects. One of the first who accounted for the dynamics correctly was William Ferrel. It took many decades for the correct theory to become accepted, and even today Hadley's theory can still be encountered occasionally, particularly in popular books and websites. Hadley's theory was the generally accepted theory long enough to make his name become universally attached to the circulation pattern in the tropical atmosphere. In 1980 Isaac Held and Arthur Hou developed the Held-Hou Model to describe the Hadley circulation. \n\nThe region in which the equatorward moving air masses converge and rise, is known as the intertropical convergence zone, or ITCZ. Within that zone develops a band of thunderstorms that produce high-precipitation.\n\nHaving lost most of its water vapor to condensation and precipitation in the upward branch of the Hadley cell circulation, the descending air is dry. As the air descends, low relative humidities are produced as the air is warmed adiabatically by compression from the overlying air, producing a region of higher pressure. The subtropics are relatively free of the convection, or thunderstorms, that are common in the equatorial belt. Many of the world's deserts are located in these subtropical latitudes. Although the deserts do not extend to the eastern side of the various continents because of ocean currents caused by the Trade Winds.\n\nThere is some evidence that the expansion of the Hadley cells is related to climate change. The majority of Earth's arid regions are located in the areas beneath the descending part of the Hadley circulation at around 30 degrees latitude. Those models show that the Hadley cell will expand with increased global mean temperature (perhaps by 2 degrees latitude over the 21st century ). This might lead to large changes in precipitation in the latitudes at the edge of the cells. Scientists fear that global warming might bring changes to the ecosystems in the deep tropics and that the deserts will become drier and expand. As the areas around 30 degrees latitude become drier, those inhabiting that region will see less rainfall than traditionally expected, which could cause difficulty with food supplies and livability. There is strong evidence of paleoclimate climate change in central Africa's rain forest in c. 850 B.C. Palynological (fossil pollen) evidence shows a drastic change in rain forest biome to that of open savannah as a consequence of wide-scale drying not connected necessarily to intermittent drought but perhaps to gradual warming. The hypothesis, that a decline in solar activity reduces the latitudinal extent of the Hadley Circulation and decreases mid-latitudinal monsoon intensity, is matched by data, showing increased dryness in central west Africa and increase in precipitation in temperate zones north. Meanwhile, mid-latitudinal storm tracks in the temperate zones increased and moved equatorward.\n\n\n"}
{"id": "15169933", "url": "https://en.wikipedia.org/wiki?curid=15169933", "title": "Jebel Ali refinery", "text": "Jebel Ali refinery\n\nThe Jebel Ali refinery was constructed from 1996 to 1999. The refinery is operated by the Emirates National Oil Co. (ENOC), which is owned by the Government of Dubai. It is a 120 kbpd gas condensate refinery, and processes mainly condensate or light crude oil. These inputs are processed to various products, including LPG, naphtha, jet fuel, diesel oil and fuel oil. The plant consists of two 60 kbit/s condensate distillation units (often referred to as condensate splitters) and five merox sweetening units. \n\nAn upgrade - costing $850 million (US) - was implemented in 2010 for the production of reformat and low-sulfur naphtha through installation of a reformer and a hydrotreater. In 2014, a contract for a new upgrade project was awarded to KBR. The upgrade will lead to the production of Euro V grade products. \n\n\n"}
{"id": "30555998", "url": "https://en.wikipedia.org/wiki?curid=30555998", "title": "Kondo insulator", "text": "Kondo insulator\n\nIn solid-state physics, Kondo insulators (also referred as Kondo semiconductors and heavy fermion semiconductors) are understood as materials with strongly correlated electrons, that open up a narrow band gap (in the order of 10 meV) at low temperatures with the chemical potential lying in the gap, whereas in heavy fermions the chemical potential is located in the conduction band. The band gap opens up at low temperatures due to hybridization of localized electrons (mostly f-electrons) with conduction electrons, a correlation effect known as the Kondo effect. As a consequence, a transition from metallic behavior to insulating behavior is seen in resistivity measurements. The band gap could be either direct or indirect. Most studied Kondo insulators are FeSi, CeBiPt, SmB, YbB, and CeNiSn.\n\nIn 1969, Menth \"et al.\" found no magnetic ordering in SmB down to 0.35 K and a change from metallic to insulating behavior in the resistivity measurement with decreasing temperature. They interpreted this phenomenon as a change of the electronic configuration of Sm.\n\nGabriel Aeppli and Zachary Fisk found a descriptive way to explain the physical properties of CeBiPt and CeNiSn in 1992. They called the materials Kondo insulators, showing Kondo lattice behavior near room temperature, but becoming semiconducting with very small energy gaps (a few Kelvin to a few tens of Kelvin) when decreasing the temperature.\n\nAt high temperatures the localized f-electrons form independent local magnetic moments. According to the Kondo effect, the dc-resistivity of Kondo insulators shows a logarithmic temperature-dependence. At low temperatures, the local magnetic moments are screened by the sea of conduction electrons, forming a so-called Kondo resonance. The interaction of the conduction band with the f-orbitals results in a hybridization and an energy gap formula_1. If the chemical potential lies in the hybridization gap, an insulating behavior can be seen in the dc-resistivity at low temperatures.\n\n"}
{"id": "6989352", "url": "https://en.wikipedia.org/wiki?curid=6989352", "title": "Kootenay Lake Crossing", "text": "Kootenay Lake Crossing\n\nKootenay Lake Crossing was a powerline crossing of Kootenay Lake, British Columbia, Canada. The idea was to move electricity from the Hydro dam on the Kootenay River\nat Brilliant to the Cominco Sullivan Mine at Kimberley. The most direct route was across Kootenay Lake. It was with a span width of the longest powerline span of North America and was fixed on two tall electricity pylons. Roebling cable provided the steel support cable for the conductor. The height of conductor over Kootenay Lake was .\nThere was a steel support tower on the east shore, and rock anchored shorter towers high up on the west shore. Kootenay Lake Crossing was inaugurated in 1958, but was demolished in 1962 by explosives placed by the Sons of Freedom religious sect of the Doukhobors. The tower was rebuilt and the span lengthened in course. The powerline is still in operation today, but uses today on each site of the lake (for each phase a separate tower) situated at 49°42'2\"N 116°51'59\"W and at 49°42'29\"N 116°54'44\"W .\n\n"}
{"id": "28394939", "url": "https://en.wikipedia.org/wiki?curid=28394939", "title": "La Magascona and Magasquila photovoltaic power stations", "text": "La Magascona and Magasquila photovoltaic power stations\n\nThe La Magascona and Magasquila photovoltaic power stations ( and is a complex of photovoltaic power stations located at La Magascona, Trujillo in Cáceres, Spain. The La Magascona photovoltaic power station covers and it has a peak output of 23.04 MW. The power station produces approximately 46 GWh of electricity per year. It was commissioned in July 2007.\n\nThe Magasquila photovoltaic power station covers and it has a peak output of 11.52 MW. It was commissioned in June 2008.\n"}
{"id": "1376805", "url": "https://en.wikipedia.org/wiki?curid=1376805", "title": "Laminate flooring", "text": "Laminate flooring\n\nLaminate flooring (also called floating wood tile in the United States) is a multi-layer synthetic flooring product fused together with a lamination process. Laminate flooring simulates wood (or sometimes stone) with a photographic applique layer under a clear protective layer. The inner core layer is usually composed of melamine resin and fiber board materials. There is a European Standard No. EN 13329:2000 specifying laminate floor covering requirements and testing methods.\n\nLaminate flooring has grown significantly in popularity, perhaps because it may be easier to install and maintain than more traditional surfaces such as hardwood flooring. It may also have the advantages of costing less and requiring less skill to install than alternative flooring materials. It is reasonably durable, hygienic (several brands contain an antimicrobial resin), and relatively easy to maintain.\n\nLaminate floors are reasonably easy for a DIY homeowner to install. Laminate flooring is packaged as a number of tongue and groove planks, which can be clicked into one another. Sometimes a glue backing is provided for ease of installation. Installed laminate floors typically \"float\" over the sub-floor on top of a foam/film underlayment, which provides moisture- and sound-reducing properties. A small () gap is required between the flooring and any immovable object such as walls, this allows the flooring to expand without being obstructed. Baseboards (skirting boards) can be removed and then reinstalled after laying of the flooring is complete for a neater finish, or the baseboard can be left in place with the flooring butted into it, then small beading trims such as shoe moulding or the larger quarter-round moulding can be fitted to the bottoms of the baseboards. Saw cuts on the planks are usually required at edges and around cupboard and door entrances, but professional installers typically use door jamb undercut saws to cut out a space to a height that allows the flooring to go under the door jamb & casing for a cleaner look.\n\nImproper installation can result in peaking, in which adjacent boards form a V shape projecting from the floor, or gaps, in which two adjacent boards are separated from each other.\n\nIt is important to keep laminate clean, as dust, dirt, and sand particles may scratch the surface over time in high-traffic areas. It is also important to keep laminate relatively dry, since sitting water/moisture can cause the planks to swell, warp, etc., though some brands are equipped with water-resistant coatings. Water spills aren't a problem if they're wiped up quickly, and not allowed to sit for a prolonged period of time.\n\nAdhesive felt pads are often placed on the feet of furniture on laminate floors to prevent scratching.\n\nInferior glueless laminate floors may gradually become separated, creating visible gaps between planks. It is important to \"tap\" the planks back together using the appropriate tool as gaps are noticed in order to prevent dirt filling the gaps, thus making it more difficult to put into place.\n\nQuality glueless laminate floors use joining mechanisms which hold the planks together under constant tension which prevent dirt entering the joints and do not need \"tapping\" back together periodically.\n\nThe North American Laminate Flooring Association (NALFA) is a trade association of laminate flooring manufacturers and laminate flooring manufacturer suppliers in the United States and Canada. It is a standards developing organization accredited by the American National Standards Institute (ANSI) to develop voluntary consensus standards for laminate flooring materials, and it has established testing and performance criteria that are used in North America.\n\nNALFA issues a certification mark named the NALFA Certification Seal which signifies that the product has passed 10 performance tests, has been proven to meet these standards by an independent, third-party testing lab, and has been manufactured in North America. The certification review includes:\n\nLaminate flooring is often made of melamine resin, a compound made with formaldehyde. The formaldehyde is more tightly bound in Melamine-Formaldehyde (MF) than it is in Urea-Formaldehyde (UF), reducing emissions and potential health effects. Thus LEED v2.2's EQ Credit 4.4 precludes the use of UF, but allows the use of MF.\n\nLaminated flooring is commonly used in LEED residential and commercial applications.\n\nLaminate flooring was invented in 1977\nby the Swedish company , and sold under the brand name Pergo. They had been making floor surfaces since 1923. The company first marketed its product to Europe in 1984, and later to the United States in 1994. Perstorp spun off its flooring division as the separate company named Pergo, now a subsidiary of Mohawk Industries. Pergo is the most widely known laminate flooring manufacturer, but the trademark PERGO is not synonymous for all laminate floors.\n\nGlueless laminate flooring was invented in 1996 by the Swedish company Välinge Aluminium (now Välinge Innovation) and sold under the names of Alloc and Fiboloc. However, a system for holding flooring panels together was also developed in parallel by the Belgian company Unilin released in 1997 and sold under the name of .\n\nThe two companies have been in a great number of legal conflicts over the years, and today most, if not all glueless locking flooring is made under license from either Välinge, Unilin, or even a combination of both.\n\n"}
{"id": "15798971", "url": "https://en.wikipedia.org/wiki?curid=15798971", "title": "Lattice plane", "text": "Lattice plane\n\nIn crystallography, a lattice plane of a given Bravais lattice is a plane (or family of parallel planes) whose intersections with the lattice (or any crystalline structure of that lattice) are periodic (i.e. are described by 2d Bravais lattices) and intersect the Bravais lattice; equivalently, a lattice plane is any plane containing at least three noncollinear Bravais lattice points. All lattice planes can be described by a set of integer Miller indices, and vice versa (all integer Miller indices define lattice planes).\n\nConversely, planes that are \"not\" lattice planes have \"aperiodic\" intersections with the lattice called quasicrystals; this is known as a \"cut-and-project\" construction of a quasicrystal (and is typically also generalized to higher dimensions).\n"}
{"id": "27700947", "url": "https://en.wikipedia.org/wiki?curid=27700947", "title": "Liquid dielectric", "text": "Liquid dielectric\n\nA liquid dielectric is a dielectric material in liquid state. Its main purpose is to prevent or rapidly quench electric discharges. Dielectric liquids are used as electrical insulators in high voltage applications, e.g. transformers, capacitors, high voltage cables, and switchgear (namely high voltage switchgear). Its function is to provide electrical insulation, suppress corona and arcing, and to serve as a coolant.\n\nA good liquid dielectric should have high dielectric strength, high thermal stability and chemical inertness against the construction materials used, non-flammability and low toxicity, good heat transfer properties, and low cost.\n\nLiquid dielectrics are self-healing; when an electric breakdown occurs, the discharge channel does not leave a permanent conductive trace in the fluid.\n\nThe electrical properties tend to be strongly influenced by dissolved gases (e.g. oxygen or carbon dioxide, dust, fibers, and especially ionic impurities and moisture. Electrical discharge may cause production of impurities degrading the dielectric's performance.\n\nSome examples of dielectric liquids are transformer oil, perfluoroalkanes, and purified water.\n\n"}
{"id": "33700433", "url": "https://en.wikipedia.org/wiki?curid=33700433", "title": "List of steam technology patents", "text": "List of steam technology patents\n\nList of steam technology patents. This is a list of patents relating to steam engines, steam locomotives, boilers, steam accumulators, condensers, etc.\n\n\n\n\n\n\n\n\n"}
{"id": "27395518", "url": "https://en.wikipedia.org/wiki?curid=27395518", "title": "Marja-Sisko Aalto", "text": "Marja-Sisko Aalto\n\nMarja-Sisko Aalto (born July 29, 1954) is a Finnish minister of the Evangelical Lutheran Church. She was the vicar of the Imatra parish from 1986 to 2010.\n\nIn November 2008, Aalto told the media that she was a trans woman and was having a sex reassignment operation. This caused a great controversy in the Church. The bishop of Mikkeli, Voitto Huotari, commented that there is no juridical obstacle for Aalto continuing as a vicar, but that there will be problems.\n\nIn 2009 almost 600 members left the Imatra parish. In November 2009 Aalto returned to the job of vicar after spending a year on leave. In March 2010, she requested to be allowed to resign.\n\n"}
{"id": "5258791", "url": "https://en.wikipedia.org/wiki?curid=5258791", "title": "Merton Street", "text": "Merton Street\n\nMerton Street is a historic and picturesque cobbled street in central Oxford, England. It joins the High Street at its northeastern end, between the Ruskin School of Drawing and Fine Art (together with the Examination Schools) and the Eastgate Hotel at the historic east gate of the city. It then runs east-west, parallel and to the south of the High Street for most of its length.\n\nMerton College, one of Oxford's older colleges, is situated to the south of the street. To the west of Merton, Corpus Christi College, one of Oxford's smallest colleges, also fronts onto the street. At the very western end, actually in Oriel Square, is an entrance to Christ Church, Oxford's largest college. At the eastern end can be found the notorious 'Pink House', as well as an entrance to University College.\n\nLogic Lane (through University College, which backs onto the street) and Magpie Lane, both narrow lanes, lead off the street to the north. Also located here is the Merton Street tennis court, a rare example of an extant real tennis court. To the south is Merton Grove (opposite Magpie Lane), providing pedestrian access between Merton College and Corpus Christi College to Christ Church Meadow to the south.\n\nThe street is designated the A420 due to the blockage of the High Street to normal traffic. To the west it continues through Oriel Square, where Oriel College is located.\n\nDespite being cobbled, the street has been repaired by Oxford City Council using asphalt.\n\nThe part of modern-day Merton Street adjoining the High Street used to be known as Coach & Horses Lane, named after a public house on the west side of the lane. From the early 18th to the late 19th century, it became known as King Street.\n\nThe rest of the street (the part running east-west) was originally known as St John Baptist's Street, named after the church which is now Merton College's chapel. In 1751, the whole street had become King Street, but by 1772 just the east-west part was called Merton Street. The entire street became known as Merton Street only in the 20th century.\n\nSiegfried Sassoon briefly took rooms in no 14 during 1919, on the recommendation of Lady Ottoline Morrell. The historian Michael Brock (1920–2014) and his wife (and co-editor) Eleanor lived in the street in the early 1950s. The academic and author J. R. R. Tolkien had rooms in Merton Street towards the end of his life in the early 1970s.\n"}
{"id": "22487049", "url": "https://en.wikipedia.org/wiki?curid=22487049", "title": "Michael McKubre", "text": "Michael McKubre\n\nMichael McKubre is an electrochemist in the forefront of cold fusion energy development. McKubre was the director of the Energy Research Center at SRI International in 1998. He is a native of New Zealand.\n\nFrom 1989 to 2002, he researched cold fusion at SRI International. Unlike other researchers in the same field, he obtained mainstream funding during all his research: first from the Electric Power Research Institute, then from the Japanese government, and in 2002 he had funding from the U.S. government.\n\nIn 2004 he and other cold fusion researchers asked the United States Department of Energy (DOE) to give a new review to the field of cold fusion, and he co-authored a report with all the available experimental and theoretical evidence since the 1989 review. The 2004 review concluded that \"while significant progress has been made in the sophistication of calorimeters since the review of this subject in 1989, the conclusions reached by the reviewers today are similar to those found in the 1989 review. \" \n\nAs of 2010, he was still making experiments with palladium cells at SRI International, and collaborates with the ENEA laboratory, where the most reliable palladium is being produced. McKubre more recently took part as one of the 22 physicists of the Steorn \"jury\".\n\nIn January 1992 a cold fusion cell exploded in an SRI lab. One of McKubre's collaborators was killed and three people including McKubre were wounded. McKubre still has pieces of glass embedded in his side. Subsequent experiments were done behind bulletproof glass.\n\n"}
{"id": "27950067", "url": "https://en.wikipedia.org/wiki?curid=27950067", "title": "Myristamine oxide", "text": "Myristamine oxide\n\nMyristamine oxide is an amine oxide based zwitterionic surfactant with a C (tetradecyl) alkyl tail. It is used as a foam stabilizer and hair conditioning agent in some shampoos and conditioners. Like other amine oxide based surfactants it is antimicrobial, being slightly more effective than lauryldimethylamine oxide against the common bacteria \"S. aureus\" and \"E. coli\".\n\n"}
{"id": "33325603", "url": "https://en.wikipedia.org/wiki?curid=33325603", "title": "National Solar Conference and World Renewable Energy Forum 2012", "text": "National Solar Conference and World Renewable Energy Forum 2012\n\nNational Solar Conference and World Renewable Energy Forum 2012 is an academic/scientific conference combined with a solar industry trade exhibition, to be held at the Denver Convention Center in Colorado, May 13 to 19, 2012. \n\nIt's organized jointly by the American Solar Energy Society, the World Renewable Energy Network, the International Solar Energy Society, the Colorado Renewable Energy Society, and the National Renewable Energy Laboratory.\n\nThe Conference incorporates the 41st annual National Solar Energy Conference, the 37tn National Passive Solar Energy Conference, the 7th ASES Policy and Marketing Conference, and a Renewable Energy Products and Services Exhibition. The Exhibition will be open to the public on Friday, May 19.\n\nConference chair is Chuck Kutscher, Ph.D., a principal engineer at the National Renewable Energy Lab.\n\n"}
{"id": "15840417", "url": "https://en.wikipedia.org/wiki?curid=15840417", "title": "Nuclearelectrica", "text": "Nuclearelectrica\n\nThe company is also undergoing negotiations for the construction of the 3 and 4 units at Cernavodă Nuclear Power Plant, project expected to cost around US$ 3.5 billion. On 7 March 2008, Nuclearelectrica, ArcelorMittal, CEZ, Electrabel, Enel, Iberdrola and RWE agreed to set up a company dedicated to the completion, commissioning and operation of the units 3 and 4. The company is expected to be registered in May 2008.\n"}
{"id": "57440610", "url": "https://en.wikipedia.org/wiki?curid=57440610", "title": "Photovoltaic system performance", "text": "Photovoltaic system performance\n\nPhotovoltaic performance monitoring systems serve several purposes. They are used to track trends in a single photovoltaic (PV) system, to identify faults in or damage to solar panels, to compare the performance of a system to design specifications or to compare PV systems at different locations. This range of applications requires various sensors and monitoring systems, adapted to the intended purpose. Sensors and monitoring systems are standardized in IEC 61724-1 and classified into three levels of accuracy, denoted by the letters “A”, “B” or “C”, or by the labels “High accuracy”, “Medium accuracy” and “Basic accuracy”. For example, Class A is appropriate for large commercial solar plants, while Class C is more appropriate for small residential installations. \n\nOn-site irradiance measurements are an important part of PV performance monitoring systems. Irradiance can be measured in the same orientation as the PV panels, so-called plane of array (POA) measurements, or horizontally, so-called global horizontal irradiance (GHI) measurements. Typical sensors used for such irradiance measurements include thermopile pyranometers, PV reference devices and photodiode sensors. To conform to a specific accuracy class, each sensor type must meet a certain set of specifications. These specifications are listed in the table below.\n\nIf an irradiance sensor is placed in POA, it must be placed at the same tilt angle as the PV module, either by attaching it to the module itself or with an extra platform or arm at the same tilt level. Checking if the sensor is properly aligned can be done with portable tilt sensors or with an integrated tilt sensor.\n\nThe standard also specifies a required maintenance schedule per accuracy class. Class C sensors need little to no additional maintenance. Class B sensors need to be re-calibrated every 2 years and require a heater to prevent precipitation or condensation. Class A sensors need to be re-calibrated once per year, require cleaning once per week, require a heater and require ventilation (for thermopile pyranometers).\n\nPV performance can also be estimated by satellite remote sensing. These measurements are indirect because the satellites measure the solar radiance reflected off the earth surface. In addition, the radiance is filtered by the spectral absorption of Earth's atmosphere. This method is typically used in non-instrumented class B and class C monitoring systems to avoid costs and maintenance of on-site sensors. If the satellite-derived data is not corrected for local conditions, an error in radiance up to 10% is possible.\n\n"}
{"id": "7373540", "url": "https://en.wikipedia.org/wiki?curid=7373540", "title": "Remote manipulator", "text": "Remote manipulator\n\nA remote manipulator, also known as a telefactor, telemanipulator, or waldo (after the short story \"Waldo\" by Robert A. Heinlein which features a man who invents and uses such devices), is a device which, through electronic, hydraulic, or mechanical linkages, allows a hand-like mechanism to be controlled by a human operator. The purpose of such a device is usually to move or manipulate hazardous materials for reasons of safety, similar to the operation and play of a claw crane game.\n\nIn 1945, the company Central Research Laboratories was given the contract to develop a remote manipulator for the Argonne National Laboratory. The intent was to replace devices which manipulated highly radioactive materials from above a sealed chamber or hot cell, with a mechanism which operated through the side wall of the chamber, allowing a researcher to stand normally while working.\n\nThe result was the Master-Slave Manipulator Mk. 8, or MSM-8, which became the iconic remote manipulator seen in newsreels and movies, such as the \"Andromeda Strain\" or \"THX 1138\".\n\nRobert A. Heinlein claimed a much earlier origin for remote manipulators. He wrote that he got the idea for \"waldos\" after reading a 1918 article in \"Popular Mechanics\" about \"a poor fellow afflicted with myasthenia gravis ... [who] devised complicated lever arrangements to enable him to use what little strength he had.\"\n\n\n"}
{"id": "55967224", "url": "https://en.wikipedia.org/wiki?curid=55967224", "title": "Renewable energy sources in Azerbaijan", "text": "Renewable energy sources in Azerbaijan\n\nRenewable energy sources are important for Azerbaijan, however there is lack of practicing renewable energy except hydro energy. One of the alternative sources of energy is the wind energy. It is also best profitable due to the cost, ecological cleanness and its renewable properties compared to other alternative energy sources.\n\nAzerbaijan is one of those countries where windmills could be perfect fit due to geographical location. In particular, the Absheron peninsula, coastline of Caspian Sea and islands in the northwestern part of Caspian Sea, the Ganja-Dashkesan zone in the west of Azerbaijan and the Sharur-Julfa area of the Nakhchivan Autonomous Republic are favorable areas. In 1999, Japan's Tomen Company, together with the Azerbaijan Scientific Research Institute of Power Engineering and Energy, installed two towers with 30 and 40 meters in Absheron, average annual wind speed was determined to be 7.9-8.1 m/sec and feasibility study about the installation of windmills with a total capacity of 30 MWt had been prepared in Qobustan region.\n\nSolar panel is also one of the most favorable sources in the world, and it is especially promising for sunny areas. The natural climate of Azerbaijan also provides extensive opportunities to increase the production of electricity and thermal energy by utilizing solar energy. Thus, the amount of sunny hours is 2400–3200 hours in Azerbaijan during the year, this means that the amount of solar rays falling on the territory of Azerbaijan is superior compared other countries that can be estimated as one of the efficiency criteria for attracting investments in the use of solar energy. The development of utilization of solar energy can partly solve energy problems in several regions of Azerbaijan.\n\nFrom ecological point of view, water is the purest energy in the world. The production of electricity from this source is being increased since 1990. The specific weight of production power of hydroelectric power plants is currently 17.8 percent in the total energy system of the Republic. There are wide opportunities for mastering hydropower resources that have not been used so far in the country. As a result of the construction of hydroelectric power plants, floodwater is regulated, electricity is ecologically produced, and new irrigation systems are created. The rivers in the territory of Azerbaijan are favorable for small hydropower stations.\n\nThere was no connection between Nakhchivan Autonomous Republic's energy system and the main energy system of the Republic, that is why medium, small and micro hydroelectric power stations need to be set up in the Nakhchivan Autonomous Republic.\n\nBiomass is also an alternative source of energy. There are following sources of biomass in Azerbaijan: industrial waste which has ability to burn, wastes from forestry and wood processing field, agricultural crops and organic compounding wastes, wastes of household and communal areas, wastes from polluted areas by oil and oil products.\n\nAccording to the research, most of the production of waste is composed of biomass products in all sectors of the economy. It is possible to obtain gas, liquid and solid biomass which are used in electricity generation from those biomass substances. Thus, more than 2.0 million tonnes of solid and industrial wastes were thrown to neutralization zones every year in Azerbaijan. Solid and industrial wastes processing can partially eliminate the difficulties of warming up of public buildings in Baku and major industrial cities of the country.\n\nThe underground temperature is widely used in many countries in industry, agriculture, household and communal fields and in medicine. The territory of Azerbaijan is rich of thermal waters. They cover large areas such as the Greater and Lesser Caucasus Mountains, the Absheron peninsula, the Talysh mountain-slope zone, the Kura basin and territories around the Caspian Sea and Guba region. It is possible to cover part of thermal energy needs in household and other areas by utilizing thermal waters in the mentioned areas.\n\nThere is high potential of alternative energy sources in Azerbaijan, and perspectives on especially the creation of wind, solar, and small hydroelectric power stations: Thermal and Hydroelectric power stations are more important to cover Azerbaijan's energy needs. Statistical data shows that this amount changes between 0.01-0.05 percent on alternative and renewable energy sources.\n\n\"Main article:\" State Agency on Alternative and Renewable Energy Sources\n\nThe State Agency on Alternative and Renewable Energy Sources of the Republic of Azerbaijan was established by the Decree of the President of Azerbaijan dated February 1, 2013, for improving the management system in the field of alternative and renewable energy.\n\nThe Charter of the State Agency on Alternative and Renewable Energy Sources was approved by the Decree of the Head of State on February 1, 2013.\n\nAccording to the Regulation, the Agency is the central executive body that carries out state policy and its regulation and efficient use of it in the field of alternative and renewable energy in Azerbaijan, effective organization of activities on alternative and renewable energy sources, coordination of activities in this field and state control.\n\nThe Agency participates in the formation of uniform state policy in the relevant area, provides for carrying out of this policy as well as the development of alternative and renewable energy, the creation of infrastructure, the application of alternative and renewable energy in the economy and social sectors, energy production on State Agency on Alternative and Renewable Energy Sources, to carry out of events related to energy consumption and energy efficiency.\n\nIn 2014, 1480.0 million kWh of electricity was generated in the country by all renewable energy sources. This, according to estimated calculations, along with saving of 298,5 thousand tons of mazut or 429.2 million m of natural gas, prevents spreading to the atmosphere 919,400 tonnes or 763,900 tonnes of carbon dioxide (CO2).\n\nIn 2015, 1816.0 million kilowatt/hours of electricity was generated by all alternative and renewable energy sources (21.5 percent more than in the previous year), and 6315.3 percent of thermal energy (15.9 percent more than in the previous year). This has resulted on savings of 464.7 million m of natural gas on average and, prevents spreading to the atmosphere 827.2 thousand tons of carbon dioxide (CO2) (Calculated based on “ The method of calculating the amount of gases the thermal effect spreading to atmosphere” approved by the Ministry of Ecology and Natural Resources dated 18.01.2006)\n\nAccording to collecting data from the preliminary official statistical and economic subjects, in 2016, 2,141.9 million kilowatt/hours electricity, or 9.3 percent of 23,073.9 million kilowatt/hours of electricity produced by the all sources in country were the total amount of alternative and renewable energy sources. Compared with the previous year, the total production of electricity amounted to 100.8%, and its production on The State Agency for Alternative and Renewable Energy Sources was 117.1%. 4212.4 Gcal of thermal energy was generated on the State Agency for Alternative and Renewable Energy Sources, which means an increase of 2.0% compared to the previous year. Efficient utilization of State Agency for Alternative and Renewable Energy Sources has resulted 548.7 million m of natural gas savings and, to prevent spreading to the atmosphere 976.7 thousand tons of carbon dioxide (CO2) (Calculated based on “The method of calculating the amount of gases the thermal effect spreading to atmosphere” approved by the Ministry of Ecology and Natural Resources dated 18.01.2006) \n\nNatural Resources of Azerbaijan\n\nEnergy in Azerbaijan\n"}
{"id": "30671368", "url": "https://en.wikipedia.org/wiki?curid=30671368", "title": "SIMPLE (dark matter experiment)", "text": "SIMPLE (dark matter experiment)\n\nSIMPLE (Superheated Instrument for Massive ParticLe Experiments) is an experiment search for direct evidence of dark matter. It is located in a 61 m cavern at the 500 level of the Laboratoire Souterrain à Bas Bruit (LSBB) near Apt in southern France. The experiment is predominantly sensitive to spin-dependent interactions of weakly interacting massive particles (or WIMPs).\n\nSIMPLE is an international collaboration with members from Portugal, France, and the United States.\n\nThe SIMPLE detector is based on superheated droplet detectors (SDDs), a suspension of 1–2% superheated liquid CClF droplets (~30 μm radius) in a viscoelastic 900 ml gel matrix, which undergo transitions to the gas phase upon energy deposition by incident radiation. The refrigerant, freon, is used as the active mass.\n\nIn effect, each droplet behaves as a miniature bubble chamber. Once a nucleation has occurred, the acoustic shock wave is picked up by microphones. Each acquired signal is then fully discriminated in terms of acoustic external noise, gel-associated noise and most recently particle discrimination. The detectors are typically operated at ~200 kPa and ~280 K. Due to the construction technique, SSDs are almost insensitive to background radiation, and their sensitivity can be adjusted by controlling the temperature and pressure of each device.\n\nThe final phase II analysis was published in Physical Review Letters in 2012. Spin-dependeant cross section limits were set for light WIMPs.\n\n\n"}
{"id": "12848723", "url": "https://en.wikipedia.org/wiki?curid=12848723", "title": "San Gabriel Mountains Regional Conservancy", "text": "San Gabriel Mountains Regional Conservancy\n\nThe San Gabriel Mountains Regional Conservancy (SGMRC) is a regional environmental organization located in the foothill area of the eastern San Gabriel Valley. It is concerned with the conservation of land, land use planning, publication of studies, watershed management, land management, and education.\n\nSGMRC is currently the only environmental organization focused on: \n\n(1)sustainable, regional stakeholder involvement through the WIN (Watershed Integrated Network) Model; \n(2) the Nature Center Network, including multi-purpose and integrated community outreach for watershed education, including workshops and library/ technology approaches; and \n(3) the connectivity, sustainability, capacity building model for the San Gabriel Valley Conservancies Network.\nThe Greater San Gabriel Valley is served, between the 14 and 15 Freeways – east/west, and the San Gabriel Mountains to the Puente Hills – north/south. The SGMRC watershed study/plan (\"Reconnecting the San Gabriel Valley\") covered of the Upper San Gabriel River and its tributaries, the largest single watershed study in Los Angeles County. The Greater San Gabriel Valley includes almost a third of the population of Los Angeles County. \n\nThe SGMRC was established in 1997 as a 501(c)(3) non-profit corporation.\n\nThe SGMRC currently operates three nature centers:\n\n"}
{"id": "12923077", "url": "https://en.wikipedia.org/wiki?curid=12923077", "title": "Sanmen Nuclear Power Station", "text": "Sanmen Nuclear Power Station\n\nThe Sanmen Nuclear Power Station () is a nuclear power station in Sanmen County, Zhejiang, China. \nSanmen is the first implementation of the AP1000 pressurized water reactor (PWR) developed by Westinghouse Electric Company. \n\nThe contract for the plant was agreed in July 2007. \nAnnouncement of the project start came roughly twelve months after Westinghouse won a bidding contest over other companies. The contract for the new plant involved The Shaw Group (now Chicago Bridge and Iron), a minority shareholder in Westinghouse. Westinghouse is controlled by Japanese Toshiba. The Shaw Group will provide engineering, procurement, commissioning, information management and project management services. The first pair of reactors were estimated to cost more than 40 billion yuan (US$5.88 billion).\nGroundbreaking for the first and second units was held February 26, 2008.\nExcavation for the first unit was completed in September 2008. Quality of the pit was certified, putting the project 67 days ahead of schedule. Construction of Sanmen Unit 1 began on April 19, 2009, as the first 5,200 m³ of concrete were poured for the foundation, in a ceremony attended by State Nuclear Power Technology Corporation (SNPTC) chair Wang Binghua and Westinghouse CEO Aris Candris.\nFirst concrete for Sanmen 2 was poured on December 15, 2009.\n\nIn June 2014, China First Heavy Industries completed the first domestically produced AP1000 reactor pressure vessel for the second AP1000 unit.\n\nThe units were originally projected to begin operation in 2014 and 2015. In April 2015, a start date of 2016 was projected for both. One month later, the start date was put back to 2017. In January 2017 China National Nuclear Corporation (CNNC) announced that the final reactor coolant pump had been installed with start of operations still foreseen for 2017. , Sanmen 1 has completed pre-fuelling safety checks but is not expected to be connected to the grid until the fall of 2018 at the earliest. Hot testing of Sanmen 1 was completed in June 2017, and fuel loading started on April 25, 2018. It subsequently became the first AP1000 reactor in the world to achieve first criticality at 2:09 AM on June 21, 2018, and was connected to the grid on June 30, 2018. \nSanmen Unit 1 entered into commercial operation on September 21, 2018.\n\nSanmen Unit 2 achieved first criticality on August 17, 2018 and was connected to the grid on August 24, 2018. Full-power demonstration testing was completed on November 5, 2018, and the unit is now considered to be in commercial operation.\n\n\n"}
{"id": "11671698", "url": "https://en.wikipedia.org/wiki?curid=11671698", "title": "Soil salinity control", "text": "Soil salinity control\n\nSoil salinity control relates to controlling the problem of soil salinity and reclaiming salinized agricultural land.\n\nThe aim of soil salinity control is to prevent soil degradation by salination and reclaim already salty (saline) soils. Soil reclamation is also called soil improvement, rehabilitation, remediation, recuperation, or amelioration.\n\nThe primary man-made cause of salinization is irrigation. River water or groundwater used in irrigation contains salts, which remain behind in the soil after the water has evaporated.\n\nThe primary method of controlling soil salinity is to permit 10-20% of the irrigation water to leach the soil,that will be drained and discharged through an appropriate drainage system. The salt concentration of the drainage water is normally 5 to 10 times higher than that of the irrigation water, thus salt export matches salt import and it will not accumulate.\n\nSalty (saline) soils are soils that have a high salt content. The predominant salt is normally sodium chloride (NaCl, \"table salt\"). Saline soils are therefore also \"sodic soils\" but there may be sodic soils that are not saline, but alkaline.\n\nAccording to a study by UN University, about , representing 20% of the world's irrigated lands are affected, up from in the early 1990s. In the Indo-Gangetic Plain, home to over 10% of the world's population, crop yield losses for wheat, rice, sugarcane and cotton grown on salt-affected lands could be 40%, 45%, 48%, and 63%, respectively.\n\nSalty soils are a common feature and an environmental problem in irrigated lands in arid and semi-arid regions, resulting in poor or little crop production. \nThe problems are often associated with high water tables, caused by a lack of natural subsurface drainage to the underground. Poor subsurface drainage may be caused by insufficient transport capacity of the aquifer or because water cannot exit the aquifer, for instance if the aquifer is situated in a topographical depression.\n\nWorldwide, the major factor in the development of saline soils is a lack of precipitation. Most naturally saline soils are found in (semi)arid regions and climates of the earth.\n\nThe primary cause of man-made salinization is the salt brought in with irrigation water. All irrigation water derived from rivers or groundwater, however 'sweet', contains salts that remain behind in the soil after the water has evaporated.\n\nFor example, assuming irrigation water with a low salt concentration of 0.3 g/l (equal to 0.3 kg/m³ corresponding to an electric conductivity of about 0.5 FdS/m) and a modest annual supply of irrigation water of 10,000 m³/ha (almost 3 mm/day) brings 3,000 kg salt/ha each year. In the absence of sufficient natural drainage (as in waterlogged soils) and without a proper leaching and drainage program to remove salts, this would lead to a high soil salinity and reduced crop yields in the long run.\n\nMuch of the water used in irrigation has a higher salt content than in this example, which is compounded by the fact that many irrigation projects use a far greater annual supply of water. Sugar cane, for example, needs about 20,000 m/ha of water per year. As a result, irrigated areas often receive more than 3,000 kg/ha of salt per year and some receive as much as 10,000 kg/ha/year.\n\nThe secondary cause of salinization is waterlogging in irrigated land. Irrigation causes changes to the natural water balance of irrigated lands. Large quantities of water in irrigation projects are not consumed by plants and must go somewhere. In irrigation projects it is impossible to achieve 100% irrigation efficiency where all the irrigation water is consumed by the plants. The maximum attainable irrigation efficiency is about 70% but usually it is less than 60%. This means that minimum 30%, but usually more than 40% of the irrigation water is not evaporated and it must go somewhere.\n\nMost of the water lost this way is stored underground which can change the original hydrology of local aquifers considerably. Many aquifers cannot absorb and transport these quantities of water and so the water table rises leading to water logging.\n\nWaterlogging causes three problems: \n\nAquifer conditions in irrigated land and the groundwater flow have an important role in soil salinization, as illustrated here :\n\nNormally, the salinization of agricultural land affects a considerable area of irrigation projects, on the order of 20 to 30%. When the agriculture in such a fraction of the land is abandoned, a new salt and water balance is attained, a new equilibrium is reached, and the situation becomes stable.\n\nIn India alone, thousands of square kilometres have been severely salinized. China and Pakistan do not lag much behind (perhaps China has even more salt affected land than India). A regional distribution of the 3,230,000 km² of saline land worldwide is shown in the following table derived from the FAO/UNESCO Soil Map of the World.\n\nAlthough the principles of the processes of salinization are fairly easy to understand, it is more difficult to explain why certain parts of the land suffer from the problems and other parts do not, or to predict accurately which part of the land will fall victim. The main reason for this is the variation of natural conditions in time and space, the usually uneven distribution of the irrigation water, and the seasonal or yearly changes of agricultural practices. Only in lands with undulating topography is the prediction simple: the depressional areas will degrade the most.\n\nThe preparation of salt and water balances for distinguishable sub-areas in the irrigation project, or the use of agro-hydro-salinity models, can be helpful in explaining or predicting the extent and severity of the problems.\n\nSoil salinity is measured as the salt concentration of the soil solution in tems of g/l or electric conductivity (EC) in dS/m. The relation between these two units is about 5/3 : y g/l => 5y/3 dS/m. Seawater may have a salt concentration of 30 g/l (3%) and an EC of 50 dS/m.\n\nThe standard for the determination of soil salinity is from an extract of a saturated paste of the soil, and the EC is then written as ECe. The extract is obtained by centrifugation. The salinity can more easily be measured, without centrifugation, in a 2:1 or 5:1 water:soil mixture (in terms of g water per g dry soil) than from a saturated paste. The relation between ECe and EC is about 4, hence : ECe = 4 EC.\n\nSoils are considered saline when the ECe > 4. When 4 < ECe < 8, the soil is called slightly saline, when 8 < ECe < 16 it is called (moderately) saline, and when ECe > 16 severely saline.\n\nSensitive crops lose their vigor already in slightly saline soils, most crops are negatively affected by (moderately) saline soils, and only salinity resistant crops thrive in severely saline soils. The University of Wyoming and the Government of Alberta report data on the salt tolerance of plants.\n\nDrainage is the primary method of controlling soil salinity. The system should permit a small fraction of the irrigation water (about 10 to 20 percent, the drainage or leaching fraction) to be drained and discharged out of the irrigation project.\nIn irrigated areas where salinity is stable, the salt concentration of the drainage water is normally 5 to 10 times higher than that of the irrigation water. Salt export matches salt import and salt will not accumulate.\n\nWhen reclaiming already salinized soils, the salt concentration of the drainage water will initially be much higher than that of the irrigation water (for example 50 times higher). Salt export will greatly exceed salt import, so that with the same drainage fraction a rapid desalinization occurs. After one or two years, the soil salinity is decreased so much, that the salinity of the drainage water has come down to a normal value and a new, favorable, equilibrium is reached.\n\nIn regions with pronounced dry and wet seasons, the drainage system may be operated in the wet season only, and closed during the dry season. This practice of checked or controlled drainage saves irrigation water.\n\nThe discharge of salty drainage water may pose \"environmental problems\" to downstream areas. The environmental hazards must be considered very carefully and, if necessary mitigating measures must be taken. If possible, the drainage must be limited to wet seasons only, when the salty effluent inflicts the least harm.\n\nLand drainage for soil salinity control is usually by \"horizontal drainage system\" (figure left), but \"vertical systems\" (figure right) are also employed.\n\nThe drainage system designed to evacuate salty water also lowers the water table. To reduce the cost of the system, the lowering must be reduced to a minimum. The highest permissible level of the water table (or the shallowest permissible depth) depends on the irrigation and agricultural practices and kind of crops.\n\nIn many cases a seasonal average water table depth of 0.6 to 0.8 m is deep enough. This means that the water table may occasionally be less than 0.6 m (say 0.2 m just after an irrigation or a rain storm). This automatically implies that, in other occasions, the water table will be deeper than 0.8 m (say 1.2 m). The fluctuation of the water table helps in the breathing function of the soil while the expulsion of carbon dioxide (CO) produced by the plant roots and the inhalation of fresh oxygen (O) is promoted.\n\nThe establishing of a not too deep water table offers the additional advantage that excessive field irrigation is discouraged, as the crop yield would be negatively affected by the resulting elevated water table, and irrigation water may be saved.\n\nThe statements made above on the optimum depth of the water table are very general, because in some instances the required water table may be still shallower than indicated (for example in rice paddies), while in other instances it must be considerably deeper (for example in some orchards). The establishment of the optimum depth of the water table is in the realm of agricultural drainage criteria.\n\nThe vadose zone of the soil below the soil surface and the watertable is subject to four main hydrological inflow and outflow factors:\nIn steady state (i.e. the amount of water stored in the unsaturated zone does not change in the long run) the water balance of the unsaturated zone reads: Inflow = Outflow, thus:\nand the \"salt balance\" is\nwhere Ci is the salt concentration of the irrigation water, Cc is the salt concentration of the capillary rise, equal to the salt concentration of the upper part of the groundwater body, Fc is the fraction of the total evaporation transpired by plants, Ce is the salt concentration of the water taken up by the plant roots, Cp is the salt concentration of the percolation water, and Ss is the increase of salt storage in the unsaturated soil. This assumes that the rainfall contains no salts. Only along the coast this may not be true. Further it is assumed that no runoff or surface drainage occurs.<br>\nThe amount of removed by plants (Evap.Fc.Ce) is usually negligibly small: Evap.Fc.Ce = 0\n\nThe salt concentration Cp can be taken as a part of the salt concentration of the soil in the unsaturated zone (Cu) giving: Cp=Le.Cu, where Le is the leaching efficiency. The leaching efficiency is often in the order of 0.7 to 0.8, but in poorly structured, heavy clay soils it may be less. In the Leziria Grande polder in the delta of the Tagus river in Portugal it was found that the leaching efficiency was only 0.15.<br>\nAssuming that one wishes to avoid the soil salinity to increase and maintain the soil salinity Cu at a desired level Cd we have: <br>\nSs = 0, Cu = Cd and Cp = Le.Cd. Hence the salt balance can be simplified to:\nSetting the amount percolation water required to fulfill this salt balance equal to Lr (the \"leaching requirement\")\nit is found that:\nSubstituting herein Irr = Evap + Perc − Rain − Cap and re-arranging gives :\nWith this the irrigation and drainage requirements for salinity control can be computed too.<br>\nIn irrigation projects in (semi)arid zones and climates it is important to check the leaching requirement, whereby the \"field irrigation efficiency\" (indicating the fraction of irrigation water percolating to the underground) is to be taken into account.<br>\nThe desired soil salinity level Cd depends on the crop tolerance to salt. The University of Wyoming, USA, and the Government of Alberta, Canada, report crop tolerance data.\n\nIn irrigated lands with scarce water resources suffering from drainage (high water table) and soil salinity problems, strip cropping is sometimes practiced with strips of land where every other strip is irrigated while the strips in between are left permanently fallow.\n\nOwing to the water application in the irrigated strips they have a higher watertable which induces flow of groundwater to the unirrigated strips. This flow functions as subsurface drainage for the irrigated strips, whereby the water table is maintained at a not-too-shallow depth, leaching of the soil is possible, and the soil salinity can be controlled at an acceptably low level.\n\nIn the unirrigated (sacrificial) strips the soil is dry and the groundwater comes up by capillary rise and evaporates leaving the salts behind, so that here the soil salinizes. Nevertheless, they can have some use for livestock, sowing salinity resistant grasses or weeds. Moreover, useful salt resistant trees can be planted like Casuarina, Eucalyptus or Atriplex, keeping in mind that the trees have deep rooting systems and the salinity of the wet subsoil is less than of the topsoil. In these ways wind erosion can be controlled. The unirrigated strips can also be used for salt harvesting.\n\nThe majority of the computer models available for water and solute transport in the soil (e.g. SWAP, DrainMod-S, UnSatChem, and Hydrus ) are based on Richard's differential equation for the movement of water in unsaturated soil in combination with Fick's differential convection–diffusion equation for advection and dispersion of salts.\n\nThe models require input of soil characteristics like the relations between variable unsaturated soil moisture content, water tension, water retention curve, unsaturated hydraulic conductivity, dispersivity and diffusivity. These relations vary to a great extent from place to place and from time to time and are not easy to measure. Further, the models are difficult to calibrate under farmer's field conditions because the soil salinity here is spatially very variable. The models use short time steps and need at least a daily, if not an hourly, data base of hydrological phenomena. Altogether this makes model application to a fairly large project the job of a team of specialists with ample facilities.\n\nSimpler models, like SaltMod, based on monthly or seasonal water and soil balances and an empirical capillary rise function, are also available. They are useful for long-term salinity predictions in relation to irrigation and drainage practices.\n\nLeachMod, using the SaltMod principles, helps in analyzing leaching experiments in which the soil salinity was monitored in various root zone layers while the model will optimize the value of the leaching efficiency of each layer so that a fit is obtained of observed with simulated soil salinity values.\n\nSpatial variations owing to variations in topography can be simulated and predicted using salinity cum groundwater models, like SahysMod.\n\n"}
{"id": "31077349", "url": "https://en.wikipedia.org/wiki?curid=31077349", "title": "Solar-Powered Aircraft Developments Solar One", "text": "Solar-Powered Aircraft Developments Solar One\n\nThe Solar-Powered Aircraft Developments Solar One is a British mid-wing, experimental, manned solar-powered aircraft that was designed by David Williams and produced by Solar-Powered Aircraft Developments under the direction of Freddie To. On 19 December 1978 it became one of the first solar-powered aircraft to fly, after the unmanned AstroFlight Sunrise and the manned Mauro Solar Riser, and the first successful British solar-powered aircraft.\n\nFreddie To was a member of the Kremer prize committee who started his own project to produce a human-powered aircraft to compete for the prize. Its structure comprised a wooden frame covered with heat-shrunk Solarfilm model aircraft film. The wing was built in three sections, a centre section and two outer wing panels to simply storage and transport. The wing spar is a laminated spruce girder box-spar design. The tail surfaces are quickly removable for storage. The resulting aircraft, at , proved too heavy for human-powered flight and so To converted it to solar power.\n\nA nose-mounted pod powerplant was installed consisting of four permanent magnet 36 V DC, 12 A Bosch electric motors, powered by a Nickel-cadmium battery pack of 24 cells with a total capacity of 25 AH, connected in series to give a maximum (open-circuit) output voltage of approximately 29 V. The motors drive a two-bladed propeller via a 3:1 bicycle-chain reduction gear. The propeller turns at a maximum of 1,100 rpm, decreasing with battery discharge. Power is controlled with a simple on/off switch. \n\nTo recharge the batteries, 750 solar cells of 3 inch diameter were installed at a cost of £6,000. At that time solar cells were very expensive. They were the most costly part of the aircraft and had to be limited in capacity to remain within the project budget of £16,000.\n\nThe output from the solar cells is not sufficient to sustain flight. Before flight they are used to charge the batteries. The batteries then provide power for takeoff and initial climb. The installed batteries provide for a climb of eight minutes plus a two-minute cruise allowance.\n\nThe first flight attempt took place at Lasham Airfield, Hampshire, United Kingdom on 19 December 1978. The propeller pitch was incorrectly set and the attempt achieved only a short hop. At the hands of pilot Ken Stewart, a successful flight took place on 13 June 1979, covering just under . The aircraft lifted off at and reached and in height. A second flight on the same day by Bill Maidment achieved a speed of . All flights were made on battery power that had been supplied on the ground from the installed solar cells.\n\nA planned flight across the English Channel was abandoned when it was found that the aircraft did not meet its endurance targets.\n"}
{"id": "27667", "url": "https://en.wikipedia.org/wiki?curid=27667", "title": "Space", "text": "Space\n\nSpace is the boundless three-dimensional extent in which objects and events have relative position and direction. Physical space is often conceived in three linear dimensions, although modern physicists usually consider it, with time, to be part of a boundless four-dimensional continuum known as spacetime. The concept of space is considered to be of fundamental importance to an understanding of the physical universe. However, disagreement continues between philosophers over whether it is itself an entity, a relationship between entities, or part of a conceptual framework.\n\nDebates concerning the nature, essence and the mode of existence of space date back to antiquity; namely, to treatises like the \"Timaeus\" of Plato, or Socrates in his reflections on what the Greeks called \"khôra\" (i.e. \"space\"), or in the \"Physics\" of Aristotle (Book IV, Delta) in the definition of \"topos\" (i.e. place), or in the later \"geometrical conception of place\" as \"space \"qua\" extension\" in the \"Discourse on Place\" (\"Qawl fi al-Makan\") of the 11th-century Arab polymath Alhazen. Many of these classical philosophical questions were discussed in the Renaissance and then reformulated in the 17th century, particularly during the early development of classical mechanics. In Isaac Newton's view, space was absolute—in the sense that it existed permanently and independently of whether there was any matter in the space. Other natural philosophers, notably Gottfried Leibniz, thought instead that space was in fact a collection of relations between objects, given by their distance and direction from one another. In the 18th century, the philosopher and theologian George Berkeley attempted to refute the \"visibility of spatial depth\" in his \"Essay Towards a New Theory of Vision\". Later, the metaphysician Immanuel Kant said that the concepts of space and time are not empirical ones derived from experiences of the outside world—they are elements of an already given systematic framework that humans possess and use to structure all experiences. Kant referred to the experience of \"space\" in his \"Critique of Pure Reason\" as being a subjective \"pure \"a priori\" form of intuition\".\n\nIn the 19th and 20th centuries mathematicians began to examine geometries that are non-Euclidean, in which space is conceived as \"curved\", rather than \"flat\". According to Albert Einstein's theory of general relativity, space around gravitational fields deviates from Euclidean space. Experimental tests of general relativity have confirmed that non-Euclidean geometries provide a better model for the shape of space.\n\nGalilean and Cartesian theories about space, matter and motion are at the foundation of the Scientific Revolution, which is understood to have culminated with the publication of Newton's \"Principia\" in 1687. Newton's theories about space and time helped him explain the movement of objects. While his theory of space is considered the most influential in Physics, it emerged from his predecessors' ideas about the same.\n\nAs one of the pioneers of modern science, Galilei revised the established Aristotelian and Ptolemaic ideas about a geocentric cosmos. He backed the Copernican theory that the universe was heliocentric, with a stationary sun at the center and the planets—including the Earth—revolving around the sun. If the Earth moved, the Aristotelian belief that its natural tendency was to remain at rest was in question. Galilei wanted to prove instead that the sun moved around its axis, that motion was as natural to an object as the state of rest. In other words, for Galilei, celestial bodies, including the Earth, were naturally inclined to move in circles. This view displaced another Aristotelian idea—that all objects gravitated towards their designated natural place-of-belonging.\n\nDescartes set out to replace the Aristotelian worldview with a theory about space and motion as determined by natural laws. In other words, he sought a metaphysical foundation or a mechanical explanation for his theories about matter and motion. Cartesian space was Euclidean in structure—infinite, uniform and flat. It was defined as that which contained matter; conversely, matter by definition had a spatial extension so that there was no such thing as empty space.\n\nThe Cartesian notion of space is closely linked to his theories about the nature of the body, mind and matter. He is famously known for his \"cogito ergo sum\" (I think therefore I am), or the idea that we can only be certain of the fact that we can doubt, and therefore think and therefore exist. His theories belong to the rationalist tradition, which attributes knowledge about the world to our ability to think rather than to our experiences, as the empiricists believe. He posited a clear distinction between the body and mind, which is referred to as the Cartesian dualism.\n\nFollowing Galilei and Descartes, during the seventeenth century the philosophy of space and time revolved around the ideas of Gottfried Leibniz, a German philosopher-mathematician, and Isaac Newton, who set out two opposing theories of what space is. Rather than being an entity that independently exists over and above other matter, Leibniz held that space is no more than the collection of spatial relations between objects in the world: \"space is that which results from places taken together\". Unoccupied regions are those that \"could\" have objects in them, and thus spatial relations with other places. For Leibniz, then, space was an idealised abstraction from the relations between individual entities or their possible locations and therefore could not be continuous but must be discrete.\nSpace could be thought of in a similar way to the relations between family members. Although people in the family are related to one another, the relations do not exist independently of the people.\nLeibniz argued that space could not exist independently of objects in the world because that implies a difference between two universes exactly alike except for the location of the material world in each universe. But since there would be no observational way of telling these universes apart then, according to the identity of indiscernibles, there would be no real difference between them. According to the principle of sufficient reason, any theory of space that implied that there could be these two possible universes must therefore be wrong.\nNewton took space to be more than relations between material objects and based his position on observation and experimentation. For a relationist there can be no real difference between inertial motion, in which the object travels with constant velocity, and non-inertial motion, in which the velocity changes with time, since all spatial measurements are relative to other objects and their motions. But Newton argued that since non-inertial motion generates forces, it must be absolute. He used the example of water in a spinning bucket to demonstrate his argument. Water in a bucket is hung from a rope and set to spin, starts with a flat surface. After a while, as the bucket continues to spin, the surface of the water becomes concave. If the bucket's spinning is stopped then the surface of the water remains concave as it continues to spin. The concave surface is therefore apparently not the result of relative motion between the bucket and the water. Instead, Newton argued, it must be a result of non-inertial motion relative to space itself. For several centuries the bucket argument was considered decisive in showing that space must exist independently of matter.\n\nIn the eighteenth century the German philosopher Immanuel Kant developed a theory of knowledge in which knowledge about space can be both \"a priori\" and \"synthetic\". According to Kant, knowledge about space is \"synthetic\", in that statements about space are not simply true by virtue of the meaning of the words in the statement. In his work, Kant rejected the view that space must be either a substance or relation. Instead he came to the conclusion that space and time are not discovered by humans to be objective features of the world, but imposed by us as part of a framework for organizing experience.\n\nEuclid's \"Elements\" contained five postulates that form the basis for Euclidean geometry. One of these, the parallel postulate, has been the subject of debate among mathematicians for many centuries. It states that on any plane on which there is a straight line \"L\" and a point \"P\" not on \"L\", there is exactly one straight line \"L\" on the plane that passes through the point \"P\" and is parallel to the straight line \"L\". Until the 19th century, few doubted the truth of the postulate; instead debate centered over whether it was necessary as an axiom, or whether it was a theory that could be derived from the other axioms. Around 1830 though, the Hungarian János Bolyai and the Russian Nikolai Ivanovich Lobachevsky separately published treatises on a type of geometry that does not include the parallel postulate, called hyperbolic geometry. In this geometry, an infinite number of parallel lines pass through the point \"P\". Consequently, the sum of angles in a triangle is less than 180° and the ratio of a circle's circumference to its diameter is greater than pi. In the 1850s, Bernhard Riemann developed an equivalent theory of elliptical geometry, in which no parallel lines pass through \"P\". In this geometry, triangles have more than 180° and circles have a ratio of circumference-to-diameter that is less than pi.\n\nAlthough there was a prevailing Kantian consensus at the time, once non-Euclidean geometries had been formalised, some began to wonder whether or not physical space is curved. Carl Friedrich Gauss, a German mathematician, was the first to consider an empirical investigation of the geometrical structure of space. He thought of making a test of the sum of the angles of an enormous stellar triangle, and there are reports that he actually carried out a test, on a small scale, by triangulating mountain tops in Germany.\n\nHenri Poincaré, a French mathematician and physicist of the late 19th century, introduced an important insight in which he attempted to demonstrate the futility of any attempt to discover which geometry applies to space by experiment. He considered the predicament that would face scientists if they were confined to the surface of an imaginary large sphere with particular properties, known as a sphere-world. In this world, the temperature is taken to vary in such a way that all objects expand and contract in similar proportions in different places on the sphere. With a suitable falloff in temperature, if the scientists try to use measuring rods to determine the sum of the angles in a triangle, they can be deceived into thinking that they inhabit a plane, rather than a spherical surface. In fact, the scientists cannot in principle determine whether they inhabit a plane or sphere and, Poincaré argued, the same is true for the debate over whether real space is Euclidean or not. For him, which geometry was used to describe space was a matter of convention. Since Euclidean geometry is simpler than non-Euclidean geometry, he assumed the former would always be used to describe the 'true' geometry of the world.\n\nIn 1905, Albert Einstein published his special theory of relativity, which led to the concept that space and time can be viewed as a single construct known as \"spacetime\". In this theory, the speed of light in a vacuum is the same for all observers—which has the result that two events that appear simultaneous to one particular observer will not be simultaneous to another observer if the observers are moving with respect to one another. Moreover, an observer will measure a moving clock to tick more slowly than one that is stationary with respect to them; and objects are measured to be shortened in the direction that they are moving with respect to the observer.\n\nSubsequently, Einstein worked on a general theory of relativity, which is a theory of how gravity interacts with spacetime. Instead of viewing gravity as a force field acting in spacetime, Einstein suggested that it modifies the geometric structure of spacetime itself. According to the general theory, time goes more slowly at places with lower gravitational potentials and rays of light bend in the presence of a gravitational field. Scientists have studied the behaviour of binary pulsars, confirming the predictions of Einstein's theories, and non-Euclidean geometry is usually used to describe spacetime.\n\nIn modern mathematics spaces are defined as sets with some added structure. They are frequently described as different types of manifolds, which are spaces that locally approximate to Euclidean space, and where the properties are defined largely on local connectedness of points that lie on the manifold. There are however, many diverse mathematical objects that are called spaces. For example, vector spaces such as function spaces may have infinite numbers of independent dimensions and a notion of distance very different from Euclidean space, and topological spaces replace the concept of distance with a more abstract idea of nearness.\n\nSpace is one of the few fundamental quantities in physics, meaning that it cannot be defined via other quantities because nothing more fundamental is known at the present. On the other hand, it can be related to other fundamental quantities. Thus, similar to other fundamental quantities (like time and mass), space can be explored via measurement and experiment.\n\nToday, our three-dimensional space is viewed as embedded in a four-dimensional spacetime, called Minkowski space (see special relativity). The idea behind space-time is that time is hyperbolic-orthogonal to each of the three spatial dimensions.\n\nBefore Einstein's work on relativistic physics, time and space were viewed as independent dimensions. Einstein's discoveries showed that due to relativity of motion our space and time can be mathematically combined into one object–spacetime. It turns out that distances in space or in time separately are not invariant with respect to Lorentz coordinate transformations, but distances in Minkowski space-time along space-time intervals are—which justifies the name.\n\nIn addition, time and space dimensions should not be viewed as exactly equivalent in Minkowski space-time. One can freely move in space but not in time. Thus, time and space coordinates are treated differently both in special relativity (where time is sometimes considered an imaginary coordinate) and in general relativity (where different signs are assigned to time and space components of spacetime metric).\n\nFurthermore, in Einstein's general theory of relativity, it is postulated that space-time is geometrically distorted- \"curved\" -near to gravitationally significant masses.\n\nOne consequence of this postulate, which follows from the equations of general relativity, is the prediction of moving ripples of space-time, called gravitational waves. While indirect evidence for these waves has been found (in the motions of the Hulse–Taylor binary system, for example) experiments attempting to directly measure these waves are ongoing at the LIGO and Virgo collaborations. LIGO scientists reported the first such direct observation of gravitational waves on 14 September 2015.\n\nRelativity theory leads to the cosmological question of what shape the universe is, and where space came from. It appears that space was created in the Big Bang, 13.8 billion years ago and has been expanding ever since. The overall shape of space is not known, but space is known to be expanding very rapidly due to the cosmic inflation.\n\nThe measurement of \"physical space\" has long been important. Although earlier societies had developed measuring systems, the International System of Units, (SI), is now the most common system of units used in the measuring of space, and is almost universally used.\n\nCurrently, the standard space interval, called a standard meter or simply meter, is defined as the distance traveled by light in a vacuum during a time interval of exactly 1/299,792,458 of a second. This definition coupled with present definition of the second is based on the special theory of relativity in which the speed of light plays the role of a fundamental constant of nature.\n\nGeography is the branch of science concerned with identifying and describing places on Earth, utilizing spatial awareness to try to understand why things exist in specific locations. Cartography is the mapping of spaces to allow better navigation, for visualization purposes and to act as a locational device. Geostatistics apply statistical concepts to collected spatial data of Earth to create an estimate for unobserved phenomena.\n\nGeographical space is often considered as land, and can have a relation to ownership usage (in which space is seen as property or territory). While some cultures assert the rights of the individual in terms of ownership, other cultures will identify with a communal approach to land ownership, while still other cultures such as Australian Aboriginals, rather than asserting ownership rights to land, invert the relationship and consider that they are in fact owned by the land. Spatial planning is a method of regulating the use of space at land-level, with decisions made at regional, national and international levels. Space can also impact on human and cultural behavior, being an important factor in architecture, where it will impact on the design of buildings and structures, and on farming.\n\nOwnership of space is not restricted to land. Ownership of airspace and of waters is decided internationally. Other forms of ownership have been recently asserted to other spaces—for example to the radio bands of the electromagnetic spectrum or to cyberspace.\n\nPublic space is a term used to define areas of land as collectively owned by the community, and managed in their name by delegated bodies; such spaces are open to all, while private property is the land culturally owned by an individual or company, for their own use and pleasure.\n\nAbstract space is a term used in geography to refer to a hypothetical space characterized by complete homogeneity. When modeling activity or behavior, it is a conceptual tool used to limit extraneous variables such as terrain.\n\nPsychologists first began to study the way space is perceived in the middle of the 19th century. Those now concerned with such studies regard it as a distinct branch of psychology. Psychologists analyzing the perception of space are concerned with how recognition of an object's physical appearance or its interactions are perceived, see, for example, visual space.\n\nOther, more specialized topics studied include amodal perception and object permanence. The perception of surroundings is important due to its necessary relevance to survival, especially with regards to hunting and self preservation as well as simply one's idea of personal space.\n\nSeveral space-related phobias have been identified, including agoraphobia (the fear of open spaces), astrophobia (the fear of celestial space) and claustrophobia (the fear of enclosed spaces).\n\nThe understanding of three-dimensional space in humans is thought to be learned during infancy using unconscious inference, and is closely related to hand-eye coordination. The visual ability to perceive the world in three dimensions is called depth perception.\n\nSpace has been studied in the social sciences from the perspectives of Marxism, feminism, postmodernism, postcolonialism, urban theory and critical geography. These theories account for the effect of the history of colonialism, transatlantic slavery and globalization on our understanding and experience of space and place. The topic has garnered attention since the 1980s, after the publication of Henri Lefebvre's \"The Production of Space .\" In this book, Lefebvre applies Marxist ideas about the production of commodities and accumulation of capital to discuss space as a social product. His focus is on the multiple and overlapping social processes that produce space.\n\nIn his book \"The Condition of Postmodernity,\" David Harvey describes what he terms the \"time-space compression.\" This is the effect of technological advances and capitalism on our perception of time, space and distance. Changes in the modes of production and consumption of capital affect and are affected by developments in transportation and technology. These advances create relationships across time and space, new markets and groups of wealthy elites in urban centers, all of which annihilate distances and affect our perception of linearity and distance.\n\nIn his book \"Thirdspace,\" Edward Soja describes space and spatiality as an integral and neglected aspect of what he calls the \"trialectics of being,\" the three modes that determine how we inhabit, experience and understand the world. He argues that critical theories in the Humanities and Social Sciences study the historical and social dimensions of our lived experience, neglecting the spatial dimension. He builds on Henri Lefebvre's work to address the dualistic way in which humans understand space—as either material/physical or as represented/imagined. Lefebvre's \"lived space\" and Soja's \"thridspace\" are terms that account for the complex ways in which humans understand and navigate place, which \"firstspace\" and \"Secondspace\" (Soja's terms for material and imagined spaces respectively) do not fully encompass.\n\nPostcolonial theorist Homi Bhabha's concept of Third Space is different from Soja's Thirdspace, even though both terms offer a way to think outside the terms of a binary logic. Bhabha's Third Space is the space in which hybrid cultural forms and identities exist. In his theories, the term hybrid describes new cultural forms that emerge through the interaction between colonizer and colonized.\n\n"}
{"id": "1527151", "url": "https://en.wikipedia.org/wiki?curid=1527151", "title": "Speeds and feeds", "text": "Speeds and feeds\n\nThe phrase speeds and feeds or feeds and speeds refers to two separate velocities in machine tool practice, cutting speed and feed rate. They are often considered as a pair because of their combined effect on the cutting process. Each, however, can also be considered and analyzed in its own right.\n\n\"Cutting speed\" (also called \"surface speed\" or simply \"speed\") is the speed difference (relative velocity) between the cutting tool and the surface of the workpiece it is operating on. It is expressed in units of distance along the workpiece surface per unit of time, typically surface feet per minute (sfm) or meters per minute (m/min). \"Feed rate\" (also often styled as a solid compound, \"feedrate\", or called simply \"feed\") is the relative velocity at which the cutter is advanced along the workpiece; its vector is perpendicular to the vector of cutting speed. Feed rate units depend on the motion of the tool and workpiece; when the workpiece rotates (\"e.g.\", in turning and boring), the units are almost always distance per spindle revolution (inches per revolution [in/rev or ipr] or millimeters per revolution [mm/rev]). When the workpiece does not rotate (\"e.g.\", in milling), the units are typically distance per time (inches per minute [in/min or ipm] or millimeters per minute [mm/min]), although distance per revolution or per cutter tooth are also sometimes used.\n\nIf variables such as cutter geometry and the rigidity of the machine tool and its tooling setup could be ideally maximized (and reduced to negligible constants), then only a lack of power (that is, kilowatts or horsepower) available to the spindle would prevent the use of the maximum possible speeds and feeds for any given workpiece material and cutter material. Of course, in reality those other variables are dynamic and not negligible; but there is still a correlation between power available and feeds and speeds employed. In practice, lack of rigidity is usually the limiting constraint.\n\nThe phrases \"speeds and feeds\" or \"feeds and speeds\" have sometimes been used metaphorically to refer to the execution details of a plan, which only skilled technicians (as opposed to designers or managers) would know.\n\nCutting speed may be defined as the rate at the workpiece surface, irrespective of the machining operation used. A cutting speed for mild steel of 100 ft/min is the same whether it is the speed of the cutter passing over the workpiece, such as in a turning operation, or the speed of the cutter moving past a workpiece, such as in a milling operation. The cutting conditions will affect the value of this surface speed for mild steel.\n\nSchematically, speed at the workpiece surface can be thought of as the tangential speed at the tool-cutter interface, that is, how fast the material moves past the cutting edge of the tool, although \"which surface to focus on\" is a topic with several valid answers. In drilling and milling, the outside diameter of the tool is the widely agreed surface. In turning and boring, the surface can be defined on either side of the depth of cut, that is, either the starting surface or the ending surface, with neither definition being \"wrong\" as long as the people involved understand the difference. An experienced machinist summed this up succinctly as \"the diameter I am turning from\" versus \"the diameter I am turning to.\" He uses the \"from\", not the \"to\", and explains why, while acknowledging that some others do not. The logic of focusing on the largest diameter involved (OD of drill or end mill, starting diameter of turned workpiece) is that this is where the highest tangential speed is, with the most heat generation, which is the main driver of tool wear.\n\nThere will be an optimum cutting speed for each material and set of machining conditions, and the spindle speed (RPM) can be calculated from this speed. Factors affecting the calculation of cutting speed are:\n\n\nCutting speeds are calculated on the assumption that optimum cutting conditions exist. These include:\n\n\nThe cutting \"speed\" is given as a set of constants that are available from the material manufacturer or supplier. The most common materials are available in reference books or charts, but will always be subject to adjustment depending on the cutting conditions. The following table gives the cutting speeds for a selection of common materials under one set of conditions. The conditions are a tool life of 1 hour, dry cutting (no coolant), and at medium feeds, so they may appear to be incorrect depending on circumstances. These cutting speeds may change if, for instance, adequate coolant is available or an improved grade of HSS is used (such as one that includes cobalt).\n\nThe machinability rating of a material attempts to quantify the machinability of various materials. It is expressed as a percentage or a normalized value. The American Iron and Steel Institute (AISI) determined machinability ratings for a wide variety of materials by running turning tests at 180 surface feet per minute (sfpm). It then arbitrarily assigned 160 Brinell B1112 steel a machinability rating of 100%. The machinability rating is determined by measuring the weighed averages of the normal cutting speed, surface finish, and tool life for each material. Note that a material with a machinability rating less than 100% would be more difficult to machine than B1112 and material and a value more than 100% would be easier.\n\nMachinability ratings can be used in conjunction with the Taylor tool life equation, in order to determine cutting speeds or tool life. It is known that B1112 has a tool life of 60 minutes at a cutting speed of 100 sfpm. If a material has a machinability rating of 70%, it can be determined, with the above knowns, that in order to maintain the same tool life (60 minutes), the cutting speed must be 70 sfpm (assuming the same tooling is used).\n\nWhen calculating for copper alloys, the machine rating is arrived at by assuming the 100 rating of 600 SFM. For example, phosphorus bronze (grades A–D) has a machinability rating of 20. This means that phosphor bronze runs at 20% the speed of 600 SFM or 120 SFM. However, 165 SFM is generally accepted as the basic 100% rating for \"grading steels\".\n\nThe spindle speed is the rotational frequency of the spindle of the machine, measured in revolutions per minute (RPM). The preferred speed is determined by working backward from the desired surface speed (sfm or m/min) and incorporating the diameter (of workpiece or cutter).\n\nThe spindle may hold the:\n\nExcessive spindle speed will cause premature tool wear, breakages, and can cause tool chatter, all of which can lead to potentially dangerous conditions. Using the correct spindle speed for the material and tools will greatly enhance tool life and the quality of the surface finish.\n\nFor a given machining operation, the cutting speed will remain constant for most situations; therefore the spindle speed will also remain constant. However, facing, forming, parting off, and recess operations on a lathe or screw machine involve the machining of a constantly changing diameter. Ideally this means changing the spindle speed as the cut advances across the face of the workpiece, producing constant surface speed (CSS). Mechanical arrangements to effect CSS have existed for centuries, but they were never applied commonly to machine tool control. In the pre-CNC era, the ideal of CSS was ignored for most work. For unusual work that demanded it, special pains were taken to achieve it. The introduction of CNC-controlled lathes has provided a practical, everyday solution via automated CSS. By means of the machine's software and variable speed electric motors, the lathe can increase the RPM of the spindle as the cutter gets closer to the center of the part.\n\nGrinding wheels are designed to be run at a maximum safe speed, the spindle speed of the grinding machine may be variable but this should only be changed with due attention to the safe working speed of the wheel. As a wheel wears it will decrease in diameter, and its effective cutting speed will be reduced. Some grinders have the provision to increase the spindle speed, which corrects for this loss of cutting ability; however, increasing the speed beyond the wheels rating will destroy the wheel and create a serious hazard to life and limb.\n\nGenerally speaking, spindle speeds and feed rates are less critical in woodworking than metalworking. Most woodworking machines including power saws such as circular saws and band saws, jointers, Thickness planers rotate at a fixed RPM. In those machines, cutting speed is regulated through the feed rate. The required feed rate can be extremely variable depending on the power of the motor, the hardness of the wood or other material being machined, and the sharpness of the cutting tool.\n\nIn woodworking, the ideal feed rate is one that is slow enough not to bog down the motor, yet fast enough to avoid burning the material. Certain woods, such as black cherry and maple are more prone to burning than others. The right feed rate is usually obtained by \"feel\" if the material is hand fed, or by trial and error if a power feeder is used. In thicknessers (planers), the wood is usually fed automatically through rubber or corrugated steel rollers. Some of these machines allow varying the feed rate, usually by changing pulleys. A slower feed rate usually results in a finer surface as more cuts are made for any length of wood.\n\nSpindle speed becomes important in the operation of routers, spindle moulders or shapers, and drills. Older and smaller routers often rotate at a fixed spindle speed, usually between 20,000 and 25,000 rpm. While these speeds are fine for small router bits, using larger bits, say more than or 25 millimeters in diameter, can be dangerous and can lead to chatter. Larger routers now have variable speeds and larger bits require slower speed. Drilling wood generally uses higher spindle speeds than metal, and the speed is not as critical. However, larger diameter drill bits do require slower speeds to avoid burning.\n\nCutting feeds and speeds, and the spindle speeds that are derived from them, are the \"ideal\" cutting conditions for a tool. If the conditions are less than ideal then adjustments are made to the spindle's speed, this adjustment is usually a reduction in RPM to the closest available speed, or one that is deemed (through knowledge and experience) to be correct.\n\nSome materials, such as machinable wax, can be cut at a wide variety of spindle speeds, while others, such as stainless steel require much more careful control as the cutting speed is critical, to avoid overheating both the cutter and workpiece. Stainless steel is one material that work hardens very easily, therefore insufficient feed rate or incorrect spindle speed can lead to less than ideal cutting conditions as the work piece will quickly harden and resist the tool's cutting action. The liberal application of cutting fluid can improve these cutting conditions; however, the correct selection of speeds is the critical factor.\n\nMost metalworking books have nomograms or tables of spindle speeds and feed rates for different cutters and workpiece materials; similar tables are also likely available from the manufacturer of the cutter used.\n\nThe spindle speeds may be calculated for all machining operations once the SFM or MPM is known. In most cases we are dealing with a cylindrical object such as a milling cutter or a workpiece turning in a lathe so we need to determine the speed at the periphery of this round object. This speed at the periphery (of a point on the circumference, moving past a stationary point) will depend on the rotational speed (RPM) and diameter of the object.\n\nOne analogy would be a skateboard rider and a bicycle rider travelling side by side along the road. For a given surface speed (the speed of this pair along the road) the rotational speed (RPM) of their wheels (large for the skater and small for the bicycle rider) will be different. This rotational speed (RPM) is what we are calculating, given a fixed surface speed (speed along the road) and known values for their wheel sizes (cutter or workpiece).\n\nThe following formulae may be used to estimate this value.\n\nThe exact RPM is not always needed, a close approximation will work (using 3 for the value of formula_1).\n\ne.g. for a cutting speed of 100 ft/min (a plain HSS steel cutter on mild steel) and diameter of 10 inches (the cutter or the work piece)\n\nand, for an example using metric values, where the cutting speed is 30 m/min and a diameter of 10 mm,\n\nHowever, for more accurate calculations, and at the expense of simplicity, this formula can be used:\n\nand using the same example\n\nand using the same example as above\n\nwhere:\n\nFeed rate is the velocity at which the cutter is fed, that is, advanced against the workpiece. It is expressed in units of distance per revolution for turning and boring (typically \"inches per revolution\" [\"ipr\"] or \"millimeters per revolution\"). It can be expressed thus for milling also, but it is often expressed in units of distance per time for milling (typically \"inches per minute\" [\"ipm\"] or \"millimeters per minute\"), with considerations of how many teeth (or flutes) the cutter has then determining what that means for each tooth.\n\nFeed rate is dependent on the:\n\n\nWhen deciding what feed rate to use for a certain cutting operation, the calculation is fairly straightforward for single-point cutting tools, because all of the cutting work is done at one point (done by \"one tooth\", as it were). With a milling machine or jointer, where multi-tipped/multi-fluted cutting tools are involved, then the desirable feed rate becomes dependent on the number of teeth on the cutter, as well as the desired amount of material per tooth to cut (expressed as chip load). The greater the number of cutting edges, the higher the feed rate permissible: for a cutting edge to work efficiently it must remove sufficient material to cut rather than rub; it also must do its fair share of work.\n\nThe ratio of the spindle speed and the feed rate controls how aggressive the cut is, and the nature of the swarf formed.\n\nThis formula can be used to figure out the feed rate that the cutter travels into or around the work. This would apply to cutters on a milling machine, drill press and a number of other machine tools. This is not to be used on the lathe for turning operations, as the feed rate on a lathe is given as \"feed per revolution.\"\n\nformula_8\n\nWhere:\n\nCutting speed and feed rate come together with \"depth of cut\" to determine the \"material removal rate\", which is the volume of workpiece material (metal, wood, plastic, etc.) that can be removed per time unit\n\nSpeed-and-feed selection is analogous to other examples of applied science, such as meteorology or pharmacology, in that the theoretical modeling is necessary and useful but can never fully predict the reality of specific cases because of the massively multivariate environment. Just as weather forecasts or drug dosages can be modeled with fair accuracy, but never with complete certainty, machinists can predict with charts and formulas the approximate speed and feed values that will work best on a particular job, but cannot know the exact optimal values until running the job. In CNC machining, usually the programmer programs speeds and feedrates that are as maximally tuned as calculations and general guidelines can supply. The operator then fine-tunes the values while running the machine, based on sights, sounds, smells, temperatures, tolerance holding, and tool tip lifespan. Under proper management, the revised values are captured for future use, so that when a program is run again later, this work need not be duplicated.\n\nAs with meteorology and pharmacology, however, the interrelationship of theory and practice has been developing over decades as the theory part of the balance becomes more advanced thanks to information technology. For example, an effort called the Machine Tool Genome Project is working toward providing the computer modeling (simulation) needed to predict optimal speed-and-feed combinations for particular setups in any internet-connected shop with less local experimentation and testing. Instead of the only option being the measuring and testing of the behavior of its own equipment, it will benefit from others' experience and simulation; in a sense, rather than 'reinventing a wheel', it will be able to 'make better use of existing wheels already developed by others in remote locations'.\n\nSpeeds and feeds have been studied scientifically since at least the 1890s. The work is typically done in engineering laboratories, with the funding coming from three basic roots: corporations, governments (including their militaries), and universities. All three types of institution have invested large amounts of money in the cause, often in collaborative partnerships. Examples of such work are highlighted below.\n\nIn the 1890s through 1910s, Frederick Winslow Taylor performed turning experiments that became famous (and seminal). He developed Taylor's Equation for Tool Life Expectancy. \n\nScientific study by Holz and De Leeuw of the Cincinnati Milling Machine Company did for milling cutters what F. W. Taylor had done for single-point cutters.\n\n\"Following World War II, many new alloys were developed. New standards were needed to increase [U.S.] American productivity. Metcut Research Associates, with technical support from the Air Force Materials Laboratory and the Army Science and Technology Laboratory, published the first Machining Data Handbook in 1966. The recommended speeds and feeds provided in this book were the result of extensive testing to determine optimum tool life under controlled conditions for every material of the day, operation and hardness.\"\n\n, studied the effect of the variation of cutting parameters in the surface integrity in turning of an AISI 304 stainless steel. They found that the feed rate has the greatest impairing effect on the quality of the surface, and that besides the achievement of the desired roughness profile, it is necessary to analyze the effect of speed and feed on the creation of micropits and microdefects on the machined surface. Moreover, they found that the conventional empirical relation that relates feed rate to roughness value does not fit adequately for low cutting speeds.\n\n\n"}
{"id": "23901899", "url": "https://en.wikipedia.org/wiki?curid=23901899", "title": "Subaru", "text": "Subaru\n\nSubaru cars are known for their use of a boxer engine layout in most vehicles above 1500 cc. The Symmetrical All Wheel Drive drive-train layout was introduced in 1972. The flat/boxer engine and all-wheel-drive became standard equipment for mid-size and smaller cars in most international markets by 1996, and is now standard in most North American market Subaru vehicles. The lone exception is the BRZ, introduced in 2012 via a partnership with Toyota, which uses the boxer engine but instead uses a rear-wheel-drive structure. Subaru also offers turbocharged versions of their passenger cars, such as the Impreza WRX and the Legacy GT.\n\nIn Western markets, the Subaru brand has traditionally been popular among a dedicated core of buyers. Marketing is targeted towards specific niches centered on those who desire the company's signature drive train engine, all-wheel/rough-road capabilities or affordable sports car markets. \n\n\"Subaru\" is the Japanese name for the Pleiades star cluster M45, or the \"Seven Sisters\" (one of whom tradition says is invisible – hence only six stars in the Subaru logo), which in turn inspires the logo and alludes to the companies that merged to create FHI. \n\nFuji Heavy Industries started out as the Aircraft Research Laboratory in 1915, headed by Chikuhei Nakajima. In 1932, the company was reorganized as \"Nakajima Aircraft Company, Ltd\" and soon became a major manufacturer of aircraft for Japan during World War II. At the end of the Second World War Nakajima Aircraft was again reorganized, this time as Fuji Sangyo Co, Ltd. In 1946, the company created the Fuji Rabbit, a motor scooter, with spare aircraft parts from the war. In 1950, Fuji Sangyo was divided into 12 smaller corporations according to the Japanese government's 1950 Corporate Credit Rearrangement Act, anti-zaibatsu legislation. Between 1953 and 1955, five of these corporations and a newly formed corporation decided to merge to form \"Fuji Heavy Industries\". These companies were: \"Fuji Kogyo\", a scooter manufacturer; coachbuilders \"Fuji Jidosha\"; engine manufacturers \"Omiya Fuji Kogyo\"; chassis builders \"Utsunomiya Sharyo\" and the \"Tokyo Fuji Dangyo\" trading company.\n\nKenji Kita, CEO of Fuji Heavy Industries at the time, wanted the new company to be involved in car manufacturing and soon began plans for building a car with the development code-name P-1. Kita canvassed the company for suggestions about naming the P1, but none of the proposals were appealing enough. In the end he gave the company a Japanese name that he \"had been cherishing in his heart\": Subaru, which is the Japanese name for the Pleiades star cluster. The first Subaru car was named the Subaru 1500. Only twenty were manufactured owing to multiple supply issues. Subsequently, the company designed and manufactured dozens of vehicles including the 1500 (1954), the tiny air-cooled 360 (1958), the Sambar (1961), and the 1000 (which saw the introduction of the Subaru boxer engine in 1965).\n\nNissan acquired a 20.4% stake in Fuji Heavy Industries, Subaru's parent company, in 1968 during a period of government-ordered merging of the Japanese auto industry in order to improve competitiveness under the administration of Prime Minister Eisaku Sato. Nissan would utilize FHI's bus manufacturing capability and expertise for their Nissan Diesel line of buses. In turn many Subaru vehicles, even today, use parts from the Nissan manufacturing keiretsu. The Subaru automatic transmission, known as the 4EAT, is also used in the first generation Nissan Pathfinder. While under this arrangement with Nissan, Subaru introduced the R-2 (1969), the Rex and the Leone (1971), the BRAT (1978), Alcyone (1985), the Legacy (1989), the Impreza (1993) (and its WRX subtype), and the Forester (1997).\n\nUpon Nissan's acquisition by Renault, its stake in FHI was sold to General Motors in 1999. Troy Clarke, of General Motors served as representative to Fuji Heavy Industries on their corporate board. During that time, Subaru introduced \nthe Baja (2003), and the Tribeca (2005). \nThe Subaru Forester was sold as a Chevrolet Forester in India in exchange for the Opel Zafira being sold as a Subaru Traviq in Japan. Also, the Chevrolet Borrego concept was presented in 2002, a crossover coupe/pickup truck being derived from the Japanese-market Legacy Turbo platform. During the brief General Motors period, a badge engineered Impreza was sold in the United States as the Saab 9-2X. An SUV (Subaru Tribeca / Saab 9-6X) was also planned<ref name=\"edmunds.com/insideline\"></ref><ref name=\"autocar.co.uk/News\"></ref> but the Saab version did not proceed, and styling was recycled in the 2008 Tribeca refresh.\n\nGM liquidated their holdings in FHI in 2005. Nearly all Saab-Subaru joint projects were dropped at that time, other than Subaru supplying parts for the Saab 9-2x. Toyota Motors bought a little over 40% of GM's former FHI stock, amounting to 8.7% of FHI. \n(The rest of GM's shares went to a Fuji stock buy-back program.) Toyota and Subaru have since collaborated on a number of projects, among them building the Toyota Camry in Subaru's Indiana U.S. plant beginning in April 2007. Subaru introduced the Exiga in 2008.\n\nToyota increased their share of FHI to 16.5% in July 2008. Subsequently, Toyota and Subaru jointly developed the Toyota 86, first sold in January 2012 as the Subaru BRZ and by Toyota under various names.\n\nSome of the advertising slogans Subaru has used in the past include \"Inexpensive, and built to stay that way\" (USA 1970s – early 1980s), \"The World's Favourite Four Wheel Drive\" (in the UK), \"Plus on y pense, plus on a le gout de la conduire\" (Literally: \"The more one thinks, the more one has the taste (or desire, impulse, drive) of driving it\") in French Quebec, \"We built our reputation by building a better car\", \"What to Drive\", \"The Beauty of All-Wheel Drive\", \"Driven by What's Inside\", \"Think, Feel, Drive\", \"Love. It's what makes Subaru, a Subaru\" (USA early 2010s) and currently \"Confidence in Motion\" in North America, \"All 4 The Driver\" in Australia, and \"Uncommon Engineering, Uncommon Stability, Uncommon Roadholding, Uncommon Sense\" in the UK and \"Technology that gives you Confidence in Motion\" in Southeast Asia.\n\nIn the 1990s, an ad firm hired by Subaru found the all wheel drive cars were popular among lesbians. The company started including subtle marketing to this demographic.\n\nAccording to Automotive Lease Guide, Subaru ranked second place in vehicles that have the highest overall predicted resale values among all industry and all luxury vehicles for MY 2009. The awards are derived after carefully studying segment competition, historical vehicle performance and industry trends. According to a study done by J.D. Power and Associates for the 2008 Customer Retention Study, Subaru ranked at 50.5%, which was above the national average of 48%.\n\nSubaru launched an animation series Wish Upon the Pleiades developed jointly with Gainax. The 4-part mini episode series was released on YouTube on February 1, 2011. It featured a magical girl plot with Subaru as a leading protagonist.\n\nSubaru's corporate headquarters are located in Ebisu, Tokyo.\n\nSubaru is distinct from many of its Japanese competitors in that as of early 2016 it still made almost 75% of its cars sold internationally in Japan. Subaru's facilities designated to automotive manufacturing are located in Ōta, Gunma Prefecture, consisting of four locations. Subaru-chō is where the Subaru BRZ/Toyota 86 is built, having been re-purposed from \"kei\" car production, Yajima Plant is where all current Subaru cars are built, Otakita Plant is where commercial \"kei\" trucks are built (originally a factory location of Nakajima Aircraft Company), and Oizumi Plant in Oizumi, Gunma Prefecture, is where engines and transmissions are built.\n\nSubaru's only overseas manufacturing facility is located in Lafayette, Indiana; the factory is called Subaru of Indiana Automotive, Inc.. Due to continued sales growth in North American markets, vehicle production capacity at the Lafayette assembly plant is set to expand to 390,000 vehicles annually. Under the current strategic plan, Subaru will have a total production capacity of 1,026,000 vehicles per year at the end of 2016.\n\nIn 1976, Canadians got their first exposure to Subaru vehicles when Subaru Auto Canada Limited (SACL) began offering the Subaru Leone. In 1989, the privately owned SACL was purchased by the Toronto-based Subaru Canada, Inc. who, under the guidance of parent company Fuji Heavy Industries, began an expansion process that would eventually see over 100 Subaru Dealers in operation across the country.\n\nSubaru Canada, Inc. is a wholly owned subsidiary of Fuji Heavy Industries of Japan. Headquartered in Mississauga, Ontario, the company markets and distributes Subaru vehicles, parts and accessories through a network of 88 authorized dealers throughout Canada.\n\nIn Asian countries outside of Japan, Subaru vehicles, parts and accessories are supplied by Motor Image Group, a wholly owned subsidiary of Hong Kong-based Tan Chong International Limited under businessman Glenn Tan.\n\nSubaru has entered the Philippine operations started in 1996 under the Columbian Motors Philippines ownership but withdrew in 2000. It returned in 2006 under a new ownership by Motor Image Pilipinas, Inc. Subaru has eleven dealerships in the country: Greenhills, Fort Bonifacio, Manila Bay, Alabang, Davao, Cebu, Cagayan de Oro, Iloilo, Santa Rosa, Batangas, and Pampanga.\n\nSubaru once had a presence in South Korea, established in 2009 in Yongsan, Seoul under Choi Seung-dal. Sales started in April 2010 with the Legacy, Outback and Forester as the initial lineup for the South Korean market. They were the fifth Japanese automobile manufacturer to enter after Toyota, Honda, Nissan and Mitsubishi. According to the company, they delayed their entry due to market dominance by Hyundai and Kia. By 2012, Subaru Korea announced that they would discontinue selling 2013 car models due to low sales.\n\nIn 1974 Robert Edmiston was finance director at sports car manufacturer Jensen Motors. When the company went bankrupt, he used a redundancy payout to set up International Motors, which acquired the UK franchise for Subaru and Isuzu. The Coleshill based company is still the parent for Subaru in the UK.\n\nSubaru of America was established in 1968 in Philadelphia by Malcolm Bricklin and Harvey Lamm. It relocated to Pennsauken, New Jersey shortly thereafter and moved to its current headquarters in Cherry Hill, New Jersey when Fuji Heavy Industries acquired full ownership. Subaru of America operates regional offices, zone offices and parts distribution centers throughout the United States. Subaru of America also operates port facilities on both the West and East coasts.\n\nIn 1989 Subaru and then-partner Isuzu opened a joint factory in Lafayette, Indiana called Subaru-Isuzu Automotive, or SIA, which initially manufactured the Subaru Legacy and Isuzu Rodeo. In 2001 Isuzu sold their stake in the plant to FHI for $1 due to flagging sales and it was renamed Subaru of Indiana Automotive, Inc. SIA has been designated a backyard wildlife habitat by the National Wildlife Federation and has achieved a zero-landfill production designation (the first automotive assembly plant in the United States to earn that designation).\n\nAccording to the Kelley Blue Book in 2015, two Subaru models Forester and the Outback in the United States had very short inventory time (the time between being received by a dealer to being sold).\n\nSubaru Rally Team Japan led by Noriyuki Koseki (founder of Subaru Tecnica International, STI) ran Subaru Leone coupé, sedan DL, RX (SRX) and RX Turbo in the World Rally Championship between 1980 and 1989. Drivers for individual rallies included Ari Vatanen, Per Eklund, Shekhar Mehta, Mike Kirkland, Possum Bourne and Harald Demut. Mike Kirkland finished 6th overall and won the A Group at the 1986 Safari Rally. That year Subaru was one of the only manufacturers combining 4WD and turbo after Audi's successful quattro system had been introduced in 1980, but Audi withdrew from the WRC after safety concerns and Ford's serious accident early in the 1986 season. Subaru changed the rally model to Legacy RS for the 1990–1992 period and took part in the first complete season in the World Rally Championship with the same model in 1993.\n\nModified versions of the Impreza WRX and WRX STi have been competing successfully in rallying. Drivers Colin McRae (1995), Richard Burns (2001) and Petter Solberg (2003) have won World Rally Championship drivers' titles with the Subaru World Rally Team and Subaru took the manufacturers' title three years in a row from 1995 to 1997. Subaru's World Rally Championship cars are prepared and run by Prodrive, the highly successful British motorsport team. Several endurance records were set in the early and mid-nineties by the Subaru Legacy. The Subaru Justy also holds the world record for the fastest sub 1.0L car without a turbo: 123.224 mph average, it was set in 1989.\n\nSubaru was briefly involved in Formula One circuit racing when it bought a controlling interest in the tiny Italian Coloni team for the 1990 season. The Coloni 3B's 12-cylinder engine was badged as a Subaru and shared the boxer layout with the company's own engines, but was an existing design built by Italian firm Motori Moderni. The cars were overweight and underpowered and the partnership broke down before the season finished. With the rise of rally racing and the Import scene in the US, the introduction of the highly anticipated Subaru Impreza WRX in 2001 was successful in bringing high-performance AWD compact cars into the sports car mainstream. Subaru supplies a factory-backed team, Subaru Rally Team USA for Rally America and has won the driver's title six times, most recently in 2011 with David Higgins. \"Grassroots Motorsports\" awarded Subaru with the Editors' Choice Award in 2002.\n\nOn 16 December 2008, it was announced that Subaru would no longer be competing in the World Rally Championships. The decision was made by Subaru’s parent company, Fuji Heavy Industries (FHI), partly as a result of the economic downturn but also because it was felt Subaru had achieved its sporting and marketing objectives. Mr Ikuo Mori denied that alterations to the WRC technical regulations in 2010 or a rumoured deterioration in the working relationship with Prodrive had any impact on the decision. He also said that the possibility of a Subaru car back in the top category of WRC in the future is not zero, but for this moment there can be no assumption of a comeback.\n\nSince 2005, Cusco Racing have entered an Impreza sedan and a BRZ in the Super GT championship. In 2008, the Impreza was the first 4-door and first 4WD vehicle to win a race.\n\nStarting in 2006, Subaru of America (SOA), as the official distributor of Subaru vehicles in the United States participates in the Subaru Road Racing Team (SRRT) with a Subaru Legacy 2.5 GT Spec-B in the Grand-Am Street Tuner class. In 2010, SRRT campaigns a Subaru Impreza WRX STI in the Grand Sport class. In 2011, SRRT switched from the hatchback to a 2011 Subaru Impreza WRX STI sedan.\n\nOn 4 May 2012, Subaru Rally Team USA announced that a new rallycross team, Subaru Puma Rallycross Team USA will participate in the 2012 Global RallyCross Championship season with Dave Mirra, Bucky Lasek, and Sverre Isachsen. They also competed in the 2014 FIA World Rallycross Championship.\n\nIn 2011, Mark Higgins used a stock Impreza to set a lap record at the Isle of Man TT course. In 2016, Higgins again broke the record in a modified WRX STI.\n\nThe impreza has won hillclimbs such as the Silverstone Race to the Sky and Mount Washington Hillclimb Auto Race.\n\nThe 2007 Frankfurt International Motor Show saw Subaru introduce a horizontally opposed, water-cooled, common rail turbodiesel using a variable geometry turbocharger called the Subaru EE engine, the first of its type to be fitted to a passenger car. Volkswagen had experimented with this idea during the 1950s and made two air-cooled boxer prototype diesel engines that were not turbocharged. VW installed one engine in a Type 1 and another in a Type 2.\n\nThe Subaru engine was rated at and with a displacement of 2.0 litres. In March 2008 Subaru offered the Legacy sedan and wagon and the Outback wagon with 2.0 litre turbodiesel in the EU with a 5-speed manual transmission.\n\nIn September 2008 Subaru announced that the diesel Forester and diesel Impreza will be introduced at the 2008 Paris Motor Show, with Forester sales to begin October 2008 and diesel Impreza sales to start January 2009. The Forester and Impreza will have a 6-speed manual transmission, whereas the Legacy and Outback have 5-speed manual transmissions.\n\nUnited States Environmental Protection Agency fuel economy estimated is:\n\n\nIn June 2006, Fuji Heavy Industries, Inc. (FHI) launched its Subaru Stella Plug-in electric vehicle which is a kei car equipped with a lithium-ion battery pack. The vehicle has a short range of but it actually costs more than the Mitsubishi iMiEV, at (), including Japanese Government consumption taxes with an exemption of $2,240. It will also qualify for a rebate from the Japanese Government of up to $14,200, bringing the price down to $30,660. The vehicle is much like the i-MiEV, with a 47-kilowatt motor and a quick-charge capability, but the two-door mini-car has a boxy shape. FHI plans to start delivery in late July and plans to sell 170 vehicles by March 2010.\n\nIn Japan, Subaru is currently testing two electric vehicles called the Subaru G4e and the Subaru R1e.\n\nThe Subaru Hybrid Tourer Concept is a four-seat vehicle with gull-wing doors that combines a 2-liter turbocharged direct-injection gasoline engine with a continuously variable transmission and two axle-mounted motors. A lithium-ion battery pack provides energy storage for the vehicle.\n\nIn early 2018, Subaru, along with Suzuki, Daihatsu and Hino Motors, joined the nascent EV C.A. Spirit venture to collaborate on electric vehicle technology development. The project was launched by Toyota, Mazda and automotive component manufacturer Denso in September 2017.\n\nWith Subaru among the few makes lacking any electric models - in the U.S., the short-lived Crosstrek hybrid was on the market from 2014-2017 - the company expects to launch a plug-in hybrid (PHEV) with the 2019 model year, based in large part on technology from shareholder Toyota's Prius Prime platform, and to be only available in California/ZEV markets. A full EV is in the works and promised for 2021.\n\nSince the 2005 model year, Subaru has adopted the CAN bus technology for the USA and Canada markets. Starting in the 2007 model year, all Subaru vehicles use the CAN technology. Typically, two CAN-buses are used on vehicles: a high-speed CAN running at 500 kbit/s for powertrain communication, and a low-speed CAN running at 125 kbit/s for body control functions and instrument panels. A body-integrated unit (BIU) is used between these two networks.\n\nClarion and Harman Kardon are among the audio, video, and navigation technology suppliers for Subaru products in North America. Clarion announced in 2015 that it was introducing its \"Smart Access\" platform, formerly only offered on Clarion's aftermarket products, to the units to be installed in certain Subaru 2015 models in North America. Smart Access is able to work with the driver's smartphone (either iPhone or Android) and allows access to various car-safe apps running on the phone via the car's built-in infotainment screen.\n\nSubaru and Clarion have also, with Liberty Mutual Insurance, introduced the \"RightTrack\" in-vehicle app which will be able to monitor the driver's habits, make suggestions for safer driving, and possibly offer insurance discounts.\nIn 2008, Subaru introduced their EyeSight driver assistance and safety system. Unlike most such systems, which use radar or sonar sensors, EyeSight uses dual video cameras mounted at the top of the windshield. Depth information is derived from the parallax between two video signals and used to judge the distance to the next vehicle for features such as pre-collision braking and adaptive cruise control. The company has continued to improve the technology, which now includes lane-keeping assistance. EyeSight is now available on most Subaru models (usually in higher trim levels) and is standard on the Subaru Ascent.\n\nSubaru debuted its new chassis design, dubbed the Subaru Global Platform, with the release of the 2017 Impreza. Having spent over a billion dollars on research and development the company plans to extend the architecture to all of its other models, with the exception of the BRZ which is co-developed with Toyota. By incorporating high-strength steel into the chassis updated vehicles will have stiffer bodies that increase safety through greater impact absorption while also improving ride comfort. Another focus of the new platform is modularity, allowing Subaru to reduce development costs by streamlining production throughout its network of facilities. The platform will be able to accommodate a variety of powertrains, including gasoline, hybrid, and fully electric designs. \n\nSubaru claims to have implemented advanced policies which include recycling, reducing harmful emissions, and educating their employees. Their efforts have helped them in their environmental initiatives. The Subaru plant in Lafayette, Indiana (SIA) was the first auto assembly plant to achieve zero landfill status; nothing from the manufacturing process goes into a landfill. The company has developed a recycling plan for the \"end-of-life\" of their cars. Most of their modern products use highly recyclable materials throughout the vehicle, in the engine, transmission, suspension and elsewhere in each vehicle leaving Subaru with a 97.3% recycling ratio rate for their end-of-life vehicles.\n\nAn excerpt from the Subaru website stated \"In 2006, SIA was awarded the United States Environmental Protection Agency´s Gold Achievement Award as a top achiever in the agency's WasteWise program to reduce waste and improve recycling.\" The website also stated that \"It also became the first U.S. automotive assembly plant to be designated a wildlife habitat.\"\n\nSubaru currently offers a Partial Zero Emissions Vehicle (PZEV) certified Legacy, Outback, Impreza, XV/Crosstrek and Forester models which are available for sale anywhere in the U.S. Subaru PZEV vehicles meet California's Super-Ultra-Low-Emission Vehicle exhaust emission standard. All other models have been certified LEV2.\n\n\n\n\n\n\n\nSubaru has partnered with various manufacturers over time – here are some of the models sold in Asia and Europe. In Japan they are in the Kei car class with either front or all wheel drive and a straight engine.\nAn article posted by Autoblog on April 16, 2008 stated that due to a corporate investment by Toyota, all Kei cars built by Subaru will be replaced by Daihatsu models beginning in 2010.\n\n\nThe following concepts vehicles did not go forward as production vehicles:\n\n\n\n"}
{"id": "10186884", "url": "https://en.wikipedia.org/wiki?curid=10186884", "title": "The Greenhouse Conspiracy", "text": "The Greenhouse Conspiracy\n\nThe Greenhouse Conspiracy is a documentary film broadcast by Channel 4 in the United Kingdom on 12 August 1990, as part of the \"Equinox\" series, which criticised the theory of global warming and asserted that scientists critical of global warming theory were denied funding. It is one of the earliest instances of the suggestion of a conspiracy to promote false claims supporting global warming. Although the title of the program implied the existence of a conspiracy, when interviewed on the program Patrick Michaels played down the idea, saying, \"It may not quite add up to a conspiracy, but certainly a coalition of interests has promoted the greenhouse theory: scientists have needed funds, the media a story, and governments a worthy cause\".\n"}
{"id": "14291783", "url": "https://en.wikipedia.org/wiki?curid=14291783", "title": "Tracer-gas leak testing", "text": "Tracer-gas leak testing\n\nA tracer-gas leak testing method is a nondestructive testing method that detects gas leaks. A variety of methods with different sensitivities exist. Tracer-gas leak testing is used in the petrochemical industry, the automotive industry, and in the manufacture of semiconductors, among other uses.\n\nSeveral tracer-gas leak testing methods exist, including:\n\nThe nature of the product or the process and the process gases will set the leak rate requirement:\nBased on the target leak rate, the table below will help to choose the most suitable method.\nTypical applications of tracer-gas leak testing include:\n\nSeveral standards apply to leak testing and more specifically to tracer-gas leak testing methods, for example:\n"}
{"id": "46898028", "url": "https://en.wikipedia.org/wiki?curid=46898028", "title": "Trouée d'Arenberg", "text": "Trouée d'Arenberg\n\nThe Trouée d'Arenberg or Tranchée de Wallers-Arenberg (English: \"Trench of Arenberg\") is a 2.4 km long cobbled road in the municipality of Wallers in Northern France, in the Département Nord. The road's official name is \"La Drève des Boules d'Hérin\" \"(\"Bullet Alley of Hérin\")\" and crosses the \"Forêt de Raismes-Saint-Amand-Wallers\", outside France better known as the Forest of Arenberg. It is best known from the annual cycling classic Paris–Roubaix held in April, where it is one of the most difficult passages of the race.\n\nOfficially, the 2,400 meters of cobbles were laid in the time of Napoleon I, in the late 18th century, crossing the large forest of Saint-Amand-Wallers, close to Wallers and just west of Valenciennes. The road is straight and narrow (3 m), dropping slightly when entering the forest from the village of Arenberg, then rising in the second half. The altitude is 25m at the start and 19m at the end. The cobbles are extremely difficult to ride because of their irregularity. Many fans have taken away cobbles as souvenirs, leading to a regular check-up of the road.\n\nFrançois Doulcier, the president of \"Les Amis de Paris-Roubaix\", the voluntary association which takes care of the race's cobblestones, said that \"objectively speaking, it’s the worst-maintained sector of cobbles in the whole race\", giving three reasons: the rough and grooved surface of the stones, resulting from poor cutting; the wide gaps between the stones; and the uneven laying of the cobbles. These difficulties mean that riders have to carefully balance the need to avoid accidents and mechanical problems with riding at high speed.\n\nIt has been noted that the road's layout, as a long straight surrounded by trees, is unique among Paris-Roubaix's cobbled \"secteurs\": Doulcier has stated that \"it gives the impression that you’re standing in a cathedral. Even if it were tarmacked over, it would be impressive\".\n\nThe Trench of Arenberg was first included in Paris–Roubaix in 1968 and has become an iconic location of the cobbled classic. It is one of three \"five star\" sections of \"pavé\", together with the sections of Mons-en-Pévèle and Carrefour de l'Arbre which come later in the race.\n\nThe introduction of the \"secteur\" was in response to the resurfacing of many cobbled roads after World War II with tarmac or asphalt. This change had a significant effect on the parcours of the cobbled classics: by the 1965 edition of Paris-Roubaix cobbled sections only accounted for 22km of the 265.5km route. As a result the race was becoming easier, with the 1967 edition being won by Jan Janssen in a small group sprint of 15 riders. In reaction to this race director Jacques Goddet asked Albert Bouvet, a recently retired rider who had been appointed course designer for the race, to find new cobbled \"secteurs\" to add to the parcours.\n\nThe site was proposed for Paris–Roubaix by former professional cyclist Jean Stablinski, who had worked in the mine under the woods of Arenberg. The mine closed in 1990 (later being used by director Claude Berri to shoot his film \"Germinal\") and the cobbled passage is now classified. Although almost 100 km from Roubaix, the sector usually proves decisive and as Stablinski stated, \"Paris–Roubaix is not won in Arenberg, but from there the group with the winners is selected\". A memorial to Stablinski stands at one end of the road.\n\nDespite his desire to increase the difficulty of the race, Goddet was initially reluctant to include the Arenberg due to its extreme difficulty. However it was included in the 1968 race, and the field raced through it without any problems. Despite this, it was removed from the race in 1974 and only returned in 1983 as a generally permanent fixture on the parcours. The race's passage through the Arenberg was broadcast on live television for the first time the following year, where the field was led by Gregor Braun and Roubaix native Alain Bondue, who were team-mates on the local La Redoute team and who both finished in the top five of the race.\n\nThe abandonment of the mines caused sections of the road to subside. In 1998 Johan Museeuw, leading the World Cup, crashed heavily on the Trouée and broke his kneecap, nearly spelling the end of his career. In 2001 French rider Philippe Gaumont broke his femur after falling at the beginning of the Trouée when leading the peloton and never returned to racing at the highest level. Consequently, the Trouée d'Arenberg was left out in 2005, as conditions had deteriorated beyond safety limits. Regional and local councils spent €250,000 to restore the road and add 50 cm to its width. The section was included again in 2006.\n\nBecause of its difficulty, it is considered a crucial site of the race, although at 85 kilometers, it is relatively far from the finish in Roubaix. Gilbert Duclos-Lassalle, twice a winner of the race, has said that \"when you leave the Arenberg badly placed or in the red it’s then that you know that you won’t be in the mix in the final... Once out of the forest you may not have won the race, but you’ll certainly know if you have lost it\". Thierry Gouvenou, a former winner of Paris–Roubaix Espoirs, suggested that \"the race can clearly be split between what happens before and what happens after the Arenberg. Because until you have crossed the threshold of the Trench it’s not even worth thinking about what comes next\".\n\nIt is also the only site of the race where guard rails are placed, as the road is narrow and fans gather in large numbers to see the race. In the earlier years of the section's inclusion, riders were often able to avoid riding on the cobbles by using the verges on the edge of the road - Gouvenou claimed that riders only had to ride 400 of the \"secteur\"'s 2400 metres on the cobblestones - however in the 1990s the barriers were introduced to prevent crashes. According to Doulcier, over 10,000 fans watch the race on the Arenberg every year.\n\nThe Trouée d'Arenberg was never included in the Tour de France, but two stages of the Tour have finished in the village of Arenberg, at the forest's entrance. In 2010 cobble specialist Thor Hushovd won the third stage with seven cobbled sectors. In 2014 Lars Boom won the fifth stage of the Tour near the entrance of the Trouée ahead of Jakob Fuglsang and Vincenzo Nibali. The stage saw defending champion Chris Froome crash out in a memorable day in inclement weather.\n\n"}
{"id": "5547312", "url": "https://en.wikipedia.org/wiki?curid=5547312", "title": "Vacuum deposition", "text": "Vacuum deposition\n\nVacuum deposition is a family of processes used to deposit layers of material atom-by-atom or molecule-by-molecule on a solid surface. These processes operate at pressures well below atmospheric pressure (i.e., vacuum). The deposited layers can range from a thickness of one atom up to millimeters, forming freestanding structures. Multiple layers of different materials can be used, for example to form optical coatings. The process can be qualified based on the vapor source; physical vapor deposition uses a liquid or solid source and chemical vapor deposition uses a chemical vapor.\n\nThe vacuum environment may serve one or more purposes:\n\nCondensing particles can be generated in various ways:\n\nIn reactive deposition, the depositing material reacts either with a component of the gaseous environment (Ti + N → TiN) or with a co-depositing species (Ti + C → TiC). A plasma environment aids in activating gaseous species (N → 2N) and in decomposition of chemical vapor precursors (SiH → Si + 4H). The plasma may also be used to provide ions for vaporization by sputtering or for bombardment of the substrate for sputter cleaning and for bombardment of the depositing material to densify the structure and tailor properties (ion plating).\n\nWhen the vapor source is a liquid or solid the process is called physical vapor deposition (PVD). When the source is a chemical vapor precursor, the process is called chemical vapor deposition (CVD). The latter has several variants: \"low-pressure chemical vapor deposition\" (LPCVD), Plasma-enhanced chemical vapor deposition (PECVD), and \"plasma-assisted CVD\" (PACVD). Often a combination of PVD and CVD processes are used in the same or connected processing chambers.\n\n\nA thickness of less than one micrometre is generally called a thin film while a thickness greater than one micrometre is called a coating.\n\n\n"}
{"id": "2669755", "url": "https://en.wikipedia.org/wiki?curid=2669755", "title": "Vienna Convention on Civil Liability for Nuclear Damage", "text": "Vienna Convention on Civil Liability for Nuclear Damage\n\nThe Vienna Convention on Civil Liability for Nuclear Damage is a 1963 treaty that governs issues of liability in cases of nuclear accident.\nIt was concluded at Vienna on 21 May 1963 and entered into force on 12 November 1977. The convention has been amended by a 1997 protocol. The depository is the International Atomic Energy Agency.\n\nAs of February 2014, the convention has been ratified by 40 states. Colombia, Israel, Morocco, Spain, and the United Kingdom have signed the convention but have not ratified it. Slovenia has denounced the treaty and withdrawn from it.\n\n"}
{"id": "964428", "url": "https://en.wikipedia.org/wiki?curid=964428", "title": "Weighing scale", "text": "Weighing scale\n\nA weighing scale (or weighing balance) is devices to measure weight or mass. They are also known as mass scales, weight scales, mass balance, weight balance, or simply scale, balance, or balance scale.\n\nThe traditional scale consists of two plates or bowls suspended at equal distance of a fulcrum. One plate holds an object of unknown mass (or weight), while known masses are added to the other plate until static equilibrium is achieved and the plates level off, which happens when the mass on each plates is equal. A spring scale will make use of a spring of known stiffness to determine mass (or weight). Suspending a certain mass will extend the spring by a certain amount depending on the spring's stiffness (or spring constant). The heavier the object, the more the spring stretches, as described in Hooke's law. Other types of scale making use of different physical principles also exist.\n\nSome of scales can be calibrated to read in units of force (weight) such as newtons instead of units of mass such as kilograms. Scales and balances are widely used in commerce and many are sold and packaged by mass.\n\nThe balance scale is such a simple device that its usage likely far predates the evidence. What has allowed archaeologists to link artifacts to weighing scales are the stones for determining absolute mass. The balance scale itself was probably used to determine relative mass long before absolute mass.\n\nThe oldest evidence for the existence of weighing scales dates to c. 2400–1800 B.C. in the Indus River valley (modern-day Pakistan). Prior to that, no banking was performed due to lack of scales. Uniform, polished stone cubes discovered in early settlements were probably used as mass-setting stones in balance scales. Although the cubes bear no markings, their masses are multiples of a common denominator. The cubes are made of many different kinds of stones with varying densities. Clearly their mass, not their size or other characteristics, was a factor in sculpting these cubes.\n\nIn Egypt, scales can be traced to around 1878 B.C., but their usage probably extends much earlier. Carved stones bearing marks denoting mass and the Egyptian hieroglyphic symbol for gold have been discovered, which suggests that Egyptian merchants had been using an established system of mass measurement to catalog gold shipments or gold mine yields. Although no actual scales from this era have survived, many sets of weighing stones as well as murals depicting the use of balance scales suggest widespread usage. In China, the earliest weighing balance excavated was from a tomb of the State of Chu of the Chinese Warring States Period dating back to the 3rd to 4th century BC in Mount Zuojiagong near Changsha, Hunan. The balance was made of wood and used bronze masses.\n\nVariations on the balance scale, including devices like the cheap and inaccurate \"bismar\" (unequal-armed scales), began to see common usage by c. 400 B.C. by many small merchants and their customers. A plethora of scale varieties each boasting advantages and improvements over one another appear throughout recorded history, with such great inventors as Leonardo da Vinci lending a personal hand in their development. \n\nEven with all the advances in weighing scale design and development, all scales until the seventeenth century AD were variations on the balance scale. The standardization of the weights used – and ensuring traders used the correct weights – was a considerable preoccupation of governments throughout this time. \n\nThe original form of a balance consisted of a beam with a fulcrum at its center. For highest accuracy, the fulcrum would consist of a sharp V-shaped pivot seated in a shallower V-shaped bearing. To determine the mass of the object, a combination of reference masses was hung on one end of the beam while the object of unknown mass was hung on the other end (see balance and steelyard balance). For high precision work, such as empirical chemistry, the center beam balance is still one of the most accurate technologies available, and is commonly used for calibrating test masses.\n\nThe balance (also balance scale, beam balance and laboratory balance) was the first mass measuring instrument invented. In its traditional form, it consists of a pivoted horizontal lever with arms of equal lengththe beamand a weighing pan suspended from each arm (hence the plural name \"\"scales\" for a weighing instrument). The unknown mass is placed in one pan and standard masses are added to the other pan until the beam is as close to equilibrium as possible. In precision balances, a more accurate determination of the mass is given by the position of a sliding mass moved along a graduated scale. Technically, a balance compares weight rather than mass, but, in a given gravitational field (such as Earth's gravity), the weight of an object is proportional to its mass, so the standard masses used with balances are usually labeled in units of mass (e.g. g or kg).\n\nUnlike spring-based scales, balances are used for the precision measurement of mass as their accuracy is not affected by variations in the local gravitational field. (On Earth, for example, these can amount to ±0.5% between locations.) A change in the strength of the gravitational field caused by moving the balance does not change the measured mass, because the moments of force on either side of the beam are affected equally. A balance will render an accurate measurement of mass at any location experiencing a constant gravity or acceleration.\n\nVery precise measurements are achieved by ensuring that the balance's fulcrum is essentially friction-free (a knife edge is the traditional solution), by attaching a pointer to the beam which amplifies any deviation from a balance position; and finally by using the lever principle, which allows fractional masses to be applied by movement of a small mass along the measuring arm of the beam, as described above. For greatest accuracy, there needs to be an allowance for the buoyancy in air, whose effect depends on the densities of the masses involved.\n\nTo reduce the need for large reference masses, an off-center beam can be used. A balance with an off-center beam can be almost as accurate as a scale with a center beam, but the off-center beam requires special reference masses and cannot be intrinsically checked for accuracy by simply swapping the contents of the pans as a center-beam balance can. To reduce the need for small graduated reference masses, a sliding weight called a poise can be installed so that it can be positioned along a calibrated scale. A poise adds further intricacies to the calibration procedure, since the exact mass of the poise must be adjusted to the exact lever ratio of the beam.\n\nFor greater convenience in placing large and awkward loads, a platform can be \"floated\" on a cantilever beam system which brings the proportional force to a \"noseiron\" bearing; this pulls on a \"stilyard rod\" to transmit the reduced force to a conveniently sized beam.\nOne still sees this design in portable beam balances of 500 kg capacity which are commonly used in harsh environments without electricity, as well as in the lighter duty mechanical bathroom scale (which actually uses a spring scale, internally). The additional pivots and bearings all reduce the accuracy and complicate calibration; the float system must be corrected for corner errors before the span is corrected by adjusting the balance beam and poise. \n\nIn 1669 the Frenchman Gilles Personne de Roberval presented a new kind of balance scale to the French Academy of Sciences. This scale consisted of a pair of vertical columns separated by a pair of equal-length arms and pivoting in the center of each arm from a central vertical column, creating a parallelogram. From the side of each vertical column a peg extended. To the amazement of observers, no matter where Roberval hung two equal weight along the peg, the scale still balanced. In this sense, the scale was revolutionary: it evolved into the more-commonly encountered form consisting of two pans placed on vertical column located above the fulcrum and the parallelogram below them. The advantage of the Roberval design is that no matter where equal weights are placed in the pans, the scale will still balance.\n\nFurther developments have included a \"gear balance\" in which the parallelogram is replaced by any odd number of interlocking gears greater than one, with alternating gears of the same size and with the central gear fixed to a stand and the outside gears fixed to pans, as well as the \"sprocket gear balance\" consisting of a bicycle-type chain looped around an odd number of sprockets with the central one fixed and the outermost two free to pivot and attached to a pan.\n\nBecause it has more moving joints which add friction, the Roberval balance is consistently less accurate than the traditional beam balance, but for many purposes this is compensated for by its usability.\n\nA microbalance is an instrument capable of making precise measurements of mass of objects of relatively small mass: of the order of a million parts of a gram.\n\nAn analytical balance is a class of balance designed to measure small mass in the sub-milligram range. The measuring pan of an analytical balance (0.1 mg or better) is inside a transparent enclosure with doors so that dust does not collect and so any air currents in the room do not affect the balance's operation. This enclosure is often called a draft shield. The use of a mechanically vented balance safety enclosure, which has uniquely designed acrylic airfoils, allows a smooth turbulence-free airflow that prevents balance fluctuation and the measure of mass down to 1 μg without fluctuations or loss of product. Also, the sample must be at room temperature to prevent natural convection from forming air currents inside the enclosure from causing an error in reading. Single-pan mechanical substitution balance maintains consistent response throughout the useful capacity is achieved by maintaining a constant load on the balance beam, thus the fulcrum, by subtracting mass on the same side of the beam to which the sample is added.\n\nElectronic analytical scales measure the force needed to counter the mass being measured rather than using actual masses. As such they must have calibration adjustments made to compensate for gravitational differences. They use an electromagnet to generate a force to counter the sample being measured and outputs the result by measuring the force needed to achieve balance. Such measurement device is called electromagnetic force restoration sensor.\n\nPendulum type scales do not use springs. This design uses pendulums and operates as a balance and is unaffected by differences in gravity. An example of application of this design are scales made by the Toledo Scale Company.\n\nThe scales (specifically, a two-pan, beam balance) are one of the traditional symbols of justice, as wielded by statues of Lady Justice. This corresponds to the use in metaphor of matters being \"held in the balance\". It has its origins in ancient Egypt.\n\nScales are also the symbol for the astrological sign Libra.\n\nAlthough records dating to the 1700s refer to spring scales for measuring mass, the earliest design for such a device dates to 1770 and credits Richard Salter, an early scale-maker. Spring scales came into wide usage in the United Kingdom after 1840 when R. W. Winfield developed the candlestick scale for weighing letters and packages, required after the introduction of the Uniform Penny Post. Postal workers could work more quickly with spring scales than balance scales, because they could be read instantaneously and did not have to be carefully balanced with each measurement.\n\nBy the 1940s, various electronic devices were being attached to these designs to make readings more accurate. Load cells – transducers that convert force to an electrical signal – have their beginnings as early as the late nineteenth century, but it was not until the late twentieth century that their widespread usage became economically and technologically viable.\n\nA mechanical scale or balance is used to describe a weighing device that is used to measure the mass, force exertion, tension and resistance of an object without the need of a power supply. Types of mechanical scale include spring scales, hanging scales, triple beam balances and force gauges.\n\nA spring scale measures mass by reporting the distance that a spring deflects under a load. This contrasts to a \"balance\", which compares the torque on the arm due to a sample weight to the torque on the arm due to a standard reference mass using a horizontal lever. Spring scales measure force, which is the tension force of constraint acting on an object, opposing the local force of gravity. They are usually calibrated so that measured force translates to mass at earth's gravity. The object to be weighed can be simply hung from the spring or set on a pivot and bearing platform.\n\nIn a spring scale, the spring either stretches (as in a hanging scale in the produce department of a grocery store) or compresses (as in a simple bathroom scale). By Hooke's law, every spring has a proportionality constant that relates how hard it is pulled to how far it stretches. Weighing scales use a spring with a known spring constant (see Hooke's law) and measure the displacement of the spring by any variety of mechanisms to produce an estimate of the gravitational force applied by the object. Rack and pinion mechanisms are often used to convert the linear spring motion to a dial reading.\n\nSpring scales have two sources of error that balances do not: the measured mass varies with the strength of the local gravitational force (by as much as 0.5% at different locations on Earth), and the elasticity of the measurement spring can vary slightly with temperature. With proper manufacturing and setup, however, spring scales can be rated as legal for commerce. To remove the temperature error, a commerce-legal spring scale must either have temperature-compensated springs or be used at a fairly constant temperature. To eliminate the effect of gravity variations, a commerce-legal spring scale must be calibrated where it is used.\n\nIt is also common in high-capacity applications such as crane scales to use hydraulic force to sense mass. The test force is applied to a piston or diaphragm and transmitted through hydraulic lines to a dial indicator based on a Bourdon tube or electronic sensor.\n\nElectronic digital scales display weight as a number, usually on a liquid crystal display (LCD). They are versatile because they may perform calculations on the measurement and transmit it to other digital devices. In a digital scale, the force of the weight causes a spring to deform, and the amount of deformation is measured by one or more transducers called strain gauges. A strain gauge is a conductor whose electrical resistance changes when its length changes. Strain gauges have limited capacity and larger digital scales may use a hydraulic transducer called a load cell instead. A voltage is applied to the device, and the weight causes the current through it to change. The current is converted to a digital number by an analog-to-digital converter, translated by digital logic to the correct units, and displayed on the display. Usually the device is run by a microprocessor chip. \n\nA digital bathroom scale is a scale on the floor which a person stands on. The weight is shown on an LED or LCD display. The digital electronics may do more than just display weight, it may calculate body fat, BMI, lean mass, muscle mass, and water ratio. Some modern bathroom scales are wirelessly connected and have features like smartphone integration, cloud storage, and fitness tracking. They are usually powered by a button cell, or battery of AA or AAA size.\n\nDigital kitchen scales are used for weighing food in a kitchen during cooking. These are usually light-weight and compact.\n\nIn electronic versions of spring scales, the deflection of a beam supporting the unknown mass is measured using a strain gauge, which is a length-sensitive electrical resistance. The capacity of such devices is only limited by the resistance of the beam to deflection. The results from several supporting locations may be added electronically, so this technique is suitable for determining the mass of very heavy objects, such as trucks and rail cars, and is used in a modern weighbridge.\n\nThese scales are used in the modern bakery, grocery, delicatessen, seafood, meat, produce and other perishable goods departments. Supermarket scales can print labels and receipts, mark mass and count, unit price, total price and in some cases tare. Some modern supermarket scales print an RFID tag that can be used to track the item for tampering or returns. In most cases, these types of scales have a sealed calibration so that the reading on the display is correct and cannot be tampered with. In the USA, the scales are certified by the National Type Evaluation Program (NTEP), in South Africa by the South African Bureau of Standards and in the UK by the International Organization of Legal Metrology.\n\nMost countries regulate the design and servicing of scales used for commerce. This has tended to cause scale technology to lag behind other technologies because expensive regulatory hurdles are involved in introducing new designs. Nevertheless, there has been a trend to \"digital load cells\" which are actually strain-gauge cells with dedicated analog converters and networking built into the cell itself. Such designs have reduced the service problems inherent with combining and transmitting a number of 20 millivolt signals in hostile environments.\n\nGovernment regulation generally requires periodic inspections by licensed technicians, using masses whose calibration is traceable to an approved laboratory. Scales intended for non-trade use, such as those used in bathrooms, doctor's offices, kitchens (portion control), and price estimation (but not official price determination) may be produced, but must by law be labelled \"Not Legal for Trade\" to ensure that they are not re-purposed in a way that jeopardizes commercial interest. In the United States, the document describing how scales must be designed, installed, and used for commercial purposes is NIST \"Handbook 44\". Legal For Trade (LFT) certification usually approve the readability as repeatability/10 to ensure a maximum margin of error of 10%.\n\nBecause gravity varies by over 0.5% over the surface of the earth, the distinction between force due to gravity and mass is relevant for accurate calibration of scales for commercial purposes. Usually the goal is to measure the mass of the sample rather than its force due to gravity at that particular location.\n\nTraditional mechanical balance-beam scales intrinsically measured mass. But ordinary electronic scales intrinsically measure the gravitational force between the sample and the earth, i.e. the weight of the sample, which varies with location. So such a scale has to be re-calibrated after installation, for that specific location, in order to obtain an accurate indication of mass.\n\nSome of the sources of error in weighing are:\n\nIn 2014 a concept of hybrid scale was introduced, the elastically deformable arm scale, which is a combination between a spring scale and a beam balance, exploiting simultaneously both principles of equilibrium and deformation. In this scale, the rigid arms of a classical beam balance (for example a steelyard) are replaced with a flexible elastic rod in an inclined frictionless sliding sleeve. The rod can reach a unique free of sliding equilibrium when two vertical dead loads (or masses) are applied at its edges. Equilibrium, which would be impossible with rigid arms, is guaranteed because configurational forces develop at the two edges of the sleeve as a consequence of both the free sliding condition and the nonlinear kinematics of the elastic rod. This mass measuring device can also work without a counterweight.\n\n"}
{"id": "20737517", "url": "https://en.wikipedia.org/wiki?curid=20737517", "title": "Westmill Wind Farm Co-operative", "text": "Westmill Wind Farm Co-operative\n\nWestmill Wind Farm Co-operative Ltd is a community-owned Industrial and Provident Society that owns 100% of the Westmill Wind Farm which is an onshore wind farm near the village of Watchfield in the Vale of White Horse, England. It has five 1.3 MW wind turbines erected in a line along the disused runway of the former RAF Watchfield. The wind farm has a power output of up to 6.5 MW, projected to produce as much electricity in a year as used by more than 2,500 homes. The turbines were erected in 8 days and the first fully month of generation was March 2008. It has an open day usually in June each year.\n\nThe wind energy cooperative was established in 2004 and has more than 2,000 members. The wind farm is intended to help to reduce dependence on fossil fuels whose emissions are considered to contribute to climate change. In 2007 Westmill Wind Farm Co-operative received a Schumacher Award. Westmill Wind Farm was originally developed by Adam Twine who was in the later stages assisted by Energy4All, a company founded to enable community owned renewable energy projects by Baywind Energy Co-operative.\n\nWestmill Sustainable Energy Trust (WeSET) is a charity formed in 2010 that receives 0.5% of the wind farm's revenue each year. Its objective is to encourage and promote the deployment of sustainable energy, in particular (but not exclusively) within a 25-mile radius from Westmill Wind Farm. Its website has educational material and details of how to visit Westmill Wind Farm.\n\nThe community-owned Westmill Solar Park is located on an adjoining site. Westmill Woodland Burial Ground, a natural burial site, is also close to the wind farm.\n\n"}
