{"id": "36053950", "url": "https://en.wikipedia.org/wiki?curid=36053950", "title": "1997 Guanabara Bay oil spill", "text": "1997 Guanabara Bay oil spill\n\nThe 1997 Guanabara Bay oil spill was one of three major oil spills in Guanabara Bay in Brazil. It leaked from the oil refinery at Duque de Caxias (REDUC) operated by Petrobras.\n\nPetrobras estimated that a leaking pipeline released 600,000 liters (160,000 US gal) of oil into the bay, but according to Sindipetro - the petroleum industry's union - it was 2,000,000 liters (530,000 US gal) of oil that had been spilled. The incident also had a large damaging effect on marine life in the ocean; as well as, other existing areas surrounding the bay area. Many fish suffered tragic deaths as they were washed up on the shore dead or \ncovered in oil. In addition, the fishing industry suffered a great downfall and local fishermen were unable to work or fish in the bay.\n\nFollowing the oil spill there were many negative after effects on the Brazilian fishermen, on the marine life and the environment. The fishing industry suffered tremendously as survival for the marine animals grew more difficult. Marine animals such as, the fish and crabs were unable to get oxygen and as a result were not able to survive. In addition, the aquatic plant life had also begun to die off and become non existent due to the oil resting on the surface of the water. This resulted in a loss of food for the marine animals as they had no aquatic plants to feed upon. The fishermen - who relied heavily on fishing in the bay for their own survival - were faced with tragic times, as it became more difficult to catch fish. This resulted in a downfall in the demand for fishermen and lead to a loss of employment for such individuals.\n\nThe cost of the clean-up exceeded just monetary value as it also affected the individuals that relied on the welfare of the bay. The Petrobras refinery took all responsibility and vowed to execute a cleanup process. In addition to the Guanabara Bay, the contaminated areas included the beaches, and the surrounding areas leading to the beaches such as, the pathways and tunnels. The company experienced great losses in the cleaning up process as they had to compensate for damages and were fined heavily by the Brazilian government. \n\nThe oil spill had a dangerous impact on the Brazilian society and the overall Brazilian economy. There was a drastic decrease in the number of fish species living in the bay after the spill, because the bay no longer remained a habitable environment. Almost 4000 fishermen - more than half - that had previously been employed to fish at the Guanabara Bay had to resort to alternative methods of income and working odd jobs to support their families. It was unlikely that after the spill one would be able to catch more than 10 kilos of fish, where in comparison to before the spill, the average of catching fish was almost 100 kilos. In addition, the quality of the fish from the bay area had also been an issue as many buyers claimed that the quality of the fish had decreased because it was now contaminated. As a result, the fish had been valued less and the prices had been reduced to about half. \n\n"}
{"id": "2482714", "url": "https://en.wikipedia.org/wiki?curid=2482714", "title": "2005 Java–Bali blackout", "text": "2005 Java–Bali blackout\n\nThe 2005 Java–Bali Blackout was a power outage across Java and Bali on 18 August 2005, affecting some 100 million people.\n\nPower went off at around 10:23 am (UTC+7) on 18 August 2005 across most areas of the two islands.\n\nA transmission line between Cilegon and Saguling, both in West Java, failed at 10:23 am local time; this led to a cascading failure that shut down two units of the Paiton Power Station in East Java and six units of the Suralaya Power Station in West Java \n\nPT. PLN, the state-owned electricity company, confirmed that the electricity grid failed at several points throughout Java and the neighbouring island of Bali, causing a supply shortfall of 2,700 MW, roughly half of the original supply.\n\nJakarta lost power completely, along with Banten; there were blackouts in parts of Central Java, along with parts of both West Java and East Java.\n\nDue to the sudden supply shortfall, power went out in most areas of Java, including all parts of the capital and largest city in Indonesia, Jakarta. Other major cities in Java, such as Surabaya, were also affected.\n\nPower resumed in most areas of Jakarta at about 5:00 pm (UTC+7) on the same day.\n\nPLN apologised for the incident and said about 293,235 customers will be compensated. Meanwhile, President Susilo Bambang Yudhoyono ordered police and the national intelligence agency to assist PLN to trace the cause of the blackouts.\n\n"}
{"id": "8983859", "url": "https://en.wikipedia.org/wiki?curid=8983859", "title": "Akabeko", "text": "Akabeko\n\nThe toy is made from two pieces of papier-mâché-covered wood, shaped and painted to look like a red cow or ox. One piece represents the cow's head and neck and the other its body. The head and neck hangs from a string and fits into the hollow body. When the toy is moved, the head thus bobs up and down and side to side. The earliest akabeko toys were created in the late 16th or early 17th century.\n\nOver time, people came to believe that the toys could ward off smallpox and other illnesses. Akabeko has become one of Fukushima Prefecture's most famous crafts and a symbol of the Aizu region. It has also been recognized as a symbol of the larger Tōhoku region, of which Fukushima Prefecture is a part.\n\nAccording to an Aizu-area legend recorded by Thomas Madden, akabeko toys are based upon a real cow that lived in CE 807. At that time, a monk named Tokuichi was supervising the construction of Enzō-ji, a temple in Yanaizu, Fukushima. Upon the temple's completion, the akabeko gave its spirit to Buddha, and its flesh immediately turned to stone.\n\nAnother version of the tale claims that the cow instead refused to leave the temple grounds after construction had been completed and became a permanent fixture there. The red cow was called and became a symbol of zealous devotion to the Buddha.\n\nAfter Toyotomi Hideyoshi had solidified his power over Japan, his representative, Gamō Ujisato, was sent to be the lord of the Aizu region in 1590. At his new post, Ujisato heard the story of akabeko and ordered his court artisans, who had accompanied him from Kyoto, to create a toy based on the red cow. These early papier-mâché akabeko introduced most of the basic elements for which the toy is known.\n\nIn the same period, Japan suffered a smallpox outbreak. People in Aizu noticed that children who owned akabeko toys did not seem to catch the illness. The akabeko's red colour may have enhanced this association, since red amulets are thought to protect against that illness. Akabeko toys became very popular as charms to ward off sickness, a Superstition that persists in modern times. The toy has since become one of the few crafts from Fukushima Prefecture to be known all over Japan and a symbol of the Aizu area.\n\nAkabeko are made from papier-mâché that is painted and lacquered. The toy consists of two main pieces: the body, and the head and neck. The body is hollow and open on one end. The neck and head fit into this opening, suspended from a piece of string. Whenever the akabeko is moved or jostled, its head bobs up and down and side to side.\n\nAkabeko are made by fewer than a dozen family-run workshops, whose members have passed down the technique for generations. The process takes about 10 days to complete. The artisan begins by wrapping wetted \"washi\" (Japanese paper) around two blocks of wood, one shaped like the cow's body, and the other shaped like the head and neck. These blocks have often been used for several generations. Once the paper dries, the artisan splits it in two lengthwise and removes the wooden blocks. The craftsperson then rejoins the pieces of molded paper by wrapping more layers of \"washi\" around them.\n\nThe artisan paints the toy, beginning with black, then adding the characteristic red, and finally the white eyes and other details. The akabeko's markings vary from workshop to workshop. For example, the Igarashi family paints the Chinese character \"kotobuki\" (寿 \"longevity\" and \"luck\") on the cow's back and a sun and moon on its side; other workshops add gold markings. The artisan finishes the toy with a thin coat of lacquer.\n\n・Okiagari-koboshi\n\n・Aizuhongo ware\n\n・Ujisato Gamo(蒲生氏郷)\n\n・Akabeko Doll\n"}
{"id": "904", "url": "https://en.wikipedia.org/wiki?curid=904", "title": "Aluminium", "text": "Aluminium\n\nAluminium or aluminum is a chemical element with symbol Al and atomic number 13. It is a silvery-white, soft, nonmagnetic and ductile metal in the boron group. By mass, aluminium makes up about 8% of the Earth's crust; it is the third most abundant element after oxygen and silicon and the most abundant metal in the crust, though it is less common in the mantle below. The chief ore of aluminium is bauxite. Aluminium metal is so chemically reactive that native specimens are rare and limited to extreme reducing environments. Instead, it is found combined in over 270 different minerals.\n\nAluminium is remarkable for its low density and its ability to resist corrosion through the phenomenon of passivation. Aluminium and its alloys are vital to the aerospace industry and important in transportation and building industries, such as building facades and window frames. The oxides and sulfates are the most useful compounds of aluminium.\n\nDespite its prevalence in the environment, no known form of life uses aluminium salts metabolically, but aluminium is well tolerated by plants and animals. Because of these salts' abundance, the potential for a biological role for them is of continuing interest, and studies continue.\n\nAluminium's atomic number is 13. Of aluminium isotopes, only one is stable: Al. This is consistent with the fact aluminium's atomic number is odd. It is the only isotope that has existed on Earth in its current form since the creation of the planet. It is essentially the only isotope representing the element on Earth, which makes aluminium a mononuclidic element and practically equates its standard atomic weight to that of the isotope. Such a low standard atomic weight of aluminium has some effects on the properties of the element (see below).\n\nAll other isotopes are radioactive and could not have survived; the most stable isotope of these is Al (half-life 720,000 years). Al is produced from argon in the atmosphere by spallation caused by cosmic ray protons and used in radiodating. The ratio of Al to Be has been used to study transport, deposition, sediment storage, burial times, and erosion on 10 to 10 year time scales. Most meteorite scientists believe that the energy released by the decay of Al was responsible for the melting and differentiation of some asteroids after their formation 4.55 billion years ago.\n\nThe remaining isotopes of aluminium, with mass numbers ranging from 21 to 43, all have half-lives well under an hour. Three metastable states are known, all with half-lives under a minute.\n\nAn aluminium atom has 13 electrons, arranged in an electron configuration of [Ne]3s3p, with three electrons beyond a stable noble gas configuration. Accordingly, the combined first three ionization energies of aluminium are far lower than the fourth ionization energy alone. Aluminium can relatively easily surrender its three outermost electrons in many chemical reactions (see below). The electronegativity of aluminium is 1.61 (Pauling scale).\n\nA free aluminium atom has a radius of 143 pm. With the three outermost electrons removed, the radius shrinks to 39 pm for a 4-coordinated atom or 53.5 pm for a 6-coordinated atom. At standard temperature and pressure, aluminium atoms (when not affected by atoms of other elements) form a face-centered cubic crystal system bound by metallic bonding provided by atoms' outermost electrons; hence aluminium (at these conditions) is a metal. This crystal system is shared by some other metals, such as lead and copper; the size of a unit cell of aluminium is comparable to that of those other metals.\n\nAluminium metal, when in quantity, is very shiny and resembles silver because it preferentially absorbs far ultraviolet radiation while reflecting all visible light so it does not impart any color to reflected light, unlike the reflectance spectra of copper and gold. Another important characteristic of aluminium is its low density, 2.70 g/cm. Aluminium is a relatively soft, durable, lightweight, ductile, and malleable with appearance ranging from silvery to dull gray, depending on the surface roughness. It is nonmagnetic and does not easily ignite. A fresh film of aluminium serves as a good reflector (approximately 92%) of visible light and an excellent reflector (as much as 98%) of medium and far infrared radiation. The yield strength of pure aluminium is 7–11 MPa, while aluminium alloys have yield strengths ranging from 200 MPa to 600 MPa. Aluminium has about one-third the density and stiffness of steel. It is easily machined, cast, drawn and extruded.\n\nAluminium atoms are arranged in a face-centered cubic (fcc) structure. Aluminium has a stacking-fault energy of approximately 200 mJ/m.\n\nAluminium is a good thermal and electrical conductor, having 59% the conductivity of copper, both thermal and electrical, while having only 30% of copper's density. Aluminium is capable of superconductivity, with a superconducting critical temperature of 1.2 kelvin and a critical magnetic field of about 100 gauss (10 milliteslas).\nAluminium is the most common material for the fabrication of superconducting qubits.\n\nAluminium's corrosion resistance can be excellent due to a thin surface layer of aluminium oxide that forms when the bare metal is exposed to air, effectively preventing further oxidation, in a process termed passivation. The strongest aluminium alloys are less corrosion resistant due to galvanic reactions with alloyed copper. This corrosion resistance is greatly reduced by aqueous salts, particularly in the presence of dissimilar metals.\n\nIn highly acidic solutions, aluminium reacts with water to form hydrogen, and in highly alkaline ones to form aluminates— protective passivation under these conditions is negligible. Primarily because it is corroded by dissolved chlorides, such as common sodium chloride, household plumbing is never made from aluminium.\n\nHowever, because of its general resistance to corrosion, aluminium is one of the few metals that retains silvery reflectance in finely powdered form, making it an important component of silver-colored paints. Aluminium mirror finish has the highest reflectance of any metal in the 200–400 nm (UV) and the 3,000–10,000 nm (far IR) regions; in the 400–700 nm visible range it is slightly outperformed by tin and silver and in the 700–3000 nm (near IR) by silver, gold, and copper.\n\nAluminium is oxidized by water at temperatures below 280 °C to produce hydrogen, aluminium hydroxide and heat:\nThis conversion is of interest for the production of hydrogen. However, commercial application of this fact has challenges in circumventing the passivating oxide layer, which inhibits the reaction, and in storing the energy required to regenerate the aluminium metal.\n\nThe vast majority of compounds, including all Al-containing minerals and all commercially significant aluminium compounds, feature aluminium in the oxidation state 3+. The coordination number of such compounds varies, but generally Al is six-coordinate or tetracoordinate. Almost all compounds of aluminium(III) are colorless.\n\nAll four trihalides are well known. Unlike the structures of the three heavier trihalides, aluminium fluoride (AlF) features six-coordinate Al. The octahedral coordination environment for AlF is related to the compactness of the fluoride ion, six of which can fit around the small Al center. AlF sublimes (with cracking) at . With heavier halides, the coordination numbers are lower. The other trihalides are dimeric or polymeric with tetrahedral Al centers. These materials are prepared by treating aluminium metal with the halogen, although other methods exist. Acidification of the oxides or hydroxides affords hydrates. In aqueous solution, the halides often form mixtures, generally containing six-coordinate Al centers that feature both halide and aquo ligands. When aluminium and fluoride are together in aqueous solution, they readily form complex ions such as , , and . In the case of chloride, polyaluminium clusters are formed such as [AlO(OH)(HO)].\n\nAluminium forms one stable oxide with the chemical formula AlO It can be found in nature in the mineral corundum. Aluminium oxide is also commonly called alumina. Sapphire and ruby are impure corundum contaminated with trace amounts of other metals. The two oxide-hydroxides, AlO(OH), are boehmite and diaspore. There are three trihydroxides: bayerite, gibbsite, and nordstrandite, which differ in their crystalline structure (polymorphs). Most are produced from ores by a variety of wet processes using acid and base. Heating the hydroxides leads to formation of corundum. These materials are of central importance to the production of aluminium and are themselves extremely useful.\n\nAluminium carbide (AlC) is made by heating a mixture of the elements above . The pale yellow crystals consist of tetrahedral aluminium centers. It reacts with water or dilute acids to give methane. The acetylide, Al(C), is made by passing acetylene over heated aluminium.\n\nAluminium nitride (AlN) is the only nitride known for aluminium. Unlike the oxides, it features tetrahedral Al centers. It can be made from the elements at . It is air-stable material with a usefully high thermal conductivity. Aluminium phosphide (AlP) is made similarly; it hydrolyses to give phosphine:\n\nAlthough the great majority of aluminium compounds feature Al centers, compounds with lower oxidation states are known and sometime of significance as precursors to the Al species.\n\nAlF, AlCl and AlBr exist in the gaseous phase when the trihalide is heated with aluminium. The composition is unstable at room temperature, converting to triiodide:\n\nA stable derivative of aluminium monoiodide is the cyclic adduct formed with triethylamine, . Also of theoretical interest but only of fleeting existence are AlO and AlS. AlO is made by heating the normal oxide, AlO, with silicon at in a vacuum. Such materials quickly disproportionate to the starting materials.\n\nVery simple Al(II) compounds are invoked or observed in the reactions of Al metal with oxidants. For example, aluminium monoxide, AlO, has been detected in the gas phase after explosion and in stellar absorption spectra. More thoroughly investigated are compounds of the formula RAl which contain an Al-Al bond and where R is a large organic ligand.\n\nA variety of compounds of empirical formula AlR and AlRCl exist. These species usually feature tetrahedral Al centers formed by dimerization with some R or Cl bridging between both Al atoms, e.g. \"trimethylaluminium\" has the formula Al(CH) (see figure). With large organic groups, triorganoaluminium compounds exist as three-coordinate monomers, such as triisobutylaluminium. Such compounds are widely used in industrial chemistry, despite the fact that they are often highly pyrophoric. Few analogues exist between organoaluminium and organoboron compounds other than large organic groups.\n\nThe industrially important aluminium hydride is lithium aluminium hydride (LiAlH), which is used in as a reducing agent in organic chemistry. It can be produced from lithium hydride and aluminium trichloride:\n\nSeveral useful derivatives of LiAlH are known, e.g. sodium bis(2-methoxyethoxy)dihydridoaluminate. The simplest hydride, aluminium hydride or alane, remains a laboratory curiosity. It is a polymer with the formula (AlH), in contrast to the corresponding boron hydride that is a dimer with the formula (BH).\n\nAluminium's per-particle abundance in the Solar System is 3.15 ppm (parts per million). It is the twelfth most abundant of all elements and third most abundant among the elements that have odd atomic numbers, after hydrogen and nitrogen. The only stable isotope of aluminium, Al, is the eighteenth most abundant nucleus in the Universe. It is created almost entirely after fusion of carbon in massive stars that will later become Type II supernovae: this fusion creates Mg, which, upon capturing free protons and neutrons becomes aluminium. Some smaller quantities of Al are created in hydrogen burning shells of evolved stars, where Mg can capture free protons. Essentially all aluminium now in existence is Al; Al was present in the early Solar System but is currently extinct. However, the trace quantities of Al that do exist are the most common gamma ray emitter in the interstellar gas.\n\nOverall, the Earth is about 1.59% aluminium by mass (seventh in abundance by mass). Aluminium occurs in greater proportion in the Earth than in the Universe because aluminium easily forms the oxide and becomes bound into rocks and aluminium stays in the Earth's crust while less reactive metals sink to the core. In the Earth's crust, aluminium is the most abundant (8.3% by mass) metallic element and the third most abundant of all elements (after oxygen and silicon). A large number of silicates in the Earth's crust contain aluminium. In contrast, the Earth's mantle is only 2.38% aluminium by mass.\n\nBecause of its strong affinity for oxygen, aluminium is almost never found in the elemental state; instead it is found in oxides or silicates. Feldspars, the most common group of minerals in the Earth's crust, are aluminosilicates. Aluminium also occurs in the minerals beryl, cryolite, garnet, spinel, and turquoise. Impurities in AlO, such as chromium and iron, yield the gemstones ruby and sapphire, respectively. Native aluminium metal can only be found as a minor phase in low oxygen fugacity environments, such as the interiors of certain volcanoes. Native aluminium has been reported in cold seeps in the northeastern continental slope of the South China Sea. It is possible that these deposits resulted from bacterial reduction of tetrahydroxoaluminate Al(OH).\n\nAlthough aluminium is a common and widespread element, not all aluminium minerals are economically viable sources of the metal. Almost all metallic aluminium is produced from the ore bauxite (AlO(OH)). Bauxite occurs as a weathering product of low iron and silica bedrock in tropical climatic conditions. In 2017, most bauxite was mined in Australia, China, Guinea, and India.\n\nThe history of aluminium has been shaped by usage of alum. The first written record of alum, made by Greek historian Herodotus, dates back to the 5th century BCE. The ancients are known to have used alum as a dyeing mordant and for city defense. After the Crusades, alum, an indispensable good in the European fabric industry, was a subject of international commerce; it was imported to Europe from the eastern Mediterranean until the mid-15th century.\n\nThe nature of alum remained unknown. Around 1530, Swiss physician Paracelsus suggested alum was a salt of an earth of alum. In 1595, German doctor and chemist Andreas Libavius experimentally confirmed this; In 1722, German chemist Friedrich Hoffmann announced his belief that the base of alum was a distinct earth. In 1754, German chemist Andreas Sigismund Marggraf synthesized alumina by boiling clay in sulfuric acid and subsequently adding potash.\n\nAttempts to produce aluminium metal date back to 1760. The first successful attempt, however, was completed in 1824 by Danish physicist and chemist Hans Christian Ørsted. He reacted anhydrous aluminium chloride with potassium amalgam, yielding a lump of metal looking similar to tin. He presented his results and demonstrated a sample of the new metal in 1825. In 1827, German chemist Friedrich Wöhler repeated Ørsted's experiments but did not identify any aluminium. (The reason for this inconsistency was only discovered in 1921.) He conducted a similar experiment in 1827 by mixing anhydrous aluminium chloride with potassium and produced a powder of aluminium. In 1845, he was able to produce small pieces of the metal and described some physical properties of this metal. For many years thereafter, Wöhler was credited as the discoverer of aluminium. As Wöhler's method could not yield great quantities of aluminium, the metal remained rare; its cost exceeded that of gold.\n\nFrench chemist Henri Etienne Sainte-Claire Deville announced an industrial method of aluminium production in 1854 at the Paris Academy of Sciences. Aluminium trichloride could be reduced by sodium, which was more convenient and less expensive than potassium, which Wöhler had used. In 1856, Deville along with companions established the world's first industrial production of aluminium. From 1855 to 1859, the price of aluminium dropped by an order of magnitude, from US$500 to $40 per pound. Even then, aluminium was still not of great purity and produced aluminium differed in properties by sample.\n\nThe first industrial large-scale production method was independently developed in 1886 by French engineer Paul Héroult and American engineer Charles Martin Hall; it is now known as the Hall–Héroult process. The Hall–Héroult process converts alumina into the metal. Austrian chemist Carl Joseph Bayer discovered a way of purifying bauxite to yield alumina, now known as the Bayer process, in 1889. Modern production of the aluminium metal is based on the Bayer and Hall–Héroult processes.\n\nPrices of aluminium dropped and aluminium became widely used in jewelry, everyday items, eyeglass frames, optical instruments, tableware, and foil in the 1890s and early 20th century. Aluminium's ability to form hard yet light alloys with other metals provided the metal many uses at the time. During World War I, major governments demanded large shipments of aluminium for light strong airframes.\n\nBy the mid-20th century, aluminium had become a part of everyday life and an essential component of housewares. During the mid-20th century, aluminium emerged as a civil engineering material, with building applications in both basic construction and interior finish work, and increasingly being used in military engineering, for both airplanes and land armor vehicle engines. Earth's first artificial satellite, launched in 1957, consisted of two separate aluminium semi-spheres joined together and all subsequent space vehicles have been made of aluminium. The aluminium can was invented in 1956 and employed as a storage for drinks in 1958.\n\nThroughout the 20th century, the production of aluminium rose rapidly: while the world production of aluminium in 1900 was 6,800 metric tons, the annual production first exceeded 100,000 metric tons in 1916; 1,000,000 tons in 1941; 10,000,000 tons in 1971. In the 1970s, the increased demand for aluminium made it an exchange commodity; it entered the London Metal Exchange, the oldest industrial metal exchange in the world, in 1978. The output continued to grow: the annual production of aluminium exceeded 50,000,000 metric tons in 2013.\n\nThe real price for aluminium declined from $14,000 per metric ton in 1900 to $2,340 in 1948 (in 1998 United States dollars). Extraction and processing costs were lowered over technological progress and the scale of the economies. However, the need to exploit lower-grade poorer quality deposits and the use of fast increasing input costs (above all, energy) increased the net cost of aluminium; the real price began to grow in the 1970s with the rise of energy cost. Production moved from the industrialized countries to countries where production was cheaper. Production costs in the late 20th century changed because of advances in technology, lower energy prices, exchange rates of the United States dollar, and alumina prices. The BRIC countries' combined share grew in the first decade of the 21st century from 32.6% to 56.5% in primary production and 21.4% to 47.8% in primary consumption. China is accumulating an especially large share of world's production thanks to abundance of resources, cheap energy, and governmental stimuli; it also increased its consumption share from 2% in 1972 to 40% in 2010. In the United States, Western Europe, and Japan, most aluminium was consumed in transportation, engineering, construction, and packaging. \n\nAluminium is named after alumina, or aluminium oxide in modern nomenclature. The word \"alumina\" comes from \"alum\", the mineral from which it was collected. The word \"alum\" comes from \"alumen\", a Latin word meaning \"bitter salt\". The word \"alumen\" stems from the Proto-Indo-European root \"*alu-\" meaning \"bitter\" or \"beer\".\n\nBritish chemist Humphry Davy, who performed a number of experiments aimed to synthesize the metal, is credited as the person who named the element. In 1808, he suggested the metal be named \"alumium\". This suggestion was criticized by contemporary chemists from France, Germany, and Sweden, who insisted the metal should be named for the oxide, alumina, from which it would be isolated. In 1812, Davy chose \"aluminum\", thus producing the modern name. However, it is spelled and pronounced differently outside of North America: \"aluminum\" is in use in the U.S. and Canada while \"aluminium\" is in use elsewhere.\n\nThe ' suffix followed the precedent set in other newly discovered elements of the time: potassium, sodium, magnesium, calcium, and strontium (all of which Davy isolated himself). Nevertheless, element names ending in ' were known at the time; for example, platinum (known to Europeans since the 16th century), molybdenum (discovered in 1778), and tantalum (discovered in 1802). The \"\" suffix is consistent with the universal spelling alumina for the oxide (as opposed to aluminia); compare to lanthana, the oxide of lanthanum, and magnesia, ceria, and thoria, the oxides of magnesium, cerium, and thorium, respectively.\n\nIn 1812, British scientist Thomas Young wrote an anonymous review of Davy's book, in which he objected to \"aluminum\" and proposed the name \"aluminium\": \"for so we shall take the liberty of writing the word, in preference to aluminum, which has a less classical sound.\" This name did catch on: while the ' spelling was occasionally used in Britain, the American scientific language used ' from the start. Most scientists used ' throughout the world in the 19th century; it still remains the standard in most other languages. In 1828, American lexicographer Noah Webster used exclusively the \"aluminum\" spelling in his \"American Dictionary of the English Language\". In the 1830s, the ' spelling started to gain usage in the United States; by the 1860s, it had become the more common spelling there outside science. In 1892, Hall used the ' spelling in his advertising handbill for his new electrolytic method of producing the metal, despite his constant use of the ' spelling in all the patents he filed between 1886 and 1903. It was subsequently suggested this was a typo rather than intended. By 1890, both spellings had been common in the U.S. overall, the ' spelling being slightly more common; by 1895, the situation had reversed; by 1900, \"aluminum\" had been twice as common as \"aluminium\"; during the following decade, the ' spelling dominated American usage. In 1925, the American Chemical Society adopted this spelling.\n\nThe International Union of Pure and Applied Chemistry (IUPAC) adopted \"aluminium\" as the standard international name for the element in 1990. In 1993, they recognized \"aluminum\" as an acceptable variant; the same is true for the most recent 2005 edition of the IUPAC nomenclature of inorganic chemistry. IUPAC official publications use the \"\" spelling as primary but list both where appropriate. English Wikipedia follows this standard by \"aluminium\" as the sole spelling in chemistry-related articles.\n\nAluminium production is highly energy-consuming, and so the producers tend to locate smelters in places where electric power is both plentiful and inexpensive. As of 2012, the world's largest smelters of aluminium are located in China, Russia, Bahrain, United Arab Emirates, and South Africa.\n\nIn 2016, China was the top producer of aluminium with a world share of fifty-five percent; the next largest producing countries were Russia, Canada, India, and the United Arab Emirates.\n\nAccording to the International Resource Panel's Metal Stocks in Society report, the global per capita stock of aluminium in use in society (i.e. in cars, buildings, electronics etc.) is . Much of this is in more-developed countries ( per capita) rather than less-developed countries ( per capita).\n\nBauxite is converted to aluminium oxide by the Bayer process. Bauxite is blended for uniform composition and then is ground. The resulting slurry is mixed with a hot solution of sodium hydroxide; the mixture is then treated in a digester vessel at a pressure well above atmospheric, dissolving the aluminium hydroxide in bauxite while converting impurities into a relatively insoluble compounds:\n\nAfter this reaction, the slurry is at a temperature above its atmospheric boiling point. It is cooled by removing steam as pressure is reduced. The bauxite residue is separated from the solution and discarded. The solution, free of solids, is seeded with small crystals of aluminium hydroxide; this causes decomposition of the [Al(OH)] ions to aluminium hydroxide. After about half of aluminium has precipitated, the mixture is sent to classifiers. Small crystals of aluminium hydroxide are collected to serve as seeding agents; coarse particles are reduced to aluminium oxide; excess solution is removed by evaporation, (if needed) purified, and recycled.\n\nThe conversion of alumina to aluminium metal is achieved by the Hall–Héroult process. In this energy-intensive process, a solution of alumina in a molten () mixture of cryolite (NaAlF) with calcium fluoride is electrolyzed to produce metallic aluminium. The liquid aluminium metal sinks to the bottom of the solution and is tapped off, and usually cast into large blocks called aluminium billets for further processing.\n\nAnodes of the electrolysis cell are made of carbon—the most resistant material against fluoride corrosion—and either bake at the process or are prebaked. The former, also called Söderberg anodes, are less power-efficient and fumes released during baking are costly to collect, which is why they are being replaced by prebaked anodes even though they save the power, energy, and labor to prebake the cathodes. Carbon for anodes should be preferably pure so that neither aluminium nor the electrolyte is contaminated with ash. Despite carbon's resistivity against corrosion, it is still consumed at a rate of 0.4–0.5 kg per each kilogram of produced aluminium. Cathodes are made of anthracite; high purity for them is not required because impurities leach only very slowly. Cathode is consumed at a rate of 0.02–0.04 kg per each kilogram of produced aluminium. A cell is usually a terminated after 2–6 years following a failure of the cathode.\n\nThe Hall–Heroult process produces aluminium with a purity of above 99%. Further purification can be done by the Hoopes process. This process involves the electrolysis of molten aluminium with a sodium, barium, and aluminium fluoride electrolyte. The resulting aluminium has a purity of 99.99%.\n\nElectric power represents about 20 to 40% of the cost of producing aluminium, depending on the location of the smelter. Aluminium production consumes roughly 5% of electricity generated in the United States. Because of this, alternatives to the Hall–Héroult process have been researched, but none has turned out to be economically feasible.\n\nRecovery of the metal through recycling has become an important task of the aluminium industry. Recycling was a low-profile activity until the late 1960s, when the growing use of aluminium beverage cans brought it to public awareness. Recycling involves melting the scrap, a process that requires only 5% of the energy used to produce aluminium from ore, though a significant part (up to 15% of the input material) is lost as dross (ash-like oxide). An aluminium stack melter produces significantly less dross, with values reported below 1%.\n\nWhite dross from primary aluminium production and from secondary recycling operations still contains useful quantities of aluminium that can be extracted industrially. The process produces aluminium billets, together with a highly complex waste material. This waste is difficult to manage. It reacts with water, releasing a mixture of gases (including, among others, hydrogen, acetylene, and ammonia), which spontaneously ignites on contact with air; contact with damp air results in the release of copious quantities of ammonia gas. Despite these difficulties, the waste is used as a filler in asphalt and concrete.\n\nAluminium is the most widely used non-ferrous metal. The global production of aluminium in 2016 was 58.8 million metric tons. It exceeded that of any other metal except iron (1,231 million metric tons).\n\nAluminium is almost always alloyed, which markedly improves its mechanical properties, especially when tempered. For example, the common aluminium foils and beverage cans are alloys of 92% to 99% aluminium. The main alloying agents are copper, zinc, magnesium, manganese, and silicon (e.g., duralumin) with the levels of other metals in a few percent by weight.\n\nThe major uses for aluminium metal are in:\n\n\nThe great majority (about 90%) of aluminium oxide is converted to metallic aluminium. Being a very hard material (Mohs hardness 9), alumina is widely used as an abrasive; being extraordinarily chemically inert, it is useful in highly reactive environments such as high pressure sodium lamps. Aluminium oxide is commonly used as a catalyst for industrial processes; e.g. the Claus process to convert hydrogen sulfide to sulfur in refineries and to alkylate amines. Many industrial catalysts are supported by alumina, meaning that the expensive catalyst material is dispersed over a surface of the inert alumina. Another principal use is as a drying agent or absorbent.\n\nSeveral sulfates of aluminium have industrial and commercial application. Aluminium sulfate (in its hydrate form) is produced on the annual scale of several millions of metric tons. About two thirds is consumed in water treatment. The next major application is in the manufacture of paper. It is also used as a mordant in dyeing, in pickling seeds, deodorizing of mineral oils, in leather tanning, and in production of other aluminium compounds. Two kinds of alum, ammonium alum and potassium alum, were formerly used as mordants and in leather tanning, but their use has significantly declined following availability of high-purity aluminium sulfate. Anhydrous aluminium chloride is used as a catalyst in chemical and petrochemical industries, the dyeing industry, and in synthesis of various inorganic and organic compounds. Aluminium hydroxychlorides are used in purifying water, in the paper industry, and as antiperspirants. Sodium aluminate is used in treating water and as an accelerator of solidification of cement.\n\nMany aluminium compounds have niche applications, for example:\n\nDespite its widespread occurrence in the Earth crust, aluminium has no known function in biology. Aluminium salts are remarkably nontoxic, aluminium sulfate having an LD of 6207 mg/kg (oral, mouse), which corresponds to 500 grams for an person.\n\nIn most people, aluminium is not as toxic as heavy metals. Aluminium is classified as a non-carcinogen by the United States Department of Health and Human Services. There is little evidence that normal exposure to aluminium presents a risk to healthy adult, and there is evidence of no toxicity if it is consumed in amounts not greater than 40 mg/day per kg of body mass. Most aluminium consumed will leave the body in feces; the small part of it that enters the body, will be excreted via urine. Aluminium that does stay in the body is accumulated in, above all, bone; and apart from that, in brain, liver, and kidney. Aluminium metal cannot pass the blood–brain barrier and natural filters before the brain, but some compounds, such as the fluoride, can.\n\nAluminium, although rarely, can cause vitamin D-resistant osteomalacia, erythropoietin-resistant microcytic anemia, and central nervous system alterations. People with kidney insufficiency are especially at a risk. Chronic ingestion of hydrated aluminium silicates (for excess gastric acidity control) may result in aluminium binding to intestinal contents and increased elimination of other metals, such as iron or zinc; sufficiently high doses (>50 g/day) can cause anemia. Since aluminium is excreted by kidneys, their function may be impaired by toxic amounts of aluminium.\n\nAn accident in England revealed that millimolar quantities of aluminium in drinking water cause significant cognitive deficits. Orally ingested aluminium salts can deposit in the brain. There is research on correlation between neurological disorders, including Alzheimer's disease, and aluminium levels, but it has been inconclusive so far.\n\nAluminium increases estrogen-related gene expression in human breast cancer cells cultured in the laboratory. In very high doses, aluminium is associated with altered function of the blood–brain barrier. A small percentage of people have contact allergies to aluminium and experience itchy red rashes, headache, muscle pain, joint pain, poor memory, insomnia, depression, asthma, irritable bowel syndrome, or other symptoms upon contact with products containing aluminium.\n\nExposure to powdered aluminium or aluminium welding fumes can cause pulmonary fibrosis. Fine aluminium powder can ignite or explode, posing another workplace hazard.\n\nFood is the main source of aluminium. Drinking water contains more aluminium than solid food; however, aluminium in food may be absorbed more than aluminium from water. Major sources of human oral exposure to aluminium include food (due to its use in food additives, food and beverage packaging, and cooking utensils), drinking water (due to its use in municipal water treatment), and aluminium-containing medications (particularly antacid/antiulcer and buffered aspirin formulations). Dietary exposure in Europeans averages to 0.2–1.5 mg/kg/week but can be as high as 2.3 mg/kg/week. Higher exposure levels of aluminium are mostly limited to miners, aluminium production workers, and dialysis patients.\n\nExcessive consumption of antacids, antiperspirants, vaccines, and cosmetics provide significant exposure levels. Consumption of acidic foods or liquids with aluminium enhances aluminium absorption, and maltol has been shown to increase the accumulation of aluminium in nerve and bone tissues.\n\nIn case of suspected sudden intake of a large amount of aluminium, the only treatment is deferoxamine mesylate which may be given to help eliminate aluminium from the body by chelation. However, this should be applied with caution as this reduces not only aluminium body levels, but also those of other metals such as copper or iron. Nutritionally, treatment of similar to those of other toxic metals and includes removal of sources of aluminium from environment, enhancing cellular energy production, enhancing activity of the eliminative organs, and chelating aluminium with nutrients.\n\nHigh levels of aluminium occur near mining sites; small amounts of aluminium are released to the environment at the coal-fired power plants or incinerators. Aluminium in the air is washed out by the rain or normally settles down but small particles of aluminium remain in the air for a long time.\n\nAcidic precipitation is the main natural factor to mobilize aluminium from natural sources and the main reason for the environmental effects of aluminium; however, the main factor of presence of aluminium in salt and freshwater are the industrial processes that also release aluminium into air.\n\nIn water, aluminium acts as a toxiс agent on gill-breathing animals such as fish by causing loss of plasma- and hemolymph ions leading to osmoregulatory failure. Organic complexes of aluminium may be easily absorbed and interfere with metabolism in mammals and birds, even though this rarely happens in practice.\n\nAluminium is primary among the factors that reduce plant growth on acidic soils. Although it is generally harmless to plant growth in pH-neutral soils, in acid soils the concentration of toxic Al cations increases and disturbs root growth and function. Wheat has developed a tolerance to aluminium, releasing organic compounds that bind to harmful aluminium cations. Sorghum is believed to have the same tolerance mechanism.\n\nAluminium production possesses its own challenges to the environment on each step of the production process. The major challenge is the greenhouse gas emissions. These gases result from electrical consumption of the smelters and the byproducts of processing. The most potent of these gases are perfluorocarbons from the smelting process. Released sulfur dioxide is one of the primary precursors of acid rain.\n\nA Spanish scientific report from 2001 claimed that the fungus \"Geotrichum candidum\" consumes the aluminium in compact discs. Other reports all refer back to that report and there is no supporting original research. Better documented, the bacterium \"Pseudomonas aeruginosa\" and the fungus \"Cladosporium resinae\" are commonly detected in aircraft fuel tanks that use kerosene-based fuels (not avgas), and laboratory cultures can degrade aluminium. However, these life forms do not directly attack or consume the aluminium; rather, the metal is corroded by microbe waste products.\n\n\n\n\n"}
{"id": "11793400", "url": "https://en.wikipedia.org/wiki?curid=11793400", "title": "Archie Robertson (footballer)", "text": "Archie Robertson (footballer)\n\nArchibald Clark Robertson (15 September 1929 – 28 January 1978) was a Scottish footballer who spent most of his career with Clyde, firstly as an inside right and latterly as manager.\n\nRobertson joined Clyde from Junior side Rutherglen Glencairn in 1947, on a part-time basis as he continued his studies towards a degree in chemistry. He spent the next 14 seasons with the \"Bully Wee\", during which time he experienced relegation on three occasions, although he also helped the side win the Division Two title twice, in 1951-52 and 1956-57.\n\nRobertson also enjoyed success with Clyde in the Scottish Cup. In the 1955 final he scored Clyde's equalising goal direct from a corner kick in the 88th minute, forcing a 1-1 draw with Celtic. Clyde went on to win the replay 1-0 with a goal by Tommy Ring. He also played in the 1958 Cup final when Clyde defeated Hibernian 1-0.\n\nHe eventually left the club when signed by Morton for £1,000 in the autumn of his career. A teammate was Allan McGraw who stated that he \"Learnt a lot\" from Robertson. He spent two seasons in Greenock before retiring in 1963.\n\nWhile with Clyde, Robertson earned 5 caps for the Scotland national football team and played in the 1958 FIFA World Cup in Sweden. He also earned selection for the Scottish League representative side on two occasions.\n\nRobertson began a managerial career with Second Division Cowdenbeath in 1964 and spent three and a half mid-table seasons with the \"Blue Brazil\". In January 1968 he succeeded Davie White as manager of old club Clyde but was not able to replicate White's 1966-67 third-place finish as Clyde's league form gradually declined in the late 1960s and early 1970s. They eventually experienced relegation in the 1971-72 season and despite leading his team to the Second Division title the following campaign, Robertson ended his association with the club in 1973. His final footballing role was as Tottenham Hotspur's Scottish scout.\n\nThroughout his career, Robertson combined his footballing role with another career. Following his graduation he worked in the scientific branch of the National Coal Board, while in later years he was a science teacher at Hunter High School in East Kilbride. One of his pupils was future Scotland striker Ally McCoist who credited the guidance of Robertson (in his role as the school team's coach) in his early development as a player.\n\n"}
{"id": "31068343", "url": "https://en.wikipedia.org/wiki?curid=31068343", "title": "Atomic Energy Authority Act", "text": "Atomic Energy Authority Act\n\nAtomic Energy Authority Act (with its variations) is a stock short title used for legislation in the United Kingdom relating to the United Kingdom Atomic Energy Authority.\n\nThe Bill for an Act with this short title will have been known as a Atomic Energy Authority Bill during its passage through Parliament.\n\n\n"}
{"id": "12026255", "url": "https://en.wikipedia.org/wiki?curid=12026255", "title": "Bindoo wind farm", "text": "Bindoo wind farm\n\nBindoo wind farm is a wind farm located close to Cootehill, Cavan, Ireland and erected in 2007. The farm was built and is run by Airtricity. The wind farm has 32 GE 1.5 MW Wind Turbines with a 65-metre hub height and 70 metre rotor diameter giving a total capacity of 48 megawatts. As of June 2007 it is the second largest wind farm (by power generated) in Ireland.\n\n"}
{"id": "37188422", "url": "https://en.wikipedia.org/wiki?curid=37188422", "title": "Brewster angle microscope", "text": "Brewster angle microscope\n\nA Brewster angle microscope (BAM) is a microscope for studying thin films on liquid surfaces, most typically Langmuir films. In a Brewster angle microscope, both the microscope and a polarized light source are aimed towards a liquid surface at that liquid's Brewster angle, in such a way for the microscope to catch an image of any light reflected from the light source via the liquid surface. Because there is no \"p\"-polarized reflection from the pure liquid when both are angled towards it at the Brewster angle, light is only reflected when some other phenomenon such as a surface film affects the liquid surface. The technique was first introduced in 1991.\n\nBrewster angle microscopes enable the visualization of Langmuir monolayers or adsorbate films at the air-water interface for example as a function of packing density. They can be used either to study the properties of the Langmuir layer, or to indicate a suitable deposition pressure for Langmuir-Blodgett (LB) deposition. They can be used for example in the LB deposition of nanoparticles. Applications include:\n\nMonolayer/film homogeneity. When combined with a Langmuir-Blodgett Trough, observation can be performed during compression/expansion at known surface pressures.\n\nOptimizing the deposition parameters. Selecting optimal deposition pressure and other deposition parameters for LB coating.\n\nMonolayer/film behavior. Observing phase changes, phase separation, domain size, shape and packing.\n\nMonitoring of surface reactions. Photochemical reactions, polymerizations and enzyme kinetics can be followed in real time.\n\nMonitoring and detection of surface active materials. For example protein adsorption and nanoparticle flotation. \n\nLee et al. used a Brewster angle microscope to study optimal deposition parameters for FeO nanoparticles.\n\nDaear et al. have written a recent review on the usage of BAMs in biological applications.\n\n\nBrewster Angle Microscopy"}
{"id": "5672", "url": "https://en.wikipedia.org/wiki?curid=5672", "title": "Cadmium", "text": "Cadmium\n\nCadmium is a chemical element with symbol Cd and atomic number 48. This soft, bluish-white metal is chemically similar to the two other stable metals in group 12, zinc and mercury. Like zinc, it demonstrates oxidation state +2 in most of its compounds, and like mercury, it has a lower melting point than the transition metals in groups 3 through 11. Cadmium and its congeners in group 12 are often not considered transition metals, in that they do not have partly filled \"d\" or \"f\" electron shells in the elemental or common oxidation states. The average concentration of cadmium in Earth's crust is between 0.1 and 0.5 parts per million (ppm). It was discovered in 1817 simultaneously by Stromeyer and Hermann, both in Germany, as an impurity in zinc carbonate.\n\nCadmium occurs as a minor component in most zinc ores and is a byproduct of zinc production. Cadmium was used for a long time as a corrosion-resistant plating on steel, and cadmium compounds are used as red, orange and yellow pigments, to color glass, and to stabilize plastic. Cadmium use is generally decreasing because it is toxic (it is specifically listed in the European Restriction of Hazardous Substances) and nickel-cadmium batteries have been replaced with nickel-metal hydride and lithium-ion batteries. One of its few new uses is cadmium telluride solar panels.\n\nAlthough cadmium has no known biological function in higher organisms, a cadmium-dependent carbonic anhydrase has been found in marine diatoms.\n\nCadmium is a soft, malleable, ductile, bluish-white divalent metal. It is similar in many respects to zinc but forms complex compounds. Unlike most other metals, cadmium is resistant to corrosion and is used as a protective plate on other metals. As a bulk metal, cadmium is insoluble in water and is not flammable; however, in its powdered form it may burn and release toxic fumes.\n\nAlthough cadmium usually has an oxidation state of +2, it also exists in the +1 state. Cadmium and its congeners are not always considered transition metals, in that they do not have partly filled d or f electron shells in the elemental or common oxidation states. Cadmium burns in air to form brown amorphous cadmium oxide (CdO); the crystalline form of this compound is a dark red which changes color when heated, similar to zinc oxide. Hydrochloric acid, sulfuric acid, and nitric acid dissolve cadmium by forming cadmium chloride (CdCl), cadmium sulfate (CdSO), or cadmium nitrate (Cd(NO)). The oxidation state +1 can be produced by dissolving cadmium in a mixture of cadmium chloride and aluminium chloride, forming the Cd cation, which is similar to the Hg cation in mercury(I) chloride.\nThe structures of many cadmium complexes with nucleobases, amino acids, and vitamins have been determined.\n\nNaturally occurring cadmium is composed of 8 isotopes. Two of them are radioactive, and three are expected to decay but have not done so under laboratory conditions. The two natural radioactive isotopes are Cd (beta decay, half-life is 7.7 × 10 years) and Cd (two-neutrino double beta decay, half-life is 2.9 × 10 years). The other three are Cd, Cd (both double electron capture), and Cd (double beta decay); only lower limits on these half-lives have been determined. At least three isotopes – Cd, Cd, and Cd – are stable. Among the isotopes that do not occur naturally, the most long-lived are Cd with a half-life of 462.6 days, and Cd with a half-life of 53.46 hours. All of the remaining radioactive isotopes have half-lives of less than 2.5 hours, and the majority have half-lives of less than 5 minutes. Cadmium has 8 known meta states, with the most stable being Cd (\"t\" = 14.1 years), Cd (\"t\" = 44.6 days), and Cd (\"t\" = 3.36 hours).\n\nThe known isotopes of cadmium range in atomic mass from 94.950 u (Cd) to 131.946 u (Cd). For isotopes lighter than 112 u, the primary decay mode is electron capture and the dominant decay product is element 47 (silver). Heavier isotopes decay mostly through beta emission producing element 49 (indium).\n\nOne isotope of cadmium, Cd, absorbs neutrons with high selectivity: With very high probability, neutrons with energy below the \"cadmium cut-off\" will be absorbed; those higher than the \"cut-off will be transmitted\". The cadmium cut-off is about 0.5 eV, and neutrons below that level are deemed slow neutrons, distinct from intermediate and fast neutrons.\n\nCadmium is created via the s-process in low- to medium-mass stars with masses of 0.6 to 10 solar masses, over thousands of years. In that process, a silver atom captures a neutron and then undergoes beta decay.\n\nCadmium (Latin \"cadmia\", Greek \"καδμεία\" meaning \"calamine\", a cadmium-bearing mixture of minerals that was named after the Greek mythological character Κάδμος, Cadmus, the founder of Thebes) was discovered simultaneously in 1817 by Friedrich Stromeyer and Karl Samuel Leberecht Hermann, both in Germany, as an impurity in zinc carbonate. Stromeyer found the new element as an impurity in zinc carbonate (calamine), and, for 100 years, Germany remained the only important producer of the metal. The metal was named after the Latin word for calamine, because it was found in this zinc ore. Stromeyer noted that some impure samples of calamine changed color when heated but pure calamine did not. He was persistent in studying these results and eventually isolated cadmium metal by roasting and reducing the sulfide. The potential for cadmium yellow as pigment was recognized in the 1840s, but the lack of cadmium limited this application.\n\nEven though cadmium and its compounds are toxic in certain forms and concentrations, the British Pharmaceutical Codex from 1907 states that cadmium iodide was used as a medication to treat \"enlarged joints, scrofulous glands, and chilblains\".\n\nIn 1907, the International Astronomical Union defined the international ångström in terms of a red cadmium spectral line (1 wavelength = 6438.46963 Å). This was adopted by the 7th General Conference on Weights and Measures in 1927. In 1960, the definitions of both the metre and ångström were changed to use krypton.\n\nAfter the industrial scale production of cadmium started in the 1930s and 1940s, the major application of cadmium was the coating of iron and steel to prevent corrosion; in 1944, 62% and in 1956, 59% of the cadmium in the United States was used for plating. In 1956, 24% of the cadmium in the United States was used for a second application in red, orange and yellow pigments from sulfides and selenides of cadmium.\n\nThe stabilizing effect of cadmium chemicals like the carboxylates cadmium laurate and cadmium stearate on PVC led to an increased use of those compounds in the 1970s and 1980s. The demand for cadmium in pigments, coatings, stabilizers, and alloys declined as a result of environmental and health regulations in the 1980s and 1990s; in 2006, only 7% of to total cadmium consumption was used for plating, and only 10% was used for pigments.\nAt the same time, these decreases in consumption were compensated by a growing demand for cadmium for nickel-cadmium batteries, which accounted for 81% of the cadmium consumption in the United States in 2006.\n\nCadmium makes up about 0.1 ppm of Earth's crust. It is much rarer than zinc, which makes up about 65 ppm. No significant deposits of cadmium-containing ores are known. The only cadmium mineral of importance, greenockite (CdS), is nearly always associated with sphalerite (ZnS). This association is caused by geochemical similarity between zinc and cadmium, with no geological process likely to separate them. Thus, cadmium is produced mainly as a byproduct of mining, smelting, and refining sulfidic ores of zinc, and, to a lesser degree, lead and copper. Small amounts of cadmium, about 10% of consumption, are produced from secondary sources, mainly from dust generated by recycling iron and steel scrap. Production in the United States began in 1907, but wide use began after World War I.\n\nMetallic cadmium can be found in the Vilyuy River basin in Siberia.\n\nRocks mined for phosphate fertilizers contain varying amounts of cadmium, resulting in a cadmium concentration of as much as 300 mg/kg in the fertilizers and a high cadmium content in agricultural soils. Coal can contain significant amounts of cadmium, which ends up mostly in flue dust. Cadmium in soil can be absorbed by crops such as rice. Chinese ministry of agriculture measured in 2002 that 28% of rice it sampled had excess lead and 10% had excess cadmium above limits defined by law. Some plants such as willow trees and poplars have been found to clean both lead and cadmium from soil.\n\nTypical background concentrations of cadmium do not exceed 5 ng/m in the atmosphere; 2 mg/kg in soil; 1 μg/L in freshwater and 50 ng/L in seawater.\n\nThe British Geological Survey reports that in 2001, China was the top producer of cadmium with almost one-sixth of the world's production, closely followed by South Korea and Japan.\n\nCadmium is a common impurity in zinc ores, and it is most often isolated during the production of zinc. Some zinc ores concentrates from sulfidic zinc ores contain up to 1.4% of cadmium. In the 1970s, the output of cadmium was 6.5 pounds per ton of zinc. Zinc sulfide ores are roasted in the presence of oxygen, converting the zinc sulfide to the oxide. Zinc metal is produced either by smelting the oxide with carbon or by electrolysis in sulfuric acid. Cadmium is isolated from the zinc metal by vacuum distillation if the zinc is smelted, or cadmium sulfate is precipitated from the electrolysis solution.\n\nCadmium is a common component of electric batteries, pigments, coatings, and electroplating.\n\nIn 2009, 86% of cadmium was used in batteries, predominantly in rechargeable nickel-cadmium batteries. Nickel-cadmium cells have a nominal cell potential of 1.2 V. The cell consists of a positive nickel hydroxide electrode and a negative cadmium electrode plate separated by an alkaline electrolyte (potassium hydroxide). The European Union put a limit on cadmium in electronics in 2004 of 0.01%, with some exceptions, and reduced the limit on cadmium content to 0.002%. Another type of battery based on cadmium is the silver-cadmium battery.\n\nCadmium electroplating, consuming 6% of the global production, is used in the aircraft industry to reduce corrosion of steel components. This coating is passivated by chromate salts. A limitation of cadmium plating is hydrogen embrittlement of high-strength steels from the electroplating process. Therefore, steel parts heat-treated to tensile strength above 1300 MPa (200 ksi) should be coated by an alternative method (such as special low-embrittlement cadmium electroplating processes or physical vapor deposition).\n\nTitanium embrittlement from cadmium-plated tool residues resulted in banishment of those tools (and the implementation of routine tool testing to detect cadmium contamination) in the A-12/SR-71, U-2, and subsequent aircraft programs that use titanium.\n\nCadmium is used in the control rods of nuclear reactors, acting as a very effective \"neutron poison\" to control neutron flux in nuclear fission. When cadmium rods are inserted in the core of a nuclear reactor, cadmium absorbs neutrons, preventing them from creating additional fission events, thus controlling the amount of reactivity. The pressurized water reactor designed by Westinghouse Electric Company uses an alloy consisting of 80% silver, 15% indium, and 5% cadmium.\n\nQLED TVs have been starting to include cadmium in construction. Some companies have been looking to reduce the environmental impact of human exposure and pollution of the material in televisions during production.\n\nCadmium oxide was used in black and white television phosphors and in the blue and green phosphors of color television cathode ray tubes. Cadmium sulfide (CdS) is used as a photoconductive surface coating for photocopier drums.\nVarious cadmium salts are used in paint pigments, with CdS as a yellow pigment being the most common. Cadmium selenide is a red pigment, commonly called \"cadmium red\". To painters who work with the pigment, cadmium provides the most brilliant and durable yellows, oranges, and reds — so much so that during production, these colors are significantly toned down before they are ground with oils and binders or blended into watercolors, gouaches, acrylics, and other paint and pigment formulations. Because these pigments are potentially toxic, users should use a barrier cream on the hands to prevent absorption through the skin even though the amount of cadmium absorbed into the body through the skin is reported to be less than 1%.\n\nIn PVC, cadmium was used as heat, light, and weathering stabilizers. Currently, cadmium stabilizers have been completely replaced with barium-zinc, calcium-zinc and organo-tin stabilizers. Cadmium is used in many kinds of solder and bearing alloys, because it has a low coefficient of friction and fatigue resistance. It is also found in some of the lowest-melting alloys, such as Wood's metal.\n\nHelium–cadmium lasers are a common source of blue-ultraviolet laser light. They operate at either 325 or 422 nm in fluorescence microscopes and various laboratory experiments. Cadmium selenide quantum dots emit bright luminescence under UV excitation (He-Cd laser, for example). The color of this luminescence can be green, yellow or red depending on the particle size. Colloidal solutions of those particles are used for imaging of biological tissues and solutions with a fluorescence microscope.\n\nCadmium is a component of some compound semiconductors, such as cadmium sulfide, cadmium selenide, and cadmium telluride, used for light detection and solar cells. HgCdTe is sensitive to infrared light and can be used as an infrared detector, motion detector, or switch in remote control devices.\n\nIn molecular biology, cadmium is used to block voltage-dependent calcium channels from fluxing calcium ions, as well as in hypoxia research to stimulate proteasome-dependent degradation of Hif-1α.\n\nCadmium-selective sensors based on the fluorophore BODIPY have been developed for imaging and sensing of cadmium in cells. One of the most popular way to monitor cadmium in aqueous environments is the use of electrochemistry, one example is by attaching a self-assembled monolayer that can help obtain a cadmium selective electrode with a ppt-level sensitivity.\n\nCadmium has no known function in higher organisms, but a cadmium-dependent carbonic anhydrase has been found in some marine diatoms. The diatoms live in environments with very low zinc concentrations and cadmium performs the function normally carried out by zinc in other anhydrases. This was discovered with X-ray absorption fluorescence spectroscopy (XAFS).\n\nThe highest concentration of cadmium is absorbed in the kidneys of humans, and up to about 30 mg of cadmium is commonly inhaled throughout human childhood and adolescence. Cadmium is under preliminary research for its toxicity in humans, potentially affecting mechanisms and risks of cancer, cardiovascular disease, and osteoporosis.\n\nThe biogeochemistry of cadmium and its release to the environment has been the subject of review, as has the speciation of cadmium in the environment.\n\nIndividuals and organizations have been reviewing cadmium's bioinorganic aspects for its toxicity. The most dangerous form of occupational exposure to cadmium is inhalation of fine dust and fumes, or ingestion of highly soluble cadmium compounds. Inhalation of cadmium fumes can result initially in metal fume fever but may progress to chemical pneumonitis, pulmonary edema, and death.\n\nCadmium is also an environmental hazard. Human exposure is primarily from fossil fuel combustion, phosphate fertilizers, natural sources, iron and steel production, cement production and related activities, nonferrous metals production, and municipal solid waste incineration. Bread, root crops, and vegetables also contribute to the cadmium in modern populations.\nThere have been a few instances of general population poisoning as the result of long-term exposure to cadmium in contaminated food and water, and research into an estrogen mimicry that may induce breast cancer is ongoing. In the decades leading up to World War II, mining operations contaminated the Jinzū River in Japan with cadmium and traces of other toxic metals. As a consequence, cadmium accumulated in the rice crops along the riverbanks downstream of the mines. Some members of the local agricultural communities consumed the contaminated rice and developed itai-itai disease and renal abnormalities, including proteinuria and glucosuria. The victims of this poisoning were almost exclusively post-menopausal women with low iron and other mineral body stores. Similar general population cadmium exposures in other parts of the world have not resulted in the same health problems because the populations maintained sufficient iron and other mineral levels. Thus, although cadmium is a major factor in the itai-itai disease in Japan, most researchers have concluded that it was one of several factors.\n\nCadmium is one of six substances banned by the European Union's Restriction on Hazardous Substances (RoHS) directive, which regulates hazardous substances in electrical and electronic equipment but allows for certain exemptions and exclusions from the scope of the law.\nThe International Agency for Research on Cancer has classified cadmium and cadmium compounds as carcinogenic to humans. Although occupational exposure to cadmium is linked to lung and prostate cancer, there is still a substantial controversy about the carcinogenicity of cadmium in low environmental exposure. Recent data from epidemiological studies suggest that intake of cadmium through diet associates to higher risk of endometrial, breast and prostate cancer as well as to osteoporosis in humans. A recent study has demonstrated that endometrial tissue is characterized by higher levels of cadmium in current and former smoking females.\n\nCadmium exposure is a risk factor associated with a large number of illnesses including kidney disease, early atherosclerosis, hypertension, and cardiovascular diseases. Although studies show a significant correlation between cadmium exposure and occurrence of disease in human populations, a necessary molecular mechanism has not been identified. One hypothesis holds that cadmium is an endocrine disruptor and some experimental studies have shown that it can interact with different hormonal signaling pathways. For example, cadmium can bind to the estrogen receptor alpha, and affect signal transduction along the estrogen and MAPK signaling pathways at low doses.\n\nThe tobacco plant readily absorbs and accumulates heavy metals, such as cadmium from the surrounding soil into its leaves. These are readily absorbed into the user's body following smoke inhalation. Tobacco smoking is the most important single source of cadmium exposure in the general population. An estimated 10% of the cadmium content of a cigarette is inhaled through smoking. Absorption of cadmium through the lungs is more effective than through the gut, and as much as 50% of the cadmium inhaled in cigarette smoke may be absorbed.\nOn average, cadmium concentrations in the blood of smokers is 4 times 5 times greater and in the kidney, 2–3 times greater than non-smokers. Despite the high cadmium content in cigarette smoke, there seems to be little exposure to cadmium from passive smoking.\n\nIn a non-smoking population, food is the greatest source of exposure. High quantities of cadmium can be found in crustaceans, mollusks, offal, and algae products. However, grains, vegetables, and starchy roots and tubers are consumed in much greater quantity in the US, and are the source of the greatest dietary exposure. Most plants bio-accumulate metal toxins like Cd, and when composted to form organic fertilizers yield a product which can often contain high amounts (e.g., over 0.5 mg) of metal toxins for every kilo of fertilizer. Fertilizers made from animal dung (e.g., cow dung) or urban waste can contain similar amounts of Cd. The Cd added to the soil from fertilizers (rock phosphates or organic fertilizers) become bio-available and toxic only if the soil pH is low (i.e., acidic soils). Zinc is chemically similar to cadmium and some evidence indicates the presence of Zn ions reduces cadmium toxicity.\n\nZinc, Cu, Ca, and Fe ions, and selenium with vitamin C are used to treat Cd intoxication, though it is not easily reversed.\n\nBecause of the adverse effects of cadmium on the environment and human health, the supply and use of cadmium is restricted in Europe under the REACH Regulation.\n\nThe EFSA Panel on Contaminants in the Food Chain specifies that 2.5 μg/kg body weight is a tolerable weekly intake for humans. The Joint FAO/WHO Expert Committee on Food Additives has declared 7 μg/kg bw to be the provisional tolerable weekly intake level.\n\nThe US Occupational Safety and Health Administration (OSHA) has set the permissible exposure limit (PEL) for cadmium at a time-weighted average (TWA) of 0.005 ppm. The National Institute for Occupational Safety and Health (NIOSH) has not set a recommended exposure limit (REL) and has designated cadmium as a known human carcinogen. The IDLH (immediately dangerous to life and health) level for cadmium is 9 mg/m.\nIn May 2006, a sale of the seats from Arsenal F.C.'s old stadium, Highbury in London, England was cancelled when the seats were discovered to contain trace amounts of cadmium. Reports of high levels of cadmium use in children's jewelry in 2010 led to a US Consumer Product Safety Commission investigation. The U.S. CPSC issued specific recall notices for cadmium content in jewelry sold by Claire's and Wal-Mart stores.\n\nIn June 2010, McDonald's voluntarily recalled more than 12 million promotional \"Shrek Forever After 3D\" Collectable Drinking Glasses because of the cadmium levels in paint pigments on the glassware. The glasses were manufactured by Arc International, of Millville, NJ, USA.\n\n\n\n"}
{"id": "43583260", "url": "https://en.wikipedia.org/wiki?curid=43583260", "title": "Castillo de Bellver oil spill", "text": "Castillo de Bellver oil spill\n\nThe MT \"Castillo de Bellver\" oil spill began on 6 August 1983, when the Spanish tanker caught fire off Saldanha Bay, approximately 70 miles northwest of Cape Town, South Africa. It was carrying 250,000 tonnes (300,000 cubic metres) of light crude oil, and was traveling through an environmentally sensitive area known for its seabird rookeries and important commercial fishing grounds. The burning vessel was abandoned and broke apart after drifting offshore. The stern capsized and sunk and the bow was sunk using explosives. A total of 145,000-170,000 tonnes (176,000-210,000 cubic metres) of oil entered the sea. Onshore impacts were considered negligible as the slick traveled seaward. The only visible impact was the oiling of 1,500 gannets that were on a nearby island.\n\nThe \"Castillo de Bellver\" was built in 1978 to carry light crude oil.\n\nOn 6 August 1983, the Spanish oil tanker \"Castillo de Bellver\" was en route from the Persian Gulf to Spain transporting 250,000 tonnes (300,000 cubic metres) of light crude oil.\n\nAround off Table Bay, South Africa, it exploded and proceeded to burn. The crew abandoned the ship, which proceeded to drift off the coast, eventually breaking in two at around 10 a.m. Approximately of light crude was initially spilled into the sea, creating a flaming oil slick. By mid-morning, the ship trailed an oil slick long and wide.\n\nA fishing trawler, \"Harvest Carina\", rescued 32 crew members from a lifeboat. A passing container ship rescued another crew member. Three additional persons were declared missing.\n\nThe stern section of the ship capsized and sank on 7 August in deep waters, off the coast. There were of oil remaining in \"Castillo de Bellver\"'s tanks. The bow had drifted towards an area that included the Langebaan Lagoon marine life sanctuary, a -long strip of coast south of Saldanha Port. The bow section was then towed away from the coast and was eventually sunk using explosives.\n\nA total of of oil was released into the sea during the incident.\n\nThe pollution threat to Cape Province beaches was initially considered \"enormous\", according to a Cape Town port official. He said that the current could move the oil slick away from the coast but that a northwesterly wind could blow the oil towards the coast.\n\nThe oil initially drifted towards the coast. The wind then changed direction and took it offshore, where the slick entered the north-west flowing Benguela Current.\n\nWeather conditions proved to be conducive to spill response and helped prevent a major onshore environmental disaster.\n\nApproximately of diluted chemical dispersant and of dispersant concentrate were sprayed at the edge of the slick, preventing it from coming within of the shore.\n\nOn August 10, the slick was almost offshore as the bow of the tanker was being towed out to sea.\n\nBy August 12, over 200 oiled birds had been recovered \"with feathers glued with oil\" and taken to conservation centers in Cape Town. Many more were expected to be received.\n\nThe accident area is both ecologically and economically sensitive, rich in flora and fauna. It is also home to a large seabird population. Half of South African lobster and fish landings are caught within this zone, which is also an important nursery area for many fish species. The known environmental consequences of the spill were considered small. About 1,500 gannets that were gathered on a nearby island in preparation for breeding season were oiled. Some seals surfaced during dispersant spraying but are not believed to have been harmed.\n\n\"Black rain\" of oil droplets fell immediately to the east of the spill during the first 24 hours on wheat-growing and sheep-grazing fields, but no long-term damage was recorded.\n\nThe impact on local fish stocks was considered minimal. According to tests conducted on sediment and water samples and plankton trawls, no abnormal presence of hydrocarbons was detected.\n"}
{"id": "28889936", "url": "https://en.wikipedia.org/wiki?curid=28889936", "title": "Chumbi Surla Wildlife Sanctuary", "text": "Chumbi Surla Wildlife Sanctuary\n\nChumbi Surla Wildlife Sanctuary (shortened as Chumbi Surla) is a wildlife sanctuary covering an area of . It is located in Khushab district and Chakwal District, Punjab, Pakistan. It was established in 1978, for the purpose of conserving the threatened species of urial among several other.\n\nThe area is surrounded by reserve forests and hill\n\n"}
{"id": "47526", "url": "https://en.wikipedia.org/wiki?curid=47526", "title": "Convection", "text": "Convection\n\nConvection is the heat transfer due to bulk movement of molecules within fluids such as gases and liquids, including molten rock (rheid). Convection takes place through advection, diffusion or both.\n\nConvection cannot take place in most solids because neither bulk current flows nor significant diffusion of matter can take place. Diffusion of heat takes place in rigid solids, but that is called heat conduction. Convection, however, can take place in soft solids or mixtures where solid particles can move past each other.\n\nThermal convection can be demonstrated by placing a heat source (e.g. a Bunsen burner) at the side of a glass filled with a liquid, and observing the changes in temperature in the glass caused by the warmer fluid circulating into cooler areas.\n\nConvective heat transfer is one of the major types of heat transfer, and convection is also a major mode of mass transfer in fluids. Convective heat and mass transfer takes place both by diffusion – the random Brownian motion of individual particles in the fluid – and by advection, in which matter or heat is transported by the larger-scale motion of currents in the fluid. In the context of heat and mass transfer, the term \"convection\" is used to refer to the combined effects of advective and diffusive transfer. Sometimes the term \"convection\" is used to refer specifically to \"free heat convection\" (natural heat convection) which is due to temperature-induced differences in buoyancy, as opposed to \"forced heat convection\" where forces other than buoyancy (such as pump or fan) move the fluid. However, in mechanics the correct use of the word \"convection\" is the general sense, and different types of convection should be qualified for clarity.\n\nConvection can be qualified in terms of being natural, forced, gravitational, granular, or thermomagnetic. It may also be said to be due to combustion, capillary action, or Marangoni and Weissenberg effects. Heat transfer by natural convection plays a role in the structure of Earth's atmosphere, its oceans, and its mantle. Discrete convective cells in the atmosphere can be seen as clouds, with stronger convection resulting in thunderstorms. Natural convection also plays a role in stellar physics.\n\nThe word \"convection\" may have slightly different but related usages in different scientific or engineering contexts or applications. The broader sense is in fluid mechanics, where \"convection\" refers to the motion of fluid regardless of cause. However, in thermodynamics \"convection\" often refers specifically to heat transfer by convection.\n\nAdditionally, convection includes fluid movement both by bulk motion (advection) and by the motion of individual particles (diffusion). However, in some cases, convection is taken to mean only advective phenomena. For instance, in the transport equation, which describes a number of different transport phenomena, terms are separated into \"convective\" and \"diffusive\" effects, with \"convective\" meaning purely advective in context.\n\nConvection occurs on a large scale in atmospheres, oceans, planetary mantles, and it provides the mechanism of heat transfer for a large fraction of the outermost interiors of our sun and all stars. Fluid movement during convection may be invisibly slow, or it may be obvious and rapid, as in a hurricane. On astronomical scales, convection of gas and dust is thought to occur in the accretion disks of black holes, at speeds which may closely approach that of light.\n\nConvective heat transfer is a mechanism of heat transfer occurring because of bulk motion (observable movement) of fluids. Heat is the entity of interest being advected (carried), and diffused (dispersed). This can be contrasted with conductive heat transfer, which is the transfer of energy by vibrations at a molecular level through a solid or fluid, and radiative heat transfer, the transfer of energy through electromagnetic waves.\n\nHeat is transferred by convection in numerous examples of naturally occurring fluid flow, such as wind, oceanic currents, and movements within the Earth's mantle. Convection is also used in engineering practices of homes, industrial processes, cooling of equipment, etc.\n\nThe rate of convective heat transfer may be improved by the use of a heat sink, often in conjunction with a fan. For instance, a typical computer CPU will have a purpose-made fan to ensure its operating temperature is kept within tolerable limits.\n\nA convection cell, also known as a Bénard cell is a characteristic fluid flow pattern in many convection systems. A rising body of fluid typically loses heat because it encounters a cold surface. In liquid, this occurs because it exchanges heat with colder liquid through direct exchange. In the example of the Earth's atmosphere, this occurs because it radiates heat. Because of this heat loss the fluid becomes denser than the fluid underneath it, which is still rising. Since it cannot descend through the rising fluid, it moves to one side. At some distance, its downward force overcomes the rising force beneath it, and the fluid begins to descend. As it descends, it warms again and the cycle repeats itself.\n\nAtmospheric circulation is the large-scale movement of air, and is a means by which thermal energy is distributed on the surface of the Earth, together with the much slower (lagged) ocean circulation system. The large-scale structure of the atmospheric circulation varies from year to year, but the basic climatological structure remains fairly constant.\n\nLatitudinal circulation occurs because incident solar radiation per unit area is highest at the heat equator, and decreases as the latitude increases, reaching minima at the poles. It consists of two primary convection cells, the Hadley cell and the polar vortex, with the Hadley cell experiencing stronger convection due to the release of latent heat energy by condensation of water vapor at higher altitudes during cloud formation.\n\nLongitudinal circulation, on the other hand, comes about because the ocean has a higher specific heat capacity than land (and also thermal conductivity, allowing the heat to penetrate further beneath the surface ) and thereby absorbs and releases more heat, but the temperature changes less than land. This brings the sea breeze, air cooled by the water, ashore in the day, and carries the land breeze, air cooled by contact with the ground, out to sea during the night. Longitudinal circulation consists of two cells, the Walker circulation and El Niño / Southern Oscillation.\n\nSome more localized phenomena than global atmospheric movement are also due to convection, including wind and some of the hydrologic cycle. For example, a foehn wind is a down-slope wind which occurs on the downwind side of a mountain range. It results from the adiabatic warming of air which has dropped most of its moisture on windward slopes. Because of the different adiabatic lapse rates of moist and dry air, the air on the leeward slopes becomes warmer than at the same height on the windward slopes.\n\nA thermal column (or thermal) is a vertical section of rising air in the lower altitudes of the Earth's atmosphere. Thermals are created by the uneven heating of the Earth's surface from solar radiation. The Sun warms the ground, which in turn warms the air directly above it. The warmer air expands, becoming less dense than the surrounding air mass, and creating a thermal low. The mass of lighter air rises, and as it does, it cools by expansion at lower air pressures. It stops rising when it has cooled to the same temperature as the surrounding air. Associated with a thermal is a downward flow surrounding the thermal column. The downward moving exterior is caused by colder air being displaced at the top of the thermal. Another convection-driven weather effect is the sea breeze.\nWarm air has a lower density than cool air, so warm air rises within cooler air, similar to hot air balloons. Clouds form as relatively warmer air carrying moisture rises within cooler air. As the moist air rises, it cools, causing some of the water vapor in the rising packet of air to condense. When the moisture condenses, it releases energy known as latent heat of condensation which allows the rising packet of air to cool less than its surrounding air, continuing the cloud's ascension. If enough instability is present in the atmosphere, this process will continue long enough for cumulonimbus clouds to form, which support lightning and thunder. Generally, thunderstorms require three conditions to form: moisture, an unstable airmass, and a lifting force (heat).\n\nAll thunderstorms, regardless of type, go through three stages: the developing stage, the mature stage, and the dissipation stage. The average thunderstorm has a diameter. Depending on the conditions present in the atmosphere, these three stages take an average of 30 minutes to go through.\n\nSolar radiation affects the oceans: warm water from the Equator tends to circulate toward the poles, while cold polar water heads towards the Equator. The surface currents are initially dictated by surface wind conditions. The trade winds blow westward in the tropics, and the westerlies blow eastward at mid-latitudes. This wind pattern applies a stress to the subtropical ocean surface with negative curl across the Northern Hemisphere, and the reverse across the Southern Hemisphere. The resulting Sverdrup transport is equatorward. Because of conservation of potential vorticity caused by the poleward-moving winds on the subtropical ridge's western periphery and the increased relative vorticity of poleward moving water, transport is balanced by a narrow, accelerating poleward current, which flows along the western boundary of the ocean basin, outweighing the effects of friction with the cold western boundary current which originates from high latitudes. The overall process, known as western intensification, causes currents on the western boundary of an ocean basin to be stronger than those on the eastern boundary.\n\nAs it travels poleward, warm water transported by strong warm water current undergoes evaporative cooling. The cooling is wind driven: wind moving over water cools the water and also causes evaporation, leaving a saltier brine. In this process, the water becomes saltier and denser. and decreases in temperature. Once sea ice forms, salts are left out of the ice, a process known as brine exclusion. These two processes produce water that is denser and colder. The water across the northern Atlantic ocean becomes so dense that it begins to sink down through less salty and less dense water. (The convective action is not unlike that of a lava lamp.) This downdraft of heavy, cold and dense water becomes a part of the North Atlantic Deep Water, a southgoing stream.\n\nMantle convection is the slow creeping motion of Earth's rocky mantle caused by convection currents carrying heat from the interior of the earth to the surface. It is one of 3 driving forces that causes tectonic plates to move around the Earth's surface.\n\nThe Earth's surface is divided into a number of tectonic plates that are continuously being created and consumed at their opposite plate boundaries. Creation (accretion) occurs as mantle is added to the growing edges of a plate. This hot added material cools down by conduction and convection of heat. At the consumption edges of the plate, the material has thermally contracted to become dense, and it sinks under its own weight in the process of subduction at an ocean trench. This subducted material sinks to some depth in the Earth's interior where it is prohibited from sinking further. The subducted oceanic crust triggers volcanism.\n\nThe Stack effect or chimney effect is the movement of air into and out of buildings, chimneys, flue gas stacks, or other containers due to buoyancy. Buoyancy occurs due to a difference in indoor-to-outdoor air density resulting from temperature and moisture differences. The greater the thermal difference and the height of the structure, the greater the buoyancy force, and thus the stack effect. The stack effect helps drive natural ventilation and infiltration. Some cooling towers operate on this principle; similarly the solar updraft tower is a proposed device to generate electricity based on the stack effect.\n\nThe convection zone of a star is the range of radii in which energy is transported primarily by convection.\n\nGranules on the photosphere of the Sun are the visible tops of convection cells in the photosphere, caused by convection of plasma in the photosphere. The rising part of the granules is located in the center where the plasma is hotter. The outer edge of the granules is darker due to the cooler descending plasma. A typical granule has a diameter on the order of 1,000 kilometers and each lasts 8 to 20 minutes before dissipating. Below the photosphere is a layer of much larger \"supergranules\" up to 30,000 kilometers in diameter, with lifespans of up to 24 hours.\nConvection may happen in fluids at all scales larger than a few atoms. There are a variety of circumstances in which the forces required for natural and forced convection arise, leading to different types of convection, described below. In broad terms, convection arises because of body forces acting within the fluid, such as gravity.\n\nThe causes of convection are generally described as one of either \"natural\" (\"free\") or \"forced\", although other mechanisms also exist (discussed below). However, the distinction between natural and forced convection is particularly important for convective heat transfer.\n\nNatural convection, or free convection, occurs due to temperature differences which affect the density, and thus relative buoyancy, of the fluid. Heavier (denser) components will fall, while lighter (less dense) components rise, leading to bulk fluid movement. Natural convection can only occur, therefore, in a gravitational field. A common example of natural convection is the rise of smoke from a fire. It can be seen in a pot of boiling water in which the hot and less-dense water on the bottom layer moves upwards in plumes, and the cool and more dense water near the top of the pot likewise sinks.\n\nNatural convection will be more likely and more rapid with a greater variation in density between the two fluids, a larger acceleration due to gravity that drives the convection or a larger distance through the convecting medium. Natural convection will be less likely and less rapid with more rapid diffusion (thereby diffusing away the thermal gradient that is causing the convection) or a more viscous (sticky) fluid.\n\nThe onset of natural convection can be determined by the Rayleigh number (Ra).\n\nNote that differences in buoyancy within a fluid can arise for reasons other than temperature variations, in which case the fluid motion is called gravitational convection (see below). However, all types of buoyant convection, including natural convection, do not occur in microgravity environments. All require the presence of an environment which experiences g-force (proper acceleration).\n\nIn forced convection, also called heat advection, fluid movement results from external surface forces such as a fan or pump. Forced convection is typically used to increase the rate of heat exchange. Many types of mixing also utilize forced convection to distribute one substance within another. Forced convection also occurs as a by-product to other processes, such as the action of a propeller in a fluid or aerodynamic heating. Fluid radiator systems, and also heating and cooling of parts of the body by blood circulation, are other familiar examples of forced convection.\n\nForced convection may happen by natural means, such as when the heat of a fire causes expansion of air and bulk air flow by this means. In microgravity, such flow (which happens in all directions) along with diffusion is the only means by which fires are able to draw in fresh oxygen to maintain themselves. The shock wave that transfers heat and mass out of explosions is also a type of forced convection.\n\nAlthough forced convection from thermal gas expansion in zero-g does not fuel a fire as well as natural convection in a gravity field, some types of artificial forced convection are far more efficient than free convection, as they are not limited by natural mechanisms. For instance, a convection oven works by forced convection, as a fan which rapidly circulates hot air forces heat into food faster than would naturally happen due to simple heating without the fan.\n\nGravitational convection is a type of natural convection induced by buoyancy variations resulting from material properties other than temperature. Typically this is caused by a variable composition of the fluid. If the varying property is a concentration gradient, it is known as solutal convection. For example, gravitational convection can be seen in the diffusion of a source of dry salt downward into wet soil due to the buoyancy of fresh water in saline.\n\nVariable salinity in water and variable water content in air masses are frequent causes of convection in the oceans and atmosphere which do not involve heat, or else involve additional compositional density factors other than the density changes from thermal expansion (see \"thermohaline circulation\"). Similarly, variable composition within the Earth's interior which has not yet achieved maximal stability and minimal energy (in other words, with densest parts deepest) continues to cause a fraction of the convection of fluid rock and molten metal within the Earth's interior (see below).\n\nGravitational convection, like natural thermal convection, also requires a g-force environment in order to occur.\n\nVibration-induced convection occurs in powders and granulated materials in containers subject to vibration where an axis of vibration is parallel to the force of gravity. When the container accelerates upward, the bottom of the container pushes the entire contents upward. In contrast, when the container accelerates downward, the sides of the container push the adjacent material downward by friction, but the material more remote from the sides is less affected. The net result is a slow circulation of particles downward at the sides, and upward in the middle.\n\nIf the container contains particles of different sizes, the downward-moving region at the sides is often narrower than the largest particles. Thus, larger particles tend to become sorted to the top of such a mixture. This is one possible explanation of the Brazil nut effect.\n\nIce convection on Pluto is believed to occur in a soft mixture of nitrogen ice and carbon monoxide ice. It has also been proposed for Europa, and other bodies in the outer solar system.\n\nThermomagnetic convection can occur when an external magnetic field is imposed on a ferrofluid with varying magnetic susceptibility. In the presence of a temperature gradient this results in a nonuniform magnetic body force, which leads to fluid movement. A ferrofluid is a liquid which becomes strongly magnetized in the presence of a magnetic field.\n\nThis form of heat transfer can be useful for cases where conventional convection fails to provide adequate heat transfer, e.g., in miniature microscale devices or under reduced gravity conditions.\n\nCapillary action is a phenomenon where liquid spontaneously rises in a narrow space such as a thin tube, or in porous materials. This effect can cause liquids to flow against the force of gravity. It occurs because of inter-molecular attractive forces between the liquid and solid surrounding surfaces; If the diameter of the tube is sufficiently small, then the combination of surface tension and forces of adhesion between the liquid and container act to lift the liquid.\n\nThe Marangoni effect is the convection of fluid along an interface between dissimilar substances because of variations in surface tension. Surface tension can vary because of inhomogeneous composition of the substances or the temperature-dependence of surface tension forces. In the latter case the effect is known as thermo-capillary convection.\n\nA well-known phenomenon exhibiting this type of convection is the \"tears of wine\".\n\nThe Weissenberg effect is a phenomenon that occurs when a spinning rod is placed into a solution of liquid polymer. Entanglements cause the polymer chains to be drawn towards the rod instead of being thrown outward as would happen with an ordinary fluid (i.e., water).\n\nIn a zero-gravity environment, there can be no buoyancy forces, and thus no natural (free) convection possible, so flames in many circumstances without gravity smother in their own waste gases. However, flames may be maintained with any type of forced convection (breeze); or (in high oxygen environments in \"still\" gas environments) entirely from the minimal forced convection that occurs as heat-induced \"expansion\" (not buoyancy) of gases allows for ventilation of the flame, as waste gases move outward and cool, and fresh high-oxygen gas moves in to take up the low pressure zones created when flame-exhaust water condenses.\n\nMathematically, convection can be described by the convection–diffusion equation, also known as the generic scalar transport equation.\n\nIn cases of mixed convection (natural and forced occurring together) one would often like to know how much of the convection is due to external constraints, such as the fluid velocity in the pump, and how much is due to natural convection occurring in the system.\n\nThe relative magnitudes of the Grashof number and the square of the Reynolds number determine which form of convection dominates. If formula_1, forced convection may be neglected, whereas if formula_2, natural convection may be neglected. If the ratio, known as the Richardson number, is approximately one, then both forced and natural convection need to be taken into account.\n\n\n"}
{"id": "50986802", "url": "https://en.wikipedia.org/wiki?curid=50986802", "title": "Dihydroxy-E,Z,E-PUFA", "text": "Dihydroxy-E,Z,E-PUFA\n\nDihydroxy-\"E\",\"Z\",\"E\"-PUFA are metabolites of polyunsaturated fatty acids (PUFA) that possess two hydroxyl residues and three in series conjugated double bonds having the \"E\",\"Z\",\"E\" cis-trans configuration. These recently classified metabolites are distinguished from the many other dihydroxy-PUFA with three conjugated double bonds that do not have this critical \"E\",\"Z\",\"E\" configuration: they inhibit the function of platelets and therefore may be involved in controlling and prove useful for inhibiting human diseases which involve the pathological activation of these blood-borne elements.\n\nDihydroxy-\"E\",\"Z\",\"E\"-PUFA are metabolites of a) docosahexaenoic acid (DHA, i.e. 4\"Z\",7\"Z\",10\"Z\",13\"Z\",16\"Z\",19\"Z\"-docosahexaenoic acid), b) α-Linolenic acid (ALA, i.e. \"9\"Z,12\"Z\",15\"Z\"-octadecatrienoic acid), an c) arachidonic acid (AA); \"E\",\"Z\",\"E\"-DHA and \"E\",\"Z\",\"E\" AA metabolites are termed poxytrins; ALA metabolites are termed linotrins. The first and perhaps most prominent member of this class of metabolites is protectin DX (PDX; i.e. 10\"R\",17\"S\"-dihydroxy-4\"Z\",7\"Z\",11\"E\",13\"Z\",15\"E\",19\"Z\"-docosahexaenoic acid). PDX is an isomer of (and sometimes confused with) neuroprotectin D1 (NPD1; i.e. 10\"R\",17\"S\"-dihydroxy-4\"Z\",7\"Z\",11\"E\",13\"E\",15\"Z\",19\"Z\"-doxahexaenoic acid; also termed protectin D1 [PD1]). NPD1 is structurally identical to PDX except that its three conjugated double bonds have the \"E\",\"E\",\"Z\" configuration as opposed to the \"E\",\"Z\",\"E\" configuration of PDX. Both compounds are members of the specialized proresolving mediators class of PUFA metabolites in that they possess potent anti-inflammatory activity; however, only PDX inhibited human platelet aggregation responses. Subsequent studies found that various other dihydroxy-\"E\",\"Z\",\"E\"-double bound-configured PUFA but not those with \"E\",\"E\",\"E\" or \"E\",\"E\",\"Z\" double bond configurations share with PDX anti-platelet activity. Cells make PDX by metabolizing DHA by double oxygenation a 15-lipoxygenase to form the 10\"R\",17\"S\"-hydroxperoxy intermediated which is reduced its 10\"R\",17\"S\"-hydroxyl product, PDX, probably by cytosolic GPX1 (i.e. glutathione peroxidase 1). Serial metabolism two different lipoxygenases or a lipoxygenase and a cytochrome P45) on PUFA possessing three double bonds in a 1\"Z\",4\"Z\",7\"Z\" configuration may also make a 1,7-dihydroxy 2\"E\",4\"Z\",6\"E\" product.\n\nOther platelet-inhibiting dihydroxy-\"E\",\"Z\",\"E\"-PUFA are: 10\"R\",17\"S\"-dihydroxy-4\"Z\",7\"Z\",11\"E\",13\"Z\",15\"E\",19\"Z\"-docosahexaenoic acid (10\"R\",17\"S\"-diHDHA); 8\"S\",15\"S\"-dihydroxy-5\"Z\",9\"E\",11\"Z\",13\"E\"-eicosatetraenoic acid (8\"S\",15\"S\"-diHETE); 9\"S\",16\"S\"-10\"E\",12\"Z\",14\"E\"-octadecatrienoic acid (linotrin-1); and 9\"R\",16\"S\"-10\"E\",12\"Z\",14\"E\"-octadecatrienoic acid (linotrin-2). 10\"R\",17\"S\"-diHDHA is the 10\"R\" diastereomer of PDX with the 10\"R\" hydroxyl residue being formed by aspirin-treated COX-2 or a cytochrome P450. Guinea pig tissues make 8\"S\",15\"S\"-diHETE probably by double oxygenation of AA by a 15-lipoxygenase (probably ALOX15) or serial metabolism by two enzymes. Linotrin-1 and linotrin-2 are among the four isomeric metabolites produced by incubating ALA with ALOX15B. The extent to which the linotrins form in cells or in vito is not clear.\n\nStimulating agents such as collagen depend to platelets to make and release thromboxane A2 (TXA2) to mediate and/or enhance their aggregating activity. 10\"R\",17\"S\"-diHDHA and to slightly lesser degrees 10\"R\",17\"S\"-diHDHA and PDX inhibit the human platelet aggregation response to collagen at ≥ 100–200 nanomolar concentrations. This inhibition appeared to reflect the ability of these metabolites to a) inhibit the activities of COX-1 and COX-2 thereby blocking the production of TXA2 and b) interfere with the activation of the TXA2 receptor (Thromboxane receptor) by TXA2. The linotrins appear to have similar or slightly lower potencies than as well as to use mechanism similar to the aforementioned metabolites. These \"E,Z,E\" PUFA are 20- to 100-fold stronger in inhibiting human platelet aggregation than two mono-hydroxyl-containing eicosanoids, 5-HETE and 12-HETE, which contain an \"E,Z\" conjugated double bond configuration.\n\nOther biologically active dihydroxy-\"E\",\"Z\",\"E\"-PUFA have not been tested for but, based on the studies cited above, are projected to possess anti-platelet activity. 10\"S\",17\"S\"-Dihydroxy-4\"Z\",7\"Z\",11\"E\",13\"Z\",15\"E\",19\"Z\"-docosahexaenoic acid. This compound is the 13\"Z\" cis-trans isomer of 10-epi-protectin D (which possesses a 13\"E\" double bond; see specialized proresolving mediators#DHA-derived protectins/neuroprotectins). Like 10-epi-protectin D1, this docosahexaenoic acid metabolite is formed by stimulated human leukocytes in vitro and possess specialized proresolving mediator (SPM) anti-inflammatory activity. A maresin termed MaR isomer or 7-epi-MaR1, i.e. 7\"S\",14\"S\"-dihydroxy-4\"Z\",8\"E\",10\"Z\",12\"E\",16\"Z\",19\"Z\"-docosahexaenoic acid (see specialized proresolving mediators#DHA-derived Maresins), likewise possesses SPM activity.\n"}
{"id": "27078765", "url": "https://en.wikipedia.org/wiki?curid=27078765", "title": "EKOenergy", "text": "EKOenergy\n\nEKOenergy is an ecolabel for electricity. It is a not-for-profit initiative of the EKOenergy Network, a group of more than 40 environmental organizations from 30 countries. EKOenergy started in 2013 in Europe. Its secretariat is based in Helsinki. Nowadays, EKOenergy is the only international ecolabel for renewable electricity. It is available all over Europe and its material is available in more than 30 languages.\n\nEvery member organization (all not-for-profit organisations) appoints one person to the EKOenergy Board, the Network’s highest governing authority. The Board takes decisions in consultation with the Advisory Group, which consist of experts from different stakeholder groups, including electricity industry, consumers and environmentalists, among others.\n\nOther language version of the label can be found in various countries. E.g. EKOénergie in France, EKOenergie in Germany or EKOenergia in Finland.\n\nOnly electricity from renewable sources can be sold as EKOenergy. But EKOenergy is more than just green electricity. EKOenergy also sets criteria for the following aspects:\n\nWell known sellers of EKOenergy certified electricity include Fortum, Vattenfall, Ekosähkö and Gesternova. Also sellers of electricity tracking certificates can help consumers to get EKOenergy: E.g. ECOHZ and South Pole Group. A full list of licensed sellers can be found on EKOenergy’s website: http://www.ekoenergy.org/buying-ekoenergy/licensees/. An increasing number of EKOenergy consumers uses the label in its own communication: e.g. Otava Printing and Saimaan juomatehdas.\n\nFor each MWh of EKOenergy sold, the seller pays a minimum of €0.10 to EKOenergy's Climate Fund. This money is used to finance climate projects that would not have happened without the contributions. These projects are managed by experienced organisations. All projects are selected in an open process, with\nsellers, buyers and independent experts actively involved. Examples of funded projects: \nWhenever hydropower is sold with the EKOenergy label, €0.10/ MWh go to the Environmental Fund, to finance river restoration projects. Examples of earlier funded projects:\n\nThe European versions of the LEED Standard explicitly recommend the use of EKOenergy labelled electricity. Buildings aiming at LEED certification can get extra points if the electricity used in that building is EKOenergy certified. The text “LEED 2009 BD+C Supplemental Reference Guide with Alternative Compliance Paths for Europe” gives EKOenergy the same status as Green-e certified RECs in the US. They write: \"The EKOenergy electricity certification scheme represents the best available pan-European option for the sustainable and additional consumption of renewable electricity within Europe. EKOenergy certifies renewable electricity that goes beyond the regulations of European directives and national governments of Europe.\"\n\nThe Greenhouse Gas Protocol is a worldwide standard for carbon accounting. It is a joint product of the World Resources Institute and the World Business Council for Sustainable Development. In January 2015, the Secretariat of the Greenhouse Gas Protocol published the Scope 2 Guidance, which gave advice about carbon accounting. The Guidance refers to EKOenergy several times. Chapter 11, which encourages companies to go one step further, refers to EKOenergy’s Climate Fund.\n\nCDP works with 5000 of the largest corporations in the world to help them calculate their carbon emissions and to help them develop effective carbon emission reduction strategies. On page 15 and 16 of its technical notes for accounting of scope 2 emissions (i.e. emissions related to the production of purchased electricity), CDP explains how companies can do more. “Ecolabels are a way for companies to do more with their purchases. EKOenergy, mentioned by the GHG protocol Scope 2 guidance, is such an option: it is a mark of quality which comes on top of tracking certificates. Electricity sold with the EKOenergy label fulfills strict environmental criteria and raises funds for new renewable energy projects. Involvement, transparency and ‘deeds not words’ are important principles of EKOenergy’s work.”\n\n"}
{"id": "27417693", "url": "https://en.wikipedia.org/wiki?curid=27417693", "title": "Eternit", "text": "Eternit\n\nEternit is a registered trademark for a brand of fibre cement. Fibre is often applied in building and construction materials, mainly in roofing and facade products.\n\nThe term \"cement\" originates from the Latin word \"Caementum\", which signifies chopped stone. Cement describes a binding substance, which will react chemically with water and develop into a material hard as stone.\nIn fibre cement there is a fibre reinforcement, which contributes to making the fibre-cement material even stronger and to better withstand tension. Together with a carefully planned production process, fibre cement makes it possible to develop strong and long-lasting construction materials.\n\nToday fibre cement is considered as a material physically suited for construction products such as cladding and roofing.\n\nFibre-reinforced cement-products were invented in the late 19th century by the Austrian Ludwig Hatschek. Principally he mixed 90% cement and 10% asbestos fibres with water and ran it through a cardboard machine. Originally, the fibres were of asbestos and the material was commonly used as siding in house buildings due to its low cost, fire-resistance, water tightness, light weight, and other useful properties. Asbestos is harmful to health and produces lung cancers years after professional or occasional exposure (asbestosis). Starting from the seventies, asbestos use became progressively prohibited; however, in several countries, it is still used for building materials. Safer fibre alternatives, based on e.g. cellulose fibers were developed in the eighties and applied to secure the widely known strength of fibre cement.\n\nFibre cement were probably amongst the latest materials on the market to have contained large quantities of asbestos. The reason is that the asbestos fibres are intimately bound to the cement matrix and were first considered to be well immobilized in the cement and less prone to be released in the environment, suspended in the air, and inhaled in the lung than in other materials or applications such as thermal insulation or flocking in which bare asbestos fibres were used. However, asbestos fibres are inevitably released during machining operations of the objects made of fibre-cement and by long-term erosion of the materials exposed to atmospheric weathering and wind when cement degrades. Occupational health concerns and the protection of workers in the fibre-cement factories have finally led to the progressive elimination of asbestos from these products. For health reasons, it is recommended that existing fibre-cement products that are in good condition are left undisturbed and possibly encapsulated, to prevent fibre release.\n\nFibre cement is a main component of long-lasting building materials. The main application areas are roofing and cladding. The list below gives some common applications. \n\nInternal cladding:\n\nExternal cladding:\n\nRoofing:\n\nFibre-cement products have found a wide usage in various sectors of construction: industrial, agricultural, domestic and residential buildings, mainly in roofing and cladding applications, for new constructions and refurbishment projects.\n\n"}
{"id": "1075005", "url": "https://en.wikipedia.org/wiki?curid=1075005", "title": "Exergy", "text": "Exergy\n\nIn thermodynamics, the exergy of a system is the maximum useful work possible during a process that brings the system into equilibrium with a heat reservoir. When the surroundings are the reservoir, exergy is the potential of a system to cause a change as it achieves equilibrium with its environment. Exergy is the energy that is available to be used. After the system and surroundings reach equilibrium, the exergy is zero. Determining exergy was also the first goal of thermodynamics. The term \"exergy\" was coined in 1956 by Zoran Rant (1904–1972) by using the Greek \"ex\" and \"ergon\" meaning \"from work\", but the concept was developed by J. Willard Gibbs in 1873.\n\nEnergy is neither created nor destroyed during a process. Energy changes from one form to another (\"see First Law of Thermodynamics\"). In contrast, exergy is always destroyed when a process is irreversible, for example loss of heat to the environment (\"see Second Law of Thermodynamics\"). This destruction is proportional to the entropy increase of the system together with its surroundings. The destroyed exergy has been called anergy. For an isothermal process, exergy and energy are interchangeable terms, and there is no anergy.\n\nExergy is a combination property of a system and its environment because it depends on the state of both the system and environment. The exergy of a system in equilibrium with the environment is zero. Exergy is neither a thermodynamic property of matter nor a thermodynamic potential of a system. Exergy and energy both have units of joules. The internal energy of a system is always measured from a fixed reference state and is therefore always a state function. Some authors define the exergy of the system to be changed when the environment changes, in which case it is not a state function. Other writers prefer a slightly alternate definition of the available energy or exergy of a system where the environment is firmly defined, as an unchangeable absolute reference state, and in this alternate definition exergy becomes a property of the state of the system alone.\n\nHowever, from a theoretical point of view, exergy may be defined without reference to any environment. If the intensive properties of different finitely extended elements of a system differ, there is always the possibility to extract mechanical work from the system.\n\nThe term exergy is also used, by analogy with its physical definition, in information theory related to reversible computing. Exergy is also synonymous with: \"availability\", \"available energy\", \"exergic energy\", \"essergy\" (considered archaic), \"utilizable energy\", \"available useful work\", \"maximum (or minimum) work\", \"maximum (or minimum) work content\", \"reversible work\", and \"ideal work\".\n\nThe exergy destruction of a cycle is the sum of the exergy destruction of the processes that compose that cycle. The exergy destruction of a cycle can also be determined without tracing the individual processed by considering the entire cycle as a single process and using one of the exergy destruction equations.\n\nExergy uses system boundaries in a way that is unfamiliar to many. We imagine the presence of a Carnot engine between the system and its reference environment even though this engine does not exist in the real world. Its only purpose is to measure the results of a \"what-if\" scenario to represent the most efficient work interaction possible between the system and its surroundings.\n\nIf a real-world reference environment is chosen that behaves like an unlimited reservoir that remains unaltered by the system, then Carnot's speculation about the consequences of a system heading towards equilibrium with time is addressed by two equivalent mathematical statements. Let \"B\", the exergy or available work, decrease with time, and \"S\", the entropy of the system and its reference environment enclosed together in a larger isolated system, increase with time:\n\nFor macroscopic systems (above the thermodynamic limit), these statements are both expressions of the second law of thermodynamics if the following expression is used for exergy:\n\nwhere the extensive quantities for the system are: \"U\" = Internal energy, \"V\" = Volume, and \"N\" = Moles of component \"i\"\n\nThe intensive quantities for the surroundings are: \"P\" = Pressure, \"T\" = temperature, \"μ\" = Chemical potential of component \"i\"\n\nIndividual terms also often have names attached to them: formula_3 is called \"available PV work\", formula_4 is called \"entropic loss\" or \"heat loss\" and the final term is called \"available chemical energy.\"\n\nOther thermodynamic potentials may be used to replace internal energy so long as proper care is taken in recognizing which natural variables correspond to which potential. For the recommended nomenclature of these potentials, see (Alberty, 2001). Equation (2) is useful for processes where system volume, entropy, and number of moles of various components change because internal energy is also a function of these variables and no others.\n\nAn alternative definition of internal energy does not separate available chemical potential from \"U\". This expression is useful (when substituted into equation (1)) for processes where system volume and entropy change, but no chemical reaction occurs:\n\nIn this case a given set of chemicals at a given entropy and volume will have a single numerical value for this thermodynamic potential. A multi-state system may complicate or simplify the problem because the Gibbs phase rule predicts that intensive quantities will no longer be completely independent from each other.\n\nIn 1848, William Thomson, 1st Baron Kelvin, asked (and immediately answered) the question\n\nWith the benefit of the hindsight contained in equation (3), we are able to understand the historical impact of Kelvin's idea on physics. Kelvin suggested that the best temperature scale would describe a constant ability for a unit of temperature in the surroundings to alter the available work from Carnot's engine. From equation (3):\n\nRudolf Clausius recognized the presence of a proportionality constant in Kelvin's analysis and gave it the name entropy in 1865 from the Greek for \"transformation\" because it describes the quantity of energy lost during transformation from heat to work. The available work from a Carnot engine is at its maximum when the surroundings are at a temperature of absolute zero.\n\nPhysicists then, as now, often look at a property with the word \"available\" or \"utilizable\" in its name with a certain unease. The idea of what is available raises the question of \"available to what?\" and raises a concern about whether such a property is anthropocentric. Laws derived using such a property may not describe the universe but instead describe what people wish to see.\n\nThe field of statistical mechanics (beginning with the work of Ludwig Boltzmann in developing the Boltzmann equation) relieved many physicists of this concern. From this discipline, we now know that macroscopic properties may all be determined from properties on a microscopic scale where entropy is more \"real\" than temperature itself (\"see Thermodynamic temperature\"). Microscopic kinetic fluctuations among particles cause entropic loss, and this energy is unavailable for work because these fluctuations occur randomly in all directions. The anthropocentric act is taken, in the eyes of some physicists and engineers today, when someone draws a hypothetical boundary, in fact he says: \"This is my system. What occurs beyond it is surroundings.\" In this context, exergy is sometimes described as an anthropocentric property, both by those who use it and those who don't. Entropy is viewed as a more fundamental property of matter.\n\nIn the field of ecology, the interactions among systems (mostly ecosystems) and their manipulation of exergy resources is of primary concern. With this perspective, the answer of \"available to what?\" is simply: \"available to the system\", because ecosystems appear to exist in the real world. With the viewpoint of systems ecology, a property of matter like absolute entropy is seen as anthropocentric because it is defined relative to an unobtainable hypothetical reference system in isolation at absolute zero temperature. With this emphasis on systems rather than matter, exergy is viewed as a more fundamental property of a system, and it is entropy that may be viewed as a co-property of a system with an idealized reference system.\n\nIn addition to formula_7 and formula_8, the other thermodynamic potentials are frequently used to determine exergy. For a given set of chemicals at a given entropy and pressure, enthalpy \"H\" is used in the expression:\n\nFor a given set of chemicals at a given temperature and volume, Helmholtz free energy \"A\" is used in the expression:\n\nFor a given set of chemicals at a given temperature and pressure, Gibbs free energy \"G\" is used in the expression:\n\nThe potentials \"A\" and \"G\" are utilized for a constant temperature process. In these cases, all energy is free to perform useful work because there is no entropic loss. A chemical reaction that generates electricity with no associated change in temperature will also experience no entropic loss. (\"See Fuel cell.\") This is true of every isothermal process. Examples are gravitational potential energy, kinetic energy (on a macroscopic scale), solar energy, electrical energy, and many others. If friction, absorption, electrical resistance or a similar energy conversion takes place that releases heat, the impact of that heat on thermodynamic potentials must be considered, and it is this impact that decreases the available energy.\n\nSimilar to thermomechanical exergy, chemical exergy depends on the temperature and pressure of a system as well as on the composition. The key difference in evaluating chemical exergy versus thermomechanical exergy is that thermomechanical exergy does not take into account the difference in a system and environment's chemical composition. If the temperature, pressure or composition of a system differs from the environment's state, then the overall system will have exergy.\n\nThe definition of chemical exergy resembles the standard definition of thermomechanical exergy, but with a few differences. Chemical exergy is defined as the maximum work that can be obtained when the considered system is brought into reaction with reference substances present in the environment. Defining the exergy reference environment is one of the most vital parts of analyzing chemical exergy. In general, the environment is defined as the composition of air at 25 °C and 1 atm of pressure. At these properties air consists of N=75.67%, O=20.35%, HO(g)=3.12%, CO=0.03% and other gases=0.83%. These molar fractions will become of use when applying Equation 8 below.\n\nCHO is the substance that is entering a system that one wants to find the maximum theoretical work of. By using the following equations, one can calculate the chemical exergy of the substance in a given system. Below, Equation 8 uses the Gibbs function of the applicable element or compound to calculate the chemical exergy. Equation 9 is similar but uses standard molar chemical exergy, which scientists have determined based on several criteria, including the ambient temperature and pressure that a system is being analyzed and the concentration of the most common components. These values can be found in thermodynamic books or in online tables.\n\nformula_12\n\nwhere:\n\nformula_13 Gibbs function of the specific substance in the system at\nformula_14. (formula_15 refers to the substance that is\nentering the system)\n\nformula_16 The Universal gas constant (8.314462 J/mol•K)\n\nformula_17 Temperature that the system is being evaluated at in absolute\ntemperature\n\nformula_18 The molar fraction of the given substance in the environment\ni.e. air\n\nformula_19\n\nwhere:\n\nformula_20 The standard molar chemical exergy taken from a table\nfor the specific conditions that the system is being evaluated\n\nEquation 9 is more commonly used due to the simplicity of only having to look up the standard chemical exergy for given substances. Using a standard table works well for most cases, even if the environment conditions vary slightly, the difference is most likely negligible.\n\nAfter finding the chemical exergy in a given system, one can find the total exergy by adding it to the thermomechanical exergy. Depending on the situation, the amount of chemical exergy added can be very small. If the system being evaluated involves combustion, the amount of chemical exergy is very large and necessary to find the total exergy of the system.\n\nIrreversibility accounts for the amount of exergy destroyed in a closed system, or in other words, the wasted work potential. For highly efficient systems, the value of I, is low, and vice versa. The equation to calculate the Irreversibility of as closed system, as it relates to the exergy of that system, is as follows:\n\nwhere: formula_22 is the entropy generated by the system processes.\n\nIf formula_23 then there are irreversibilities present in the system.\nIf formula_24 then there are no irreversibilities present in the system.\n\nThe value of I, the irreversibility, can not be negative, as it is not a property. On the contrary, the availability is a different story, which is a property of the system.\n\nExergy analysis is based on the relation between the actual work and the maximal work, that could be obtained in the reversible process:\n\nThe first term at the right part is related with the difference in exergy at inlet and outlet of the system:\n\nFor an Isolated System:\n\nNo heat or work interactions with the surroundings occur, and therefore, there are no transfers of availability between the system and its surroundings. The change in exergy of an isolated system is equivalent, but opposite the value for irreversibility of that system.\n\nApplying equation (1) to a subsystem yields:\n\nThis expression applies equally well for theoretical ideals in a wide variety of applications: electrolysis (decrease in \"G\"), galvanic cells and fuel cells (increase in \"G\"), explosives (increase in \"A\"), heating and refrigeration (exchange of \"H\"), motors (decrease in \"U\") and generators (increase in \"U\").\n\nUtilization of the exergy concept often requires careful consideration of the choice of reference environment because, as Carnot knew, unlimited reservoirs do not exist in the real world. A system may be maintained at a constant temperature to simulate an unlimited reservoir in the lab or in a factory, but those systems cannot then be isolated from a larger surrounding environment. However, with a proper choice of system boundaries, a reasonable constant reservoir can be imagined. A process sometimes must be compared to \"the most realistic impossibility,\" and this invariably involves a certain amount of guesswork.\n\nApplication of exergy to unit operations in chemical plants was partially responsible for the huge growth of the chemical industry during the 20th century. During this time it was usually called \"availability\" or \"available work\".\n\nAs a simple example of exergy, air at atmospheric conditions of temperature, pressure, \"and composition\" contains energy but no exergy when it is chosen as the thermodynamic reference state known as ambient. Individual processes on Earth such as combustion in a power plant often eventually result in products that are incorporated into the atmosphere, so defining this reference state for exergy is useful even though the atmosphere itself is not at equilibrium and is full of long and short term variations.\n\nIf standard ambient conditions are used for calculations during chemical plant operation when the actual weather is very cold or hot, then certain parts of a chemical plant might seem to have an exergy efficiency of greater than 100% and without taking into account the non-standard atmospheric temperature variation can give an impression of being a perpetual motion machine. Using actual conditions will give actual values, but standard ambient conditions are useful for initial design calculations.\n\nOne goal of energy and exergy methods in engineering is to compute what comes into and out of several possible designs before a factory is built. Energy input and output will always balance according to the First Law of Thermodynamics or the energy conservation principle. Exergy output will not balance the exergy input for real processes since a part of the exergy input is always destroyed according to the Second Law of Thermodynamics for real processes. After the input and output are completed, the engineer will often want to select the most efficient process. An energy efficiency or \"first law efficiency\" will determine the most efficient process based on wasting as little energy as possible relative to energy inputs. An exergy efficiency or \"second-law efficiency\" will determine the most efficient process based on wasting \"and destroying\" as little available work as possible from a given input of available work.\n\nIn recent decades, utilization of exergy has spread outside of physics and engineering to the fields of industrial ecology, ecological economics, systems ecology, and energetics. Defining where one field ends and the next begins is a matter of semantics, but applications of exergy can be placed into rigid categories.\n\nResearchers in ecological economics and environmental accounting perform exergy-cost analyses in order to evaluate the impact of human activity on the current natural environment. As with ambient air, this often requires the unrealistic substitution of properties from a natural environment in place of the reference state environment of Carnot. For example, ecologists and others have developed reference conditions for the ocean and for the Earth's crust. Exergy values for human activity using this information can be useful for comparing policy alternatives based on the efficiency of utilizing natural resources to perform work. Typical questions that may be answered are:\n\nThere has been some progress in standardizing and applying these methods.\n\nMeasuring exergy requires the evaluation of a system’s reference state environment. With respect to the applications of exergy on natural resource utilization, the process of quantifying a system requires the assignment of value (both utilized and potential) to resources that are not always easily dissected into typical cost-benefit terms. However, to fully realize the potential of a system to do work, it is becoming increasingly imperative to understand exergetic potential of natural resources, and how human interference alters this potential.\n\nReferencing the inherent qualities of a system in place of a reference state environment is the most direct way that ecologists determine the exergy of a natural resource. Specifically, it is easiest to examine the thermodynamic properties of a system, and the reference substances that are acceptable within the reference environment. This determination allows for the assumption of qualities in a natural state: deviation from these levels may indicate a change in the environment caused by outside sources. There are three kinds of reference substances that are acceptable, due to their proliferation on the planet: gases within the atmosphere, solids within the Earth’s crust, and molecules or ions in seawater. By understanding these basic models, it’s possible to determine the exergy of multiple earth systems interacting, like the effects of solar radiation on plant life. These basic categories are utilized as the main components of a reference environment when examining how exergy can be defined through natural resources.\n\nOther qualities within a reference state environment include temperature, pressure, and any number of combinations of substances within a defined area. Again, the exergy of a system is determined by the potential of that system to do work, so it is necessary to determine the baseline qualities of a system before it is possible to understand the potential of that system. The thermodynamic value of a resource can be found by multiplying the exergy of the resource by the cost of obtaining the resource and processing it.\n\nToday, it is becoming increasingly popular to analyze the environmental impacts of natural resource utilization, especially for energy usage. To understand the ramifications of these practices, exergy is utilized as a tool for determining the impact potential of emissions, fuels, and other sources of energy. Combustion of fossil fuels, for example, is examined with respect to assessing the environmental impacts of burning coal, oil, and natural gas. The current methods for analyzing the emissions from these three products can be compared to the process of determining the exergy of the systems affected; specifically, it is useful to examine these with regard to the reference state environment of gases within the atmosphere. In this way, it is easier to determine how human action is affecting the natural environment.\n\nIn systems ecology, researchers sometimes consider the exergy of the current formation of natural resources from a small number of exergy inputs (usually solar radiation, tidal forces, and geothermal heat). This application not only requires assumptions about reference states, but it also requires assumptions about the real environments of the past that might have been close to those reference states. Can we decide which is the most \"realistic impossibility\" over such a long period of time when we are only speculating about the reality?\n\nFor instance, comparing oil exergy to coal exergy using a common reference state would require geothermal exergy inputs to describe the transition from biological material to fossil fuels during millions of years in the Earth's crust, and solar radiation exergy inputs to describe the material's history before then when it was part of the biosphere. This would need to be carried out mathematically backwards through time, to a presumed era when the oil and coal could be assumed to be receiving the same exergy inputs from these sources. A speculation about a past environment is different from assigning a reference state with respect to known environments today. Reasonable guesses about real ancient environments may be made, but they are untestable guesses, and so some regard this application as pseudoscience or pseudo-engineering.\n\nThe field describes this accumulated exergy in a natural resource over time as embodied energy with units of the \"embodied joule\" or \"emjoule\".\n\nThe important application of this research is to address sustainability issues in a quantitative fashion through a sustainability measurement:\n\nA technique proposed by systems ecologists is to consolidate the three exergy inputs described in the last section into the single exergy input of solar radiation, and to express the total input of exergy into an economic good as a \"solar embodied joule\" or \"sej\". (\"See Emergy\") Exergy inputs from solar, tidal, and geothermal forces all at one time had their origins at the beginning of the solar system under conditions which could be chosen as an initial reference state, and other speculative reference states could in theory be traced back to that time. With this tool we would be able to answer:\n\nNo additional thermodynamic laws are required for this idea, and the principles of energetics may confuse many issues for those outside the field. The combination of untestable hypotheses, unfamiliar jargon that contradicts accepted jargon, intense advocacy among its supporters, and some degree of isolation from other disciplines have contributed to this protoscience being regarded by many as a pseudoscience. However, its basic tenets are only a further utilization of the exergy concept.\n\nA common hypothesis in systems ecology is that the design engineer's observation that a greater capital investment is needed to create a process with increased exergy efficiency is actually the economic result of a fundamental law of nature. By this view, exergy is the analogue of economic currency in the natural world. The analogy to capital investment is the accumulation of exergy into a system over long periods of time resulting in embodied energy. The analogy of capital investment resulting in a factory with high exergy efficiency is an increase in natural organizational structures with high exergy efficiency. (\"See Maximum power\"). Researchers in these fields describe biological evolution in terms of increases in organism complexity due to the requirement for increased exergy efficiency because of competition for limited sources of exergy.\n\nSome biologists have a similar hypothesis. A biological system (or a chemical plant) with a number of intermediate compartments and intermediate reactions is more efficient because the process is divided up into many small substeps, and this is closer to the reversible ideal of an infinite number of infinitesimal substeps. Of course, an excessively large number of intermediate compartments comes at a capital cost that may be too high.\n\nTesting this idea in living organisms or ecosystems is impossible for all practical purposes because of the large time scales and small exergy inputs involved for changes to take place. However, if this idea is correct, it would not be a new fundamental law of nature. It would simply be living systems and ecosystems maximizing their exergy efficiency by utilizing laws of thermodynamics developed in the 19th century.\n\nSome proponents of utilizing exergy concepts describe them as a biocentric or ecocentric alternative for terms like quality and value. The \"deep ecology\" movement views economic usage of these terms as an anthropocentric philosophy which should be discarded. A possible universal thermodynamic concept of value or utility appeals to those with an interest in monism.\n\nFor some, the end result of this line of thinking about tracking exergy into the deep past is a restatement of the cosmological argument that the universe was once at equilibrium and an input of exergy from some First Cause created a universe full of available work. Current science is unable to describe the first 10 seconds of the universe (\"See Timeline of the Big Bang\"). An external reference state is not able to be defined for such an event, and (regardless of its merits), such an argument may be better expressed in terms of entropy.\n\nThe ratio of exergy to energy in a substance can be considered a measure of energy quality. Forms of energy such as macroscopic kinetic energy, electrical energy, and chemical Gibbs free energy are 100% recoverable as work, and therefore have an exergy equal to their energy. However, forms of energy such as radiation and thermal energy can not be converted completely to work, and have exergy content less than their energy content. The exact proportion of exergy in a substance depends on the amount of entropy relative to the surrounding environment as determined by the Second Law of Thermodynamics.\n\nExergy is useful when measuring the efficiency of an energy conversion process. The exergetic, or 2nd Law, efficiency is a ratio of the exergy output divided by the exergy input. This formulation takes into account the quality of the energy, often offering a more accurate and useful analysis than efficiency estimates only using the First Law of Thermodynamics.\n\nWork can be extracted also from bodies colder than the surroundings. When the flow of energy is coming into the body, work is performed by this energy obtained from the large reservoir, the surrounding. A quantitative treatment of the notion of energy quality rests on the definition of energy. According to the standard definition, Energy is a measure of the ability to do work. Work can involve the movement of a mass by a force that results from a transformation of energy. If there is an energy transformation, the second principle of energy flow transformations says that this process must involve the dissipation of some energy as heat. Measuring the amount of heat released is one way of quantifying the energy, or ability to do work and apply a force over a distance.\n\nMaximal possible conversion of heat to work, or exergy content of heat, depends on the temperature at which heat is available and the temperature level at which the reject heat can be disposed, that is the temperature of the surrounding. The upper limit for conversion is known as Carnot efficiency and was discovered by Nicolas Léonard Sadi Carnot in 1824. See also Carnot heat engine.\n\nCarnot efficiency is\n\nwhere \"T\" is the higher temperature and \"T\" is the lower temperature, both as absolute temperature. From Equation 15 it is clear that in order to maximize efficiency one should maximize \"T\" and minimize \"T\".\n\nExergy exchanged is then:\n\nwhere \"T\" is the temperature of the heat source, and \"T\" is the temperature of the surrounding.\n\nExergy in a sense can be understood as a measure of value of energy. Since high-exergy energy carriers can be used in for more versatile purposes, due to their ability to do more work, they can be postulated to hold more economic value. This can be seen in prices of energy carriers, i.e. high-exergy energy carriers such as electricity tend to be more valuable than low-exergy ones such as various fuels or heat. This has led to substitution of more valuable high-exergy energy carriers with low-exergy energy carriers, when possible. An example is heating systems, where higher investment to heating systems allows using low-exergy energy sources. Thus high-exergy content is being substituted with capital investments.\n\nExergy of a system is the maximum useful work possible during a process that brings the system into equilibrium with a heat reservoir. Wall clearly states the relation between exergy analysis and resource accounting. This intuition confirmed by DeWulf Sciubba lead to exergo-economic accounting and to methods specifically dedicated to LCA such as exergetic material input per unit of service (EMIPS). The\nconcept of material input per unit of service (MIPS) is quantified in terms of the second law of thermodynamics, allowing the calculation of both resource input and service output in exergy terms. This exergetic material input per unit of service (EMIPS) has been elaborated for transport technology. The service not only takes into account the total mass to be transported\nand the total distance, but also the mass per single transport and the delivery time. The applicability of the EMIPS methodology relates specifically to transport system and allows an effective coupling with life cycle assessment.\n\nIn 1824, Sadi Carnot studied the improvements developed for steam engines by James Watt and others. Carnot utilized a purely theoretical perspective for these engines and developed new ideas. He wrote:\n\nThe question has often been raised whether the motive power of heat is unbounded, whether the possible improvements in steam engines have an assignable limit—a limit by which the nature of things will not allow to be passed by any means whatever... In order to consider in the most general way the principle of the production of motion by heat, it must be considered independently of any mechanism or any particular agent. It is necessary to establish principles applicable not only to steam-engines but to all imaginable heat-engines... The production of motion in steam-engines is always accompanied by a circumstance on which we should fix our attention. This circumstance is the re-establishing of equilibrium… Imagine two bodies A and B, kept each at a constant temperature, that of A being higher than that of B. These two bodies, to which we can give or from which we can remove the heat without causing their temperatures to vary, exercise the functions of two unlimited reservoirs...\n\nCarnot next described what is now called the Carnot engine, and proved by a thought experiment that any heat engine performing better than this engine would be a perpetual motion machine. Even in the 1820s, there was a long history of science forbidding such devices. According to Carnot, \"Such a creation is entirely contrary to ideas now accepted, to the laws of mechanics and of sound physics. It is inadmissible.\"\n\nThis description of an upper bound to the work that may be done by an engine was the earliest modern formulation of the second law of thermodynamics. Because it involves no mathematics, it still often serves as the entry point for a modern understanding of both the second law and entropy. Carnot's focus on heat engines, equilibrium, and heat reservoirs is also the best entry point for understanding the closely related concept of exergy.\n\nCarnot believed in the incorrect caloric theory of heat that was popular during his time, but his thought experiment nevertheless described a fundamental limit of nature. As kinetic theory replaced caloric theory through the early and mid-19th century (\"see Timeline of thermodynamics\"), several scientists added mathematical precision to the first and second laws of thermodynamics and developed the concept of entropy. Carnot's focus on processes at the human scale (above the thermodynamic limit) led to the most universally applicable concepts in physics. Entropy and the second-law are applied today in fields ranging from quantum mechanics to physical cosmology.\n\nIn the 1870s, Josiah Willard Gibbs unified a large quantity of 19th century thermochemistry into one compact theory. Gibbs's theory incorporated the new concept of a chemical potential to cause change when distant from a chemical equilibrium into the older work begun by Carnot in describing thermal and mechanical equilibrium and their potentials for change. Gibbs's unifying theory resulted in the thermodynamic potential state functions describing differences from thermodynamic equilibrium.\n\nIn 1873, Gibbs derived the mathematics of \"available energy of the body and medium\" into the form it has today. (See the equations above). The physics describing exergy has changed little since that time.\n\n\n\n\n"}
{"id": "6479887", "url": "https://en.wikipedia.org/wiki?curid=6479887", "title": "FCC Environment", "text": "FCC Environment\n\nFCC Environment (UK) Limited is a waste management company headquartered in Northampton, United Kingdom and a wholly owned subsidiary of Fomento de Construcciones y Contratas. It was formed in May 2012 through the merger and rebranding of Focsa Services and Waste Recycling Group.\n\nWaste Recycling Group acquired Hanson Waste Management from Hanson plc for £185 million in cash in December 2000.\n\nWaste Recycling Group was acquired by the private equity group Terra Firma Capital Partners for £315.2 million in June 2003.\n\nIn September 2006 Fomento de Construcciones y Contratas acquired Waste Recycling Group, excluding its landfill gas operations, from Terra Firma for £1.4 billion.\n\nIn May 2012 Focsa Services and Waste Recycling Group were merged and rebranded \"FCC Environment\".\n\nFCC Environment's principal services are:\n\n\nFCC Environment's head office is in Northampton with a major regional office located in Doncaster. FCC Environment has around 2,400 staff and operates around 200 facilities across England, Scotland and Wales.\n\n\n"}
{"id": "1059742", "url": "https://en.wikipedia.org/wiki?curid=1059742", "title": "Fire brick", "text": "Fire brick\n\nA fire brick, firebrick, or refractory brick is a block of refractory ceramic material used in lining furnaces, kilns, fireboxes, and fireplaces. A refractory brick is built primarily to withstand high temperature, but will also usually have a low thermal conductivity for greater energy efficiency. Usually dense firebricks are used in applications with extreme mechanical, chemical, or thermal stresses, such as the inside of a wood-fired kiln or a furnace, which is subject to abrasion from wood, fluxing from ash or slag, and high temperatures. In other, less harsh situations, such as in an electric or natural gas fired kiln, more porous bricks, commonly known as \"kiln bricks\" are a better choice. They are weaker, but they are much lighter, easier to form, and insulate far better than dense bricks. In any case, firebricks should not spall, and their strength should hold up well during rapid temperature changes.\n\nIn the making of firebrick, fireclay is fired in the kiln until it is partly vitrified, and for special purposes may also be glazed. There are two standard sizes of fire-brick; one is 9×4½×3 inches (229×114×76 mm) and the other is 9×4½×2½ inches (229×114×64 mm). Also available are firebrick “splits” which are half the thickness and are often used to line wood stoves and fireplace inserts. The dimensions of a split are usually 9×4½×1¼ inches (229×114×32 mm). Fire brick was first invented in 1822 by William Weston Young in the Neath Valley of Wales.\n\nFire bricks have an aluminium oxide content that can be as high as 50–80% (with correspondingly less silica).\n\nThe silica firebricks that line steel-making furnaces are used at temperatures up to 1648°C (3000°F), which would melt many other types of ceramic, and in fact part of the silica firebrick liquefies. High-temperature Reusable Surface Insulation (HRSI), a material with the same composition, was used in the insulating tiles of the Space Shuttle.\n\nNon-ferrous metallurgical processes use \"basic\" refractory bricks because the slags used in these processes readily dissolve the “acidic” silica bricks. The most common basic refractory bricks used in smelting non-ferrous metal concentrates are “chrome-magnesite” or “magnesite-chrome” bricks (depending on the relative ratios of magnesite and chromite ores used in their manufacture).\n\nA range of other materials find use as firebricks for lower temperature applications. Magnesium oxide is often used as a lining for furnaces. Silica bricks are the most common type of bricks used for the inner lining of furnaces and incinerators. As the inner lining is usually of sacrificial nature, fire bricks of higher alumina content may be employed to lengthen the duration between re-linings. Very often cracks can be seen in this sacrificial inner lining shortly after being put into operation. They revealed more expansion joints should have been put in the first place, but these now become expansion joints themselves and are of no concern as long as structural integrity is not affected. Silicon carbide, with high abrasive strength, is a popular material for hearths of incinerators and cremators. Common red clay brick are used for chimneys and wood-fired ovens.\n\n"}
{"id": "35531907", "url": "https://en.wikipedia.org/wiki?curid=35531907", "title": "Flottweg", "text": "Flottweg\n\nFlottweg SE is a manufacturer of machines and systems for mechanical liquid-solid separation. The headquarters is located in Vilsbiburg (Bavaria), Germany. The company develops and produces decanter centrifuges, separators and belt presses. Flottweg has subsidiaries and branch offices with service centers in the United States (Flottweg Separation Technology, Inc.), People´s Republic of China, Russia, Italy, Poland, France, Australia and Mexico.\nIn 1910 Gustav Otto (son of the inventor of the gasoline engines) founded Otto-Flugzeugwerke (Gustav Otto Aircraft Machine Works) based in Munich, Germany. On March 7, 1916, this company was registered as the Bayrische Flugzeugwerke (Bavarian Aircraft Works), the precursor to Bayrische Motorenwerke (Bayerischen Motorenwerke“ or \"BMW\"). In 1918 Otto opened a new factory in Munich, and began in 1920 to produce motorized bicycles, which were given the brand name \"Flottweg\" as the German words \"flott\" means \"quick\" and \"weg\" means \"away\". In 1932' Dr. Georg Bruckmayer acquired the Flottweg name and founded the engine factory \"Flottweg-Motoren-Werke\". In 1933, the company began manufacturing and distributing motorcycles and aircraft engines in Munich.\n\nIn 1943 World War II forced Flottweg to move to Vilsbiburg. In 1946, the company began to manufacture specialized equipment for the printing industry (until 1984).\n\nIn 1953 Flottweg expanded its into a business sector: the production of liquid-solid separation machines. The first product was the Flottweg Decanter, a solid bowl decanter centrifuge; the first model, DECANTER Z1, was sold to BASF. Further development lead to a higher speed and more efficient decanter in 1964. In 1966 an adjustable impeller for decanter was added, which allowed the operator to optimise the pond depth while the decanter was operating; this was useful in the extraction of olive oil. After further development, Flottweg sold a 3-phase decanter for olive oil in Spain in 1971. In 1977 Flottweg began to manufacture belt filter presses.\n\nBy 1988 Krauss-Maffei, Munich, was the main shareholder, owning 90% of Flottweg. In 1989, under license agreement, the Krauss-Maffei product line of decanter centrifuges was procured by Flottweg. In the same year, Flottweg opened new sales and service offices in France and Russia, and in the next few years sales and service centers in Düsseldorf and Leipzig, Germany were opened.\n\nIn 1998 Flottweg acquired Veronesi, an Italian manufacturer of disc stack centrifuges. In the same year, Flottweg began to develop and produce disc stack machines at Vilsbiburg.\n\nIn 2007, Flottweg Separation Technology, Inc. was founded in Independence, Kentucky . In 2012 , Flottweg was converted to Flottweg SE, legally registered in the Societas Europaea (also called \"Europe AG\") in the European Union.\n\nIn 2015, Flottweg produces decanter centrifuges, which are used for continuous separation of solids from liquids, clarification of liquids, and classification of fine pigments. These are used to separate municipal and industrial wastewaters , in the manufacture of plastics, in extraction and processing of animal and vegetable raw materials, pclarification of drinks, in the mining and processing industry, and in the processing of biofuels.\n\nFlottweg also produces separators (disc stack centrifuges), which are used in the food and beverage, chemical, pharmaceutical and petroleum industries. The company also makes belt presses, used to produce fruit and vegetable juices, dewater spent grains, create algae and herbal extracts, and processing of coffee grounds and industrial sludge.\n\n"}
{"id": "19355364", "url": "https://en.wikipedia.org/wiki?curid=19355364", "title": "Fântânele Power Station", "text": "Fântânele Power Station\n\nThe Fântânele Power Station is a large thermal power plant located in Mureş County having 5 generation groups of 50 MW having a total electricity generation capacity of 250 MW.\n\n"}
{"id": "13160", "url": "https://en.wikipedia.org/wiki?curid=13160", "title": "Gelatin", "text": "Gelatin\n\nGelatin or gelatine (from meaning \"stiff\" or \"frozen\") is a translucent, colorless, brittle (when dry), flavorless food ingredient that is derived from collagen obtained from various animal body parts. It is also referred to as hydrolyzed collagen, collagen hydrolysate, gelatine hydrolysate, hydrolyzed gelatine, and collagen peptides. It is commonly used as a gelling agent in food, medications, drug and vitamin capsules, photographic films and papers, and cosmetics.\n\nSubstances containing gelatin or functioning in a similar way are called \"gelatinous\". Gelatin is an irreversibly hydrolyzed form of collagen, wherein the hydrolysis results in the reduction of protein fibrils into smaller peptides, which will have broad molecular weight ranges associated with physical and chemical methods of denaturation, based on the process of hydrolysis. It is found in most gummy candy, as well as other products such as marshmallows, gelatin desserts, and some ice creams, dips, and yogurts. Gelatin for recipe use comes in the form of powder, granules, or sheets. Instant types can be added to the food as they are; others need to be soaked in water beforehand.\n\nHydrolyzed collagen is produced from collagen found in the bones, skin, and connective tissue of animals. The process of hydrolysis involves breaking down the molecular bonds between individual collagen strands and peptides using combinations of physical, chemical or biological means. Typically, with skin-sourced collagen (Type-I collagens), hides are put in a lime slurry pit for up to 3 months, loosening collagen bonds; the hides are then washed to remove lime, and the collagen extracted in boiling water. The extracted collagen is evaporator concentrated, desiccated with drum driers, and pulverized.\n\nHydrolysis results in the reduction of collagen protein fibrils of about 300,000 Da into smaller peptides. Depending upon the process of hydrolysis, peptides will have broad molecular weight ranges associated with physical and chemical methods of denaturation.\n\nThe amino acid content of hydrolyzed collagen is the same as collagen. Hydrolyzed collagen contains 19 amino acids, predominantly glycine, proline and hydroxyproline, which together represent around 50% of the total amino acid content.\n\nHydrolyzed collagen contains 8 out of 9 essential amino acids, including glycine and arginine—two amino-acid precursors necessary for the biosynthesis of creatine. It contains no tryptophan and is deficient in isoleucine, threonine, and methionine.\n\nThe bioavailability of hydrolyzed collagen in mice was demonstrated in a 1999 study; orally administered C hydrolyzed collagen was digested and more than 90% absorbed within 6 hours, with measurable accumulation in cartilage and skin.\nA 2005 study in humans found hydrolyzed collagen absorbed as small peptides in the blood.\n\nIngestion of hydrolyzed collagen may affect the skin by increasing the density of collagen fibrils and fibroblasts, thereby stimulating collagen production. It has been suggested, based on mouse and in vitro studies, that hydrolyzed collagen peptides have chemotactic properties on fibroblasts or an influence on growth of fibroblasts.\n\nSome clinical studies report that the oral ingestion of hydrolyzed collagen decreases joint pain, those with the most severe symptoms showing the most benefit. Beneficial action is likely due to hydrolyzed collagen accumulation in the cartilage and stimulated production of collagen by the chondrocytes, the cells of cartilage. Several studies have shown that a daily intake of hydrolyzed collagen increases bone mass density in rats. It seems that hydrolyzed collagen peptides stimulated differentiation and osteoblasts activity- the cells that build bone- over that of osteoclasts (cells that destroy bone).\n\nHowever, other clinical trials have yielded mixed results. In 2011, the European Food Safety Authority Panel on Dietetic Products, Nutrition and Allergies concluded that \"a cause and effect relationship has not been established between the consumption of collagen hydrolysate and maintenance of joints\". Four other studies reported benefit with no side effects; however, the studies were not extensive, and all recommended further controlled study. One study found that oral collagen only improved symptoms in a minority of patients and reported nausea as a side effect. Another study reported no improvement in disease activity in patients with rheumatoid arthritis. Another study found that collagen treatment may actually cause an exacerbation of rheumatoid arthritis symptoms.\n\nHydrolyzed collagen, like gelatin, is made from animal by-products from the meat industry, including skin, bones, and connective tissue.\n\nThe U.S. Food and Drug Administration (FDA), with support from the TSE (Transmissible spongiform encephalopathy) Advisory Committee, has since 1997 been monitoring the potential risk of transmitting animal diseases, especially bovine spongiform encephalopathy (BSE). The FDA study concluded: \"...steps such as heat, alkaline treatment, and filtration could be effective in reducing the level of contaminating TSE agents; however, scientific evidence is insufficient at this time to demonstrate that these treatments would effectively remove the BSE infectious agent if present in the source material.\"\n\nIn cosmetics, hydrolyzed collagen may be found in topical creams, acting as a product texture conditioner, and moisturizer.\n\nGelatin is a mixture of peptides and proteins produced by partial hydrolysis of collagen extracted from the skin, bones, and connective tissues of animals such as domesticated cattle, chicken, pigs, and fish. During hydrolysis, the natural molecular bonds between individual collagen strands are broken down into a form that rearranges more easily. Its chemical composition is, in many aspects, closely similar to that of its parent collagen. Photographic and pharmaceutical grades of gelatin generally are sourced from cattle bones and pig skin. Gelatin has proline, hydroxyproline and glycine in its polypeptide chain. Glycine is responsible for close packing of the chains. Presence of proline restricts the conformation. This is important for gelation properties of gelatin.\n\nGelatin readily dissolves in hot water and sets to a gel on cooling. When added directly to cold water, it does not dissolve well, however. Gelatin also is soluble in most polar solvents. Gelatin solutions show viscoelastic flow and streaming birefringence. Solubility is determined by the method of manufacture. Typically, gelatin can be dispersed in a relatively concentrated acid. Such dispersions are stable for 10–15 days with little or no chemical changes and are suitable for coating purposes or for extrusion into a precipitating bath.\n\nThe mechanical properties of gelatin gels are very sensitive to temperature variations, the previous thermal history of the gels, and the amount of time elapsing. These gels exist over only a small temperature range, the upper limit being the melting point of the gel, which depends on gelatin grade and concentration, but typically, is less than and the lower limit the freezing point at which ice crystallizes. The upper melting point is below human body temperature, a factor that is important for mouthfeel of foods produced with gelatin. The viscosity of the gelatin-water mixture is greatest when the gelatin concentration is high and the mixture is kept cool at about . The gel strength is quantified using the Bloom test. Gelatin's strength (but not viscosity) declines if it is subjected to temperatures above , or if it is held at temperatures near 100 °C for an extended period of time.\n\nThe worldwide production amount of gelatin is about . On a commercial scale, gelatin is made from by-products of the meat and leather industries.\nMost gelatin is derived from pork skins, pork and cattle bones, or split cattle hides. Gelatin made from fish by-products avoids some of the religious objections to gelatin consumption. The raw materials are prepared by different curing, acid, and alkali processes that are employed to extract the dried collagen hydrolysate. These processes may take several weeks, and differences in such processes have great effects on the properties of the final gelatin products.\n\nGelatin also can be prepared at home. Boiling certain cartilaginous cuts of meat or bones results in gelatin being dissolved into the water. Depending on the concentration, the resulting stock (when cooled) will form a jelly or gel naturally. This process is used for aspic.\n\nWhile many processes exist whereby collagen may be converted to gelatin, they all have several factors in common. The intermolecular and intramolecular bonds that stabilize insoluble collagen must be broken, and also, the hydrogen bonds that stabilize the collagen helix must be broken. The manufacturing processes of gelatin consists of three main stages:\n\nIf the raw material used in the production of the gelatin is derived from bones, dilute acid solutions are used to remove calcium and other salts. Hot water or several solvents may be used to reduce the fat content, which should not exceed 1% before the main extraction step. If the raw material consists of hides and skin; size reduction, washing, removal of hair from hides, and degreasing are necessary to prepare the hides and skins for the main extraction step.\n\nCollagen hydrolysis is performed by one of three different methods: acid-, alkali-, and enzymatic hydrolysis. Acid treatment is especially suitable for less fully cross-linked materials such as pig skin collagen and normally requires 10 to 48 hours. Alkali treatment is suitable for more complex collagen such as that found in bovine hides and requires more time, normally several weeks. The purpose of the alkali treatment is to destroy certain chemical crosslinks still present in collagen. Within the gelatin industry, the gelatin obtained from acid-treated raw material has been called type-A gelatin and the gelatin obtained from alkali-treated raw material is referred to as type-B gelatin.\n\nAdvances are occurring to optimize the yield of gelatin using enzymatic hydrolysis of collagen. The treatment time is shorter than that required for alkali treatment, however, and results in almost complete conversion to the pure product. The physical properties of the final gelatin product are considered better.\n\nAfter preparation of the raw material, i.e., reducing cross-links between collagen components and removing some of the impurities such as fat and salts, partially purified collagen is converted into gelatin by extraction with either water or acid solutions at appropriate temperatures. All industrial processes are based on neutral or acid pH values because although alkali treatments speed up conversion, they also promote degradation processes. Acidic extraction conditions are extensively used in the industry, but the degree of acid varies with different processes. This extraction step is a multistage process, and the extraction temperature usually is increased in later extraction steps, which ensures minimum thermal degradation of the extracted gelatin.\n\nThis process includes several steps such as filtration, evaporation, drying, grinding, and sifting. These operations are concentration-dependent and also dependent on the particular gelatin used. Gelatin degradation should be avoided and minimized, so the lowest temperature possible is used for the recovery process. Most recoveries are rapid, with all of the processes being done in several stages to avoid extensive deterioration of the peptide structure. A deteriorated peptide structure would result in a low gel strength, which is not generally desired.\n\nThe first use of gelatin in foods is attributed to medieval Britain (1400s) when cattle hooves were boiled to produce a gel. Further commercial development occurred in 1754 when a British manufacturing patent was issued. Food applications in France and the United States during 1800–1900 appear to have established the versatility of gelatin, including the origin of its popularity in the US as Jell-O. Over middle-late 1800s, Charles and Rose Knox of New York manufactured and marketed gelatin powder, diversifying the appeal and applications of gelatin.\n\nProbably best known as a gelling agent in cooking, different types and grades of gelatin are used in a wide range of food and nonfood products. Common examples of foods that contain gelatin are gelatin desserts, trifles, aspic, marshmallows, candy corn, and confections such as Peeps, gummy bears, fruit snacks, and jelly babies. Gelatin may be used as a stabilizer, thickener, or texturizer in foods such as yogurt, cream cheese, and margarine; it is used, as well, in fat-reduced foods to simulate the mouthfeel of fat and to create volume. It also is used in the production of several types of Chinese soup dumplings, specifically Shanghainese soup dumplings, or \"xiaolongbao\", as well as \"Shengjian mantou\", a type of fried and steamed dumpling. The fillings of both are made by combining ground pork with gelatin cubes, and in the process of cooking, the gelatin melts, creating a soupy interior with a characteristic gelatinous stickiness.\n\nGelatin is used for the clarification of juices, such as apple juice, and of vinegar.\n\nIsinglass is obtained from the swim bladders of fish. It is used as a fining agent for wine and beer. Besides hartshorn jelly, from deer antlers (hence the name \"hartshorn\"), isinglass was one of the oldest sources of gelatin.\n\n\nThe consumption of gelatin from particular animals may be forbidden by religious rules or cultural taboos. For example, Jewish kosher and Islamic halal customs require gelatin from sources other than pigs, such as cattle (that have been slaughtered according to the religious regulations) or fish (that they are allowed to consume). Roma people are cautious of gelatin products that may have been made from horses, as their culture forbids the consumption of horses. Some companies specify the source of the gelatin used.\n\nVegans and vegetarians do not eat foods containing gelatin made from animals. Likewise, Sikh, Hindu, and Jain customs may require gelatin alternatives from sources other than animals, as many Hindus, most Jains and some Sikhs are vegetarian. Other people simply consider gelatin unpalatable due to the ingredients used in its production.\n\nPartial alternatives to gelatins derived from animals include the seaweed extracts agar and carrageenan, and the plant extracts pectin and konjac. Research into synthetic collagen is ongoing; as of 2011, partial success has been achieved in replicating collagen's structure using self-assembling peptides.\n\nAlthough gelatin is 98–99% protein by dry weight, it has little additional nutritional value, varying according to the source of the raw material and processing technique.\n\nAmino acids present in gelatin are variable, due to varying sources and batches, but are approximately:\n\nIn 1997, the U.S. Food and Drug Administration (FDA), with support from the TSE (transmissible spongiform encephalopathy) Advisory Committee, began monitoring the potential risk of transmitting animal diseases, especially bovine spongiform encephalopathy (BSE), commonly known as \"mad cow disease\". An FDA study from that year stated: \"...steps such as heat, alkaline treatment, and filtration could be effective in reducing the level of contaminating TSE agents; however, scientific evidence is insufficient at this time to demonstrate that these treatments would effectively remove the BSE infectious agent if present in the source material.\" On March 18, 2016 the FDA finalized three previously-issued interim final rules designed to further reduce the potential risk of BSE in human food. The final rule clarified that \"gelatin is not considered a prohibited cattle material if it is manufactured using the customary industry processes specified.\"\n\nThe Scientific Steering Committee (SSC) of the European Union in 2003 stated that the risk associated with bovine bone gelatin is very low or zero.\n\nIn 2006, the European Food Safety Authority stated that the SSC opinion was confirmed, that the BSE risk of bone-derived gelatin was small, and that it recommended removal of the 2003 request to exclude the skull, brain, and vertebrae of bovine origin older than 12 months from the material used in gelatin manufacturing.\n\nIn 2011, the European Food Safety Authority Panel on Dietetic Products, Nutrition, and Allergies concluded that \"a cause and effect relationship has not been established between the consumption of collagen hydrolysate and maintenance of joints\". A 2012 review also found insufficient evidence to support its use for osteoarthritis. By contrast, in 2013, Health Canada approved a label for \"hydrolyzed collagen\", specifying that the label may make a health claim that supplemental dietary amino acid intake from hydrolyzed collagen \"helps to reduce joint pain associated with osteoarthritis\".\n"}
{"id": "34125520", "url": "https://en.wikipedia.org/wiki?curid=34125520", "title": "Governance of hydropower in Scandinavia", "text": "Governance of hydropower in Scandinavia\n\nGovernance of hydropower in Scandinavia, and the implementation of hydropower projects, is controlled by self-organising networks, with an open decision making process. \nScandinavia is one of the largest producers of hydropower in the world.\n\n\n\n\nNational governments in Scandinavia consult with all stakeholders affected by hydropower plants. The interaction between stakeholders on hydropower projects in Norway can be classified as participatory governance. After the Scandinavian countries de-regulated their markets, they connected their individual markets into one common market, Nord Pool Spot. The energy that is not traded through the market is traded through contracts between suppliers, retailers and consumers.\n\nThe Swedish electricity market was deregulated in 1996, transforming the industry from a government hierarchical system to a governance steering structure. The law stated that ‘power trading and network operations may not be conducted by the same organisation but numerous organisations thus the production and trade of electricity became competitive. The industry would be regulated by public authorities, the Energy Market Inspectorate within the Swedish Energy Agency. Its responsibilities included monitoring network tariffs and ensuring that network operators do not subsidise other interests.\n\nThe different levels of governance in Norway concerning hydropower can be seen through the economic interests, and the social responses, to the installation and expansion of hydropower projects.\n\nMost of Denmark’s hydropower electricity comes from Norway and Sweden, supplied partly through Nord Pool Spot. \nDenmark has increased its renewable energy sources (wind and biomass) from approximately 0% in 1970 to 20% in 2005, which leaves them on target for the RES directive. Wind share was 39% in 2014.\n\nAt the local level hydropower can have an enormous effect on communities; creating employment, improving infrastructure, and building new or enhanced roads. However, some communities can be resistant to hydropower, especially when local incomes are dependent on tourism. For example, fishing is an important recreational sport in Norway; and hydropower may prevent local people from participating. Also the installation of hydropower upon historic sites, and areas of natural beauty, is other major concern. Local communities have been known to bring in NGO’s such as WWF Norway, or Friends of the Earth, in order to resist the installation of hydropower plants.\n\nThe municipality’s response to hydropower typically represents the response of local communities. Municipality must manage the concerns of the stakeholder’s communities, energy companies and NGO’s regarding the installation of hydropower plants.\n\nVarious municipalities have shares and direct ownership of hydropower companies. Energy companies rely on municipalities to provide information on the local environment, and cooperate to create environmental impact assessments. This can involve numerous municipalities, as the watercourse may flow through several municipalities, and therefore are bound by regulations to work together on hydropower projects.\n\nHydropower companies pay fees to the local municipalities in the form of taxes and license fees. Smaller hydropower plants are exempt from taxes in order to economically stimulate local development. The Norwegian Water Resources and Energy Directorate (NVE) determines the license fees paid to municipalities by hydropower companies. The NVE assess various factors such as degree of environmental disturbance and the profitability of the project.\n\nHydropower has long been \"associated with a nation-building process, representing the key infrastructure for economic growth and welfare, through electrification\".\n\nIn 1991 Norway de-regulated its market and these institutions became the network for governing hydropower in Norway:\n\n\n\n\n\n"}
{"id": "11865676", "url": "https://en.wikipedia.org/wiki?curid=11865676", "title": "High-temperature engineering test reactor", "text": "High-temperature engineering test reactor\n\nThe high-temperature test reactor (HTTR) is a graphite-moderated gas-cooled research reactor in Oarai, Ibaraki, Japan operated by the Japan Atomic Energy Agency. It uses long hexagonal fuel assemblies, unlike the competing pebble bed reactor designs.\n\nHTTR first reached its full design power of 30 MW (thermal) in 1999. Other tests have shown that the core can reach temperatures sufficient for hydrogen production via the sulfur-iodine cycle.\n\nThe primary coolant is helium gas at a pressure of about 4 MPa, the inlet temperature of 395 °C, and the outlet temperature of 850–950 °C. The fuel is uranium oxide (enriched to an average of about 6%).\n\n\n"}
{"id": "8522849", "url": "https://en.wikipedia.org/wiki?curid=8522849", "title": "Independent Pricing and Regulatory Tribunal of New South Wales", "text": "Independent Pricing and Regulatory Tribunal of New South Wales\n\nThe Independent Pricing and Regulatory Tribunal of New South Wales (IPART) is an independent regulatory and pricing tribunal that oversees regulation in water, gas, electricity and transport industries in the Australian state of New South Wales. IPART was established in 1992 by Government of New South Wales with the primary purpose of regulating the maximum prices for monopoly services by government utilities and other monopoly businesses such as public transport.\n\nIPART’s organisational arm or Secretariat is managed by the Chief Executive Officer. IPART has approximately 140 staff members and an annual budget of 25 million. The current Chairman is Dr Peter Boxall . IPART is responsible to the Premier of New South Wales, presently Gladys Berejiklian MP.\n\nIPART's role is set out in various pieces of state legislation including the \"Independent Pricing and Regulatory Tribunal Act 1992\", the \"Gas Supply Act 1996\", the \"Electricity Supply Act 1995\", the \"National Electricity (NSW) Law 1997\" and the \"Transport Administration Act 1996\".\n\nIPART's core functions are conferred by legislation, rules and access regimes established by legislation. These functions are to:\n\nIn addition to these functions, IPART also undertakes reviews and investigations to advise the NSW government and its agencies on a range of economic and policy issues such as pricing, efficiency, industry structure and competition.\n\nThe Tribunal comprises three permanent members: a Part-Time Chairman and 2 Part-Time Tribunal Members. Tribunal members are appointed by the Premier. The Premier can also appoint any number of additional temporary members. As of April 2017, the permanent Tribunal members are:\n\n\n"}
{"id": "44761707", "url": "https://en.wikipedia.org/wiki?curid=44761707", "title": "Insular energy system", "text": "Insular energy system\n\nAn insular energy system or isolated energy system is defined by a country’s inability, due to smallness and/or remoteness, to interconnect with other electricity generators and consumers through a wider transmission grid outside its national borders. As a result, the country cannot take advantage of the more efficient neighboring electricity markets. This type of energy system is typically detected in small islands or in mainland countries where the costs for constructing infrastructure for power transmission purposes are prohibitively high, or in cases where a country may be isolated due to political issues.\n\nThe energy mix of insular energy systems is dominated by diesel and heavy fuel oil. The vast amounts of imported fossil fuel necessary to fulfil the energy needs of these systems create instability in the economy as well as the security of the countries. Additionally, the domination of fossil-fueled energy generation is strongly supported by several other factors, including the inefficiency of indigenous energy resources, the limited infrastructure of energy delivery, the lack of storage, and the flexibility of the power generators to meet seasonal needs. Insular energy systems typically have only a few independent power producers and a limited range of power generation technologies. Furthermore, there is lack of attractive support schemes or incentives for the progression of the system from fossil fuels to renewable and low-carbon energy sources. At the same time, their efforts to meet international or European obligations often fail and are normally also costly.\n\nA number of issues make the energy generation of insular systems extremely expensive and less secure. First, the great dependency of insular energy systems on imported energy sources for electricity generation and their associated high transportation and shipping costs are reflected in the electricity pricing. Additionally, the small sizes of these systems limit not only the production and consumption capacities, but also the establishment and growth of significant internal markets. Also, the dominance of a sole public or private energy producer means that a single stakeholder is fully responsible for generating, transmitting and distributing electricity and in control of the associated investment decisions, programmes, and tariff setting. The reduction of GHG emissions is another great challenge for insular energy systems given that most of their electricity production is based on fossil fuels.\n\nInsular energy systems can be divided into three categories according to their installed power capacity and location:\nThe countries found in this category have limited energy demand and large distance from the mainland. Only a limited number of these countries utilize renewable energy sources, contributing up to 5% of the energy mix.\n\nThe majority of the islands in this category exploit, to a small or large extent, renewable energy sources due to their larger consumption demand and higher GDPs.\n\nThe GDP of the majority of countries found in this category is very low. The political situation in these countries limits the investment in electricity infrastructure, in view of the fact that health and military issues are typically more alarming and urgent to resolve. Conversely, this category also includes mainland nations that do not export or import electricity to their neighbours and are as also major petroleum exporting countries including Qatar, UAE and Saudi Arabia. It is evident that the abundance of fossil fuels found in these countries has not alarmed them to turn to renewable energy resources.\n\nThe transition to smart systems can be achieved through a variety of measures and policies including:\n\nHowever, every action should preliminarily take into consideration the local conditions of the energy system, as well as the economic feasibility.\nExploiting renewable energy sources can contribute significantly in reducing the level of energy imports of insular energy systems with positive impacts for the balance of trade and security of supply. Yet, there are obstacles that need to be overcome before renewable energy generation penetrates the insular energy systems. Renewable energy technologies are less reliable than conventional technologies due to the fact that the energy production is variable and weather dependent and thus additional technologies such as energy storage are required. Monopolistic power sectors also prevent the development of smaller scale renewable electricity generation that would be more efficient and cost-competitive and put conventional technologies in a preferential position due to the earlier profit resulting from the lower capital costs.\n"}
{"id": "5580651", "url": "https://en.wikipedia.org/wiki?curid=5580651", "title": "Jesmonite", "text": "Jesmonite\n\nJesmonite is a composite material used in fine arts, crafts, and construction. It consists of a gypsum-based material in an acrylic resin.\n\nJesmonite is a versatile material and is used in several ways. It is typically used for creating sculptures and other three-dimensional works, but can be used with other materials as a ground for painting. It can be used as a surface material in building and construction. It is considered an attractive alternative to other resin-based materials, such as polyester and fiberglass. It can be used for casting and laminating. \n\nBesides its popularity in sculpture, jesmonite is popular in other areas where casting and molding are common, such as architectural stone and plasterwork that has a requirement to be very lightweight, taxidermy, archaeology, and palaeontology.\n\nJesmonite is considered durable, flame resistant, and resistant to impact. It can be used to fabricate both small and large objects. When mixed, it accepts colored pigments and metal powders. Its surface can be finished to resemble plaster, stone, metal, and wood. \n\nJesmonite is considered a low hazard material. The finished composite emits no toxic fumes. The mixing process requires no harmful solvents. However, the mixing should be performed with rubber gloves, eye protection, and dust mask, and should take place in a well-ventilated area. Cleanup is performed with water.\n\nIn the 2012 Thames Diamond Jubilee Pageant, the ornate prow sculptures on the Royal barges \"Gloriana\" and MV \"Spirit of Chartwell\" were carved and moulded in Jesmonite and decorated with gold leaf. These included dolphins, relief plaques and \"Old Father Thames\".\n\n"}
{"id": "22419108", "url": "https://en.wikipedia.org/wiki?curid=22419108", "title": "Lane hydrogen producer", "text": "Lane hydrogen producer\n\nThe Lane hydrogen producer was an apparatus for hydrogen production based on the steam-iron process and water gas invented in 1903 by British engineer, Howard Lane.\n\nThe first commercial Lane hydrogen producer was commissioned in 1904. By 1913, of hydrogen was manufactured annually by this process.\n\nIn the early-part of the 20th century, the process found some use as a means of producing hydrogen lifting gas for airships, as it could produce large volumes of gas cheaply. Lane producers were installed at some British airship stations so the gas could be manufactured on-site. To work efficiently however, the plant required skilled operators and to be running as a quasi-continuous process. A competing process, referred to as the \"Silicol Process\", reacted Ferrosilicon with a strong Sodium hydroxide solution and had the advantage of flexibility.\n\nIn the 1940s the Lane process was superseded by cheaper methods of hydrogen production that used oil or natural gas as a feedstock.\n\nWhere hydrogen was commonly produced with the single retort like the Messerschmitt and the Bamag type, Lane introduced the multiple retort type. In the Lane generator water gas was used to heat the retorts up to 600-800 °C after which water gas-air was used in the retorts. In the steam-iron process the iron oxidizes and has to be replaced with fresh metal, in the Lane hydrogen producer the iron is reduced with water gas back to its metallic condition, after which the process restarts.\n\nThe chemical reactions are\n\nThe net chemical reaction is:\n\n"}
{"id": "60777", "url": "https://en.wikipedia.org/wiki?curid=60777", "title": "Limonite", "text": "Limonite\n\nLimonite () is an iron ore consisting of a mixture of hydrated iron(III) oxide-hydroxides in varying composition. The generic formula is frequently written as FeO(OH)·HO, although this is not entirely accurate as the ratio of oxide to hydroxide can vary quite widely. Limonite is one of the three principal iron ores, the others being hematite and magnetite, and has been mined for the production of iron since at least 2500 BCE.\n\nLimonite is named for the Greek word (/leː.mɔ̌ːn/), meaning \"wet meadow\", or (/lím.nɛː/), meaning “marshy lake” as an allusion to its occurrence as bog iron ore in meadows and marshes. In its brown form it is sometimes called brown hematite or brown iron ore. In its bright yellow form it is sometimes called lemon rock or yellow iron ore.\n\nLimonite is relatively dense with a specific gravity varying from 2.7 to 4.3. It varies in colour from a bright lemony yellow to a drab greyish brown. The streak of limonite on an unglazed porcelain plate is always brownish, a character which distinguishes it from hematite with a red streak, or from magnetite with a black streak. The hardness is variable, but generally in the 4 - 5.5 range.\n\nAlthough originally defined as a single mineral, limonite is now recognized as a mixture of related hydrated iron oxide minerals, among them goethite, akaganeite, lepidocrocite, and jarosite. Individual minerals in limonite may form crystals, but limonite does not, although specimens may show a fibrous or microcrystalline structure, and limonite often occurs in concretionary forms or in compact and earthy masses; sometimes mammillary, botryoidal, reniform or stalactitic. Because of its amorphous nature, and occurrence in hydrated areas limonite often presents as a clay or mudstone. However, there are limonite pseudomorphs after other minerals such as pyrite. This means that chemical weathering transforms the crystals of pyrite into limonite by hydrating the molecules, but the external shape of the pyrite crystal remains. Limonite pseudomorphs have also been formed from other iron oxides, hematite and magnetite; from the carbonate siderite and from iron rich silicates such as almandine garnets.\n\nLimonite usually forms from the hydration of hematite and magnetite, from the oxidation and hydration of iron rich sulfide minerals, and chemical weathering of other iron rich minerals such as olivine, pyroxene, amphibole, and biotite. It is often the major iron component in lateritic soils. It is often deposited in run-off streams from mining operations.\n\nOne of the first uses was as a pigment. The yellow form produced yellow ochre for which Cyprus was famous, while the darker forms produced more earthy tones. Roasting the limonite changed it partially to hematite, producing red ochres, burnt umbers and siennas.\n\nBog iron ore and limonite mudstones are mined as a source of iron, although commercial mining of them has ceased in the United States.\n\n\"Iron caps\" or gossans of siliceous iron oxide typically form as the result of intensive oxidation of sulfide ore deposits. These gossans were used by prospectors as guides to buried ore. In addition the oxidation of those sulfide deposits which contained gold, often resulted in the concentration of gold in the iron oxide and quartz of the gossans. Goldbearing limonite gossans were productively mined in the Shasta County, California mining district. Similar deposits were mined near Rio Tinto in Spain and Mount Morgan in Australia. In the Dahlonega gold belt in Lumpkin County, Georgia gold was mined from limonite-rich lateritic or saprolite soil. The gold of the primary veins was concentrated into the limonites of the deeply weathered rocks. In another example the deeply weathered iron formations of Brazil served to concentrate gold with the limonite of the resulting soils.\n\nWhile the first iron ore was likely meteoric iron, and hematite was far easier to smelt, in Africa, where the first evidence of iron metallurgy occurs, limonite is the most prevalent iron ore. Before smelting, as the ore was heated and the water driven off, more and more of the limonite was converted to hematite. The ore was then pounded as it was heated above 1250 °C, at which temperature the metallic iron begins sticking together and non-metallic impurities are thrown off as sparks. Complex systems developed, notably in Tanzania, to process limonite. Nonetheless, hematite and magnetite remained the ores of choice when smelting was by bloomeries, and it was only with the development of blast furnaces in 1st century BCE in China and about 1150 CE in Europe, that the brown iron ore of limonite could be used to best advantage.\nAs regards to the use of limonite for pigments, it was one of the earliest man-used materials and can be seen in Neolithic cave paintings and pictographs.\n\n\n"}
{"id": "38802940", "url": "https://en.wikipedia.org/wiki?curid=38802940", "title": "List of bioluminescent organisms", "text": "List of bioluminescent organisms\n\nBioluminescence is the production of light by living organisms. This list of bioluminescent organisms is organized by environment, covering terrestrial, marine and microorganisms.\n\n\n\n\n\n\n"}
{"id": "1948561", "url": "https://en.wikipedia.org/wiki?curid=1948561", "title": "Nest box", "text": "Nest box\n\nA nest box, also spelled nestbox, is a man-made enclosure provided for animals to nest in. Nest boxes are most frequently utilized for birds, in which case they are also called birdhouses or a birdbox/bird box, but some mammalian species may also use them. Placing nestboxes or roosting boxes may also be used to help maintain populations of particular species in an area. The nest box was invented by the British conservationist Charles Waterton in the early 19th century to encourage more birdlife and wildfowl on the nature reserve he set up on his estate.\n\nNest boxes are getting more attention because industrialization, deforestation and other human activities since the mid-20th century have caused severe declines in birds' natural habitats, introducing hurdles to breeding. A nest box can help prevent bird extinction. \n\nNest boxes are usually wooden, although the purple martin will nest in metal. Some boxes are made from a mixture of wood and concrete, called \"woodcrete\". Ceramic and plastic nestboxes too are not suitable.\n\nNest boxes should be made from untreated wood with an overhanging, sloped roof, a recessed floor, drainage and ventilation holes, a way to access the interior for monitoring and cleaning, and have no outside perches which could assist predators. Boxes may either have an entrance hole or be open-fronted. Some nest boxes can be highly decorated and complex, sometimes mimicking human houses or other structures. They may also contain nest box cameras so that use of, and activity within, the box can be monitored.\n\nThe diameter of the opening in a nest-box has a very strong influence on the species of birds that will use the box. Many small birds select boxes with a hole only just large enough for an adult bird to pass through. This may be an adaptation to prevent other birds from raiding it. In European countries, an opening of 2.5 cm in diameter will attract \"Poecile palustris\", \"Poecile montanus\"; an opening of 2.8 cm in diameter will attract \"Ficedula hypoleuca\", and an opening of 3 cm in diameter will attract \"Parus major\", \"Passer montanus\", an opening of 3 cm in diameter will attract \"Passer domesticus\".\n\nThe size of the nest box also affects the bird species likely to use the box. Very small boxes attract wrens and treecreepers and very large ones may attract ducks and owls. Seasonally removing old nest material and parasites is important if they are to be successfully re-used.\n\nThe material used in the construction may also be significant. Sparrows have been shown to prefer woodcrete boxes rather than wooden ones. Birds nesting in woodcrete sites had earlier clutches, a shorter incubation period, and more reproductive success, perhaps because the synthetic nests were warmer than their wooden counterparts.\n\nPlacement of the nest box obviously is also significant. Some birds (including birds of prey) prefer their nest box to be at an optimum height. Some birds (such as ducks) prefer nest sites them to be very low or even at ground level. For many birds orientation relative to the sun is of importance with many birds preferring an orientation away from direct sun and sheltered from the prevailing rain.\n\nBat boxes differ from bird nest-boxes in typical design, with the larger opening on the underside of the box, and are more often referred to as bat boxes, although in regard to the rearing of young, they serve the same purpose. Some threatened bat species can be locally supported with the provision of appropriately placed bat-boxes, however species that roost in foliage or large cavities will not use bat boxes. Bat boxes are typically made out of wood, and there are several designs for boxes with single or multiple chambers. Directions for making the open bottom bat houses for small and large colonies, as well as locations to purchase them are available on the internet. Colour and placement is important to ensuring that bat boxes are used; bat boxes that are too shaded will not heat up enough to attract a maternity colony of bats. Australian bat box projects have been running for over 12 years in particular at the Organ Pipes National Park. Currently there are 42 roost boxes using the \"Stebbings Design\" which have peaked at 280 bats roosting in them. The biggest problem with roosting boxes of any kind is the ongoing maintenance; problems include boxes falling down, wood deteriorating, and pests such as ants, the occasional rat, possums, and spiders.\n\nNest boxes are marketed not only for birds, but also for butterflies and other mammals, especially arboreal ones such as squirrels and opossums. Depending on the animal, these boxes may be used for roosting, breeding, or both. Or, as in the case with butterflies, hibernation.\n\nWasps may build their nests inside a nest box intended for other animals, and may exclude the intended species.\n\n\n"}
{"id": "54010132", "url": "https://en.wikipedia.org/wiki?curid=54010132", "title": "Ngong Forest", "text": "Ngong Forest\n\nNgong Forest is one of the few natural forests in a capital city of Kenya. Located from the central business district of Nairobi, the Ngong Forest is in Ngong Hills along Ngong Road. Managed by the Ngong Road Forest Sanctuaries, this habitat of indigenous trees is home to many wild animals, including leopards, spotted hyenas, and Cape bushbucks. Other animal residents include crowned eagles, owls, snakes, and many other birds and reptiles.\n\nNgong Forest, which extends to Rift Valley Province, has undergone deforestation due to the settlement of Karen and Ngong, as well as to the development of the Lenana School and the Ngong Racecourse. These events have reduced Ngong Forest's original to the current .\n"}
{"id": "29650450", "url": "https://en.wikipedia.org/wiki?curid=29650450", "title": "Odigha Odigha", "text": "Odigha Odigha\n\nOdigha Odigha is a Nigerian educator, environmentalist and activist. He was awarded the Goldman Environmental Prize in 2003, for his efforts on protection of the rainforests of Cross River State from industrial logging.\n\n"}
{"id": "276546", "url": "https://en.wikipedia.org/wiki?curid=276546", "title": "Oriented strand board", "text": "Oriented strand board\n\nOriented strand board (OSB), also known as flakeboard, sterling board and aspenite in British English, is a type of engineered wood similar to particle board, formed by adding adhesives and then compressing layers of wood strands (flakes) in specific orientations. It was invented by Armin Elmendorf in California in 1963. OSB may have a rough and variegated surface with the individual strips of around , lying unevenly across each other and comes in a variety of types and thicknesses.\n\nOSB is a material with favorable mechanical properties that make it particularly suitable for load-bearing applications in construction. It is now more popular than plywood, commanding 66% of the structural panel market. The most common uses are as sheathing in walls, flooring, and roof decking. For exterior wall applications, panels are available with a radiant-barrier layer pre-laminated to one side; this eases installation and increases energy performance of the building envelope. OSB also sees some use in furniture production.\n\nOriented strand board is manufactured in wide mats from cross-oriented layers of thin, rectangular wooden strips compressed and bonded together with wax and synthetic resin adhesives (95% wood, 5% wax and resin). \n\nThe adhesive resins types used include : Urea-formaldehyde (OSB type 1, non-structural, non-waterproof); isocyanate based glue (or PMDI poly-Methylene diphenyl diisocyanate based) in inner regions with Melamine-Urea-formaldehyde or Phenol formaldehyde resin glues at surface (OSB type 2, structural, water resistant on face); Phenol formaldehyde resin throughout (OSB types 3 and 4, structural, for use in damp and outside environments).\n\nThe layers are created by shredding the wood into strips, which are sifted and then oriented on a belt or wire cauls. The mat is made in a forming line. Wood strips on the external layers are aligned to the panel's strength axis, while internal layers are perpendicular. The number of layers placed is determined partly by the thickness of the panel but is limited by the equipment installed at the manufacturing site. Individual layers can also vary in thickness to give different finished panel thicknesses (typically, a layer will produce a panel thickness). The mat is placed in a thermal press to compress the flakes and bond them by heat activation and curing of the resin that has been coated on the flakes. Individual panels are then cut from the mats into finished sizes. Most of the world's OSB is made in the United States and Canada in large production facilities. The largest production facilities can make over of OSB per day.\n\nMaterials other than wood have been used to produce products similar to oriented strand board. Oriented structural straw board is an engineered board that is made by splitting straw and formed by adding P-MDI adhesives and then hot compressing layers of straw in specific orientations. Strand board can also be made from bagasse.\n\nIn 2005, Canadian production was ( basis) of which () was exported, almost entirely to the United States. In 2014, Romania became the largest OSB exporting country in Europe, with 28 % of the exports going to Russia and 16 % to Ukraine.\n\nAdjustments to the manufacturing process can impact differences in thickness, panel size, strength, and rigidity. OSB panels have no internal gaps or voids, and can be water-resistant, although they do require additional membranes to achieve impermeability to water and are not recommended for exterior use. The finished product has properties similar to plywood, but is uniform and cheaper. When tested to failure, OSB has a greater load-bearing capacity than milled wood panels. It has replaced plywood in many environments, especially the North American structural panel market. Unfortunately, its use as sheathing has contributed to the leaky condo crisis, as it is less able to breathe and release moisture than plywood.\n\nWhile OSB does not have a continuous grain like a natural wood, it does have an axis along which its strength is greatest. This can be seen by observing the alignment of the surface wood chips.\n\nAll wood-based structural use panels can be cut and installed with the same ease and types of equipment used with solid wood.\n\nThe resins used to create OSB have raised questions regarding the potential for OSB to emit volatile organic compounds (VOCs) such as formaldehyde. Urea-formaldehyde is more toxic and should be avoided in home use. Phenol-formaldehyde products are considered to be relatively hazard-free. Some newer types of OSB, so-called \"New-generation\" OSB panels, use isocyanate resins that do not contain formaldehyde and are considered non-volatile when cured. Industry trade groups assert that formaldehyde emissions from North American OSB are \"negligible or nonexistent\".\n\nSome manufacturers treat the wood chips with various borate compounds which are toxic to termites, wood boring beetles, molds, and fungi, but not mammals in applied doses.\n\nFour grades of OSB are defined in EN 300 in terms of their mechanical performance and relative resistance to moisture:\n\n"}
{"id": "11441272", "url": "https://en.wikipedia.org/wiki?curid=11441272", "title": "Paperless office", "text": "Paperless office\n\nA paperless office (or paper-free office) is a work environment in which the use of paper is eliminated or greatly reduced. This is done by converting documents and other papers into digital form, a process known as digitization. Proponents claim that \"going paperless\" can save money, boost productivity, save space, make documentation and information sharing easier, keep personal information more secure, and help the environment. The concept can be extended to communications outside the office as well.\n\nThe paperless world was a publicist's slogan, intended to describe the office of the future. It was facilitated by the popularization of video display computer terminals like the 1964 IBM 2260. An early prediction of the paperless office was made in a 1975 \"Business Week\" article. The idea was that office automation would make paper redundant for routine tasks such as record-keeping and bookkeeping, and it came to prominence with the introduction of the personal computer. While the prediction of a PC on every desk was remarkably prophetic, the \"paperless office\" was not. Improvements in printers and photocopiers have made it much easier to reproduce documents in bulk, causing the worldwide use of office paper to more than double from 1980 to 2000. This has been attributed to the increased ease of document production and widespread use of electronic communication, which has resulted in users receiving large numbers of documents that are often printed out. However, since about 2000, at least in the US, the use of office paper has leveled off and is now decreasing, which has been attributed to a generation shift; younger people are believed to be less inclined to print out documents, and more inclined to read them on a full-color interactive display screen. According to the United States Environmental Protection Agency, the average office worker generates approximately two pounds of paper and paperboard products each day.\n\nThe term \"The Paperless Office\" was first used in commerce by Micronet, Inc., an automated office equipment company, in 1978.\n\nTraditional offices have paper-based filing systems, which may include filing cabinets, folders, shelves, microfiche systems, and drawing cabinets, all of which require maintenance, equipment, considerable space, and are resource-intensive. In contrast, a paperless office could simply have a desk, chair, and computer (with a modest amount of local or network storage), and all of the information would be stored in digital form. Speech recognition and speech synthesis could also be used to facilitate the storage of information digitally.\n\nOnce computer data is printed on paper, it becomes out-of-sync with computer database updates. Paper is difficult to search and arrange in multiple sort arrangements, and similar paper data stored in multiple locations is often difficult and costly to track and update. A paperless office would have a single-source collection point for distributed database updates, and a publish-subscribe system. Modern computer screens make reading less exhausting for the eyes; a laptop computer can be used on a couch or in bed. With tablet computers and smartphones, with many other low-cost value-added features like video animation, video clips, and full-length movies, many argue that paper is now obsolete to all but those who are resistant to technological change. eBooks are often free or low cost compared to hard-copy books.\n\nOthers argue that paper will always have a place because it affords different uses than screens [Sellen, A. J., & Harper, R. H. R. (2003). The myth of the paperless office. Cambridge, Massachusetts: MIT Press.]\n\nPaper product manufacturing contributes significantly to deforestation and man-made climate change, and produces greenhouse gases. According to the American Forest & Paper Association, paper manufacturing is the third largest user of fossil fuels worldwide. Although measures such as recycling and using tree-free paper can help reduce the environmental impact of paper, most paper still ends up in landfills. Paper production also leads to air pollution, as paper manufacturing releases nitrogen dioxide (NO), sulfur dioxide (SO), and carbon dioxide (CO). Nitrogen dioxide and sulfur dioxide are major contributors to acid rain, whereas CO is a greenhouse gas responsible for climate change. Waste water discharged from pulp and paper mills contains solids, nutrients, and dissolved organic matter that are classified as pollutants. Nutrients such as nitrogen and phosphorus can cause or exacerbate eutrophication of fresh water bodies.\n\nPrinting inks and toners are very expensive and use environment-damaging volatile organic compounds, heavy metals and non-renewable oils, although standards for the amount of heavy metals in ink have been set by some regulatory bodies. Deinking recycled paper pulp results in a waste slurry, sometimes weighing 22% of the weight of the recycled wastepaper, which may go to landfills.\n\nThe need for paper is eliminated by using online systems, such as replacing index cards and rolodexes with databases, typed letters and faxes with email, and reference books with the internet. Another way to eliminate paper is to automate paper-based processes that rely on forms, applications and surveys to capture and share data. This method is referred to as \"electronic forms\" or e-forms and is typically accomplished by using existing print-perfect documents in electronic format to allow for pre-filling of existing data, capturing data manually entered online by end-users, providing secure methods to submit form data to processing systems, and digitally signing the electronic documents without printing.\n\nThe technologies that may be used with electronic forms automation include -\n\nOne of the main issues that has kept companies from adopting paperwork automation is difficulty capturing digital signatures in a cost-effective and compliant manner. The E-Sign Act of 2000 in the United States provided that a document cannot be rejected on the basis of an electronic signature and required all companies to accept digital signatures on documents. Today there are sufficient cost-effective options available, including solutions that do not require end-users to purchase hardware or software.\n\nOne of the great benefits of this type of software is that you can use OCR (Optical Character Recognition) to search the full text of any file. Additionally, tags can be added to each file to make it easier to locate certain files throughout the entire system.\n\nSome paperless software offers a scanner, hardware and software and works seamlessly in separating and organizing important documents. Paperless software might also allow people to enable online signatures for important documents that can be used in any small business or office. Document management and archiving systems do offer some methods of automating forms. Typically, the point in which document management systems start working with a document is when the document is scanned and/or sent into the system. Many document management systems include the ability to read documents via optical character recognition (OCR) and use that data within the document management system’s framework. While this technology is essential to achieving a paperless office it does not address the processes that generate paper in the first place.\n\nAnother key aspect of the paperless office philosophy is the conversion of paper documents, photos, engineering plans, microfiche and all the other paper based systems to digital documents. Technologies that may be used for this include scanners, digital mail solutions, book copiers, wide format scanners (for engineering drawings), microfiche scanners, fax to PDF conversion, online post offices, multifunction printers and document management systems. Each of these technologies uses software that converts the raster formats (bitmaps) into other forms depending on need. Generally, they involve some form of image compression technology that produces smaller raster images or use optical character recognition (OCR) to convert a document into text. A combination of OCR and raster is used to enable search ability while maintaining the original form of the document. An important step is the labeling related to paper-to-digital conversion and the cataloging of scanned documents. Some technologies have been developed to do this, but they generally involve either human cataloging or automated indexing on the OCR document. However, scanners and software continue to improve with the development of small, portable scanners that are able to scan doubled-sided A4 documents at around 30-35ppm to a raster format (typically TIFF fax 4 or PDF).\n\nAn issue faced by those wishing to take the paperless philosophy to the limit has been copyright laws. These laws may restrict the transfer of documents protected by copyright from one medium to another, such as converting books to electronic format.\n\nAs awareness of identity theft and data breaches became more widespread, new laws and regulations were enacted, requiring companies that manage or store personally identifiable information to take proper care of those documents. Paperless office systems are easier to secure than traditional filing cabinets, and can track individual accesses to each document.\n\nA major difficulty in \"going paperless\" is that much of a business's communication is with other businesses and individuals, as opposed to just being internal. Electronic communication requires both the sender and the recipient to have easy access to appropriate software and hardware. Costs and temporary productivity losses when converting to a paperless office are also a factor, as are government regulations, industry standards, legal requirements, and business policies which may also slow down the change. Businesses may encounter technological difficulties such as file format compatibility, longevity of digital documents, system stability, and employees and clients not having appropriate technological skills.\n\nFor these reasons, while there may be a reduction of paper, some uses of paper will likely remain indefinitely. However, a 2015 questionnaire suggested that nearly half of small/medium-sized businesses believed they were or could go paperless by the end of that year.\n\n\n\n"}
{"id": "2508448", "url": "https://en.wikipedia.org/wiki?curid=2508448", "title": "Power Wheels", "text": "Power Wheels\n\nPower Wheels is a brand of battery-powered ride-on toy cars for kids ages 12 months to seven years old. Power Wheels ride-ons are built with kid-sized, realistic features – in some cases, real working features like FM radios, opening/closing doors and hoods, power lock brakes, and both forward and reverse motion.\n\nThe Power Wheels brand name dates back to 1984, when San Francisco-based toy company Kransco acquired Pines of America, makers of battery-powered vehicles for children. Two years later, Kransco renamed the line \"Power Wheels\". By 1990 sales of the battery-powered vehicles reached over 1,000,000 per year.\n\nIn 1994, Power Wheels was purchased by Mattel. Following Mattel's acquisition of Kransco, the Power Wheels line immediately became part of Fisher-Price toys out of East Aurora, New York. With the addition of new vehicle licenses the new Power Wheels by Fisher-Price and Mattel lines did well.\n\nIn 1999, Fisher-Price announced the Harley-Davidson Motorcycle Ride-On – which contributed to a year of record sales for the entire product line.\n\nPower Wheels ride-on cars, trucks and motorcycles have been sold with more than 100 model names.\n\nThe latest line of Power Wheels features small scale versions of popular real world vehicles, including the Jeep Wrangler, Jeep Hurricane, Ford F-150, Ford Mustang, Kawasaki KFX quad, Harley-Davidson motorcycle, Cadillac Escalade EXT as well as Lightning McQueen from Pixar’s movie, \"Cars\".\n\nPower Wheels vehicles have been the subject of Safety Recalls.\n\nThe first recall in 1991 involved the 18 Volt Porsche 911, in which the contacts in the foot pedal switch could weld together in use. If this were to happen, the motor would remain running and the vehicle would continue moving forward, unable to stop. A new accelerator pedal was fitted that eliminated the possibility of welded contacts.\n\nIn 1998, Fisher-Price undertook a monumental recall of up to 10 million Power Wheels 12 volt and Super 6 volt vehicles manufactured since 1986. The recall and repair program was conducted to replace battery fuses and strengthen battery connectors in order to prevent the units from overheating. The main difference of a post recall Power Wheel is that the original \"H\" (or on very early Power Wheels, \"S\") connectors are removed and replaced with the larger, black \"A\" connectors. If a Power Wheels ride-on was built in or before 1998 and has the Black \"A\" connectors, then the recall work has probably been performed.\n\nThe third recall in 2000 involved the Harley-Davidson motor cycle ride-ons, In cooperation with the U.S. Consumer Product Safety Commission (CPSC), Fisher-Price is recalling about 218,000 battery-powered Power Wheels Harley-Davidson motorcycle ride-ons for repair. The foot pedals, which activate the ride-ons, can stick in the \"on\" position. Children can be injured when the motorcycle ride-ons fail to stop and strike other objects. The recalled Power Wheels Harley-Davidson motorcycle ride-ons have model numbers 74290, 74293 (with a red body) and 74298 (with a black body). A \"Power Wheels by Fisher-Price\" logo is located on the left side of the seat on the product, and the model number is located on a label in the battery compartment. Only model numbers 74290, 74293 and 74298 are being recalled. These vehicles are intended for children 3 and older, and the vehicles' speed is 2.5 or 5 mph. Toy, mass merchandise and discount stores, and Harley-Davidson dealerships sold the motorcycle ride-ons nationwide from September 1999 through August 2000 for about $190 for the red vehicle and $700 for the black vehicle.\n\n"}
{"id": "782836", "url": "https://en.wikipedia.org/wiki?curid=782836", "title": "Power over Ethernet", "text": "Power over Ethernet\n\nPower over Ethernet or PoE describes any of several standard or ad-hoc systems which pass electric power along with data on twisted pair Ethernet cabling. This allows a single cable to provide both data connection and electric power to devices such as wireless access points, IP cameras, and VoIP phones.\n\nThere are several common techniques for transmitting power over Ethernet cabling. Two of them have been standardized by IEEE 802.3 since 2003. These standards are known as \"Alternative A\" and \"Alternative B\". For 10BASE-T and 100BASE-TX, only two of the four signal pairs in typical Cat 5 cable are used. \"Alternative B\" separates the data and the power conductors, making troubleshooting easier. It also makes full use of all four twisted pairs in a typical Cat 5 cable. The positive voltage runs along pins 4 and 5, and the negative along pins 7 and 8.\n\n\"Alternative A\" transports power on the same wires as data for 10 and 100 Mbit/s Ethernet variants. This is similar to the phantom power technique commonly used for powering condenser microphones. Power is transmitted on the data conductors by applying a common voltage to each pair. Because twisted-pair Ethernet uses differential signaling, this does not interfere with data transmission. The common-mode voltage is easily extracted using the center tap of the standard Ethernet pulse transformer. For Gigabit Ethernet and faster, all four pairs are used for data transmission, so both Alternatives A and B transport power on wire pairs also used for data.\n\nIn addition to standardizing existing practice for spare-pair (\"Alternative B\") and common-mode data pair power (\"Alternative A\") transmission, the IEEE PoE standards provide for signaling between the power sourcing equipment (PSE) and powered device (PD). This signaling allows the presence of a conformant device to be detected by the power source, and allows the device and source to negotiate the amount of power required or available.\n\nThe original IEEE 802.3af-2003 PoE standard provides up to of DC power (minimum and ) on each port. Only is assured to be available at the powered device as some power dissipates in the cable. The updated IEEE 802.3at-2009 PoE standard also known as PoE+ or PoE plus, provides up to of power for \"Type 2\" devices. The 2009 standard prohibits a powered device from using all four pairs for power. Both of these standards have since been incorporated into the IEEE 802.3-2012 publication.\n\nThe IEEE 802.3bu-2016 amendment introduced \"single-pair\" Power over Data Lines (PoDL) for the single-pair Ethernet standards 100BASE-T1 and 1000BASE-T1 intended for automotive and industrial applications. On the two-pair or four-pair standards power is transmitted only \"between pairs\", so that within each pair there is no voltage present other than that representing the transmitted data. With single-pair Ethernet, power is transmitted in parallel to the data. PoDL defines 10 power classes, ranging from .5 to 50 W (at PD).\n\nLooking at ways of increasing the amount of power transmitted, IEEE has defined IEEE 802.3bt in September 2018. The standard introduces two additional power types: up to 55 W (Type 3) and up to 90-100 W (Type 4). Each pair of twisted pairs needs to handle a current of up to 600 mA (Type 3) or 960 mA (Type 4). Additionally, support for 2.5GBASE-T, 5GBASE-T and 10GBASE-T is included. This development opens the door to new applications and expands the use of applications such as high-performance wireless access points and surveillance cameras.\n\nExamples of devices powered by PoE include:\n\nPower sourcing equipment (PSE) refers to devices such as a network switch that provides (\"sources\") power on the Ethernet cable. When the device is a switch, it is commonly called an \"endspan\" (although IEEE 802.3af refers to it as endpoint). Otherwise, if it's an intermediary device between a non PoE capable switch and a PoE device, it's called a \"midspan\". An external PoE \"injector\" is a \"midspan\" device.\n\nPowered device (PD) refers to devices powered by a PSE and thus consuming energy. Examples include wireless access points, VoIP phones, and IP cameras.\n\nMany powered devices have an auxiliary power connector for an optional, external, power supply. Depending on the PD design, some, none, or all power can be supplied from the auxiliary port, with the auxiliary port sometimes acting as backup power in case PoE-supplied power fails.\n\nAdvocates of PoE expect PoE to become a global long term DC power cabling standard and replace a multiplicity of individual AC adapters, which cannot be easily centrally managed. Critics of this approach argue that PoE is inherently less efficient than AC power due to the lower voltage, and this is made worse by the thin conductors of Ethernet. Advocates of PoE, like the Ethernet Alliance, point out that quoted losses are for worst case scenarios in terms of cable quality, length and power consumption by powered devices. In any case, where the central PoE supply replaces several dedicated AC circuits, transformers and inverters, the power loss in cabling can be justifiable.\n\nAfter integration of PoE with the IEEE 802.3az Energy-Efficient Ethernet (EEE) standard potentially produces additional energy savings. Pre-standard integrations of EEE and PoE (such as Marvell's EEPoE outlined in a May 2011 white paper) claim to achieve a savings upwards of 3 W per link. This saving is especially significant as higher power devices come online. Marvell claims that:\n\nThe IEEE standards for PoE require category 5 cable or better for high power levels but allow using category 3 cable if less power is required.\n\nPower is supplied in common mode over two or more of the differential pairs of wires found in the Ethernet cables and comes from a power supply within a PoE-enabled networking device such as an Ethernet switch or can be \"injected\" into a cable run with a \"midspan\" power supply. A midspan power supply, also known as a \"PoE power injector\", is an additional PoE power source that can be used in combination with a non-PoE switch.\n\nStandards-based Power over Ethernet is implemented following the specifications in IEEE 802.3af-2003 (which was later incorporated as clause 33 into IEEE 802.3-2005) or the 2009 update, IEEE 802.3at. A phantom power technique is used to allow the powered pairs to also carry data. This permits its use not only with 10BASE-T and 100BASE-TX, which use only two of the four pairs in the cable, but also with 1000BASE-T (gigabit Ethernet), which uses all four pairs for data transmission. This is possible because all versions of Ethernet over twisted pair cable specify differential data transmission over each pair with transformer coupling; the DC supply and load connections can be made to the transformer center-taps at each end. Each pair thus operates in common mode as one side of the DC supply, so two pairs are required to complete the circuit. The polarity of the DC supply may be inverted by crossover cables; the powered device must operate with either pair: spare pairs 4–5 and 7–8 or data pairs 1–2 and 3–6. Polarity is required on data pairs, and ambiguously implemented for spare pairs, with the use of a diode bridge.\n\nNotes:\nThree modes, A, B, and 4-pair are available. Mode A delivers power on the data pairs of 100BASE-TX or 10BASE-T. Mode B delivers power on the spare pairs. 4-pair delivers power on all four pairs. PoE can also be used on 1000BASE-T Ethernet, in which case there are no spare pairs and all power is delivered using the phantom technique.\n\nMode A has two alternate configurations (MDI and MDI-X), using the same pairs but with different polarities. In mode A, pins 1 and 2 (pair #2 in T568B wiring) form one side of the 48 V DC, and pins 3 and 6 (pair #3 in T568B) form the other side. These are the same two pairs used for data transmission in 10BASE-T and 100BASE-TX, allowing the provision of both power and data over only two pairs in such networks. The free polarity allows PoE to accommodate for crossover cables, patch cables and Auto MDI-X.\n\nIn mode B, pins 4–5 (pair #1 in both T568A and T568B) form one side of the DC supply and pins 7–8 (pair #4 in both T568A and T568B) provide the return; these are the \"spare\" pairs in 10BASE-T and 100BASE-TX. Mode B, therefore, requires a 4-pair cable.\n\nThe PSE (power sourcing equipment), not the PD (powered device), decides whether power mode A or B shall be used. PDs that implement only Mode A or Mode B are disallowed by the standard.\n\nThe PSE can implement mode A or B or both. A PD indicates that it is standards-compliant by placing a 25 kΩ resistor between the powered pairs. If the PSE detects a resistance that is too high or too low (including a short circuit), no power is applied. This protects devices that do not support PoE. An optional \"power class\" feature allows the PD to indicate its power requirements by changing the sense resistance at higher voltages. To stay powered, the PD must continuously use 5–10 mA for at least 60 ms with no more than 400 ms since last use or else it will be unpowered by the PSE.\n\nThere are two types of PSEs: endspans and midspans. Endspans (commonly called PoE switches) are Ethernet switches that include the power over Ethernet transmission circuitry. Midspans are power injectors that stand between a regular Ethernet switch and the powered device, injecting power without affecting the data.\n\nEndspans are normally used on new installations or when the switch has to be replaced for other reasons (such as moving from 10/100 Mbit/s to 1 Gbit/s or adding security protocols), which makes it convenient to add the PoE capability. Midspans are used when there is no desire to replace and configure a new Ethernet switch, and only PoE needs to be added to the network.\n\nIEEE 802.3at capable devices are also referred to as \"type 2\". An 802.3at PSE may also use layer2 communication to signal 802.3at capability.\n\nClass 4 can only be used by IEEE 802.3at (type 2) devices, requiring valid Class 2 and Mark 2 currents for the power up stages. An 802.3af device presenting a class 4 current is considered non-compliant and, instead, will be treated as a Class 0 device.\n\nThe setup phases are as follows:\n\nThe rules for this power negotiation are:\n\nSome Cisco manufactured WLAN access points and IP phones supporting a proprietary form of PoE many years before there was an IEEE standard for delivering PoE. Cisco's original PoE implementation is not software upgradeable to the IEEE 802.3af standard. Cisco's original PoE equipment is capable of delivering up to per port. The amount of power to be delivered is negotiated between the endpoint and the Cisco switch based on a power value that was added to the Cisco proprietary Cisco Discovery Protocol (CDP). CDP is also responsible for dynamically communicating the Voice VLAN value from the Cisco switch to the Cisco IP Phone.\n\nUnder Cisco's pre-standard scheme, the PSE (switch) will send a Fast Link Pulse (FLP) on the transmit pair. The PD (device) connects the transmit line to the receive line via a low pass filter. And thus the PSE gets the FLP in return. And a common mode current between pair 1 and 2 will be provided resulting in and default of allocated power. The PD has then to provide Ethernet link within to the auto-negotiation mode switch port. A later CDP message with a type-length-value tells the PSE its final power requirement. A discontinued link pulses shuts down power.\n\nIn 2014, Cisco created another non-standard PoE implementation called Universal Power over Ethernet (UPOE). UPOE can use all 4 pairs, after negotiation, to supply up to 60 W.\n\nA proprietary high-power development called LTPoE++ using a single CAT-5e Ethernet cable is capable of supplying varying levels at 38.7 W, 52.7 W, 70 W, and 90 W in addition to being backwards compatible with IEEE 802.at.\n\nPowerDsine, acquired by Microsemi in 2007, has been selling midspan power injectors since 1999 with its proprietary \"Power over LAN\" solution. Several companies such as Polycom, 3Com, Lucent and Nortel utilize PowerDsine's Power over LAN.\n\nThe common 100 Mbit/s passive applications use the pinout of 802.3af mode B - with DC plus on pins 4 and 5 and DC minus on 7 and 8 (see chart below) and data on 1-2 and 3-6. Gigabit passive injectors use a transformer on the data pins to allow power and data to share the cable and is typically compatible with 802.3af Mode A. In the common \"passive\" PoE system, the injector does not communicate with the powered device to negotiate its voltage or wattage requirements, but merely supplies power at all times. Passive midspan injectors up to 12 ports simplify installations. \n\nDevices needing 5 Volts cannot typically use PoE at 5 V on Ethernet cable beyond short distances (about ) as the voltage drop of the cable becomes too significant, so a 24 V or 48 V to 5 V DC-DC converter is required at the remote end. \n\nPassive PoE power sources are commonly used with a variety of indoor and outdoor wireless radio equipment, most commonly from Motorola (now Cambium), Ubiquiti, Mikrotik and others. Earlier versions of passive PoE 24VDC power sources shipped with 802.11a, 802.11g and 802.11n based radios are commonly 100 Mbit/s only. Specifications vary by manufacturer and model, but some of the common specifications include:\n\n\nPassive DC-to-DC injectors also exist which convert a 9 V to 36 V DC, or 36 V to 72 V DC power source to a stabilized 24 V 1 A, 48 V 0.5 A, or up to 48V 2.0A PoE feed with '+' on pins 4 & 5 and '−' on pins 7 & 8. These DC-to-DC PoE injectors are used in various telecom applications.\n\nThe ISO/IEC TR 29125 and Cenelec EN 50174-99-1 draft standards outline the cable bundle temperature rise that can be expected from the use of 4PPoE. A distinction is made between two scenarios: 1.) bundles heating up from the inside to the outside, and 2.) bundles heating up from the outside to match the ambient temperature. The second scenario largely depends on the way that the cable bundle has been installed, whereas the first is solely influenced by the physical make-up of the cable. In a standard U/UTP cable, the PoE-related temperature rise increases by a factor of 5. In a shielded cable, this value drops to between 2.5 and 3, depending on the design. Put another way, the temperature increases by twice as much in a U/UTP cable bundle than in a comparable bundle of S/FTP cables.\n\n"}
{"id": "945689", "url": "https://en.wikipedia.org/wiki?curid=945689", "title": "Quibdó", "text": "Quibdó\n\nQuibdó () is the capital city of Chocó Department, in western Colombia, on the Atrato River. The municipality of Quibdó has an area of 3,337.5 km² and a population of 100,000, mainly consisting of black and Zambo Colombians. \nIn prehistoric times the Chocó rainforest served as a major barrier isolating the Mesoamerican and Andean civilisations, and the extremely humid climate also failed to attract the Spanish colonists. The region was eventually granted by the Emberá Indians to the Franciscan order in 1648, but subsequent attacks by hostile tribes meant attempts at settlement were abandoned, only to be established again six years later.\n\nIt was not until the nineteenth century when there was interest in finding a shipping route between the Atlantic and Pacific Oceans to avoid traveling via the Straits of Magellan that the Chocó region again became of significant interest to European colonial powers, as the Atrato River Valley was thought the best possibility for this purpose by the explorer Alexander von Humboldt; however it was eventually shelved in favour of the Panama Canal. At the same time research on using the Chocó to connect the Pacific and Atlantic was being carried out, gold and platinum were discovered in the Atrato Valley and this ensured Quibdó’s growth and status as the chief town in the region.\n\nAnother crucial development at this time was the movement of freed black slaves into the Chocó, primarily engaging in shifting cultivation to cope with the extreme leaching from the super-humid climate, though fishing and the collection of forest products also helped these groups maintain their livelihood; 1853 watercolors by Manuel María Paz document two mestizo or European men with an Afro-Colombian street vendor, and depict the dress of Afro-Colombian and European women in the town square. These black communities established trade with highland cities such as Medellín via rough mule trails that lasted until the 1950s, after which a combination of population growth and declining values for the region’s natural resources gradually led to an economic downturn for the region and especially Quibdó.\n\nQuibdó has an extremely wet and cloudy tropical rainforest climate (Köppen \"Af\") without noticeable seasons and by a large margin the heaviest rainfall in South America and of any city of its size or greater—the wettest city of larger size, Monrovia in Liberia, receives less than Quibdó. The extreme rainfall cause is because the Andes to the east of the city block the westerly winds driven by the Intertropical Convergence Zone which throughout the year, owing to the Humboldt Current off the west coast of South America, remains centred in the north of the continent at Quibdó’s longitudes. The result is that the extremely unstable ascending air from the Intertropical Convergence Zone is consistently forced to rise over the Chocó plain and as it cools it gives up enormous quantities of moisture. What is more, due to the exuberant nature and biodiversity in the region, a biotic pump phenomena causes the Choco low-level-jet as another important factor in driving atmospheric moisture from the Pacific into the Colombian Andes .\n\nRain precipitates almost every day from clouds in intense thunderstorms with an eternal wet season year round; that means 309 days (84%) of days are rainy, and sunny periods seldom last more than a few hours after sunrise. Quibdó only has 1,276 hours of sunshine annually, making one of the cloudiest cities on the world, with the sunniest month is July, with 135 hours of sunshine.\n</div>\n\n"}
{"id": "2101106", "url": "https://en.wikipedia.org/wiki?curid=2101106", "title": "Reverse logistics", "text": "Reverse logistics\n\nReverse logistics is for all operations related to the reuse of products and materials. It is \"the process of moving goods from their typical final destination for the purpose of capturing value, or proper disposal. Remanufacturing and refurbishing activities also may be included in the definition of reverse logistics.\" Growing green concerns and advancement of green supply chain management concepts and practices make it all the more relevant. The number of publications on the topic of reverse logistics have increased significantly over the past two decades. The first use of the term \"reverse logistics\" in a publication was by James R. Stock in a White Paper titled \"Reverse Logistics,\" published by the Council of Logistics Management in 1992. The concept was further refined in subsequent publications by Stock (1998) in another Council of Logistics Management book, titled Development and Implementation of Reverse Logistics Programs, and by Rogers and Tibben-Lembke (1999) in a book published by the Reverse Logistics Association titled Going Backwards: Reverse Logistics Trends and Practices. The reverse logistics process includes the management and the sale of surplus as well as returned equipment and machines from the hardware leasing business. Normally, logistics deal with events that bring the product towards the customer. In the case of reverse logistics, the resource goes at least one step back in the supply chain. For instance, goods move from the customer to the distributor or to the manufacturer.\n\nWhen a manufacturer's product normally moves through the supply chain network, it is to reach the distributor or customer. Any process or management after the delivery of the product involves reverse logistics. If the product is defective, the customer would return the product. The manufacturing firm would then have to organise shipping of the defective product, testing the product, dismantling, repairing, recycling or disposing the product. The product would travel in reverse through the supply chain network in order to retain any use from the defective product. The logistics for such matters is reverse logistics.\n\nIn today's marketplace, many retailers treat merchandise returns as individual, disjointed transactions. \"The challenge for retailers and vendors is to process returns at a proficiency level that allows quick, efficient and cost-effective collection and return of merchandise. Customer requirements facilitate demand for a high standard of service that includes accuracy and timeliness. It’s the logistic company's responsibility to shorten the link from return origination to the time of resell.\" By following returns management best practices, retailers can achieve a returns process that addresses both the operational and customer retention issues associated with merchandise returns. Further, because of the connection between reverse logistics and customer retention, it has become a key component within Service Lifecycle Management (SLM), a business strategy aimed at retaining customers by bundling even more coordination of a company's services data together to achieve greater efficiency in its operations.\n\nReverse logistics is more than just returns management, it is \"activities related to returns avoidance, gatekeeping, disposal and all other after-market supply chain issues\". Returns management—increasingly being recognized as affecting competitive positioning—provides an important link between marketing and logistics. The broad nature of its cross-functional impact suggests that firms would benefit by improving internal integration efforts. In particular, a firm's ability to react to and plan for the influence of external factors on the returns management process is improved by such internal integration. In a firm's planning for returns, a primary factor is the remaining value of the material returning and how to recover that value. \"Returned goods, or elements of the product, could even be returned to suppliers and supply chain partners for them to re-manufacture\".\n\nThird-party logistics providers see that up to 7% of an enterprise's gross sales are captured by return costs. Almost all reverse logistics contracts are customized to fit the size and type of company contracting. The 3PL's themselves realize 12% to 15% profits on this business.\n\n\"Studies have shown that an average of 4% to 6% of all retail purchases are returned, costing the industry about $40 billion per year.\" \n\nReverse logistics research has also found that 84.6 percent of companies in the United States use secondary market and 70 percent see the secondary market as a \"competitive advantage.\"\n\nIn certain industries, goods are distributed to downstream members in the supply chain with the understanding that the goods may be returned for credit if they are not sold e.g., newspapers and magazines. This acts as an incentive for downstream members to carry more stock, because the risk of obsolescence is borne by the upstream supply chain members. However, there is also a distinct risk attached to this logistics concept. The downstream member in the supply chain might exploit the situation by ordering more stock than is required and returning large volumes. In this way, the downstream partner is able to offer high level of service without carrying the risks associated with large inventories. The supplier effectively finances the inventory for the downstream member. It is therefore important to analyze customers’ accounts for hidden costs.\n\nReusable packaging systems require a closed-loop logistics system. Examples include reusable pallets, bulk boxes such as Euro containers, Reusable bottles for milk, soda, and beer, compressed gas cylinders, beer kegs, etc.\n\nIn case of e-commerce business, many websites offer the flexibility of cash on delivery (COD) to their customers. Sometimes customers refuse the product at the time of delivery, as there is no commitment to take the product. Then the logistics service provider follows the process of reverse logistics on the refused cargo. It is also known as Return to Origin (RTO). In this process, the e-commerce company adds the refused cargo to its inventory stock again, after proper quality checks as per the company's rules.\n\nIn case of the Demonstration of Products to the client as part of Pre-Sales process, The Demonstration equipment is sent to the Customer and has to be returned to maintain Revolving Inventory.\n\n"}
{"id": "34084212", "url": "https://en.wikipedia.org/wiki?curid=34084212", "title": "Senftenberg Solarpark", "text": "Senftenberg Solarpark\n\nThe Senftenberg Solarpark is a photovoltaic power station located on former open-pit mining areas close to the city of Senftenberg, in Eastern Germany. The 78 MW Phase 1 of the plant was completed within three months and generates power for about 25,000 households. It was opened on September 24, 2011, by the Prime Minister of the German state of Brandenburg. The ground-based PV plant is designed for a total capacity of 148 MW.\n\nSolarpark Senftenberg/Schipkau\n"}
{"id": "6328047", "url": "https://en.wikipedia.org/wiki?curid=6328047", "title": "Solar power by country", "text": "Solar power by country\n\nMany nations have installed significant solar power capacity into their electrical grids to supplement or provide an alternative to conventional energy sources. \nSolar power plants use one of two technologies:\n\nWorldwide growth of photovoltaics is extremely dynamic and varies strongly by country. \nBy the end of 2016, cumulative photovoltaic capacity increased by more than 75 gigawatt (GW) and reached at least 303 GW, sufficient to supply approximately 1.8 percent of the world's total electricity consumption. \nThe top installers of 2016 were China, the United States, and India. \nThere are more than 24 countries around the world with a cumulative PV capacity of more than one gigawatt. \nAustria, Chile, and South Africa, all crossed the one gigawatt-mark in 2016. \nThe available solar PV capacity in Honduras is now sufficient to supply 12.5% of the nation's electrical power while Italy, Germany and Greece can produce between 7% and 8% of their respective domestic electricity consumption.\n\nAfter an almost two decade long hiatus, deployment of CSP resumed in 2007. \nHowever, the design for several new projects is being changed to cheaper photovoltaics. \nMost operational CSP stations are located in Spain and the United States, while large solar farms using photovoltaics are being constructed in an expanding list of geographic regions.\nAs of January 2017, the largest solar power plants in the world are the 850 MW Longyangxia Dam Solar Park in China for PV and the 377 MW Ivanpah Solar Power Facility in the United States for CSP.\n\nMany African countries receive on average a very high amount of days per year with bright sunlight, especially the dry areas, which include the deserts (such as the Sahara) and the steppes (such as the Sahel). This gives solar power the potential to bring energy to virtually any location in Africa without the need for expensive large scale grid level infrastructural developments.\nThe distribution of solar resources across Africa is fairly uniform, with more than 85% of the continent's landscape receiving at least 2,000 kWh/(m² year). A recent study indicates that a solar generating facility covering just 0.3% of the area comprising North Africa could supply all of the energy required by the European Union.\n\nSolar power in Morocco is enabled by the country having one of the highest rates of solar insolation among other countries— about 3,000 hours per year of sunshine but up to 3,600 hours in the desert. Morocco has launched one of the world’s largest solar energy projects costing an estimated $9 billion. The aim of the project is to create 2,000 megawatts of solar generation capacity by the year 2020. Five solar power stations are to be constructed, including both photovoltaic and concentrated solar power technology. The Moroccan Agency for Solar Energy (MASEN), a public-private venture, has been established to lead the project. The first plant will be commissioned in 2015, and the entire project in 2020. Once completed, the solar project will provide 38% of Morocco’s annual electricity generation.\n\nSouth Africa had 1329 MW of PV installations and 100 MW of concentrating solar thermal at the end of 2016. It is expected to reach an installed capacity 8,400 MW by 2030, along with 8,400 MW of wind power. The country's insolation greatly exceeds the average values in Europe, Russia, and most of North America.\n\nChina is leading the world in solar PV generation, with the total installed capacity exceeding 100 GW. China is the world's largest market for both photovoltaics and solar thermal energy. and in the last few years, more than half of the total PV additions came from the country. Solar power in the People's Republic of China is one of the biggest industries and the subsidies by the government have helped in bringing down the cost of solar power, not only in China, but the whole world. Three of the top four largest photovoltaic power stations in operation are located in China, including the world's largest Tengger Desert Solar Park. China also leads the world in solar water heating with 290 GWth in operation at the end of 2014, accounting for about 70% of the total world capacity. China's goal is to reach 1,300 GW of Solar Capacity by 2050.\n\nIndia has the world's third fastest expanding solar power program (next only to China & USA). In the year 2017 alone India added a record 9,255 MW of solar power with another 9,627 MW of solar projects under development. India launched its National Solar Mission in 2010 under the National Action Plan on Climate Change, with plans to generate 20 GW by 2022. This target has been achieved four years ahead of its deadline with India surpassing 20 GW of installed solar capacity in January 2018. In January 2015, Indian Prime Minister Narendra Modi announced an initiative to increase the solar capacity to 100 GW and total renewable power capacity to 175 gigawatts (GW) by 2022. This target is ambitious considering the worldwide installed solar capacity at that time was 177 GW, out of which only 2.5 GW was installed in India.\n\nTo reach the goal of 100 GW of installed solar capacity by 2022, Modi's government has set a target to auction at least 77 gigawatts of additional solar power capacity by March 2020. A total of 1.2 GW of solar power is tendered in the first week of 2018 and a solar power tender of 20 GW, world's largest so far, is to be auctioned off in one go in 2018. Several large grid-scale solar parks are in operation, several of which are among the world's largest such as Kurnool Ultra Mega Solar Park with the capacity of 1,000 MW, the Kamuthi Solar Power Project with the capacity of 648 MW, the 345 MW Charanka Solar Park, the 480 MW Bhadla Solar Park with a proposed capacity of 2,255 MW and the Gujarat solar parks with a combined capacity of 605 MW. In July 2017, Indian Railways rolled out trains with rooftop solar to power the lights, fans and displays inside the coaches. Cochin International Airport, seventh busiest in India, is the first one in the world to run entirely on solar power, handling more than 1,000 flights a week. Similarly, the Union Territory of Diu is fully run by solar power.\n\nSolar power features prominently in Modi government’s USD $2.5 billion SAUBHAGYA scheme launched in July 2015 to electrify every Indian household by 2019 — a huge task considering around 300 million people were without electricity. The use of local mini-grids run on solar power is “a big part of the push, with 60 percent of new connections expected to be to renewable power\", according to a report by the International Energy Agency. The government provides subsidy of up to 90% of the upfront capital cost to install solar-powered water pumping systems for irrigation and drinking water. As of 30 November 2017, more than 142,000 solar pumps have been installed to irrigate the agricultural fields. This scheme weans farmers away from diesel-powered pumps and generates extra income for them by allowing to sell surplus power to the grid. It is one of the innovative ways that the government is empowering the rural population with the help of solar energy by addressing specific issues such as water availability. The solar panels are being built over the irrigation canals to preserve water from evaporation in drought-prone sunny areas. The world's first canal-top solar project was set up on Narmada in Gujarat in 2012. For the last mile connectivity in remote and inaccessible areas, the government provides solar power packs of 200 to 300 watt-peak (Wp), along with battery bank, that includes five LED lights, one DC fan and one DC power plug. Other schemes includes Solar Street Light Scheme, providing solar direct current lighting systems, solar lanterns, solar cookers, etc.\n\nIn January 2016, the Prime Minister of India, Narendra Modi, and the former President of France, François Hollande, laid the foundation stone for the headquarters of the International Solar Alliance (ISA) in Gwalpahari, India, an alliance of 121 countries, announced at the Paris COP21 climate summit. The ISA focuses on promoting and developing solar energy and reducing production and development costs through wider deployment of solar technologies in the developing world. On June 30, 2016, the alliance entered into a partnership with the World Bank for accelerating mobilization of finance for solar energy — an estimated US $1000 billion in investments that will be needed by 2030, to meet ISA's goals for the massive deployment of affordable solar energy worldwide.\n\nAt the World Future Energy Summit (WFES) held in Abu Dhabi in January 2018, the government of India announced the setting up of a $350 million solar development fund to enable financing of solar projects. Prime Minister Narendra Modi promoted solar energy during the plenary speech at World Economic Forum annual meet in Davos in 2018 and invited investments in the sector in India promising ease of doing business. Modi's ambitious plan when announced in the leading up to the Paris COP21 climate summit received much skepticism and the government's strategy to scale-up the renewable energy by relying on competitive bidding to reduce the cost was regarded as infeasible. However, starting around 2016-2017, new renewable energy became cheaper to build than running existing coal-fired plants in India. As of January 2018, 65% of coal power generation in India is being sold at higher rates than new renewable energy bids in competitive power auctions. India has scrapped tenders for coal-fired power stations and around 80% of new coal-fired power plants under planning have been halted or canceled. In the month of May 2017 alone, plans for building coal power for nearly 14 GW – about the same as the total amount in the UK – were canceled on account of declining solar costs. Analyst Tim Buckley said “Measures taken by the Indian Government to improve energy efficiency coupled with ambitious renewable energy targets and the plummeting cost of solar has had an impact on existing as well as proposed coal fired power plants, rendering an increasing number as financially unviable. India’s solar tariffs have literally been free falling in recent months.\" As reported by NYTimes in May 2017, \"According to research released last week at a United Nations climate meeting in Germany, China and India should easily exceed the targets they set for themselves in the 2015 Paris Agreement... India is now expected to obtain 40 percent of its electricity from non-fossil fuel sources by 2022, eight years ahead of schedule.\"\n\nSolar power in Japan has been expanding since the late 1990s. By the end of 2017, cumulative installed PV capacity reached over 50 GW with nearly 8 GW installed in the year 2017. The country is a leading manufacturer of solar panels and is in the top 4 ranking for countries with the most solar PV installed. Overall installed capacity is now estimated to be sufficient to supply 2.5% of the nation's annual electricity demand. The insolation is good at about 4.3 to 4.8 kWh/(m²·day).\n\nJapan was the world's second largest market for solar PV growth in 2013 and 2014, adding a record 6.9 GW and 9.6 GW of nominal nameplate capacity, respectively.\n\nPakistan has set up a solar power park, funded by the Chinese company TBEA, in the Cholistan desert near Yazman, about 30 kilometers from the eastern city of Bahawalpur. The solar project, which is set up on 5,000 acres, is producing 100 MW . Another Chinese company, Zonergy is setting up 900MW of Solar Power Plant in the same region.\n\nThe first unit was completed with a cost of 15 billion rupees in a short period of eleven months. The electricity generated by the project will be added to the national grid through grid stations and power supply transmission lines. Second phase of the park will comprise 900 MW which will be completed with the help of Chinese Government.\n\nIn 2012, the Philippines generated a modest 1,320 MWh of solar energy.\n\nThe Sinan solar power plant is a 24 MW photovoltaic power station in Sinan, Jeollanam-do, South Korea. , it is the largest photovoltaic installation in Asia. The project was developed by the German company Conergy and it cost US$150 million. It was built by the Dongyang Engineering & Construction Corporation.\n\nThe government has a long-term plan to make the solar capacity become 4,500 MW by 2020 and to make 7.5 million Taiwan residents to utilize solar energy by 2030. To give further incentives, the government has designated solar energy and LED industries as two industries to actively develop in the near future.\n\nIn 2015, Thailand has more solar power capacity than all the rest of Southeast Asia combined. Thailand's solar capacity will rise to 2,500-2,800 MW in the end of 2015 from about 1,300 MW in 2014. Thailand aims to increase its solar capacity to 6,000 MW by 2036. That would account for 9% of total electricity generation.\n\nThere is no oil on Israeli land and the country's tenuous relations with its oil-rich neighbors (see Arab–Israeli conflict) has made the search for a stable source of energy a national priority. So Israel has embraced solar energy.\nIsraeli innovation and research has advanced solar technology to a degree that it is almost cost-competitive with fossil fuels. Its abundant sun made the country a natural location for the promising technology. The high amount of sunshine received by the Negev Desert every year has spurred an internationally renowned solar research and development industry, with Arnold Goldman (founder of Luz, Luz II and BrightSource Energy), Harry Tabor and David Faiman of the National Solar Energy Center its more prominent members. At the end of 2008 a feed-in tariff scheme was approved, which immediately put in motion the building of many residential and commercial solar energy power station projects. Luz and Bright Source R&D centers in Jerusalem pioneered industrial scale solar energy fields with initial installations in California's Mojave Desert.\n\nThe Saudi agency in charge of developing the nations renewable energy sector, Ka-care, announced in May 2012 that the nation would install 41 gigawatts of solar capacity by 2032, this plan was later was revised to 9.5 GW installed capacity. \nAt the time of this announcement, Saudi Arabia had only 0.003 gigawatts of installed solar energy capacity.\n\nIn 2018 there has been a proposal for a total of 200 GW of solar power capacity by 2030. The newly announced project is estimated to cost $200 billion through 2030.\n\nIn 2013, the Shams solar power station, a 100 MW Concentrated solar power plant near Abu Dhabi became operational. The US$600 million Shams 1 is the largest CSP plant outside the United States and Spain and is expected to be followed by two more stations, Shams 2 and Shams 3.\n\nEuropean deployment of photovoltaics has slowed down considerably since the record year of 2011. This is mainly due to the strong decline of new installations in some major markets such as Germany and Italy, while the United Kingdom and some smaller European countries are still expected to break new records in 2014.\nSpain deployed about 350 MW (+18%) of concentrated solar power (CSP) in 2013, and remains a worldwide leader of this technology. European countries still account for about 60 percent of worldwide deployed capacity of solar power in 2013.\n\nAustria had 421.7 MW of photovoltaics at the end of 2012, 234.5 MW of which was installed that year. Most of it is grid connected. Photovoltaic deployment in Austria had been rather modest for many years, while in other European countries, such as Germany, Italy or Spain installations were booming with new records year after year until 2011. The tide has turned in 2012. New PV installations jumped to more than 200 megawatt per year in Austria in an overall declining European solar market. The European Photovoltaic Industry Association forecasts, that Austria, together with other midsized countries, will contribute significantly to European PV deployment in the coming years.\n\nIn October 2009, the city of Antwerp announced that they wanted to install 2,500 m² of solar panels on the roofs of public buildings, which would be worth 265,000 kWh per annum.\n\nIn December 2009, Katoen Natie announced that they would install 800,000 m² of solar panels in various places, including Antwerp. It is expected that the installed solar power in the Flemish region will be increased by 25% when finished, resulting in the largest installation in Europe., the total cost being 166 million euros.\n\nBulgaria had seen a record year in 2012 when its PV capacity multiplied several times over to more than 1 GW. In 2013, however, further deployment came to a halt.\n\nGermany is among the top 4 ranking countries in terms of installed photovoltaic solar capacity and number one regarding per capita installation of PV. The overall capacity has reached 42.98 gigawatts (GW) by the end of 2017. Photovoltaics contribute almost 6% to the national electricity demands. Germany has seen an outstanding period of photovoltaic installations from 2010 until 2012. During this boom, about 22 GW, or a third of the worldwide PV installations of that period was deployed in Germany alone. However, the boom period ended in 2012, and Germany's national PV market has since declined significantly, due to the amendments in the German Renewable Energy Act (EEG) that reduced feed-in tariffs and set constraints on utility-scaled installations, limiting their size to no more than 10 MW.\n\nThe current version of the EEG only guarantees financial assistance as long as the overall PV capacity has not yet reached 52 GW. It also foresees to regulate annual PV growth within a range of 2.5 GW to 3.5 GW by adjusting the guaranteed fees accordingly. The legislative reforms stipulates a 40 to 45 percent share from renewable energy sources by 2025 and a 55 to 60 percent share by 2035.\n\nLarge PV power plants in Germany include Senftenberg Solarpark, Finsterwalde Solar Park, Lieberose Photovoltaic Park, Strasskirchen Solar Park, Waldpolenz Solar Park, and Köthen Solar Park.\n\nBy September 2013, the total installed photovoltaic capacity in Greece had reached 2,523.5 MWp from which the 987.2 MWp were installed in the period between January–September 2013 despite the unprecedented financial crisis. Greece ranks 5th worldwide with regard to per capita installed PV capacity. It is expected that PV produced energy will cover up to 7% of the country's electricity demand in 2014.\n\nA large solar PV plant is planned for the island of Crete. Research continues into ways to make the actual solar collecting cells less expensive and more efficient. Smaller solar PV farms exist throughout the country.\n\nItaly added nearly 400 MW of solar PV capacity in the year 2017 reaching a total installed PV capacity of around 19.7 GW.\n\nAt the end of 2010 there were 155,977 solar PV plants, with a total capacity of 3,469.9 MW. \nThe number of plants and the total capacity surged in 2009 and 2010 following high incentives from \"Conto Energia\". \nThe total power capacity installed tripled and plants installed doubled in 2010 compared to 2009, with an increase of plant's average dimensions.\n\nEnergy production from photovoltaics was 1,905.7 GWh in 2010. \nAnnual growth rates were fast in recent years: 251% in 2009 and 182% in 2010. More than a fifth of the total production in 2010 came from the southern region of Apulia.\n\nIn December 2012, solar PV in Italy provided employment to 100,000 people especially in design and installation.\n\nA large photovoltaic power project, the Serpa solar power plant, has been completed in Portugal, in one of Europe's sunniest areas. The 11 megawatt plant covers and comprises 52,000 PV panels. The panels are raised 2 metres off the ground and the area will remain productive grazing land. The project will provide enough energy for 8,000 homes and will save an estimated 30,000 tonnes of carbon dioxide emissions per year.\n\nThe Moura photovoltaic power station is located in the municipality of Moura, in the interior region of Alentejo, Portugal.Its construction involves two stages, with the first one being constructed in 13 months and completed in 2008, and the other will be completed by 2010, at a total cost of €250 million for the project.\n\nRomania has an installed capacity of 1.2 GW as of 2014. Romania is located in an area with a good solar potential of 210 sunny days per year and with an annual solar energy flux between 1,000 kWh/m2/year and 1,300 kWh/m2/year. The most important solar regions of Romania are the Black Sea coast, Dobrogea and Oltenia.\n\nCurrent production of 5 MW is very modest, however there are plans for an expansion in capacity by 70 MW in 2012-13 in a $210 million joint project by Rosnano and Renova. The development of renewable energy in Russia has been held back by the lack of a conducive framework and government policy.\n\nSpain was an early adopter in the development of solar energy, since it is one of the countries of Europe with more hours of sunshine. The Spanish government committed to achieving a target of 12 percent of primary energy from renewable energy by 2010 with an installed solar generating capacity of 3000 megawatts (MW). Spain is the top tenth in the installed PV solar capacity and used to export 80 percent of solar power output to Germany. Total solar power in Spain reached nearly 7 GW by the end of 2016 including both installed PV and CSP. Nearly 8 TWh of electricity was generated from photovoltaics, and 5 TWh from CSP plants in 2016. Solar PV accounted for nearly 3% of total electricity generation in 2016 along with an additional of 1.9% from solar thermal.\n\nThrough a ministerial ruling in March 2004, the Spanish government removed economic barriers to the connection of renewable energy technologies to the electricity grid. The Royal Decree 436/2004 equalized conditions for large-scale solar thermal and photovoltaic plants and guaranteed feed-in tariffs, which led to a boost in solar power adoption in Spain. In the wake of the 2008 financial crisis, the Spanish government drastically cut its subsidies for solar power and capped future increases in capacity at 500 MW per year leading to a stagnation in the new installations.\n\nRegistered solar capacity of Turkey stood at 3,420 MW by the end of 2017, although the actual installation can be lower. The increase in registrations mostly happened in December and was attributed to a reduction in feed-in tariffs starting from 2018 (from 0.13 USD to 0.10 USD).\n\nAt the end of 2011, there were 230,000 solar power projects in the United Kingdom, with a total installed generating capacity of 750 megawatts (MW). By February 2012 the installed capacity had reached 1,000 MW. Solar power use has increased very rapidly in recent years, albeit from a small base, as a result of reductions in the cost of photovoltaic (PV) panels, and the introduction of a Feed-in tariff (FIT) subsidy in April 2010. In 2012, the government said that 4 million homes across the UK will be powered by the sun within eight years, representing 22,000 MW of installed solar power capacity by 2020. As of April 2015, PV capacity had risen to 6,562 MW across 698,860 installations. The latest government figures indicates UK solar photovoltaic (PV) generation capacity has reached 12,404 MW in December 2017.\n\nSarnia Photovoltaic Power Plant near Sarnia, Ontario, was in September 2010 the world's largest photovoltaic plant with an installed capacity of 80 MW. until surpassed by a plant in China. The Sarnia plant covers and contains about 966,000 square metres (96.6 ha), which is about 1.3 million thin film panels. The expected annual energy yield is about 120,000 MW·h, which if produced in a coal-fired plant would require emission of 39,000 tonnes of CO per year.\n\nCanada has many regions that are sparsely populated and difficult to access, but also does not have optimal access to sunlight given the high latitudes of much of the country. Photovoltaic cells are increasingly used as standalone units, mostly as off-grid distributed electricity generation to power remote homes, telecommunications equipment, oil and pipeline monitoring stations and navigational devices. The Canadian PV market has grown quickly and Canadian companies make solar modules, controls, specialized water pumps, high efficiency refrigerators and solar lighting systems. Ontario has subsidized solar power energy to promote its growth.\n\nOne of the most important uses for PV cells is in northern communities, many of which depend on high-cost diesel fuel to generate electricity. Since the 1970s, the federal government and industry has encouraged the development of solar technologies for these communities. Some of these efforts have focused on the use of hybrid systems that provide power 24 hours a day, using solar power when sunlight is available, in combination with another energy source.\n\nThe 246MW El Romero solar photovoltaic plant open in November 2016 at Vallenar in the Atacama region It was the largest solar farm in Latin America when it opened.\n\nBy the first half of 2015 Chile reached 546 MW of PV installed capacity, and 1,647 MW are under construction.\n\nIn the Dominican Republic, the Monte Plata Project is the largest operating solar plant in the Caribbean with an installed capacity of 69MW.\n\nIn 2014, a 1.6 MW photovoltaic rooftop system at a seaside resort, located near the parish capital, Lucea in the parish of Hanover, was inaugurated. It was developed by Sofos Jamaica, and is the largest in Jamaica until a 20 MW utility-scale solar PV plant is constructed in the Parish of Clarendon in 2015.\n\nNo central database yet exists with information on installed capacity but, web searches reveal media articles, press releases and vendor web pages that share some details. Based on these sources up to the middle of 2015, there was over 3.7 MW connected to the grid but, a sizeable portion of that total, including the 1.6 MW rooftop system of a seaside resort and a commercial 500 kW-system in the country's capital, Kingston, do not feed power back to the grid despite being interconnected.\n\nMexico was the greatest solar energy producer in Latin America until passed by Chile. It is planning a solar trough based plant with 30 MW which will use a combined cycle gas turbine about 400 MW to provide electricity to the city of Agua Prieta, Sonora. To date, the World Bank has given US$50 million to finance this project.\n\nSolar power in the United States includes utility-scale solar power plants as well as local distributed generation, mostly from rooftop photovoltaics. Installations have been growing rapidly in recent years as costs have declined with the U.S. hitting over 50 GW of installed solar PV capacity at the end of 2017. United States is in the top 4 ranking for countries with the most solar PV installed. The American Solar Energy Industries Association projects that total solar PV capacity will reach over 100 GW by 2021.\n\nElectrical generation has been rising in tandem with capacity as U.S. Energy Information Administration data show that utility-scale solar power generated 0.65% of total U.S. electricity in 2015, up from 0.01% in 2005. This figure is even higher in certain states, already reaching 7% of generation in California for example.\n\nThe United States conducted much early research in photovoltaics and concentrated solar power and is among the top countries in the world in deploying the technology, being home to 4 of the 10 largest utility-scale photovoltaic power stations in the world as of 2017. The energy resource continues to be encouraged through official policy with 29 states having set mandatory renewable energy targets as of October 2015, solar power being specifically included in 20 of them. Aside from utility projects, roughly 784,000 homes and businesses in the nation have installed solar systems through the second quarter of 2015.\n\nA number of Pacific island states have committed to high percentages of renewable energy use, both to serve as an example to other countries and to cut the high costs of imported fuels. A number of solar installations have been financed and assisted by Japan, New Zealand and the United Arab Emirates. Solar farms have gone online in Tuvalu, Fiji and Kiribati. UAE-Pacific Partnership Fund solar projects completed by Masdar in 2016 included: 1MW in the Solomon Islands, 500 kW in Nauru, 600 kW in the Marshall Islands, 600 kW in Micronesia and a 450 kW solar-diesel hybrid plant in Palau. American Samoa has 2 MW of solar installed at Pago Pago Airport.\n\nAustralia had over 10,131 megawatts (MW) of installed photovoltaic (PV) solar power by September 2018. The largest solar power station in Australia is the 220 MW Bungala solar plant. Other significant solar arrays include the 150 MW Coleambally Solar Farm, 148 MW Ross River Solar Farm, 124 MW Sun Metals Solar Farm and the 110 MW Darling Downs Solar Farm.\n\nA 9 MW (megawatts, electrical) solar thermal `coal saver' system was constructed at Liddell power station. The system used `compact linear Fresnel reflector' technology developed in Australia. It provided solar-powered steam to the 600 MW black coal power station's boiler feedwater heater. By 2016, it was \"effectively\" closed and an effort to build a similar 40 MW solar boost at Kogan Creek coal power station was stopped.\n\nSolar power in New Zealand currently only generates 0.1 percent of New Zealand's electricity since more emphasis has been placed on hydroelectric, geothermal, and wind power in New Zealand's push for renewable energy. Solar power systems were installed in 42 schools in New Zealand in the Schoolgen program, a program developed by Genesis Energy to educate students in solar power. Each participating school has a 2 kW solar panel. Between February 2007 and 29 December 29, 2012, 395.714 MWh were produced.\n\nIn 2010, New Zealand's largest thin film solar array was the 20 kW array installed at Hubbard Foods A 21.6 kW photovoltaic array was installed in Queenstown in 2009. In April 2012, New Zealand's largest solar power plant was the 68.4 kW array installed to meet 70% of the electricity needs of South Auckland Forging Engineering Ltd, which is expected to pay for itself in eight to nine years.\n"}
{"id": "11255291", "url": "https://en.wikipedia.org/wiki?curid=11255291", "title": "The Climate Registry", "text": "The Climate Registry\n\nThe Climate Registry (TCR) is a non-profit organization governed by U.S. states and Canadian provinces and territories. TCR designs and operates voluntary and compliance greenhouse gas (GHG) reporting programs globally, and assists organizations in measuring, reporting and verifying the carbon in their operations in order to manage and reduce it. TCR also consults with governments nationally and internationally on all aspects of GHG measurement, reporting, and verification.\n\nEstablished in 2007, The Climate Registry was formed to continue the work of the California Climate Action Registry (CCAR). Created by the State of California in 2001, CCAR promoted and protected businesses’ early actions to manage and reduce their greenhouse gas (GHG) emissions. Through the state mandate, CCAR established protocols to guide emissions inventories and manage an online reporting tool, the Climate Action Registry Reporting Tool (CARROT), to serve as a central database for emissions reports. Members of CCAR served as true leaders in environmental responsibility and were among the first in the world to measure their emissions according to comprehensive and rigorous standards and make their emissions publicly accessible online. Together, CCAR and its members influenced California climate change policy, including Assembly Bill 32 (AB 32), and worked to ensure proper recognition from the state for early actions to reduce emissions. Recognizing climate change is a global issue and that success in emissions reporting must be based on consistent data in an integrated system that stretched beyond California’s borders, CCAR was instrumental in establishing TCR with the mission of expanding CCAR’s emissions reporting work to include all of North America. CCAR accepted its last emissions inventory reports for 2009 in December 2010 and officially transitioned its members to The Climate Registry. CCAR is now a program of The Climate Registry’s sister organization, the Climate Action Reserve.\n\nAt the launch of The Climate Registry, the following were the participants:\nUS States:\n\nCanadian provinces:\n\nAs of September 24, 2009 all Canadian Provinces and Territories are participating in the greenhouse gas registry.\n\nNative American tribes:\n\n\n"}
{"id": "9915610", "url": "https://en.wikipedia.org/wiki?curid=9915610", "title": "Tornado family", "text": "Tornado family\n\nA tornado family is a series of tornadoes spawned by the same supercell thunderstorm. These families form a line of successive or parallel tornado paths and can cover a short span or a vast distance. Tornado families are sometimes mistaken as a single continuous tornado, especially prior to the 1970s. Sometimes the tornado tracks can overlap and expert analysis is necessary to determine whether or not damage was created by a family or a single tornado. In some cases, such as the Hesston-Goessel, Kansas tornadoes of March 1990, different tornadoes of a tornado family merge, making discerning whether an event was continuous or not more difficult.\n\nSome tornado damage remains a mystery even today due to a lack of evidence. The Tri-State Tornado of March 1925 was one such tornado. It could either have been the longest single tornado recorded or a family of tornadoes. A thorough re-analyses project found that it was probably one continuous tornado for most of its path, likely bounded by separate tornadoes at the beginning and end of the very long track (VLT) tornado, and likely another significant tornado spawned many miles later. However, many other exceptionally VLT events were later found to be tornado families with much shorter tornado path segments than originally thought, notably the Woodward, Oklahoma tornado family of April 1947 and the Charleston-Mattoon, Illinois tornado family of May 1917.\n\nTornado families can be a result of satellite tornadoes, cyclic tornadogenesis, or some combination thereof. Intense downbursts may also cause damage paths to appear continuous, although this was more an issue for historic tornadoes as such damage usually is now distinguishable as from straight-line winds. Especially when newly forming, tornadoes may sometimes exhibit brief breaks in the damage path even as the parent circulation is continuous. Such events may be considered as \"skipping\", a term that originally referred to what now is typically a tornado family. Successive tornadoes may be considered by some as separate tornadoes (and thus constituting a tornado family) only when spawned by a new tornadocyclone or low-level mesocyclone (and wall cloud).\n\n"}
{"id": "263487", "url": "https://en.wikipedia.org/wiki?curid=263487", "title": "Trans fat", "text": "Trans fat\n\nTrans fat, also called unsaturated fatty acids or trans fatty acids, are a type of unsaturated fat that occur in small amounts in nature, but became widely produced industrially from vegetable fats starting in the 1950s for use in margarine, snack food, packaged baked goods, and for frying fast food. Trans fat has been shown to be associated consistently, in an intake-dependent way, with increased risk of coronary artery disease, a leading cause of death in Western nations.\n\nFats contain long hydrocarbon chains, which can be either unsaturated, i.e., have double bonds, or saturated, i.e., have no double bonds. In nature, unsaturated fatty acids generally have \"cis\" as opposed to \"trans\" configurations. In food production, liquid cis-unsaturated fats such as vegetable oils are hydrogenated to produce saturated fats, which have more desirable physical properties, e.g., they melt at a desirable temperature (30–40 °C). Partial hydrogenation of the unsaturated fat converts some of the cis double bonds into trans double bonds by an isomerization reaction with the catalyst used for the hydrogenation, which yields a trans fat.\n\nAlthough trans fats are edible, consuming trans fats has been shown to increase the risk of coronary artery disease in part by raising levels of low-density lipoprotein (LDL, often termed \"bad cholesterol\"), lowering levels of high-density lipoprotein (HDL, often termed \"good cholesterol\"), increasing triglycerides in the bloodstream and promoting systemic inflammation.\n\nTrans fats also occur naturally, e.g., vaccenic acid and some isomers of conjugated linoleic acid (CLA). These trans fats occur naturally in meat and dairy products from ruminants. Butter, for example, contains about 3% trans fat. Two Canadian studies have shown that vaccenic acid could be beneficial compared to hydrogenated vegetable shortening, or a mixture of pork lard and soy fat, by lowering total LDL and triglyceride levels. A study by the US Department of Agriculture showed that vaccenic acid raises both HDL and LDL cholesterol, whereas industrial trans fats only raise LDL with no beneficial effect on HDL.\n\nIn light of recognized evidence and scientific agreement, nutritional authorities consider all trans fats equally harmful for health and recommend that their consumption be reduced to trace amounts. The World Health Organization recommended that trans fats make up no more than 1% of a person's diet in 2003 and, in 2018, introduced a 6-step guide to eliminate industrially-produced trans-fatty acids from the global food supply.\n\nIn many countries, there are legal limits to trans fat content. Trans fats levels can be reduced or eliminated using saturated fats such as lard, palm oil, or fully hydrogenated fats, or by using interesterified fat. Other alternative formulations can also allow unsaturated fats to be used to replace saturated or partially hydrogenated fats. Hydrogenated oil is not a synonym for trans fat: complete hydrogenation removes all unsaturated fats.\nNobel laureate Paul Sabatier worked in the late 1890s to develop the chemistry of hydrogenation, which enabled the margarine, oil hydrogenation, and synthetic methanol industries. Whereas Sabatier considered hydrogenation of only vapors, the German chemist Wilhelm Normann showed in 1901 that liquid oils could be hydrogenated, and patented the process in 1902. During the years 1905–1910, Normann built a fat-hardening facility in the Herford company. At the same time, the invention was extended to a large-scale plant in Warrington, England, at Joseph Crosfield & Sons, Limited. It took only two years until the hardened fat could be successfully produced in the plant in Warrington, commencing production in the autumn of 1909. The initial year's production totalled nearly 3,000 tonnes.\nIn 1909, Procter & Gamble acquired the United States rights to the Normann patent; in 1911, they began marketing the first hydrogenated shortening, Crisco (composed largely of partially hydrogenated cottonseed oil). Further success came from the marketing technique of giving away free cookbooks in which every recipe called for Crisco.\n\nNormann's hydrogenation process made it possible to stabilize affordable whale oil or fish oil for human consumption, a practice kept secret to avoid consumer distaste.\n\nBefore 1910, dietary fats in industrialized nations consisted mostly of butterfat, beef tallow, and lard. During Napoleon's reign in France in the early 19th century, a type of margarine was invented to feed the troops using tallow and buttermilk. It was not accepted in the United States. In the early 20th century, soybeans began to be imported into the United States as a source of protein; soybean oil was a by-product. What to do with that oil became an issue. At the same time, there was not enough butterfat available for consumers. The method of hydrogenating fat and turning a liquid fat into a solid one had been discovered, and now the ingredients (soybeans) and the \"need\" (shortage of butter) were there. Later, the means for storage, the refrigerator, was a factor in trans fat development. The fat industry found that hydrogenated fats provided some special features to margarines, which allowed margarine, unlike butter, to be taken out of a refrigerator and immediately spread on bread. By some minor changes to the chemical composition of hydrogenated fat, such hydrogenated fat was found to provide superior baking properties compared to lard. Margarine made from hydrogenated soybean oil began to replace butterfat. Hydrogenated fat such as Crisco and Spry, sold in England, began to replace butter and lard in baking bread, pies, cookies, and cakes in 1920.\n\nProduction of hydrogenated fats increased steadily until the 1960s, as processed vegetable fats replaced animal fats in the United States and other Western countries. At first, the argument was a financial one due to lower costs; advocates also said that the unsaturated trans fats of margarine were healthier than the saturated fats of butter.\n\nAs early as 1956, there were suggestions in the scientific literature that \"trans\" fats could be a cause of the large increase in coronary artery disease but after three decades the concerns were still largely unaddressed. Instead, by the 1980s, fats of animal origin had become one of the greatest concerns of dieticians. Activists, such as Phil Sokolof, who took out full page ads in major newspapers, attacked the use of beef tallow in McDonald's french fries and urged fast-food companies to switch to vegetable oils. The result was an almost overnight switch by most fast-food outlets to trans fats.\n\nStudies in the early 1990s, however, brought renewed scrutiny and confirmation of the negative health impact of trans fats. In 1994, it was estimated that trans fats caused 20,000 deaths annually in the United States from heart disease.\n\nMandatory food labeling for \"trans\" fats was introduced in several countries. Campaigns were launched by activists to bring attention to the issue and change the practices of food manufacturers. In January 2007, faced with the prospect of an outright ban on the sale of their product, Crisco was reformulated to meet the United States Food and Drug Administration (FDA) definition of \"zero grams trans fats per serving\" (that is less than one gram per tablespoon, or up to 7% by weight; or less than 0.5 grams per serving size) by boosting the saturation and then diluting the resulting solid fat with unsaturated vegetable oils.\n\nA University of Guelph research group has found a way to mix oils (such as olive, soybean, and canola), water, monoglycerides, and fatty acids to form a \"cooking fat\" that acts the same way as trans and saturated fats.\n\nIn chemical terms, \"trans fat\" is a fat (lipid) molecule that contains one or more double bonds in \"trans\" geometric configuration.\n\nA double bond may exhibit one of two possible configurations: \"trans\" or \"cis\". In \"trans\" configuration, the carbon chain extends from opposite sides of the double bond, whereas, in \"cis\" configuration, the carbon chain extends from the same side of the double bond. The \"trans\" molecule is a straighter molecule. The \"cis\" molecule is bent.\n\nA fatty acid is characterized as either \"saturated\" or \"unsaturated\" based on the presence of double bonds in its structure. If the molecule contains no double bonds, it is said to be saturated; otherwise, it is unsaturated to some degree.\n\nOnly unsaturated fats can be \"trans\" \"or\" \"cis\" fat, since only a double bond can be locked to these orientations. Saturated fatty acids are never called \"trans fats\" because they have no double bonds. Thus, all their bonds are freely rotatable. Other types of fatty acids, such as crepenynic acid, which contains a triple bond, are rare and of no nutritional significance.\n\nCarbon atoms are tetravalent, forming four covalent bonds with other atoms, whereas hydrogen atoms bond with only one other atom. In saturated fatty acids, each carbon atom (besides the last) is connected to its two neighbour carbon atoms and to two hydrogen atoms. In unsaturated fatty acids, the carbon atoms that are missing a hydrogen atom are joined by double bonds rather than single bonds so that each carbon atom participates in four bonds.\n\nHydrogenation of an unsaturated fatty acid refers to the addition of hydrogen atoms to the acid, causing double bonds to become single ones, as carbon atoms acquire new hydrogen partners (to maintain four bonds per carbon atom). Full hydrogenation results in a molecule containing the maximum amount of hydrogen (in other words, the conversion of an unsaturated fatty acid into a saturated one). Partial hydrogenation results in the addition of hydrogen atoms at some of the empty positions, with a corresponding reduction in the number of double bonds. Typical commercial hydrogenation is partial to obtain a malleable mixture of fats that is solid at room temperature, but melts during baking, or consumption.\n\nIn most naturally occurring unsaturated fatty acids, the hydrogen atoms are on the same side of the double bonds of the carbon chain (\"cis\" configuration – from the Latin, meaning \"on the same side\"). However, partial hydrogenation reconfigures most of the double bonds that do not become chemically saturated, twisting them so that the hydrogen atoms end up on different sides of the chain. This type of configuration is called \"trans\", from the Latin, meaning \"across\". The trans configuration is the lower energy form, and is favored when catalytically equilibrated as a side reaction in hydrogenation.\n\nThe same molecule, containing the same number of atoms, with a double bond in the same location, can be either a \"trans\" or a \"cis\" fatty acid depending on the configuration of the double bond. For example, oleic acid and elaidic acid are both unsaturated fatty acids with the chemical formula CHCHO. They both have a double bond located midway along the carbon chain. It is the configuration of this bond that sets them apart. The configuration has implications for the physical-chemical properties of the molecule. The \"trans\" configuration is straighter, while the \"cis\" configuration is noticeably kinked as can be seen from the three-dimensional representation shown above.\n\nThe \"trans\" fatty acid elaidic acid has different chemical and physical properties, owing to the slightly different bond configuration. It has a much higher melting point, 45 °C, than oleic acid, 13.4 °C, due to the ability of the trans molecules to pack more tightly, forming a solid that is more difficult to break apart. This notably means that it is a solid at human body temperatures.\n\nIn food production, the goal is not to simply change the configuration of double bonds while maintaining the same ratios of hydrogen to carbon. Instead, the goal is to decrease the number of double bonds and increase the amount of hydrogen in the fatty acid. This changes the consistency of the fatty acid and makes it less prone to rancidity (in which free radicals attack double bonds). Production of trans fatty acids is thus an undesirable side effect of partial hydrogenation.\n\nCatalytic partial hydrogenation necessarily produces \"trans\"-fats, because of the reaction mechanism. In the first reaction step, one hydrogen is added, with the other, coordinatively unsaturated, carbon being attached to the catalyst. The second step is the addition of hydrogen to the remaining carbon, producing a saturated fatty acid. The first step is reversible, such that the hydrogen is readsorbed on the catalyst and the double bond is re-formed. The intermediate with only one hydrogen added contains no double bond and can freely rotate. Thus, the double bond can re-form as either \"cis\" or \"trans\", of which \"trans\" is favored, regardless the starting material. Complete hydrogenation also hydrogenates any produced \"trans\" fats to give saturated fats.\n\nResearchers at the United States Department of Agriculture have investigated whether hydrogenation can be achieved without the side effect of trans fat production. They varied the pressure under which the chemical reaction was conducted – applying 1400 kPa (200 psi) of pressure to soybean oil in a 2-liter vessel while heating it to between 140 °C and 170 °C. The standard 140 kPa (20 psi) process of hydrogenation produces a product of about 40% trans fatty acid by weight, compared to about 17% using the high-pressure method. Blended with unhydrogenated liquid soybean oil, the high-pressure-processed oil produced margarine containing 5 to 6% trans fat. Based on current U.S. labeling requirements (see below), the manufacturer could claim the product was free of trans fat. The level of trans fat may also be altered by modification of the temperature and the length of time during hydrogenation.\n\nTrans fat levels may be measured. Measurement techniques include chromatography (by silver ion chromatography on thin layer chromatography plates, or small high-performance liquid chromatography columns of silica gel with bonded phenylsulfonic acid groups whose hydrogen atoms have been exchanged for silver ions). The role of silver lies in its ability to form complexes with unsaturated compounds. Gas chromatography and mid-infrared spectroscopy are other methods in use.\n\nA type of trans fat occurs naturally in the milk and body fat of ruminants (such as cattle and sheep) at a level of 2–5% of total fat. Natural \"trans\" fats, which include conjugated linoleic acid (CLA) and vaccenic acid, originate in the rumen of these animals. CLA has two double bonds, one in the \"cis\" configuration and one in \"trans\", which makes it simultaneously a \"cis\"- and a \"trans\"-fatty acid.\n\nAnimal-based fats were once the only \"trans\" fats consumed, but by far the largest amount of \"trans\" fat consumed today is created by the processed food industry as a side effect of partially hydrogenating unsaturated plant fats (generally vegetable oils). These partially hydrogenated fats have displaced natural solid fats and liquid oils in many areas, the most notable ones being in the fast food, snack food, fried food, and baked goods industries.\n\nPartially hydrogenated oils have been used in food for many reasons. Hydrogenation increases product shelf life and decreases refrigeration requirements. Many baked foods require semi-solid fats to suspend solids at room temperature; partially hydrogenated oils have the right consistency to replace animal fats such as butter and lard at lower cost. They are also an inexpensive alternative to other semi-solid oils such as palm oil.\n\nUp to 45% of the total fat in those foods containing artificial \"trans\" fats formed by partially hydrogenating plant fats may be \"trans\" fat. Baking shortenings, unless reformulated, contain around 30% \"trans\" fats compared to their total fats. High-fat dairy products such as butter contain about 4%. Margarines not reformulated to reduce \"trans\" fats may contain up to 15% \"trans\" fat by weight, but some reformulated ones are less than 1% trans fat.\n\nIt has been established that \"trans\" fats in human breast milk fluctuate with maternal consumption of trans fat, and that the amount of trans fats in the bloodstream of breastfed infants fluctuates with the amounts found in their milk. In 1999, reported percentages of trans fats (compared to total fats) in human milk ranged from 1% in Spain, 2% in France, 4% in Germany, and 7% in Canada and the United States.\n\nTrans fats are used in shortenings for deep-frying in restaurants, as they can be used for longer than most conventional oils before becoming rancid. In the early 21st century, non-hydrogenated vegetable oils that have lifespans exceeding that of the frying shortenings became available. As fast-food chains routinely use different fats in different locations, trans fat levels in fast food can have large variations. For example, an analysis of samples of McDonald's French fries collected in 2004 and 2005 found that fries served in New York City contained twice as much trans fat as in Hungary, and 28 times as much as in Denmark, where trans fats are restricted. At KFC, the pattern was reversed, with Hungary's product containing twice the trans fat of the New York product. Even within the United States there was variation, with fries in New York containing 30% more trans fat than those from Atlanta.\n\nThe National Academy of Sciences (NAS) advises the United States and Canadian governments on nutritional science for use in public policy and product labeling programs. Their 2002 \"Dietary Reference Intakes for Energy, Carbohydrate, Fiber, Fat, Fatty Acids, Cholesterol, Protein, and Amino Acids\" contains their findings and recommendations regarding consumption of trans fat (summary).\n\nTheir recommendations are based on two key facts. First, \"trans fatty acids are not essential and provide no known benefit to human health\", whether of animal or plant origin. Second, while both saturated and trans fats increase levels of LDL, trans fats also lower levels of HDL; thus increasing the risk of coronary artery disease. The NAS is concerned \"that dietary trans fatty acids are more deleterious with respect to coronary artery disease than saturated fatty acids\". This analysis is supported by a 2006 New England Journal of Medicine (NEJM) scientific review that states \"from a nutritional standpoint, the consumption of trans fatty acids results in considerable potential harm but no apparent benefit.\"\n\nBecause of these facts and concerns, the NAS has concluded there is no safe level of trans fat consumption. There is no adequate level, recommended daily amount or tolerable upper limit for trans fats. This is because any incremental increase in trans fat intake increases the risk of coronary artery disease.\n\nDespite this concern, the NAS dietary recommendations have not included eliminating trans fat from the diet. This is because trans fat is naturally present in many animal foods in trace quantities, and thus its removal from ordinary diets might introduce undesirable side effects and nutritional imbalances if proper nutritional planning is not undertaken. The NAS has, thus, \"recommended that trans fatty acid consumption be as low as possible while consuming a nutritionally adequate diet\". Like the NAS, the World Health Organization has tried to balance public health goals with a practical level of trans fat consumption, recommending in 2003 that trans fats be limited to less than 1% of overall energy intake.\n\nThe US National Dairy Council has asserted that the trans fats present in animal foods are of a different type than those in partially hydrogenated oils, and do not appear to exhibit the same negative effects. While a recent scientific review agrees with the conclusion (stating that \"the sum of the current evidence suggests that the Public health implications of consuming trans fats from ruminant products are relatively limited\"), it cautions that this may be due to the low consumption of trans fats from animal sources compared to artificial ones.\n\nMore recent inquiry (independent of the dairy industry) has found in a 2008 Dutch meta-analysis that all trans fats, regardless of natural or artificial origin equally raise LDL and lower HDL levels. Other studies though have shown different results when it comes to animal based trans fats like conjugated linoleic acid (CLA). Although CLA is known for its anticancer properties, researchers have also found that the cis-9, trans-11 form of CLA can reduce the risk for cardiovascular disease and help fight inflammation.\n\nPartially hydrogenated vegetable oils have been an increasingly significant part of the human diet for about 100 years (in particular, since the later half of the 20th century and where more processed foods are consumed), and some deleterious effects of trans fat consumption are scientifically accepted, forming the basis of the health guidelines discussed above.\n\nThe exact biochemical process by which trans fats produce specific health problems are a topic of continuing research. One theory is that the human lipase enzyme works only on the cis configuration and cannot metabolize a trans fat although this theory has been overturned by the recognition that trans fat is metabolized but competitively inhibits the metabolism of other fatty acids. Intake of dietary trans fat perturbs the body's ability to metabolize essential fatty acids (EFAs, including Omega-3) leading to changes in the phospholipid fatty acid composition in the aorta, the main artery of the heart, thereby raising risk of coronary artery disease.\n\nWhile the mechanisms through which trans fatty acids contribute to coronary artery disease are fairly well understood, the mechanism for their effects on diabetes is still under investigation. They may impair the metabolism of long-chain polyunsaturated fatty acids (LCPUFAs), but maternal pregnancy trans fatty acid intake has been inversely associated with LCPUFAs levels in infants at birth thought to underlie the positive association between breastfeeding and intelligence.\n\nHigh intake of trans fatty acids can lead to many health problems throughout one's life. They are abundant in fast food restaurants. They are consumed in greater quantities by people who lack access to a diet consisting of fewer hydrogenated fats, or who often consume fast food. A diet high in trans fats can contribute to obesity, high blood pressure, and higher risk for heart disease. Trans fat has also been implicated in the development of Type 2 diabetes.\n\nThe primary health risk identified for trans fat consumption is an elevated risk of coronary artery disease (CHD). A 1994 study estimated that over 30,000 cardiac deaths per year in the United States are attributable to the consumption of trans fats. By 2006 upper estimates of 100,000 deaths were suggested. A comprehensive review of studies of trans fats published in 2006 in the New England Journal of Medicine reports a strong and reliable connection between trans fat consumption and CHD, concluding that \"On a per-calorie basis, trans fats appear to increase the risk of CHD more than any other macronutrient, conferring a substantially increased risk at low levels of consumption (1 to 3% of total energy intake)\".\n\nThe major evidence for the effect of trans fat on CHD comes from the Nurses' Health Study – a cohort study that has been following 120,000 female nurses since its inception in 1976. In this study, Hu and colleagues analyzed data from 900 coronary events from the study's population during 14 years of followup. He determined that a nurse's CHD risk roughly doubled (relative risk of 1.93, CI: 1.43 to 2.61) for each 2% increase in trans fat calories consumed (instead of carbohydrate calories). By contrast, for each 5% increase in saturated fat calories (instead of carbohydrate calories) there was a 17% increase in risk (relative risk of 1.17, CI: 0.97 to 1.41). \"The replacement of saturated fat or trans unsaturated fat by cis (unhydrogenated) unsaturated fats was associated with larger reductions in risk than an isocaloric replacement by carbohydrates.\" Hu also reports on the benefits of reducing trans fat consumption. Replacing 2% of food energy from trans fat with non-trans unsaturated fats more than halves the risk of CHD (53%). By comparison, replacing a larger 5% of food energy from saturated fat with non-trans unsaturated fats reduces the risk of CHD by 43%.\n\nAnother study considered deaths due to CHD, with consumption of trans fats being linked to an increase in mortality, and consumption of polyunsaturated fats being linked to a decrease in mortality.\n\nThere are two accepted tests that measure an individual's risk for coronary artery disease, both blood tests. The first considers ratios of two types of cholesterol, the other the amount of a cell-signalling cytokine called C-reactive protein. The ratio test is more accepted, while the cytokine test may be more powerful but is still being studied. The effect of trans fat consumption has been documented on each as follows:\n\nThere are suggestions that the negative consequences of trans fat consumption go beyond the cardiovascular risk. In general, there is much less scientific consensus asserting that eating trans fat specifically increases the risk of other chronic health problems:\n\nThe international trade in food is standardized in the Codex Alimentarius. Hydrogenated oils and fats come under the scope of Codex Stan 19. Non-dairy fat spreads are covered by Codex Stan 256-2007. In the Codex Alimentarius, trans fat to be labelled as such is defined as the geometrical isomers of monounsaturated and polyunsaturated fatty acids having non-conjugated [interrupted by at least one methylene group (−CH−)] carbon-carbon double bonds in the trans configuration. This definition excludes specifically the trans fats (vaccenic acid and conjugated linoleic acid) that are present especially in human milk, dairy products, and beef.\n\nin 2018 the World Health Organization launched a plan to eliminate trans fat from the global food supply. They estimate that trans fat leads to more than 500,000 deaths from cardiovascular disease yearly.\n\nSince August 2006, food products should be labelled with the amount of trans fat in them. Since 2010, vegetable oils and fats sold to consumers directly must contain only 2% of trans fat over total fat, and other food must contain less than 5% of their total fat. Starting on 10 December 2014, Argentina has on effect a total ban on food with trans fat, a move that could save the government more than US$100 million a year on healthcare.\n\nThe Australian federal government has indicated that it wants to pursue actively a policy to reduce trans fats from fast foods. The former federal assistant health minister, Christopher Pyne, asked fast food outlets to reduce their trans fat use. A draft plan was proposed, with a September 2007 timetable, to reduce reliance on trans fats and saturated fats.\n\n, Australia's food labeling laws do not require trans fats to be shown separately from the total fat content. However, margarine in Australia has been mostly free of trans fat since 1996.\n\nTrans fat content limited to 4% of total fat, 2% on products that contain more than 20% fat.\n\nThe Conseil Supérieur de la Santé published in 2012 a science-policy advisory report on industrially produced trans fatty acids that focuses on the general population. Its recommendation to the legislature was to prohibit more than 2 g of trans fatty acids per 100 g of fat in food products.\n\nResolution 360 of 23 December 2003 by the Brazilian ministry of health required for the first time in the country that the amount of trans fat to be specified in labels of food products. On 31 July 2006, such labelling of trans fat contents became mandatory. In 2007 the ministry established a target to reduce the total amount of trans fat in any industrialized food sold in Brazil to a maximum of 2% by the end of 2010.\n\nIn November 2004, an opposition day motion seeking a ban similar to Denmark's was introduced by Jack Layton of the New Democratic Party, and passed through the House of Commons by an overwhelming 193–73 vote. Like all Commons motions, it served as an expression of the views of the House but was not binding on the government and has no force under the law.\n\nSince December 2005, Health Canada has required that food labels list the amount of trans fat in the nutrition facts section for most foods. Products with less than 0.2 grams of trans fat per serving may be labeled as free of trans fats. These labelling allowances are not widely known, but as an awareness of them develops, controversy over truthful labelling is growing. In Canada, trans fat quantities on labels include naturally occurring trans fats from animal sources.\n\nIn June 2006, a task force co-chaired by Health Canada and the Heart and Stroke Foundation of Canada recommended a limit of 5% trans fat (of total fat) in all products sold to consumers in Canada (2% for tub margarines and spreads). The amount was selected such that \"most of the industrially produced trans fats would be removed from the Canadian diet, and about half of the remaining trans fat intake would be of naturally occurring trans fats\". This recommendation has been endorsed by the Canadian Restaurant and Foodservices Association and Food & Consumer Products of Canada has congratulated the task force on the report, although it did not recommend delaying implementation to 2010 as they had formerly advocated.\n\nTen months after submitting their report the Heart and Stroke Foundation of Canada and Toronto Public Health issued a plea to the government of Canada: \"to act immediately on the task force's recommendations and to eliminate harmful trans fat from Canada's food supply.\"\n\nOn 20 June 2007, the federal government announced its intention to regulate trans fats to the June 2006 standard unless the food industry voluntarily complied with these limits within two years.\n\nOn 1 January 2008, Calgary became the first city in Canada to reduce trans fats from restaurants and fast food chains. Trans fats present in cooking oils may not exceed 2% of the total fat content. However, the replacement of local health regions with the Alberta Health Services Board in 2009 has temporarily eliminated all enforcement of the law.\n\nEffective 30 September 2009, British Columbia became the first province in Canada to mandate the June 2006 recommendation in provincially regulated food services establishments.\n\nOn 15 September 2017, Health Canada announced that trans fat will be completely banned effective on 15 September 2018.\n\nA cross-sectional study was conducted in Regina, Saskatchewan in February 2009 at 3 different grocery stores located in 3 different regions that had the same median income before taxes of around $30,000. Of the 211 respondents to the study, most were women who purchased most of the food for their household. When asked how they decide what food to buy, the most important factors were price, nutritional value, and need. When looking at the nutritional facts, however, they indicated that they looked at the ingredients, and neglected to pay attention to the amount of trans fat. This means that trans fat is not on their minds unless they are specifically told of it. When asked if they ever heard about trans fat, 98% said, \"Yes.\" However, only 27% said that it was unhealthy. Also, 79% said that they only knew a little about trans fats, and could have been more educated. Respondents aged 41–60 were more likely to view trans fat as a major health concern, compared to ages 18–40. When asked if they would stop buying their favorite snacks if they knew it contained trans fat, most said they would continue purchasing it, especially the younger respondents. Also, of the respondents that called trans fat a major concern, 56% of them still wouldn't change their diet to non-trans fat snacks. This is because taste and food gratification take precedence over perceived risk to health. \"The consumption of trans fats and the associated increased risk of CHD is a public health concern regardless of age and socioeconomic status\".\n\nDenmark became the first country to introduce laws strictly regulating the sale of many foods containing trans fats in March 2003, a move that effectively bans partially hydrogenated oils. The limit is 2% of fats and oils destined for human consumption. This restriction is on the \"ingredients\" rather than the final products. This regulatory approach has made Denmark the only country in which it is possible to eat \"far less\" than 1 g of industrially produced trans fats daily, even with a diet including prepared foods. It is hypothesized that the Danish government's efforts to decrease trans fat intake from 6 g to 1 g per day over 20 years is related to a 50% decrease in deaths from ischemic heart disease.\n\nIn 2004, the European Food Safety Authority produced a scientific opinion on trans fatty acids, surmising that \"higher intakes of TFA may increase risk for coronary heart disease\".\n\nLaw in Greece limits content of trans fats sold in school canteens to 0.1% (Ministerial Decision Υ1γ/ΓΠ/οικ 81025/ΦΕΚ 2135/τ.Β’/29-08-2013 as modified by Ministerial Decision Υ1γ/ Γ.Π/οικ 96605/ΦΕΚ 2800 τ.Β/4-11-201).\n\nTotal trans fat content was limited in 2010 to 2% of total fat content.\n\nSince 2014, It is obligatory to mark food products with more than 2% (by weight) fat. The nutritional facts must contain the amount of trans fats.\n\nThe parliament gave the government a mandate in 2011 to submit without delay a law prohibiting the use of industrially produced trans fats in foods, as of 2017 the law has not yet been implemented.\n\nSwitzerland followed Denmark's trans fats ban, and implemented its own starting in April 2008.\n\nIn October 2005, the Food Standards Agency (FSA) asked for better labelling in the UK. In the edition of 29 July 2006 of the \"British Medical Journal\", an editorial also called for better labelling. In January 2007, the British Retail Consortium announced that major UK retailers, including Asda, Boots, Co-op, Iceland, Marks and Spencer, Sainsbury's, Tesco and Waitrose intended to cease adding trans fatty acids to their own products by the end of 2007.\n\nSainsbury's became the first UK major retailer to ban all trans fat from all their own store brand foods.\n\nOn 13 December 2007, the Food Standards Agency issued news releases stating that voluntary measures to reduce trans fats in food had already resulted in safe levels of consumer intake.\n\nOn 15 April 2010, a \"British Medical Journal\" editorial called for trans fats to be \"virtually eliminated in the United Kingdom by next year\".\n\nThe June 2010 National Institute for Health and Clinical Excellence (NICE) report \"Prevention of cardiovascular disease\" declared that 40,000 cardiovascular disease deaths in 2006 were \"mostly preventable\". To achieve this, NICE offered 24 recommendations including product labelling, public education, protecting under–16s from marketing of unhealthy foods, promoting exercise and physically active travel, and even reforming the Common Agricultural Policy to reduce production of unhealthy foods. Fast-food outlets were mentioned as a risk factor, with (in 2007) 170 g of McDonald's fries and 160 g nuggets containing 6 to 8 g of trans fats, conferring a substantially increased risk of coronary artery disease death. NICE made three specific recommendation for diet: (1) reduction of dietary salt to 3 g per day by 2025; (2) halving consumption of saturated fats; and (3) eliminating the use of industrially produced trans fatty acids in food. However, the recommendations were greeted unhappily by the food industry, which stated that it was already voluntarily dropping the trans fat levels to below the WHO recommendations of a maximum of 2%.\n\nRejecting an outright ban, the Health Secretary Andrew Lansley launched on 15 March 2012 a voluntary pledge to remove artificial trans fats by the end of the year. Asda, Pizza Hut, Burger King, Tesco, Unilever and United Biscuits are some of 73 businesses who have agreed to do so. Lansley and his special Adviser Bill Morgan formerly worked for firms with interests in the food industry and some journalists have alleged that this results in a conflict of interest. Many health professionals are not happy with the voluntary nature of the deal. Simon Capewell, Professor of Clinical Epidemiology at the University of Liverpool, felt that justifying intake on the basis of average figures was unsuitable since some members of the community could considerably exceed this.\n\nBefore 2006, consumers in the United States could not directly determine the presence, or quantity, of trans fats in food products. This information could only be inferred from the ingredient list, notably from the partially hydrogenated ingredients. In 2010, according to the FDA, the average American consumed 5.8 grams of trans fat per day (2.6% of energy intake). Monoglycerides and diglycerides are not considered fats by the FDA, despite their nearly equal calorie per weight contribution during ingestion.\n\nOn 11 July 2003, the Food and Drug Administration (FDA) issued a regulation requiring manufacturers to list trans fat on the Nutrition Facts panel of foods and some dietary supplements. The new labeling rule became mandatory across the board, even for companies that petitioned for extensions, on 1 January 2008. However, unlike in many other countries, trans fat levels of less than 0.5 grams per serving can be listed as 0 grams trans fat on the food label. According to a study published in the Journal of Public Policy & Marketing, without an interpretive footnote or further information on recommended daily value, many consumers do not know how to interpret the meaning of trans fat content on the Nutrition Facts panel. Without specific prior knowledge about trans fat and its negative health effects, consumers, including those at risk for heart disease, may misinterpret nutrient information provided on the panel. The FDA did not approve nutrient content claims such as \"trans fat free\" or \"low trans fat\", as they could not determine a \"recommended daily value\". Nevertheless, the agency is planning a consumer study to evaluate the consumer understanding of such claims and perhaps consider a regulation allowing their use on packaged foods. However, there is no requirement to list trans fats on institutional food packaging; thus bulk purchasers such as schools, hospitals, jails and cafeterias are unable to evaluate the trans fat content of commercial food items. \nCritics of the plan, including FDA advisor Dr. Carlos Camargo, have expressed concern that the 0.5 gram per serving threshold is too high to refer to a food as free of trans fat. This is because a person eating many servings of a product, or eating multiple products over the course of the day may still consume a significant amount of trans fat. Despite this, the FDA estimates that by 2009, trans fat labeling will have prevented from 600 to 1,200 cases of coronary artery disease, and 250 to 500 deaths, yearly. This benefit is expected to result from consumers choosing alternative foods lower in trans fats, and manufacturers reducing the amount of trans fats in their products.\n\nThe American Medical Association supports any state and federal efforts to ban the use of artificial trans fats in U.S. restaurants and bakeries.\n\nThe American Public Health Association adopted a new policy statement regarding trans fats in 2007. These new guidelines, entitled \"Restricting Trans Fatty Acids in the Food Supply\", recommend that the government require nutrition facts labeling of trans fats on all commercial food products. They also urge federal, state, and local governments to ban and monitor use of trans fats in restaurants. Furthermore, the APHA recommends barring the sales and availability of foods containing significant amounts of trans fat in public facilities including universities, prisons, and day care facilities etc.\n\nOn 7 November 2013, the FDA issued a preliminary determination that trans fats are not \"generally recognized as safe\", which was widely seen as a precursor to reclassifying trans fats as a \"food additive,\" meaning they could not be used in foods without specific regulatory authorization. This would have the effect of virtually eliminating trans fats from the US food supply. The ruling was formally enacted on 16 June 2015, requiring that within three years, all food prepared in the United States must not include trans fats, unless approved by the FDA.\n\nThe FDA agreed in May 2018 to give companies one more year to find another ingredient for enhancing product flavors or grease industrial baking pans. Also, while new products can no longer be made with trans fats, they will give foods already on the shelves some time to cycle out of the market.\n\nEven before the federal ban, the state of California and several U.S. cities took action to reduce consumption of trans fats.\n\nIn 2005, Tiburon, California, became the first American city where all restaurants voluntarily cook with trans fat-free oils. In 2007, Montgomery County, Maryland, approved a ban on partially hydrogenated oils, becoming the first county in the nation to restrict trans fats.\n\nNew York City embarked on a campaign in 2005 to reduce consumption of trans fats, noting that heart disease is the primary cause of resident deaths. This has included a Public education campaign and a request to restaurant owners to eliminate trans fat from their offerings voluntarily. Finding that the voluntary program was not successful, New York City's Board of Health in 2006 solicited public comments on a proposal to ban artificial trans fats in restaurants. The board voted to ban trans fat in restaurant food on 5 December 2006. New York was the first large US city to strictly limit trans fats in restaurants. Restaurants were barred from using most frying and spreading fats containing artificial trans fats above 0.5 g per serving on 1 July 2007, and were supposed to have met the same target in all of their foods by 1 July 2008.\n\nThe Philadelphia City Council unanimously voted to enacted a ban in February 2007, and the measure was signed into law by Mayor John F. Street. The ordinance does not apply to prepackaged foods sold in the city, but did require restaurants in the city to stop frying food in trans fats by 1 September 2007. The ordinance also contained a provision going into effect one year later that barred trans fat from being used as an ingredient in commercial kitchens. The law On 10 October 2007, the Philadelphia City Council approved the use of trans fats by small bakeries throughout the city.\n\nNassau County, a suburban county on Long Island, New York, banned trans fats in restaurants effective 1 April 2008. Bakeries were granted an extension until 1 April 2011.\n\nAlbany County of New York passed a ban on trans fats. The ban was adopted after a unanimous vote by the county legislature on 14 May 2007. The decision was made after New York City's decision, but no plan has been put into place. Legislators received a letter from Rick J. Sampson, president and CEO of the New York State Restaurant Association, calling on them to \"delay any action on this issue until the full impact of the New York City ban is known.\"\n\nSan Francisco officially asked its restaurants to stop using trans fat in January 2008. The voluntary program will grant a city decal to restaurants that comply and apply for the decal. Legislators say the next step will be a mandatory ban.\n\nChicago also passed partial ban on oils and posting requirements for fast food restaurants.\n\nTrans fat bans were also introduced in the state legislatures of Massachusetts, Maryland, and Vermont.\n\nIn March 2008, the Boston Public Health Commission's Board of Health passed a regulation food service establishments from selling foods containing artificial trans fats at more than 0.5 grams per serving, which is similar to the New York City regulation; there are some exceptions for clearly labeled packaged foods and charitable bake sales.\n\nKing County, Washington passed a ban on artificial trans fats effective 1 February 2009.\n\nIn July 2008, California became the first state to ban trans fats in restaurants effective 1 January 2010; Governor Arnold Schwarzenegger signed the bill into law. California restaurants are prohibited from using oil, shortening, and margarine containing artificial trans fats in spreads or for frying, with the exception of deep frying doughnuts. As of 1 January 2011, doughnuts and other baked goods have been prohibited from containing artificial trans fats. Packaged foods are not covered by the ban and can legally contain trans fats.\n\nIn 2007, the American Heart Association launched its \"Face the Fats\" campaign to help educate the public about the negative effects of trans fats.\n\nIn 2009, at the age of 94, University of Illinois professor Fred Kummerow, a trans fat researcher who had campaigned for decades for a federal ban on the substance, filed a petition with the U.S. Food and Drug Administration (FDA) seeking elimination of artificial trans fats from the U.S. food supply. The FDA did not act on his petition for four years, and in 2013 Kummerow filed a lawsuit against the FDA and the U.S. Department of Health and Human Services, seeking to compel the FDA to respond to his petition and \"to ban partially hydrogenated oils unless a complete administrative review finds new evidence for their safety.\" Kummerow's petition stated that \"Artificial trans fat is a poisonous and deleterious substance, and the FDA has acknowledged the danger.\"\n\nThree months after the suit was filed, on 16 June 2015, the FDA moved to eliminate artificial trans fats from the U.S. food supply, giving manufacturers a deadline of three years. The FDA specifically ruled that trans fat was not generally recognized as safe and \"could no longer be added to food after June 18, 2018, unless a manufacturer could present convincing scientific evidence that a particular use was safe.\" Kummerow stated: \"Science won out.\"\n\nThe ban is believed to prevent about 90,000 premature deaths annually. The FDA estimates the ban will cost the food industry $6.2 billion over 20 years as the industry reformulates products and substitutes new ingredients for trans fat. The benefits are estimated at $140 billion over 20 years mainly from lower health care spending. Food companies can petition the FDA for approval of specific uses of partially hydrogenated oils if the companies submit data proving the oils' use is safe.\n\nPalm oil, a natural oil extracted from the fruit of oil palm trees that is semi-solid at room temperature (15–25 degrees Celsius), can potentially serve as a substitute for partially hydrogenated fats in baking and processed food applications, although there is disagreement about whether replacing partially hydrogenated fats with palm oil confers any health benefits. A 2006 study supported by the National Institutes of Health and the USDA Agricultural Research Service concluded that palm oil is not a safe substitute for partially hydrogenated fats (trans fats) in the food industry, because palm oil results in adverse changes in the blood concentrations of LDL and apolipoprotein B just as trans fat does.\n\nIn May 2003, BanTransFats.com Inc., a U.S. non-profit corporation, filed a lawsuit against the food manufacturer Kraft Foods in an attempt to force Kraft to remove trans fats from the Oreo cookie. The lawsuit was withdrawn when Kraft agreed to work on ways to find a substitute for the trans fat in the Oreo.\n\nThe J.M. Smucker Company, American manufacturer of Crisco (the original partially hydrogenated vegetable shortening), in 2004 released a new formulation made from solid saturated palm oil cut with soybean oil and sunflower oil. This blend yielded an equivalent shortening much like the prior partially hydrogenated Crisco, and was labelled zero grams of trans fat per 1 tablespoon serving (as compared with 1.5 grams per tablespoon of original Crisco). As of 24 January 2007, Smucker claims that all Crisco shortening products in the US have been reformulated to contain less than one gram of trans fat per serving while keeping saturated fat content less than butter. The separately marketed trans fat free version introduced in 2004 was discontinued.\n\nOn 22 May 2004, Unilever, the corporate descendant of Joseph Crosfield & Sons (the original producer of Wilhelm Normann's hydrogenation hardened oils) announced that they have eliminated trans fats from all their margarine products in Canada, including their flagship Becel brand.\n\nAgribusiness giant Bunge Limited, through their Bunge Oils division, are now producing and marketing an \"NT\" product line of non-hydrogenated oils, margarines and shortenings, made from corn, canola, and soy oils.\n\nSince 2003, Loders Croklaan, a wholly owned subsidiary of Malaysia's IOI Group has been providing trans fat free bakery and confectionery fats, made from palm oil, for giant food companies in the United States to make margarine.\n\nSome major food chains have chosen to remove or reduce trans fats in their products. In some cases these changes have been voluntary. In other cases, however, food vendors have been targeted by legal action that has generated a lot of media attention.\n\nMajor fast food chain menus and product lines with significant artificial trans fat include Popeyes \n\nThe following major fast food chain menus and product lines are artificial trans fat free (that is, < 0.5 g/serving): Taco Bell, Chick-fil-A, Girl Scout Cookies, KFC (eliminated from all but Mac and cheese, biscuits and chicken potpie in '07, the rest in '09), McDonald's, Burger King, and Wendy's have greatly reduced partially hydrogenated oils (containing artificial trans fats) in their food; most of the remaining trans fat is naturally occurring, in the form of about a gram per 1/4 lb. burger patty, and smaller amounts in fatty dairy products such as cheese, butter, and cream. Naturally occurring trans fat causes the Baconator, for example, to have 2.5 grams. A large chain's large fries typically had about 6 grams until around 2007.\n\nThese reformulations can be partly attributed to '06 Center for Science in the Public Interest class action complaints, and to New York's trans fat ban, with companies such as McDonald's's stating they would not be selling a unique product just for New York customers but would implement a nationwide or worldwide change.\n\nAlthough IHOP restaurants pledged in a 2007 press release to eliminate transfat from their food, the nutrition information on the company website for the Summer/Fall 2015 Core Menu shows that they still have a considerable amount of trans fat in their food, including 4.5 grams in their \"mega monster cheeseburger\".\n\nThe Girl Scouts of the USA announced in November 2006 that all of their cookies contain less than 0.5 g trans fats per serving, thus meeting or exceeding the FDA guidelines for the \"zero trans fat\" designation. High levels of trans fats remain common in packaged baked goods.\n\nHealth Canada's monitoring program, which tracks the changing amounts of TFA and SFA in fast and prepared foods shows considerable progress in TFA reduction by some industrial users while others, as of 2007, lag behind. In many cases, SFAs have been substituted for the TFAs.\n\n\n\n"}
{"id": "38797174", "url": "https://en.wikipedia.org/wiki?curid=38797174", "title": "Tropical vegetation", "text": "Tropical vegetation\n\nTropical vegetation is any vegetation in tropical latitudes. Plant life that occurs in climates that are warm year-round is in general more biologically diverse that in other latitudes. Some tropical areas may receive abundant rain the whole year round, but others have long dry seasons which last several months and may vary in length and intensity with geographic location. These seasonal droughts have great impact on the vegetation, such as in the Madagascar spiny forests.\n\nPlant species native to the tropics found in tropical ecosystems are known as tropical plants. Some examples of tropical ecosystem are the Guinean Forests of West Africa, the Madagascar dry deciduous forests and the broadleaf forests of the Thai highlands and the El Yunque National Forest in Puerto Rico.\n\nThe term 'Tropical vegetation' is frequently used in the sense of lush and luxuriant, but not all the vegetation of the areas of the Earth under a tropical climate could be adequately defined as such. Despite their lush appearance, often the soils of tropical forests are low in nutrient content making them quite vulnerable to slash-and-burn deforestation techniques, which are sometimes an element of shifting cultivation agricultural systems.\nTropical vegetation may include the following habitat types:\n\nTropical rainforest ecosystems include significant areas of biodiversity, often coupled with high species endemism. Rainforests are home to half of all the living animal and plant species on the planet and roughly two-thirds of all flowering plants can be found in rainforests. The most representative are the Borneo rainforest, one of the oldest rainforests in the world, the Brazilian and Venezuelan Amazon Rainforest, as well as the eastern Costa Paulon rainforests.\n\nSeasonal tropical forests generally receive high total rainfall, averaging more than 1000 mm per year, but with a distinct dry season. They include: the Congolian forests, a broad belt of highland tropical moist broadleaf forest which extends across the basin of the Congo River; Central American tropical forests in Panama and Nicaragua; the seasonal forests that predominate across much the Indian subcontinent, Indochina and in northern Australia: Queensland.\n\nTropical dry broadleaf forests are territories with a forest cover that is not very dense and has often an unkempt, irregular appearance, especially in the dry season. This type of forests often include bamboo and teak as the dominant large tree species, such as in the Phi Pan Nam Range, part of the Central Indochina dry forests. They are affected by often very long and intense seasonal dry periods and, though less biologically diverse than rainforests, tropical dry forests are home to a wide variety of wildlife.\n\nTropical grasslands, savannas, and shrublands are spread over a large area of the tropics with a vegetation made up mainly of low shrubs and grasses, often including sclerophyll species. Some of the most representative are the Western Zambezian grasslands in Zambia and Angola, as well as the Einasleigh upland savanna in Australia. Tree species such as Acacia and baobab may be present in these ecosystems depending on the region.\n\n\n\n"}
{"id": "11325243", "url": "https://en.wikipedia.org/wiki?curid=11325243", "title": "Vandellòs Nuclear Power Plant", "text": "Vandellòs Nuclear Power Plant\n\nThe Vandellòs Nuclear Power Plant is a nuclear power plant in Vandellòs located close to the Coll de Balaguer pass (Baix Camp comarca) in Catalonia, Spain.\n\nUnit one was a 508 MWe carbon dioxide gas cooled reactor modeled on the UNGG reactor at the Saint Laurent Nuclear Power Plant in France. It was shut down on July 31, 1990, following an accident that damaged one of its two turbogenerators on 19 October 1989. Important nuclear safety functions in the plant were impaired by the fire, and the event was later classified as a level 3 event in the International Nuclear Event Scale.\n\nUnit two is a 1087 MWe PWR. The station's owners are: 72% Endesa and 28% Iberdrola.\n\n\n"}
{"id": "25591323", "url": "https://en.wikipedia.org/wiki?curid=25591323", "title": "Wodginite", "text": "Wodginite\n\nWodginite is a manganese, tin, tantalum oxide mineral with formula Mn(Sn,Ta)TaO. It may include significant niobium.\n\nWodginite was first described in 1963 for an occurrence in the Wodgina pegmatite, Wodgina, Pilbara Region, Western Australia.\n\nTypical occurrence is in zoned pegmatites in amphibolite. It is associated with tantalite, albite, quartz, muscovite, tapiolite,\nmicrolite and microcline.\n\nIt occurs in pegmatites in a wide variety of locations. The most studied is the Tanco pegmatite in Manitoba, Canada; also in Red Lake, Ontario. It is reported from the Strickland quarry, Portland, Middlesex County, Connecticut; the Herbb #2 pegmatite, Powhatan County, Virginia; the McAllister mine, Rockford, Coosa County, Alabama; the Peerless mine, Pennington County, South Dakota. Also from Paraíba and Minas Gerais, Brazil; Krasonice, Czech Republic; Orivesi, Finland; Kalba, eastern Kazakhstan; Ankole, Uganda; Miami district, Zimbabwe and \nKaribib and Kohero, Namibia.\n"}
{"id": "3096353", "url": "https://en.wikipedia.org/wiki?curid=3096353", "title": "Zirconium alloy", "text": "Zirconium alloy\n\nZirconium alloys are solid solutions of zirconium or other metals, a common subgroup having the trade mark Zircaloy. Zirconium has very low absorption cross-section of thermal neutrons, high hardness, ductility and corrosion resistance. One of the main uses of zirconium alloys is in nuclear technology, as cladding of fuel rods in nuclear reactors, especially water reactors. A typical composition of nuclear-grade zirconium alloys is more than 95 weight percent zirconium and less than 2% of tin, niobium, iron, chromium, nickel and other metals, which are added to improve mechanical properties and corrosion resistance.\n\nThe water cooling of reactor zirconium alloys elevates requirement for their resistance to oxidation-related nodular corrosion. Furthermore, oxidative reaction of zirconium with water releases hydrogen gas, which partly diffuses into the alloy and forms zirconium hydrides. The hydrides are less dense and are weaker mechanically than the alloy; their formation results in blistering and cracking of the cladding – a phenomenon known as hydrogen embrittlement.\n\nCommercial non-nuclear grade zirconium typically contains 1–5% of hafnium, whose neutron absorption cross-section is 600x that of zirconium. Hafnium must therefore be almost entirely removed (reduced to < 0.02% of the alloy) for reactor applications.\n\nNuclear-grade zirconium alloys contain more than 95% Zr, and therefore most of their properties are similar to those of pure zirconium. The absorption cross section for thermal neutrons is 0.18 barn for zirconium, which is much lower than that for such common metals as iron (2.4 barn) and nickel (4.5 barn). The composition and the main applications of common reactor-grade alloys are summarized below. These alloys contain less than 0.3% of iron and chromium and 0.1–0.14% oxygen.\n\nZIRLO stands for zirconium low oxidation.\n\nAt temperatures below 1100 K, zirconium alloys belong to the hexagonal crystal family (HCP). Its microstructure, revealed by chemical attack, shows needle-like grains typical of a Widmanstätten pattern. Upon annealing below the phase transition temperature (αZr to βZr) the grains are equiaxed with sizes varying from 3 to 5 μm.\n\nZircaloy 1 was developed as a replacement for existing tube bundles in submarine reactors in the 1950s, owing to a combination of strength, low neutron cross section and corrosion resistance. Zircaloy-2 was inadvertently developed, by melting Zircaloy-1 in a crucible previously used for stainless steel. Newer alloys are Ni-free, including Zircaloy-4, ZIRLO and M5.\n\nZirconium alloys readily react with oxygen, forming a nanometer-thin passivation layer. The corrosion resistance of the alloys may degrade significantly when some impurities (e.g. more than 40 ppm of carbon or more than 300 ppm of nitrogen) are present. Corrosion resistance of zirconium alloys is enhanced by intentional development of thicker passivation layer of black lustrous zirconium oxide. Nitride coatings might also be used.\n\nWhereas there is no consensus on whether zirconium and zirconium alloy have the same oxidation rate, Zircaloys 2 and 4 do behave very similarly in this respect. Oxidation occurs at the same rate in air or in water and proceeds in ambient condition or in high vacuum. A sub-micrometer thin layer of zirconium dioxide is rapidly formed in the surface and stops the further diffusion of oxygen to the bulk and the subsequent oxidation. The dependence of oxidation rate R on temperature and pressure can be expressed as\n\nThe oxidation rate R is here expressed in gram/(cm·second); P is the pressure in atmosphere, that is the factor P = 1 at ambient pressure; the activation energy is 1.47 eV; k is the Boltzmann constant (8.617 eV/K) and T is the absolute temperature in kelvins.\n\nThus the oxidation rate R is 10 g per 1 m area per second at 0 °C, 6 g m s at 300 °C, 5.4 mg m s at 700 °C and 300 mg m s at 1000 °C. Whereas there is no clear threshold of oxidation, it becomes noticeable at macroscopic scales at temperatures of several hundred °C.\n\nOne disadvantage of metallic zirconium is that in the case of a loss-of-coolant accident in a nuclear reactor, zirconium cladding rapidly reacts with water steam at high temperature. Oxidation of zirconium by water is accompanied by release of hydrogen gas. This oxidation is accelerated at high temperatures, e.g. inside a reactor core if the fuel assemblies are no longer completely covered by liquid water and insufficiently cooled. Metallic zirconium is then oxidized by the protons of water to form hydrogen gas according to the following redox reaction:\n\nZirconium cladding in the presence of D2O deuterium oxide frequently used as the moderator and coolant in next gen pressurized heavy water reactors that CANDU designed nuclear reactors use would express the same oxidation on exposure to deuterium oxide steam as follows:\n\nThis exothermic reaction, although only occurring at high temperature, is similar to that of alkali metals (such as sodium or potassium) with water. It also closely resembles the anaerobic oxidation of iron by water (reaction used at high temperature by Antoine Lavoisier to produce hydrogen for his experiments).\n\nThis reaction was responsible for a small hydrogen explosion accident first observed inside the reactor building of Three Mile Island Nuclear Generating Station in 1979 that did not damage the containment building. This same reaction occurred in boiling water reactors 1, 2 and 3 of the Fukushima Daiichi Nuclear Power Plant (Japan) after reactor cooling was interrupted by related earthquake and tsunami events during the disaster of March 11, 2011, leading to the Fukushima Daiichi nuclear disaster. Hydrogen gas was vented into the reactor maintenance halls and the resulting explosive mixture of hydrogen with air oxygen detonated. The explosions severely damaged external buildings and at least one containment building. The reaction also occurred during the Chernobyl Accident, when the steam from the reactor began to escape. Many water cooled reactor containment buildings have catalyst-based recombinator units installed to rapidly convert hydrogen and oxygen into water at room temperature before the explosive limit is reached.\n\nAlso, 5–20% of hydrogen diffuses into the zirconium alloy cladding forming zirconium hydrides. The hydrogen production process also mechanically weakens the rods cladding because the hydrides have lower ductility and density than zirconium or its alloys, and thus blisters and cracks form upon hydrogen accumulation. This process is also known as hydrogen embrittlement. It has been reported that the concentration of hydrogen within hydrides is also dependent on the nucleation site of the precipitates .\n\nIn case of Loss-of-Coolant Accident (LOCA) in a damaged nuclear reactor, hydrogen embrittlement accelerates the degradation of the zirconium alloy cladding of the fuel rods exposed to high temperature steam.\n\nZirconium alloys are corrosion resistant and biocompatible, and therefore can be used for body implants. In one particular application, a Zr-2.5Nb alloy is formed into a knee or hip implant and then oxidized to produce a hard ceramic surface for use in bearing against a polyethylene component. This oxidized zirconium alloy material provides the beneficial surface properties of a ceramic (reduced friction and increased abrasion resistance), while retaining the beneficial bulk properties of the underlying metal (manufacturability, fracture toughness, and ductility), providing a good solution for these medical implant applications.\n\nReduction of zirconium demand in Russia due to nuclear demilitarization after the end of the cold war resulted in the exotic production of household zirconium items such as the vodka shot glass shown in the picture.\n\n"}
