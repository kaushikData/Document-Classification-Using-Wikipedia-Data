{"id": "8026589", "url": "https://en.wikipedia.org/wiki?curid=8026589", "title": "2000–01 fires in the Western United States", "text": "2000–01 fires in the Western United States\n\nThe 2000-2001 Western United States wildfires were a series of unusually severe wildfires that caused more than $2 billion (USD) in damage and resulted in the deaths of four firefighters. Overall, 6,966,995 acres burned across the United States and 2.2 million of those acres were in Idaho and Montana alone. A declaration of a state of emergency brought six military battalions and fire fighting teams from as far away as Australia and New Zealand to the Western United States. Federal and state land management organizations recognize the fires as historic \"both in extent and duration.\" The ten year fire season average is 3.1 million acres. The fires in 2000 destroyed more than double that acreage. Nearly $900 million (USD) was spent fighting fires. Long lasting ecological damage, including flooding, top soil runoff, and air quality damage has continues to this day.\n\nThe damage was particularly severe in the Bitterroot National Forest. One of the most stunning photos from these fires are two elk seeking shelter in the East Fork of the Bitterroot River. The photo became known as Elk Bath.\n\n\n"}
{"id": "56395397", "url": "https://en.wikipedia.org/wiki?curid=56395397", "title": "2001 Faraz Qeshm Airlines Yak-40 crash", "text": "2001 Faraz Qeshm Airlines Yak-40 crash\n\nThe 2001 Faraz Qeshm Airlines Yak-40 crash occurred on 17 May 2001 when a short-haul trijet Yakovlev Yak-40 being operated by Faraz Qeshm Airlines crashed while en route to Gorgan Airport from Tehran-Mehrabad Airport in Iran. The aircraft crashed in mountainous terrain while flying in poor weather conditions about twenty kilometers south of Sari killing all thirty people on board. Passengers aboard the aircraft included Rahman Dadman, Iran's Minister of Roads and Transportation, and six members of parliament.\n\nThe Yakovlev Yak-40 operated by Faraz Qeshm Airlines was on lease from Armenian Airlines and the crew, including both pilots, consisted of Armenian nationals.\n\nOn 17 May 2001, a Russian-built Yakovlev Yak-40, registration EP-TQP, operated by Faraz Qeshm Airlines took off from Tehran-Mehrabad Airport at 06:45 and proceeded northeast to Gorgan Airport with a crew of five and twenty-five passengers which included Iran's Transportation Minister Rahman Dadman and six members of parliament. They were part of a delegation to inaugurate the opening of Gorgan Airport, according to Golestan Province Governor Ali Asghar Ahmadi.\n\nWhile flying in deteriorating weather conditions, which included heavy rains, the aircraft was struck by lightning possibly affecting its navigational equipment. About ten minutes before its scheduled arrival, the pilot communicated to air traffic control that they would either have to make an emergency landing or divert to another airport. At around 07:45 the aircraft crashed in a heavily forested section of the Alborz Mountains, thirteen miles southeast of the city of Sari, between Gorgan and Shahroud.\n\nAll of the passengers aboard the aircraft were Iranian nationals. The crew members were Armenian nationals from Armenian Airlines.\n"}
{"id": "22211736", "url": "https://en.wikipedia.org/wiki?curid=22211736", "title": "Aerosol impaction", "text": "Aerosol impaction\n\nAerosol Impaction is the process in which particles are removed from an air stream by forcing the gases to make a sharp bend. Particles above a certain size possess so much momentum that they can not follow the air stream and strike a collection surface which is available for later analysis of mass and composition. Removal of particles from an air-stream by impaction followed by mass and composition analysis has always been a different approach as to filter sampling, yet has been little utilized for routine analysis because of lack of suitable analytical methods.\n\nThe most clear and important advantage of impaction, as opposed to filtration, is that two key aerosol parameters, size and composition, can be simultaneously established.\n\nThere are many advantages of impaction as a sampling method. For two of the most common configurations, an orifice and an infinite slot, theoretical predictions can be made and empirically verified that give the cuts point and shape of the collection efficiency of an impaction stage. The air stream moves over the sample, not through it as in filtration, reducing desiccation and chemical transformations of the collected sample. Almost complete control of the type of surface on which the particles are impacted, as opposed to the limited choice of filter types. By varying the speed of the air stream and the sharpness of the bend, one can separate particles into numerous size classifications while retaining a sample for analysis. It creates nasal abnormalities when dipped into an extent amount.\n\nThere are also several disadvantages to impaction as a sampling method. Only a limited amount of material is available for mass and compositional analysis, as one can not collect more than a few monolayers of particles before particle bounce and mis-sizing are a potential problem. Impactors are complex, with key parameters that must be maintained by either maintenance or instrumentation. Because impactors are not widely used, there is a lack of quality assurance experience. Because impactors are not widely used, there are few people or groups expert in their application. Interpretation of impactor data is often complex. Costs of analysis can be high.\n\n"}
{"id": "22932137", "url": "https://en.wikipedia.org/wiki?curid=22932137", "title": "Annaguly Deryayev", "text": "Annaguly Deryayev\n\nAnnaguly Deryayev (born 1973) is a Turkmen politician and the head of the state oil company Türkmennebit. He is a former minister of oil and gas industry and mineral resources of Turkmenistan. \n\nAnnaguly Deryayev was born in the village of Ekerem of Esenguly District of Balkan Province in 1973. He graduated from the Turkmen Polytechnic Institute in 1994 as mining engineer.\n\nAfter graduation, he worked as drill operator in test wells in Goturdep and also served in the Turkmen army from 1997 to 1998.\n\nIn 1998-2002, he worked as a senior engineer for drilling technologies at the Burnebitgas trust of Türkmennebit and from 2002 head of the Türkmennebit's department. After that, he served as the first deputy chairman of the company.\n\nFrom October 2008 until October 2009 he served as minister of oil and gas industry and mineral resources of Turkmenistan. On 13 October 2009, he was demoted from the ministerial position and was appointed the head of Türkmennebit.\n"}
{"id": "897099", "url": "https://en.wikipedia.org/wiki?curid=897099", "title": "Arica", "text": "Arica\n\nArica ( ; ) is a commune and a port city with a population of 196,590 in the Arica Province of northern Chile's Arica y Parinacota Region. It is Chile's northernmost city, being located only south of the border with Peru. The city is the capital of both the Arica Province and the Arica and Parinacota Region. Arica has a mild, temperate climate with some of the lowest annual rainfall rates anywhere in the world, consequently there are rarely any clouds over Arica. Arica is located at the bend of South America's western coast known as the Arica Bend or Arica Elbow. At the location of the city are two lush valleys that dissect the Atacama Desert converge: Azapa and Lluta. These valleys provide fruit for export.\n\nArica is an important port for a large inland region of South America. The city serves a free port for Bolivia and manages a substantial part of that country's trade. In addition it is the end station of the Bolivian oil pipeline beginning in Oruro. The city's strategic position is enhanced by being next to the Pan-American Highway, being connected to both Tacna in Peru and La Paz in Bolivia by railroad and being served by an international airport.\n\nIts mild weather has made Arica known as the \"city of the eternal spring\" in Chile while its beaches are frequented by Bolivians. The city was an important port already during Spanish colonial rule. Chile seized the city from Peru in 1880 following the War of the Pacific and was recognized as Chilean by Peru in 1929. A substantial part of African Chileans live in or trace their origins to Arica.\n\nArchaeological findings indicate that Arica was inhabited by different native groups dating back 10,000 years.\n\nSpaniards settled the land under captain Lucas Martinez de Begazo in 1541, and in 1570, the area was grandly retitled as \"La Muy Ilustre y Real Ciudad San Marcos de Arica\" (the very illustrious and royal city of San Marcos of Arica). By 1545, Arica was the main export entrepot for Bolivian silver coming down from Potosí, which then possessed the world's largest silver mine. Arica thus held the crucial role as one of the leading ports of the Spanish Empire. The envious riches made Arica the target for pirates, buccaneers, and privateers, among whom Francis Drake, Thomas Cavendish, Richard Hawkins, Joris van Spilbergen, John Watling, Simon de Cordes, Leandro de Valencia, Bartholomew Sharp, William Dampier, and John Clipperton all took part in looting the city.\n\nFollowing the collapse of Spanish rule, in 1821, Arica was part of the recently independent Peruvian Republic. The Peruvian Constitution of 1823 regards it as a province of the Department of Arequipa.\n\nIn 1855, Peru inaugurated the Arica-Tacna railroad (53 km long), one of the first in Latin America. The rail line still functions today.\n\nThe earthquake of August 13, 1868 struck near the city with an estimated magnitude of 8.0 to 9.0 Estimates on the death toll vary greatly, some estimates have the number at 25,000 to 70,000 people. Others estimate that the population of Arica was less than 3,000 people and the death toll was around 300. It triggered a tsunami, measurable across the Pacific in Hawaii, Japan and New Zealand. As Arica lies very close to the subduction zone known as the Peru–Chile Trench where the Nazca Plate dives beneath the South American Plate, the city is subject to megathrust earthquakes.\n\nChilean forces occupied the region following the War of the Pacific. The Treaty of Ancón in 1883 formally acceded the region to Chilean control. The 1929 Tacna-Arica compromise in the Treaty of Lima subsequently restored Tacna to Peru but Arica remained part of Chile.\n\nIn 1958, the Chilean Government established the \"Junta de Adelanto de Arica\" (Board of Development for Arica), which promulgated many tax incentives for the establishment of industries, such as vehicle assembly plants, a tax-free zone, and a casino, among others.\nMany car manufacturers opened plants in Arica, such as Citroën, Peugeot, Volvo, Ford and General Motors, which produced the Chevrolet LUV pickup until 2008.\n\nIn 1975, together with Chile's new open economy policies, the \"Junta de Adelanto de Arica\" was abolished.\n\nThe Arica and Parinacota Region was created on October 8, 2007 under Law 20.175, promulgated on March 23, 2007 by President Michelle Bachelet in the city of Arica.\n\nAccording to the 2002 census by the National Statistics Institute, Arica spans an area of and has 185,268 inhabitants (91,742 men and 93,526 women). Of these, 175,441 (94.7%) lived in urban areas and 9,827 (5.3%) in rural areas. The population grew by 8.8% (14,964 people) between the 1992 and 2002 censuses. Arica is home to 97.7% of the total population of the region.\n\nThe population of Arica is made up of various long-established groups to the region, and other more recent arrivals settled at differing times. Among the long-established groups, the oldest consists of indigenous Amerindians, such as the Aymara, whose presence in the region is of several millennia. These are followed by the second oldest, the local colonial-era groups, which includes local mestizos (of mixed Spanish-Amerindian origin), local criollos (whites of colonial Spanish origin), and local afrodescendants of colonial-era slaves. The third oldest group consists of early post-colonial Chinese Chileans who first arrived as miners and rail workers in the 1890s.\n\nThese long-established groups of Ariqueños have been augmented by various later settlers, mostly other criollos and mestizo Chileans from elsewhere around Chile, but also numerous Europeans, who arrived in the 1900s, including more Spaniards arriving from Spain, as well as Italians, Greeks, British, and French. These arrived at different times during the last century.\n\nSome Ariqueños, primarily the indigenous Amerindians, but also the afrodescendants, share cultural affinities to counterpart populations in neighbouring border areas of Peru, and more distantly, Bolivia.\n\nThe urban area of Arica has 175,441 inhabitants in an area of 41.89 km². Arica in 2007 had more than 185,000 inhabitants (not counting the inhabitants of the valleys and Lluta Azapa, with that reach almost to the 194.000 inhabitants). The growing city of Arica spreads outward into the desert and the Peru-Chile border. The Azapa Valley has developed a year-round agricultural economy due to improvements in irrigation and transportation of its products.\n\nThe villages that make up the commune are Villa Frontera and San Miguel de Azapa. Some hamlets are Poconchile, Molinas, Sora, Las Maitas and Caleta Vitor.\n\nArica was made famous in 1970 by the spiritual master Oscar Ichazo when he held a 10-month training there for 50 North Americans from the Esalen Institute in California. The Arica School, based in the United States of America, has influenced thousands of people all over the world.\n\nThe commune of Arica is composed of 19 census districts.\n\n\nArica is the economical powerhouse of its region. It is an enormous trade and shipping point and vital for the maritimal access of Bolivia.\n\nThe \"Morro de Arica\" is a steep and tall hill located in the city. Its height is 139 meters above sea level. It was the last bulwark of defence for the Peruvian troops who garrisoned the city. It was assaulted and captured on June 7, 1880 by Chilean troops in the last part of their \"Campaña del Desierto\" (Desert Campaign) during the War of the Pacific.\n\nNear the city is the Azapa Valley, an oasis where vegetables and Azapa olives are grown. Economically, it is an important port for Chilean ore, and its tropical latitude, dry climate, and the city's beach, have made Arica a popular tourist destination. It is also a center of rail communication with Bolivia and has its own international airport. Arica has strong ties with the city of Tacna, Peru; many people cross the border daily to travel between the cities, partly because many services (for example, dentists) are cheaper on the Peruvian side. Arica is connected to Tacna in Peru and to La Paz in Bolivia by separate railroad lines.\n\nAccording to the Köppen Climate Classification system, Arica features the rare mild desert climate, abbreviated \"BWn\" on climate maps. Unlike many other cities with arid climates, Arica seldom sees extreme temperatures throughout the course of the year. Arica is also known as the driest inhabited place on Earth, at least as measured by rainfall: average annual precipitation is 0.76 mm (0.03 inches), as measured at the airport meteorological station. Despite its lack of rainfall, Arica typically has a relatively high humidity, with levels similar to those of equatorial climates. The sunshine intensity is similar to the Sahara desert region in the Northern Hemisphere (or like the Cape Verde islands). \n\nAs a commune, Arica is a third-level administrative division of Chile administered by a municipal council, headed by an alcalde who is directly elected every four years. The 2008–2012 alcalde is Waldo Sankán Martínez (Independent).\n\nWithin the electoral divisions of Chile, Arica is represented in the Chamber of Deputies by Mr. Vlado Mirosevic (Partido Liberal) and Mr. Luis Rocaful as part of the 1st electoral district, which includes the entire Arica and Parinacota Region. The commune is represented in the Senate by Fulvio Rossi Ciocca (PS) and Jaime Orpis Bouchon (UDI) as part of the 1st senatorial constituency (Arica and Parinacota Region and Tarapacá Region).\n\nArica was one of the four host cities of the 1962 FIFA World Cup, and it was the venue for a Rip Curl Pro Search surfing event that took place from June 20 to July 1, 2007.\nArica Plays host to a leg of the International Bodyboarding Association's world tour event every year at the notorious \"el flops\" surf break. The event has been running since 2004.\n\n\nOther attractions include the former house of the Governor, the House of Culture, railway station Arica-La Paz, the Archaeological and Anthropological Museum of San Miguel de Azapa, Sea and Historical Arms and Arica. For evening entertainment there is the Casino de Arica.\n\nMore than 20 km of beaches, many known for the quality of surfing, span across the Coastal Range in the northern sector. The harbored location makes these beaches unique from other cities in Chile in terms of topography.\n\nFrom north to south the beaches are located Las Machas, Chinchorro, del Alacrán, El Laucho, La Lisera, Brava, Arenillas Negras, La Capilla, Corazones and La Liserilla.\n\n\nPassenger train services on the Arica–La Paz railway ceased in 1996, but as of 2017 there were proposals to restart services from Arica as a tourist attraction (and for freight).\n\nIn 2011, Chile announced plans to privatise the Port of Arica. These were opposed by Bolivia, as Arica is its main sea port.\n\nChacalluta International Airport is the main airport in Arica and is located 18.5 km to north of the city. In this terminal operates three domestic airlines, LAN Chile, Principal Airlines and Sky Airlines to many Chilean airports and also to Arequipa, Peru and La Paz, Bolivia.\n\n\n"}
{"id": "4555960", "url": "https://en.wikipedia.org/wiki?curid=4555960", "title": "Arsenolite", "text": "Arsenolite\n\nArsenolite is an arsenic mineral, chemical formula AsO. It is formed as an oxidation product of arsenic sulfides. Commonly found as small octahedra it is white, but impurities of realgar or orpiment may give it a pink or yellow hue. It can be associated with its dimorph claudetite (a monoclinic form of AsO) as well as realgar (AsS), orpiment (AsS) and erythrite, Co(AsO)·8HO.\n\nArsenolite belongs to the minerals which are highly toxic.\n\nIt was first described in 1854 for an occurrence in the St Andreasberg District, Harz Mountains, Lower Saxony, Germany.\n\nIt occurs by the oxidation of arsenic-bearing sulfides in hydrothermal veins. It also occurs as a result of mine or coal seam fires.\n"}
{"id": "1775884", "url": "https://en.wikipedia.org/wiki?curid=1775884", "title": "Betaine", "text": "Betaine\n\nA betaine () in chemistry is any neutral chemical compound with a positively charged cationic functional group such as a quaternary ammonium or phosphonium cation (generally: onium ions) that bears no hydrogen atom and with a negatively charged functional group such as a carboxylate group that may not be adjacent to the cationic site. A betaine thus may be a specific type of zwitterion. Historically, the term was reserved for TMG (trimethylglycine) only. Biologically, betaine is involved in methylation reactions and detoxification of homocysteine.\n\nThe pronunciation of the compound reflects its origin and first isolation from sugar \"beets\" (\"Beta vulgaris\" subsp. \"vulgaris\"), and does not derive from the Greek letter beta (β), however, it often is pronounced beta-INE or BEE-tayn. \n\nIn biological systems, many naturally occurring betaines serve as organic osmolytes, substances synthesized or taken up from the environment by cells for protection against osmotic stress, drought, high salinity, or high temperature. Intracellular accumulation of betaines, non-perturbing to enzyme function, protein structure, and membrane integrity, permits water retention in cells, thus protecting from the effects of dehydration. It is also a methyl donor of increasingly recognised significance in biology.\n\nThe original betaine, \"N\",\"N\",\"N\"-trimethylglycine, was named after its discovery in sugar beet (\"Beta vulgaris\" subsp. \"vulgaris\") in the nineteenth century. It is a small \"N\"-trimethylated amino acid, existing in zwitterionic form at neutral pH. In the 21st century, this substance is called \"glycine betaine\" to distinguish it from other betaines that are widely distributed in microorganisms, plants, and animals.\n\nPhosphonium betaines are intermediates in the Wittig reaction.\n\nThe addition of betaine to polymerase chain reactions improves the amplification of DNA by reducing the formation of secondary structure in GC-rich regions. The addition of betaine has been reported to enhance the specificity of the polymerase chain reaction by eliminating the base pair composition dependence of DNA melting.\n\n"}
{"id": "11087760", "url": "https://en.wikipedia.org/wiki?curid=11087760", "title": "Biomimetic material", "text": "Biomimetic material\n\nBiomimetic materials are materials developed using inspiration from nature. This may be useful in the design of composite materials. Natural structures have inspired and innovated human creations. Notable examples of these natural structures include: honeycomb structure of the beehive, strength of spider silks, bird flight mechanics, and shark skin water repellency. The ethymological roots of the neologism (new term) biomimetic dervive from Greek, since \"bios\" means \"life\" and \"mimetikos\" means \"imitative\",\n\nBiomimetic materials in tissue engineering are materials that have been designed such that they elicit specified cellular responses mediated by interactions with scaffold-tethered peptides from extracellular matrix (ECM) proteins; essentially, the incorporation of cell-binding peptides into biomaterials via chemical or physical modification. Amino acids located within the peptides are used as building blocks by other biological structures. These peptides are often referred to as \"self-assembling peptides\", since they can be modified to contain biologically active motifs. This allows them to replicate information derived from tissue and to reproduce the same information independently. Thus, these peptides act as building blocks capable of conducting multiple biochemical activities, including tissue engineering. It should be noted that tissue engineering research currently being performed on both short chain and long chain peptides is still in early stages.\n\nSuch peptides include both native long chains of ECM proteins as well as short peptide sequences derived from intact ECM proteins. The idea is that the biomimetic material will mimic some of the roles that an ECM plays in neural tissue. In addition to promoting cellular growth and mobilization, the incorporated peptides could also mediate by specific protease enzymes or initiate cellular responses not present in a local native tissue.\n\nIn the beginning, long chains of ECM proteins including fibronectin (FN), vitronectin (VN), and laminin (LN) were used, but more recently the advantages of using short peptides have been discovered. Short peptides are more advantageous because, unlike the long chains that fold randomly upon adsorption causing the active protein domains to be sterically unavailable, short peptides remain stable and do not hide the receptor binding domains when adsorbed. Another advantage to short peptides is that they can be replicated more economically due to the smaller size. A bi-functional cross-linker with a long spacer arm is used to tether peptides to the substrate surface. If a functional group is not available for attaching the cross-linker, photochemical immobilization may be used.\n\nIn addition to modifying the surface, biomaterials can be modified in bulk, meaning that the cell signaling peptides and recognition sites are present not just on the surface but also throughout the bulk of the material. The strength of cell attachment, cell migration rate, and extent of cytoskeletal organization formation is determined by the receptor binding to the ligand bound to the material; thus, receptor-ligand affinity, the density of the ligand, and the spatial distribution of the ligand must be carefully considered when designing a biomimetic material.\n\nProteins of the developing enamel extracellular matrix (such as Amelogenin) control initial mineral deposition (nucleation) and subsequent crystal growth, ultimately determining the physico-mechanical properties of the mature mineralized tissue.\n\nMutations in enamel ECM proteins result in enamel defects such as amelogenesis imperfect.\n\nType-I collagen is thought to have a similar role for the formation of dentin and bone.\n\nNucleators bring together mineral ions from the surrounding fluids (such as saliva) into the form of a crystal lattice structure, by stabilizing small nuclei to permit crystal growth, forming mineral tissue.\n\nDental enamel mineral (as well as dentin and bone) is made of hydroxylapatite with foreign ions incorporated in the structure.\n\nCarbonate, fluoride, and magnesium are the most common heteroionic substituents.\n\nIn a biomimetic mineralization strategy based on normal enamel histogenesis, a three-dimensional scaffold is formed to attract and arrange calcium and/or phosphate ions to induce de novo precipitation of hydroxylapatite.\n\nTwo general strategies have been applied. One is using fragments known to support natural mineralization proteins, such as Amelogenin, Collagen, or Dentin Phosphophoryn as the basis.\nAlternatively, de novo macromolecular structures have been designed to support mineralization, not based on natural molecules, but on rational design.\n\nMany studies utilize laminin-1 when designing a biomimetic material. Laminin is a component of the extracellular matrix that is able to promote neuron attachment and differentiation, in addition to axon growth guidance. Its primary functional site for bioactivity is its core protein domain isoleucine-lysine-valine-alanine-valine (IKVAV), which is located in the α-1 chain of laminin.\n\nA recent study by Wu, Zheng et al., synthesized a self-assembled IKVAV peptide nanofiber and tested its effect on the adhesion of neuron-like pc12 cells. Early cell adhesion is very important for preventing cell degeneration; the longer cells are suspended in culture, the more likely they are to degenerate. The purpose was to develop a biomaterial with good cell adherence and bioactivity with IKVAV, which is able to inhibit differentiation and adhesion of glial cells in addition to promoting neuronal cell adhesion and differentiation. The IKVAV peptide domain is on the surface of the nanofibers so that it is exposed and accessible for promoting cell contact interactions. The IKVAV nanofibers promoted stronger cell adherence than the electrostatic attraction induced by poly-L-lysine, and cell adherence increased with increasing density of IKVAV until the saturation point was reached. IKVAV does not exhibit time dependent effects because the adherence was shown to be the same at 1 hour and at 3 hours.\n\nLaminin is known to stimulate neurite outgrowth and it plays a role in the developing nervous system. It is known that gradients are critical for the guidance of growth cones to their target tissues in the developing nervous system. There has been much research done on soluble gradients; however, little emphasis has been placed on gradients of substratum bound substances of the extracellular matrix such as laminin. Dodla and Bellamkonda, fabricated an anisotropic 3D agarose gel with gradients of coupled laminin-1 (LN-1). Concentration gradients of LN-1 were shown to promote faster neurite extension than the highest neurite growth rate observed with isotropic LN-1 concentrations. Neurites grew both up and down the gradients, but growth was faster at less steep gradients and was faster up the gradients than down the gradients.\n\nElectroactive polymers (EAPs) are also known as artificial muscles. EAPs are polymeric materials and they are able to produce large deformation when applied in an electric field. This provides large potential in applications in biotechnology and robotics, sensors, and actuators.\n\nThe production of structural colours concerns a large array of oganisms. From bacteria (\"Flavobacterium\" strain IR1) to multicellular organisms, (\"Hibiscus trionum\", \"Doryteuthis pealeii\" (squid), or \"Chrysochroa fulgidissima\" (beetle)), manipulation of light is not limited to rare and exotic life forms. Different organisms evolved different mechanisms to produce structural colours: multilayered cuticle in some insects and plants, grating like surface in plants, gerometrically organised cells in bacteria... all of theme stand for a source of inspiration towards the development of structurally coloured materials.\nStudy of the firefly abdomen revealed the presence of a 3-layer system comprising the cuticle, the Photogenic layer and then a reflector layer. Microscopy of the reflector layer revealed a granulate structure. Directly inspired from the fire fly Reflector layer, an artificial granulate film composed of hollow silica beads of about 1.05 μm was correlated with a high reflection index and could be used to improve light emission in chemiluminescent systems.\n\nArtificial enzyme is a synthetic materials that can mimic (partial) function of a natural enzyme. Among them, some nanomaterials have been used to mimic natural enzymes. These nanomaterials are termed as nanozymes. Nanozymes as well as other artificial enzymes have found wide applications, from biosensing and immunoassays, to stem cell growth and pollutant removal.\n"}
{"id": "1899305", "url": "https://en.wikipedia.org/wiki?curid=1899305", "title": "Boltzmann relation", "text": "Boltzmann relation\n\nIn a plasma, the Boltzmann relation describes the number density of an isothermal charged particle fluid when the thermal and the electrostatic forces acting on the fluid have reached equilibrium.\n\nIn many situations, the electron density of a plasma is assumed to behave according to the Boltzmann relation, due to their small mass and high mobility.\n\nIf the local electrostatic potentials at two nearby locations are φ and φ, the Boltzmann relation for the electrons takes the form:\n\nwhere \"n\" is the electron number density, \"T\" the temperature of the plasma, and \"k\" is Boltzmann constant.\n\nA simple derivation of the Boltzmann relation for the electrons can be obtained using the momentum fluid equation of the two-fluid model of plasma physics in absence of a magnetic field. When the electrons reach dynamic equilibrium, the inertial and the collisional terms of the momentum equations are zero, and the only terms left in the equation are the pressure and electric terms. For an isothermal fluid, the pressure force takes the form \nwhile the electric term is \nIntegration leads to the expression given above.\n\nIn many problems of plasma physics, it is not useful to calculate the electric potential on the basis of the Poisson equation because the electron and ion densities are not known \"a priori\", and if they were, because of quasineutrality the net charge density is the small difference of two large quantities, the electron and ion charge densities. If the electron density is known and the assumptions hold sufficiently well, the electric potential can be calculated simply from the Boltzmann relation.\n\nDiscrepancies with the Boltzmann relation can occur, for example, when oscillations occur so fast that the electrons cannot find a new equilibrium (see e.g. plasma oscillations) or when the electrons are prevented from moving by a magnetic field (see e.g. lower hybrid oscillations).\n\n"}
{"id": "1217124", "url": "https://en.wikipedia.org/wiki?curid=1217124", "title": "CERN Axion Solar Telescope", "text": "CERN Axion Solar Telescope\n\nThe CERN Axion Solar Telescope (CAST) is an experiment in astroparticle physics to search for axions originating from the Sun. The experiment, sited at CERN in Switzerland, came online in 2002 with the first data-taking run starting in May 2003. The successful detection of solar axions would constitute a major discovery in particle physics, and would also open up a brand new window on the astrophysics of the solar core.\n\nIf the axions exist, they may be produced in the Sun's core when X-rays scatter off electrons and protons in the presence of strong electric fields. The experimental setup is built around a 9.26 m long decommissioned test magnet for the LHC capable of producing a field of up to . This strong magnetic field is expected to convert solar axions back into X-rays for subsequent detection by X-ray detectors. The telescope observes the Sun for about 1.5 hours at sunrise and another 1.5 hours at sunset each day. The remaining 21 hours, with the instrument pointing away from the Sun, are spent measuring background axion levels.\n\nCAST began operation in 2003 searching for axions up to . In 2005, Helium-4 was added to the magnet, extending sensitivity to masses up to 0.39 eV, then Helium-3 was used during 2008–2011 for masses up to 1.15 eV. CAST then ran with vacuum again searching for axions below 0.02 eV.\n\nAs of 2014, CAST has not turned up definitive evidence for solar axions. It has considerably narrowed down the range of parameters where these elusive particles may exist. CAST has set significant limits on axion coupling to electrons and photons.\n\nA 2017 paper using data from the 2013-2015 run reported a new best limit on axion-photon coupling of 0.66E-10 / GeV.\n\n"}
{"id": "2566560", "url": "https://en.wikipedia.org/wiki?curid=2566560", "title": "Canadian Environment Awards", "text": "Canadian Environment Awards\n\nThe Canadian Environment Awards were established in 2002 through a partnership between the Government of Canada and Canadian Geographic Enterprises. The national program recognized dedicated Canadians who act locally to help protect, preserve and restore Canada's environment. Founding sponsor Shell Canada's participation of the event sparked protests due to their controversial Klappan Coalbed Methane Project in northern British Columbia. In 2009, the award was replaced by the 3M Environmental Innovation Award.\n\nThere were three levels of award: the flagship Community Awards, Green Team Challenge for youth and the Citation of Lifetime Achievement, recognizing exceptional individuals. The founding corporate sponsor, Shell Canada, sponsored the awards for five years and supported the Community Awards program where Gold Award recipients received $5,000 and Silver Award winners $2,500 for the environmental cause of his or her choice.\n\n\n"}
{"id": "35818361", "url": "https://en.wikipedia.org/wiki?curid=35818361", "title": "Caring for the Lagoon", "text": "Caring for the Lagoon\n\nCaring for the Lagoon is a documentary directed by Oliver Dickinson about how the Mahorans of Mayotte are trying to preserve their lagoon.\n\nThe film has been selected by numerous festivals throughout the world (i.e. \"Al Jazeera Documentary Film Festival, Kuala Lumpur Eco Film festival, Roshd International Film Festival, Ménigoute Wildlife Film Festival\") and has won many awards (i.e. Grand Prix of Ecology at the \"Warsaw FilmAT Festival 2012\", Best Ecology Film Award at the \"Silver Lake International Film Festival 2011\", Cousteau Award at the \"Ecollywood Film Festival 2011\").\n\n\n"}
{"id": "15107994", "url": "https://en.wikipedia.org/wiki?curid=15107994", "title": "Caulk boots", "text": "Caulk boots\n\nCaulk boots or calk boots (also called cork boots, timber boots, logger boots, logging boots, or corks) are a form of rugged footwear, most often associated with the timber trade but also suitable for use in hiking and in other heavy industry such as manufacturing and construction, due to their safety features. \n\nTimber boots are typically made of leather or suede uppers extending over the ankle, with a thick rubber sole to provide protection and grip, and bearing deep treads or spikes. The ankle is typically cuffed with padded leather. \n\n\n"}
{"id": "1222650", "url": "https://en.wikipedia.org/wiki?curid=1222650", "title": "Central Electricity Authority", "text": "Central Electricity Authority\n\nThe Central Electricity Authority (CEA) was a body that ran the electricity supply industry in England and Wales between 1954 and 1957. The CEA replaced the earlier British Electricity Authority (BEA) as a result of the Electricity Reorganisation (Scotland) Act 1954, which moved responsibility for Scottish electricity supply to the Scottish Office.\n\nThe CEA was in turn dissolved by the Electricity Act 1957 and replaced by the Central Electricity Generating Board and the Electricity Council.\n\n\n"}
{"id": "2309187", "url": "https://en.wikipedia.org/wiki?curid=2309187", "title": "Centro de Estudos e Pesquisas Ambientais", "text": "Centro de Estudos e Pesquisas Ambientais\n\nCEPA is a center of environmental studies, situated in São Francisco do Sul and in São Bento in the state of Santa Catarina, Brazil. It is part of the University of Joinville - UNIVILLE and harbours scientists studying the ecology of the Mata atlântica, the Brazilian Atlantic rain forest as well as marine biology and plant systematics.\n\n"}
{"id": "4717990", "url": "https://en.wikipedia.org/wiki?curid=4717990", "title": "Connaught Bridge Power Station", "text": "Connaught Bridge Power Station\n\nConnaught Bridge Power Station is a combined cycle (2 gas turbines and 1 steam turbine) and open cycle ( 4 gas turbines) power station located near Kampong Java in Klang, Selangor, Malaysia. It is one of the oldest power station in the country.\n\nIt was opened on 26 March 1953 by the High Commissioner for the Federation of Malaya, Sir Gerald Templer.\n\nThe Station has an installed generation capacity of 895 MW, the third largest of seven sister TNB power plants in the country. The power is produced through a combined-cycle block producing 315 MW, and 4 x GT13E1 open cycle gas turbines of 130 MW each.\n\nAs part of the Generation Division of the Tenaga Nasional Group of Malaysia, it shares and operates in support of the parent group’s targets and objectives.\n\nThe Connaught Bridge Power Station is going through a redevelopment project.\n\nThe EPC contract was awarded to Sinohydro Corporation Ltd and Sinohydro Corporation (M) Sdn Bhd Consortium on 2 May 2013 with a contract period of 28 months. After the redevelopment, the plant with capacity of 384.7 MW will commence operation on 1 September 2015.\n"}
{"id": "1192111", "url": "https://en.wikipedia.org/wiki?curid=1192111", "title": "Conservation headland", "text": "Conservation headland\n\nA conservation headland is a strip along the edge of an agricultural field, where pesticides is sprayed only in a selective manner. This increases the number and type of weed and insect species present, and benefits the bird species that depend on them. The grey partridge is one such bird.\nConservation headlands were introduced in the 1980s by scientists working for Game & Wildlife Conservation Trust in Great Britain. Trials have taken place in southern Sweden.\n\n\n"}
{"id": "145040", "url": "https://en.wikipedia.org/wiki?curid=145040", "title": "Conservation of mass", "text": "Conservation of mass\n\nThe law of conservation of mass or principle of mass conservation states that for any system closed to all transfers of matter and energy, the mass of the system must remain constant over time, as system's mass cannot change, so quantity cannot be added nor removed. Hence, the quantity of mass is conserved over time.\n\nThe law implies that mass can neither be created nor destroyed, although it may be rearranged in space, or the entities associated with it may be changed in form. For example, in chemical reactions, the mass of the chemical components before the reaction is equal to the mass of the components after the reaction. Thus, during any chemical reaction and low-energy thermodynamic processes in an isolated system, the total mass of the reactants, or starting materials, must be equal to the mass of the products.\n\nThe concept of mass conservation is widely used in many fields such as chemistry, mechanics, and fluid dynamics. Historically, mass conservation was demonstrated in chemical reactions independently by Mikhail Lomonosov and later rediscovered by Antoine Lavoisier in the late 18th century. The formulation of this law was of crucial importance in the progress from alchemy to the modern natural science of chemistry.\n\nThe conservation of mass only holds approximately and is considered part of a series of assumptions coming from classical mechanics. The law has to be modified to comply with the laws of quantum mechanics and special relativity under the principle of mass-energy equivalence, which states that energy and mass form one conserved quantity. For very energetic systems the conservation of mass-only is shown not to hold, as is the case in nuclear reactions and particle-antiparticle annihilation in particle physics.\n\nMass is also not generally conserved in open systems. Such is the case when various forms of energy and matter are allowed into, or out of, the system. However, unless radioactivity or nuclear reactions are involved, the amount of energy escaping (or entering) such systems as heat, mechanical work, or electromagnetic radiation is usually too small to be measured as a decrease (or increase) in the mass of the system.\n\nFor systems where large gravitational fields are involved, general relativity has to be taken into account, where mass-energy conservation becomes a more complex concept, subject to different definitions, and neither mass nor energy is as strictly and simply conserved as is the case in special relativity.\n\nThe law of conservation of mass can only be formulated in classical mechanics when the energy scales associated to an isolated system are much smaller than formula_1, where formula_2is the mass of a typical object in the system, measured in the frame of reference where the object is at rest, and formula_3 is the speed of light. \n\nThe law can be formulated mathematically in the fields of fluid mechanics and continuum mechanics, where the conservation of mass is usually expressed using the continuity equation, given in differential form asformula_4where formula_5 is the density (mass per unit volume), formula_6 is the time, formula_7 is the divergence, and formula_8 is the flow velocity field.\n\nThe interpretation of the continuity equation for mass is the following: For a given closed surface in the system, the change in time of the mass enclosed by the surface is equal to the mass that traverses the surface, positive if matter goes in and negative if matter goes out. For the whole isolated system, this condition implies that the total mass formula_9, sum of the masses of all components in the system, does not change in time, i.e. formula_10,where formula_11 is the differential that defines the integral over the whole volume of the system.\n\nThe continuity equation for the mass is part of Euler equations of fluid dynamics. Many other convection–diffusion equations describe the conservation and flow of mass and matter in a given system.\n\nIn chemistry, the calculation of the amount of reactant and products in a chemical reaction, or stoichiometry, is founded on the principle of conservation of mass. The principle implies that during a chemical reaction the total mass of the reactants is equal to the total mass of the products. For example, in the following reaction\nwhere one molecule of methane () and two oxygen molecules are converted into one molecule of carbon dioxide () and two of water (). The number of molecules as result from the reaction can be derived from the principle of conservation of mass, as initially four hydrogen atoms, 4 oxygen atoms and one carbon atom are present (as well as in the final state), then the number water molecules produced must be exactly two per molecule of carbon dioxide produced.\n\nMany engineering problems are solved by following the mass distribution in time of a given system, this practice is known as mass balance.\n\nAn important idea in ancient Greek philosophy was that \"Nothing comes from nothing\", so that what exists now has always existed: no new matter can come into existence where there was none before. An explicit statement of this, along with the further principle that nothing can pass away into nothing, is found in Empedocles (approx. 4th century BC): \"For it is impossible for anything to come to be from what is not, and it cannot be brought about or heard of that what is should be utterly destroyed.\"\n\nA further principle of conservation was stated by Epicurus around 3rd century BC, who, describing the nature of the Universe, wrote that \"the totality of things was always such as it is now, and always will be\".\n\nJain philosophy, a non-creationist philosophy based on the teachings of Mahavira (6th century BC), states that the universe and its constituents such as matter cannot be destroyed or created. The Jain text Tattvarthasutra (2nd century CE) states that a substance is permanent, but its modes are characterised by creation and destruction. A principle of the conservation of matter was also stated by Nasīr al-Dīn al-Tūsī (around 13th century CE). He wrote that \"A body of matter cannot disappear completely. It only changes its form, condition, composition, color and other properties and turns into a different complex or elementary matter\".\n\nBy the 18th century the principle of conservation of mass during chemical reactions was widely used and was an important assumption during experiments, even before a definition was formally established, as can be seen in the works of Joseph Black, Henry Cavendish, and Jean Rey. The first to outline the principle was given by Mikhail Lomonosov in 1756. He demonstrated it by experiments and had discussed the principle before in 1774 in correspondence with Leonhard Euler, though his claim on the subject is sometimes challenged. A more refined series of experiments were later carried out by Antoine Lavoisier who expressed his conclusion in 1773 and popularized the principle of conservation of mass. The demonstrations of the principle led alternatives theories obsolete, like the phlogiston theory that claimed that mass could be gained or lost in combustion and heat processes.\n\nThe conservation of mass was obscure for millennia because of the buoyancy effect of the Earth's atmosphere on the weight of gases. For example, a piece of wood weighs less after burning; this seemed to suggest that some of its mass disappears, or is transformed or lost. This was not disproved until careful experiments were performed in which chemical reactions such as rusting were allowed to take place in sealed glass ampoules; it was found that the chemical reaction did not change the weight of the sealed container and its contents. Weighing of gases using scales was not possible until the invention of the vacuum pump in 17th century.\n\nOnce understood, the conservation of mass was of great importance in progressing from alchemy to modern chemistry. Once early chemists realized that chemical substances never disappeared but were only transformed into other substances with the same weight, these scientists could for the first time embark on quantitative studies of the transformations of substances. The idea of mass conservation plus a surmise that certain \"elemental substances\" also could not be transformed into others by chemical reactions, in turn led to an understanding of chemical elements, as well as the idea that all chemical processes and transformations (such as burning and metabolic reactions) are reactions between invariant amounts or weights of these chemical elements.\n\nFollowing to pioneering work of Lavoisier the prolonged and exhaustive experiments of Jean Stas supported the strict accuracy of this law in chemical reactions, even though they were carried out with other intentions. His research indicated that in certain reactions the loss or gain could not have been more than from 2 to 4 parts in 100,000. The difference in the accuracy aimed at and attained by Lavoisier on the one hand, and by Morley and Stas on the other, is enormous.\n\nThe law of conservation of mass was challenged with the advent of special relativity. In one of the Annus Mirabilis papers of Albert Einstein in 1905, he suggested an equivalence between mass and energy. This theory implied several assertions, like the idea that internal energy of a system could contribute to the mass of whole the system, or that mass could be converted into electromagnetic radiation. However, as Max Planck pointed out, a change in mass as a result of extraction or addition of chemical energy, as predicted by Einstein's theory, is so small that it could not be measured with the available instruments and could not be presented as a test to the special relativity. Einstein speculated that the energies associated with newly discovered radioactivity were significant enough, compared with the mass of systems producing them, to enable their mass-change to be measured, once the energy of the reaction had been removed from the system. This later indeed proved to be possible, although it was eventually to be the first artificial nuclear transmutation reaction in 1932, demonstrated by Cockcroft and Walton, that proved the first successful test of Einstein's theory regarding mass-loss with energy-loss.\n\nThe law conservation of mass and the analogous law of conservation of energy were finally overruled by a more general principle known as the mass–energy equivalence. Special relativity also redefines the concept of mass and energy, which can be used interchangeably and are relative to the frame of reference. Several definitions had to be defined for consistency like \"rest mass\" of a particle (mass in the rest frame of the particle) and \"relativistic mass\" (in another frame). The latter term is usually less frequently used. For more discussion, see Mass in special relativity.\n\nIn special relativity, the conservation of mass does not apply if the system is open and energy escapes. However, it does continue to apply to totally closed (isolated) systems. If energy cannot escape a system, its mass cannot decrease. In relativity theory, so long as any type of energy is retained within a system, this energy exhibits mass.\n\nAlso, mass must be differentiated from matter (see below), since matter may \"not\" be perfectly conserved in isolated systems, even though mass is always conserved in such systems. However, matter is so nearly conserved in chemistry that violations of matter conservation were not measured until the nuclear age, and the assumption of matter conservation remains an important practical concept in most systems in chemistry and other studies that do not involve the high energies typical of radioactivity and nuclear reactions.\n\nThe change in mass of certain kinds of open systems where atoms or massive particles are not allowed to escape, but other types of energy (such as light or heat) are allowed to enter or escape, went unnoticed during the 19th century, because the change in mass associated with addition or loss of small quantities of thermal or radiant energy in chemical reactions is very small. (In theory, mass would not change at all for experiments conducted in isolated systems where heat and work were not allowed in or out.)\n\nThe conservation of relativistic mass implies the viewpoint of a single observer (or the view from a single inertial frame) since changing inertial frames may result in a change of the total energy (relativistic energy) for systems, and this quantity determines the relativistic mass.\n\nThe principle that the mass of a system of particles must be equal to the sum of their rest masses, even though true in classical physics, may be false in special relativity. The reason that rest masses cannot be simply added is that this does not take into account other forms of energy, such as kinetic and potential energy, and massless particles such as photons, all of which may (or may not) affect the total mass of systems.\n\nFor moving massive particles in a system, examining the rest masses of the various particles also amounts to introducing many different inertial observation frames (which is prohibited if total system energy and momentum are to be conserved), and also when in the rest frame of one particle, this procedure ignores the momenta of other particles, which affect the system mass if the other particles are in motion in this frame.\n\nFor the special type of mass called invariant mass, changing the inertial frame of observation for a whole closed system has no effect on the measure of invariant mass of the system, which remains both conserved and invariant (unchanging), even for different observers who view the entire system. Invariant mass is a system combination of energy and momentum, which is invariant for any observer, because in any inertial frame, the energies and momenta of the various particles always add to the same quantity (the momentum may be negative, so the addition amounts to a subtraction). The invariant mass is the relativistic mass of the system when viewed in the center of momentum frame. It is the minimum mass which a system may exhibit, as viewed from all possible inertial frames.\n\nThe conservation of both relativistic and invariant mass applies even to systems of particles created by pair production, where energy for new particles may come from kinetic energy of other particles, or from one or more photons as part of a system that includes other particles besides a photon. Again, neither the relativistic nor the invariant mass of totally closed (that is, isolated) systems changes when new particles are created. However, different inertial observers will disagree on the value of this conserved mass, if it is the relativistic mass (i.e., relativistic mass is conserved but not invariant). However, all observers agree on the value of the conserved mass if the mass being measured is the invariant mass (i.e., invariant mass is both conserved and invariant).\n\nThe mass-energy equivalence formula gives a different prediction in non-isolated systems, since if energy is allowed to escape a system, both relativistic mass and invariant mass will escape also. In this case, the mass-energy equivalence formula predicts that the \"change\" in mass of a system is associated with the \"change\" in its energy due to energy being added or subtracted: formula_12 This form involving changes was the form in which this famous equation was originally presented by Einstein. In this sense, mass changes in any system are explained simply if the mass of the energy added or removed from the system, are taken into account.\n\nThe formula implies that bound systems have an invariant mass (rest mass for the system) less than the sum of their parts, if the binding energy has been allowed to escape the system after the system has been bound. This may happen by converting system potential energy into some other kind of active energy, such as kinetic energy or photons, which easily escape a bound system. The difference in system masses, called a mass defect, is a measure of the binding energy in bound systems – in other words, the energy needed to break the system apart. The greater the mass defect, the larger the binding energy. The binding energy (which itself has mass) must be released (as light or heat) when the parts combine to form the bound system, and this is the reason the mass of the bound system decreases when the energy leaves the system. The total invariant mass is actually conserved, when the mass of the binding energy that has escaped, is taken into account.\n\nIn general relativity, the total invariant mass of photons in an expanding volume of space will decrease, due to the red shift of such an expansions. The conservation of both mass and energy therefore depends on various corrections made to energy in the theory, due to the changing gravitational potential energy of such systems.\n\n"}
{"id": "9930466", "url": "https://en.wikipedia.org/wiki?curid=9930466", "title": "Controlled aerodynamic instability phenomena", "text": "Controlled aerodynamic instability phenomena\n\nThe term controlled aerodynamic instability phenomena was first used by Cristiano Augusto Trein in the \"Nineteenth KKCNN Symposium on Civil Engineering\" held in Kyoto – Japan in 2006. The concept is based on the idea that aerodynamic instability phenomena, such as Kármán vortex street, flutter, galloping and buffeting, can be driven into a controlled motion and be used to extract energy from the flow, becoming an alternative approach for wind power generation systems.\n\nNowadays, when a discussion is established around the theme wind power generation, what is promptly addressed is the image of a big wind turbine getting turned by the wind. However, some alternative approaches have already been proposed in the latter decades, showing that wind turbines are not the only possibility for the exploitation of the wind for power generation purposes.\n\nIn 1977 Jeffery experimented with an oscillating aerofoil system based on a vertically mounted pivoting wing which flapped in the wind. Farthing discovered that this free flutter could automatically cease for high wind protection and developed floating and pile based models for pumping surface and well water as well as compressing air with auxiliary battery charging. McKinney and DeLaurier in 1981 proposed a system called \"wingmill\", based on a rigid horizontal airfoil with articulated pitching and plunging to extract energy from the flow. This system has stimulated Moores in 2003 to conduct further investigations on applications of such idea.\n\nFollowing the same trend, other studies have already been carried out, for example the \"flutter power generation system\" proposed by Isogai et al. in 2003, which uses the flutter instability caused by the wind on an aerofoil to extract energy from the flow. In this branch, Matsumoto et al. went further, proposing enhancements for that system and assessing the feasibility of its usage with bluff bodies. The \"kite motors\" of Dave Santos utilize aerofoil instabilities.\n\nThe wind interacts with the obstacles it reaches in its way by transferring a part of its energy to those interactions, which are converted into forces over the bodies, leading them to different levels of motion, which are directly dependent on their aeroelastic and geometric characteristics. A large amount of studies and researches has been conducted concerning these interactions and their dependencies, aiming the understanding of the aerodynamic phenomena that arise due to them, such as the Kármán vortex street, galloping, buffeting and flutter, mainly regarding bluff bodies. By the understanding of such phenomena it is possible to predict instabilities and their consequent motions, feeding the designers with the data they need in order to arrange the structures properly.\n\nIn the great majority of the cases – e.g.: in civil buildings – such motions are useless and undesirable, in a manner that all the designing approaches are focused on avoiding them. However these instabilities may also be used in a profitable manner: if they are controlled and driven to a predictable motion, they can provide mechanical power supply to run, for example, turbines, machinery and electricity generators.\n\nSo, by using the knowledge acquired by now regarding those aerodynamic instabilities and by developing new features, it is possible to propose ways to stimulate them to an optimal state, using them for power generation purposes. That way, alternative approaches to the windmill may be proposed and developed. Farthing Econologica applies the practical requirements for a windmill to greatly whittle down the possibilities.\n\n"}
{"id": "53616779", "url": "https://en.wikipedia.org/wiki?curid=53616779", "title": "Coquille board", "text": "Coquille board\n\nCoquille board, also known as stipple board, is a type of drawing paper with a pebbled texture. Used with a soft crayon or carbon pencil, coquille produces a shading effect similar to stippling, and is useful in works to be reproduced in print, such as scientific illustration and cartooning.\n"}
{"id": "6821110", "url": "https://en.wikipedia.org/wiki?curid=6821110", "title": "Disston Saw Works", "text": "Disston Saw Works\n\nDisston Saw Works of Philadelphia was one of the better known and highly regarded manufacturers of handsaws in the United States. During the Machine Age, as Henry Disston & Sons, Inc., it was a supplier of industrial saw blades. A successor corporation, still active in Philadelphia, is called Disston Precision.\n\nThe story of handsaws in the United States mirrors the technical and political development of steel. Sheffield, England, was the center of handsaw production during the 18th century and through most of the 19th century because of its fine steel and skilled craftsmen. But England's political and economic lock on steel making in the colonies held American sawmakers at bay until well after the Revolutionary War. American steel producers could not compete until import tariffs leveled the playing field in 1861.\n\nThis was the environment in which young Henry Disston (1819–1878) began his career as an American sawmaker in Philadelphia. He had immigrated from England in 1833 and started making saws and squares in 1840. In 1850, he founded the company that would become the largest sawmaker in the world: the Keystone Saw Works.\n\nSome five years later, Disston built a furnace—perhaps the first melting plant for steel in America—and began producing the first crucible saw steel ever made in the United States. While his competitors were buying good steel from Britain, he was making his own, to his own specification, for his own needs. Disston subsequently constructed a special rolling mill exclusively for saw blades.\n\nOver the following decade, the Disston company continued to grow, even while dedicating itself to the Union Army's war effort. In 1865, when his son Hamilton Disston rejoined the business after serving in the Civil War, Disston changed the company's name to Henry Disston & Son. Henry Disston and his sons set the standards for American sawmakers, both in terms of producing high-quality saws and developing innovative manufacturing techniques. Disston also started making files in 1865.\n\nIn September 1872, Henry Disston and two other men dug part of the foundation for what was to become the largest saw manufacturing facility in the world: Disston Saw Works. This was in the Tacony section of Philadelphia. Having previously moved his expanding business from near Second and Market Streets to Front and Laurel Streets, Disston sought to establish his business away from this cramped area. It took over 25 years to move the entire facility to Tacony. This Philadelphia neighborhood seems to have been the only company town in the United States established within an existing city.\n\nHenry Disston was renowned for having one of the first industries that exhibited environmental responsibility, as well as a paternalistic view towards his employees. For example, he had thousands of homes built in Tacony for his workmen. Funds to purchase these homes were made available through a building and loan association set up by the Disston firm. Mr. Disston was ready to grant any assistance needed to see to it that his workers could purchase a home, even if advances needed to be made.\n\nOther examples of Henry Disston's caring influence on the community was evident in everyday life. To meet employees' cultural needs, a hall and a library were built with Henry Disston agreeing to pay a fixed sum towards its maintenance. The Tacony Music Hall was erected in 1885, also with the assistance of Disston money.\n\nHenry Disston had fallen ill by 1877 and never truly recovered; he suffered a stroke and died the next year. This came only one and a half years after seeing his products receive the highest honors at the great Philadelphia Centennial Exposition of 1876. His vision of a working class community and the completion of the transfer of his enormous saw plant was carried out by his wife and his sons.\n\nThe company, known as Henry Disston and Sons, Inc by the early 20th century, cast the first crucible steel in the nation from an electric furnace in 1906. The firm's armor-plating building near Princeton Avenue and Milnor Street contributed tremendously to the World War II effort, building a volume of armor plates for steel tanks.\n\nBut the company's innovation and industriousness would not last forever. In 1955, with mounting cash-flow problems and waning interest on the family's part to run the firm, Henry Disston and Sons was sold to the H.K. Porter Company of Pittsburgh. Porter's Disston Division was sold in 1978 and became the Henry Disston Division of Sandvik Saw of Sweden. This division was then sold in 1984 to R.A.F. Industries of Philadelphia and became known as Disston Precision Incorporated, maker of specialized flat steel products. Although the company has ceased making Disston handsaws, the Disston brand name still exists in this firm.\n\n\n"}
{"id": "331784", "url": "https://en.wikipedia.org/wiki?curid=331784", "title": "Drywall", "text": "Drywall\n\nDrywall (also known as plasterboard, wallboard, sheet rock, gypsum board, or gypsum panel) is a panel made of calcium sulfate dihydrate (gypsum), with or without additives, typically extruded between thick sheets of facer and backer paper, used in the construction of interior walls and ceilings. The plaster is mixed with fiber (typically paper, fiberglass, asbestos, or a combination of these materials), plasticizer, foaming agent, and various additives that can reduce mildew, flammability, and water absorption.\n\nDrywall construction became prevalent in North America as a time and labor saving alternative to traditional lath and plaster.\n\nThe first plasterboard plant in the UK was opened in 1888 in Rochester, Kent. Sackett Board was invented in 1894 by Augustine Sackett and Fred Kane, graduates of Rensselaer Polytechnic Institute. It was made by layering plaster within four plies of wool felt paper. Sheets were 36\" × 36\" × 1/4\" (914 × 914 × 6.4 mm) thick with open (untaped) edges.\n\nGypsum board evolved between 1910 and 1930 beginning with wrapped board edges and elimination of the two inner layers of felt paper in favor of paper-based facings. In 1910 United States Gypsum Corporation bought Sackett Plaster Board Company and by 1917 introduced \"Sheetrock\". Providing efficiency of installation, it was developed additionally as a measure of fire resistance. Later air entrainment technology made boards lighter and less brittle, and joint treatment materials and systems also evolved. Gypsum lath was an early substrate for plaster. An alternative to traditional wood or metal lath, it was a panel made up of compressed gypsum plaster board that was sometimes grooved or punched with holes to allow wet plaster to key into its surface. As it evolved, it was faced with paper impregnated with gypsum crystals that bonded with the applied facing layer of plaster. In 1936 US Gypsum trademarked ROCKLATH for their gypsum lath product.\n\nIn 2002 the European Commission imposed fines totaling €478 million on the companies Lafarge, BPB, and Gyproc Benelux, which had operated a cartel on the market which affected 80% of consumers in France, the UK, Germany and the Benelux countries.\n\nA wallboard panel consists of a layer of gypsum plaster sandwiched between two layers of paper. The raw gypsum, CaSO·2 HO, is heated to drive off the water then slightly rehydrated to produce the hemihydrate of calcium sulfate (CaSO·½ HO). The plaster is mixed with fibre (typically paper and/or fibreglass), plasticizer, foaming agent, finely ground gypsum crystal as an accelerator, EDTA, starch or other chelate as a retarder, various additives that may decrease mildew and increase fire resistance, and wax emulsion or silanes for lower water absorption. The board is then formed by sandwiching a core of the wet mixture between two sheets of heavy paper or fibreglass mats. When the core sets it is then dried in a large drying chamber, and the sandwich becomes rigid and strong enough for use as a building material.\n\nDrying chambers typically use natural gas today. To dry 1 MSF () of wallboard, between is required. Organic dispersants/plasticisers are used so the slurry will flow during manufacture, and to reduce the water and hence the drying time. Coal-fired power stations include devices called scrubbers to remove sulphur from their exhaust emissions. The sulphur is absorbed by powdered limestone in a process called flue-gas desulphurization (FGD), which produces a number of new substances. One is called \"FGD gypsum\". This is commonly used in drywall construction in the United States and elsewhere.\n\nDrywall panels in the United States are manufactured in , and wide panels in varying lengths to suit the application, though 48-inch is by far the most common width. Lengths up to 16 feet (4.9 m) are commonly available, though the most common length is 8 feet (2.4 m). Common panel thicknesses are and , with panels also available in , , 3/4-inch (19.0 mm) and 1-inch (25.4 mm) for specific applications.\n\nIn Europe, most plasterboard is made in 120 cm-wide sheets, though 90 cm and 60 cm wide sheets are also made. 120 cm wide plasterboard is most commonly made in 240 cm lengths, though 250, 260, 270, 280, 300 cm and even longer (if ordered) are commonly available. Thicknesses of plasterboard available are 9.5 mm to 25 mm.\n\nPlasterboard is commonly made with one of three different edge treatments: tapered edge, where the long edges of the board are tapered with a wide bevel at the front to allow for jointing materials to be finished flush with the main board face; plain edge, used where the whole surface will receive a thin coating (skim coat) of finishing plaster; and, finally, beveled on all four sides, used in products specialized for roofing. However, four-side chamfered drywall is not currently offered by major UK manufacturers for general use.\n\nThe term plasterboard is used in Australia and New Zealand, and in the latter country it is also known as Gibraltar board; it is manufactured in thicknesses of 10 mm, 13 mm, and 16 mm, and sometimes other thicknesses up to 25 mm. Panels are commonly sold in 1200 × 2400 mm, 1200 × 4800 mm, and 1200 × 6000 mm sheets. Sheets are usually secured to either a timber or cold-formed steel frames anywhere from 150 to 300 mm centres along the beam and 400 to 600 mm across members.\n\nVarious companies, such as Boral and CSR, manufacture plasterboard under various brand names including Gyprock.\n\nAs an alternative to a week-long plaster application, an entire house can be drywalled in one or two days by two experienced drywallers, and drywall is easy enough to use that it can be installed by many amateur home carpenters. In large-scale commercial construction, the work of installing and finishing drywall is often split between the drywall mechanics, or \"hangers\", who install the wallboard, and the \"tapers\" and \"mudmen\", or \"float crew\", who finish the joints and cover the fastener heads with drywall compound. Dry wall can be finished anywhere from a level 0 to a level 5, where 0 is not finished in any fashion and 5 is the most \"pristine\". Depending on how significant the finish is to the customer the extra steps in the finish may or may not be necessary, though priming and painting of drywall is recommended in any location where it may be exposed to any wear.\n\nDrywall is cut to size, using a large T-square, by scoring the paper on the finished side (usually white) with a utility knife, breaking the sheet along the cut, and cutting the paper backing. Small features such as holes for outlets and light switches are usually cut using a keyhole saw or a small high-speed bit in a rotary tool. Drywall is then fixed to the wall structure with nails or drywall screws and often glue. \"Drywall fasteners\", also referred to as \"drywall clips\" or \"stops\", are gaining popularity in both residential and commercial construction. Drywall fasteners are used for supporting interior drywall corners and replacing the non-structural wood or metal blocking that traditionally was used to install drywall. Their function serves to save on material and labour expenses, to minimize call-backs due to truss uplift, to increase energy efficiency, and to make plumbing and electrical installation simpler.\n\nWhen driven fully home, drywall screws countersink their heads slightly into the drywall. They use a 'bugle head', a concave taper, rather than the conventional conical countersunk head; this compresses the drywall surface rather than cutting into it and so avoids tearing the paper. Screws for light-gauge steel framing have an acute point and finely spaced threads. If the steel framing is heavier than 20-gauge, self-tapping screws with finely spaced threads must be used. In some applications, the drywall may be attached to the wall with adhesives.\n\nAfter the sheets are secured to the wall studs or ceiling joists, the installer conceals the seams between drywall sheets with 'joint tape' and several layers of 'joint compound' (sometimes called 'mud'), typically spread with a taping knife or putty knife. This compound is also applied to any screw holes or defects. The compound is allowed to air dry then typically sanded smooth before painting. Alternatively, for a better finish, the entire wall may be given a 'skim coat', a thin layer (about 1 mm or 1/16 inch) of finishing compound, to minimize the visual differences between the paper and mudded areas after painting.\n\nAnother similar skim coating is always done in a process called veneer plastering, although it is done slightly thicker (about 2 mm or 1/8 inch). Veneering uses a slightly different specialized setting compound (\"finish plaster\") that contains gypsum and lime putty. This application uses blueboard, which has special treated paper to accelerate the setting of the gypsum plaster component. This setting has far less shrinkage than the air-dry compounds normally used in drywall, so it only requires one coat. Blueboard also has square edges rather than the tapered-edge drywall boards. The tapered drywall boards are used to countersink the tape in taped jointing whereas the tape in veneer plastering is buried beneath a level surface. One coat veneer plaster over dry board is an intermediate style step between full multi-coat \"wet\" plaster and the limited joint-treatment-only given \"dry\" wall.\n\nThe method of installation and type of drywall can reduce sound transmission through walls and ceilings. Several builders' books state that thicker drywall reduces sound transmission, but engineering manuals recommend using multiple layers of drywall, sometimes of different thicknesses and glued together, or special types of drywall designed to reduce noise. Also important are the construction details of the framing with steel studs, wider stud spacing, double studding, insulation, and other details reducing sound transmission. Sound transmission class (STC) ratings can be increased from 33 for an ordinary stud-wall to as high as 59 with double 1/2\" drywall on both sides of a wood stud wall with resilient channels on one side and fiberglass batt insulation between the studs.\n\nSound transmission may be slightly reduced using regular -inch panels (with or without light-gauge resilient metal channels and/or insulation), but it is more effective to use two layers of drywall, sometimes in combination with other factors, or specially designed, sound-resistant drywall.\n\nDrywall is highly vulnerable to moisture due to the inherent properties of the materials that comprise it: gypsum, paper, and organic additives and binders. Gypsum will soften with exposure to moisture, and eventually turn to a gooey paste with prolonged immersion, such as during a flood. During such incidents, some or all of the drywall in an entire building may need to be removed and replaced. Furthermore, the paper facings and organic additives mixed with the gypsum core are food for mold.\n\nThe porosity of the board—introduced during manufacturing to reduce the weight of the board, lowering construction time and transportation costs—enables water to rapidly reach the core through capillary action, where mold can grow inside. Water that enters a room from overhead may cause ceiling drywall tape to separate from the ceiling as a result of the grooves immediately behind the tape where the drywall pieces meet becoming saturated. The drywall may also soften around the screws holding the drywall in place and with the aid of gravity, the weight of the water may cause the drywall to sag and eventually collapse, requiring replacement.\n\nDrywall's paper facings are edible to termites, which can eat the paper if they are infesting a wall cavity that is covered with drywall. This causes the painted surface to crumble to the touch, its paper backing material having been eaten. In addition to the necessity of patching the damaged surface and repainting, if enough of the paper has been eaten, the gypsum core can easily crack or crumble without it and the drywall must be removed and replaced.\n\nIn many circumstances, especially when the drywall has been exposed to water or moisture for less than 48 hours, professional restoration experts can avoid the cost, inconvenience, and difficulty of removing and replacing the affected drywall. They use rapid drying techniques that eliminate the elements required to support microbial activity while also restoring most or all of the drywall. \n\nIt is for these reasons that greenboard and ideally cement board are used for rooms expected to have high humidity, primarily kitchens, bathrooms, and laundry rooms.\n\nA substantial amount of defective drywall was imported into the United States from China and incorporated into tens of thousands of homes during rebuilding in 2006 and 2007 following Hurricane Katrina and in other places. Complaints included foul odour, health effects, and corrosion of metal within the structure. This is caused by the emission of sulphurous gases. The same drywall was sold in Asia without problems resulting, but U.S. homes are built much more tightly than homes in China, with less ventilation. Volatile sulphur compounds, including hydrogen sulphide, have been detected as emissions from the imported drywall and may be linked to health problems. These compounds are emitted from many different types of drywall.\n\nA number of lawsuits are underway in many jurisdictions, but many of the sheets of drywall are simply marked, \"Made in China\", thus making identification of the manufacturer difficult. An investigation by the Consumer Product Safety Commission, CPSC, was underway in 2009. In November 2009, the CPSC reported a \"strong association\" between Chinese drywall and corrosion of pipes and wires reported by thousands of homeowners in the United States. The issue was resolved in 2011 and now all drywall must be tested for volatile sulfur and any containing more than 10 ppm is unable to be sold in the US.\n\nDrywall is made primarily from gypsum (CaSO•2HO). As its chemical formula shows, gypsum contains chemically combined water (approximately 50% by volume). When gypsum panels are exposed to fire, heat is absorbed as a portion of the combined water is driven off as steam. This chemical process is called calcination. The thermal energy that converts the water to steam is thus diverted and absorbed, keeping the opposite side of the gypsum panels cool as long as there is crystalline water left to be converted into steam or until the gypsum panel is breached. In the case of regular gypsum board, as the crystalline water is driven off, the reduction of volume within the gypsum core causes large cracks to form, eventually causing the panel to fail due to loss of structural integrity. This is similar to the cracking that can be observed in a dry lake or river bed.\n\nWhen used as a component in fire barriers, drywall is a passive fire protection item. In its natural state, gypsum contains the water of crystallization bound in the form of hydrates. When exposed to heat or fire, this water is vaporized, over a range of temperatures from 80° to 170 °C (see calcium sulphate), retarding heat transfer until the water in the gypsum is gone. This makes drywall an ablative material because as the hydrates sublime, a crumbly dust is left behind, which, along with the paper, is sacrificial. Generally, the more layers of Type X drywall one adds, the more one increases the fire-resistance of the assembly, up to four hours for walls and three hours for ceilings. Evidence of this can be found both in publicly available design catalogues, including DIN 4102 Part 4 and the Canadian Building Code on the topic, as well as common certification listings, including certification listings provided by Underwriters Laboratories and Underwriters Laboratories of Canada (ULC). \"Type X\" drywall is formulated by adding glass fibres to the gypsum, to increase the resistance to fires, especially once the hydrates are spent, which leaves the gypsum in powder form. Type X is typically the material chosen to construct walls and ceilings that are required to have a fire-resistance rating.\n\nFire testing of drywall assemblies for the purpose of expanding national catalogues, such as the National Building Code of Canada, Germany's Part 4 of DIN4102 and its British cousin BS476, are a matter of routine research and development work in more than one nation and can be sponsored jointly by national authorities and representatives of the drywall industry. For example, the National Research Council of Canada routinely publishes such findings. The results are printed as approved designs in the back of the building code. Generally, exposure of drywall on a panel furnace removes the water and calcines the exposed drywall and also heats the studs and fasteners holding the drywall. This typically results in deflection of the assembly towards the fire, as that is the location where the sublimation occurs, which weakens the assembly, due to the fire influence.\n\nCosponsored tests result in code recognized designs with assigned fire-resistance ratings. The resulting designs become part of the code and are not limited to use by any one manufacturer. However, individual manufacturers may also have proprietary designs that they have had third-party tested and approved. This is provided that the material used in the field configuration can be demonstrated to meet the minimum requirements of Type X drywall (such as an entry in the appropriate category of the UL Building Materials Directory or in the Gypsum Association Fire Resistance and Sound Control Design Manual) and that sufficient layers and thicknesses are used. Fire test reports for such unique third party tests are confidential but may be made available to code officials upon special request.\n\nIt's important to consider deflection of drywall assemblies to maintain their assembly integrity to preserve their ratings. Deflection of drywall assemblies can vary somewhat from one test to another. Importantly, penetrants do not follow the deflection movement of the drywall assemblies they penetrate. For example, see cable tray movement in a . It is, therefore, important to test firestops in full scale wall panel tests, so that the deflection of each applicable assembly can be taken into account.\n\nThe size of the test wall assembly alone is not the only consideration for firestop tests. If the penetrants are mounted to and hung off the drywall assembly itself during the test, this does not constitute a realistic deflection exposure insofar as the firestop is concerned. In reality, on a construction site, penetrants are hung off the ceiling above. Penetrants may increase in length, push and pull as a result of operational temperature changes (e.g., hot and cold water in a pipe), particularly in a fire. But it is a physical impossibility to have the penetrants follow the movement of drywall assemblies that they penetrate, since they are not mounted to the drywalls in a building.\n\nIt is, therefore, counterproductive to suspend penetrants from the drywall assembly during a fire test. As downward deflection of the drywall assembly and buckling towards the fire occurs, the top of the firestop is squeezed and the bottom of the firestop is pulled. This is motion above that caused by expansion of metallic penetrants due to heat exposure in a fire. Both types of motion occur because metal first expands in a fire, and then softens once the critical temperature has been reached, as is explained under structural steel. To simulate the drywall deflection effect, one can simply mount the penetrants to the steel frame holding the test assembly. The operational and fire-induced motion of the penetrants, which is independent of the assemblies penetrated, can be separately arranged.\n\nDrywall provides a thermal resistance R-value (in US units) of 0.32 for -inch board, 0.45 for -inch, 0.56 for -inch, and 0.83 for 1-inch board. In addition to increased R-value, thicker drywall has a higher sound transmission class.\n\nIn Type X gypsum board, special glass fibers are intermixed with the gypsum to reinforce the core of the panels. These fibers have the\neffect of reducing the size of the cracks that form as the water is driven off, thereby extending the length of time the gypsum panels resist fire without failure.\n\nType C gypsum panels provide even greater fire resistance than Type X. As with the Type X panels, the core of the Type C panels contains glass fibers, only in a much higher percent by weight. In addition to the greater amount of glass fiber, the core of the Type C panels also contains vermiculite, which acts as a shrinkage-compensating additive that expands when exposed to elevated temperatures of a fire. This expansion occurs at roughly the same temperature as the calcination of the gypsum in the core. It allows the core of the Type C panels to remain dimensionally stable in the presence of fire, which in turn allows the panels to remain in place for a longer period of time even after the combined water has been driven off.\n\nNorth America is one of the largest gypsum board users in the world with a total wallboard plant capacity of per year (worldwide per year). Moreover, the homebuilding and remodeling markets in North America in the late 1990s and early 2000s increased demand. The gypsum board market was one of the biggest beneficiaries of the housing boom as \"an average new American home contains more than 7.31 metric tons of gypsum.\"\n\nThe introduction in March 2005 of the Clean Air Interstate Rule by the United States Environmental Protection Agency requires power plants to \"cut sulfur dioxide emissions by 73%\" by 2018. The Clean Air Interstate Rule also requested that the power plants install new scrubbers (industrial pollution control devices) to remove sulfur dioxide present in the output waste gas. Scrubbers use the technique of flue-gas desulfurization (FGD), which produces synthetic gypsum as a usable by-product. In response to the new supply of this raw material, the gypsum board market was predicted to shift significantly. However, issues such as mercury release during calcining need to be resolved.\n\n\nBecause up to 12% of drywall is wasted during the manufacturing and installation processes and the drywall material is frequently not re-used, disposal can become a problem. Some landfill sites have banned the dumping of drywall. Some manufacturers take back waste wallboard from construction sites and recycle it into new wallboard. Recycled paper is typically used during manufacturing. More recently, recycling at the construction site itself is being researched. There is potential for using crushed drywall to amend certain soils at building sites, such as sodic clay and silt mixtures (bay mud), as well as using it in compost. As of 2016, industry standards are being developed to ensure that when and if wallboard is taken back for recycling, quality and composition are maintained.\n\n"}
{"id": "9683136", "url": "https://en.wikipedia.org/wiki?curid=9683136", "title": "Effective medium approximations", "text": "Effective medium approximations\n\nEffective medium approximations (abbreviated as EMA) or effective medium theory (EMT) pertain to analytical or theoretical modeling that describes the macroscopic properties of composite materials. EMAs or EMTs are developed from averaging the multiple values of the constituents that directly make up the composite material. At the constituent level, the values of the materials vary and are inhomogeneous. Precise calculation of the many constituent values is nearly impossible. However, theories have been developed that can produce acceptable approximations which in turn describe useful parameters and properties of the composite material as a whole. In this sense, effective medium approximations are descriptions of a medium (composite material) based on the properties and the relative fractions of its components and are derived from calculations.\n\nThere are many different effective medium approximations, each of them being more or less accurate in distinct conditions. Nevertheless, they all assume that the macroscopic system is homogeneous and typical of all mean field theories, they fail to predict the properties of a multiphase medium close to the percolation threshold due to the absence of long-range correlations or critical fluctuations in the theory.\n\nThe properties under consideration are usually the conductivity formula_1 or the dielectric constant formula_2 of the medium. These parameters are interchangeable in the formulas in a whole range of models due to the wide applicability of the Laplace equation. The problems that fall outside of this class are mainly in the field of elasticity and hydrodynamics, due to the higher order tensorial character of the effective medium constants.\n\nEMAs can be discrete models such as applied to resistor networks, or continuum theories as applied to elasticity or viscosity. but most of the current theories have difficulty in describing percolating systems. Indeed, among the numerous effective medium approximations, only Bruggeman's symmetrical theory is able to predict a threshold. This characteristic feature of the latter theory puts it in the same category as other mean field theories of critical phenomena.\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,(9)</math>\n\n\n"}
{"id": "52196908", "url": "https://en.wikipedia.org/wiki?curid=52196908", "title": "Energy in Turkmenistan", "text": "Energy in Turkmenistan\n\nTurkmenistan had a total primary energy supply (TPES) of 26.75 Mtoe in 2014. Electricity consumption was 14.64 TWh. \nMost of this primary energy came from fossil fuels.\nAll of the electricity is generated with natural gas.\n\n"}
{"id": "15341790", "url": "https://en.wikipedia.org/wiki?curid=15341790", "title": "Fauna of Africa", "text": "Fauna of Africa\n\nThe fauna of Africa, in its broader sense, is all the animals living in Africa and its surrounding seas and islands. The more characteristic African fauna is found in the Afrotropical ecoregion. Lying almost entirely within the tropics, and equally to north and south of the equator creates favourable conditions for rich wildlife.\n\nWhereas the earliest traces of life in fossil record of Africa date back to the earliest times, the formation of African fauna as we know it today, began with the splitting up of the Gondwana supercontinent in the mid-Mesozoic era. \nAfter that, four to six faunal assemblages, the so-called African Faunal Strata (AFSs) can be distinguished. The isolation of Africa was broken intermittently by discontinuous \"filter routes\" that linked it to some other Gondwanan continents (Madagascar, South America, and perhaps India), but mainly to Laurasia. Interchanges with Gondwana were rare and mainly \"out-of-Africa\" dispersals, whereas interchanges with Laurasia were numerous and bidirectional, although mainly from Laurasia to Africa. Despite these connections, isolation resulted in remarkable absences, poor diversity, and emergence of endemic taxa in Africa. Madagascar separated from continental Africa during the break-up of Gondwanaland early in the Cretaceous, but was probably connected to the mainland again in the Eocene.\n\nThe first Neogene faunal interchange took place in the Middle Miocene (the introduction of Myocricetodontinae, Democricetodontinae, and Dendromurinae). A major terrestrial faunal exchange between North Africa and Europe began at about 6.1 Ma, some 0.4 Myr before the beginning of the Messinian salinity crisis(for example introduction of Murinae, immigrants from southern Asia)\n\nDuring the early Tertiary, Africa was covered by a vast evergreen forest inhabited by an endemic forest fauna with many types common to southern Asia. In the Pliocene the climate became dry and most of the forest was destroyed, the forest animals taking refuge in the remaining forest islands. At the same time a broad land-bridge connected Africa with Asia and there was a great invasion of animals of the steppe fauna into Africa. At the beginning of the Pleistocene a moist period set in and much of the forest was renewed while the grassland fauna was divided and isolated, as the forest fauna had previously been. The present forest fauna is therefore of double origin, partly descended of the endemic fauna and partly from steppe forms that adapted themselves to forest life, while the present savanna fauna is similarly explained. The isolation in past times has resulted in the presence of closely related subspecies in widely separated regions Africa, where humans originated, shows much less evidence of loss in the Pleistocene megafaunal extinction, perhaps because co-evolution of large animals alongside early humans provided enough time for them to develop effective defenses. Its situation in the tropics spared it also from Pleistocene glaciations and the climate has not changed much.\n\nThere are large gaps in human knowledge about African invertebrates. East Africa has a rich coral fauna with about 400 known species. More than 400 species of Echinoderms and 500 species of Bryozoa live there too, as well as one Cubozoan species (\"Carybdea alata\"). Of Nematodes, the \"Onchocerca volvulus\", \"Necator americanus\", \"Wuchereria bancrofti\" and \"Dracunculus medinensis\" are human parasites. Some of important plant-parasitic nematodes of crops include \"Meloidogyne\", \"Pratylenchus\", \"Hirschmanniella\", \"Radopholus\", \"Scutellonema\" and \"Helicotylenchus\". Of the few Onychophorans, \"Peripatopsis\" and \"Opisthopatus\" live in Africa. Greatest diversity of freshwater mollusks is found in East African lakes. Of marine snails, less diversity is present in Atlantic coast, more in tropical Western Indian Ocean region (over 3000 species of gastropods with 81 endemic species). Cowry shells have been used as a money by native Africans. The land snail fauna is especially rich in Afromontane regions, and there are some endemic families in Africa (e.g. Achatinidae, Chlamydephoridae) but other tropical families are common too (Charopidae, Streptaxidae, Cyclophoridae, Subulinidae, Rhytididae).\n156 tardigrade species have been found, and about 8000 species of arachnids. The African millipede \"Archispirostreptus gigas\" is one of the largest in the world. 20 genera of freshwater crabs are present.\n\nThe soil animal communities tropical Africa are poorly known. A few ecological studies have been undertaken on macrofauna, mainly in West Africa. Earthworms are being extensively studied in West and South Africa.\n\nApproximately 100,000 species of insects have been described from sub-Saharan Africa, but there are very few overviews of the fauna as a whole (it has been estimated that the African insects make up about 10-20% of the global insect species richness, and about 15% of new species descriptions come from Afrotropics). The only endemic African insect order is Mantophasmatodea.\n\nAbout 875 African species of dragonflies have been recorded.\n\nThe migratory locust and desert locust have been serious threats to African economies and human welfare.\n\nAfrica has the biggest number of termite genera of all continents, and over 1,000 termite species.\n\nOf Diptera, the number of described African species is about 17,000. Natalimyzidae, a new family of acalyptrate flies has been recently described from South Africa. \"Anopheles gambiae\", \"Aedes aegypti\" and Tsetse fly are important vectors of diseases.\n1600 species of bees and 2000 species of ants among other Hymenopterans are known from Africa.\n\nThere live also 3,607 species of butterflies, being the best known group of insects. The caterpillars of mopani moth are part of the South African cuisine. Among the numerous species of African beetles are the famous sacred scarab, the centaurus beetle, the manticora tiger beetle and enormous Goliath beetles.\n\nHotspots for butterflies include the Congolian forests and the Guinean forest-savanna mosaic. Some butterflies (\"Hamanumida daedalus\", \"Precis\", \"Eurema\") are grassland or savannah specialists. Many of these have very large populations and a vast range. South Africa has one of the highest proportions of Lycaenid butterflies (48%) for any region in the world with many species restricted in range. North Africa is in the Palaearctic region and has a different species assemblage.\n\nGenera which are species rich in Africa include \"Charaxes\", \"Acraea\", \"Colotis\" and \"Papilio\", most notably \"Papilio antimachus\" and \"Papilio zalmoxis\". The subfamily Lipteninae is endemic to the Afrotropics and includes species rich genera such as \"Ornipholidotos\", \"Liptenara\", \"Pentila\", \"Baliochila\", \"Hypophytala\", \"Teriomima\", \"Deloneura\" and \"Mimacraea\". The Miletinae are mostly African, notably \"Lachnocnema\". Endemic Nymphalidae include \"Euphaedra\", \"Bebearia\", \"Heteropsis\", \"Precis\", \"Pseudacraea\", \"Bicyclus\" and \"Euxanthe\". Endemic Pieridae include \"Pseudopontia paradoxa\" and \"Mylothris\". Endemic skippers include \"Sarangesa\"and \"Kedestes\". The highest species diversity is in the Democratic Republic of the Congo, home to 2,040 species 181 of which are endemic.\n\nAfrica is the richest continent of freshwater fish, with about 3000 species. The East African Great Lakes (Victoria, Malawi, and Tanganyika) are the center of biodiversity of many fish, especially cichlids (they harbor more than two-thirds of the estimated 2000 species in the family). The West African coastal rivers region covers only a fraction of West Africa, but harbours 322 of West African’s fish species, with 247 restricted to this area and 129 restricted even to smaller ranges. The central rivers fauna comprises 194 fish species, with 119 endemics and only 33 restricted to small areas. The marine diversity is greatest near the Indian Ocean shore with about 2000 species.\n\nCharacteristic to African fauna are Perciformes (\"Lates\", tilapias, Dichistiidae, Anabantidae, Mudskippers, \"Parachanna\", \"Acentrogobius\", \"Croilia\", \"Glossogobius\", \"Hemichromis\", \"Nanochromis\", \"Oligolepis\", \"Oreochromis\", \"Redigobius\", \"Sarotherodon\", \"Stenogobius\" and others), Gonorhynchiformes (Kneriidae, Phractolaemidae), some lungfishes (\"Protopterus\"), many Characiformes (Distichodontidae, Hepsetidae, Citharinidae, Alestiidae), Osteoglossiformes (African knifefish, Gymnarchidae, Mormyridae, Pantodontidae), Siluriformes (Amphiliidae, Anchariidae, Ariidae, Austroglanididae, Clariidae, Claroteidae, Malapteruridae, Mochokidae, Schilbeidae), Osmeriformes (Galaxiidae), Cyprinodontiformes (Aplocheilidae, Poeciliidae) and Cypriniformes (\"Labeobarbus\", \"Pseudobarbus\", \"Tanakia\" and others).\n\nEndemic to Africa are the families Arthroleptidae, Astylosternidae, Heleophrynidae, Hemisotidae, Hyperoliidae, Petropedetidae, Mantellidae. Also widespread are Bufonidae (\"Bufo\", \"Churamiti\", \"Capensibufo\", \"Mertensophryne\", \"Nectophryne\", \"Nectophrynoides\", \"Schismaderma\", \"Stephopaedes\", \"Werneria\", \"Wolterstorffina\"), Microhylidae (\"Breviceps\", \"Callulina\", \"Probreviceps\", Cophylinae, \"Dyscophus\", Melanobatrachinae, Scaphiophryninae), Rhacophoridae (\"Chiromantis\"), Ranidae (\"Afrana\", \"Amietia\", \"Amnirana\", \"Aubria\", \"Conraua\", \"Hildebrandtia\", \"Lanzarana\", \"Ptychadena\", \"Strongylopus\", \"Tomopterna\") and Pipidae (\"Hymenochirus\", \"Pseudhymenochirus\", \"Xenopus\").\nThe 2002–2004 ‘Global Amphibian Assessment’ by IUCN, Conservation International and NatureServe revealed that for only about 50% of the Afrotropical amphibians, there is least concern about their conservation status; approximately 130 species are endangered, about one-fourth of which are at a critical stage. Almost all of the amphibians of Madagascar (238 species) are endemic to that region. The West African goliath frog is the largest frog species in the world.\n\nThe center of chameleon diversity is Madagascar. Snakes found in Africa include atractaspidids, elapids (cobras, \"Aspidelaps\", \"Boulengerina\", \"Dendroaspis\", \"Elapsoidea\", \"Hemachatus\", \"Homoroselaps\" and \"Paranaja\"), viperines, \"Atheris\", \"Bitis\", \"Cerastes\", \"Causus\", \"Echis\", \"Macrovipera\", \"Montatheris\", \"Proatheris\", \"Vipera\"), colubrids (\"Dendrolycus\", \"Dispholidus\", \"Gonionotophis\", \"Grayia\", \"Hormonotus\", \"Lamprophis\", \"Psammophis\", \"Leioheterodon\", \"Madagascarophis\", \"Poecilopholis\", \"Dasypeltis\" etc.), the pythonids (\"Python\"), typhlopids (\"Typhlops\") and leptotyphlopids (\"Leptotyphlops\", \"Rhinoleptus\").\n\nOf the lizards, many species of geckos (day geckos, \"Afroedura\", \"Afrogecko\", \"Colopus\", \"Pachydactylus\", \"Hemidactylus\", \"Narudasia\", \"Paroedura\", \"Pristurus\", \"Quedenfeldtia\", \"Rhoptropus\", \"Tropiocolotes\", \"Uroplatus\"), Cordylidae, as well as Lacertidae (\"Nucras\", \"Lacerta\", \"Mesalina\", \"Acanthodactylus\", \"Pedioplanis\"), Agamas, skinks, plated lizards and some monitor lizards are common. There are 12 genera and 58 species of African amphisbaenians (e.g. \"Chirindia\", \"Zygaspis\", \"Monopeltis\", \"Dalophia\").\n\nSeveral genera of tortoises (\"Kinixys\", \"Pelusios\", \"Psammobates\", \"Geochelone\", \"Homopus\", \"Chersina\"), turtles (Pelomedusidae, \"Cyclanorbis\", \"Cycloderma\", \"Erymnochelys\"), and three species of crocodiles (the Nile crocodile, slender-snouted crocodile and dwarf crocodile) are also present.\n\n There live (temporarily or permanently) more than 2600 bird species in Africa (about 1500 of them passerines). Some 114 of them are threatened species.\nThe Afrotropic has various endemic bird families, including ostriches (Struthionidae), mesites, sunbirds, secretary bird (Sagittariidae), guineafowl (Numididae), and mousebirds (Coliidae). Also, several families of passerines are limited to the Afrotropics. These include rock-jumpers (Chaetopidae), bushshrikes (Malaconotidae), wattle-eyes, (Platysteiridae) and rockfowl (Picathartidae). Other common birds include parrots (lovebirds, \"Poicephalus\", \"Psittacus\"), various cranes (crowned canes, blue crane, wattled crane), storks (slaty egret, black heron, marabous, Abdim's stork, shoebill), bustards (kori bustard, \"Neotis\", \"Eupodotis\", \"Lissotis\"), sandgrouse (\"Pterocles\"), Coraciiformes (bee-eaters, hornbills, \"Ceratogymna\"), phasianids (francolins, Congo peafowl, blue quail, harlequin quail, stone partridge, Madagascar partridge). The woodpeckers and allies include honeyguides, African barbets, African piculet, ground woodpecker, \"Dendropicos\" and \"Campethera\". The birds of prey include the buzzards, harriers, Old World vultures, bateleur, \"Circaetus\", \"Melierax\" and others. Trogons are represented by one genus (\"Apaloderma\"). African penguin is the only penguin species. Madagascar was once home to the now extinct elephant birds.\n\nAfrica is home to numerous songbirds (pipits, orioles, antpeckers, brubrus, cisticolas, negrofinches, olivebacks, pytilias, green-backed twinspot, crimson-wings, seedcrackers, bluebills, firefinches, waxbills, amandavas, quailfinches, munias, weavers, tit-hylia, \"Amadina\", \"Anthoscopus\", \"Mirafra\", \"Hypargos\", \"Eremomela\", \"Euschistospiza\", \"Erythrocercus\", \"Malimbus\", \"Pitta\", \"Uraeginthus\", pied crow, white-necked raven, thick-billed raven, Cape crow and others). The red-billed quelea is the most abundant bird species in the world.\n\nOf the 589 species of birds (excluding seabirds) that breed in the Palaearctic (temperate Europe and Asia), 40% spend the winter elsewhere. Of those species that leave for the winter, 98% travel south to Africa.\nSee also: Endemic birds of western and central Africa, Endemic birds of southern Africa.\n\nMore than 1100 mammal species live in Africa.\nAfrica has three endemic orders of mammals, the Tubulidentata (aardvarks), Afrosoricida (tenrecs and golden moles), and Macroscelidea (elephant shrews). The current research of mammalian phylogeny has proposed an Afrotheria clade (including the exclusively African orders). The East-African plains are well known for their diversity of large mammals.\n\nAfrican Soricomorpha include the Myosoricinae and Crocidurinae subfamilies. Hedgehogs include desert hedgehogs, \"Atelerix\" and others. The rodents are represented by African bush squirrels, African ground squirrels, African striped squirrels, gerbils, cane rats, acacia rats, Nesomyidae, springhare, mole rats, dassie rats, striped grass mice, sun squirrels, thicket rats, Old World porcupines, target rats, maned rats, Deomyinae, \"Aethomys\", \"Arvicanthis\", \"Colomys\", \"Dasymys\", \"Dephomys\", \"Epixerus\", \"Grammomys\", \"Graphiurus\", \"Hybomys\", \"Hylomyscus\", \"Malacomys\", \"Mastomys\", \"Mus\", \"Mylomys\", \"Myomyscus\", \"Oenomys\", \"Otomys\", \"Parotomys\", \"Pelomys\", \"Praomys\", \"Rhabdomys\", \"Stenocephalemys\" and many others. African rabbits and hares include riverine rabbit, Bunyoro rabbit, Cape hare, scrub hare, Ethiopian highland hare, African savanna hare, Abyssinian hare and several species of \"Pronolagus\".\nAmong the marine mammals there are several species of dolphins, 2 species of sirenians and seals (e.g. Cape fur seals). Of the carnivorans there are 60 species, including the conspicuous hyenas, lions, leopards, cheetahs, serval, as well as the less prominent bat-eared fox, striped polecat, African striped weasel, caracal, honey badger, speckle-throated otter, several mongooses, jackals and civets. The family Eupleridae is restricted to Madagascar.\n\nThe African list of ungulates is longer than in any other continent. The largest number of modern bovids is found in Africa (African buffalo, duikers, impala, rhebok, Reduncinae, oryx, dik-dik, klipspringer, oribi, gerenuk, true gazelles, hartebeest, wildebeest, dibatag, eland, \"Tragelaphus\", \"Hippotragus\", \"Neotragus\", \"Raphicerus\", \"Damaliscus\"). Other even-toed ungulates include giraffes, hippopotamuses, warthogs, giant forest hogs, red river hogs and bushpigs. Odd-toed ungulates are represented by three species of zebras, African wild ass, black and white rhinoceros. The biggest African mammal is the African bush elephant, the second largest being its smaller counterpart, the African forest elephant. Four species of pangolins can be found in Africa.\n\nAfrican fauna contains 64 species of primates. Four species of great apes (Hominidae) are endemic to Africa: both species of gorilla (western gorilla, \"Gorilla gorilla\", and eastern gorilla, \"Gorilla beringei\") and both species of chimpanzee (common chimpanzee, \"Pan troglodytes\", and bonobo, \"Pan paniscus\"). Humans and their ancestors originated in Africa. Other primates include colobuses, baboons, geladas, vervet monkeys, guenons, macaques, mandrills, crested mangabeys, white-eyelid mangabeys, kipunji, Allen's swamp monkeys, Patas monkeys and talapoins. Lemurs and aye-aye are characteristic of Madagascar. See also Lists of mammals of Africa.\n\n\n"}
{"id": "24261417", "url": "https://en.wikipedia.org/wiki?curid=24261417", "title": "Ferroelectric polymers", "text": "Ferroelectric polymers\n\nFerroelectric polymers\nare a group of crystalline polar polymers that are also ferroelectric, meaning that they maintain a permanent electric polarization that can be reversed, or switched, in an external electric field. \nFerroelectric polymers, such as polyvinylidene fluoride (PVDF), are used in acoustic transducers and electromechanical actuators because of their inherent piezoelectric response, and as heat sensors because of their inherent pyroelectric response. \n\nFirst reported in 1971, ferroelectric polymers are polymer chains that must exhibit ferroelectric behavior, hence piezoelectric and pyroelectric behavior.\n\nA ferroelectric polymer must contain permanent electrical polarization that can be reversed repeatedly, by an opposing electric field. In the polymer, dipoles can be randomly oriented, but application of an electric field will align the dipoles, leading to ferroelectric behavior. In order for this effect to happen, the material must be below its Curie Temperature. Above the Curie Temperature, the polymer exhibits paraelectric behavior, which does not allow for ferroelectric behavior because the electric fields do not align. \n\nA consequence of ferroelectric behavior leads to piezoelectric behavior, where the polymer will generate an electric field when stress is applied, or change shape upon application of an electric field. This is viewed as shrinking, or changes in conformation of the polymer in an electric field; or by stretching and compressing the polymer, measure generated electric fields. Pyroelectric behavior stems from the change in temperature causing electric behavior of the material. While only ferroelectric behavior is required for a ferroelectric polymer, current ferroelectric polymers exhibit pyroelectric and piezoelectric behavior.\n\nIn order to have an electric polarization that can be reversed, ferroelectric polymers are often crystalline, much like other ferroelectric materials. Ferroelectric properties are derived from electrets, which are defined as a dielectric body that polarizes when an electric field and heat is applied. Ferroelectric polymers differ in that the entire body undergoes polarization, and the requirement of heat is not necessary. Although they differ from electrets, they are referred to as electrets often. Ferroelectric polymers fall into a category of ferroelectric materials known as a 'order-disorder' material. This material undergoes a change from randomly oriented dipoles which are paraelectric, to ordered dipoles which become ferroelectric.\n\nAfter the discovery of PVDF, many other polymers have been sought after that contain ferroelectric, piezoelectric, and pyroelectric properties. Initially different blends and copolymers of PVDF were discovered, such as a polyvinylidene fluoride with poly(methyl methacrylate).\n\nOther structures were discovered to possess ferroelectric properties, such as polytrifluoroethylene\nand odd-numbered nylon.\n\nThe concept of ferroelectricity was first discovered in 1921. This phenomenon began to play a much larger role in electronic applications during the 1950s after the increased use of BaTiO. This ferroelectric material is part of the corner-sharing oxygen octahedral structure, but ferroelectrics can also be grouped into three other categories. These categories include organic polymers, ceramic polymer composites, and compounds containing hydrogen-bonded radicals. It wasn't until 1969 that Kawai first observed the piezoelectric effect in a polymer polyvinylidene fluoride. Two years later, the ferroelectric properties of the same polymer were reported. Throughout the 1970s and 1980s, these polymers were applied to data storage and retrieval. Subsequently, there has been tremendous growth during the past decade in exploring the materials science, physics, and technology of poly(vinylidenefluoride) and other fluorinated polymers. Copolymer PVDF with trifluoroethylene and odd-numbered nylons were additional polymers that were discovered to be ferroelectric. This propelled a number of developing applications on piezoelectricity and pyroelectricity.\n\nThe easiest way of synthesizing PVDF is the radical polymerization of vinylidene fluoride (VF), however, the polymerization is not completely regiospecific. The asymmetric structure of VF leads to the orientation isomers during the polymerization. The configuration of the monomer in the chain can be either \"head to head\" or \"head to tail\".\n\nTo get more control on the regiospecific polymer synthesis, copolymerization was proposed. One of these methods is introducing the precursor polymer made from copolymerization of VF with either 1-chloro-2,2-difluoroethylene (CVF) or 1-bromo-2,2-difluoroethylene( BVF). The chlorinated or brominated monomers are attacked at their CF carbon by growing –CHCF∙ radical. After reductive dechlorination or debromination with tri-n-butyltin hydride they become a reversed VF unit in the final polymer.Therefore, a regioisomer of PVDF is formed.\n\nTo minimize the potential energy of the chains arising from internal steric and electrostatic interactions, the rotation about single bonds happens in the chain of PVDF. There are two most favorable torsional bond arrangements: trans ( t ) and gauche ( g ). In the case of “ t”, the substituents are at 180° to each other. In the case of “g”, the substituents are at ±60° to each other. PVDF molecules contain two hydrogen and two fluorine atoms per repeat unit, so they have a choice of multiple conformations. However, rotational barriers are relatively high, the chains can be stabilized into favorable conformations other than that of lowest energy. The three known conformations of PVDF are all-trans, tgtg, and . The first two conformations are the most common ones and are sketched out in the figure on right. In the tgtg conformation, the inclination of dipoles to the chain axis leads to the polar components of both perpendicular (4.0 × 10 C-m per repeat) and parallel to the chain (3.4 × 10 C-m per repeat). In the all trans structure, the alignment of all its dipoles are in the same direction normal to the chain axis. In this way, it can be expected that the all trans is the most highly polar conformation in PVDF (7.× 10 C-m per repeat). These polar conformations are the crucial factors that lead to the ferroelectric properties.\n\nFerroelectric polymers and other materials have been incorporated into many applications, but there is still cutting edge research that is currently being done. For example, there is research being conducted on novel ferroelectric polymer composites with high dielectric constants. Ferroelectric polymers, such as polyvinylidene fluoride and poly[(vinylidenefluoride-co-trifluoroethylene] [P(VDF-TrFE)], are very attractive for many applications because they exhibit good piezoelectric and pyroelectric responses and low acoustic impedance, which matches water and human skin. More importantly, they can be tailored to meet various requirements. A common approach for enhancing the dielectric constant is to disperse a high-dielectric-constant ceramic powder into the polymers. Popular ceramic powders are lead based complexes such as PbTiO and Pb(Zr,Ti)O. This can be disadvantageous because lead can be potentially harmful and at high particulate loading, the polymers lose their flexibility and a low quality composite is obtained. Current advances use a blending procedure to make composites that are based on the simple combination of PVDF and cheap metal powders. Specifically, Ni powders were used to make up the composites. The dielectric constants were enhanced from values there were less than 10 to approximately 400. This large enhancement is explained by the percolation theory.\n\nThese ferroelectric materials have also been used as sensors. More specifically, these types of polymers have been used for high pressure and shock compression sensors. It has been discovered that ferroelectric polymers exhibit piezoluminescence upon the application of stress. Piezoluminescence has been looked for in materials that are piezoelectric.\n\nThese types of polymers have played a role in biomedical and robotic applications and liquid crystalline polymers. In 1974, R.B. Meyer predicted ferroelectricity in chiral smectic liquid crystals by pure symmetry conditions. Shortly after, Clark and Lagerwall had done work on the fast electrooptic effect in a surface-stabilized ferroelectric liquid crystal (SSFLC) structure. This opened up promising possibility of technical applications of ferroelectric liquid crystals in high-information display devices. Through applied research, it was shown that SSFLC structure has faster switching times and bistability behavior in comparison with commonly used nematic liquid crystal displays. In the same time period, the first side-chain liquid crystalline polymers (SCLCP) were synthesized. These comb-like polymers has mesogenic side chains that are covalently bonded (via flexible spacer units) to the polymer backbone. The most important feature of the SCLCP's is their glassy state. In other words, these polymers have a \"frozen\" ordered state along one axis when cooled below their glass transition temperature. This is advantageous for research in the area of nonlinear optical and optical data storage devices. The disadvantage is that these SCLCP's suffered from their slow switching times due to their high rotational viscosity.\n\nThe ferroelectric property exhibits polarization–electric-field-hysteresis loop, which is related to \"memory\". One application is integrating ferroelectric polymer Langmuir–Blodgett (LB) films with semiconductor technology to produce nonvolatile ferroelectric random-access memory(NV-FRAM or NV-FeRAM) and data-storage devices. Recent research with LB films and more conventional solvent formed films shows that the VDF copolymers(consisting of 70% vinylidene fluoride (VDF) and 30% trifluoroethylene (TrFE)) are promising materials for nonvolatile memory applications. The device is built in the form of the metal–ferroelectric–insulator–semiconductor (MFIS) capacitance memory. The results demonstrated that LB films can provide devices with low-voltage operation.\n\nThin Film Electronics successfully demonstrated roll-to-roll printed non-volatile memories based on ferroelectric polymers in 2009.\n\nThe ferroelectric effect always relates the various force to electric properties, which can be applied in transducers. The flexibility and low cost of polymers facilitates the application of ferroelectric polymers in transducers. The device configuration is simple, it usually consists of a piece of ferroelectric film with an electrode on the top\nand bottom surfaces. Contacts to the two electrodes complete the design.\n\nWhen the device functions as a sensor, a mechanical or acoustic force applied to one of the surfaces causes a compression of the material. Via the direct piezoelectric effect, a voltage is generated between the electrodes.\n\nIn actuators, a voltage applied between the electrodes causes a strain on the film through the inverse piezoelectric effect.\n\nSoft transducers in the form of ferroelectric polymer foams have been proved to have great potential.\n\n"}
{"id": "29609127", "url": "https://en.wikipedia.org/wiki?curid=29609127", "title": "Forged composite", "text": "Forged composite\n\nForged composite is a technology that uses carbon fiber composite material and was developed jointly between Lamborghini and Callaway Golf Company who introduced it in their Sesto Elemento concept car and Diablo Octane drivers, respectively. The application of this technology was originally developed at the Automobili Lamborghini Advanced Composite Structures Laboratory (ACSL) in Seattle, WA, where the Lamborghini-Callaway partnership was initially formed. This partnership, along with forged composite technology, was announced at the 2010 Paris Motor Show when Lamborghini unveiled its concept car. The United States trademark for forged composite was filed on July 13, 2010, in the category Toys and Sporting Goods Products. \n\nForged Composite is not a specific material, but an encompassing technology which combines the material, preform definition, molding and curing processes, and specific design features to create a forged composite carbon fiber part. The material which is used in Forged Composite technology is an evolution of carbon fiber sheet molding compounds (CFSMC) with improved fibers, higher fiber volume content, and an improved molding process; all of which increase the average strength values and reduce variability over standard carbon fiber SMC. The material is one-third as dense as metal titanium but stronger, and consists of chopped carbon fiber tows that are sandwiched between two layers of filmed resin. It uses about 500,000 intertwined turbostratic fibers per square inch. The structure of the high strength carbon fibers is described as turbostratic, that is the fibers contain intertwined and folded sheets of carbon atoms aligned with the length of the fiber, and the intertwining improved the fiber strength. The result, which is superior to predecessor alloys used in the automotive and golf industries, is significantly improved load carrying capacity as measured in bending per unit of mass. Due to its chopped nature, it can be molded into much more complex geometries than traditional carbon fiber composites, and is suitable to make three-dimensional parts and parts which feature complex details such as thickness transitions, holes, compound curvature, etc. Chopped carbon fiber materials produced by Quantum Composites Inc. of Bay City, MI, as well as Mitsubishi Rayon of Toyohashi, Japan, can be used for Forged Composite technology.\n\nLamborghini originally used Forged Composite technology on both the inner monocoque and the wishbone suspension arms of the Sesto Elemento. Since then, Lamborghini has used Forged Composite for the interior trim and seats of the 2012 Aventador J, 2012 Urus concept, 2013 Veneno, and the 2014 Veneno Roadster. The 2015 Huracan features a Forged Composite engine bay cover and optional interior trim package which won the JEC Composites Innovation Award for Automotive Interiors in 2016. In June 2016, the world's first carbon fiber connecting rods, made using Forged Composite technology, were unveiled at the Lamborghini ACSL facility in Seattle. Union Binding Company entered a partnership with the Lamborghini ACSL in order to develop a snowboard binding using Forged Composite technology. In 2014, the all-Forged Composite \"Union FC\" snowboard binding was launched and subsequently won the 2014 ISPO Product of the Year Award.\n\nThis technology marks the first collaborative product of the ongoing Callaway and Lamborghini alliance to combine research and development efforts towards their mutual goal of power-to-weight ratio and weight reduction. The effect of the improvement is that the Callaway Diablo Octane drivers average about greater distance per drive than their predecessor Diablo Edge drivers and the Sesto Elemento, with its Forged Composite chassis, is one-third lighter than the Lamborghini Superleggera and goes from zero to almost a full second faster.\n"}
{"id": "29735940", "url": "https://en.wikipedia.org/wiki?curid=29735940", "title": "Gdańsk Power Station", "text": "Gdańsk Power Station\n\nGdańsk Power Station () is a combined heat and power station in Gdańsk, Poland. It is operated by PGE Energia Ciepła Oddział Wybrzeże, a subsidiary of PGE.\n\nThe power station has 5 power generation units and 2 boilers without electricity generation. The parameters of these installations are:\n\nGdansk Power Station has three flue gas stacks: one with a height of , which is the tallest structure in Gdańsk, and two with a height of .\n"}
{"id": "54011162", "url": "https://en.wikipedia.org/wiki?curid=54011162", "title": "Gloppedalsura", "text": "Gloppedalsura\n\nGloppedalsura, or Gloppura, is a scree in Gloppedalen, Rogaland, Norway, in the Gjesdal and Bjerkreim municipalities. It is one of the largest screes in Scandinavia and Northern Europe.\n\nDuring the invasion of Norway, 250 Norwegian soldiers used the scree as a natural fortress and held back two \nGerman battalions. One Norwegian soldier lost his life in the battle. Losses on the German side are uncertain and varies between 12 and 44 soldiers.\n"}
{"id": "18601487", "url": "https://en.wikipedia.org/wiki?curid=18601487", "title": "Hepburn Wind Project", "text": "Hepburn Wind Project\n\nThe Hepburn Wind Project is a wind farm built and owned by Hepburn Wind, a community co-operative, and supported by the Victorian Government. The location of the project is Leonards Hill, south of Daylesford, Victoria, north-west of Melbourne, Victoria. It comprises two individual 2.05 MW wind turbines supplied by REpower System AG which are projected to produce enough energy for 2,300 households, almost as many houses are in the twin-towns of Daylesford and Hepburn Springs.\n\nThe project is the first community-owned wind farm in Australia. The initiative emerged because the community felt that the state and federal governments were not doing enough to address climate change. Hepburn Wind is the first project of the Hepburn Renewable Energy Association, now known as SHARE.\n\nHepburn Wind Project formally launched their share offer on 25 July 2008, and as of June 2011 over 1700 members had subscribed a total of $9.0 million. The project has secured over $13.1 million in funding with the additional funds being bank debt and Government grants. Shares have been issued with priority to the local residents of Daylesford and Hepburn Springs.\n\nAs the project is under 30 MW, it was assessed under the local planning guidelines administered through the Hepburn Shire Council. The planning permit was issued by Hepburn Shire in February 2007. The council received 325 submissions in support of the proposal and 18 objections.\nThe permit was subsequently challenged unsuccessfully at the Victorian Civil and Administrative Tribunal (VCAT) in June 2007.\n\nOn 28 April 2010 the project signed a contract with REpower Systems for the turnkey construction of the wind farm. The official ground breaking ceremony was held on 8 October 2010 and was officiated by the Hon Gavin Jennings, the Victorian Minister for the Environment at the time.\n\nConstruction began in November 2010. The turbines left Germany on 5 December 2010 on the SE Panthea and arrived in Melbourne on 22 February 2011.\n\nA community picnic day was held to view and celebrate the erection of the first turbine on Saturday, 19 April 2011.\n\nThe wind farm began generating power into the local electricity network on Wednesday, 22 June 2011.\n\nCommunity-owned wind farms are common in Denmark and Germany, which have high levels of wind power use. Community wind projects are also emerging in Canada, the Netherlands, the United Kingdom and the United States.\n\n\n"}
{"id": "10538780", "url": "https://en.wikipedia.org/wiki?curid=10538780", "title": "High-power impulse magnetron sputtering", "text": "High-power impulse magnetron sputtering\n\nHigh-power impulse magnetron sputtering (HIPIMS or HiPIMS, also known as high-power pulsed magnetron sputtering, HPPMS) is a method for physical vapor deposition of thin films which is based on magnetron sputter deposition. HIPIMS utilises extremely high power densities of the order of kW⋅cm in short pulses (impulses) of tens of microseconds at low duty cycle (on/off time ratio) of < 10%. Distinguishing features of HIPIMS are a high degree of ionisation of the sputtered metal and a high rate of molecular gas dissociation which result in high density of deposited films. The ionization and dissociation degree increase according to the peak cathode power. The limit is determined by the transition of the discharge from glow to arc phase. The peak power and the duty cycle are selected so as to maintain an average cathode power similar to conventional sputtering (1–10 W⋅cm).\n\nHIPIMS is used for:\n\nHIPIMS plasma is generated by a glow discharge where the discharge current density can reach several A⋅cm, whilst the discharge voltage is maintained at several hundred volts. The discharge is homogeneously distributed across the surface of the cathode (target) however above a certain threshold of current density it becomes concentrated in narrow ionization zones that move along a path known as the target erosion \"racetrack\".\n\nHIPIMS generates a high density plasma of the order of 10 ions⋅cm containing high fractions of target metal ions. The main ionisation mechanism is electron impact, which is balanced by charge exchange, diffusion, and plasma ejection in flares. The ionisation rates depend on the plasma density.<br>\nThe ionisation degree of the metal vapour is a strong function of the peak current density of the discharge. At high current densities, sputtered ions with charge 2+ and higher – up to 5+ for V – can be generated. The appearance of target ions with charge states higher than 1+ is responsible for a potential secondary electron emission process that has a higher emission coefficient than the kinetic secondary emission found in conventional glow discharges. The establishment of a potential secondary electron emission may enhance the current of the discharge.<br> HIPIMS is typically operated in short pulse (impulse) mode with a low duty cycle in order to avoid overheating of the target and other system components. In every pulse the discharge goes through several stages:\n\nThe negative voltage (bias voltage) applied to the substrate influences the energy and direction of motion of the positively charged particles that hit the substrate. The on-off cycle has a period on the order of milliseconds. Because the duty cycle is small (< 10%), only low average cathode power is the result (1–10 kW). The target can cool down during the “off time”, thereby maintaining process stability.\n\nThe discharge that maintains HIPIMS is a high-current glow discharge, which is \"transient\" or \"quasistationary\". Each pulse remains a glow up to a critical duration after which it transits to an arc discharge. If pulse length is kept below the critical, the discharge operates in a stable fashion indefinitely.\n\nInitial observations by fast camera imaging in 2008 were recorded independently, demonstrated with better precision, and confirmed demonstrating that most ionization processes occur in spatially very limited ionization zones. The drift velocity was measured to be of the order of 10 m/s, which is about only 10% of the electron drift velocity.\n\nSubstrate pretreatment in a plasma environment is required prior to deposition of thin films on mechanical components such as automotive parts, metal cutting tools and decorative fittings. The substrates are immersed in a plasma and biased to a high voltage of a few hundred volts. This causes high energy ion bombardment that sputters away any contamination. In cases when the plasma contains metal ions, they can be implanted into the substrate to a depth of a few nm. HIPIMS is used to generate a plasma with a high density and high proportion of metal ions. When looking at the film-substrate interface in cross-section, one can see a clean interface. Epitaxy or atomic registry is typical between the crystal of a nitride film and the crystal of a metal substrate when HIPIMS is used for pretreatment. HIPIMS has been used for the pretreatment of steel substrates for the first time in February 2001 by A.P. Ehiasarian.\n\nSubstrate biasing during pretreatment uses high voltages, which require purpose-designed arc detection and suppression technology. Dedicated DC substrate biasing units provide the most versatile option as they maximize substrate etch rates, minimise substrate damage, and can operate in systems with multiple cathodes. An alternative is the use of two HIPIMS power supplies synchronised in a master–slave configuration: one to establish the discharge and one to produce a pulsed substrate bias\n\nThin films deposited by HIPIMS at discharge current density > 0.5 A⋅cm have a dense columnar structure with no voids. \nThe deposition of copper films by HIPIMS was reported for the first time by V. Kouznetsov for the application of filling 1 µm vias with aspect ratio of 1:1.2\n\nTransition metal nitride (CrN) thin films were deposited by HIPIMS for the first time in February 2001 by A.P. Ehiasarian. The first thorough investigation of films deposited by HIPIMS by TEM demonstrated a dense microstructure, free of large scale defects. The films had a high hardness, good corrosion resistance and low sliding wear coefficient. The commercialisation of HIPIMS hardware that followed made the technology accessible to the wider scientific community and triggered developments in a number of areas.\n\nThe following materials have, among others, been deposited successfully by HIPIMS:\n\nHIPIMS has been successfully applied for the deposition of thin films in industry. The first HIPIMS coating units appeared on the market in 2006.\n\nThe main advantages of HIPIMS coatings include a denser coating morphology and an increased ratio of hardness to Young’s modulus compared to conventional PVD coatings. Whereas comparable conventional nano-structured (Ti,Al)N coatings have a hardness of 25 GPa and a Young’s modulus of 460 GPa, the hardness of the new HIPIMS coating is higher than 30 GPa with a Young’s modulus of 368 GPa. The ratio between hardness and Young’s modulus is a measure of the toughness properties of the coating. The desirable condition is high hardness with a relatively small Young’s modulus, such as can be found in HIPIMS coatings. Recently, innovative applications of HIPIMS coated surfaces for biomedical applications were reported by Rtimi et al.<ref name=\"Growth of TiO2/Cu films by HiPIMS for accelerated bacterial loss of viability\"></ref>\n\n\n\n"}
{"id": "1929545", "url": "https://en.wikipedia.org/wiki?curid=1929545", "title": "History of superconductivity", "text": "History of superconductivity\n\nSuperconductivity is the phenomenon of certain materials exhibiting zero electrical resistance and the expulsion of magnetic fields below a characteristic temperature. The history of superconductivity began with Dutch physicist Heike Kamerlingh Onnes's discovery of superconductivity in mercury in 1911. Since then, many other superconducting materials have been discovered and the theory of superconductivity has been developed. These subjects remain active areas of study in the field of condensed matter physics.\n\nJames Dewar initiated research into electrical resistance at low temperatures. Dewar and John Ambrose Fleming predicted that at absolute zero, pure metals would become perfect electromagnetic conductors (though, later, Dewar altered his opinion on the disappearance of resistance, believing that there would always be some resistance). Walther Hermann Nernst developed the third law of thermodynamics and stated that absolute zero was unattainable. Carl von Linde and William Hampson, both commercial researchers, nearly at the same time filed for patents on the Joule–Thomson effect for the liquefaction of gases. Linde's patent was the climax of 20 years of systematic investigation of established facts, using a regenerative counterflow method. Hampson's designs was also of a regenerative method. The combined process became known as the Hampson–Linde liquefaction process.\n\nOnnes purchased a Linde machine for his research. On March 21, 1900, Nikola Tesla was granted a US patent for the means for increasing the intensity of electrical oscillations by lowering the temperature, which was caused by lowered resistance. Within this patent it describes the increased intensity and duration of electric oscillations of a low temperature resonating circuit. It is believed that Tesla had intended that Linde's machine would be used to attain the cooling agents.\n\nA milestone was achieved on July 10, 1908 when Heike Kamerlingh Onnes at Leiden University in the Netherlands produced, for the first time, liquified helium, which has a boiling point of 4.2 kelvins at atmospheric pressure.\n\nHeike Kamerlingh Onnes and Jacob Clay reinvestigated Dewar's earlier experiments on the reduction of resistance at low temperatures. Onnes began the investigations with platinum and gold, replacing these later with mercury (a more readily refinable material). Onnes's research into the resistivity of solid mercury at cryogenic temperatures was accomplished by using liquid helium as a refrigerant. On April 8, 1911, 16:00 hours Onnes noted \"Kwik nagenoeg nul\", which translates as \"[Resistance of] mercury almost zero.\" At the temperature of 4.19 K, he observed that the resistivity abruptly disappeared (the measuring device Onnes was using did not indicate any resistance). Onnes disclosed his research in 1911, in a paper titled \"On the Sudden Rate at Which the Resistance of Mercury Disappears.\" Onnes stated in that paper that the \"specific resistance\" became thousands of times less in amount relative to the best conductor at ordinary temperature. Onnes later reversed the process and found that at 4.2 K, the resistance returned to the material. The next year, Onnes published more articles about the phenomenon. Initially, Onnes called the phenomenon \"\"supraconductivity\" (1913) and, only later, adopted the term \"superconductivity.\"\" For his research, he was awarded the Nobel Prize in Physics in 1913.\n\nOnnes conducted an experiment, in 1912, on the usability of superconductivity. Onnes introduced an electric current into a superconductive ring and removed the battery that generated it. Upon measuring the electric current, Onnes found that its intensity did not diminish with the time. The current persisted due to the superconductive state of the conductive medium. In subsequent decades, superconductivity was found in several other materials. In 1913, lead was found to superconduct at 7 K, and in 1941 niobium nitride was found to superconduct at 16 K.\n\nThe next important step in understanding superconductivity occurred in 1933, when Walther Meissner and Robert Ochsenfeld discovered that superconductors expelled applied magnetic fields, a phenomenon that has come to be known as the Meissner effect. In 1935, brothers Fritz London and Heinz London showed that the Meissner effect was a consequence of the minimization of the electromagnetic free energy carried by superconducting current. In 1950, the phenomenological Ginzburg-Landau theory of superconductivity was devised by Lev Landau and Vitaly Ginzburg.\n\nThe Ginzburg-Landau theory, which combined Landau's theory of second-order phase transitions with a Schrödinger-like wave equation, had great success in explaining the macroscopic properties of superconductors. In particular, Alexei Abrikosov showed that Ginzburg-Landau theory predicts the division of superconductors into the two categories now referred to as Type I and Type II. Abrikosov and Ginzburg were awarded the 2003 Nobel Prize in Physics for their work (Landau having died in 1968). Also in 1950, Emanuel Maxwell and, almost simultaneously, C.A. Reynolds \"et al.\" found that the critical temperature of a superconductor depends on the isotopic mass of the constituent element. This important discovery pointed to the electron-phonon interaction as the microscopic mechanism responsible for superconductivity.\n\nThe complete microscopic theory of superconductivity was finally proposed in 1957 by John Bardeen, Leon N. Cooper, and Robert Schrieffer. This BCS theory explained the superconducting current as a superfluid of Cooper pairs, pairs of electrons interacting through the exchange of phonons. For this work, the authors were awarded the Nobel Prize in Physics in 1972. The BCS theory was set on a firmer footing in 1958, when Nikolai Nikolaevich Bogolyubov showed that the BCS wavefunction, which had originally been derived from a variational argument, could be obtained using a canonical transformation of the electronic Hamiltonian. In 1959, Lev Gor'kov showed that the BCS theory reduced to the Ginzburg-Landau theory close to the critical temperature. Gor'kov was the first to derive the superconducting phase evolution equation formula_1.\n\nThe Little-Parks effect was discovered in 1962 in experiments with empty and thin-walled superconducting cylinders subjected to a parallel magnetic field. The electrical resistance of such cylinders shows a periodic oscillation with the magnetic flux through the cylinder, the period being \"h\"/2\"e\" = 2.07×10 V·s. The explanation provided by William Little and Ronald Parks is that the resistance oscillation reflects a more fundamental phenomenon, i.e. periodic oscillation of the superconducting critical temperature (\"T\"). This is the temperature at which the sample becomes superconducting. The Little-Parks effect is a result of collective quantum behavior of superconducting electrons. It reflects the general fact that it is the fluxoid rather than the flux which is quantized in superconductors. The Little-Parks effect demonstrates that the vector potential couples to an observable physical quantity, namely the superconducting critical temperature.\n\nSoon after discovering superconductivity in 1911, Kamerlingh Onnes attempted to make an electromagnet with superconducting windings but found that relatively low magnetic fields destroyed superconductivity in the materials he investigated. Much later, in 1955, George Yntema succeeded in constructing a small 0.7-tesla iron-core electromagnet with superconducting niobium wire windings. Then, in 1961, J. E. Kunzler, E. Buehler, F. S. L. Hsu, and J. H. Wernick made the startling discovery that at 4.2 kelvins, a compound consisting of three parts niobium and one part tin was capable of supporting a current density of more than 100,000 amperes per square centimeter in a magnetic field of 8.8 tesla. Despite being brittle and difficult to fabricate, niobium-tin has since proved extremely useful in supermagnets generating magnetic fields as high as 20 teslas. In 1962, Ted Berlincourt and Richard Hake discovered that alloys of niobium and titanium are suitable for applications up to 10 teslas. Promptly thereafter, commercial production of niobium-titanium supermagnet wire commenced at Westinghouse Electric Corporation and at Wah Chang Corporation. Although niobium-titanium boasts less-impressive superconductng properties than those of niobium-tin, niobium-titanium has, nevertheless, become the most widely used “workhorse” supermagnet material, in large measure a consequence of its very high ductility and ease of fabrication. However, both niobium-tin and niobium-titanium find wide application in MRI medical imagers, bending and focusing magnets for enormous high-energy particle accelerators, and a host of other applications. Conectus, a European consortium for superconductivity, estimated that in 2014, global economic activity, for which superconductivity was indispensable, amounted to about five billion euros, with MRI systems accounting for about 80% of that total.\n\nIn 1962, Brian Josephson made the important theoretical prediction that a supercurrent can flow between two pieces of superconductor separated by a thin layer of insulator. This phenomenon, now called the Josephson effect, is exploited by superconducting devices such as SQUIDs. It is used in the most accurate available measurements of the magnetic flux quantum \"h\"/2\"e\", and thus (coupled with the quantum Hall resistivity) for Planck's constant \"h\". Josephson was awarded the Nobel Prize in Physics for this work in 1973.\n\nIn 1973 found to have \"T\" of 23 K, which remained the highest ambient-pressure \"T\" until the discovery of the cuprate high-temperature superconductors in 1986 (see below).\n\nIn 1986, J. Georg Bednorz and K. Alex Mueller discovered superconductivity in a lanthanum-based cuprate perovskite material, which had a transition temperature of 35 K (Nobel Prize in Physics, 1987) and was the first of the high-temperature superconductors. It was shortly found (by Ching-Wu Chu) that replacing the lanthanum with yttrium, i.e. making YBCO, raised the critical temperature to 92 K, which was important because liquid nitrogen could then be used as a refrigerant (at atmospheric pressure, the boiling point of nitrogen is 77 K). This is important commercially because liquid nitrogen can be produced cheaply on-site with no raw materials, and is not prone to some of the problems (solid air plugs, etc.) of helium in piping. Many other cuprate superconductors have since been discovered, and the theory of superconductivity in these materials is one of the major outstanding challenges of theoretical condensed-matter physics.\n\nIn March 2001, superconductivity of magnesium diboride () was found with \"T\" = 39 K.\n\nIn 2008, the oxypnictide or iron-based superconductors were discovered, which led to a flurry of work in the hope that studying them would provide a theory of the cuprate superconductors.\n\nIn 2013, room-temperature superconductivity was attained in YBCO for picoseconds, using short pulses of infrared laser light to deform the material's crystal structure.\n\nIn 2017 it was suggested that undiscovered superhard materials (e.g. critically doped beta-titanium Au) might be a candidate for a new superconductor with Tc substantially higher than HgBaCuO (138K) possibly up to 233K which would be higher even than HS.\nA lot of research suggests that additionally nickel could replace copper in some perovskites, offering another route to room temperature.\nLi+ doped materials can also be used i.e. the spinel battery material LiTi2Ox and the lattice pressure can increase Tc to over 13.8K\nAlso LiHx has been theorized to metallise at a substantially lower pressure than H and could be a candidate for a Type 1 superconductor.\n\nPapers by H.K. Onnes\n\n\nBCS theory\n\nOther key papers\n\nPatents\n\n\n"}
{"id": "11708867", "url": "https://en.wikipedia.org/wiki?curid=11708867", "title": "List of photovoltaic power stations", "text": "List of photovoltaic power stations\n\nThe following is a list of photovoltaic power stations that are larger than 200 megawatts (MW) in current net capacity. Most are individual photovoltaic power stations, but some are groups of co-located plants owned by different independent power producers and with separate transformer connections to the grid. Wiki-Solar reports total global capacity of utility-scale photovoltaic plants to be some 96 GW which generated 1.3% of global power by the end of 2016.\n\nThe size of photovoltaic power stations has increased progressively over the last decade with frequent new capacity records. The 97 MW Sarnia Photovoltaic Power Plant went online in 2010. Huanghe Hydropower Golmud Solar Park reached 200 MW in 2012. In August 2012, Agua Caliente Solar Project in Arizona reached 247 MW only to be passed by three larger plants in 2013. In 2014, two plants were tied as largest: Topaz Solar Farm, a PV solar plant at 550 MW in central coast area and a second 550-MW plant, the Desert Sunlight Solar Farm located in the far eastern desert region of California. These two plants were superseded by a new world's largest facility in June 2015 when the 579 MW Solar Star project went online in the Antelope Valley region of Los Angeles County, California. In 2016, the largest photovoltaic power station in the world was the 850 MW Longyangxia Dam Solar Park, in Gonghe County, Qinghai, China. Additional larger solar plants, including one over 200,000 MW, have been proposed around the world.\n\nOperating solar farms that are 200 MW or larger\n\n\n"}
{"id": "31366108", "url": "https://en.wikipedia.org/wiki?curid=31366108", "title": "Lubachevsky–Stillinger algorithm", "text": "Lubachevsky–Stillinger algorithm\n\nLubachevsky-Stillinger (compression) algorithm (LS algorithm, LSA, or LS protocol) is a numerical procedure suggested by F. H. Stillinger and B.D. Lubachevsky that simulates or imitates a physical process of compressing an assembly of hard particles. As the LSA may need thousands of arithmetic operations even for a few particles, it is usually carried out on a digital computer.\n\nA physical process of compression often involves a contracting hard boundary of the container, such as a piston pressing against the particles. The LSA is able to simulate such a scenario. However, the LSA was originally introduced in the setting without a hard boundary where the virtual particles were \"swelling\" or expanding in a fixed, finite virtual volume with periodic boundary conditions. The absolute sizes of the particles were increasing but particle-to-particle relative sizes remained constant. In general, the LSA can handle an external compression and an internal particle expansion, both occurring simultaneously and possibly, but not necessarily, combined with a hard boundary. In addition, the boundary can be mobile.\n\nIn a final, compressed, or \"jammed\" state, some particles are not jammed, they are able to move within \"cages\" formed by their immobile, jammed neighbors and the hard boundary, if any. These free-to-move particles are not an artifact, or pre-designed, or target feature of the LSA, but rather a real phenomenon. The simulation revealed this phenomenon, somewhat unexpectedly for the authors of the LSA. Frank H. Stillinger coined the term \"rattlers\" for the free-to-move particles, because if one physically shakes a compressed bunch of hard particles, the rattlers will be rattling.\n\nIn the \"pre-jammed\" mode when the density of the configuration is low and when the particles are mobile, the compression and expansion can be stopped, if so desired. Then the LSA, in effect, would be simulating a granular flow. Various dynamics of the instantaneous collisions can be simulated such as: with or without a full restitution, with or without tangential friction.\nDifferences in masses of the particles can be taken into account. It is also easy and sometimes proves useful to \"fluidize\" a jammed configuration, by decreasing the sizes of all or some of the particles. Another possible extension of the LSA is replacing the hard collision force potential (zero outside the particle, infinity at or inside) with a piece-wise constant force potential. The LSA thus modified would approximately simulate molecular dynamics with continuous\nshort range particle-particle force interaction. External force fields, such as gravitation, can be also introduced, as long as the inter-collision motion of each particle can be represented by a simple one-step calculation.\n\nUsing LSA for spherical particles of different sizes and/or for jamming in a non-commeasureable size container proved to be a useful technique for generating and studying micro-structures formed under conditions of a crystallographic defect or a geometrical frustration It should be added that the original LS protocol was designed primarily for spheres of same or different sizes.\n\nAny deviation from the spherical (or circular in two dimensions) shape, even a simplest one, when spheres are replaced with ellipsoids (or ellipses in two dimensions), causes thus modified LSA to slow down substantially.\nBut as long as the shape is spherical, the LSA is able to handle particle assemblies in tens to hundreds of thousands\non today's (2011) standard personal computers. Only a very limited experience was reported\nin using the LSA in dimensions higher than 3.\n\nThe state of particle jamming is achieved via simulating a granular flow. The flow is rendered as a discrete event simulation, the events being particle-particle or particle-boundary collisions. Ideally, the calculations should have been\nperformed with the infinite precision. Then the jamming would have occurred ad infinitum. In practice, the precision is finite as is the available resolution of representing the real numbers in the computer memory, for example, a double-precision resolution. The real calculations are stopped when inter-collision runs of the non-rattler particles become\nsmaller than an explicitly or implicitly specified small threshold. For example, it is useless to continue the calculations when inter-collision runs are smaller than the roundoff error.\n\nThe LSA is efficient in the sense that the events are processed essentially in an event-driven fashion, rather than in a \ntime-driven fashion. This means almost no calculation is wasted on computing or maintaining the positions and velocities\nof the particles between the collisions. Among the event-driven algorithms intended for the same task of simulating granular flow, like, for example, the algorithm of D.C. Rapaport, the LSA is distinguished by a simpler data structure and data handling.\n\nFor any particle at any stage of calculations the LSA keeps record of only two events: an old, already processed committed event, which comprises the committed event time stamp, the particle state (including position and velocity), and, perhaps, the \"partner\" which could be another particle or boundary identification, the one with which the particle collided in the past,\nand a new event proposed for a future processing with a similar set of parameters. The new event is not committed. The maximum of the committed old event times must never exceed the minimum of the non-committed new event times.\n\nNext particle to be examined by the algorithm has the current minimum of new event times. At examining the chosen particle,\nwhat was previously the new event, is declared to be the old one and to be committed, whereas the next new event is being scheduled, with its new time stamp, new state, and new partner, if any. As the next new event for a particle is being set,\nsome of the neighboring particles may update their non-committed new events to better account for the new information.\n\nAs the calculations of the LSA progress, the collision rates of particles may and usually do increase. Still the LSA successfully approaches the jamming state as long as those rates remain comparable among all the particles, except for the rattlers. (Rattlers experience consistently low collision rates. This property allows one to detect rattlers.) However, \nit is possible for a few particles, even just for a single particle, to experience a very high collision rate along the approach to a certain simulated time. The rate will be increasing without a bound in proportion to the rates of collisions in the rest of the particle ensemble. If this happens, then the simulation will be stuck in time, it won't be able to progress toward the state of jamming.\n\nThe stuck-in-time failure can also occur when simulating a granular flow without particle compression or expansion. This failure mode was recognized by the practitioners of granular flow simulations as an \"inelastic collapse\" because it often occurs in such simulations when the restitution coefficient in collisions is low (i.e. inelastic). The failure is not specific to only the LSA algorithm. Techniques to avoid the failure have been proposed.\n\nThe LSA was a by-product of an attempt to find a fair measure of speedup in parallel simulations. The Time Warp parallel simulation algorithm by David Jefferson was advanced as a method to simulate asynchronous spatial interactions of fighting units in combat models on a parallel computer. Colliding particles models offered similar simulation tasks with spatial interactions of particles but clear of the details that are non-essential for exposing the simulation techniques. The speedup was presented as the ratio of the execution time on a uniprocessor over that on a multiprocessor, when executing the same parallel Time Warp algorithm. Boris D. Lubachevsky noticed that such a speedup assessment might be faulty because executing a parallel algorithm for a task on a uniprocessor is not necessarily the fastest way to perform the task on such a machine. The LSA was created in an attempt to produce a faster uniprocessor simulation and hence to have a more fair assessment of the parallel speedup. Later on, a parallel simulation algorithm,\ndifferent from the Time Warp, was also proposed, that, when run on a uniprocessor, reduces to the LSA.\n\n"}
{"id": "20640976", "url": "https://en.wikipedia.org/wiki?curid=20640976", "title": "Maximum bubble pressure method", "text": "Maximum bubble pressure method\n\nIn physics, the maximum bubble pressure method, or in short bubble pressure method, is a technique to measure the surface tension of a liquid, with surfactants.\n\nWhen the liquid forms an interface with a gas phase, a molecule on the border has quite different physical properties due to the unbalance of attracting forces by the neighboring molecules. At the equilibrium state of the liquid, interior molecules are under the balanced forces with uniformly distributed adjacent molecules.\n\nHowever, relatively fewer number of molecules in the gas phase above the interface than condensed liquid phase makes overall sum of forces applied to the surface molecule direct inside of the liquid and thus surface molecules tend to minimize their own surface area.\n\nSuch an inequality of molecular forces induces continuous movement of molecules from the inside to the surface, which means the surface molecules has extra energy, which is called surface free energy or potential energy, and such an energy acting on reduced unit area is defined as surface tension.\n\nThis is a frame work to interpret relevant phenomena which occurs surface or interface of materials and many methods to measure the surface tension has been developed.\n\nAmong the various ways to determine surface tension, Du Noüy ring method and Wilhelmy slide method are based on the separation of a solid object from the liquid surface, and Pendant drop method and Sessile drop or bubble method depend on the deformation of the spherical shape of a liquid drop.\n\nEven though these methods are relatively simple and commonly used to determine the static surface tension, in case that the impurities are added to the liquid, measurement of surface tension based on the dynamic equilibrium should be applied since it takes more time to obtain a completely formed surface and this means that it is difficult to achieve the static equilibrium as a pure liquid does.\n\nThe most typical impurity to induce dynamic surface tension measurement is a surfactant molecule which has both of hydrophilic segment, generally called “head group” and hydrophobic segment, generally called “tail group” in a same molecule. Due to the characteristic molecular structure, surfactants migrate to the liquid surface bordering gas phase until an external force disperse the accumulated molecules from the interface or surface is fully occupied and thus cannot accommodate extra molecules. During this process, surface tension decrease as function of time and finally approach the equilibrium surface tension (σ). Such a process is illustrated in figure 1. (Image was reproduced from reference)\n\nOne of the useful methods to determine the dynamic surface tension is measuring the \"maximum bubble pressure method\" or, simply, bubble pressure method.\n\nBubble pressure tensiometer produces gas bubbles (ex. air) at constant rate and blows them through a capillary which is submerged in the sample liquid and its radius is already known.\n\nThe pressure (\"P\") inside of the gas bubble continues to increase and the maximum value is obtained when the bubble has the completely hemispherical shape whose radius is exactly corresponding to the radius of the capillary.\n\nFigure 2 shows each step of bubble formation and corresponding change of bubble radius and each step is described below. (Image was reproduced from reference)\n\nA, B: A bubble appears on the end of the capillary. As the size increases, the radius of curvature of the bubble decreases.\n\nC: At the point of the maximum bubble pressure, the bubble has a complete hemispherical shape whose radius is identical to the radius of the capillary denoted by Rcap. The surface tension can be determined using the Young–Laplace equation in the reduced form for spherical bubble shape within the liquid.\n\nformula_1\n\nD, E: After the maximum pressure, the pressure of the bubble decreases and the radius of the bubble increases until the bubble is detached from the end of a capillary and a new cycle begins. This is not relevant to determine the surface tension.\n\nCurrently developed and commercialized tensiometers monitors the pressure needed to form a bubble, the pressure difference between inside and outside the bubble, the radius of the bubble, and the surface tension of the sample are calculated in one time and a data acquisition is carried out via PC control.\n\nBubble pressure method is commonly used to measure the dynamic surface tension for the system containing surfactants or other impurities because it does not require contact angle measurement and has high accuracy even though the measurement is done rapidly. “Bubble pressure method” can be applied to measure the dynamic surface tension, particularly for the systems which contain surfactants. Moreover, this method is an appropriate technique to apply to biological fluids like serum because it does not require a large amount of liquid sample for the measurements. Finally, the method is used for an indirect determination of the surfactant content of industrial cleaning or coating baths because the dynamic surface tension in a particular range of bubble formation rates shows a strong correlation with the concentration. \n\n"}
{"id": "1155556", "url": "https://en.wikipedia.org/wiki?curid=1155556", "title": "McLeod gauge", "text": "McLeod gauge\n\nA McLeod gauge is a scientific instrument used to measure very low pressures, down to 10 Torr. It was invented in 1874 by Herbert McLeod (1841–1923). McLeod gauges were once commonly found attached to equipment that operates under vacuum, such as a lyophilizer. Today, however, these gauges have largely been replaced by electronic vacuum gauges.\n\nThe design of a McLeod gauge is somewhat similar to that of a mercury-column manometer. Typically it is filled with mercury. If used incorrectly, this mercury can escape and contaminate the vacuum system attached to the gauge.\nMcLeod gauges operate by taking in a sample volume of gas from a vacuum chamber, then compressing it by tilting and infilling with mercury. The pressure in this smaller volume is then measured by a mercury manometer, and knowing the compression ratio (the ratio of the initial and final volumes), the pressure of the original vacuum can be determined by applying Boyle's law.\n\nThis method is fairly accurate for non-condensible gases, such as oxygen and nitrogen. However, condensible gases, such as water vapour, ammonia, carbon dioxide, and pump-oil vapors may be in gaseous form in the low pressure of the vacuum chamber, but will condense when compressed by the McLeod gauge. The result is an erroneous reading, showing a pressure much lower than actually present. A cold trap may be used in conjunction with a McLeod gauge to condense these vapors before they enter the gauge.\n\nThe McLeod gauge has the advantage that it is simple to use and that its calibration is nearly the same for all non-condensable gases.\nThe device can be manually operated and the scale read visually, or the process can be automated in various ways. For example, a small electric motor can periodically rotate the assembly to collect a gas sample. If a fine platinum wire is in the capillary tube, its resistance indicates the height of the mercury column around it.\n\nModern electronic vacuum gauges are simpler to use, less fragile, and do not present a mercury hazard, but their reading is highly dependent on the chemical nature of the gas being measured, and their calibration is unstable. For this reason, McLeod gauges continue to be used as a calibration standard for electronic gauges.\n\n\n"}
{"id": "31670496", "url": "https://en.wikipedia.org/wiki?curid=31670496", "title": "Minamata: The Victims and Their World", "text": "Minamata: The Victims and Their World\n\nThe film focuses on the residents of Minamata and nearby communities who suffered damage to their nervous systems, or who were born deformed, due to the ingestion of fish containing abnormal amounts of mercury released into the sea by a fertilizer factory owned by Chisso. It not only shows their current condition and the hardships borne by their families, but also the discrimination they had suffered from other Minamata residents, the insufficient response by Chisso, the slowness of government action, and the problems faced by victims who had not been officially designated as suffering from Minamata disease. The main action of the last part of the film is the effort of victims and their supporters to buy shares of Chisso in small quantities so that they can attend the annual stockholders' meeting and confront the corporate leadership. The documentary takes the side of the victims in their struggle, but it also devotes much time to understanding their lifestyle, especially their traditions and their close relationship with the sea.\n\n\"Minamata: The Victims and Their World\" screened at numerous film festivals and won several awards, including the Film Ducat at the Mannheim-Heidelberg International Film Festival. The critic Mark Cousins has programmed it as one of \"ten documentaries that shook the world.\"\n\nThe original Japanese film is 167 minutes long. The version currently available on DVD with English subtitles is 120 minutes long and was first prepared by Tsuchimoto for international environmental conferences and film festivals.\n\n"}
{"id": "20011788", "url": "https://en.wikipedia.org/wiki?curid=20011788", "title": "Murgash Wind Farm", "text": "Murgash Wind Farm\n\nThe Murgash Wind Farm is a proposed wind power project in Murgash, Bulgaria. It will have 50 individual turbines with a nominal output of around 2 MW which will deliver up to 100 MW of power.\n\n"}
{"id": "23318", "url": "https://en.wikipedia.org/wiki?curid=23318", "title": "Phosphorus", "text": "Phosphorus\n\nPhosphorus is a chemical element with symbol P and atomic number 15. Elemental phosphorus exists in two major forms, white phosphorus and red phosphorus, but because it is highly reactive, phosphorus is never found as a free element on Earth. It has a concentration in the Earth's crust of about one gram per kilogram (compare copper at about 0.06 grams). With few exceptions, minerals containing phosphorus are in the maximally oxidized state as inorganic phosphate rocks.\n\nElemental phosphorus was first isolated (as white phosphorus) in 1669 and emitted a faint glow when exposed to oxygen – hence the name, taken from Greek mythology, meaning \"light-bearer\" (Latin \"Lucifer\"), referring to the \"Morning Star\", the planet Venus. The term \"phosphorescence\", meaning glow after illumination, derives from this property of phosphorus, although the word has since been used for a different physical process that produces a glow. The glow of phosphorus is caused by oxidation of the white (but not red) phosphorus — a process now called chemiluminescence. Together with nitrogen, arsenic, antimony, and bismuth, phosphorus is classified as a pnictogen.\n\nPhosphorus is essential for life. Phosphates (compounds containing the phosphate ion, PO) are a component of DNA, RNA, ATP, and phospholipids. Elemental phosphorus was first isolated from human urine, and bone ash was an important early phosphate source. Phosphate mines contain fossils because phosphate is present in the fossilized deposits of animal remains and excreta. Low phosphate levels are an important limit to growth in some aquatic systems. The vast majority of phosphorus compounds mined are consumed as fertilisers. Phosphate is needed to replace the phosphorus that plants remove from the soil, and its annual demand is rising nearly twice as fast as the growth of the human population. Other applications include organophosphorus compounds in detergents, pesticides, and nerve agents.\n\nPhosphorus has several allotropes that exhibit strikingly different properties. The two most common allotropes are white phosphorus and red phosphorus.\n\nFrom the perspective of applications and chemical literature, the most important form of elemental phosphorus is white phosphorus, often abbreviated as WP. It is a soft, waxy solid which consists of tetrahedral molecules, in which each atom is bound to the other three atoms by a single bond. This tetrahedron is also present in liquid and gaseous phosphorus up to the temperature of when it starts decomposing to molecules. White phosphorus exists in two crystalline forms: α (alpha) and β (beta). At room temperature, the α-form is stable, which is more common and it has cubic crystal structure and at , it transforms into β-form, which has hexagonal crystal structure. These forms differ in terms of the relative orientations of the constituent P tetrahedra.\n\nWhite phosphorus is the least stable, the most reactive, the most volatile, the least dense, and the most toxic of the allotropes. White phosphorus gradually changes to red phosphorus. This transformation is accelerated by light and heat, and samples of white phosphorus almost always contain some red phosphorus and accordingly appear yellow. For this reason, white phosphorus that is aged or otherwise impure (e.g., weapons-grade, not lab-grade WP) is also called yellow phosphorus. When exposed to oxygen, white phosphorus glows in the dark with a very faint tinge of green and blue. It is highly flammable and pyrophoric (self-igniting) upon contact with air. Owing to its pyrophoricity, white phosphorus is used as an additive in napalm. The odour of combustion of this form has a characteristic garlic smell, and samples are commonly coated with white \"phosphorus pentoxide\", which consists of tetrahedra with oxygen inserted between the phosphorus atoms and at their vertices. White phosphorus is insoluble in water but soluble in carbon disulfide.\n\nThermolysis of P at 1100 kelvin gives diphosphorus, P. This species is not stable as a solid or liquid. The dimeric unit contains a triple bond and is analogous to N. It can also be generated as a transient intermediate in solution by thermolysis of organophosphorus precursor reagents. At still higher temperatures, P dissociates into atomic P.\n\nRed phosphorus is polymeric in structure. It can be viewed as a derivative of P wherein one P-P bond is broken, and one additional bond is formed with the neighbouring tetrahedron resulting in a chain-like structure. Red phosphorus may be formed by heating white phosphorus to or by exposing white phosphorus to sunlight. Phosphorus after this treatment is amorphous. Upon further heating, this material crystallises. In this sense, red phosphorus is not an allotrope, but rather an intermediate phase between the white and violet phosphorus, and most of its properties have a range of values. For example, freshly prepared, bright red phosphorus is highly reactive and ignites at about , though it is more stable than white phosphorus, which ignites at about . After prolonged heating or storage, the color darkens (see infobox images); the resulting product is more stable and does not spontaneously ignite in air.\n\nViolet phosphorus is a form of phosphorus that can be produced by day-long annealing of red phosphorus above 550 °C. In 1865, Hittorf discovered that when phosphorus was recrystallised from molten lead, a red/purple form is obtained. Therefore, this form is sometimes known as \"Hittorf's phosphorus\" (or violet or α-metallic phosphorus).\n\nBlack phosphorus is the least reactive allotrope and the thermodynamically stable form below . It is also known as β-metallic phosphorus and has a structure somewhat resembling that of graphite. It is obtained by heating white phosphorus under high pressures (about ). It can also be produced at ambient conditions using metal salts, e.g. mercury, as catalysts. In appearance, properties, and structure, it resembles graphite, being black and flaky, a conductor of electricity, and has puckered sheets of linked atoms.\n\nAnother form, scarlet phosphorus, is obtained by allowing a solution of white phosphorus in carbon disulfide to evaporate in sunlight.\n\nWhen first isolated, it was observed that the green glow emanating from white phosphorus would persist for a time in a stoppered jar, but then cease. Robert Boyle in the 1680s ascribed it to \"debilitation\" of the air. Actually, it is oxygen being consumed. By the 18th century, it was known that in pure oxygen, phosphorus does not glow at all; there is only a range of partial pressures at which it does. Heat can be applied to drive the reaction at higher pressures.\n\nIn 1974, the glow was explained by R. J. van Zee and A. U. Khan. A reaction with oxygen takes place at the surface of the solid (or liquid) phosphorus, forming the short-lived molecules HPO and that both emit visible light. The reaction is slow and only very little of the intermediates are required to produce the luminescence, hence the extended time the glow continues in a stoppered jar.\n\nSince its discovery, \"phosphors\" and \"phosphorescence\" were used loosely to describe substances that shine in the dark without burning. Although the term phosphorescence is derived from phosphorus, the reaction that gives phosphorus its glow is properly called chemiluminescence (glowing due to a cold chemical reaction), not phosphorescence (re-emitting light that previously fell onto a substance and excited it).\n\nTwenty-three isotopes of phosphorus are known, including all possibilities from up to . Only is stable and is therefore present at 100% abundance. The half-integer nuclear spin and high abundance of P make phosphorus-31 NMR spectroscopy a very useful analytical tool in studies of phosphorus-containing samples.\n\nTwo radioactive isotopes of phosphorus have half-lives suitable for biological scientific experiments. These are:\nThe high energy beta particles from penetrate skin and corneas and any ingested, inhaled, or absorbed is readily incorporated into bone and nucleic acids. For these reasons, Occupational Safety and Health Administration in the United States, and similar institutions in other developed countries require personnel working with to wear lab coats, disposable gloves, and safety glasses or goggles to protect the eyes, and avoid working directly over open containers. Monitoring personal, clothing, and surface contamination is also required. Shielding requires special consideration. The high energy of the beta particles gives rise to secondary emission of X-rays via Bremsstrahlung (braking radiation) in dense shielding materials such as lead. Therefore, the radiation must be shielded with low density materials such as acrylic or other plastic, water, or (when transparency is not required), even wood.\n\nIn 2013, astronomers detected phosphorus in Cassiopeia A, which confirmed that this element is produced in supernovae as a byproduct of supernova nucleosynthesis. The phosphorus-to-iron ratio in material from the supernova remnant could be up to 100 times higher than in the Milky Way in general.\n\nPhosphorus has a concentration in the Earth's crust of about one gram per kilogram (compare copper at about 0.06 grams). It is not found free in nature, but is widely distributed in many minerals, usually as phosphates. Inorganic phosphate rock, which is partially made of apatite (a group of minerals being, generally, pentacalcium triorthophosphate fluoride (hydroxide)), is today the chief commercial source of this element. According to the US Geological Survey (USGS), about 50 percent of the global phosphorus reserves are in the Arab nations. Large deposits of apatite are located in China, Russia, Morocco, Florida, Idaho, Tennessee, Utah, and elsewhere. Albright and Wilson in the UK and their Niagara Falls plant, for instance, were using phosphate rock in the 1890s and 1900s from Tennessee, Florida, and the Îles du Connétable (guano island sources of phosphate); by 1950, they were using phosphate rock mainly from Tennessee and North Africa.\n\nOrganic sources, namely urine, bone ash and (in the latter 19th century) guano, were historically of importance but had only limited commercial success. As urine contains phosphorus, it has fertilising qualities which are still harnessed today in some countries, including Sweden, using methods for reuse of excreta. To this end, urine can be used as a fertiliser in its pure form or part of being mixed with water in the form of sewage or sewage sludge.\n\nThe most prevalent compounds of phosphorus are derivatives of phosphate (PO), a tetrahedral anion. Phosphate is the conjugate base of phosphoric acid, which is produced on a massive scale for use in fertilisers. Being triprotic, phosphoric acid converts stepwise to three conjugate bases:\n\nPhosphate exhibits a tendency to form chains and rings containing P-O-P bonds. Many polyphosphates are known, including ATP. Polyphosphates arise by dehydration of hydrogen phosphates such as HPO and HPO. For example, the industrially important pentasodium triphosphate (also known as sodium tripolyphosphate, STPP) is produced industrially on by the megatonne by this condensation reaction:\nPhosphorus pentoxide (PO) is the acid anhydride of phosphoric acid, but several intermediates between the two are known. This waxy white solid reacts vigorously with water.\n\nWith metal cations, phosphate forms a variety of salts. These solids are polymeric, featuring P-O-M linkages. When the metal cation has a charge of 2+ or 3+, the salts are generally insoluble, hence they exist as common minerals. Many phosphate salts are derived from hydrogen phosphate (HPO).\n\nPCl and PF are common compounds. PF is a colourless gas and the molecules have trigonal bipyramidal geometry. PCl is a colourless solid which has an ionic formulation of PCl PCl, but adopts the trigonal bipyramidal geometry when molten or in the vapour phase. PBr is an unstable solid formulated as PBrBrand PI is not known. The pentachloride and pentafluoride are Lewis acids. With fluoride, PF forms PF, an anion that is isoelectronic with SF. The most important oxyhalide is phosphorus oxychloride, (POCl), which is approximately tetrahedral.\n\nBefore extensive computer calculations were feasible, it was thought that bonding in phosphorus(V) compounds involved \"d\" orbitals. Computer modeling of molecular orbital theory indicates that this bonding involves only s- and p-orbitals.\n\nAll four symmetrical trihalides are well known: gaseous PF, the yellowish liquids PCl and PBr, and the solid PI. These materials are moisture sensitive, hydrolysing to give phosphorous acid. The trichloride, a common reagent, is produced by chlorination of white phosphorus:\nThe trifluoride is produced from the trichloride by halide exchange. PF is toxic because it binds to haemoglobin.\n\nPhosphorus(III) oxide, PO (also called tetraphosphorus hexoxide) is the anhydride of P(OH), the minor tautomer of phosphorous acid. The structure of PO is like that of PO without the terminal oxide groups.\n\nThese compounds generally feature P-P bonds. Examples include catenated derivatives of phosphine and organophosphines. Compounds containing P=P double bonds have also been observed, although they are rare.\n\nPhosphides arise by reaction of metals with red phosphorus. The alkali metals (group 1) and alkaline earth metals can form ionic compounds containing the phosphide ion, P. These compounds react with water to form phosphine. Other phosphides, for example NaP, are known for these reactive metals. With the transition metals as well as the monophosphides there are metal-rich phosphides, which are generally hard refractory compounds with a metallic lustre, and phosphorus-rich phosphides which are less stable and include semiconductors. Schreibersite is a naturally occurring metal-rich phosphide found in meteorites. The structures of the metal-rich and phosphorus-rich phosphides can be complex.\n\nPhosphine (PH) and its organic derivatives (PR) are structural analogues of ammonia (NH), but the bond angles at phosphorus are closer to 90° for phosphine and its organic derivatives. It is an ill-smelling, toxic compound. Phosphorus has an oxidation number of -3 in phosphine. Phosphine is produced by hydrolysis of calcium phosphide, CaP. Unlike ammonia, phosphine is oxidised by air. Phosphine is also far less basic than ammonia. Other phosphines are known which contain chains of up to nine phosphorus atoms and have the formula PH. The highly flammable gas diphosphine (PH) is an analogue of hydrazine.\n\nPhosphorous oxoacids are extensive, often commercially important, and sometimes structurally complicated. They all have acidic protons bound to oxygen atoms, some have nonacidic protons that are bonded directly to phosphorus and some contain phosphorus - phosphorus bonds. Although many oxoacids of phosphorus are formed, only nine are commercially important, and three of them, hypophosphorous acid, phosphorous acid, and phosphoric acid, are particularly important.\n\nThe PN molecule is considered unstable, but is a product of crystalline phosphorus nitride decomposition at 1100 K. Similarly, HPN is considered unstable, and phosphorus nitride halogens like FPN, ClPN, BrPN, and IPN oligomerise into cyclic Polyphosphazenes. For example, compounds of the formula (PNCl) exist mainly as rings such as the trimer hexachlorophosphazene. The phosphazenes arise by treatment of phosphorus pentachloride with ammonium chloride:PCl + NHCl → 1/\"n\" (NPCl) + 4 HClWhen the chloride groups are replaced by alkoxide (RO), a family of polymers is produced with potentially useful properties.\n\nPhosphorus forms a wide range of sulfides, where the phosphorus can be in P(V), P(III) or other oxidation states. The most famous is the three-fold symmetric PS which is used in strike-anywhere matches. PS and PO have analogous structures. Mixed oxyhalides and oxyhydrides of phosphorus(III) are almost unknown.\n\nCompounds with P-C and P-O-C bonds are often classified as organophosphorus compounds. They are widely used commercially. The PCl serves as a source of P in routes to organophosphorus(III) compounds. For example, it is the precursor to triphenylphosphine:\nTreatment of phosphorus trihalides with alcohols and phenols gives phosphites, e.g. triphenylphosphite:\nSimilar reactions occur for phosphorus oxychloride, affording triphenylphosphate:\n\nThe name \"Phosphorus\" in Ancient Greece was the name for the planet Venus and is derived from the Greek words (φῶς = light, φέρω = carry), which roughly translates as light-bringer or light carrier. (In Greek mythology and tradition, Augerinus (Αυγερινός = morning star, still in use today), Hesperus or Hesperinus (΄Εσπερος or Εσπερινός or Αποσπερίτης = evening star, still in use today) and Eosphorus (Εωσφόρος = dawnbearer, not in use for the planet after Christianity) are close homologues, and also associated with Phosphorus-the-planet).\n\nAccording to the Oxford English Dictionary, the correct spelling of the element is phosphorus. The word phosphorous is the adjectival form of the P valence: so, just as sulfur forms sulfurous and sulfuric compounds, phosphorus forms phosphorous compounds (e.g., phosphorous acid) and P valence phosphoric compounds (e.g., phosphoric acids and phosphates).\n\nThe discovery of phosphorus, the first element to be discovered that was not known since ancient times, is credited to the German alchemist Hennig Brand in 1669, although other chemists might have discovered phosphorus around the same time. Brand experimented with urine, which contains considerable quantities of dissolved phosphates from normal metabolism. Working in Hamburg, Brand attempted to create the fabled philosopher's stone through the distillation of some salts by evaporating urine, and in the process produced a white material that glowed in the dark and burned brilliantly. It was named \"phosphorus mirabilis\" (\"miraculous bearer of light\").\n\nBrand's process originally involved letting urine stand for days until it gave off a terrible smell. Then he boiled it down to a paste, heated this paste to a high temperature, and led the vapours through water, where he hoped they would condense to gold. Instead, he obtained a white, waxy substance that glowed in the dark. Brand had discovered phosphorus. We now know that Brand produced ammonium sodium hydrogen phosphate, . While the quantities were essentially correct (it took about of urine to make about 60 g of phosphorus), it was unnecessary to allow the urine to rot first. Later scientists discovered that fresh urine yielded the same amount of phosphorus.\n\nBrand at first tried to keep the method secret, but later sold the recipe for 200 thalers to D. Krafft from Dresden, who could now make it as well, and toured much of Europe with it, including England, where he met with Robert Boyle. The secret that it was made from urine leaked out and first Johann Kunckel (1630–1703) in Sweden (1678) and later Boyle in London (1680) also managed to make phosphorus, possibly with the aid of his assistant, Ambrose Godfrey-Hanckwitz, who later made a business of the manufacture of phosphorus.\n\nBoyle states that Krafft gave him no information as to the preparation of phosphorus other than that it was derived from \"somewhat that belonged to the body of man\". This gave Boyle a valuable clue, so that he, too, managed to make phosphorus, and published the method of its manufacture. Later he improved Brand's process by using sand in the reaction (still using urine as base material),\n\nRobert Boyle was the first to use phosphorus to ignite sulfur-tipped wooden splints, forerunners of our modern matches, in 1680.\n\nPhosphorus was the 13th element to be discovered. For this reason, and due to its use in explosives, poisons and nerve agents, it is sometimes referred to as \"the Devil's element\".\n\nIn 1769, Johan Gottlieb Gahn and Carl Wilhelm Scheele showed that calcium phosphate () is found in bones, and they obtained elemental phosphorus from bone ash. Antoine Lavoisier recognised phosphorus as an element in 1777. Bone ash was the major source of phosphorus until the 1840s. The method started by roasting bones, then employed the use of clay retorts encased in a very hot brick furnace to distill out the highly toxic elemental phosphorus product. Alternately, precipitated phosphates could be made from ground-up bones that had been de-greased and treated with strong acids. White phosphorus could then be made by heating the precipitated phosphates, mixed with ground coal or charcoal in an iron pot, and distilling off phosphorus vapour in a retort. Carbon monoxide and other flammable gases produced during the reduction process were burnt off in a flare stack.\n\nIn the 1840s, world phosphate production turned to the mining of tropical island deposits formed from bird and bat guano (see also Guano Islands Act). These became an important source of phosphates for fertiliser in the latter half of the 19th century.\n\nPhosphate rock, which usually contains calcium phosphate, was first used in 1850 to make phosphorus, and following the introduction of the electric arc furnace by James Burgess Readman in 1888 (patented 1889), elemental phosphorus production switched from the bone-ash heating, to electric arc production from phosphate rock. After the depletion of world guano sources about the same time, mineral phosphates became the major source of phosphate fertiliser production. Phosphate rock production greatly increased after World War II, and remains the primary global source of phosphorus and phosphorus chemicals today. See the article on peak phosphorus for more information on the history and present state of phosphate mining. Phosphate rock remains a feedstock in the fertiliser industry, where it is treated with sulfuric acid to produce various \"superphosphate\" fertiliser products.\n\nWhite phosphorus was first made commercially in the 19th century for the match industry. This used bone ash for a phosphate source, as described above. The bone-ash process became obsolete when the submerged-arc furnace for phosphorus production was introduced to reduce phosphate rock. The electric furnace method allowed production to increase to the point where phosphorus could be used in weapons of war. In World War I, it was used in incendiaries, smoke screens and tracer bullets. A special incendiary bullet was developed to shoot at hydrogen-filled Zeppelins over Britain (hydrogen being highly flammable). During World War II, Molotov cocktails made of phosphorus dissolved in petrol were distributed in Britain to specially selected civilians within the British resistance operation, for defence; and phosphorus incendiary bombs were used in war on a large scale. Burning phosphorus is difficult to extinguish and if it splashes onto human skin it has horrific effects.\n\nEarly matches used white phosphorus in their composition, which was dangerous due to its toxicity. Murders, suicides and accidental poisonings resulted from its use. (An apocryphal tale tells of a woman attempting to murder her husband with white phosphorus in his food, which was detected by the stew's giving off luminous steam). In addition, exposure to the vapours gave match workers a severe necrosis of the bones of the jaw, the infamous \"phossy jaw\". When a safe process for manufacturing red phosphorus was discovered, with its far lower flammability and toxicity, laws were enacted, under the Berne Convention (1906), requiring its adoption as a safer alternative for match manufacture. The toxicity of white phosphorus led to discontinuation of its use in matches. The Allies used phosphorus incendiary bombs in World War II to destroy Hamburg, the place where the \"miraculous bearer of light\" was first discovered.\n\nMost production of phosphorus-bearing material is for agriculture fertilisers. For this purpose, phosphate minerals are converted to phosphoric acid. It follows two distinct chemical routes, the main one being treatment of phosphate minerals with sulfuric acid. The other process utilises white phosphorus, which may be produced by reaction and distillation from very low grade phosphate sources. The white phosphorus is then oxidised to phosphoric acid and subsequently neutralised with base to give phosphate salts. Phosphoric acid produced from white phosphorus is relatively pure and is the main route for the production of phosphates for all purposes, including detergent production.\n\nIn the early 1990s, Albright and Wilson's purified wet phosphoric acid business was being adversely affected by phosphate rock sales by China and the entry of their long-standing Moroccan phosphate suppliers into the purified wet phosphoric acid business.\n\nIn 2017, the USGS estimated 68 billion tons of world reserves, where reserve figures refer to the amount assumed recoverable at current market prices; 0.261 billion tons were mined in 2016. Critical to contemporary agriculture, its annual demand is rising nearly twice as fast as the growth of the human population.\n\nThe production of phosphorus may have peaked already (as per 2011), leading to the possibility of global shortages by 2040. In 2007, at the rate of consumption, the supply of phosphorus was estimated to run out in 345 years. However, some scientists now believe that a \"peak phosphorus\" will occur in 30 years and that \"At current rates, reserves will be depleted in the next 50 to 100 years.\" Cofounder of Boston-based investment firm and environmental foundation Jeremy Grantham wrote in \"Nature\" in November 2012 that consumption of the element \"must be drastically reduced in the next 20-40 years or we will begin to starve.\" According to N.N. Greenwood and A. Earnshaw, authors of the textbook, \"Chemistry of the Elements,\" however, phosphorus comprises about 0.1% by mass of the average rock, and consequently the Earth's supply is vast, although dilute.\n\nPresently, about of elemental phosphorus is produced annually. Calcium phosphate (phosphate rock), mostly mined in Florida and North Africa, can be heated to 1,200–1,500 °C with sand, which is mostly , and coke (refined coal) to produce vaporised . The product is subsequently condensed into a white powder under water to prevent oxidation by air. Even under water, white phosphorus is slowly converted to the more stable red phosphorus allotrope. The chemical equation for this process when starting with fluoroapatite, a common phosphate mineral, is:\nSide products from this process include ferrophosphorus, a crude form of FeP, resulting from iron impurities in the mineral precursors. The silicate slag is a useful construction material. The fluoride is sometimes recovered for use in water fluoridation. More problematic is a \"mud\" containing significant amounts of white phosphorus. Production of white phosphorus is conducted in large facilities in part because it is energy intensive. The white phosphorus is transported in molten form. Some major accidents have occurred during transportation; train derailments at Brownston, Nebraska and Miamisburg, Ohio led to large fires. The worst incident in recent times was an environmental contamination in 1968 when the sea was polluted from spillage and/or inadequately treated sewage from a white phosphorus plant at Placentia Bay, Newfoundland.\n\nAnother process by which elemental phosphorus is extracted includes applying at high temperatures (1500 °C):\n\nHistorically, before the development of mineral-based extractions, white phosphorus was isolated on an industrial scale from bone ash. In this process, the tricalcium phosphate in bone ash is converted to monocalcium phosphate with sulfuric acid:\n\nMonocalcium phosphate is then dehydrated to the corresponding metaphosphate:\n\nWhen ignited to a white heat with charcoal, calcium metaphosphate yields two-thirds of its weight of white phosphorus while one-third of the phosphorus remains in the residue as calcium orthophosphate:\n\nPhosphorus is an essential plant nutrient (often the limiting nutrient), and the bulk of all phosphorus production is in concentrated phosphoric acids for agriculture fertilisers, containing as much as 70% to 75% PO. Its annual demand is rising nearly twice as fast as the growth of the human population. That led to large increase in phosphate (PO) production in the second half of the 20th century. Artificial phosphate fertilisation is necessary because phosphorus is essential to all living organisms; natural phosphorus-bearing compounds are mostly insoluble and inaccessible to plants, and the natural cycle of phosphorus is very slow. Fertiliser is often in the form of superphosphate of lime, a mixture of calcium dihydrogen phosphate (Ca(HPO)), and calcium sulfate dihydrate (CaSO·2HO) produced reacting sulfuric acid and water with calcium phosphate.\n\nProcessing phosphate minerals with sulfuric acid for obtaining fertiliser is so important to the global economy that this is the primary industrial market for sulfuric acid and the greatest industrial use of elemental sulfur.\n\nWhite phosphorus is widely used to make organophosphorus compounds through intermediate phosphorus chlorides and two phosphorus sulfides, phosphorus pentasulfide and phosphorus sesquisulfide. Organophosphorus compounds have many applications, including in plasticisers, flame retardants, pesticides, extraction agents, nerve agents and water treatment.\n\nPhosphorus is also an important component in steel production, in the making of phosphor bronze, and in many other related products. Phosphorus is added to metallic copper during its smelting process to react with oxygen present as an impurity in copper and to produce phosphorus-containing copper (CuOFP) alloys with a higher hydrogen embrittlement resistance than normal copper.\n\nThe first striking match with a phosphorus head was invented by Charles Sauria in 1830. These matches (and subsequent modifications) were made with heads of white phosphorus, an oxygen-releasing compound (potassium chlorate, lead dioxide, or sometimes nitrate), and a binder. They were poisonous to the workers in manufacture, sensitive to storage conditions, toxic if ingested, and hazardous when accidentally ignited on a rough surface. Production in several countries was banned between 1872 and 1925. The international Berne Convention, ratified in 1906, prohibited the use of white phosphorus in matches.\n\nIn consequence, the 'strike-anywhere' matches were gradually replaced by 'safety matches', wherein the white phosphorus was replaced by phosphorus sesquisulfide (PS), sulfur, or antimony sulfide. Such matches are difficult to ignite on any surface other than a special strip. The strip contains red phosphorus that heats up upon striking, reacts with the oxygen-releasing compound in the head, and ignites the flammable material of the head.\n\nSodium tripolyphosphate made from phosphoric acid is used in laundry detergents in some countries, but banned for this use in others. This compound softens the water to enhance the performance of the detergents and to prevent pipe/boiler tube corrosion.\n\n\nInorganic phosphorus in the form of the phosphate is required for all known forms of life. Phosphorus plays a major role in the structural framework of DNA and RNA. Living cells use phosphate to transport cellular energy with adenosine triphosphate (ATP), necessary for every cellular process that uses energy. ATP is also important for phosphorylation, a key regulatory event in cells. Phospholipids are the main structural components of all cellular membranes. Calcium phosphate salts assist in stiffening bones. Biochemists commonly use the abbreviation \"Pi\" to refer to inorganic phosphate.\n\nEvery living cell is encased in a membrane that separates it from its surroundings. Cellular membranes are composed of a phospholipid matrix and proteins, typically in the form of a bilayer. Phospholipids are derived from glycerol with two of the glycerol hydroxyl (OH) protons replaced by fatty acids as an ester, and the third hydroxyl proton has been replaced with phosphate bonded to another alcohol.\n\nAn average adult human contains about 0.7 kg of phosphorus, about 85–90% in bones and teeth in the form of apatite, and the remainder in soft tissues and extracellular fluids (~1%). The phosphorus content increases from about 0.5 weight% in infancy to 0.65–1.1 weight% in adults. Average phosphorus concentration in the blood is about 0.4 g/L, about 70% of that is organic and 30% inorganic phosphates. An adult with healthy diet consumes and excretes about 1–3 grams of phosphorus per day, with consumption in the form of inorganic phosphate and phosphorus-containing biomolecules such as nucleic acids and phospholipids; and excretion almost exclusively in the form of phosphate ions such as and . Only about 0.1% of body phosphate circulates in the blood, paralleling the amount of phosphate available to soft tissue cells.\n\nThe main component of bone is hydroxyapatite as well as amorphous forms of calcium phosphate, possibly including carbonate. Hydroxyapatite is the main component of tooth enamel. Water fluoridation enhances the resistance of teeth to decay by the partial conversion of this mineral to the still harder material called fluoroapatite:\n\nIn medicine, phosphate deficiency syndrome may be caused by malnutrition, by failure to absorb phosphate, and by metabolic syndromes that draw phosphate from the blood (such as in refeeding syndrome after malnutrition) or pass too much of it into the urine. All are characterised by hypophosphatemia, which is a condition of low levels of soluble phosphate levels in the blood serum and inside the cells. Symptoms of hypophosphatemia include neurological dysfunction and disruption of muscle and blood cells due to lack of ATP. Too much phosphate can lead to diarrhoea and calcification (hardening) of organs and soft tissue, and can interfere with the body's ability to use iron, calcium, magnesium, and zinc.\n\nPhosphorus is an essential macromineral for plants, which is studied extensively in edaphology to understand plant uptake from soil systems. Phosphorus is a limiting factor in many ecosystems; that is, the scarcity of phosphorus limits the rate of organism growth. An excess of phosphorus can also be problematic, especially in aquatic systems where eutrophication sometimes leads to algal blooms.\n\nThe U.S. Institute of Medicine (IOM) updated Estimated Average Requirements (EARs) and Recommended Dietary Allowances (RDAs) for phosphorus in 1997. If there is not sufficient information to establish EARs and RDAs, an estimate designated Adequate Intake (AI) is used instead. The current EAR for phosphorus for people ages 19 and up is 580 mg/day. The RDA is 700 mg/day. RDAs are higher than EARs so as to identify amounts that will cover people with higher than average requirements. RDA for pregnancy and lactation are also 700 mg/day. For children ages 1–18 years the RDA increases with age from 460 to 1250 mg/day. As for safety, the IOM sets Tolerable upper intake levels (ULs) for vitamins and minerals when evidence is sufficient. In the case of phosphorus the UL is 4000 mg/day. Collectively the EARs, RDAs, AIs and ULs are referred to as Dietary Reference Intakes (DRIs).\n\nThe European Food Safety Authority (EFSA) refers to the collective set of information as Dietary Reference Values, with Population Reference Intake (PRI) instead of RDA, and Average Requirement instead of EAR. AI and UL defined the same as in United States. For people ages 15 and older, including pregnancy and lactation, the AI is set at 550 mg/day. For children ages 4–10 years the AI is 440 mg/day, for ages 11–17 640 mg/day. These AIs are lower than the U.S RDAs. In both systems, teenagers need more than adults. The European Food Safety Authority reviewed the same safety question and decided that there was not sufficient information to set a UL.\n\nFor U.S. food and dietary supplement labeling purposes the amount in a serving is expressed as a percent of Daily Value (%DV). For phosphorus labeling purposes 100% of the Daily Value was 1000 mg, but as of May 27, 2016 it was revised to 1250 mg to bring it into agreement with the RDA. A table of the old and new adult Daily Values is provided at Reference Daily Intake. The original deadline to be in compliance was July 28, 2018, but on September 29, 2017 the FDA released a proposed rule that extended the deadline to January 1, 2020 for large companies and January 1, 2021 for small companies.\n\nThe main food sources for phosphorus are the same as those containing protein, although proteins do not contain phosphorus. For example, milk, meat, and soya typically also have phosphorus. As a rule, if a diet has sufficient protein and calcium, the amount of phosphorus is probably sufficient.\n\nOrganic compounds of phosphorus form a wide class of materials; many are required for life, but some are extremely toxic. Fluorophosphate esters are among the most potent neurotoxins known. A wide range of organophosphorus compounds are used for their toxicity as pesticides (herbicides, insecticides, fungicides, etc.) and weaponised as nerve agents against enemy humans. Most inorganic phosphates are relatively nontoxic and essential nutrients.\n\nThe white phosphorus allotrope presents a significant hazard because it ignites in air and produces phosphoric acid residue. Chronic white phosphorus poisoning leads to necrosis of the jaw called \"phossy jaw\". White phosphorus is toxic, causing severe liver damage on ingestion and may cause a condition known as \"Smoking Stool Syndrome\".\n\nIn the past, external exposure to elemental phosphorus was treated by washing the affected area with 2% copper sulfate solution to form harmless compounds that are then washed away. According to the recent \"US Navy's Treatment of Chemical Agent Casualties and Conventional Military Chemical Injuries: FM8-285: Part 2 Conventional Military Chemical Injuries\", \"Cupric (copper(II)) sulfate has been used by U.S. personnel in the past and is still being used by some nations. However, copper sulfate is toxic and its use will be discontinued. Copper sulfate may produce kidney and cerebral toxicity as well as intravascular hemolysis.\"\n\nThe manual suggests instead \"a bicarbonate solution to neutralise phosphoric acid, which will then allow removal of visible white phosphorus. Particles often can be located by their emission of smoke when air strikes them, or by their phosphorescence in the dark. In dark surroundings, fragments are seen as luminescent spots. Promptly debride the burn if the patient's condition will permit removal of bits of WP (white phosphorus) that might be absorbed later and possibly produce systemic poisoning. DO NOT apply oily-based ointments until it is certain that all WP has been removed. Following complete removal of the particles, treat the lesions as thermal burns.\" As white phosphorus readily mixes with oils, any oily substances or ointments are not recommended until the area is thoroughly cleaned and all white phosphorus removed.\n\nPeople can be exposed to phosphorus in the workplace by inhalation, ingestion, skin contact, and eye contact. The Occupational Safety and Health Administration (OSHA) has set the phosphorus exposure limit (Permissible exposure limit) in the workplace at 0.1 mg/m over an 8-hour workday. The National Institute for Occupational Safety and Health (NIOSH) has set a Recommended exposure limit (REL) of 0.1 mg/m over an 8-hour workday. At levels of 5 mg/m, phosphorus is immediately dangerous to life and health.\n\nPhosphorus can reduce elemental iodine to hydroiodic acid, which is a reagent effective for reducing ephedrine or pseudoephedrine to methamphetamine. For this reason, red and white phosphorus were designated by the United States Drug Enforcement Administration as List I precursor chemicals under 21 CFR 1310.02 effective on November 17, 2001. In the United States, handlers of red or white phosphorus are subject to stringent regulatory controls.\n\n"}
{"id": "47334296", "url": "https://en.wikipedia.org/wiki?curid=47334296", "title": "Photovoltaics manufacturing in Malaysia", "text": "Photovoltaics manufacturing in Malaysia\n\nIn 2014, Malaysia was the world's third largest manufacturer of photovoltaics equipment, behind China and the European Union.\n\nIn 2013, Malaysia's total production capacity for solar wafers, solar cells and solar panels totalled 4,042 MW.\n\nMalaysia is a major hub for solar equipment manufacturing, with factories of companies like First Solar, Panasonic, TS Solartech, Jinko Solar, JA Solar, SunPower, Q-Cells,and SunEdison in locations like Kulim, Penang, Malacca, Cyberjaya, and Ipoh.\n\nMany international companies have the majority of production capacity located in Malaysia, such as the American company First Solar which has over 2000 MW of production capacity located in Kulim and only 280 MW located in Ohio, and German-based Q-Cells which produces 1,100 MW worth of solar cells in Cyberjaya while producing only 200 MW worth of solar cells in Germany. SunPower's largest manufacturing facility with a capacity of 1400 MW is also located in Malacca.\n\n\n"}
{"id": "34086984", "url": "https://en.wikipedia.org/wiki?curid=34086984", "title": "Plasmonic nanoparticles", "text": "Plasmonic nanoparticles\n\nPlasmonic nanoparticles are particles whose electron density can couple with electromagnetic radiation of wavelengths that are far larger than the particle due to the nature of the dielectric-metal interface between the medium and the particles: unlike in a pure metal where there is a maximum limit on what size wavelength can be effectively coupled based on the material size.\n\nWhat differentiates these particles from normal surface plasmons is that plasmonic nanoparticles also exhibit interesting scattering, absorbance, and coupling properties based on their geometries and relative positions. These unique properties have made them a focus of research in many applications including solar cells, spectroscopy, signal enhancement for imaging, and cancer treatment. As well owing to their high sensitivity appear to be good candidates for designing mechano-optical instrumentation. \n\nPlasmons are the oscillations of free electrons that are the consequence of the formation of a dipole in the material due to electromagnetic waves. The electrons migrate in the material to restore its initial state; however, the light waves oscillate, leading to a constant shift in the dipole that forces the electrons to oscillate at the same frequency as the light. This coupling only occurs when the frequency of the light is equal to or less than the plasma frequency and is greatest at the plasma frequency that is therefore called the resonant frequency. The scattering and absorbance cross-sections describe the intensity of a given frequency to be scattered or absorbed. Many fabrication processes or chemical synthesis methods exist for preparation of such nanoparticles, depending on the desired size and geometry.\n\nThe nanoparticles can form clusters (the so-called \"plasmonic molecules\") and interact with each other to form cluster states. The symmetry of the nanoparticles and the distribution of the electrons within them can affect a type of bonding or antibonding character between the nanoparticles similarly to molecular orbitals. Since light couples with the electrons, polarized light can be used to control the distribution of the electrons and alter the mulliken term symbol for the irreducible representation. Changing the geometry of the nanoparticles can be used to manipulate the optical activity and properties of the system, but so can the polarized light by lowering the symmetry of the conductive electrons inside the particles and changing the dipole moment of the cluster. These clusters can be used to manipulate light on the nano scale.\n\nThe quasistatic equations that describe the scattering and absorbance cross-sections for very small spherical nanoparticles are:\n\nformula_1\n\nformula_2\n\nwhere formula_3 is the wavenumber of the electric field, formula_4 is the radius of the particle, formula_5 is the relative permittivity of the dielectric medium and formula_5 is the relative permittivity of the nanoparticle defined by\n\nformula_7\n\nalso known as the Drude Model for free electrons where formula_5 is the plasma frequency, formula_9 is the relaxation frequency of the charge carries, and formula_10 is the frequency of the electromagnetic radiation. This equation is the result of solving the differential equation for a harmonic oscillator with a driving force proportional to the electric field that the particle is subjected to. For a more thorough derivation, see surface plasmon.\n\nIt logically follows that the resonance conditions for these equations is reached when the denominator is around zero such that\n\nformula_11\n\nWhen this condition is fulfilled the cross-sections are at their maximum.\n\nThese cross-sections are for single, spherical particles. The equations change when particles are non-spherical, or are coupled to 1 or more other nanoparticles, such as when their geometry changes. This principle is important for several applications.\n\nRigorous electrodynamic analysis of plasma oscillations in a spherical metal nanoparticle of a finite size was performed in .\n\nDue to their ability to scatter light back into the photovoltaic structure and low absorption, plasmonic nanoparticles are under investigation as a method for increasing solar cell efficiency. Forcing more light to be absorbed by the dielectric increases efficiency.\n\nPlasmons can be excited by optical radiation and induce an electric current from hot electrons in materials fabricated from gold particles and light-sensitive molecules of porphin, of precise sizes and specific patterns. The wavelength to which the plasmon responds is a function of the size and spacing of the particles. The material is fabricated using ferroelectric nanolithography. Compared to conventional photoexcitation, the material produced three to 10 times the current.\n\nIn the past 5 years plasmonic nanoparticles have been explored as a method for high resolution spectroscopy. One group utilized 40 nm gold nanoparticles that had been functionalized such that they would bind specifically to epidermal growth factor receptors to determine the density of those receptors on a cell. This technique relies on the fact that the effective geometry of the particles change when they appear within one particle diameter (40 nm) of each other. Within that range, quantitative information on the EGFR density in the cell membrane can be retrieved based on the shift in resonant frequency of the plasmonic particles.\n\nPreliminary research indicates that the absorption of gold nanorods functionalized with epidermal growth factor is enough to amplify the effects of low power laser light such that it can be used for targeted radiation treatments.\n\n"}
{"id": "4566476", "url": "https://en.wikipedia.org/wiki?curid=4566476", "title": "Pulverized coal-fired boiler", "text": "Pulverized coal-fired boiler\n\nA pulverized coal-fired boiler is an industrial or utility boiler that generates thermal energy by burning pulverized coal (also known as powdered coal or coal dust since it is as fine as face powder in cosmetic makeup) that is blown into the firebox. \n\nThe basic idea of a firing system using pulverised fuel is to use the whole volume of the furnace for the combustion of solid fuels. Coal is ground to the size of a fine grain, mixed with air and burned in the flue gas flow. Biomass and other materials can also be added to the mixture. Coal contains mineral matter which is converted to ash during combustion. The ash is removed as bottom ash and fly ash. The bottom ash is removed at the furnace bottom.\n\nThis type of boiler dominates the electric power industry, providing steam to drive large turbines. Pulverized coal provides the thermal energy which produces about 50% of the world's electric supply.\n\nPrior to the developments leading to the use of pulverized coal most boilers utilized grate firing where the fuel was mechanically distributed onto a moving grate at the bottom of the firebox in a partially crushed gravel like form. Air for combustion was blown upward through the grate carrying the lighter ash and smaller particles of unburned coal up with it, some of which would adhere to the sides of the firebox. In 1918 The Milwaukee Electric Railway and Light Company, later Wisconsin Electric, conducted tests in the use of pulverized coal at its Oneida Street power plant. These experiments helped Fred L. Dornbrook to develop methods of controlling the pulverized coal's tarry ash residues with boiler feed water tube jackets that served to reduce the surface temperature of the firebox walls and allowed the ash deposits be easily removed. This plant became the first central power station in the United States to use pulverized fuel. \n\nThe Oneida Street power plant near Milwaukee's City Hall was subsequently decommissioned and renovated in 1987. It is now the site of the Milwaukee Repertory Theatre.\n\nThe concept of burning coal that has been pulverized into a fine powder stems from the belief that if the coal is made fine enough, it will burn almost as easily and efficiently as a gas. The feeding rate of coal according to the boiler demand and the amount of air available for drying and transporting the pulverized coal fuel is controlled by computers. Pieces of coal are crushed between balls or cylindrical rollers that move between two tracks or \"races.\" The raw coal is then fed into the pulverizer along with air heated to about 650°F / 340°C from the boiler. As the coal gets crushed by the rolling action, the hot air dries it and blows the usable fine coal powder out to be used as fuel. The powdered coal from the pulverizer is directly blown to a burner in the boiler. The burner mixes the powdered coal in the air suspension with additional pre-heated combustion air and forces it out of a nozzle similar in action to fuel being atomized by a fuel injector in modern cars. Under operating conditions, there is enough heat in the combustion zone to ignite all the incoming fuel.\n\nThere are two methods of ash removal at furnace bottom:\n\nThe fly ash is carried away with the flue gas and is separated in various hoppers in the path and finally in an ESP or a bag filter.\n\nPulverized coal power plants are broken down into three categories; subcritical pulverized coal (SubCPC) plants, supercritical pulverized coal (SCPC) plants, and ultra-supercritical pulverized coal (USCPC) plants. The primary difference between the three types of pulverized coal boilers are the operating temperatures and pressures. Subcritical plants operate below the critical point of water (647.096 K and 22.064 MPa). Supercritical and ultra-supercritical plants operate above the critical point. As the pressures and temperatures increase, so does the operating efficiency. Subcritical plants are at about 37%, supercriticals at about 40% and ultra-supercriticals in the 42-45% \n\nThere are many type of pulverized coal with having different calorific values (CV), like Indonesian coal, steel grade coal (Indian coal),\n\nPulverized coal firing has been used, to a limited extent, in steam locomotives. For example, see Prussian G 12.\nIn 1929 the United States Shipping Board evaluated a pulverized coal-boiler on the \"Mercer\", a 9,500 ton merchant ship. According to their report the Mercer with its pulverized coal-boiler ran at 95% efficiency of its best oil fuelled journey. It was cheaper to operate and install than ship's boilers using oil as fuel.\n\n\n"}
{"id": "14596780", "url": "https://en.wikipedia.org/wiki?curid=14596780", "title": "Ramial chipped wood", "text": "Ramial chipped wood\n\nRamial chipped wood (RCW), also called BRF (from the French name, bois raméal fragmenté, \"chipped branch-wood\"), is a type of woodchips made solely from small to medium-sized branches. The adjective \"ramial\" refers to branches (rami). RCW is a forest product used in agriculture for mulching and soil enrichment. It may be laid on top of the soil (as in mulching), mixed into it (as a green manure), or composted first and then applied.\n\nRCW consists of the twigs and branches of trees and woody shrubs, preferably deciduous, including small limbs up to 7 cm. (2 in.) in diameter. It is processed into small pieces by chipping, and the resulting product has a relatively high ratio of cambium to cellulose compared to other chipped wood products. Thus, it is higher in nutrients and is an effective promoter of the growth of soil fungi and of soil-building in general. The goal is to develop an airy and spongy soil that holds an ideal amount of water and resists evaporation and compaction, while containing a long-term source of fertility. It can effectively serve as a panacea for depleted and eroded soils.\n\nThe raw material is primarily a byproduct of the hardwood logging industry, where it was traditionally regarded as a waste material. Research into forest soils and ecosystems at Laval University (Quebec, Canada) led to the recognition of the value of this material and to research into its uses. \n\nThe wood from heartwood and branches larger than 3 inches in diameter is not desirable due to its high C/N (carbon to nitrogen) ratio (averaging 600:1), which then requires additional nitrogen for decomposition. Only the sapwood and young branches from the various noble hardwoods (such as oak, chestnut, maple, beech, and acacia) are used because the heartwood in larger branches is high in tannin.\n\nBecause of their specific lignin, Conifers may be used only in combination with deciduous RCW, and in no greater ratio than 10 to 20%. Conifer resin has no aggradation character because it consists of derivatives of diterpenes (resin part) and monoterpenes (part turpentine). Note that only the genera Pinus, Picea, Larix and Pseudotsuga have resin canals. The cedars are characterized by their constituents of the heartwood toxic to microorganisms, tropolone derivatives (thujaplicines) phenolic nature, and are therefore to be avoided in the production of ramial chipped wood.\n\nThe acidification of soils by RCW has not been observed. In contrast, acidic soils tend to have their pH raised by RCW applications.\n\nWhile some species, such as Black Locust and Black Walnut, bear heartwood containing resins that make them resistant to rot; in practice their RCW decomposes well on a moist soil. Even Larch, which resists decomposition and is also a gymnosperm, promoted successful forest regeneration in Quebec and was found to be the best of the gymnosperms for use in RCW (even better than some hardwoods).\n\nBecause they are the most exposed part of the tree to the light, and the most actively growing, young branches (and young trees) used in RCW are from the richest parts of the trees. They contain 75% of the minerals, amino acids, proteins, phytohormones and biological catalysts (enzymes) found in the tree.\n\n"}
{"id": "2183802", "url": "https://en.wikipedia.org/wiki?curid=2183802", "title": "Redcedar bolt", "text": "Redcedar bolt\n\nRedcedar bolts are relatively small (1 foot x 1 foot x 1 foot is common) cubes of Western Redcedar which are later processed into redcedar roof shingles.\n"}
{"id": "52307694", "url": "https://en.wikipedia.org/wiki?curid=52307694", "title": "Robert Cekuta", "text": "Robert Cekuta\n\nRobert Francis Cekuta (born 1954) is a career Foreign Service Officer and the current U.S. Ambassador to Azerbaijan.\n\nCekuta attended Georgetown University’s School of Foreign Service, graduating in 1976 with a B.S. He then went to the Thunderbird School of Global Management, earning a master’s degree in international marketing in 1978. He later earned another master's degree in national security strategies from the National War College.\n\nCekuta joined the U.S. Foreign Service in 1978 and his early assignments included Vienna, Austria; Baghdad, Iraq; Johannesburg, South Africa; and Sana’a, Yemen. He also directed a task force in Kosovo during the conflict there and served in the Bureau of Near East and South Asian Affairs. From 1996 to 1999, he was deputy chief of mission in the U.S. Embassy in Tirana, Albania.\n\nMuch of Cekuta’s career has focused on business and trade issues. In 1999, he was senior advisor to the Office of the U.S. Trade Representative and in 2000 he was named director of Economic Policy Analysis and Public Diplomacy in the State Department. Cekuta in 2002 was named director of the Iraq Economic Group in the Bureau of Economic and Business Affairs. In 2002, he was also the bureau's special negotiator for biotechnology. Beginning in 2003, Cekuta was economic minister-counselor at the embassy in Berlin and in 2007 he was sent to Tokyo as the minister-counselor for economic affairs.\n\nCekuta came home in 2010, first as senior advisor for food security in the State Department and later that year as Deputy Assistant Secretary of State for Energy, Sanctions and Commodities. One of his more prominent roles involved working with the jewelry industry on compliance with regulations on conflict diamonds and gold.\n\nIn 2011, Cekuta became the Principal Deputy Assistant Secretary of State in the Bureau of Energy Resources. In this capacity, he acted as a point man for the State Department’s views on the proposed Keystone XL pipeline.\n\nCekuta was nominated by President Barack Obama on July 8, 2014 to be U.S. ambassador to Azerbaijan. Cekuta testified before the Senate Foreign Relations Committee on September 17, 2014, and was confirmed on December 16.\n\nCekuta presented his credentials to President Ilham Aliyev on February 19, 2015.\n\nCekuta and his wife, Anne, have three children. In addition to English he speaks German, Arabic, and Albanian.\n\n"}
{"id": "42812764", "url": "https://en.wikipedia.org/wiki?curid=42812764", "title": "Selection shadow", "text": "Selection shadow\n\nThe selection shadow is a concept involved with the evolutionary theories of ageing that states that selection pressures on an individual decrease as an individual ages and passes sexual maturity, resulting in a \"shadow\" of time where selective fitness is not considered. Over generations, this results in maladaptive mutations that accumulate later in life due to aging being non-adaptive toward reproductive fitness. The concept was first worked out by J. B. S. Haldane and Peter Medawar in the 1940s, with Medawar creating the first graphical model.\n\nThe model developed by Medawar states that due to the dangerous conditions and pressures from the environment, including predators and diseases, most individuals in the wild die not long after sexual maturity. Therefore, there is a low probability for individuals to survive to an advanced age and suffer the effects related to aging. In conjunction with this, the effects of natural selection decrease as age increases, so that later individual performance is ignored by selection forces. This results in beneficial mutations not being selected for if they only have a positive result later in life, along with later in life deleterious mutations not being selected against. Due to the fitness of an individual not being affected once it is past its reproductive prime, later mutations and effects are considered to be in the \"shadow\" of selection.\n\nThis concept would later be adapted into Medawar's 1952 mutation accumulation hypothesis, which was itself expanded upon by George C. Williams in his 1957 antagonistic pleiotropy hypothesis.\n\nA classical requirement and constraint of the model is that the number of individuals within a population that live to reach senescence must be small in number. If this is not true for a population, then the effects of old age will not be under a selection shadow and instead affect adaptation and evolution of the population as a whole. At the same time, however, this requirement has been challenged by increasing evidence of senescence being more common in wild populations than previously expected, especially among birds and mammals, while the effects of the selection shadow remain present.\n\nSome scientists, however, have criticized the idea of aging being non-adaptive, instead adopting the theory of \"Death by Design\". This theory follows the work of August Weismann, which states that aging specifically evolved as an adaptation, and disagrees with Medawar's model as a perceived oversimplification of the impact older organisms have on evolution. It is also claimed that older organisms have a higher reproductive capacity due to being better fit in order to reach their age, rather than their capacity being equal as in Medawar's calculations.\n"}
{"id": "275606", "url": "https://en.wikipedia.org/wiki?curid=275606", "title": "Tornado watch", "text": "Tornado watch\n\nA tornado watch (SAME code: TOA) is issued when weather conditions are favorable for the development of severe thunderstorms called a supercell that are capable of producing tornadoes. A tornado watch therefore implies that it is also a severe thunderstorm watch. A tornado watch must not be confused with a tornado warning. In most cases, the potential exists for large hail and/or damaging winds in addition to tornadoes.\n\nA watch does not mean that the severe weather is actually occurring, only that atmospheric conditions have created a significant risk for it. If severe weather actually does occur, a tornado warning or severe thunderstorm warning would then be issued. Note that a watch is not required for a warning to be issued; tornado warnings are occasionally issued when a tornado watch is not active (i.e. when a severe thunderstorm watch is active, or when no watches are in effect), if a severe thunderstorm develops and has a confirmed tornado or strong rotation.\n\nIn the United States, the Storm Prediction Center – a national guidance center of the National Weather Service (NWS) – issues watches for areas likely to produce tornadoes and severe thunderstorms. The watch boxes (or weather watches, WWs) are usually outlined in the format of \"x\" miles north and south, or east and west, or either side of a line from \"y\" miles \"direction\" of \"city, state\", to \"z\" miles \"another direction\" of \"another city, state\". For example: \"50 miles either side of a line from 10 miles northeast of Columbia, South Carolina to 15 miles south-southwest of Montgomery, Alabama\" (\"Either side\" means perpendicular to the center line). In addition, a list of all counties included in its area of responsibility is now issued by each NWS forecast office for each watch.\n\nIn the event that a tornado watch is likely to lead to a major tornado outbreak along with possible destructive winds and hail, enhanced wording with the words Particularly Dangerous Situation (PDS) can be added to the watch; this is occasionally issued when atmospheric conditions are deemed sufficient to breed development of severe thunderstorms that are capable of producing significant tornadoes (either from supercells that develop strong, low-level rotation under typical tornadogenesis during the storm's maturation stage, or by thunderstorms that initiate mesocyclone maturation during their early development through a combination of sufficient wind shear and very high convective available potential energy (CAPE) values). Occasionally, a tornado watch may replace a severe thunderstorm watch (or a portion of one) should conditions that were originally forecast to be conducive for non-tornadic severe thunderstorms change to allow possible tornado formation.\n\nIn Canada, the criteria used to issue a tornado watch are the same and watches are issued by regional offices of the Meteorological Service of Canada of Environment Canada in Vancouver, Edmonton, Toronto, Montreal and Halifax, on a county or regional basis.\n\nThe term \"red box\" refers to the assigned coloring used for watch box outlines used in Storm Prediction Center and National Weather Service products; television stations typically assign other colors (colors used may vary, but very seldom if ever red, which is usually reserved for identifying tornado warnings; green is the most commonly used to denote them) to highlight tornado watches in severe weather advisory displays.\n\nWatch Outline Updates are relayed (and at the initial watch issuance, issued) by the Storm Prediction Center, however it is the local National Weather Service Weather Forecast Offices that decide what counties (in their warning area) are included or excluded in the watch, via a conference call with the SPC. As a result, watch products will sometimes display counties inside the watch outline that are not included in the counties listed, and vice versa; however the local Weather Forecast Office will need to expand to add these counties into the watch. A Watch Status Message works in a similar fashion; the SPC designates which areas it thinks where a threat still exists (the most common designation for this is on the basis of the location of surface features such as cold fronts and drylines that would delineate where the threat of severe thunderstorms has ended and where it will remain a possibility), and the NWS offices decide what counties to remove from the watch (the local offices will almost always follow the SPC recommendation on the status messages).\n\nIf conditions are no longer favorable for tornadoes in the watch area (either because atmospheric conditions that earlier supported tornadic development have become less conducive for them or were not present compared to predictions in earlier forecasts to form tornadoes), the tornado watch may either be replaced by a severe thunderstorm watch or cancelled outright; if no thunderstorm activity occurs, this leads to a tornado watch \"bust\", which may also factor into the Storm Prediction Center's decision as to whether to cancel the watch.\n\n URGENT – IMMEDIATE BROADCAST REQUESTED\n\n BULLETIN – IMMEDIATE BROADCAST REQUESTED\n\n"}
{"id": "247326", "url": "https://en.wikipedia.org/wiki?curid=247326", "title": "Volcanic glass", "text": "Volcanic glass\n\nVolcanic glass is the amorphous (uncrystallized) product of rapidly cooling magma. Like all types of glass, it is a state of matter intermediate between the close-packed, highly ordered array of a crystal and the highly disordered array of gas. Volcanic glass can refer to the interstitial, or matrix, material in an aphanitic (fine grained) volcanic rock or can refer to any of several types of vitreous igneous rocks. Most commonly, it refers to obsidian, a rhyolitic glass with high silica (SiO) content.\n\nOther types of volcanic glass include:\n"}
{"id": "13731710", "url": "https://en.wikipedia.org/wiki?curid=13731710", "title": "Windbelt", "text": "Windbelt\n\nThe Windbelt is a wind power harvesting device invented by Shawn Frayn in 2004 for converting wind power to electricity. It consists of a flexible polymer ribbon stretched between supports transverse to the wind direction, with magnets glued to it. When the wind blows across it, the ribbon vibrates due to aeroelastic flutter, similar to the action of an aeolian harp. The vibrating movement of the magnets induces current in nearby pickup coils by electromagnetic induction.\n\nOne prototype has powered two LEDs, a radio, and a clock (separately) using wind generated from a household fan. The cost of the materials was well under US$10. $2–$5 for 40 mW is a cost of $50–$125 per watt.\n\nThere are three sizes in development:\n\nThe Windbelt's inventor, Shawn Frayn, was a winner of the 2007 Breakthrough Award from the publishers of the magazine, \"Popular Mechanics\". He is trying to make the Windbelt cheaper.\n\nThe inventor's claims that the device is 10 - 30 times more efficient than small wind turbines have been refuted by tests. The microWindbelt could generate 0.2 mW at a wind speed of 3.5 m/s and 5 mW at 7.5 m/s, which represent efficiencies (ηC) of 0.21 and 0.53 respectively. Wind turbines typically have efficiencies of 1% to 10%. Since the Windbelt a number of other \"flutter\" wind harvester devices have been designed, but like the Windbelt almost all have efficiencies below turbine machines.\n\n\n"}
{"id": "12744625", "url": "https://en.wikipedia.org/wiki?curid=12744625", "title": "Workbench (woodworking)", "text": "Workbench (woodworking)\n\nA workbench is a table used by woodworkers to hold workpieces while they are worked by other tools. There are many styles of woodworking benches, each reflecting the type of work to be done or the craftsman's way of working. Most benches have two features in common: they are heavy and rigid enough to keep still while the wood is being worked, and there is some method for holding the work in place at a comfortable position and height so that the worker is free to use both hands on the tools. The main thing that distinguishes benches is the way in which the work is held in place. Most benches have more than one way to do this, depending on the operation being performed.\n\nProbably the oldest and most basic method of holding the work is a planing stop or dog ear, which is simply a peg or small piece of wood or metal that stands just above the surface at the end of the bench top. The work is placed on the bench with the end pushed against the stop. The force of the planing keeps the board in place, so long as the force is always toward the stop. Planing against a stop gives the woodworker good feedback - he can tell a lot about what is going on just by the pressure, force and balance required. A stop can take the form of a batten attached to the end of the bench, or it can be adjustable, able to be moved up and down according to the size of the work - or pushed down below the surface when not needed. A simple bench dog can serve as a planing stop.\n\nAnother ancient method of holding the work is the hold fast or holdfast. A holdfast looks like a shepherd's crook. The shank goes into a hole in the bench top and the tip of the hook is pressed against the work from above. The holdfast is set by rapping the top with a mallet, and released by hitting the back side. A good holdfast works remarkably well, and is inexpensive and easy to install.\n\nThe holdfast can also be used for clamping work to the side of the bench for jointing. If the legs on your base are not too far under the top, simply bore a hole in the side of the leg and use the holdfast horizontally. A woodworker can do just about anything he needs on a bench with only a planing stop and a holdfast or two.\n\nIt is common to have holes in the benchtop that tools or jigs can be bolted to. In applications where repeated removal and reinstallation of the tool or jig is desirable, screwing into the wood of the benchtop with woodscrews or lag bolts is not an ideal solution, because the wooden threads don't lend themselves to repeated disassembly and reassembly. In such cases, it is useful to create hardpoints, which are metal threads embedded in the wood. These hardpoints make repeated disassembly and reassembly trouble-free. They are essentially nuts that are embedded into the wood in one way or another. T-nuts (aka tee nuts) are an easy way to create a hardpoint. Custom nuts similar to T-nuts but with holes for woodscrews in place of the spikes are sometimes machined for the purpose.\n\nLong ago, just as today, woodworkers required a more helpful way to keep the wood in place while it was being worked. A device was needed that could be used effectively on different sizes of wood. Probably the first such device used two stops - at least one of which was adjustable for position - and wedges between them and the work to fix it in place. This is still a cheap and effective method for holding the work.\n\nA screw is really just a wedge in the round. Today, most vises use a big screw to apply the clamping force. The vise is often used to hold objects in place when working on a piece.\n\nThere are two main categories of vise (\"vice\" in UK English sp.) : vises on the end of the bench and vises on the front of the bench. End vises (also called 'tail vises') are usually mounted on the right side of the bench for right-handed workers. They can typically hold work in two ways: between the jaws and along the top of the bench using moveable 'dogs' in place of jaws. Not all benches have tail vises. A front vise (also called 'face vise' or 'shoulder vise') is typically mounted on the left front side of the bench. They may be used for holding a board to be edge jointed, or sometimes for sawing out dovetails and the like.\n\nProbably the oldest front vise design is the leg vise. It's called a leg vise because one of the bench's legs is an integral part of it - usually forming the inside jaw. The outside jaw also goes all the way to the floor - or nearly so. There is a single screw mounted between a quarter and a third of the way down that goes through both jaws with the nut on the back of the leg. Finally, there is some sort of horizontal beam at the bottom to act as a fulcrum. This beam may take the form of a board that can be adjusted by means of holes and pegs, or it can even be another screw. The leg vise is probably the simplest and least expensive of the front vises, and it is very strong.\n\nAnother old design is the shoulder vise. The best thing about this design is that it allows clamping directly behind the screw. This yields unobstructed vertical clamping for cutting dovetails and similar operations. There is also typically a little play in the screw/jaw attachment that provides for clamping of tapered work. This is one vise that should be designed into the bench from the beginning, as it is difficult to retrofit into an existing bench. The primary drawback of the shoulder vise is its fragility, unless the \"arm\" is attached to the \"end cap\" using a dovetail or finger joint, usually glued or \"pinned\" to eliminate rotative movement about the joint, otherwise it is fairly easy to break it with a big steel bench screw. But one should never really have to put \"that\" much force on it. Some woodworkers say that the big vise gets in the way of some jobs, others find it unobtrusive. Implicit in a shoulder vise is an integral planing stop, formed by the intersection of the jaw and the jaw spacer, and which allows the shoulder vise to perform multiple duties, such as jointing long boards with a \"bench slave\" to hold the opposite end. In earlier times, a \"crochet\" and a holdfast would perform the same function.\n\nMany of the commercial European benches have a front vise that uses a wooden jaw with a metal screw and built-in anti-racking hardware. These vises are also available as inexpensive kits that can be mounted on almost any bench.\n\nPerhaps the easiest face vise to install is the self-contained iron vise, sometimes called the 'quick-action' vise (except they are not all quick-action). This tool comes already assembled and only has to be mounted to the bench. Usually, auxiliary wooden jaws are added. The quick-action feature makes setting it much quicker and is quickly taken for granted. Not only are these vises easy to install and use, they are also robust. Their main drawback is the relatively high cost.\n\nThe patternmaker's vise is sometimes used as a front vise. This style was originally designed for patternmakers, the folks who make the forms used in metal casting. Pattern making is exacting work using shapes not normally encountered by a cabinetmaker. The patternmaker's vise can hold odd shapes at various angles, and it can certainly hold simple shapes at regular angles. The drawbacks of this vise are the expense, the moderately complicated mounting, and a tendency to fragility. The most sought-after is an antique Emmert, but there are several clones on the market today, including one by Lee Valley Tools that is made of an aluminum alloy—which should be less likely to break—and several from Taiwan and which are clones of the smaller Emmert. A possible disadvantage of the patternmaker's vise is it usually requires installation on a bench which is at least 1-3/4\" thick\n\nThis is another old design making a comeback thanks to Lee Valley's Veritas Toolworks. The twin-screw vise was popular during the late eighteenth and early nineteenth centuries, particularly with chair makers. The updated Veritas design uses a chain to connect the two screws, keeping them slaved to each other. There is also a provision for decoupling the screws so that tapered work can be held. This design has many of the advantages of the classic shoulder vise and single screw face vise, with few of the disadvantages. It can also be used effectively as an end vise. The main drawbacks of the twin-screw vise are the expense and the relatively difficult installation.\n\nThe traditional tail vise uses one large screw, either wooden or metal. It is made in the form of a frame, with the back part of the frame fitting under the bench, and the movement of that frame located and restrained by a complex system of sliding tongues and grooves, and runners, such that smooth left and right movements of the frame are possible, but forward and backward movements, or rotative movements of the frame are impossible. The jaw has a face that contacts the bench top, and it has one or more dog holes on the top—often 3 to 4, each spaced 5 inches apart—that are in line with the dog holes located on the front face (apron) of the bench—numerous holes, each also spaced 5 inches apart. This is the least expensive option for a tail vise, but it is by far the most complex to design, construct and maintain. Tage Frid and Frank Klausz popularized this type of tail vise in North America, although its origin dates back to northern Europe (most probably Germany) in the 18th century.\n\nThis traditional tail vise also uses one large screw, either wooden or metal. It consists of a movable block with one or more dog holes in it, the movable block rides in a large mortise in the workbench. The jaw has a face that contacts the bench top, and the dog holes are in line with the dog holes on the bench top. The two main varieties of this vise depend on whether the screw nut is mounted in the bench or on the dog hole block. When the screw nut is mounted on the dog hole block the installation is more complicated and expensive, but the screw does not move in and out as the vise is used.\n\nA newer form of tail vise does away with the need of a frame. It uses steel plates for its structure - one steel plate with the nut is mounted on the side of the bench, two others are built into a sliding jaw along with the bench screw. This is a robust design and it's easier to install and adjust than the older style. However, only a few sizes are commercially available (although larger sizes have been custom made).\n\nSome bench designers have adapted face vises for use as tail vises - with differing levels of success. Unfortunately, we are most likely to find the continental style vise used this way, and it's really least suited to the task. When used as a tail vise it has a strong tendency to \"wrack\" (twist or distort) because of the side forces. It isn't long before the hardware begins to show wear.\n\nThe steel quick-action vise doesn't suffer so much from this problem. With one exception, it functions well on the end of the bench. Its main drawback as a tail vise is the distance of the dog from the edge of the vise. Ideally, the dog hole strip should be fairly close to the edge of the bench. This puts your weight more directly over the work and behind the plane, enabling you to put more power and control into the operation with less strain. It is also important to keep the dog holes near the edge so that fenced planes can easily be used. With even a small quick-action vise the dog hole strip is still pretty far from the edge. So if you decide to use a quick-action vise as a tail vise, get the smallest good one you can find.\n\nThe twin-screw vise marketed by Lee Valley works well as a tail vise - that's really what it's designed for. The old wooden twin-screw design isn't suited for this task because there is no facility for holding the offside jaw open.\n\nMost workbenches are made from solid wood; the most expensive and desirable are made of solid hardwood. Benches may also be made from plywood and Masonite or hardboard, and bases of treated pine and even steel. There are trade offs with the choice of construction material. Solid wood has many advantages including strength, workability, appearance. A plywood or hardboard bench top has the advantage of being stable, relatively inexpensive, and in some ways it's easier to work with—particularly for a woodworker who doesn't yet have hand tools. The practical drawbacks of a plywood or composite bench top are that they don't hold their corners and edges well, and they can't be resurfaced with a plane—something that is needed from time to time.\n\nWorkbenches are fairly forgiving in the choice of wood. Maple, cherry, mahogany, or pine rarely give problems. Beech, oak, walnut, and fir make good benches. Benches are occasionally made using more exotic woods like purpleheart and teak, though the cost is high. The choice of wood is not as important as the integrity of the design—cross grain construction and inadequate joinery typically have a more destructive effect than the use of a less-than-ideal wood.\n\nOne popular and cheap source for bench top material is old bowling alley lanes. These are usually made from thick, high-quality laminated maple. Two problems present themselves with bowling alley wood: first, the waxes used on the surface for bowling frequently contain silicone and other substances that can play havoc with work pieces at finishing time—a little silicone on a project will cause trouble with many finishes, and won't manifest itself until it's too late. The other problem with bowling alley wood is nails. Most pieces have loads of nails buried in them, which do not mix well with woodworking tools. Such nails may be mitigated by using a metal detecting wand during stock preparation.\n\nMany benches use different species of woods together. Small business woodworkers who work in a store-front sometimes use various species so that their clients can see examples of the different woods in a finished state. If this is done, it is important to use woods that are compatible with each other, particularly in the area of relative movement. Otherwise changes in temperature and humidity will stress the structure out of shape or it may even break.\n\nThe most common use for exotic woods in bench construction is for auxiliary parts such as oil cups, bench dogs, vise handles, sawing stops and the parts of wooden vises.\n\nThe optimum size of a bench depends on the work to be done, space considerations, and budget. In general, bigger is better - though most woodworkers find that most work is done on the front few inches of the top, and then mostly in the front vise or right around the tail vise. So a smaller, narrow bench isn't as much of a drawback as might be expected - and it is far better than no bench at all. Tage Frid's classic bench is relatively small and it is one of the most copied designs. A big disadvantage of a smaller bench is that they are usually too light to resist heavy work without skidding around - but this problem can be overcome by attaching the bench to the floor.\n\nWoodworkers seem to be evenly divided on the subject of bench positioning. Some like to be able to access their benches from all sides, while others like their bench against a wall. The advantage of wall placement - besides the saved space - is that tools can be stored on the wall over the bench, within easy reach. This keeps the tool storage out of the way, and the tools can still be reached without turning around or bending down.\n\nA workbench base should support the top so that it doesn't move, while at the same time keeping clear of the work. There are two main types: open bases and bases with built in storage. Open bases are easier to build and there is less chance of the base hindering the work - plus, it is usually necessary to compromise the strength and rigidity of a base in order to accommodate storage.\n\nProbably the most popular style is the sled-foot trestle base. With this design, each pair of legs is put together in the form of an 'I' with two vertical bars. The leg pairs are connected by a pair of stretchers. These stretchers can be permanently fixed to the leg-pairs, or they can be made removable with tusk tenons or a bed-bolt arrangement. One of the advantages of this style is that there is no end-grain resting on the floor, so the legs are not as prone to wick-up moisture and rot.\n\nAnother popular style is a simple post and rail table structure. This is probably best implemented in heavy gauge steel, as wood doesn't really give enough resistance to the side forces that develop during heavy work. Most woodworkers who use this style with wood end up making another base before very long.\n\nA hybrid design of the sled-foot trestle and the post and rail styles can work well. Instead of an 'I' structure, the sled foot is moved up to become a rail - sort of an 'H' with a bar across the top. This puts end-grain on the floor, but it is otherwise a strong design and somewhat easier to build. Plus, the feet don't get in the way of the work as sled-feet sometimes do.\n\nCast iron leg kits are available for woodworkers who do not want to design and build their own base.\n\n"}
