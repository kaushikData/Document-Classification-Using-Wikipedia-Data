{"id": "35633531", "url": "https://en.wikipedia.org/wiki?curid=35633531", "title": "2061: An Exceptional Year", "text": "2061: An Exceptional Year\n\n2061 – An exceptional year () is a 2007 Italian action comedy film directed by Carlo Vanzina.\n\nItaly, year 2061: After a terrible energy crisis due to the depletion of oil stocks, the Earth is plunged into a kind of new Middle Ages. Italy as a nation no longer exists, the peninsula has returned to being a divided country, almost pre-Risorgimento, where now reigns the political situation is similar to that before the reunification of 1861:\n\nJust from the South, a group of adventurous patriots of the insurrectional movement \"Young Italy\", made up of volunteers Tony, Pride, Grosso, Salvim and Taned, and led by Ademaro Maroncelli (teacher at the Classical Lyceum Massimo D'Alema of Turin) undertakes a difficult journey to the Piedmont, in order, two hundred years later, to reunify Italy.\n\n"}
{"id": "13480981", "url": "https://en.wikipedia.org/wiki?curid=13480981", "title": "Abdullah bin Hamad Al Attiyah", "text": "Abdullah bin Hamad Al Attiyah\n\nAbdullah bin Hamad Al Attiyah ( ; , born 1951) is the former deputy prime minister of Qatar and the head of the Emir's court.\n\nAttiyah was born in 1951 or 1952. In 1976, he graduated from the University of Alexandria, Egypt with a bachelor's degree.\n\nAttiyah started his career in 1972 with the ministry of finance and petroleum of Qatar. From 1973 to 1986, he held a post of the head of international and public relations at the ministry. From 1986 to 1989, he served as the director of the office of the minister, and from 1989 to 1992, as the director of the office of the minister of interior and as the acting minister of finance and petroleum. From September 1992 to January 2011, Attiyah was the minister of energy and industry. On 12 January 1999, he also assumed the responsibility for electricity and water issues as these sectors were merged into the ministry of energy and industry. On 16 September 2003 he was appointed second deputy prime minister and on 3 April 2007, deputy prime minister. On 18 January 2011 he was named head of the Amiri Diwan while remaining in the post of the deputy prime minister. In the post of minister of industry and energy he was replaced by Mohammed Saleh Al Sada.\n\nSince 1975, Attiyah has been the director of the Gulf Helicopters Corporation. From 1987 to 1995, he served as the deputy chairman of QTel. Since 1986, he has been member of the directors board of Gulf Airways Corporation and since 1992, the director of Qatar Petroleum.\n\nOn 23 November 1993, Attiyah was named OPEC president and a member of the OPEC's quota compliance committee. On 30 June 2009, at the eight ministerial meeting of the Gas Exporting Countries Forum in Doha, Attiyah was elected as the chairman of the organization. Although Gas Exporting Countries Forum has seen by some experts as an attempt to form 'gas-OPEC', Attiyah ruled out a creation of OPEC-like cartel.\n\nIn 2011, Attiyah was appointed head of former Emir Hamad Al Thani's office and president of the Qatar Administrative \nControl and Transparency Authority.\n\nDuring the 2012 United Nations Climate Change Conference in Doha, Attiyah served as the chairman.\n\nIn 2007, London-based the British Petroleum Intelligence Bulletin chose Attiyah as the Man of the Year in the field of development of hydrocarbon industry.\n\nIn 2011, Texas A&M University awarded Attiyah with an Honorary Degree. \n\nAttiyah is married and has six children. His interests are reading, fishing and radio communications.\n"}
{"id": "14773174", "url": "https://en.wikipedia.org/wiki?curid=14773174", "title": "Cirrus intortus cloud", "text": "Cirrus intortus cloud\n\nCirrus intortus is a variety of cirrus cloud. The name \"cirrus intortus\" is derived from Latin, meaning \"twisted, wound\". The variety of intortus clouds is specific to cirrus clouds, and they appear as interwound strands of cirrus clouds with a purely random pattern. The filaments are often curved in a very irregular pattern.\n\nLike other cirrus clouds, cirrus intortus occur at high altitudes.\n\n\n"}
{"id": "567472", "url": "https://en.wikipedia.org/wiki?curid=567472", "title": "Cloud condensation nuclei", "text": "Cloud condensation nuclei\n\nCloud condensation nuclei or CCNs (also known as cloud seeds) are small particles typically 0.2 µm, or 1/100th the size of a cloud droplet on which water vapor condenses. Water requires a non-gaseous surface to make the transition from a vapour to a liquid; this process is called condensation. In the atmosphere, this surface presents itself as tiny solid or liquid particles called CCNs. When no CCNs are present, water vapour can be supercooled at about −13°C (8°F) for 5–6 hours before droplets spontaneously form (this is the basis of the cloud chamber for detecting subatomic particles). In above freezing temperatures the air would have to be supersaturated to around 400% before the droplets could form.\n\nThe concept of cloud condensation nuclei is used in cloud seeding, that tries to encourage rainfall by seeding the air with condensation nuclei. It has further been suggested that creating such nuclei could be used for marine cloud brightening, a climate engineering technique.\n\nA typical raindrop is about 2 mm in diameter, a typical cloud droplet is on the order of 0.02 mm, and a typical cloud condensation nucleus (aerosol) is on the order of 0.0001 mm or 0.1 µm or greater in diameter. The number of cloud condensation nuclei in the air can be measured and ranges between around 100 to 1000 per cubic centimetre. The total mass of CCNs injected into the atmosphere has been estimated at 2x10 kg over a year's time.\n\nThere are many different types of atmospheric particulates that can act as CCN. The particles may be composed of dust or clay, soot or black carbon from grassland or forest fires, sea salt from ocean wave spray, soot from factory smokestacks or internal combustion engines, sulfate from volcanic activity, phytoplankton or the oxidation of sulfur dioxide and secondary organic matter formed by the oxidation of volatile organic compounds. The ability of these different types of particles to form cloud droplets varies according to their size and also their exact composition, as the hygroscopic properties of these different constituents are very different. Sulfate and sea salt, for instance, readily absorb water whereas soot, organic carbon and mineral particles do not. This is made even more complicated by the fact that many of the chemical species may be mixed within the particles (in particular the sulfate and organic carbon). Additionally, while some particles (such as soot and minerals) do not make very good CCN, they do act as ice nuclei in colder parts of the atmosphere.\n\nThe number and type of CCNs can affect the precipitation amount, lifetimes and radiative properties of clouds as well as the amount and hence have an influence on climate change; details are not well understood but are the subject of research. There is also speculation that solar variation may affect cloud properties via CCNs, and hence affect climate.\n\nSulfate aerosol (SO and methanesulfonic acid droplets) act as CCNs. These sulfate aerosols form partly from the dimethyl sulfide (DMS) produced by phytoplankton in the open ocean. Large algal blooms in ocean surface waters occur in a wide range of latitudes and contribute considerable DMS into the atmosphere to act as nuclei. The idea that an increase in global temperature would also increase phytoplankton activity and therefore CCN numbers was seen as a possible natural phenomenon that would counteract climate change. An increase of phytoplankton has been observed by scientists in certain areas but the causes are unclear.\n\nA counter-hypothesis is advanced in \"The Revenge of Gaia\", the book by James Lovelock. Warming oceans are likely to become stratified, with most ocean nutrients trapped in the cold bottom layers while most of the light needed for photosynthesis in the warm top layer. Under this scenario, deprived of nutrients, marine phytoplankton would decline, as would sulfate cloud condensation nuclei, and the high albedo associated with low clouds. This is known as the CLAW hypothesis (named after the authors' initials of a 1987 \"Nature\" paper) but no conclusive evidence to support this has yet been reported.\n\n\n"}
{"id": "5381096", "url": "https://en.wikipedia.org/wiki?curid=5381096", "title": "Complex reflection group", "text": "Complex reflection group\n\nIn mathematics, a complex reflection group is a finite group acting on a finite-dimensional complex vector space that is generated by complex reflections: non-trivial elements that fix a complex hyperplane pointwise.\n\nComplex reflection groups arise in the study of the invariant theory of polynomial rings. In the mid-20th century, they were completely classified in work of Shephard and Todd. Special cases include the symmetric group of permutations, the dihedral groups, and more generally all finite real reflection groups (the Coxeter groups or Weyl groups, including the symmetry groups of regular polyhedra).\n\nA (complex) reflection \"r\" (sometimes also called pseudo reflection or unitary reflection) of a finite-dimensional complex vector space \"V\" is an element formula_1 of finite order that fixes a complex hyperplane pointwise. I.e., the fixed-space formula_2 has codimension 1.\n\nA (finite) complex reflection group formula_3 is a finite subgroup of formula_4 that is generated by reflections.\n\nAny real reflection group becomes a complex reflection group if we extend the scalars from \nR to C. In particular all Coxeter groups or Weyl groups give examples of complex reflection groups.\n\nA complex reflection group \"W\" is irreducible if the only \"W\"-invariant proper subspace of the corresponding vector space is the origin. In this case, the dimension of the vector space is called the rank of \"W\".\n\nThe Coxeter number formula_5 of an irreducible complex reflection group \"W\" of rank formula_6 is defined as formula_7 where formula_8 denotes the set of reflections and formula_9 denotes the set of reflecting hyperplanes.\nIn the case of real reflection groups, this definition reduces to the usual definition of the Coxeter number for finite Coxeter systems.\n\nAny complex reflection group is a product of irreducible complex reflection groups, acting on the sum of the corresponding vector spaces. So it is sufficient to classify the irreducible complex reflection groups.\n\nThe irreducible complex reflection groups were classified by . They found an infinite family \"G\"(\"m\",\"p\",\"n\") depending on 3 positive integer parameters (with \"p\" dividing \"m\"), and 34 exceptional cases, that they numbered from 4 to 37, listed below. The group\n\"G\"(\"m\",\"p\",\"n\"), of order \"m\"\"n\"!/\"p\", is the semidirect product of the abelian group\nof order \"m\"/\"p\" whose elements are (θ,θ, ...,θ), by the symmetric group \"S\" acting by permutations of the coordinates, where θ is a primitive \"m\"th root of unity and Σ\"a\"≡ 0 mod \"p\"; it is an index \"p\" subgroup of the generalized symmetric group formula_10\n\nSpecial cases of \"G\"(\"m\",\"p\",\"n\"):\n\nThere are a few duplicates in the first 3 lines of this list; see the previous section for details.\nFor more information, including diagrams, presentations, and codegrees of complex reflection groups, see the tables in .\n\nShephard and Todd proved that a finite group acting on a complex vector space is a complex reflection group if and only if its ring of invariants is a polynomial ring (Chevalley–Shephard–Todd theorem). For formula_11 being the \"rank\" of the reflection group, the degrees formula_12 of the generators of the ring of invariants are called \"degrees of W\" and are listed in the column above headed \"degrees\". They also showed that many other invariants of the group are determined by the degrees as follows:\n\n\nFor formula_11 being the \"rank\" of the reflection group, the codegrees formula_15 of W can be defined by\nformula_16\n\n\nBy definition, every complex reflection group is generated by its reflections. The set of reflections is not a minimal generating set, however, and every irreducible complex reflection groups of rank has a minimal generating set consisting of either or reflections. In the former case, the group is said to be \"well-generated\".\n\nThe property of being well-generated is equivalent to the condition formula_17 for all formula_18. Thus, for example, one can read off from the classification that the group is well-generated if and only if \"p\" = 1 or \"m\".\n\nFor irreducible well-generated complex reflection groups, the \"Coxeter number\" defined above equals the largest degree, formula_19. A reducible complex reflection group is said to be well-generated if it is a product of irreducible well-generated complex reflection groups. Every finite real reflection group is well-generated.\n\nThe well-generated complex reflection groups include a subset called the \"Shephard groups\". These groups are the symmetry groups of regular complex polytopes. In particular, they include the symmetry groups of regular real polyhedra. The Shephard groups may be characterized as the complex reflection groups that admit a \"Coxeter-like\" presentation with a linear diagram. That is, a Shephard group has associated positive integers and such that there is a generating set satisfying the relations\nand\n\nThis information is sometimes collected in the Coxeter-type symbol , as seen in the table above.\n\nAmong groups in the infinite family , the Shephard groups are those in which . There are also 18 exceptional Shephard groups, of which three are real.\n\nAn extended Cartan matrix defines the Unitary group. Shephard groups of rank \"n\" group have \"n\" generators.\n\nOrdinary Cartan matrices have diagonal elements 2, while unitary reflections do not have this restriction.\n\nFor example, the rank 1 group, p[], , is defined by a 1×1 matrix [1-formula_24].\n\nGiven: formula_25.\n\n\n"}
{"id": "1142686", "url": "https://en.wikipedia.org/wiki?curid=1142686", "title": "Cordtex", "text": "Cordtex\n\nCordtex is a type of detonating cord generally used in mining. It uses an explosive core of pentaerythritol tetranitrate (PETN) inside its plastic coating. \n\nIt is commonly the thickness of electrical extension cord and has a velocity of detonation of approximately 6000–7000 metres per second. It is used to \"daisychain\" a sequence of explosives together.\n\nIt can also be used in short lengths in simple boobytraps and early warning devices.\n\n"}
{"id": "8769764", "url": "https://en.wikipedia.org/wiki?curid=8769764", "title": "Cryochemistry", "text": "Cryochemistry\n\nCryochemistry is the study of chemical interactions at temperatures below . It is derived from the Greek word \"cryos\", meaning 'cold'. It overlaps with many other sciences, including chemistry, cryobiology, condensed matter physics, and even astrochemistry.\nCryochemistry has been a topic of interest since liquid nitrogen, which freezes at −210°C, became commonly available. Cryogenic-temperature chemical interactions are an important mechanism for studying the detailed pathways of chemical reactions by reducing the confusion introduced by thermal fluctuations. Cryochemistry forms the foundation for cryobiology, which uses slowed or stopped biological processes for medical and research purposes.\n\nAs a material cools, the relative motion of its component molecules/atoms decreases - its temperature decreases. Cooling can continue until all motion ceases, and its kinetic energy, or energy of motion, disappears. This condition is known as absolute zero and it forms the basis for the Kelvin temperature scale, which measures the temperature above absolute zero. Zero degrees Celsius (°C) coincides with 273 Kelvin.\n\nAt absolute zero most elements become a solid, but not all behave as predictably as this; for instance, helium becomes a highly unusual liquid. The chemistry between substances, however, does not disappear, even near absolute zero temperatures, since separated molecules/atom can always combine to lower their total energy. Almost every molecule or element will show different properties at different temperatures; if cold enough, some functions are lost entirely. Cryogenic chemistry can lead to very different results compared with standard chemistry, and new chemical routes to substances may be available at cryogenic temperatures, such as the formation of argon fluorohydride, which is only a stable compound at or below .\n\nOne method that used to cool molecules to temperatures near absolute zero is laser cooling. In the Doppler cooling process, lasers are used to remove energy from electrons of a given molecule to slow or cool the molecule down. This method has applications in quantum mechanics and is related to particle traps and the Bose–Einstein condensate. All of these methods use a \"trap\" consisting of lasers pointed at opposite equatorial angles on a specific point in space. The wavelengths from the laser beams eventually hit the gaseous atoms and their outer spinning electrons. This clash of wavelengths decreases the kinetic energy state fraction by fraction to slow or cool the molecules down. Laser cooling has also been used to help improve atomic clocks and atom optics. Ultracold studies are not usually focused on chemical interactions, but rather on fundamental chemical properties.\n\nBecause of the extremely low temperatures, diagnosing the chemical status is a major issue when studying low temperature physics and chemistry. The primary techniques in use today are optical - many types of spectroscopy are available, but these require special apparatus with vacuum windows that provide room temperature access to cryogenic processes.\n\n\n"}
{"id": "51393", "url": "https://en.wikipedia.org/wiki?curid=51393", "title": "Czech Biomass Association", "text": "Czech Biomass Association\n\nThe Czech Biomass Association (CZ BIOM) is a NGO, which supports the development of phytoenergetics in the Czech Republic. Members of CZ BIOM are scientists, specialists, entrepreneurs, and activists interested in using biomass as an energy resource. CZ BIOM is a member of the European Biomass Association.\n\n"}
{"id": "188731", "url": "https://en.wikipedia.org/wiki?curid=188731", "title": "Decomposition", "text": "Decomposition\n\nDecomposition is the process by which organic substances are broken down into a more simple organic matter. The process is a part of the nutrient cycle and is essential for recycling the finite matter that occupies physical space in the biosphere. Bodies of living organisms begin to decompose shortly after death. Animals, such as worms, also help decompose the organic materials. Organisms that do this are known as decomposers. Although no two organisms decompose in the same way, they all undergo the same sequential stages of decomposition. The science which studies decomposition is generally referred to as \"taphonomy\" from the Greek word \"taphos\", meaning tomb.\n\nOne can differentiate abiotic from biotic decomposition (biodegradation). The former means \"degradation of a substance by chemical or physical processes, e.g., hydrolysis. The latter means \"the metabolic breakdown of materials into simpler components by living organisms\", typically by microorganisms.\n\nDecomposition begins at the moment of death, caused by two factors: 1.) autolysis, the breaking down of tissues by the body's own internal chemicals and enzymes, and 2.) putrefaction, the breakdown of tissues by bacteria. These processes release compounds such as cadaverine and putrescine, that are the chief source of the unmistakably putrid odor of decaying animal tissue.\n\nPrime decomposers are bacteria or fungi, though larger scavengers also play an important role in decomposition if the body is accessible to insects, mites and other animals. The most important arthropods that are involved in the process include carrion beetles, mites, the flesh-flies (Sarcophagidae) and blow-flies (Calliphoridae), such as the green-bottle fly seen in the summer. In North America, the most important non-insect animals that are typically involved in the process include mammal and bird scavengers, such as coyotes, dogs, wolves, foxes, rats, crows and vultures. Some of these scavengers also remove and scatter bones, which they ingest at a later time. Aquatic and marine environments have break-down agents that include bacteria, fish, crustaceans, fly larvae and other carrion scavengers.\n\nFive general stages are used to describe the process of decomposition in vertebrate animals: fresh, bloat, active decay, advanced decay, and dry/remains. The general stages of decomposition are coupled with two stages of chemical decomposition: autolysis and putrefaction. These two stages contribute to the chemical process of decomposition, which breaks down the main components of the body. With death the microbiome of the living organism collapses and is followed by the necrobiome that undergoes predictable changes over time.\n\nAmong those animals that have a heart, the \"fresh\" stage begins immediately after the heart stops beating. From the moment of death, the body begins cooling or warming to match the temperature of the ambient environment, during a stage called rigor mortis. Shortly after death, within three to six hours, the muscular tissues become rigid and incapable of relaxing, during a stage called rigor mortis. Since blood is no longer being pumped through the body, gravity causes it to drain to the dependent portions of the body, creating an overall bluish-purple discolouration termed livor mortis or, more commonly, lividity.\n\nOnce the heart stops, the blood can no longer supply oxygen or remove carbon dioxide from the tissues. The resulting decrease in pH and other chemical changes causes cells to lose their structural integrity, bringing about the release of cellular enzymes capable of initiating the breakdown of surrounding cells and tissues. This process is known as autolysis.\n\nVisible changes caused by decomposition are limited during the fresh stage, although autolysis may cause blisters to appear at the surface of the skin.\n\nThe small amount of oxygen remaining in the body is quickly depleted by cellular metabolism and aerobic microbes naturally present in respiratory and gastrointestinal tracts, creating an ideal environment for the proliferation of anaerobic organisms. These multiply, consuming the body's carbohydrates, lipids, and proteins, to produce a variety of substances including propionic acid, lactic acid, methane, hydrogen sulfide, and ammonia. The process of microbial proliferation within a body is referred to as putrefaction and leads to the second stage of decomposition, known as bloat.\n\nBlowflies and flesh flies are the first carrion insects to arrive, and they seek a suitable oviposition site.\n\nThe bloat stage provides the first clear visual sign that microbial proliferation is underway. In this stage, anaerobic metabolism takes place, leading to the accumulation of gases, such as hydrogen sulfide, carbon dioxide, methane, and nitrogen. The accumulation of gases within the bodily cavity causes the distention of the abdomen and gives a cadaver its overall bloated appearance. The gases produced also cause natural liquids and liquefying tissues to become frothy. As the pressure of the gases within the body increases, fluids are forced to escape from natural orifices, such as the nose, mouth, and anus, and enter the surrounding environment. The buildup of pressure combined with the loss of integrity of the skin may also cause the body to rupture.\n\nIntestinal anaerobic bacteria transform haemoglobin into sulfhemoglobin and other colored pigments. The associated gases which accumulate within the body at this time aid in the transport of sulfhemoglobin throughout the body via the circulatory and lymphatic systems, giving the body an overall marbled appearance.\n\nIf insects have access, maggots hatch and begin to feed on the body's tissues. Maggot activity, typically confined to natural orifices, and masses under the skin, causes the skin to slip, and hair to detach from the skin. Maggot feeding, and the accumulation of gases within the body, eventually leads to post-mortem skin ruptures which will then further allow purging of gases and fluids into the surrounding environment. Ruptures in the skin allow oxygen to re-enter the body and provide more surface area for the development of fly larvae and the activity of aerobic microorganisms. The purging of gases and fluids results in the strong distinctive odors associated with decay.\n\nActive decay is characterized by the period of greatest mass loss. This loss occurs as a result of both the voracious feeding of maggots and the purging of decomposition fluids into the surrounding environment. The purged fluids accumulate around the body and create a cadaver decomposition island (CDI). Liquefaction of tissues and disintegration become apparent during this time and strong odors persist. The end of active decay is signaled by the migration of maggots away from the body to pupate.\n\nDecomposition is largely inhibited during advanced decay due to the loss of readily available cadaveric material. Insect activity is also reduced during this stage. When the carcass is located on soil, the area surrounding it will show evidence of vegetation death. The CDI surrounding the carcass will display an increase in soil carbon and nutrients, such as phosphorus, potassium, calcium, and magnesium; changes in pH; and a significant increase in soil nitrogen.\n\nDuring the dry/remains stage, the resurgence of plant growth around the CDI may occur and is a sign that the nutrients present in the surrounding soil have not yet returned to their normal levels. All that remains of the cadaver at this stage is dry skin, cartilage, and bones, which will become dry and bleached if exposed to the elements. If all soft tissue is removed from the cadaver, it is referred to as completely skeletonized, but if only portions of the bones are exposed, it is referred to as partially skeletonised.\n\nA dead body that has been exposed to the open elements, such as water and air, will decompose more quickly and attract much more insect activity than a body that is buried or confined in special protective gear or artifacts. This is due, in part, to the limited number of insects that can penetrate a coffin and the lower temperatures under soil.\n\nThe rate and manner of decomposition in an animal body is strongly affected by several factors. In roughly descending degrees of importance, they are:\n\nThe speed at which decomposition occurs varies greatly. Factors such as temperature, humidity, and the season of death all determine how fast a fresh body will skeletonize or mummify. A basic guide for the effect of environment on decomposition is given as Casper's Law (or Ratio): if all other factors are equal, then, when there is free access of air a body decomposes twice as fast than if immersed in water and eight times faster than if buried in earth. Ultimately, the rate of bacterial decomposition acting on the tissue will depend upon the temperature of the surroundings. Colder temperatures decrease the rate of decomposition while warmer temperatures increase it. A dry body will not decompose efficiently. Moisture helps the growth of microorganisms that decompose the organic matter, but too much moisture could lead to anaerobic conditions slowing down the decomposition process.\n\nThe most important variable is a body's accessibility to insects, particularly flies. On the surface in tropical areas, invertebrates alone can easily reduce a fully fleshed corpse to clean bones in under two weeks. The skeleton itself is not permanent; acids in soils can reduce it to unrecognizable components. This is one reason given for the lack of human remains found in the wreckage of the \"Titanic\", even in parts of the ship considered inaccessible to scavengers. Freshly skeletonized bone is often called \"green\" bone and has a characteristic greasy feel. Under certain conditions (normally cool, damp soil), bodies may undergo saponification and develop a waxy substance called adipocere, caused by the action of soil chemicals on the body's proteins and fats. The formation of adipocere slows decomposition by inhibiting the bacteria that cause putrefaction.\n\nIn extremely dry or cold conditions, the normal process of decomposition is halted – by either lack of moisture or temperature controls on bacterial and enzymatic action – causing the body to be preserved as a mummy. Frozen mummies commonly restart the decomposition process when thawed (see Ötzi the Iceman), whilst heat-desiccated mummies remain so unless exposed to moisture.\n\nThe bodies of newborns who never ingested food are an important exception to the normal process of decomposition. They lack the internal microbial flora that produce much of decomposition and quite commonly mummify if kept in even moderately dry conditions.\n\nAerobic decomposition takes place in the presence of oxygen. This is most common to occur in nature. Living organisms that use oxygen to survive feed on the body. Anaerobic decomposition takes place in the absence of oxygen. This could be place where the body is buried in organic material and oxygen can not reach it. This process of putrefaction has a bad odor accompanied by it due to the hydrogen sulfide and organic matter containing sulfur.\n\nEmbalming is the practice of delaying decomposition of human and animal remains. Embalming slows decomposition somewhat, but does not forestall it indefinitely. Embalmers typically pay great attention to parts of the body seen by mourners, such as the face and hands. The chemicals used in embalming repel most insects, and slow down bacterial putrefaction by either killing existing bacteria in or on the body themselves or by \"fixing\" cellular proteins, which means that they cannot act as a nutrient source for subsequent bacterial infections. In sufficiently dry environments, an embalmed body may end up mummified and it is not uncommon for bodies to remain preserved to a viewable extent after decades. Notable viewable embalmed bodies include those of:\n\nA body buried in a sufficiently dry environment may be well preserved for decades. This was observed in the case for murdered civil rights activist Medgar Evers, who was found to be almost perfectly preserved over 30 years after his death, permitting an accurate autopsy when the case of his murder was re-opened in the 1990s.\n\nBodies submerged in a peat bog may become naturally \"embalmed\", arresting decomposition and resulting in a preserved specimen known as a bog body. The time for an embalmed body to be reduced to a skeleton varies greatly. Even when a body is decomposed, embalming treatment can still be achieved (the arterial system decays more slowly) but would not restore a natural appearance without extensive reconstruction and cosmetic work, and is largely used to control the foul odors due to decomposition.\n\nAn animal can be preserved almost perfectly, for millions of years in a resin such as amber.\n\nThere are some examples where bodies have been inexplicably preserved (with no human intervention) for decades or centuries and appear almost the same as when they died. In some religious groups, this is known as incorruptibility. It is not known whether or for how long a body can stay free of decay without artificial preservation.\n\nVarious sciences study the decomposition of bodies under the general rubric of forensic science because the usual motive for such studies is to determine the time and cause of death for legal purposes:\n\nThe University of Tennessee Anthropological Research Facility (better known as the Body Farm) in Knoxville, Tennessee has a number of bodies laid out in various situations in a fenced-in plot near the medical center. Scientists at the Body Farm study how the human body decays in various circumstances to gain a better understanding of decomposition.\n\nDecomposition of plant matter occurs in many stages. It begins with leaching by water; the most easily lost and soluble carbon compounds are liberated in this process. Another early process is physical breakup or fragmentation of the plant material into smaller bits which have greater surface area for microbial colonization and attack. In smaller dead plants, this process is largely carried out by the soil invertebrate fauna, whereas in the larger plants, primarily parasitic life-forms such as insects and fungi play a major breakdown role and are not assisted by numerous detritivore species.\n\nFollowing this, the plant detritus (consisting of cellulose, hemicellulose, microbial products, and lignin) undergoes chemical alteration by microbes. Different types of compounds decompose at different rates. This is dependent on their chemical structure.\n\nFor instance, lignin is a component of wood, which is relatively resistant to decomposition and can in fact only be decomposed by certain fungi, such as the black-rot fungi. Wood decomposition is a complex process involving fungi which transport nutrients to the nutritionally scarce wood from outside environment. Because of this nutritional enrichment the fauna of saproxylic insects may develop and in turn affect dead wood, contributing to wood decomposition and nutrient cycling in the forest floor. Lignin is one such remaining product of decomposing plants with a very complex chemical structure causing the rate of microbial breakdown to slow. Warmth increases the speed of plant decay, by the same amount regardless of the composition of the plant\n\nIn most grassland ecosystems, natural damage from fire, insects that feed on decaying matter, termites, grazing mammals, and the physical movement of animals through the grass are the primary agents of breakdown and nutrient cycling, while bacteria and fungi play the main roles in further decomposition.\n\nThe chemical aspects of plant decomposition always involve the release of carbon dioxide. In fact, decomposition contributes over 90 percent of carbon dioxide released each year.\n\nThe decomposition of food, either plant or animal, called \"spoilage\" in this context, is an important field of study within food science. Food decomposition can be slowed down by conservation. The spoilage of meat occurs, if the meat is untreated, in a matter of hours or days and results in the meat becoming unappetizing, poisonous or infectious. Spoilage is caused by the practically unavoidable infection and subsequent decomposition of meat by bacteria and fungi, which are borne by the animal itself, by the people handling the meat, and by their implements. Meat can be kept edible for a much longer time – though not indefinitely – if proper hygiene is observed during production and processing, and if appropriate food safety, food preservation and food storage procedures are applied.\n\nSpoilage of food is attributed to contamination from microorganisms such as bacteria, molds, and yeasts, along with natural decay of the food. These decomposition bacteria reproduce at rapid rates under conditions of moisture and preferred temperatures. When the proper conditions are lacking the bacteria may form spores which lurk until suitable conditions arise to continue reproduction.\n\nThe rate of decomposition is governed by three sets of factors—the physical environment (temperature, moisture and soil properties), the quantity and quality of the dead material available to decomposers, and the nature of the microbial community itself.\n\nDecomposition rates are low under very wet or very dry conditions. Decomposition rates are highest in wet, moist conditions with adequate levels of oxygen. Wet soils tend to become deficient in oxygen (this is especially true in wetlands), which slows microbial growth. In dry soils, decomposition slows as well, but bacteria continue to grow (albeit at a slower rate) even after soils become too dry to support plant growth. When the rains return and soils become wet, the osmotic gradient between the bacterial cells and the soil water causes the cells to gain water quickly. Under these conditions, many bacterial cells burst, releasing a pulse of nutrients. Decomposition rates also tend to be slower in acidic soils. Soils which are rich in clay minerals tend to have lower decomposition rates, and thus, higher levels of organic matter. The smaller particles of clay result in a larger surface area that can hold water. The higher the water content of a soil, the lower the oxygen content and consequently, the lower the rate of decomposition. Clay minerals also bind particles of organic material to their surface, making them less accessible to microbes. Soil disturbance like tilling increases decomposition by increasing the amount of oxygen in the soil and by exposing new organic matter to soil microbes.\n\nThe quality and quantity of the material available to decomposers is another major factor that influences the rate of decomposition. Substances like sugars and amino acids decompose readily and are considered labile. Cellulose and hemicellulose, which are broken down more slowly, are \"moderately labile\". Compounds which are more resistant to decay, like lignin or cutin, are considered recalcitrant. Litter with a higher proportion of labile compounds decomposes much more rapidly than does litter with a higher proportion of recalcitrant material. Consequently, dead animals decompose more rapidly than dead leaves, which themselves decompose more rapidly than fallen branches. As organic material in the soil ages, its quality decreases. The more labile compounds decompose quickly, leaving an increasing proportion of recalcitrant material. Microbial cell walls also contain recalcitrant materials like chitin, and these also accumulate as the microbes die, further reducing the quality of older soil organic matter.\n\n\n"}
{"id": "16396377", "url": "https://en.wikipedia.org/wiki?curid=16396377", "title": "EXPEC Advanced Research Center", "text": "EXPEC Advanced Research Center\n\nThe Exploration and Petroleum Engineering Center - Advanced Research Center (EXPEC ARC) is located in Dhahran, Saudi Arabia. It is a research center that belongs to Saudi Aramco and is responsible for upstream oil and gas technology development. The center has over 250 scientists from various disciplines, spread across six technology teams and one laboratory division which tackle various aspects of oil and gas exploration, development, and production. These teams are: Geophysics Technology, Geology Technology, Reservoir Engineering Technology, Computational Modeling Technology, Production Technology, and Drilling Technology.\n\nEXPEC ARC carries out cutting-edge upstream research and development at Saudi Aramco. EXPEC ARC researchers focus on developing innovative technologies and field testing them in the largest, most productive oil and gas fields in the world – those in Saudi Arabia. For many, the most exciting aspect of working at EXPEC ARC is the opportunity to put their far reaching research ideas, theories and developments into practice in these renowned oil and gas fields. \nEXPEC ARC research teams develop a vast range of dynamic upstream technologies spanning from the nano-scale, as illustrated by reservoir nano-agents, Resbots, all the way to the giga-scale, as in the giga-cell reservoir simulation technology, GigaPOWERS, and many other bold new concepts in between.\n\nEXPEC ARC is proactive in collaboration initiatives with scientific and research institutes and technology providers as well as independent researchers and developers from around the world.\n\nEXPEC ARC is an integrated organization, where researchers, technologists and strategists work together as teams developing inventive and original solutions to the company’s upstream technical challenges. Pioneering ideas and research are also generated through collaborative partnerships with premier universities and technology providers as well as independent researchers and developers from around the world.\n\nEXPEC ARC has developed ground-breaking services and products, especially in the areas of land seismic, simulation, and visualization. Regardless of the specialty, ideas are freely exchanged through mutual consultation and open discussion in an environment in which “pushing the technology envelope” is expected.\n\nScientists and researchers at EXPEC ARC have been granted numerous patents with many in process. Their work has been recognized with international awards, is widely published, and has several times been used as a foundation for further development by innovators, universities and research institutions throughout the world.\n\nEXPEC ARC provides the cradle for the unique and leading technologies designed to meet the challenges of today and tomorrow’s world energy demands.\n"}
{"id": "5568223", "url": "https://en.wikipedia.org/wiki?curid=5568223", "title": "Ecoprovince", "text": "Ecoprovince\n\nAn ecoprovince is a biogeographic unit smaller than an ecozone that contains one or more ecoregions. According to Demarchi (1996), an ecoprovince encompasses areas of uniform climate, geological history and physiography (i.e. mountain ranges, large valleys, plateaus). Their size and broad internal uniformity make them ideal units for the implementation of natural resource policies.\n\nDenis A.Demarchi (1996). An Introduction to the Ecoregions of British Columbia\", Wildlife Branch, Ministry of Environment, Lands and Parks, Victoria, British Columbia\n"}
{"id": "47299951", "url": "https://en.wikipedia.org/wiki?curid=47299951", "title": "Edouard Mennig", "text": "Edouard Mennig\n\nThe Société Anonyme des Ateliers Edouard Mennig was a Belgic manufacturer of machine tools, especially for woodworking.\n\nEdouard Mennig founded his company in Avenue Van Volxem 310-312 in Brussels. His wood working machines were driven by a series of pulleys and belts and used for many operations from chopping to crafting.\n"}
{"id": "38257", "url": "https://en.wikipedia.org/wiki?curid=38257", "title": "Electrolysis", "text": "Electrolysis\n\nIn chemistry and manufacturing, electrolysis is a technique that uses a direct electric current (DC) to drive an otherwise non-spontaneous chemical reaction. Electrolysis is commercially important as a stage in the separation of elements from naturally occurring sources such as ores using an electrolytic cell. The voltage that is needed for electrolysis to occur is called the decomposition potential.\n\nThe word \"electrolysis\" was introduced by Michael Faraday in the 19th century, on the suggestion of the Rev. William Whewell, using the Greek words \"amber\", which since the 17th century was associated with electric phenomena, and \"\" meaning \"dissolution\". Nevertheless, electrolysis, as a tool to study chemical reactions and obtain pure elements, precedes the coinage of the term and formal description by Faraday.\n\nElectrolysis is the passing of a direct electric current through an ionic substance that is either molten or dissolved in a suitable solvent, producing chemical reactions at the electrodes and a separation of the materials.\n\nThe main components required to achieve electrolysis are:\n\nElectrodes of metal, graphite and semiconductor material are widely used. Choice of suitable electrode depends on chemical reactivity between the electrode and electrolyte and manufacturing cost.\n\nThe key process of electrolysis is the interchange of atoms and ions by the removal or addition of electrons from the external circuit. The desired products of electrolysis are often in a different physical state from the electrolyte and can be removed by some physical processes. For example, in the electrolysis of brine to produce hydrogen and chlorine, the products are gaseous. These gaseous products bubble from the electrolyte and are collected.\n\nA liquid containing electrolyte is produced by:\n\nAn electrical potential is applied across a pair of electrodes immersed in the electrolyte. \n\nEach electrode attracts ions that are of the opposite charge. Positively charged ions (cations) move towards the electron-providing (negative) cathode. Negatively charged ions (anions) move towards the electron-extracting (positive) anode.\n\nIn this process electrons are either absorbed or released. Neutral atoms gain or lose electrons and become charged ions that then pass into the electrolyte. The formation of uncharged atoms from ions is called discharging. When an ion gains or loses enough electrons to become uncharged (neutral) atoms, the newly formed atoms separate from the electrolyte. Positive metal ions like Cudeposit onto the cathode in a layer. The terms for this are electroplating, electrowinning, and electrorefining. When an ion gains or loses electrons without becoming neutral, its electronic charge is altered in the process. In chemistry, the loss of electrons is called Oxidation, while electron gain is called reduction.\n\nOxidation of ions or neutral molecules occurs at the anode. For example, it is possible to oxidize ferrous ions to ferric ions at the anode:\nReduction of ions or neutral molecules occurs at the cathode.\n\nIt is possible to reduce ferricyanide ions to ferrocyanide ions at the cathode:\n\nNeutral molecules can also react at either of the electrodes. For example: p-Benzoquinone can be reduced to hydroquinone at the cathode:\n\nIn the last example, H ions (hydrogen ions) also take part in the reaction, and are provided by an acid in the solution, or by the solvent itself (water, methanol etc.). Electrolysis reactions involving H ions are fairly common in acidic solutions. In aqueous alkaline solutions, reactions involving OH (hydroxide ions) are common.\n\nSometimes the solvents themselves (usually water) are oxidized or reduced at the electrodes. It is even possible to have electrolysis involving gases. Such as when using a Gas diffusion electrode.\n\nThe amount of electrical energy that must be added equals the change in Gibbs free energy of the reaction plus the losses in the system. The losses can (in theory) be arbitrarily close to zero, so the maximum thermodynamic efficiency equals the enthalpy change divided by the free energy change of the reaction. In most cases, the electric input is larger than the enthalpy change of the reaction, so some energy is released in the form of heat. In some cases, for instance, in the electrolysis of steam into hydrogen and oxygen at high temperature, the opposite is true and heat energy is absorbed. This heat is absorbed from the surroundings, and the heating value of the produced hydrogen is higher than the electric input.\n\nThe following techniques are related to electrolysis:\n\nIn 1832, Michael Faraday reported that the quantity of elements separated by passing an electric current through a molten or dissolved salt is proportional to the quantity of electric charge passed through the circuit. This became the basis of the first law of electrolysis:\n\nor\n\nwhere;\ne is known as electrochemical equivalent of the metal deposited or of the gas liberated at the electrode.\n\nFaraday discovered that when the same amount of current is passed through different electrolytes/elements connected in series, the mass of substance liberated/deposited at the electrodes is directly proportional to their equivalent weight.\n\n\nElectrolysis has many other uses:\n\nElectrolysis is also used in the cleaning and preservation of old artifacts. Because the process separates the non-metallic particles from the metallic ones, it is very useful for cleaning a wide variety of metallic objects, from old coins to even larger objects including rusted cast iron cylinder blocks and heads when rebuilding automobile engines. Rust removal from small iron or steel objects by electrolysis can be done in a home workshop using simple materials such as a plastic bucket, tap water, lengths of rebar, washing soda, baling wire, and a battery charger.\n\nIn manufacturing, electrolysis can be used for:\n\nUsing a cell containing inert platinum electrodes, electrolysis of aqueous solutions of some salts leads to reduction of the cations (e.g., metal deposition with, e.g., zinc salts) and oxidation of the anions (e.g. evolution of bromine with bromides). However, with salts of some metals (e.g. sodium) hydrogen is evolved at the cathode, and for salts containing some anions (e.g. sulfate SO) oxygen is evolved at the anode. In both cases this is due to water being reduced to form hydrogen or oxidized to form oxygen.\nIn principle the voltage required to electrolyze a salt solution can be derived from the standard electrode potential for the reactions at the anode and cathode. The standard electrode potential is directly related to the Gibbs free energy, ΔG, for the reactions at each electrode and refers to an electrode with no current flowing. An extract from the table of standard electrode potentials is shown below.\nIn terms of electrolysis, this table should be interpreted as follows:\n\nUsing the Nernst equation the electrode potential can be calculated for a specific concentration of ions, temperature and the number of electrons involved. For pure water (pH 7):\nComparable figures calculated in a similar way, for 1M zinc bromide, ZnBr, are −0.76 V for the reduction to Zn metal and +1.10 V for the oxidation producing bromine.\nThe conclusion from these figures is that hydrogen should be produced at the cathode and oxygen at the anode from the electrolysis of water—which is at variance with the experimental observation that zinc metal is deposited and bromine is produced.\nThe explanation is that these calculated potentials only indicate the thermodynamically preferred reaction. In practice many other factors have to be taken into account such as the kinetics of some of the reaction steps involved. These factors together mean that a higher potential is required for the reduction and oxidation of water than predicted, and these are termed overpotentials. Experimentally it is known that overpotentials depend on the design of the cell and the nature of the electrodes.\n\nFor the electrolysis of a neutral (pH 7) sodium chloride solution, the reduction of sodium ion is thermodynamically very difficult and water is reduced evolving hydrogen leaving hydroxide ions in solution. At the anode the oxidation of chlorine is observed rather than the oxidation of water since the overpotential for the oxidation of chloride to chlorine is lower than the overpotential for the oxidation of water to oxygen. The hydroxide ions and dissolved chlorine gas react further to form hypochlorous acid. The aqueous solutions resulting from this process is called electrolyzed water and is used as a disinfectant and cleaning agent.\n\nThe electrochemical reduction or electrocatalytic conversion of CO can produce value-added chemicals such methane, ethylene, ethane, etc. The electrolysis of carbon dioxide gives formate or carbon monoxide, but sometimes more elaborate organic compounds such as ethylene. This technology is under research as a carbon-neutral route to organic compounds.\n\nElectrolysis of water produces hydrogen.\n\nThe energy efficiency of water electrolysis varies widely. The efficiency of an electrolyser is a measure of the enthalpy contained in the hydrogen (to undergo combustion with oxygen, or some other later reaction), compared with the input electrical energy. Heat/enthalpy values for hydrogen are well published in science and engineering texts, as 144 MJ/kg. Note that fuel cells (not electrolysers) cannot use this full amount of heat/enthalpy, which has led to some confusion when calculating efficiency values for both types of technology. In the reaction, some energy is lost as heat. Some reports quote efficiencies between 50% and 70% for alkaline electrolysers; however, much higher practical efficiencies are available with the use of PEM (Polymer Electrolyte Membrane electrolysis) and catalytic technology, such as 95% efficiency.\n\nNREL estimated that 1 kg of hydrogen (roughly equivalent to 3 kg, or 4 L, of petroleum in energy terms) could be produced by wind powered electrolysis for between $5.55 in the near term and $2.27 in the long term.\n\nAbout 4% of hydrogen gas produced worldwide is generated by electrolysis, and normally used onsite. Hydrogen is used for the creation of ammonia for fertilizer via the Haber process, and converting heavy petroleum sources to lighter fractions via hydrocracking.\n\nRecently, to reduce the energy input, the utilization of carbon (coal), alcohols (hydrocarbon solution), and organic solution (glycerol, formic acid, ethylene glycol, etc.) with co-electrolysis of water has been proposed as a viable option. The carbon/hydrocarbon assisted water electrolysis (so-called CAWE) process for hydrogen generation would perform this operation in a single electrochemical reactor. This system energy balance can be required only around 40% electric input with 60% coming from the chemical energy of carbon or hydrocarbon. This process utilizes solid coal/carbon particles or powder as fuels dispersed in acid/alkaline electrolyte in the form of slurry and the carbon contained source co-assist in the electrolysis process as following theoretical overall reactions :\n\nCarbon/Coal slurry (C + 2HO) -> CO + 2H E' = 0.21 V (reversible voltage) / E' = 0.46 V (thermo-neutral voltage)\n\nor\n\nCarbon/Coal slurry (C + HO) -> CO + H E' = 0.52 V reversible voltage) / E' = 0.91 V (thermo-neutral voltage)\n\nThus, this CAWE approach is that the actual cell overpotential can be significantly reduced to below 1 V as compared to 1.5 V for conventional water electrolysis.\n\nA specialized application of electrolysis involves the growth of conductive crystals on one of the electrodes from oxidized or reduced species that are generated in situ. The technique has been used to obtain single crystals of low-dimensional electrical conductors, such as charge-transfer salts.\n\nScientific pioneers of electrolysis include:\n\n\nPioneers of batteries:\n"}
{"id": "29821661", "url": "https://en.wikipedia.org/wiki?curid=29821661", "title": "Elmer Smith Power Plant", "text": "Elmer Smith Power Plant\n\nThe Elmer Smith Power Plant is a coal-fired power plant owned and operated by the city of Owensboro, Kentucky. It opened in 1964.\n\nIt was announced in June 2018 that Elmer Smith will close by June 2020. This is due to Owensboro Municipal Utility Commission approving a contract with Big Rivers Electric to supply electricity for Owensboro.\n\n\n\n"}
{"id": "4802906", "url": "https://en.wikipedia.org/wiki?curid=4802906", "title": "Engineered stone", "text": "Engineered stone\n\nEngineered stone is a composite material made of crushed stone bound together by an adhesive, (most commonly polymer resin, with some newer versions using cement mix). This category includes engineered quartz, polymer concrete and engineered marble stone. The application of these products depends on the original stone used. For engineered marbles the most common application is indoor flooring and walls, while the quartz based product is used primarily for kitchen countertops as an alternative to laminate or granite. Related materials include geopolymers and cast stone. Unlike terrazzo, the material is factory made in either blocks or slabs, cut and polished by fabricators, and assembled at the worksite.\n\nEngineered stone (US name) is also commonly referred to as agglomerate or agglomerated stone, the last term being that recognised by European Standards (EN 14618), although to add to the terminological confusion, this standard also includes materials manufactured with a cementitious binder. The quartz version (which end consumers are much more likely to directly deal with) is commonly known as 'quartz surface' or just 'quartz'.\n\nBreton S.P.A., a privately held company of Treviso, Italy, that developed large-scale Breton method in 1960s, is the dominant supplier of equipment for making engineered stone. Although Breton was the original manufacturer of moulding equipment and still holds multiple international patents on the process, there are now several other companies producing similar machinery.\n\nStone aggregates is the major filler, although other material like coloured glass, shells, metals, or mirrors might be added. A typical resin based material will consist of 93% stone aggregates by weight and 7% resin (66% quartz and 34% resin by volume) . Different types of resins are used by different manufacturers. Epoxy and polyester resin are the most common types. Chemicals such as UV absorbers and stabilizers are added. To aid curing, hydrogen peroxide is added.\n\nCompaction by vibrocompression vacuum process uses elastomeric molds in which a crushed stone/resin mix is cast on a moving belt. The mixture of approximately 93% stone aggregates and 7% polyester resin by weight (66% quartz and 34% resin by volume) is heated and compressed under vacuum in a large press. The vibration helps compact the mixture and results in an isotropic slab with virtually no porosity. Engineered stone is then processed in basically the same manner as its natural counterpart.\n\nSome companies import boulders themselves to crush into agglomerates (stone powders) of various grain size for their products, others simply buy already-crushed stone powders.\n\nEngineered stone is typically worked in the same way as natural stone using a water jet cutter or a diamond blade. This is in contrast with solid surface materials which can be cut with regular saws.\n\nThe material can be produced in either 12 mm, 20 mm or 30 mm thicknesses. The most common slab format is 3040 mm x 1440 mm for Quartz and 3050 mm x 1240 mm for Breton-based marbles, but other sizes like 3040 mm x 1650 mm are produced according to market demand.\n\nEngineered stone is non porous, more flexible, and harder than many types of natural stone. Since it has a uniform internal structure, it does not have hidden cracks or flaws that may exist in natural stone and also has a color/pattern consistency from slab to slab. Polyester resin binding agents allow some flexibility, preventing cracking under flexural pressure. But, the binding agents often continue to harden, leading to a loss of flexural strength over time. The polyester resins are not completely UV stable and engineered stone should not be used in outdoor applications. Continuous exposure to UV can cause discoloration of the stone, and breakdown of the resin binder.\n\nThe material is sometimes damaged by direct application of heat. Quartz engineered stone is less heat resistant than other stone surfaces including most granite, marble and limestone; but is not affected by temperatures lower than 150 °C (300 °F). Quartz engineered stone can be damaged by sudden temperature changes. Manufacturers recommend that hot pots and pans never be placed directly on the surface, and that a hot pad or trivet is used under portable cooking units.\n\nWhen used as floor tiles, care is required in ensuring compatibility with the adhesive used. Reaction resin adhesives and rapid drying cementitious adhesives are generally successful, but bond failure can occur with other cementitious adhesives. Additionally, agglomerate stone tiles are more sensitive to both thermal expansion and contraction and to dampness from underlying screeds, necessitating the inclusion of movement joints at a higher frequency than for stone or ceramic floor tiles (see for example British Standard BS 5385-5: 2011) and verification by testing of the dryness of underlying layers.\n\nAlthough both the marble- and quartz-based engineered stones are created through a similar process, and multiple companies produce both at the same time, there are distinct differences in their properties and applications.\n\nMarble is a relatively soft material which is prone to scratching, but also makes maintaining it much less difficult. Typically they can be re-polished all the way until they become too thin. Marble is also much more common and accessible around the world and comes in a wider variety, which gives their engineered counterpart a significant edge in pricing and the variety in its pattern and colors. Engineered marbles are typically most popular as flooring materials for large commercial projects such as hotels, shopping centers, business lobbies, where they combine the attractive appearance of marbles with more budget-friendly prices and reliable delivery times.\n\nQuartz meanwhile is a much harder material. The Mohs scale of marble is roughly 3, where as quartz are usually at 7. This makes them much more resistant to scratching, however it also makes re-polishing and general processing of them a more difficult task, which is why they are most commonly used for kitchen counter tops, where the value added through processing can offset their considerably higher cost.\n\nThere is also a difference between Quartz and Quartzite, which are often confused because of the similarities in the name. Quartz countertops are man-made even through quartz is a natural material. Quartzite countertops on the other hand are natural and they are considered to be a high end countertop material.\n\nItaly was the most dominant country in the supply of engineered stone products from the 1980s until the early 2000s, especially in engineered marble. The growth of the Chinese economy has changed the market drastically as China now has the most producers and largest overall quantity produced. There are estimated to be more than 100 engineered stone suppliers in China alone. India also has roughly 40 slab producing units as of December 2012. The original companies that operate Breton machines such as the Italian companies, Quarella, Santa Margherita and the Taiwanese company Romastone remain the most recognizable brands for marble. The private Spanish company Cosentino brand Silestone and the public Israeli company Caesarstone are the most recognizable brands for quartz also Totem Quartz, the Iranian company which has a huge market in middle east and Central Asia. Gulfstone company, Oman based company is the only producer of engineered quartz stone in the GCC. China is now probably the largest market for engineered marble due to new construction projects, while engineered quartz is primarily sold in North America and Europe as high end residential kitchen counter tops. \n\nAs with any silica-containing stone, silicosis may result from breathing dust produced when cutting or processing engineered stone made with quartz. The risk of inhaling quartz dust can be mitigated by taking appropriate safety precautions. Risk of silicosis is high when little or no safety precautions or protective equipment are used. This may occur in small shops or in countries where the industry is not regulated or monitored.\n\n"}
{"id": "20011681", "url": "https://en.wikipedia.org/wiki?curid=20011681", "title": "Eolica Varna Wind Farm", "text": "Eolica Varna Wind Farm\n\nThe Eolica Varna Wind Farm () is a proposed wind power project in Varna, Bulgaria. It will have 30 individual wind turbines with a nominal output of around 2 MW which will deliver up to 60 MW of power, enough to power over 23,940 homes, with a capital investment required of approximately US$120 million.\n\n"}
{"id": "44444604", "url": "https://en.wikipedia.org/wiki?curid=44444604", "title": "Forbes Marshall", "text": "Forbes Marshall\n\nForbes Marshall is an Indian engineering and energy conservation solutions provider, especially for the process industry. Forbes Marshall makes steam engineering and control instrumentation products and solutions. Its origins trace to a venture by J. N. Marshall and Darius Forbes in Pune in the 1940s, with a factory established at Kasarwadi, Pune in 1958. It is based in Pune.\n\nIn 2013, Forbes Marshall was voted one of India's top ten best companies to work for.\n\nForbes Marshall has four manufacturing units in India which are located at Kasarwadi, Pimpri, Hyderabad and Chakan near Pune.\n"}
{"id": "25572106", "url": "https://en.wikipedia.org/wiki?curid=25572106", "title": "Frank M. Folsom", "text": "Frank M. Folsom\n\nFrank Marion Folsom (14 May 1894, Sprague, Washington - 12 January 1970, New York City) was an electronics company executive and was a permanent representative of the Holy See.\n\nHe graduated from Manhattan College, Fordham University, and held an LLD from the University of San Francisco.\n\nFolsom began his career in 1910 at the Montgomery Ward & Co. in the merchandising field. He held the position of manager of west coast operations and Marketing Vice President. From 1940 to 1941 he worked for department store, Goldblatt Brothers in Chicago, Illinois, and was assistant coordinator of purchases for OPM from 1941 to 1942. In 1942 he was appointed as special assistant to the Under Secretary of the Navy. It was here where he met David Sarnoff, who got him his next job as Executive Vice President of the RCA Victor Division. He was promoted to President in 1949 and served as Chairman of the Executive Committee Board from 1957. He also served as director of the Coca-Cola Bottling Company, the John P. Maguire Company, NBC, the General Cable Corporation, RCA Commons, Crown Cork & Seal, Tishman International, and served as the representative of the Holy See to the International Atomic Energy Agency.\n\nHe was a member of the Augusta National Golf Club, the Blind Brook Club of Port Chester, the Bohemian Club, the Everglades Club of Palm Beach, Florida, the Order of the Holy Sepulchre, Pilgrims of the United States and the Catholic fraternity KÖHV Alpenland (Austria).\n"}
{"id": "7127168", "url": "https://en.wikipedia.org/wiki?curid=7127168", "title": "Friction loss", "text": "Friction loss\n\nIn fluid flow, friction loss (or skin friction) is the loss of pressure or “head” that occurs in pipe or duct flow due to the effect of the fluid's viscosity near the surface of the pipe or duct.\nIn mechanical systems such as internal combustion engines, the term refers to the power lost in overcoming the friction between two moving surfaces, a different phenomenon.\n\nFriction loss is a significant economic concern wherever fluids are made to flow, whether entirely enclosed in a pipe or duct, or with a surface open to the air.\n\nIn the following discussion, we define volumetric flow rate V̇ (i.e. volume of fluid flowing) \nV̇ = πrv\n\nwhere\nIn long pipes, the loss in pressure (assuming the pipe is level) is proportional to the length of pipe involved.\nFriction loss is then the change in pressure Δp per unit length of pipe \"L\"\nWhen the pressure is expressed in terms of the equivalent height of a column of that fluid, as is common with water, the friction loss is expressed as \"S\", the \"head loss\" per length of pipe, a dimensionless quantity also known as the \"hydraulic slope\". \nwhere\n\nFriction loss, which is due to the shear stress between the pipe surface and the fluid flowing within, depends on the conditions of flow and the physical properties of the system. These conditions can be encapsulated into a dimensionless number Re, known as the Reynolds number\nwhere \"V\" is the mean fluid velocity and \"D\" the diameter of the (cylindrical) pipe. In this expression, the properties of the fluid itself are reduced to the kinematic viscosity ν\nwhere\n\nThe friction loss In a uniform, straight sections of pipe, known as \"major loss\", is caused by the effects of viscosity, the movement of fluid molecules against each other or against the (possibly rough) wall of the pipe. Here, it is greatly affected by whether the flow is laminar (Re < 2000) or turbulent (Re > 4000):\n\nFactors other than straight pipe flow induce friction loss; these are known as “minor loss”:\nFor the purposes of calculating the total friction loss of a system, the sources of form friction are sometimes reduced to an equivalent length of pipe.\n\nBecause of the importance of friction loss in civil engineering and in industry, it has been studied extensively for over a century.\n\nThe roughness of the surface of the pipe or duct affects the fluid flow in the regime of turbulent flow. Usually denoted by ε, values used for calculations of water flow, for some representative materials are:\nValues used in calculating friction loss in ducts (for, e.g., air) are:\n\nLaminar flow is encountered in practice with very viscous fluids, such as motor oil, flowing through small-diameter tubes, at low velocity. Friction loss under conditions of laminar flow follow the Hagen–Poiseuille equation, which is an exact solution to the Navier-Stokes equations. For a circular pipe with a fluid of density \"ρ\" and viscosity \"μ\", the hydraulic slope \"S\" can be expressed\nIn laminar flow (that is, with Re < ~2000), the hydraulic slope is proportional to the flow velocity.\n\nIn many practical engineering applications, the fluid flow is more rapid, therefore turbulent rather than laminar. Under turbulent flow, the friction loss is found to be roughly proportional to the square of the flow velocity and inversely proportional to the pipe diameter, that is, the friction loss follows the phenomenological Darcy–Weisbach equation in which the \"hydraulic slope\" \"S\" can be expressed\nwhere we have introduced the Darcy friction factor \"f\" (but see \"Confusion with the Fanning friction factor\");\nNote that the value of this dimensionless factor depends on the pipe diameter \"D\" and the roughness of the pipe surface ε. Furthermore, it varies as well with the flow velocity \"V\" and on the physical properties of the fluid (usually cast together into the Reynolds number Re). Thus, the friction loss is not precisely proportional to the flow velocity squared, nor to the inverse of the pipe diameter: the friction factor takes account of the remaining dependency on these parameters.\n\nFrom experimental measurements, the general features of the variation of \"f\" are, for fixed \"relative roughness\" ε / \"D\" and for Reynolds number Re = \"V\" \"D\" / ν > ~2000,\n\nThe experimentally measured values of \"f\" are fit to reasonable accuracy by the (recursive) Colebrook–White equation, depicted graphically in the Moody chart which plots friction factor \"f\" versus Reynolds number Re for selected values of relative roughness ε / \"D\".\n\nIn a design problem, one may select pipe for a particular hydraulic slope \"S\" based on the candidate pipe's diameter \"D\" and its roughness ε. \nWith these quantities as inputs, the friction factor \"f\" can be expressed in closed form in the Colebrook–White equation or other fitting function, and the flow volume \"Q\" and flow velocity \"V\" can be calculated therefrom.\n\nIn the case of water (ρ = 1 g/cc, μ = 1 g/m/s) flowing through a 12-inch (300 mm) Schedule-40 PVC pipe (ε = 0.0015 mm, \"D\" = 11.938 in.), a hydraulic slope \"S\" = 0.01 (1%) is reached at a flow rate \"Q\" = 157 lps (liters per second), or at a velocity \"V\" = 2.17 m/s (meters per second). \nThe following table gives Reynolds number Re, Darcy friction factor \"f\", flow rate \"Q\", and velocity \"V\" such that hydraulic slope \"S\" = \"h\" / \"L\" = 0.01, for a variety of nominal pipe (NPS) sizes. \n\nNote that the cited sources recommend that flow velocity be kept below 5 feet / second (~1.5 m/s).\n\nFriction loss takes place as a gas, say air, flows through duct work.\nThe difference in the character of the flow from the case of water in a pipe stems from the differing Reynolds number Re and the roughness of the duct.\n\nThe friction loss is customarily given as pressure loss for a given duct length, Δ\"p\" / \"L\", in units of (US) inches of water for 100 feet or (SI) kg / m / s.\n\nFor specific choices of duct material, and assuming air at standard temperature and pressure (STP), standard charts can be used to calculate the expected friction loss. The chart exhibited in this section can be used to graphically determine the required diameter of duct to be installed in an application where the volume of flow is determined and where the goal is to keep the pressure loss per unit length of duct \"S\" below some target value in all portions of the system under study. First, select the desired pressure loss Δ\"p\" / \"L\", say 1 kg / m / s (0.12 in HO per 100 ft) on the vertical axis (ordinate). Next scan horizontally to the needed flow volume \"Q\", say 1 m / s (2000 cfm): the choice of duct with diameter \"D\" = 0.5 m (20 in.) will result in a pressure loss rate Δ\"p\" / \"L\" less than the target value. Note in passing that selecting a duct with diameter \"D\" = 0.6 m (24 in.) will result in a loss Δ\"p\" / \"L\" of 0.02 kg / m / s (0.02 in HO per 100 ft), illustrating the great gains in blower efficiency to be achieved by using modestly larger ducts.\n\nThe following table gives flow rate \"Q\" such that friction loss per unit length Δ\"p\" / \"L\" (SI kg / m / s) is 0.082, 0.245, and 0.816, respectively, for a variety of nominal duct sizes. The three values chosen for friction loss correspond to, in US units inch water column per 100 feet, 0.01, .03, and 0.1. Note that, in approximation, for a given value of flow volume, a step up in duct size (say from 100mm to 120mm) will reduce the friction loss by a factor of 3. \nNote that, for the chart and table presented here, flow is in the turbulent, smooth pipe domain, with R* < 5 in all cases.\n\n"}
{"id": "725995", "url": "https://en.wikipedia.org/wiki?curid=725995", "title": "Glovebox", "text": "Glovebox\n\nA glovebox (or glove box) is a sealed container that is designed to allow one to manipulate objects where a separate atmosphere is desired. Built into the sides of the glovebox are gloves arranged in such a way that the user can place their hands into the gloves and perform tasks inside the box without breaking containment. Part or all of the box is usually transparent to allow the user to see what is being manipulated. Two types of gloveboxes exist. The first allows a person to work with hazardous substances, such as radioactive materials or infectious disease agents, and the second allows manipulation of substances that must be contained within a very high purity inert atmosphere, such as argon or nitrogen. It is also possible to use a glovebox for manipulation of items in a vacuum chamber.\n\nThe gas in a glovebox is pumped through a series of treatment devices which remove solvents, water and oxygen from the gas. Heated copper metal (or some other finely divided metal) is commonly used to remove oxygen, this oxygen removing column is normally regenerated by passing a hydrogen/nitrogen mixture through it while it is heated: the water formed is passed out of the box with the excess hydrogen and nitrogen. It is common to use molecular sieves to remove water by absorbing it in the molecular sieves' pores. Such a box is often used by organometallic chemists to transfer dry solids from one container to another container.\n\nAn alternative to using a glovebox for air sensitive work is to employ Schlenk methods using a Schlenk line. One disadvantage of working in a glovebox is that organic solvents will attack the plastic seals. As a result, the box will start to leak and water and oxygen can then enter the box. Another disadvantage of a glovebox is that oxygen and water can diffuse through the plastic gloves.\n\nInert atmosphere gloveboxes are typically kept at a higher pressure than the surrounding air, so that any microscopic leaks are mostly leaking inert gas out of the box instead of letting air in.\n\nAt the now-deactivated Rocky Flats Plant, which manufactured plutonium triggers, also called \"pits\", production facilities consisted of linked stainless steel gloveboxes up to 64 feet, or 20 meters, in length, which contained the equipment which forged and machined the trigger parts. The gloves were lead-lined. Other materials used in the gloveboxes included acrylic viewing windows and Benelex shielding composed of wood fiber and plastic which shielded against neutron radiation. Manipulation of the lead-lined gloves was onerous work.\n\nSome gloveboxes for radioactive work are under inert conditions, for instance, one nitrogen-filled box contains an argon-filled box. The argon box is fitted with a gas treatment system to keep the gas very pure to enable electrochemical experiments in molten salts.\n\nGloveboxes are also used in the biological sciences when dealing with anaerobes or high-biosafety level pathogens.\n\nGloveboxes used for hazardous materials are generally maintained at a lower pressure than the surrounding atmosphere, so that microscopic leaks result in air intake rather than hazard outflow. Gloveboxes used for hazardous materials generally incorporate HEPA filters into the exhaust, to keep the hazard contained.\n\n\n"}
{"id": "24375439", "url": "https://en.wikipedia.org/wiki?curid=24375439", "title": "Grimshaw v. Ford Motor Co.", "text": "Grimshaw v. Ford Motor Co.\n\nGrimshaw v. Ford Motor Company (119 Cal.App.3d 757, 174 Cal.Rptr. 348) was a personal injury tort case decided in Orange County California in February 1978 and affirmed by a California appellate court in May 1981. The lawsuit involved the safety of the design of the Ford Pinto automobile, manufactured by the Ford Motor Company. The jury awarded plaintiffs $127.8 million in damages, the largest ever in US product liability and personal injury cases. \"Grimshaw v. Ford Motor Company\" was one of the most widely publicized of the more than a hundred lawsuits brought against Ford in connection with rear-end accidents in the Pinto.\n\nOn appeal, Ford contested the trial court judgement on the basis of errors, and contested the punitive damages award on the grounds of an absence of malice and that the punitive damages award was not authorized by statute and was unconstitutional. The appellate court affirmed the trial court.\n\nA 1972 Pinto rear-end impact and fire in Orange County, California, resulted in the death of the driver Lily Gray and severe injury to passenger Richard Grimshaw. Lawsuits were combined for trial. The jury awarded $127.8 million in damages; $125 million in punitive damages and $2,841,000 in compensatory damages to Grimshaw and $665,000 in compensatory damages to the family of Gray. The jury award was the largest ever in US product liability and personal injury cases. The jury award was the largest against an automaker until $150 million in \"Hardy vs. General Motors\" in 1996.\n\nThe judge reduced the jury's punitive damages award to $3.5 million, which he later said was \"still larger than any other punitive damage award in the state by a factor of about five.\"\n\nUniversity of California Los Angeles law professor Gary T. Schwartz, writing in 1990, said that the jury verdict was plausible, and that the \"core of the Pinto story\" was:\n\nGiven this description of the Pinto's design problem, some comments can be offered on the decision-making process within Ford that resulted in the Pinto. As shown above, a famous Ford report cannot be interpreted as showing Ford balancing lives against dollars in designing the Pinto. To state that the report does not itself reveal such a process does not mean, however, that such a process did not take place. Accordingly, I have consulted the Grimshaw record to learn what light it sheds on this question. As far as basic gas tank location is concerned, I am persuaded that the trunk capacity problem, in conjunction with American auto custom, provides the best explanation for Ford's choice to place the Pinto gas tank behind the axle. As for additional design proposals brought forward by the plaintiffs, several of them-for example, a bladder within the gas tank, and a \"tank within the tank\"-concerned somewhat innovative technology that had never been utilized in actual auto production. At trial, there was testimony that a bladder would have been feasible in the early 1970's, but also rebuttal testimony that a bladder was at this time beyond the bounds of feasibility. The jury's general verdict does not reveal whether and how the jury resolved such conflicts in the evidence; and I am in no position to resolve them here. Consider now, however, the combination of a stronger bumper, a smooth (bolt-free) differential, and the addition of both hat sections and horizontal cross-members. This combination of design changes clearly would have improved the Pinto's safety to some appreciable extent. According to the evidence, the overall cost of this combination would have been $9; and it makes sense to assume that these items were turned down by Ford in planning the Pinto primarily on account of their monetary costs. It is plausible to believe, then, that because of these costs, Ford decided not to improve the Pinto's design, knowing that its decision would increase the chances of the loss of consumer life. Once a variety of misconceptions are stripped away, this limited core of the Pinto story remains. And this is a core that may well be strong enough to support the \"myth\" of the Ford Pinto case in the second of the meanings described above. What follows is my effort to explicate the elements of that myth.\n\nAccording to the \"Los Angeles Times\" in 2010, the award \"signalled to the auto industry that it would be harshly sanctioned for ignoring known defects.\"\n\nIn discussing the appellate court findings of fact Schwartz (1990) notes that the appellate court was under strict rules of interpretation. He states:\nFor reasons quite beyond the court's control, its opinion must be treated cautiously as a source of actual facts. Because the defendant was appealing a jury verdict in favor of the plaintiffs, the court was under an obligation to view all the evidence in a way most favorable to the plaintiffs and essentially to ignore evidence in the record that might be favorable to the defendant. See id. at 773, 820, 174 Cal. Rptr. at 359, 388. In fact, Ford's basic position at trial-which the court's opinion at no point mentions-was that the approaching car (a Ford Galaxie) had not slowed down at all, and had struck the Gray car at a speed in excess of 50 miles per hour. There was an enormous amount of evidence at trial supporting each of the parties' factual claims as to the Galaxie's closing speed. Had the jury accepted Ford's speed estimate, there would not have been much of an issue of crashworthiness: for the plaintiffs' position throughout trial was that even a state-of-the-art fuel system could not maintain integrity in a 50 mile-per hour collision.\n\nThe trial court's findings of fact regarding the accident, the design of the Pinto, Ford's crash testing, and Ford's cost benefit analysis were not contested on appeal, and were accepted by the appellate court \"in accordance with established principles of appellate review.\"\n\nA 1972 Ford Pinto hatchback stalled on a freeway, erupting into flames when it was rear-ended by a Ford Galaxie proceeding in the same direction. Lilly Gray, the driver of the Pinto, suffered severe burns to her entire body and resulted in her death by congestive heart failure. 13-year-old Richard Grimshaw, a passenger, suffered severe, permanently disfiguring burns to his entire body. Grimshaw underwent numerous skin grafts and extensive surgeries, but still lost portions of the fingers on his left hand and his left ear in the accident. Doctors estimated that Grimshaw would require many more surgeries within the next 10 years.\n\nThe plaintiff's expert testified that the Pinto's gas tank was pushed forward upon impact and punctured by a flange or bolt on the differential housing. Fuel sprayed from the tank and entered the passenger compartment through the gaps between the rear wheel wells and the floor.\n\nIn 1968, Ford began designing the subcompact car that would eventually become known as the Pinto.\n\nThe courts found that Lee Iacocca, at the time a vice president at Ford, conceived the Pinto \"project and was its moving force\" and said that \"Ford's objective was to build a car at or below 2,000 pounds to sell for no more than $2,000.\" The courts described the Pinto as a \"rush project,\" and said that while standard automotive industry practice was that \"engineering studies precede the styling,\" in the case of the Pinto project \"styling preceded engineering and dictated engineering design to a greater degree than usual.\"\nThe court found that the Pinto's styling required the gas tank to be placed behind the rear axle, instead of over the rear axle as was \"the preferred practice in Europe and Japan\" and that the Pinto had \"only 9 or 10 inches\" of \"crush space,\" \"far less than in any other American automobile or Ford overseas subcompact.\" The court found that the Pinto's bumper \"was little more than a chrome strip, less substantial than the bumper of any other American car produced then or later.\" The court found that the Pinto's rear structure lacked reinforcement \"found in all automobiles produced by Ford's overseas operations,\" rendering the Pinto \"less crush resistant than other vehicles.\" The court found that a flange and a line of bolts on the Pinto's differential housing \"were sufficient to puncture a gas tank driven forward upon rear impact.\"\n\nFord tested two production models of the Pinto and prototypes, some of which \"were true duplicates of the design car,\" \"to determine, among other things, the integrity of the fuel system in rear-end accidents.\" Proposed federal regulations required impacts \"without significant fuel spillage,\" up to impacts by 1972 and by 1973.\n\nCrash tests proved that the Pinto could not meet the proposed regulations. A collision from the rear \"caused the fuel tank to be driven forward and to be punctured, causing fuel leakage.\" A collision of a production Pinto \"caused the fuel neck to be torn from the gas tank and the tank to be punctured by a bolt head on the differential housing.\" In at least one test collision \"spilled fuel entered the driver's compartment through gaps resulting from the separation of the seams joining the rear wheel wells to the floor pan,\" separations due in part to \"the lack of reinforcement in the rear structure.\"\n\nFord tested modified Pinto prototypes, which \"proved safe at speeds at which the Pinto failed,\" including modifications to line the fuel tank with a rubber bladder, to locate the fuel tank above rather than behind the rear axle, and to add reinforcement.\n\nThe courts found that, while \"the standard of care for engineers in the industry\" after a failed safety test was to \"redesign and retest,\" and although fixes were inexpensive, \"Ford produced and sold the Pinto to the public without doing anything to remedy the defects.\"\n\nDesign changes that would have enhanced the fuel system at very little cost included:\n\n\nEquipping the car with a reinforced rear structure, smooth axle, improved bumper and additional crush space at a total cost of $15.30 would have made the fuel tank safe in a 34 to 38-mile-per-hour rear-end collision by a vehicle the size of the Ford Galaxie. If, in addition to the foregoing, a bladder or tank within a tank were used or if the tank were protected with a shield, it would have been safe in a 40 to 45-mile-per-hour rear impact. If the tank had been located over the rear axle, it would have been safe in a rear impact at 50 miles per hour or more.\n\nThe Pinto project team held regular product review meetings chaired and attended by Ford vice presidents. The Pinto was approved by Ford's Product Planning Committee, which included Iacocca and other Ford vice presidents. At an April, 1971 product review meeting, a report prepared by Ford engineers entitled \"Fuel System Integrity Program Financial Review\" was distributed and discussed, which referred to the crash tests of Ford vehicles and estimated the financial impact of design changes to comply with the proposed federal fuel system integrity standards. The report recommended deferring fixes in order to accrue cost savings. Harley Copp, a former Ford engineer and the executive in charge of the crash testing program, \"testified that the highest level of Ford's management made the decision to go forward with the production of the Pinto, knowing that the gas tank was vulnerable to puncture and rupture at low rear impact speeds creating a significant risk of death or injury from fire and knowing that 'fixes' were feasible at nominal cost.\"\n\nThe appellate court affirmed the trial court.\n\nFord contested the trial court judgement on the basis of errors, and contested the punitive damages award on the grounds of an absence of malice and that the punitive damages award was not authorized by statute and was unconstitutional.\n\nThe appellate court \"concluded that Ford has failed to demonstrate that any errors or irregularities occurred during the trial which resulted in a miscarriage of justice requiring reversal.\"\n\nFord contended that the trial court erroneously admitted Ford's \"Fuel System Integrity Program Financial Review\" report as irrelevant and prejudicial. In the document, Ford engineers recommending deferring the installation of \"flak suits\" or \"bladders,\" available at a cost of $4 to $8 per car, in all Ford cars to 1976, which allowed the company to realize a savings of $20.9 million. The appellate court ruled that the report was highly relevant in that \"A reasonable inference may be drawn from the evidence that despite management's knowledge that the Pinto's fuel system could be made safe at a cost of but $4 to $8 per car, it decided to defer corrective measures to save money and enhance profits.\"\n\nFord contended the punitive damages on two grounds: 1) punitive damages are statutorily and constitutionally impermissible in design defect cases; and 2) there was no evidentiary support for a finding of malice or corporate responsibility for malice.\n\n\nThe appellate court found no statutory impediments to punitive damages and said that \"Ford's contention that the statute is unconstitutional has been repeatedly rejected.\"\n\n\nFord contended that the \"Exemplary damages\" section of California's civil code required an \"evil motive,\" or an intent to injure the person harmed, for punitive damages, and argued absence of malice. The appellate court cited precedent that \"malice\" as used in California's \"Exemplary damages\" code included \"not only a malicious intention to injure the specific person harmed, but conduct evincing 'a conscious disregard of the probability that the actor's conduct will result in injury to others.'\" In Taylor v. Superior Court, the California Superior Court held that a conscious disregard of the safety of others is sufficient to meet the \"animus malus\" required for punitive damages awards, adding: \"In order to justify an award of punitive damages on this basis, the plaintiff must establish that the defendant was aware of the probable dangerous consequences of his conduct, and that he wilfully and deliberately failed to avoid those consequences.\" In a commercial context, the imposition of punitive damages deters the furtherance of \"objectional corporate policies\" and encourages the remedy of safety concerns that might otherwise go unchecked.\n\nAccording to the appellate court decision,\n\nThere was ample evidence to support a finding of malice and Ford's responsibility for malice. Through the results of the crash tests Ford knew that the Pinto's fuel tank and rear structure would expose consumers to serious injury or death in a 20- to 30-mile-per-hour collision. There was evidence that Ford could have corrected the hazardous design defects at minimal cost but decided to defer correction of the shortcomings by engaging in a cost-benefit analysis balancing human lives and limbs against corporate profits. Ford's institutional mentality was shown to be one of callous indifference to public safety. There was substantial evidence that Ford's conduct constituted \"conscious disregard\" of the probability of injury to members of the consuming public...There is substantial evidence that management was aware of the crash tests showing the vulnerability of the Pinto's fuel tank to rupture at low speed rear impacts with consequent significant risk of injury or death of the occupants by fire. There was testimony from several sources that the test results were forwarded up the chain of command;...While much of the evidence was necessarily circumstantial, there was substantial evidence from which the jury could reasonably find that Ford's management decided to proceed with the production of the Pinto with knowledge of test results revealing design defects which rendered the fuel tank extremely vulnerable on rear impact at low speeds and endangered the safety and lives of the occupants. Such conduct constitutes corporate malice.\n\nFord argued that the amount awarded in punitive damages was excessive. The test for deciding whether the amount was excessive as a matter of law or was so grossly disproportionate as to raise the presumption that it was the product of passion or prejudice is four-prong: 1) the degree of reprehensibility of defendant's conduct; 2) wealth of the defendant; 3) the amount of compensatory damages; and 4) an amount which would serve as a deterrent effect on like conduct by the defendant and others. The appellate court held that\n\n...the conduct of Ford's management was reprehensible in the extreme. It exhibited a conscious and callous disregard of public safety in order to maximize corporate profits...Ford's tortious conduct endangered the lives of thousands of Pinto purchasers. Weighed against the factor of reprehensibility, the punitive damage award as reduced by the trial judge was not excessive.\"\n\nIn light of Ford's 7.7 billion dollar net worth and 983 million dollar income after taxes in 1976, the court found that the punitive award was approximately 0.005% of Ford's net worth and 0.03% of its income. The ratio of punitive damages to compensatory damages was approximately 1.4:1. Significantly, Ford did not argue about the excessiveness of the compensatory damages. Lastly, the punitive award was sufficient to require Ford to take notice, rather than allowing the company to write it off as a mere business expense.\n\nGrimshaw appeals from the order granting a conditional new trial and from the amended judgment entered pursuant to the order. Grimshaw argues that 1) the punitive damages awarded by the jury where not excessive as a matter of law; 2) the specification of reasons was inadequate; and 3) the court abused its discretion in cutting the award so drastically. The appellate court held that the trial court did not err in reducing the jury's award of punitive damages from more than 122 million to 3.5 million or in granting a new trial for excessive damages. There was no evidence showing that the trial judge abused his discretion, nor that he acted in any way that was not fair and reasonable under the circumstances.\n\n\n\n"}
{"id": "16184041", "url": "https://en.wikipedia.org/wiki?curid=16184041", "title": "Howard Wilson Emmons", "text": "Howard Wilson Emmons\n\nHoward Wilson Emmons (1912–1998) was a professor in the department of Mechanical Engineering at Harvard University. During his career he conducted original research on fluid mechanics, combustion and fire safety. Today he is most widely known for his pioneering work in the field of fire safety engineering. He has been called \"the father of modern fire science\" for his contribution to the understanding of flame propagation and fire dynamics. He also helped design the first supersonic wind tunnel, identified a signature of the transition to turbulence in boundary layer flows (now known as \"Emmons spots\"), and was the first to observe compressor stall in a gas turbine compressor (still a major item of research today). He initiated studies on diffusion flames inside a boundary layer, and Emmons problem is named after him. He was eventually awarded the Timoshenko Medal by the American Society of Mechanical Engineers and the 1968 Sir Alfred Egerton Gold Medal from The Combustion Institute.\n\nUpon Professor Emmons' death, Professor Patrick Pagni wrote, \"It is not possible to properly summarize the magnitude of Professor Emmons' unique contributions to the establishment of fire safety science as a discipline, other than to call him \"Mr. Fire Research\".\n\nHe continues to be remembered through the Emmons Lecture at International Symposium of The International Association for Fire Safety Science and the Howard W. Emmons Distinguished Scholar Endowment at Worcester Polytechnic Institute.\n\n\n\n\"The Drop Condensation of Vapors\"\nHarvard University Thesis (S.D.), 1938.\n\n\"Gas dynamics tables for air\"\nDover: New York, NY, 1947.\n\n\"Fundamentals of Gas Dynamics\"\nPrinceton University Press: Princeton NJ, 1958.\n\n\"Fluid mechanics and combustion\"\nProceedings of the 13th International Symposium on Combustion, p. 1-18\nPittsburgh, Pa., Combustion Institute, 1971.\n\n“The Further History of Fire Science” Combustion Science and Technology, 40,\n1984 (reprinted in Fire Technology, 21(3), 1985 )\n\n\"Thermodynamic properties of helium to 50.000°K\"\nby Wilbert James Lick, Howard Wilson Emmons\nHarvard University Press: Cambridge, MA, 1962.\n\n\"Transport properties of helium from 200 to 50.000°K\"\nby Wilbert James Lick, Howard Wilson Emmons\nHarvard University Press: Cambridge, MA, 1965.\n\n\"The fire whirl\"\nby Howard W. Emmons and Shuh-Jing Ying\nProceedings of the 11th International Symposium on Combustion, p. 475-486\nPittsburgh, Pa., Combustion Institute, 1967.\n\n\n"}
{"id": "33662690", "url": "https://en.wikipedia.org/wiki?curid=33662690", "title": "Ice nucleus", "text": "Ice nucleus\n\nAn ice nucleus is a particle which acts as the nucleus for the formation of an ice crystal in the atmosphere.\n\nThere are a number of mechanisms of ice nucleation in the atmosphere through which ice nuclei can catalyse the formation of ice particles. In the upper troposphere, water vapor can deposit directly onto solid particle. In clouds warmer than about −37 °C where liquid water can persist in a supercooled state, ice nuclei can trigger droplets to freeze. \n\nContact nucleation can occur if an ice nucleus collides with a supercooled droplet, but the more important mechanism of freezing is when an ice nucleus becomes immersed in a supercooled water droplet and then triggers freezing.\n\nIn the absence of an ice nucleating particle, pure water droplets can persist in a supercooled state to temperatures approaching −37 °C where they freeze homogeneously.\n\nIce particles can have a significant effect on cloud dynamics. They are known to be important in the processes by which clouds can become electrified, which causes lightning. They are also known to be able to form the seeds for rain droplets.\n\nMany different types of atmospheric particulate matter can act as ice nuclei, both natural and anthropogenic, including those composed of desert dust, soot, organic matter, bacteria (e.g. \"Pseudomonas syringae\"), pollen, fungal spores and volcanic ash amongst others. However, the exact nucleation potential of each type varies greatly, depending on the exact atmospheric conditions. Very little is known about the spatial distribution of these particles, their overall importance for global climate through ice cloud formation, and whether human activity has played a major role in changing these effects.\n\n"}
{"id": "14752", "url": "https://en.wikipedia.org/wiki?curid=14752", "title": "Iridium", "text": "Iridium\n\nIridium is a chemical element with symbol Ir and atomic number 77. A very hard, brittle, silvery-white transition metal of the platinum group, iridium is the second-densest element (after osmium) with a density of as defined by experimental X-ray crystallography. However at room temperature and standard atmospheric pressure, iridium has a density of , higher than osmium measured the same way. It is the most corrosion-resistant metal, even at temperatures as high as 2000 °C. Although only certain molten salts and halogens are corrosive to solid iridium, finely divided iridium dust is much more reactive and can be flammable.\n\nIridium was discovered in 1803 among insoluble impurities in natural platinum. Smithson Tennant, the primary discoverer, named iridium for the Greek goddess Iris, personification of the rainbow, because of the striking and diverse colors of its salts. Iridium is one of the rarest elements in Earth's crust, with annual production and consumption of only three tonnes. Ir and Ir are the only two naturally occurring isotopes of iridium, as well as the only stable isotopes; the latter is the more abundant of the two.\n\nThe most important iridium compounds in use are the salts and acids it forms with chlorine, though iridium also forms a number of organometallic compounds used in industrial catalysis, and in research. Iridium metal is employed when high corrosion resistance at high temperatures is needed, as in high-performance spark plugs, crucibles for recrystallization of semiconductors at high temperatures, and electrodes for the production of chlorine in the chloralkali process. Iridium radioisotopes are used in some radioisotope thermoelectric generators.\n\nIridium is found in meteorites in much higher abundance than in the Earth's crust. For this reason, the unusually high abundance of iridium in the clay layer at the Cretaceous–Paleogene boundary gave rise to the Alvarez hypothesis that the impact of a massive extraterrestrial object caused the extinction of dinosaurs and many other species 66 million years ago. Similarly, an iridium anomaly in core samples from the Pacific Ocean suggested the Eltanin impact of about 2.5 million years ago.\n\nIt is thought that the total amount of iridium in the planet Earth is much higher than that observed in crustal rocks, but as with other platinum-group metals, the high density and tendency of iridium to bond with iron caused most iridium to descend below the crust when the planet was young and still molten.\n\nA member of the platinum group metals, iridium is white, resembling platinum, but with a slight yellowish cast. Because of its hardness, brittleness, and very high melting point, solid iridium is difficult to machine, form, or work; thus powder metallurgy is commonly employed instead. It is the only metal to maintain good mechanical properties in air at temperatures above . It has the 10th highest boiling point among all elements and becomes a superconductor at temperatures below 0.14 K.\n\nIridium's modulus of elasticity is the second-highest among the metals, only being surpassed by osmium. This, together with a high shear modulus and a very low figure for Poisson's ratio (the relationship of longitudinal to lateral strain), indicate the high degree of stiffness and resistance to deformation that have rendered its fabrication into useful components a matter of great difficulty. Despite these limitations and iridium's high cost, a number of applications have developed where mechanical strength is an essential factor in some of the extremely severe conditions encountered in modern technology.\n\nThe measured density of iridium is only slightly lower (by about 0.12%) than that of osmium, the densest element known. Some ambiguity occurred regarding which of the two elements was denser, due to the small size of the difference in density and difficulties in measuring it accurately, but, with increased accuracy in factors used for calculating density X-ray crystallographic data yielded densities of 22.56 g/cm for iridium and 22.59 g/cm for osmium.\n\nIridium is the most corrosion-resistant metal known: it is not attacked by almost any acid, aqua regia, molten metals, or silicates at high temperatures. It can, however, be attacked by some molten salts, such as sodium cyanide and potassium cyanide, as well as oxygen and the halogens (particularly fluorine) at higher temperatures.\n\nIridium forms compounds in oxidation states between −3 and +9; the most common oxidation states are +3 and +4. Well-characterized examples of the high +6 oxidation state are rare, but include and two mixed oxides and . In addition, it was reported in 2009 that iridium(VIII) oxide () was prepared under matrix isolation conditions (6 K in Ar) by UV irradiation of an iridium-peroxo complex. This species, however, is not expected to be stable as a bulk solid at higher temperatures. The highest oxidation state (+9), which is also the highest recorded for \"any\" element, is only known in one cation, ; it is only known as gas-phase species and is not known to form any salts.\n\nIridium dioxide, , a blue black solid, is the only well-characterized oxide of iridium. A sesquioxide, , has been described as a blue-black powder which is oxidized to by . The corresponding disulfides, diselenides, sesquisulfides, and sesquiselenides are known, and has also been reported. Iridium also forms iridates with oxidation states +4 and +5, such as and , which can be prepared from the reaction of potassium oxide or potassium superoxide with iridium at high temperatures.\n\nAlthough no binary hydrides of iridium, are known, complexes are known that contain and , where iridium has the +1 and +3 oxidation states, respectively. The ternary hydride is believed to contain both the and the 18-electron anion.\n\nNo monohalides or dihalides are known, whereas trihalides, , are known for all of the halogens. For oxidation states +4 and above, only the tetrafluoride, pentafluoride and hexafluoride are known. Iridium hexafluoride, , is a volatile and highly reactive yellow solid, composed of octahedral molecules. It decomposes in water and is reduced to , a crystalline solid, by iridium black. Iridium pentafluoride has similar properties but it is actually a tetramer, , formed by four corner-sharing octahedra. Iridium metal dissolves in molten alkali-metal cyanides to produce the (hexacyanoiridate) ion.\nHexachloroiridic(IV) acid, , and its ammonium salt are the most important iridium compounds from an industrial perspective. They are involved in the purification of iridium and used as precursors for most other iridium compounds, as well as in the preparation of anode coatings. The ion has an intense dark brown color, and can be readily reduced to the lighter-colored and vice versa. Iridium trichloride, , which can be obtained in anhydrous form from direct oxidation of iridium powder by chlorine at 650 °C, or in hydrated form by dissolving in hydrochloric acid, is often used as a starting material for the synthesis of other Ir(III) compounds. Another compound used as a starting material is ammonium hexachloroiridate(III), . Iridium(III) complexes are diamagnetic (low-spin) and generally have an octahedral molecular geometry.\n\nOrganoiridium compounds contain iridium–carbon bonds where the metal is usually in lower oxidation states. For example, oxidation state zero is found in tetrairidium dodecacarbonyl, , which is the most common and stable binary carbonyl of iridium. In this compound, each of the iridium atoms is bonded to the other three, forming a tetrahedral cluster. Some organometallic Ir(I) compounds are notable enough to be named after their discoverers. One is Vaska's complex, , which has the unusual property of binding to the dioxygen molecule, . Another one is Crabtree's catalyst, a homogeneous catalyst for hydrogenation reactions. These compounds are both square planar, d complexes, with a total of 16 valence electrons, which accounts for their reactivity.\n\nAn iridium-based organic LED material has been documented, and found to be much brighter than DPA or PPV, so could be the basis for flexible OLED lighting in the future.\n\nIridium has two naturally occurring, stable isotopes, Ir and Ir, with natural abundances of 37.3% and 62.7%, respectively. At least 37 radioisotopes have also been synthesized, ranging in mass number from 164 to 202. Ir, which falls between the two stable isotopes, is the most stable radioisotope, with a half-life of 73.827 days, and finds application in brachytherapy and in industrial radiography, particularly for nondestructive testing of welds in steel in the oil and gas industries; iridium-192 sources have been involved in a number of radiological accidents. Three other isotopes have half-lives of at least a day—Ir, Ir, and Ir. Isotopes with masses below 191 decay by some combination of β decay, α decay, and (rare) proton emission, with the exceptions of Ir, which decays by electron capture. Synthetic isotopes heavier than 191 decay by β decay, although Ir also has a minor electron capture decay path. All known isotopes of iridium were discovered between 1934 and 2008, with the most recent discoveries being Ir.\n\nAt least 32 metastable isomers have been characterized, ranging in mass number from 164 to 197. The most stable of these is Ir, which decays by isomeric transition with a half-life of 241 years, making it more stable than any of iridium's synthetic isotopes in their ground states. The least stable isomer is Ir with a half-life of only 2 µs. The isotope Ir was the first one of any element to be shown to present a Mössbauer effect. This renders it useful for Mössbauer spectroscopy for research in physics, chemistry, biochemistry, metallurgy, and mineralogy.\n\nThe discovery of iridium is intertwined with that of platinum and the other metals of the platinum group. Native platinum used by ancient Ethiopians and by South American cultures always contained a small amount of the other platinum group metals, including iridium. Platinum reached Europe as \"platina\" (\"silverette\"), found in the 17th century by the Spanish conquerors in a region today known as the department of Chocó in Colombia. The discovery that this metal was not an alloy of known elements, but instead a distinct new element, did not occur until 1748.\n\nChemists who studied platinum dissolved it in aqua regia (a mixture of hydrochloric and nitric acids) to create soluble salts. They always observed a small amount of a dark, insoluble residue. Joseph Louis Proust thought that the residue was graphite. The French chemists Victor Collet-Descotils, Antoine François, comte de Fourcroy, and Louis Nicolas Vauquelin also observed the black residue in 1803, but did not obtain enough for further experiments.\n\nIn 1803, British scientist Smithson Tennant (1761–1815) analyzed the insoluble residue and concluded that it must contain a new metal. Vauquelin treated the powder alternately with alkali and acids and obtained a volatile new oxide, which he believed to be of this new metal—which he named \"ptene\", from the Greek word \"ptēnós\", \"winged\". Tennant, who had the advantage of a much greater amount of residue, continued his research and identified the two previously undiscovered elements in the black residue, iridium and osmium. He obtained dark red crystals (probably of ]·\"n\") by a sequence of reactions with sodium hydroxide and hydrochloric acid. He named iridium after Iris (), the Greek winged goddess of the rainbow and the messenger of the Olympian gods, because many of the salts he obtained were strongly colored. Discovery of the new elements was documented in a letter to the Royal Society on June 21, 1804.\n\nBritish scientist John George Children was the first to melt a sample of iridium in 1813 with the aid of \"the greatest galvanic battery that has ever been constructed\" (at that time). The first to obtain high-purity iridium was Robert Hare in 1842. He found it had a density of around 21.8 g/cm and noted the metal is nearly immalleable and very hard. The first melting in appreciable quantity was done by Henri Sainte-Claire Deville and Jules Henri Debray in 1860. They required burning more than 300 liters of pure and gas for each kilogram of iridium.\n\nThese extreme difficulties in melting the metal limited the possibilities for handling iridium. John Isaac Hawkins was looking to obtain a fine and hard point for fountain pen nibs, and in 1834 managed to create an iridium-pointed gold pen. In 1880, John Holland and William Lofland Dudley were able to melt iridium by adding phosphorus and patented the process in the United States; British company Johnson Matthey later stated they had been using a similar process since 1837 and had already presented fused iridium at a number of World Fairs. The first use of an alloy of iridium with ruthenium in thermocouples was made by Otto Feussner in 1933. These allowed for the measurement of high temperatures in air up to 2000 °C.\n\nIn Munich, Germany in 1957 Rudolf Mössbauer, in what has been called one of the \"landmark experiments in twentieth-century physics\", discovered the resonant and recoil-free emission and absorption of gamma rays by atoms in a solid metal sample containing only Ir. This phenomenon, known as the Mössbauer effect (which has since been observed for other nuclei, such as Fe), and developed as Mössbauer spectroscopy, has made important contributions to research in physics, chemistry, biochemistry, metallurgy, and mineralogy. Mössbauer received the Nobel Prize in Physics in 1961, at the age 32, just three years after he published his discovery. In 1986 Rudolf Mössbauer was honored for his achievements with the Albert Einstein Medal and the Elliot Cresson Medal.\n\nIridium is one of the nine least abundant stable elements in Earth's crust, having an average mass fraction of 0.001 ppm in crustal rock; platinum is 10 times more abundant, gold is 40 times more abundant, and silver and mercury are 80 times more abundant. Tellurium is about as abundant as iridium. In contrast to its low abundance in crustal rock, iridium is relatively common in meteorites, with concentrations of 0.5 ppm or more. The overall concentration of iridium on Earth is thought to be much higher than what is observed in crustal rocks, but because of the density and siderophilic (\"iron-loving\") character of iridium, it descended below the crust and into Earth's core when the planet was still molten.\n\nIridium is found in nature as an uncombined element or in natural alloys; especially the iridium–osmium alloys, osmiridium (osmium-rich), and iridosmium (iridium-rich). In the nickel and copper deposits, the platinum group metals occur as sulfides (i.e. (Pt,Pd)S), tellurides (i.e. PtBiTe), antimonides (PdSb), and arsenides (i.e. ). In all of these compounds, platinum is exchanged by a small amount of iridium and osmium. As with all of the platinum group metals, iridium can be found naturally in alloys with raw nickel or raw copper. A number of iridium-dominant minerals, with iridium as the species-forming element, are known. They are exceedingly rare and often represent the iridium analogues of the above-given ones. The examples are irarsite and cuproiridsite, to mention some.\n\nWithin Earth's crust, iridium is found at highest concentrations in three types of geologic structure: igneous deposits (crustal intrusions from below), impact craters, and deposits reworked from one of the former structures. The largest known primary reserves are in the Bushveld igneous complex in South Africa, (near the largest known impact crater, the Vredefort crater) though the large copper–nickel deposits near Norilsk in Russia, and the Sudbury Basin (also an impact crater) in Canada are also significant sources of iridium. Smaller reserves are found in the United States. Iridium is also found in secondary deposits, combined with platinum and other platinum group metals in alluvial deposits. The alluvial deposits used by pre-Columbian people in the Chocó Department of Colombia are still a source for platinum-group metals. As of 2003, world reserves have not been estimated.\n\nThe [Cretaceous–Paleogene boundary] of 66 million years ago, marking the temporal border between the Cretaceous and Paleogene periods of geological time, was identified by a thin stratum of iridium-rich clay. A team led by Luis Alvarez proposed in 1980 an extraterrestrial origin for this iridium, attributing it to an asteroid or comet impact. Their theory, known as the Alvarez hypothesis, is now widely accepted to explain the extinction of the non-avian dinosaurs. A large buried impact crater structure with an estimated age of about 66 million years was later identified under what is now the Yucatán Peninsula (the Chicxulub crater). Dewey M. McLean and others argue that the iridium may have been of volcanic origin instead, because Earth's core is rich in iridium, and active volcanoes such as Piton de la Fournaise, in the island of Réunion, are still releasing iridium.\n\nIridium is also obtained commercially as a by-product from nickel and copper mining and processing. During electrorefining of copper and nickel, noble metals such as silver, gold and the platinum group metals as well as selenium and tellurium settle to the bottom of the cell as \"anode mud\", which forms the starting point for their extraction. To separate the metals, they must first be brought into solution. Several separation methods are available depending on the nature of the mixture; two representative methods are fusion with sodium peroxide followed by dissolution in aqua regia, and dissolution in a mixture of chlorine with hydrochloric acid.\n\nAfter the mixture is dissolved, iridium is separated from the other platinum group metals by precipitating ammonium hexachloroiridate () or by extracting with organic amines. The first method is similar to the procedure Tennant and Wollaston used for their separation. The second method can be planned as continuous liquid–liquid extraction and is therefore more suitable for industrial scale production. In either case, the product is reduced using hydrogen, yielding the metal as a powder or \"sponge\" that can be treated using powder metallurgy techniques.\n\nIridium prices have fluctuated over a considerable range. With a relatively small volume in the world market (compared to other industrial metals like aluminium or copper), the iridium price reacts strongly to instabilities in production, demand, speculation, hoarding, and politics in the producing countries.\nAs a substance with rare properties, its price has been particularly influenced by changes in modern technology:\nThe gradual decrease between 2001 and 2003 has been related to an oversupply of Ir crucibles used for industrial growth of large single crystals.\nLikewise the prices above 1000 USD/oz between 2010 and 2014 have been explained with the installation of production facilities for single crystal sapphire used in LED backlights for TVs.\n\nThe demand for iridium surged from 2.5 tonnes in 2009 to 10.4 tonnes in 2010, mostly because of electronics-related applications that saw a rise from 0.2 to 6 tonnes – iridium crucibles are commonly used for growing large high-quality single crystals, demand for which has increased sharply. This increase in iridium consumption is predicted to saturate due to accumulating stocks of crucibles, as happened earlier in the 2000s. Other major applications include spark plugs that consumed 0.78 tonnes of iridium in 2007, electrodes for the chloralkali process (1.1 t in 2007) and chemical catalysts (0.75 t in 2007).\n\nThe high melting point, hardness and corrosion resistance of iridium and its alloys determine most of its applications. Iridium (or sometimes platinum alloys or osmium) and mostly iridium alloys have a low wear and are used, for example, for multi-pored spinnerets, through which a plastic polymer melt is extruded to form fibers, such as rayon. Osmium–iridium is used for compass bearings and for balances.\n\nTheir resistance to arc erosion makes iridium alloys ideal for electrical contacts for spark plugs, and iridium-based spark plugs are particularly used in aviation.\n\nPure iridium is extremely brittle, to the point of being hard to weld because the heat-affected zone cracks, but it can be made more ductile by addition of small quantities of titanium and zirconium (0.2% of each apparently works well)\n\nCorrosion and heat resistance makes iridium an important alloying agent. Certain long-life aircraft engine parts are made of an iridium alloy, and an iridium–titanium alloy is used for deep-water pipes because of its corrosion resistance. Iridium is also used as a hardening agent in platinum alloys. The Vickers hardness of pure platinum is 56 HV, whereas platinum with 50% of iridium can reach over 500 HV.\n\nDevices that must withstand extremely high temperatures are often made from iridium. For example, high-temperature crucibles made of iridium are used in the Czochralski process to produce oxide single-crystals (such as sapphires) for use in computer memory devices and in solid state lasers. The crystals, such as gadolinium gallium garnet and yttrium gallium garnet, are grown by melting pre-sintered charges of mixed oxides under oxidizing conditions at temperatures up to 2100 °C.\n\nIridium compounds are used as catalysts in the Cativa process for carbonylation of methanol to produce acetic acid.\n\nThe radioisotope iridium-192 is one of the two most important sources of energy for use in industrial γ-radiography for non-destructive testing of metals. Additionally, Ir is used as a source of gamma radiation for the treatment of cancer using brachytherapy, a form of radiotherapy where a sealed radioactive source is placed inside or next to the area requiring treatment. Specific treatments include high-dose-rate prostate brachytherapy, bilary duct brachytherapy, and intracavitary cervix brachytherapy.\n\nIridium is a good catalyst for the decomposition of hydrazine (into hot nitrogen and ammonia), and this is used in practice in low-thrust rocket engines; there are more details in the monopropellant rocket article.\n\nAn alloy of 90% platinum and 10% iridium was used in 1889 to construct the International Prototype Metre and kilogram mass, kept by the International Bureau of Weights and Measures near Paris. The meter bar was replaced as the definition of the fundamental unit of length in 1960 by a line in the atomic spectrum of krypton, but the kilogram prototype is still the international standard of mass (until 20 May 2019).\n\nIridium has been used in the radioisotope thermoelectric generators of unmanned spacecraft such as the \"Voyager\", \"Viking\", \"Pioneer\", \"Cassini\", \"Galileo\", and \"New Horizons\". Iridium was chosen to encapsulate the plutonium-238 fuel in the generator because it can withstand the operating temperatures of up to 2000 °C and for its great strength.\n\nAnother use concerns X-ray optics, especially X-ray telescopes. The mirrors of the Chandra X-ray Observatory are coated with a layer of iridium 60 nm thick. Iridium proved to be the best choice for reflecting X-rays after nickel, gold, and platinum were also tested. The iridium layer, which had to be smooth to within a few atoms, was applied by depositing iridium vapor under high vacuum on a base layer of chromium.\n\nIridium is used in particle physics for the production of antiprotons, a form of antimatter. Antiprotons are made by shooting a high-intensity proton beam at a \"conversion target\", which needs to be made from a very high density material. Although tungsten may be used instead, iridium has the advantage of better stability under the shock waves induced by the temperature rise due to the incident beam.\nCarbon–hydrogen bond activation (C–H activation) is an area of research on reactions that cleave carbon–hydrogen bonds, which were traditionally regarded as unreactive. The first reported successes at activating C–H bonds in saturated hydrocarbons, published in 1982, used organometallic iridium complexes that undergo an oxidative addition with the hydrocarbon.\n\nIridium complexes are being investigated as catalysts for asymmetric hydrogenation. These catalysts have been used in the synthesis of natural products and able to hydrogenate certain difficult substrates, such as unfunctionalized alkenes, enantioselectively (generating only one of the two possible enantiomers).\n\nIridium forms a variety of complexes of fundamental interest in triplet harvesting.\n\nIridium–osmium alloys were used in fountain pen nib tips. The first major use of iridium was in 1834 in nibs mounted on gold. Since 1944, the famous Parker 51 fountain pen was fitted with a nib tipped by a ruthenium and iridium alloy (with 3.8% iridium). The tip material in modern fountain pens is still conventionally called \"iridium\", although there is seldom any iridium in it; other metals such as ruthenium, osmium and tungsten have taken its place.\n\nAn iridium–platinum alloy was used for the touch holes or vent pieces of cannon. According to a report of the Paris Exhibition of 1867, one of the pieces being exhibited by Johnson and Matthey \"has been used in a Withworth gun for more than 3000 rounds, and scarcely shows signs of wear yet. Those who know the constant trouble and expense which are occasioned by the wearing of the vent-pieces of cannon when in active service, will appreciate this important adaptation\".\n\nThe pigment \"iridium black\", which consists of very finely divided iridium, is used for painting porcelain an intense black; it was said that \"all other porcelain black colors appear grey by the side of it\".\n\nIridium in bulk metallic form is not biologically important or hazardous to health due to its lack of reactivity with tissues; there are only about 20 parts per trillion of iridium in human tissue. Like most metals, finely divided iridium powder can be hazardous to handle, as it is an irritant and may ignite in air.\nVery little is known about the toxicity of iridium compounds, because they are used in very small amounts, but soluble salts, such as the iridium halides, could be hazardous due to elements other than iridium or due to iridium itself. However, most iridium compounds are insoluble, which makes absorption into the body difficult.\n\nA radioisotope of iridium, , is dangerous, like other radioactive isotopes. The only reported injuries related to iridium concern accidental exposure to radiation from used in brachytherapy. High-energy gamma radiation from can increase the risk of cancer. External exposure can cause burns, radiation poisoning, and death. Ingestion of Ir can burn the linings of the stomach and the intestines. Ir, Ir, and Ir tend to deposit in the liver, and can pose health hazards from both gamma and beta radiation.\n\n"}
{"id": "22347533", "url": "https://en.wikipedia.org/wiki?curid=22347533", "title": "Kyoto box", "text": "Kyoto box\n\nThe Kyoto Box is a solar cooker constructed from polypropylene with acryl glass cover. The oven would trap the suns rays, creating enough heat to cook or boil water. It was invented by Jon Bohmer, a Norwegian born inventor based in Kenya. It was marketed as part of the Kyoto Energy product range.\n\nTwo cardboard boxes, a newer, commercial version was constructed from polypropylene, making it more robust and durable. Temperatures inside the box quickly reached 80 degrees Celsius (176 degrees Fahrenheit) on a sunny day. According to the Kyoto Energy website, temperatures could reach a maximum of 165 degrees Celsius. Bohmer claimed that the oven was capable of boiling 10 liters of water in two to three hours. \n\nSolar cookers are being used by hundreds of thousands of people throughout the world. Solar cookers can also pasteurize or sterilize water to provide safe drinking water without using or collecting firewood.\n\nAfter the award of the Financial Times Climate Change Challenge 2009 the design of the Kyoto Box was modified from the original cardboard to polypropylene, boosting the cooking performance. A further innovation was the use of a white reflector instead of one covered with aluminum foil or Mylar.\n\nThe Norwegian inventor, Jon Bohmer, made the first model of the Kyoto Box with his daughters then aged 10 and 5 years old. It was first just a project with his children, but later won the FT Climate Change Challenge award. He won the first prize, since the invention reduced carbon emissions by eliminating the need to burn wood.\n\n\n"}
{"id": "224312", "url": "https://en.wikipedia.org/wiki?curid=224312", "title": "Lapse rate", "text": "Lapse rate\n\nThe lapse rate is the rate at which temperature in Earth's atmosphere decreases with an increase in altitude, or increases with the decrease in altitude. \"Lapse rate\" arises from the word \"lapse\", in the sense of a gradual change. \n\nAlthough this concept is most often applied to the Earth's troposphere, it can be extended to any gravitationally supported parcel of gas.\n\nA formal definition from the \"Glossary of Meteorology\" is:\n\nIn general, a lapse rate is the negative of the rate of temperature change with altitude change, thus:\n\nwhere formula_2 (sometimes formula_3) is the lapse rate given in units of temperature divided by units of altitude, \"T\" is temperature, and \"z\" is altitude.\n\nThe temperature profile of the atmosphere is a result of an interaction between radiation and convection. Sunlight hits the ground and heats it. The ground then heats the air at the surface. If radiation were the only way to transfer heat from the ground to space, the greenhouse effect of gases in the atmosphere would keep the ground at roughly , and the temperature would decay exponentially with height.\n\nHowever, when air is hot, it tends to expand, which lowers its density. Thus, hot air tends to rise and transfer heat upward. This is the process of convection. Convection comes to equilibrium when a parcel of air at a given altitude has the same density as the other air at the same elevation.\n\nWhen a parcel of air expands, it pushes on the air around it, doing work (thermodynamics). Since the parcel does work but gains no heat, it loses internal energy so that its temperature decreases. The process of expanding and contracting without exchanging heat is an adiabatic process. The term \"adiabatic\" means that no heat transfer occurs into or out of the parcel. Air has low thermal conductivity, and the bodies of air involved are very large, so transfer of heat by conduction is negligibly small.\n\nThe adiabatic process for air has a characteristic temperature-pressure curve, so the process determines the lapse rate. When the air contains little water, this lapse rate is known as the dry adiabatic lapse rate: the rate of temperature decrease is ( per 1,000 ft) (3.0 °C/1,000 ft). The reverse occurs for a sinking parcel of air.\n\nOnly the troposphere (up to approximately of altitude) in the Earth's atmosphere undergoes convection: the stratosphere does not generally convect. However, some exceptionally energetic convection processes -- notably volcanic eruption columns and overshooting tops associated with severe supercell thunderstorms -- may \"locally\" and \"temporarily\" inject convection through the tropopause and into the stratosphere.\n\nThese calculation use a very simple model of an atmosphere, either dry or moist, within a still vertical column at equilibrium. \n\nThermodynamics defines an adiabatic process as:\n\nthe first law of thermodynamics can be written as\n\nAlso, since formula_6 and formula_7, we can show that:\n\nwhere formula_9 is the specific heat at constant pressure and formula_10 is the specific volume.\n\nAssuming an atmosphere in hydrostatic equilibrium:\n\nwhere \"g\" is the standard gravity and \"formula_12\" is the density. Combining these two equations to eliminate the pressure, one arrives at the result for the dry adiabatic lapse rate (DALR),\n\nThe presence of water within the atmosphere(usually the troposphere) complicates the process of convection. Water vapor contains latent heat of vaporization. As a parcel of air rises and cools, it eventually becomes saturated; that is, the vapor pressure of water in equilibrium with liquid water has decreased (as temperature has decreased) to the point where it is equal to the actual vapor pressure of water. With further decrease in temperature the water vapor in excess of the equilibrium amount condenses, forming cloud, and releasing heat (latent heat of condensation). Before saturation, the rising air follows the dry adiabatic lapse rate. After saturation, the rising air follows the moist adiabatic lapse rate. The release of latent heat is an important source of energy in the development of thunderstorms.\n\nWhile the dry adiabatic lapse rate is a constant ( per 1,000 ft, ), the moist adiabatic lapse rate varies strongly with temperature. A typical value is around , (, , ). The formula for the moist adiabatic lapse rate is given by:\n\nwhere:\n\nThe environmental lapse rate (ELR), is the rate of decrease of temperature with altitude in the stationary atmosphere at a given time and location. As an average, the International Civil Aviation Organization (ICAO) defines an international standard atmosphere (ISA) with a temperature lapse rate of or from sea level to 11 km or . From 11 km up to 20 km or , the constant temperature is , which is the lowest assumed temperature in the ISA. The standard atmosphere contains no moisture. Unlike the idealized ISA, the temperature of the actual atmosphere does not always fall at a uniform rate with height. For example, there can be an inversion layer in which the temperature increases with altitude.\n\nThe varying environmental lapse rates throughout the Earth's atmosphere are of critical importance in meteorology, particularly within the troposphere. They are used to determine if the parcel of rising air will rise high enough for its water to condense to form clouds, and, having formed clouds, whether the air will continue to rise and form bigger shower clouds, and whether these clouds will get even bigger and form cumulonimbus clouds (thunder clouds).\n\nAs unsaturated air rises, its temperature drops at the dry adiabatic rate. The dew point also drops (as a result of decreasing air pressure) but much more slowly, typically about per 1,000 m. If unsaturated air rises far enough, eventually its temperature will reach its dew point, and condensation will begin to form. This altitude is known as the lifting condensation level (LCL) when mechanical lift is present and the convective condensation level (CCL) when mechanical lift is absent, in which case, the parcel must be heated from below to its convective temperature. The cloud base will be somewhere within the layer bounded by these parameters.\n\nThe difference between the dry adiabatic lapse rate and the rate at which the dew point drops is around per 1,000 m. Given a difference in temperature and dew point readings on the ground, one can easily find the LCL by multiplying the difference by 125 m/°C.\n\nIf the environmental lapse rate is less than the moist adiabatic lapse rate, the air is absolutely stable — rising air will cool faster than the surrounding air and lose buoyancy. This often happens in the early morning, when the air near the ground has cooled overnight. Cloud formation in stable air is unlikely.\n\nIf the environmental lapse rate is between the moist and dry adiabatic lapse rates, the air is conditionally unstable — an unsaturated parcel of air does not have sufficient buoyancy to rise to the LCL or CCL, and it is stable to weak vertical displacements in either direction. If the parcel is saturated it is unstable and will rise to the LCL or CCL, and either be halted due to an inversion layer of convective inhibition, or if lifting continues, deep, moist convection (DMC) may ensue, as a parcel rises to the level of free convection (LFC), after which it enters the free convective layer (FCL) and usually rises to the equilibrium level (EL).\n\nIf the environmental lapse rate is larger than the dry adiabatic lapse rate, it has a superadiabatic lapse rate, the air is absolutely unstable — a parcel of air will gain buoyancy as it rises both below and above the lifting condensation level or convective condensation level. This often happens in the afternoon mainly over land masses. In these conditions, the likelihood of cumulus clouds, showers or even thunderstorms is increased.\n\nMeteorologists use radiosondes to measure the environmental lapse rate and compare it to the predicted adiabatic lapse rate to forecast the likelihood that air will rise. Charts of the environmental lapse rate are known as thermodynamic diagrams, examples of which include Skew-T log-P diagrams and tephigrams. (See also Thermals).\n\nThe difference in moist adiabatic lapse rate and the dry rate is the cause of foehn wind phenomenon (also known as \"Chinook winds\" in parts of North America). The phenomenon exists because warm moist air rises through orographic lifting up and over the top of a mountain range or large mountain. The temperature decreases with the dry adiabatic lapse rate, until it hits the dew point, where water vapor in the air begins to condense. Above that altitude, the adiabatic lapse rate decreases to the moist adiabatic lapse rate as the air continues to rise. Condensation is also commonly followed by precipitation on the top and windward sides of the mountain. As the air descends on the leeward side, it is warmed by adiabatic compression at the dry adiabatic lapse rate. Thus, the foehn wind at a certain altitude is warmer than the corresponding altitude on the windward side of the mountain range. In addition, because the air has lost much of its original water vapor content, the descending air creates an arid region on the leeward side of the mountain.\n\n\n\n"}
{"id": "53154894", "url": "https://en.wikipedia.org/wiki?curid=53154894", "title": "Leipzig L-IV experiment accident", "text": "Leipzig L-IV experiment accident\n\nOn June 23, 1942 the experimental early type of a nuclear reactor L-IV led to the first nuclear accident in history, consisting of steam explosion and reactor fire in Leipzig, Nazi Germany.\n\nShortly after the Leipzig L-IV atomic pile—worked on by Werner Heisenberg and Robert Döpel—demonstrated Germany's first signs of neutron propagation, the device was checked for a possible heavy water leak. During the inspection, air leaked in, igniting the uranium powder inside. The burning uranium boiled the water jacket, generating enough steam pressure to blow the reactor apart. Burning uranium powder scattered throughout the lab causing a larger fire at the facility.\n\nThis happened after 20 days of operation when Werner Paschen opened the machine at the request of Döpel after blisters formed at the gasket. As glowing Uranium powder shot to the 6 meter high ceiling and the apparatus heated up to 1.000 degree Heisenberg was asked for help but couldn't provide it.\n\nResults from trial L-IV, in the first half of 1942, indicated that the spherical geometry, with five metric tons of heavy water and 10 metric tons of metallic uranium, could sustain a fission reaction. So, \"the Germans were the first physicists in the world, with their Leipzig pile L-IV, to achieve positive neutron production.\" The results were set forth in an article by Robert Döpel, Klara Döpel and W. Heisenberg. The article was published at first in the \"Kernphysikalische Forschungsberichte\" (\"Research Reports in Nuclear Physics\"), a classified internal reporting vehicle of the Uranverein.\n\nThe Leipzig research group was led by Heisenberg until 1942 who in winter 1939/1940 reported on the possibilities and feasibility of energy extraction from Uranium for a Uranium-machine and nuclear bomb. After the report Heisenberg withdrew from practical experiments and left the execution of the experiments L-I, L-II, L-III and L-IV mostly up to his coworkers. The accident ended the Leipzig Uranium projects.\n\n"}
{"id": "25675599", "url": "https://en.wikipedia.org/wiki?curid=25675599", "title": "List of natural gas power stations", "text": "List of natural gas power stations\n\nThe following page lists power stations that run on natural gas, a non-renewable resource. \nStations that are only at a \"proposed stage\" or \"decommissioned\", and power stations that are smaller than in nameplate capacity, are not included in this list. \n"}
{"id": "563456", "url": "https://en.wikipedia.org/wiki?curid=563456", "title": "Methanogen", "text": "Methanogen\n\nMethanogens are microorganisms that produce methane as a metabolic byproduct in anoxic conditions. They are prokaryotic and belong to the domain of archaea. They are common in wetlands, where they are responsible for marsh gas, and in the digestive tracts of animals such as ruminants and humans, where they are responsible for the methane content of belching in ruminants and flatulence in humans. In marine sediments the biological production of methane, also termed methanogenesis, is generally confined to where sulfates are depleted, below the top layers. Moreover, the methanogenic archaea populations play an indispensable role in anaerobic wastewater treatments. Others are extremophiles, found in environments such as hot springs and submarine hydrothermal vents as well as in the \"solid\" rock of the Earth's crust, kilometers below the surface.\n\nMethanogens are coccoid (spherical shaped) or bacilli (rod shaped). There are over 50 described species of methanogens, which do not form a monophyletic group, although all methanogens belong to Archaea. They are mostly anaerobic organisms that cannot function under aerobic conditions, but recently a species (\"Candidatus Methanothrix paradoxum\") has been identified that can function in anoxic microsites within aerobic environments. They are very sensitive to the presence of oxygen even at trace level. Usually, they cannot sustain oxygen stress for a prolonged time. However, \"Methanosarcina barkeri\" is exceptional in possessing a superoxide dismutase (SOD) enzyme, and may survive longer than the others in the presence of O. Some methanogens, called hydrogenotrophic, use carbon dioxide (CO) as a source of carbon, and hydrogen as a reducing agent.\n\nThe reduction of carbon dioxide into methane in the presence of hydrogen can be expressed as follows:\n\nSome of the CO is reacted with the hydrogen to produce methane, which creates an electrochemical gradient across cell membrane, used to generate ATP through chemiosmosis. In contrast, plants and algae use water as their reducing agent.\n\nMethanogens lack peptidoglycan, a polymer that is found in the cell walls of the Bacteria but not in those of Archaea. Some methanogens have a cell wall that is composed of pseudopeptidoglycan. Other methanogens do not, but have at least one paracrystalline array (S-layer) made up of proteins that fit together like a jigsaw puzzle.\n\nMethanogens play the vital ecological role in anaerobic environments of removing excess hydrogen and fermentation products that have been produced by other forms of anaerobic respiration. Methanogens typically thrive in environments in which all electron acceptors other than CO (such as oxygen, nitrate, ferriciron (Fe(III)), and sulfate) have been depleted. In deep basaltic rocks near the mid ocean ridges, they can obtain their hydrogen from the serpentinisation reaction of olivine as observed in the hydrothermal field of Lost City.\n\nThe thermal breakdown of water and water radiolysis are other possible sources of hydrogen.\n\nMethanogens are key agents of remineralization of organic carbon in continental margin sediments and other aquatic sediments with high rates of sedimentation and high sediment organic matter. Under the correct conditions of pressure and temperature, biogenic methane can accumulate in massive deposits of methane clathrates, which account for a significant fraction of organic carbon in continental margin sediments and represent a key reservoir of a potent greenhouse gas.\n\nMethanogens have been found in several extreme environments on Earth – buried under kilometres of ice in Greenland and living in hot, dry desert soil. They are known to be the most common archaebacteria in deep subterranean habitats. Live microbes making methane were found in a glacial ice core sample retrieved from about three kilometres under Greenland by researchers from the University of California, Berkeley. They also found a constant metabolism able to repair macromolecular damage, at temperatures of 145 to –40 °C.\n\nAnother study has also discovered methanogens in a harsh environment on Earth. Researchers studied dozens of soil and vapour samples from five different desert environments in Utah, Idaho and California in the United States, and in Canada and Chile. Of these, five soil samples and three vapour samples from the vicinity of the Mars Desert Research Station in Utah were found to have signs of viable methanogens.\n\nSome scientists have proposed that the presence of methane in the Martian atmosphere may be indicative of native methanogens on that planet.\n\nClosely related to the methanogens are the anaerobic methane oxidizers, which utilize methane as a substrate in conjunction with the reduction of sulfate and nitrate. Most methanogens are autotrophic producers, but those that oxidize CHCOO are classed as chemotroph instead.\n\nComparative genomic analysis has led to the identification of 31 signature proteins which are specific for the methanogens (also known as \"Methanoarchaeota\"). Most of these proteins are related to methanogenesis, and they could serve as potential molecular markers for the methanogens. Additionally, 10 proteins found in all methanogens which are shared by \"Archaeoglobus\", suggest that these two groups are related. In phylogenetic trees, the methanogens are not monophyletic and they are generally split into three clades. Hence, the unique shared presence of large numbers of proteins by all methanogens could be due to lateral gene transfers.\n\nMethanogens are known to produce methane from substrates such as H/CO, acetate, formate, methanol and methylamines in a process called methanogenesis. Different methanogenic reactions are catalyzed by unique sets of enzymes and coenzymes. While reaction mechanism and energetics vary between one reaction and another, all of these reactions contribute to net positive energy production by creating ion concentration gradients that are used to drive ATP synthesis. The overall reaction for H/CO methanogenesis is:\n\nWell-studied organisms that produce methane via H2/CO methanogenesis include \"Methanosarcina barkeri\", \"Methanobacterium thermoautotrophicum\", and \"Methanobacterium wolfei\". These organism are typically found in anaerobic environments.\n\nIn the earliest stage of H/CO methanogenesis, CO binds to methylfuran (MF) and is reduced to formyl-MF. This endergonic reductive process (∆G˚’= +16 kJ/mol) is dependent on the availability of H and is catalyzed by the enzyme formyl-MF dehydrogenase.\n\nThe formyl constituent of formyl-MF is then transferred to the coenzyme tetrahydromethanopterin (H4MPT) and is catalyzed by a soluble enzyme known as formyl transferase. This results in the formation of formyl-H4MPT.\n\nFormyl-H4MPT is subsequently reduced to methenyl-H4MPT. Methenyl-H4MPT then undergoes a one-step hydrolysis followed by a two-step reduction to methyl-H4MPT. The two-step reversible reduction is assisted by coenzyme F whose hydride acceptor spontaneously oxidizes. Once oxidized, F’s electron supply is replenished by accepting electrons from H. This step is catalyzed by methylene H4MPT dehydrogenase.\n\nNext, the methyl group of methyl-M4MPT is transferred to coenzyme M via a methyltransferase-catalyzed reaction.\n\nThe final step of H/CO methanogenic involves methyl-coenzyme M reductase and two coenzymes: N-7 mercaptoheptanoylthreonine phosphate (HS-HTP) and coenzyme F. HS-HTP donates electrons to methyl-coenzyme M allowing the formation of methane and mixed disulfide of HS-CoM. F, on the other hand, serves as a prosthetic group to the reductase. H donates electrons to the mixed disulfide of HS-CoM and regenerates coenzyme M.\n\nMethanogens are widely used in anaerobic digestors to treat wastewater as well as aqueous organic pollutants. Industries have selected methanogens for their ability to perform biomethanation during wastewater decomposition thereby rendering the process sustainable and cost-effective.\n\nBio-decomposition in the anaerobic digester involves a four-staged cooperative action performed by different microorganisms. The first stage is the hydrolysis of insoluble polymerized organic matter by anaerobes such as Streptococcus and Enterobacterium. In the second stage, acidogens breakdown dissolved organic pollutants in wastewater to fatty acids. In the third stage, acetogens convert fatty acids to acetates. In the final stage, methanogens metabolize acetates to gaseous methane. The byproduct methane leaves the aqueous layer and serves as an energy source to power wastewater-processing within the digestor, thus generating a self-sustaining mechanism.\n\nMethanogens also effectively decrease the concentration of organic matter in wastewater run-off and minimizes greenhouse gas emissions. For instance, agricultural wastewater, highly rich in organic material, has been a major cause of aquatic ecosystem degradation. The chemical imbalances can lead to severe ramifications such as eutrophication. Through anaerobic digestion, the purification of wastewater can prevent unexpected blooms in water systems as well as trap methanogenesis within digesters. This allocates biomethane for energy production and prevents a potent greenhouse gas, methane, from being released into the atmosphere.\n\nThe organic components of wastewater vary vastly. Chemical structures of the organic matter select for specific methanogens to perform anaerobic digestion. An example is the members of Methanosaeta genus dominate the digestion of palm oil mill effluent (POME) and brewery waste. Modernizing wastewater treatment systems to incorporate higher diversity of microorganisms to decrease organic content in treatment is under active research in the field of microbiological and chemical engineering. Current new generations of Staged Multi-Phase Anaerobic reactors and Upflow Sludge Bed reactor systems are designed to have innovated features to counter high loading wastewater input, extreme temperature conditions, and possible inhibitory compounds.\n\n\n"}
{"id": "2740949", "url": "https://en.wikipedia.org/wiki?curid=2740949", "title": "Mott insulator", "text": "Mott insulator\n\nMott insulators are a class of materials that should conduct electricity under conventional band theories, but are insulators when measured (particularly at low temperatures). This effect is due to electron–electron interactions, which are not considered in conventional band theory.\n\nThe bandgap in a Mott insulator exists between bands of like character, such as 3d character, whereas the bandgap in charge transfer insulators exists between anion and cation states, such as between O 2p and Ni 3d bands in NiO.\nAlthough the band theory of solids had been very successful in describing various electrical properties of materials, in 1937 Jan Hendrik de Boer and Evert Johannes Willem Verwey pointed out that a variety of transition metal oxides predicted to be conductors by band theory (because they have an odd number of electrons per unit cell) are insulators. Nevill Mott and Rudolf Peierls then (also in 1937) predicted that this anomaly can be explained by including interactions between electrons.\n\nIn 1949, in particular, Mott proposed a model for NiO as an insulator, where conduction is based on the formula\n\nIn this situation, the formation of an energy gap preventing conduction can be understood as the competition between the Coulomb potential \"U\" between 3\"d\" electrons and the transfer integral \"t\" of 3\"d\" electrons between neighboring atoms (the transfer integral is a part of the tight-binding approximation). The total energy gap is then\n\nwhere \"z\" is the number of nearest-neighbor atoms.\n\nIn general, Mott insulators occur when the repulsive Coulomb potential \"U\" is large enough to create an energy gap. One of the simplest theories of Mott insulators is the 1963 Hubbard model. The crossover from a metal to a Mott insulator as \"U\" is increased can be predicted within the so-called dynamical mean field theory.\n\n\"Mottism\" denotes the additional ingredient, aside from antiferromagnetic ordering, which is necessary to fully describe a Mott Insulator. In other words, we might write\n\nThus, mottism accounts for all of the properties of Mott insulators that cannot be attributed simply to antiferromagnetism.\n\nThere are a number of properties of Mott insulators, derived from both experimental and theoretical observations, which cannot be attributed to antiferromagnetic ordering and thus constitute mottism. These properties include\n\n\nMott insulators are of growing interest in advanced physics research, and are not yet fully understood. They have applications in thin-film magnetic heterostructures and high-temperature superconductivity, for example.\n\nThis kind of insulator can become a conductor by changing some parameters, which may be composition, pressure, strain, voltage, or magnetic field. The effect is known as a Mott transition and can be used to build smaller field-effect transistors, switches and memory devices than possible with conventional materials.\n\n\n"}
{"id": "4981136", "url": "https://en.wikipedia.org/wiki?curid=4981136", "title": "Multiple-effect humidification", "text": "Multiple-effect humidification\n\nMultiple-effect humidification (MEH) is a method used for thermal desalination of sea water. It uses multiple evaporation–condensation cycles at separate temperature levels to minimize the total energy consumption of solar humidification processes.\n"}
{"id": "2687833", "url": "https://en.wikipedia.org/wiki?curid=2687833", "title": "Net positive suction head", "text": "Net positive suction head\n\nIn a hydraulic circuit, net positive suction head (NPSH) may refer to one of two quantities in the analysis of cavitation:\n\nNPSH is particularly relevant inside centrifugal pumps and turbines, which are parts of a hydraulic system that are most vulnerable to cavitation. If cavitation occurs, the drag coefficient of the impeller vanes will increase drastically—possibly stopping flow altogether—and prolonged exposure will damage the impeller.\n\nIn a pump, cavitation will first occur at the inlet of the impeller. Denoting the inlet by \"i\", the NPSH at this point is defined as:\n\nformula_1\n\nwhere formula_2 is the absolute pressure at the inlet, formula_3 is the average velocity at the inlet, formula_4 is the fluid density, formula_5 is the acceleration of gravity and formula_6 is the vapor pressure of the fluid. Note that it is equivalent to the sum of both the static and dynamic heads – that is, the stagnation head – from which one deducts the head corresponding to the equilibrium vapor pressure, hence \"net positive suction head\".\n\nApplying the first law of thermodynamics for control volumes enclosing the suction free surface 0 and the pump inlet \"i\", under the assumption that the kinetic energy at 0 is negligible, that the fluid is inviscid, and that the fluid density is constant:\n\nformula_7\n\nUsing the above application of Bernoulli to eliminate the velocity term and local pressure terms in the definition of NPSH:\n\nformula_8\n\nThis is the standard expression for the available NPSH at point. Cavitation will occur at the point \"i\" when the available NPSH is less than the NPSH required to prevent cavitation (NPSH). For simple impeller systems, NPSH can be derived theoretically, but very often it is determined empirically. Note NPSHand NPSH are in absolute units and usually expressed in \"m abs\" not \"psia\".\n\nExperimentally, NPSH is often defined as the NPSH, the point at which the head output of the pump decreases by 3 % at a given flow due to reduced hydraulic performance. On multi-stage pumps this is limited to a 3 % drop in the first stage head.\n\nThe calculation of NPSH in a reaction turbine is different to the calculation of NPSH in a pump, because the point at which cavitation will first occur is in a different place. In a reaction turbine, cavitation will first occur at the outlet of the impeller, at the entrance of the draft tube. Denoting the entrance of the draft tube by \"e\", the NPSH is defined in the same way as for pumps:\n\nformula_9\n\nApplying Bernoulli's principle from the draft tube entrance \"e\" to the lower free surface 0, under the assumption that the kinetic energy at 0 is negligible, that the fluid is inviscid, and that the fluid density is constant:\n\nformula_10\n\nUsing the above application of Bernoulli to eliminate the velocity term and local pressure terms in the definition of NPSH:\n\nformula_11\n\nNote that, in turbines minor losses (formula_12) alleviate the effect of cavitation - opposite to what happens in pumps.\n\nVapour pressure is strongly dependent on temperature, and thus so will both NPSH and NPSH. Centrifugal pumps are particularly vulnerable especially when pumping heated solution near the vapor pressure, whereas positive displacement pumps are less affected by cavitation, as they are better able to pump two-phase flow (the mixture of gas and liquid), however, the resultant flow rate of the pump will be diminished because of the gas volumetrically displacing a disproportion of liquid. Careful design is required to pump high temperature liquids with a centrifugal pump when the liquid is near its boiling point.\n\nThe violent collapse of the cavitation bubble creates a shock wave that can carve material from internal pump components (usually the leading edge of the impeller) and creates noise often described as \"pumping gravel\". Additionally, the inevitable increase in vibration can cause other mechanical faults in the pump and associated equipment.\n\nThe NPSH appears in a number of other cavitation-relevant parameters. The suction head coefficient is a dimensionless measure of NPSH:\n\nformula_13\n\nWhere formula_14 is the angular velocity (in rad/s) of the turbo-machine shaft, and formula_15 is the turbo-machine impeller diameter. Thoma's cavitation number is defined as:\n\nformula_16\n\nWhere formula_17 is the head across the turbo-machine.\n\n(based on sea level).\n\nExample 1: A tank with a liquid level 2 metres above the pump intake, plus the atmospheric pressure of 10 metres, minus a 2 metre friction loss into the pump (say for pipe & valve loss), minus the NPSH curve (say 2.5 metres) of the pre-designed pump (see the manufacturers curve) = an NPSH (available) of 7.5 metres. (not forgetting the flow duty). This equates to 3 times the NPSH required. This pump will operate well so long as all other parameters are correct.\n\nRemember that positive or negative flow duty will change the reading on the pump manufacture NPSH curve. The lower the flow, the lower the NPSH, and vice versa.\n\nLifting out of a well will also create negative NPSH; however remember that atmospheric pressure at sea level is 10 metres! This helps us, as it gives us a bonus boost or “push” into the pump intake. (Remember that you only have 10 metres of atmospheric pressure as a bonus and nothing more!).\n\nExample 2: A well or bore with an operating level of 5 metres below the intake, minus a 2 metre friction loss into pump (pipe loss), minus the NPSH curve (say 2.4 metres) of the pre-designed pump = an NPSH (available) of (negative) -9.4 metres. NOW we add the atmospheric pressure of 10 metres. We have a positive NPSH of 0.6 metres. (minimum requirement is 0.6 metres above NPSH), so the pump should lift from the well.\n\nNow we will try the situation from example 2 above, but will pump 70 degrees Celsius (158F) water from a hot spring, creating negative NPSH.\n\nExample 3: A well or bore running at 70 degrees Celsius (158F) with an operating level of 5 metres below the intake, minus a 2 metre friction loss into pump (pipe loss), minus the NPSH curve (say 2.4 metres) of the pre-designed pump, minus a temperature loss of 3 metres/10 feet = an NPSH (available) of (negative) -12.4 metres. NOW we add the atmospheric pressure of 10 metres and we have a negative NPSH of -2.4 metres remaining.\n\nRemembering that the minimum requirement is 600 mm above the NPSH therefore this pump will not be able to pump the 70 degree Celsius liquid and will cavitate and lose performance and cause damage. To work efficiently, the pump must be buried in the ground at a depth of 2.4 metres plus the required 600 mm minimum, totalling a total depth of 3 metres into the pit. (3.5 metres to be completely safe).\n\nA minimum of 600 mm (0.06 bar) and a recommended 1.5 metre (0.15 bar) head pressure “higher” than the NPSH pressure value required by the manufacturer is required to allow the pump to operate properly.\n\nSerious damage may occur if a large pump has been sited incorrectly with an incorrect NPSH value and this may result in a very expensive pump or installation repair.\n\nNPSH problems may be able to be solved by changing the NPSH or by re-siting the pump.\n\nIf an NPSH is say 10 bar then the pump you are using will deliver exactly 10 bar more over the entire operational curve of a pump than its listed operational curve.\n\nExample: A pump with a max. pressure head of 8 bar (80 metres) will actually run at 18 bar if the NPSH is 10 bar.\n\ni.e.: 8 bar (pump curve) plus 10 bar NPSH = 18 bar.\n\nThis phenomenon is what manufacturers use when they design multistage pumps, (Pumps with more than one impeller). Each multi stacked impeller boosts the succeeding impeller to raise the pressure head. Some pumps can have up to 150 stages or more, in order to boost heads up to hundreds of metres.\n"}
{"id": "10342676", "url": "https://en.wikipedia.org/wiki?curid=10342676", "title": "Oil fields operated by BP", "text": "Oil fields operated by BP\n\nThis is a list of oil and gas fields operated by BP.\n\nAll fields in British territory are operated as part of the BP's North Sea Strategic Performance Unit from their office in Dyce, Aberdeen. This includes some fields not strictly in the North Sea itself. Fields in the Norwegian sector are operated from Stavanger.\nRecently Transferred to Perenco Ownership\n\n\n\n\n\n\nAll fields of the Trinidad and Tobago business unit are operated from the BPTT office in Port of Spain.\n\nThe Gulf of Mexico business unit is operated from Houston, Texas.\n\n\nThe BP office for the Alaska business unit is located in Anchorage.\n\n\nThe Egypt business unit is operated from Cairo.\n\n\nVietnam operations are run from Sunbury-on-Thames, England.\n\nOperations in Angola are run from the BP office in Sunbury-on-Thames, England.\n\nOperations in Australasia are managed from Jakarta, Indonesia and Perth, Australia.\n"}
{"id": "14659630", "url": "https://en.wikipedia.org/wiki?curid=14659630", "title": "Oil megaprojects (2011)", "text": "Oil megaprojects (2011)\n\nFollowing is a list of Oil megaprojects in the year 2011, projects that propose to bring more than of new liquid fuel capacity to market with the first production of fuel. This is part of the Wikipedia summary of Oil Megaprojects.\n\nTerminology\n"}
{"id": "419495", "url": "https://en.wikipedia.org/wiki?curid=419495", "title": "Oka (mass)", "text": "Oka (mass)\n\nThe oka, okka, or oke (Ottoman Turkish اوقه) was an Ottoman measure of mass, equal to 400 dirhems (Ottoman drams). Its value varied, but it was standardized in the late empire as 1.2829 kilograms. 'Oka' is the most usual spelling today; 'oke' was the usual contemporary English spelling; 'okka' is the modern Turkish spelling, and is usually used in academic work about the Ottoman Empire.\n\nIn Turkey, the traditional unit is now called the \"eski okka\" 'old oka' or \"kara okka\" 'black okka'; the \"yeni okka\" 'new okka' is the kilogram.\n\nIn Greece, the oka (οκά, plural οκάδες) was standardized at 1.282 kg and remained in use until traditional units were abolished on March 31, 1953—the metric system had been adopted in 1876, but the older units remained in use. In Cyprus, the oka remained in use until the 1980s.\n\nIn Egypt, the monetary oka weighted 1.23536 kg. In Tripolitania, it weighed 1.2208 kg, equal to 2½ artals.\n\nThe oka was also used as a unit of volume. In Wallachia, it was 1.283 liters of liquid and 1.537 l of grain (dry measure). In Greece, an oka of oil was 1.280 kg.\n\n"}
{"id": "2356642", "url": "https://en.wikipedia.org/wiki?curid=2356642", "title": "Omer (submarine)", "text": "Omer (submarine)\n\nOmer is the name of a series of human-powered submarines. The submarines were built by students of the \"École de technologie supérieure\" (School of Higher Technology) in Montreal, Quebec, for the International Submarine Races. , \"Omer\" teams hold the human-powered submarine world speed records for two-seater and non-propeller categories. Those records are :\n\n\n\"Omer 11\" is the latest generation of \"Omer\" submarine.\n\nSince 1992, a group of students from the É.T.S. in electrical, mechanical, software and automated automation engineering have taken part in the completion of \"Omer\", a human-powered submarine. The first generation of \"Omer\" was a two-seater submarine that made its mark around the world. Despite the students' efforts and with numerous prizes and awards in conception and innovation, \"Omer\" was not the world's fastest submarine. Otherwise, at the first presence of the \"Omer\" team, a variable pitch system was operational and gave good performances and helped the next \"Omer\" submarines to increase their speed.\n\nIn September 1995, the team decided that \"Omer\" would become the world's champion. With new members' ideas and older members' expertise, \"Omer 2\" was born. In four months, a new hull was made and a new onboard computer was added to control the variable pitch from the propeller, and to give some information to the pilot as well. \n\n\"Omer\" went to California for a competition in May 1996, where it won a world record in the two-seater category. With that victory in hand, the team decided to aim for first place in the one-seater category. Construction on \"Omer 3\" began.\n\nThe team found success in Washington in June 1997, winning world record in all categories. In the summer of 2001, \"Omer 4\" was set to beat the previous world record of 7.192 knots. Since then, a new electrical control system has been tested and integrated for the 2002 competition in California.\n\nSince their creation, \"Omer\", \"Omer 2\" and \"Omer 3\" had a lot of visibility in the media, and gained offers of help from individuals and corporations.\n\nIn the summer of 2003, \"Omer 5\", the new two-seater submarine took first place again using the experience acquired from \"Omer 4\". The new submarine is now much more stable than \"Omer 4\". In 2005, it beat the world speed record for a two-seater submarine setting the mark at 7.061 knots.\n\nIn the summer of 2007, the submarine \"Omer 5\" set the new world record reaching a speed of 8.035 knots. Omer 6 a submarine using non-propeller reach 4.642 knots, with its driver Nicolas Tardif, setting also a world record for non-propeller submarine.\n\nCarderock, Maryland, June 28, 2007 – Speed results by division in the 9th International Submarine Races held June 25–29 at the Naval Surface Warfare Center's Carderock Division David Taylor Model Basin in Bethesda, Maryland:\n\nTwo Person Propeller:\nOne Person Propeller:\n\nOne-Person, Non-Propeller:\n\n"}
{"id": "27729206", "url": "https://en.wikipedia.org/wiki?curid=27729206", "title": "Pensacola Pass", "text": "Pensacola Pass\n\nPensacola Pass, separating mainland Florida from Santa Rosa Island, is the mouth of Pensacola Bay.\nPensacola Pass forms a water passage that connects Pensacola Bay with the Gulf of Mexico to the south, in the U.S. state of Florida, east of the Alabama/Florida state line. The surrounding area is heavily developed, with high-rise condominiums. However, there are nearby beach-front parks, with Fort Pickens on the eastern side of Pensacola Pass.\n\nPensacola Pass is the name of the waterway connecting the Gulf to Pensacola Bay. Ships and boats use this passage to travel between the two. During the daily flood tide, fresh saltwater enters Pensacola Pass from the Gulf of Mexico; waters are pulled out on the ebb tide, flushing the bay.\n\nFollowing the Deepwater Horizon oil spill (called the \"Gulf oil spill\"), the entrance to Pensacola Pass was closed,\nwith a floating barrier system in June 2010, to control tidal flow of oil entering from the Gulf of Mexico. The daily high tide was causing oil-contaminated water to enter Pensacola Bay. The barrier system was to be designed to allow boats to travel through Pensacola Pass during the ebb tide, but to close during the rising tide.\n\nAlthough this was the initial plan, the booming plan was never fully implemented. Due to strong currents in the Pass, the boom broke. There was no alternative system in place in areas of less current, nor was there a plan to trap incoming oil, therefore oil product freely entered the Pass.\n"}
{"id": "10788921", "url": "https://en.wikipedia.org/wiki?curid=10788921", "title": "Pocking Solar Park", "text": "Pocking Solar Park\n\nThe Pocking Solar Park is a photovoltaic power station in Pocking, Lower Bavaria, Germany. It has installed capacity of 10 megawatts. Construction and assembly of the power plant begun in August 2005 and was completed in March 2006.\n\nThe power plant is located on on the former military training area. It has 57,912 modules.\n\n"}
{"id": "51069553", "url": "https://en.wikipedia.org/wiki?curid=51069553", "title": "Polypropylene raffia", "text": "Polypropylene raffia\n\nPolypropylene raffia, or PP raffia is a packaging material made from weaving ribbons of polypropylene. It is named after the raffia palm, which the packaging emulates to some extent. Polypropylene raffia is considered to be a \"widely used material for atmospheric capture\".\n"}
{"id": "23947961", "url": "https://en.wikipedia.org/wiki?curid=23947961", "title": "Professional Electrical Apparatus Recyclers League", "text": "Professional Electrical Apparatus Recyclers League\n\nThe Professional Electrical Apparatus Reconditioning League or PEARL is an international professional organization and standards group based in Denver, Colorado. PEARL is focused on developing ethical business practices and technical standards related to inspecting, testing, and reconditioning circuit breakers, transformer, motor controls, switchgear, disconnect switches, protective relays, bus duct, motor starters and other electrical equipment and apparatus used in the electrical distribution systems of commercial, industrial, and utility facilities.\n\nPEARL's standards for inspecting, testing and reconditioning electrical equipment, components and apparatus help ensure the reliable, safe operation of devices such as circuit breakers, transformers, switches, protective relays, and contactors. PEARL also disseminates information on electrical safety news and counterfeit notices relating to electrical equipment utilized at commercial, industrial and utility facilities.\n\nPEARL is an \"American National Standards Institute (ANSI) Developer of Reconditioning Standards\" for returning electrical equipment to safe and reliable service. PEARL's goal is to develop a single set of consensus technical standards for reconditioning electrical equipment used in industrial and commercial installations and accepted by a pool of industry professionals and stakeholders.\n\nTo this end, PEARL is seeking qualified individuals to review and provide input and comment on new standards and any revision to a standard. These individuals will have expertise and knowledge of inspecting, testing and reconditioning electrical equipment used in industrial and commercial facilities.\n\nCounterfeit electrical apparatus pose a growing threat to all sectors of the electrical marketplace, from the OEMs who lose revenue and brand prestige, to distributors and suppliers that risk liability, to electrical contractors end users who can face financial or physical liabilities as a result of potentially dangerous counterfeit electrical devices. Independent suppliers of electrical product are particularly susceptible to fraudulent counterfeit goods because most OEMs will not sell their 'new' product at wholesale prices directly to non-licensed distributors, forcing independent electrical supply houses to alternate sources.\n\nIn September 2007, PEARL held a special board meeting to discuss counterfeit electrical power equipment. Among other actions, PEARL's Standards and Practices Committee issued a policy directive to all members to pro-actively assist OEMs and other organizations with identifying, reporting, and policing counterfeit electrical product and the companies and individuals that sell it. Since 2007, PEARL members have helped OEMs and other industry associations locate several shipments of counterfeit product.\n\nPEARL sponsors an annual \"Electrical Safety, Reliability and Sustainability Conference & Exhibition.\nConference topics include but are not limited to: \n\nIn 2010 PEARL published a white paper \"Reconditioning: The Ultimate Form of Recycling\" outling how reuse, reconditioning, and remanufacturing use a fraction of the energy of new production, keep millions of tons of waste from landfills every year, reduce raw material consumption, and create 3 to 5 times more skilled jobs than automated production lines.\n\nBecause the remanufacturing process only consumes about 15% of the energy used to create a new product, remanufacturing in the U.S. saves 400 trillion BTUs annually, the equivalent of 16 million barrels of crude oil, or enough gasoline to run 6 million cars for a year. Based on a weighted average of 140 pounds of gas pollution for every 1 million BTUs of energy consumed, remanufacturing reduces generation by 28 million tons each year, which is equal to the output of 10, 500-megawatt coal-burning electrical plants. Remanufacturing also saves the U.S. enough raw materials to fill 155,000 railway cars each year.\n\nBoston University's Prof. Robert Lund estimated the U.S. remanufacturing industry at $53 billion in sales in 1996, employing approximately 480,000 people. This figure only includes a portion of the electrical equipment market related to electrical motors, and doesn't include remanufacturing of other electrical products such as circuit breakers, transformers, etc. Remanufacturing electrical equipment keeps thousands of tons of waste from U.S. landfills every year, based on the inventory turnarounds shown just by PEARL member companies. Also, unlike recycling which only reclaims part of the materials within a waste stream, reconditioning reclaims more material savings, as well as most of the energy and labor energy used to manufacturer the original product, making reconditioning a more environmentally sustainable practice than recycling. In 2003, the OEM Product-Services Institute (OPI) said U.S. electrical generation facilities alone spent $3.1 billion on remanufacturing, overhaul, and rebuilding.\n\nBetween 1980 and 1992, the National Institute for Occupational Safety and Health estimated that on average 411 workers died in the U.S. each year from electrocution. Safety of remanufactured electrical equipment is a prime focus of PEARL.\n\nAlthough numbers on energy savings and pollution reduction thanks solely to electrical reconditioning do not exist, PEARL has recently been recognized by the California Integrated Waste Management Board (CIWMB) through its Waste Reduction Award Program (WRAP). as an organization that has help the state meet its waste reduction goals.\n\nPEARL's original corporate members first came together in Denver, CO, in 1996 to discuss emerging issues surrounding new electronic data interchange (EDI) systems for the ordering and purchase of electric apparatus and equipment for commercial and industrial markets. As a group, these independent suppliers of new, surplus, and used electrical apparatus and equipment for commercial and industrial electrical applications typically were neither members of horizontal electrical industry associations and standards organizations, such as the National Electrical Manufacturers Association (NEMA), who develops general electrical enclosure and interconnect standards for electrical original equipment manufacturers (OEM); nor vertical trade associations such as the Electrical Apparatus Service Association Inc. (EASA), which develops standards for servicing electrical motors, nor the International Electrical Testing Association (NETA), which develops standards for electrical field testing and field equipment maintenance. EASA and NETA would go on to become standards development groups for the American National Standards Institute (ANSI).\n\nAlthough OEMs of electrical equipment did develop maintenance and repair documents for their individual company products and offer for-fee remanufacturing services, neither OEM's nor their trade associations collected these repair documents, standardized the processes from different companies, validated the processes through third party engineering review, or offered them as a group of standards to the electrical industry at large. EASA had developed standards for rewinding electric motors, excluding motor control circuits, NETA had developed the standards to calibrate electrical test equipment used in both EASA and eventually PEARL standards, but in 1996, PEARL's founding members saw that the industry did not have the technical standards necessary to ensure the safety of reconditioned electrical equipment used by industrial and commercial industries, ranging from circuit breakers and transformer to conduit and bus duct.\n\nAs a result of these conditions within the electrical industry, In 1997, 20 charter members formed PEARL to collect, create, and disseminate information, policies, procedures, and standards to ensure the proper recycling and reuse of electrical power equipment, as well as to prevent fraudulent electrical apparatus labeling and misrepresentation of electrical equipment. As of May 25, 2009, PEARL's 51 corporate voting members and 30 affiliate members, representing more than $500 million in annual sales revenues from companies in the U.S. and Canada, have contributed to the development of 137 electrical Reconditioning Standards for Electrical Equipment ranging from circuit breakers and transformers to conduit and bus duct.\n"}
{"id": "214764", "url": "https://en.wikipedia.org/wiki?curid=214764", "title": "Stagecoach", "text": "Stagecoach\n\nA stagecoach is a four-wheeled public coach used to carry paying passengers and light packages on journeys long enough to need a change of horses. It is strongly sprung and generally drawn by four horses.\n\nWidely used before steam-powered rail transport was available a stagecoach made long scheduled trips using \"stage stations\" or posts where the stagecoach's horses would be replaced by fresh horses. The business of running stagecoaches or the act of journeying in them was known as staging.\n\nOriginating in England, familiar images of the stagecoach are that of a Royal Mail coach passing through a turnpike gate, a Dickensian passenger coach covered in snow pulling up at a coaching inn, and a highwayman demanding a coach to \"stand and deliver\". The yard of ale drinking glass is associated by legend with stagecoach drivers, though it was mainly used for drinking feats and special toasts.\n\nThe stagecoach was a closed four-wheeled vehicle drawn by horses or hard-going mules. It was used as a public conveyance on an established route usually to a regular schedule. Spent horses were replaced with fresh horses at stage stations, posts, or relays. \n\nA simplified and lightened form of stagecoach, known as a stage wagon, mud-coach, or mud-wagon, was used in the United States under difficult conditions. These were the vehicles that opened up the new stage routes in America's West. In addition to the stage driver or coachman who guided the vehicle, a shotgun messenger armed with a coach gun might travel as a guard beside him.\n\nA stagecoach traveled at an average speed of about , with the average daily mileage covered being around .\n\n'Stage' originally referred to the distance between stage stations on a route but through metonymy it came to be applied to the stagecoach.\n\nThe first crude depiction of a coach was in an English manuscript from the 13th century. The first stagecoach route started in 1610 and ran from Edinburgh to Leith. This was followed by a steady proliferation of other routes around the island. By the mid 17th century, a basic stagecoach infrastructure had been put in place. A string of coaching inns operated as stopping points for travellers on the route between London and Liverpool. The stagecoach would depart every Monday and Thursday and took roughly ten days to make the journey during the summer months. Stagecoaches also became widely adopted for travel in and around London by mid-century and generally travelled at a few miles per hour. Shakespeare's first plays were performed at coaching inns such as The George Inn, Southwark.\n\nBy the end of the 17th century stagecoach routes ran up and down the three main roads in England. The London-York route was advertised in 1698:\n\nThe novelty of this method of transport excited much controversy at the time. One pamphleteer denounced the stagecoach as a \"great evil [...] mischievous to trade and destructive to the public health.\" Another writer, however, argued that:\n\nThe speed of travel remained constant until the mid-18th century. Reforms of the turnpike trusts, new methods of road building and the improved construction of coaches led to a sustained rise in the comfort and speed of the average journey - from an average journey length of 2 days for the Cambridge-London route in 1750 to a length of under 7 hours in 1820.\n\nRobert Hooke helped in the construction of some of the first spring-suspended coaches in the 1660s and spoked wheels with iron rim brakes were introduced, improving the characteristics of the coach.\n\nIn 1754, a Manchester-based company began a new service called the \"Flying Coach\". It was advertised with the following announcement - \"However incredible it may appear, this coach will actually (barring accidents) arrive in London in four days and a half after leaving Manchester.\" A similar service was begun from Liverpool three years later, using coaches with steel spring suspension. This coach took an unprecedented three days to reach London with an average speed of eight miles per hour.\n\nEven more dramatic improvements were made by John Palmer at the British Post Office. The postal delivery service in Britain had existed in the same form for about 150 years—from its introduction in 1635, mounted carriers had ridden between \"posts\" where the postmaster would remove the letters for the local area before handing the remaining letters and any additions to the next rider. The riders were frequent targets for robbers, and the system was inefficient.\n\nPalmer made much use of the \"flying\" stagecoach services between cities in the course of his business, and noted that it seemed far more efficient than the system of mail delivery then in operation. His travel from Bath to London took a single day to the mail's three days. It occurred to him that this stagecoach service could be developed into a national mail delivery service, so in 1782 he suggested to the Post Office in London that they take up the idea. He met resistance from officials who believed that the existing system could not be improved, but eventually the Chancellor of the Exchequer, William Pitt, allowed him to carry out an experimental run between Bristol and London. Under the old system the journey had taken up to 38 hours. The stagecoach, funded by Palmer, left Bristol at 4 pm on 2 August 1784 and arrived in London just 16 hours later.\n\nImpressed by the trial run, Pitt authorised the creation of new routes. Within the month the service had been extended from London to Norwich, Nottingham, Liverpool and Manchester, and by the end of 1785 services to the following major towns and cities of England and Wales had also been linked: Leeds, Dover, Portsmouth, Poole, Exeter, Gloucester, Worcester, Holyhead and Carlisle. A service to Edinburgh was added the next year, and Palmer was rewarded by being made Surveyor and Comptroller General of the Post Office. By 1797 there were forty-two routes.\n\nThe period from 1800 to 1830 saw great improvements in the design of coaches, notably by John Besant in 1792 and 1795. His coach had a greatly improved turning capacity and braking system, and a novel feature that prevented the wheels from falling off while the coach was in motion. Besant, with his partner John Vidler, enjoyed a monopoly on the supply of stagecoaches to the Royal Mail and a virtual monopoly on their upkeep and servicing for the following few decades.\n\nSteel springs had been used in suspensions for vehicles since 1695. Coachbuilder Obadiah Elliott obtained a patent covering the use of elliptic springs - which were not his invention. His patent lasted 14 years delaying development because Elliott allowed no others to license and use his patent. Elliott mounted each wheel with two durable elliptic steel leaf springs on each side and the body of the carriage was fixed directly to the springs attached to the axles. After the expiry of his patent most British horse carriages were equipped with elliptic springs; wooden springs in the case of light one-horse vehicles to avoid taxation, and steel springs in larger vehicles.\n\nSteady improvements in road construction were also made at this time, most importantly the widespread implementation of Macadam roads up and down the country. The speed of coaches in this period rose from around 6 miles per hour (including stops for provisioning) to 8 miles per hour and greatly increased the level of mobility in the country, both for people and for mail. Each route had an average of four coaches operating on it at one time - two for both directions and a further two spares in case of a breakdown en route. Joseph Ballard described the stagecoach industry in 1815:\n\nThe development of railways in the 1830s spelled the end for stagecoaches and mail coaches. The first rail delivery between Liverpool and Manchester took place on 11 November 1830. By the early 1840s most London-based coaches had been withdrawn from service.\n\nSome stagecoaches remained in use for commercial or recreational purposes. They came to be known as road coaches and were used by their enterprising (or nostalgic) owners to provide scheduled passenger services where rail had not yet reached and also on certain routes at certain times of the year for the pleasure of an (often amateur) coachman and his daring passengers.\n\nWhile stagecoaches vanished as rail penetrated the countryside the 1860s did see the start of a coaching revival spurred on by the popularity of Four-in-hand driving as a sporting pursuit (the Four-In-Hand Driving Club was founded in 1856 and the Coaching Club in 1871).\n\nNew coaches often known as Park Drags began to be built to order. Some owners would parade their vehicles and magnificently dressed passengers in fashionable locations. Other owners would take more enthusiastic suitably-dressed passengers and indulge in competitive driving. Very similar in design to stagecoaches their vehicles were lighter and sportier.\n\nThese owners were (often very expert) amateur gentlemen-coachmen, occasionally gentlewomen. A professional coachman might accompany them to avert disaster. Professionals called these vehicles 'butterflies'. They only appeared in summer.\n\nThe \"diligence\" (\"dilly\" for short), a solidly built coach with four or more horses, was the French analogue for public conveyance, especially in France, with minor varieties in Germany such as the \"Stellwagen\" and \"Eilwagen\". The \"diligence\" from Le Havre to Paris was described by a fastidious English visitor of 1803 with a thoroughness that distinguished it from its English contemporary, the \"stage coach\".\nA more uncouth clumsy machine can scarcely be imagined. In the front is a cabriolet fixed to the body of the coach, for the accommodation of three passengers, who are protected from the rain above, by the projecting roof of the coach, and in front by two heavy curtains of leather, well oiled, and smelling somewhat offensively, fastened to the roof. The inside, which is capacious, and lofty, and will hold six people in great comfort is lined with leather padded, and surrounded with little pockets, in which travellers deposit their bread, snuff, night caps, and pocket handkerchiefs, which generally enjoy each others company, in the same delicate depository. From the roof depends a large net work which is generally crouded with hats, swords, and band boxes, the whole is convenient, and when all parties are seated and arranged, the accommodations are by no means unpleasant.<br><br>\n\nUpon the roof, on the outside, is the imperial, which is generally filled with six or seven persons more, and a heap of luggage, which latter also occupies the basket, and generally presents a pile, half as high again as the coach, which is secured by ropes and chains, tightened by a large iron windlass, which also constitutes another appendage of this moving mass. The body of the carriage rests upon large thongs of leather, fastened to heavy blocks of wood, instead of springs, and the whole is drawn by seven horses. \n\nThe English visitor noted the small, sturdy Norman horses \"running away with our cumbrous machine, at the rate of six or seven miles an hour.\" At this speed stagecoaches could compete with canal boats, but they were rendered obsolete in Europe wherever the rail network expanded in the 19th century. Where the rail network did not reach, the \"diligence\" was not fully superseded until the arrival of the autobus.\n\nIn France, between 1765 and 1780, the \"turgotines\", big mail coaches named for their originator, Louis XVI's economist minister Turgot, and improved roads, where a coach could travel at full gallop across levels, combined with more staging posts at shorter intervals, cut the time required to travel across the country sometimes by half.\n\nBeginning in the 18th century crude wagons began to be used to carry passengers between cities and towns, first within New England by 1744, then between New York and Philadelphia by 1756. Travel time was reduced on this later run from three days to two in 1766 with an improved coach called the \"Flying Machine\". The first mail coaches appeared in the later 18th century carrying passengers and the mails, replacing the earlier post riders on the main roads. Coachmen carried letters, packages, and money, often transacting business or delivering messages for their customers. By 1829 Boston was the hub of 77 stagecoach lines; by 1832 there were 106. Coaches with iron or steel springs were uncomfortable and had short useful lives. Two men in Concord, New Hampshire developed what became a popular solution. They built their first Concord stagecoach in 1827 employing long leather straps under their stagecoaches which gave a swinging motion. \n\nIn his 1861 book \"Roughing It\", Mark Twain described the Concord stage's ride as like \"a cradle on wheels\". Around twenty years later in 1880 John Pleasant Gray recorded after travelling from Tucson to Tombstone on J.D. Kinnear's mail and express line: \"“That day’s stage ride will always live in my memory — but not for its beauty spots. Jammed like sardines on the hard seats of an old time leather spring coach — a Concord — leaving Pantano, creeping much of the way, letting the horses walk, through miles of alkali dust that the wheels rolled up in thick clouds of which we received the full benefit . . . It is always a mystery to the passenger how many can be wedged into and on top of a stagecoach. If it had not been for the long stretches when the horses had to walk, enabling most of us to get out and ‘foot it’ as a relaxation, it seems as if we could never have survived the trip.”\" The horses were changed three times on the trip normally completed in 17 hours.\n\nStories that prominently involve a stagecoach include:\n\n\n\n"}
{"id": "19986090", "url": "https://en.wikipedia.org/wiki?curid=19986090", "title": "Staged reforming", "text": "Staged reforming\n\nStaged reforming is a thermochemical process to convert organic material or bio waste such as wood, dung or hay into combustible gases containing methane, carbon monoxide and hydrogen. The single-stage reforming of bio materials results in high dust and tar yields in the produced gas restricting its use, hence the use of staged reforming. After reforming the output is approximately 80% fuel gas and 20% cokes.\n\nIn staged reforming technology, gas conversion is a separate stage after pyrolysis.\n\nOrganic material is decomposed into gas and coal at approximately 600°C.\n\nGas produced by the first stage is reformed with water vapor and heat energy from the cokes into a dust and residue-free fuel gas. \n\nSecond stage process steps:\n\n\n\n"}
{"id": "11524334", "url": "https://en.wikipedia.org/wiki?curid=11524334", "title": "Standpipe (street)", "text": "Standpipe (street)\n\nA standpipe is a freestanding pipe fitted with a tap which is installed outdoors to dispense water in areas which do not have a running water supply to the buildings.\n\nIn the United Kingdom, an \"Emergency Drought Order\" permits a water company to shut off the primary water supply to homes, and to supply water instead from tanks or standpipes in the streets. This was done in some areas during the 1976 heat wave, for example.\n\nIn some Middle Eastern, Caribbean and North African countries a standpipe is used as a communal water supply for neighbourhoods which lack individual housing water service. In areas such as Morocco, standpipes often yield unreliable service and lead to water scarcity for large numbers of people. This outcome is not based upon the unreliability of the hardware, but on the underlying condition of the population exceeding the carrying capacity of the region with respect to water resources.\n"}
{"id": "15189466", "url": "https://en.wikipedia.org/wiki?curid=15189466", "title": "Thornwood, South Elgin", "text": "Thornwood, South Elgin\n\nThornwood is a master planned community of single-family homes and townhomes located in South Elgin, Illinois at the intersection of Silver Glen and Randall Roads on over of land and centered on a $3.8 million clubhouse that is situated on of land. Construction started in 1998 and finished in 2005, three years ahead of schedule.\n"}
{"id": "264748", "url": "https://en.wikipedia.org/wiki?curid=264748", "title": "Unsaturated fat", "text": "Unsaturated fat\n\nAn unsaturated fat is a fat or fatty acid in which there is at least one double bond within the fatty acid chain. A fatty acid chain is monounsaturated if it contains one double bond, and polyunsaturated if it contains more than one double bond.\n\nWhere double bonds are formed, hydrogen atoms are subtracted from the carbon chain. Thus, a saturated fat has no double bonds, has the maximum number of hydrogens bonded to the carbons, and therefore is \"saturated\" with hydrogen atoms. In cellular metabolism, unsaturated fat molecules contain somewhat less energy (i.e., fewer calories) than an equivalent amount of saturated fat. The greater the degree of unsaturation in a fatty acid (i.e., the more double bonds in the fatty acid) the more vulnerable it is to lipid peroxidation (rancidity). Antioxidants can protect unsaturated fat from lipid peroxidation.\n\nDouble bonds may be in either a \"cis\" or a \"trans\" isomer, depending on the geometry of the double bond. In the \"cis\" isomer, hydrogen atoms are on the same side of the double bond; whereas in the \"trans\" isomer, they are on opposite sides of the double bond (see trans fat). Saturated fats are useful in processed foods because saturated fats are less vulnerable to rancidity and usually more solid at room temperature than unsaturated fats. Unsaturated chains have a lower melting point, hence these molecules increase the fluidity of cell membranes.\n\nAlthough both monounsaturated and polyunsaturated fats can replace saturated fat in the diet, trans unsaturated fats should not. Replacing saturated fats with unsaturated fats helps lower levels of total cholesterol and LDL cholesterol in the blood. Trans unsaturated fats are an exception because the double bond stereochemistry predisposes the carbon chains to assume a linear conformation, which conforms to rigid packing as in plaque formation. The geometry of the cis double bond induces a bend in the molecule, thereby precluding rigid formations (see links above for drawings that illustrate this). Natural sources of fatty acids (see above) are rich in the cis isomer.\n\nAlthough polyunsaturated fats are protective against cardiac arrhythmias, a study of post-menopauseal women with a relatively low fat intake showed that polyunsaturated fat is positively associated with progression of coronary atherosclerosis, whereas monounsaturated fat is not. This probably is an indication of the greater vulnerability of polyunsaturated fats to lipid peroxidation, against which vitamin E has been shown to be protective.\n\nExamples of unsaturated fatty acids are palmitoleic acid, oleic acid, myristoleic acid, linoleic acid, and arachidonic acid. Foods containing unsaturated fats include avocado, nuts, olive oils, and vegetable oils such as canola. Meat products contain both saturated and unsaturated fats.\n\nAlthough unsaturated fats are conventionally regarded as 'healthier' than saturated fats, the United States Food and Drug Administration (FDA) recommendation stated that the amount of unsaturated fat consumed should not exceed 30% of one's daily caloric intake. Most foods contain both unsaturated and saturated fats. Marketers advertise only one or the other, depending on which one makes up the majority. Thus, various unsaturated fat vegetable oils, such as olive oils, also contain saturated fat.\n\nIn chemical analysis, fatty acids are separated by gas chromatography of methyl esters; additionally, a separation of unsaturated isomers is possible by argentation thin-layer chromatography.\n\nIncidence of insulin resistance is lowered with diets higher in monounsaturated fats (especially oleic acid), while the opposite is true for diets high in polyunsaturated fats (especially large amounts of arachidonic acid) as well as saturated fats (such as arachidic acid). These ratios can be indexed in the phospholipids of human skeletal muscle and in other tissues as well. This relationship between dietary fats and insulin resistance is presumed secondary to the relationship between insulin resistance and inflammation, which is partially modulated by dietary fat ratios (Omega-3/6/9) with both omega 3 and 9 thought to be anti-inflammatory, and omega 6 pro-inflammatory (as well as by numerous other dietary components, particularly polyphenols and exercise, with both of these anti-inflammatory). Although both pro- and anti-inflammatory types of fat are biologically necessary, fat dietary ratios in most US diets are skewed towards Omega 6, with subsequent disinhibition of inflammation and potentiation of insulin resistance. But this is contrary to the suggestion of more recent studies, in which polyunsaturated fats are shown as protective against insulin resistance.\n\nStudies on the cell membranes of mammals and reptiles discovered that mammalian cell membranes are composed of a higher proportion of polyunsaturated fatty acids (DHA, omega-3 fatty acid) than reptiles. Studies on bird fatty acid composition have noted similar proportions to mammals but with 1/3rd less omega-3 fatty acids as compared to omega-6 for a given body size. This fatty acid composition results in a more fluid cell membrane but also one that is permeable to various ions (H+ & Na+), resulting in cell membranes that are more costly to maintain. This maintenance cost has been argued to be one of the key causes for the high metabolic rates and concomitant warm-bloodedness of mammals and birds. However polyunsaturation of cell membranes may also occur in response to chronic cold temperatures as well. In fish increasingly cold environments lead to increasingly high cell membrane content of both monounsaturated and polyunsaturated fatty acids, to maintain greater membrane fluidity (and functionality) at the lower temperatures.\n\n"}
{"id": "4746264", "url": "https://en.wikipedia.org/wiki?curid=4746264", "title": "Uranium pentafluoride", "text": "Uranium pentafluoride\n\nUranium pentafluoride is the inorganic compound with the chemical formula UF. It is a pale yellow paramagnetic solid. The compound has attracted interest because it is related to uranium hexafluoride, which is widely used to produce uranium fuel. It crystallizes in two polymorphs, called α- and β-UF.\n\nIt can be produced by reduction of the hexafluoride with carbon monoxide at elevated temperatures. \nOther reducing agents have been examined.\n\nThe α form is a linear coordination polymer consisting of chains of octahedral uranium centers in which one of the five fluoride anion forms a bridge to the next uranium atom. The structure is reminiscent of that for vanadium pentafluoride.\n\nIn β form, the uranium centers adopt a square antiprismatic structure. The β polymorph gradually converts to α at 130 °C.\nOf theoretical interest, molecular UF can be generated as a transient monomer by UV-photolysis of uranium hexafluoride. It is thought to adopt a square pyramidal geometry. \n"}
{"id": "49474034", "url": "https://en.wikipedia.org/wiki?curid=49474034", "title": "Urpek", "text": "Urpek\n\nUrpek is a village in Amangeldi District of Kostanay Region, northern Kazakhstan.\n\nThe Turgay triradial swastika, one of a number of Neolithic earth constructions known to archaeologists as Steppe Geoglyphs, is at , only about half a mile east of Urpek.\n"}
{"id": "13649015", "url": "https://en.wikipedia.org/wiki?curid=13649015", "title": "Wey (unit)", "text": "Wey (unit)\n\nThe wey or weight (Old English: , \"waege\",  \"weight\") was an English unit of weight and dry volume by at least  900, when it begins to be mentioned in surviving legal codes. \n\nA statute of Edgar the Peaceful set a price floor on wool by threatening both the seller and purchaser who agreed to trade a wool wey for less than 120 pence (i.e., ½ pound of sterling silver per wey), but the wey itself varied over time and by location. The wey was standardized as 14 stone of 12½ merchants' pounds each (175 lbs. or around 76.5 kg) by the time of the Assize of Weights and Measures . This wey was applied to lead, soap, and cheese as well as wool. 2 wey made a sack, 12 a load, and 24 a last.\n\nThe wool wey was later figured as 2 hundredweight of 8 stone of 14 avoirdupois pounds each (224 lbs. or about 101.7 kg).\n\nThe Suffolk wey was 356 avoirdupois pounds (around 161.5 kg). It was used as a measure for butter and cheese.\n\nAs a measure of volume for dry commodities, it denoted roughly 40 bushels or 320 gallons.\n\n"}
{"id": "20297061", "url": "https://en.wikipedia.org/wiki?curid=20297061", "title": "Wild by Law", "text": "Wild by Law\n\nWild by Law: The Rise of Environmentalism and the Creation of the Wilderness Act is a 1991 documentary film produced by Lawrence Hott and Diane Garey. It was nominated for an Academy Award for Best Documentary Feature.\n\nThe film is about the work of Aldo Leopold, Bob Marshall, founder of The Wilderness Society and Howard Zahniser. The film gives the philosophical and political underpinnings of the Wilderness Act of 1964. It was narrated by Linda Hunt.\n"}
{"id": "36263961", "url": "https://en.wikipedia.org/wiki?curid=36263961", "title": "Wind power in the Philippines", "text": "Wind power in the Philippines\n\nWind power in the Philippines makes up a small percentage of the total energy output of the Philippines. The country wind energy sector has significant potential and could provide up to 76GW of power. Some of the most recent developments are the Bangui Wind Farm, Burgos Wind Farm, and Caparispisan Wind Farm in Ilocos Norte, the Wind Energy Power System (WEPS) in Puerto Galera, Mindoro Oriental and Pililla Wind Farm Pililla, Rizal.\n\n"}
