{"id": "23179021", "url": "https://en.wikipedia.org/wiki?curid=23179021", "title": "Alum Tulcea Power Station", "text": "Alum Tulcea Power Station\n\nThe Alum Tulcea Power Station will be a large electricity producer in Romania, having 5 natural gas-fired groups of 50 MW each totalling an installed capacity of 250 MW and an electricity generation capacity of around 0.6 TWh/year.\n\nThe power plant will be situated in the Tulcea County (Eastern Romania) near Alum Tulcea's industrial facility in Tulcea.\n"}
{"id": "34378672", "url": "https://en.wikipedia.org/wiki?curid=34378672", "title": "Bare mass", "text": "Bare mass\n\nIn quantum field theory, specifically the theory of renormalization, the bare mass of an elementary particle is the limit of its mass as the scale of distance approaches zero or, equivalently, as the energy of a particle collision approaches infinity. It differs from the invariant mass as usually understood because the latter includes the 'clothing' of the particle by pairs of virtual particles that are temporarily created by the force-fields around the particle. In some versions of QFT, the bare mass of some particles may be plus or minus infinity. In the theory of the electroweak interaction using the Higgs boson, all particles have a bare mass of zero.\n\nThis allows us to write formula_1, where m denotes the experimentally observable mass of the particle, formula_2 its bare mass, and formula_3 the increase in mass owing to the interaction of the particle with the medium or field.\n"}
{"id": "551271", "url": "https://en.wikipedia.org/wiki?curid=551271", "title": "British Thomson-Houston", "text": "British Thomson-Houston\n\nBritish Thomson-Houston (BTH) was a British engineering and heavy industrial company, based at Rugby, Warwickshire, England and founded as a subsidiary of the General Electric Company (GE) of Schenectady, New York, USA. They were known primarily for their electrical systems and steam turbines. BTH was taken into British ownership and amalgamated with the similar Metropolitan-Vickers company in 1928 to form Associated Electrical Industries (AEI), but the two brand identities were maintained until 1960. The holding company, Associated Electrical Industries (AEI), later merged with GEC, the remnants of which exist today as Marconi Corporation.\n\nIn the 1960s BTH apprenticeships were highly thought-of, with apprentices exposed to production of a wide range of industrial products. Each year in Rugby there was a big parade of floats run by its apprentices, many of whom lodged in the nearby Coton House apprentice hostel.\n\nIn 1980, G.E.C. Turbine Generators Ltd, on the Rugby site, was awarded a Queen's Awards for Enterprise.\n\nThe company Laing, Wharton and Down was formed in 1886 to sell products from Thomson-Houston, an American firm known as the American Electric Corporation until 1883. Laing, Wharton and Down soon won a contract for electrical lighting for the east end of London.\n\nIn 1894 Laing, Wharton and Down purchased patents and exclusive production rights from the American company, now known as General Electric after Thomson-Houston merged with Edison General Electric Company in 1892. At this stage Laing, Wharton and Down was renamed as British Thomson-Houston and General Electric became the majority owner of the company. \n\nOnce BTH had the production licences for Thomson-Houston's products it started setting up factories in the English Midlands, with Rugby, Warwickshire chosen as the main location due to its good accessibility by rail and a local coal supply. In 1900 BTH bought Glebe Farm on the west side of Mill Road north of the railway in Rugby for £10,000, from Thos. Hunter & Co., to build their factory on it. The Mill Road factory opened in 1902 and made electric motors and generators. In the same year BTH got a licence to produce the Curtis steam turbine, which became one of the company's major products. In 1905 BTH made its first turbo-alternator and in 1911 got licences for all of General Electric's drawn-wire light bulbs, which it produced under the Mazda trademark.\n\nFor much of the late 19th century BTH competed for electrical generation and distribution contracts with British Westinghouse, mirroring the same company's battles in the US between their parents, General Electric and Westinghouse. The Power Act 1900 let BTH and British Westinghouse get new contracts to supply electric power to large areas.\n\nAs well as manufacturing, BTH also began to move into transport. On 22 December 1898 BTH opened the Cork Electric Tramways and Lighting Company, followed by the Isle of Thanet Electric Tramways on 4 April 1901 and the Chatham and District Light Railways Company in June 1902. In 1907 BTH started a joint venture with Wolseley Motors to make petrol-electric buses and in 1909 the company supplied major coal-fired steam generators to London to power an electric trolley system that was being set up.\n\nDuring World War I BTH expanded into naval electrical equipment, supplying the Royal Navy with various lighting, radio and signalling gear.\n\nAfter the war BTH expanded dramatically, adding or expanding factories at Willesden, Birmingham, Chesterfield, and Lutterworth. It later had factories in Coventry, and in Larne in Northern Ireland. From 1924 to 1927 Demetrius Comino worked as an apprentice for BTH.\n\nIn 1926 Gerard Swope, president of General Electric, proposed that BTH, Westinghouse, General Electric Company (GEC) and English Electric should amalgamate. Lord Hirst of GEC was not interested in Swope’s scheme, but a new holding company was formed, Associated Electrical Industries (AEI), and in 1928 AEI bought BTH and Metropolitan-Vickers (Metrovick). BTH had been in the process of buying Edison Swan (Ediswan) and Ferguson, Pailin & Co, with AEI completing the purchases in 1929. Howard C. Levis, chairman of BTH from 1916, became chairman of AEI in 1928, retiring the following year.\n\nIn 1927 BTH sold the Chatham and District Light Railways Company to Maidstone and District Motor Services Ltd.\n\nThroughout the 1920s BTH made turbo generators and motors for ocean liners including , , and . The BTH factory in Northern Ireland made the turbo generator and propulsion motor for one of the world's first turbo-electric merchant ships, the banana boat SS \"San Benito\", in 1921. This was followed by turbo generators and propulsion motors for the banana boats , and .\n\nThe site at Rugby was also developed. Building 52, the research laboratory, was purpose-built in 1924. In the late 1920s AEI started to build buildings west of the footpath that runs north through the AEI site in Rugby to the Leicester Road (known in the area as the Black Path because it was surfaced with cinders). During World War II BTH expanded north of the River Avon into the Boughton Road site to make magnetos for aircraft engines and other war products.\nBTH had a major role in developing the world's first prototype jet engine, which was built by Frank Whittle's Power Jets company built at the BTH works in Rugby in 1937. Development was later moved to the Lutterworth works, which were falling into disuse at the time. BTH's directors seemed skeptical of the design and offered little help, and in 1940 decided they were not really interested in making jet engines due to their commitment to electrical equipment. Rover was soon selected to make jet engines, but exchanged jet engine production with Rolls-Royce for making tank engines in 1943. In 1944 the Lutterworth Power Jets work was nationalised.\n\nAfter World War II Oliver Lyttelton took over as chairman of AEI, and started a massive expansion. He returned as chairman between 1954 and 1963 and oversaw the opening of a massive new £8 million turbine works was opened at Larne in 1957. In 1955 AEI acquired Siemens Brothers, which was merged with Edison Swan in 1957 to form the Siemens Edison Swan subsidiary. Rivalry with Metrovick intensified, particularly after BTH won the contract to build the new Buenos Aires Central Costanera S.A. power station, valued at £35 million, in 1957. Lyttelton continued to try to reduce this friction, leading to several unsuccessful reorganisations and slipping profits.\n\nThe postwar period saw continued development at BTH. The Hungarian scientist Dennis Gabor invented holography at the BTH site in Rugby in 1947, and in 1951 BTH supplied a gas turbine for the \"Auris\", the first commercial ship to use gas-turbine propulsion. In 1955 BTH supplied 18 New Zealand DSC class locomotive Rolls-Royce powered locomotives for New Zealand Railways. The Ediswan trademark appeared on semiconductors in 1956 and the following year British Rail Class 15 diesel-electric locomotives were designed by BTH.\n\nTo try to cure internal political and efficiency problems, AEI stopped using the BTH and Metrovick names on 1 January 1960. This led to a huge decline in sales because no-one had heard of \"AEI\" before, and in turn, a massive drop in AEI's stock price. Continued attempts to streamline what was two separate management structures continued to fail, and by the mid-60s the entire AEI group was in financial trouble. The AEI name was first used on products in 1961. By 1967 AEI brands included Siemens Edison Swan, Hotpoint, Birlec and W.T. Henley.\n\nBritain's first commercial nuclear power facility was built between 1956 and 1962 at Berkeley. This was followed by the building and commissioning of the 25M Chilbolton (radar) Dish at Chilbolton Observatory between 1963 and 1967. The AEI research lab (building BR57) was built in 1960 at the Boughton Road site. At this point the size of the Rugby site peaked, with all of the company's land west of the Black Path built over.\n\nIn 1967 GEC bought AEI outright and became the UK's largest electrical group. A year later GEC acquired English Electric, prompting a series of mergers and reorganisations. GEC-AEI Electronics (Blackbird Road and New Parks, Leicester) was merged with Marconi's Radar Division (Chelmsford) and Elliott's Aerospace Control Division to form Marconi Radar Systems Ltd. (MRSL) in 1969.\n\nIn 1980 GEC Turbine Generators Ltd received .\n\nDuring the 1980s GEC Rugby shrank and buildings were demolished. The south part of the area to the west of the Black Path became a supermarket site. The Boughton Road site became several separate small firms. In 1989 GEC Rugby split into GEC Alsthom and Cegelec Projects, which were reunited in 1998 as Alstom.\n\nThe firm's clubhouse on Hillmorton Road was demolished in 2007, and the south edge of its surrounding sports field was encroached along for house building. By 2011 the site was greatly changed and included Rugby College. Quartzelec, and Converteam worked on electrical engineering projects in some of the early BTH buildings, notably buildings 4, 193 and 140. A public road was built through the site between its former east and west gates. In 2012 Converteam was bought out by General Electric, therefore coming full circle back to when they were partnered in AEI. Converteam (now GE) produced rotating machines and used former-BTH equipment (machines) for running tests.\n\nDuring post-World War II Britain, AEI established a consolidated research effort at Aldermaston in Berkshire, England. The research centre was based at Aldermaston Court a large stately home owned by AEI that had been requisitioned for military use in the war era.\n\nOne of the BTH-built batch of New Zealand Railways DSC class Bo-Bo shunters has been preserved and is used in industrial service, complete with original Rolls-Royce engines. The locomotive (DSC406) is the primary motive power at Alliance Ltd, Pukeuri, New Zealand. All the others were scrapped between 1986 and 1990.\n\n\n"}
{"id": "3756", "url": "https://en.wikipedia.org/wiki?curid=3756", "title": "Bromine", "text": "Bromine\n\nBromine is a chemical element with symbol Br and atomic number 35. It is the third-lightest halogen, and is a fuming red-brown liquid at room temperature that evaporates readily to form a similarly coloured gas. Its properties are thus intermediate between those of chlorine and iodine. Isolated independently by two chemists, Carl Jacob Löwig (in 1825) and Antoine Jérôme Balard (in 1826), its name was derived from the Ancient Greek βρῶμος (\"stench\"), referencing its sharp and disagreeable smell.\n\nElemental bromine is very reactive and thus does not occur free in nature, but in colourless soluble crystalline mineral halide salts, analogous to table salt. While it is rather rare in the Earth's crust, the high solubility of the bromide ion (Br) has caused its accumulation in the oceans. Commercially the element is easily extracted from brine pools, mostly in the United States, Israel and China. The mass of bromine in the oceans is about one three-hundredth that of chlorine.\n\nAt high temperatures, organobromine compounds readily dissociate to yield free bromine atoms, a process that stops free radical chemical chain reactions. This effect makes organobromine compounds useful as fire retardants, and more than half the bromine produced worldwide each year is put to this purpose. The same property causes ultraviolet sunlight to dissociate volatile organobromine compounds in the atmosphere to yield free bromine atoms, causing ozone depletion. As a result, many organobromide compounds—such as the pesticide methyl bromide—are no longer used. Bromine compounds are still used in well drilling fluids, in photographic film, and as an intermediate in the manufacture of organic chemicals.\n\nLarge amounts of bromide salts are toxic from the action of soluble bromide ion, causing bromism. However, a clear biological role for bromide ion and hypobromous acid has recently been elucidated, and it now appears that bromine is an essential trace element in humans. The role of biological organobromine compounds in sea life such as algae has been known for much longer. As a pharmaceutical, the simple bromide ion (Br) has inhibitory effects on the central nervous system, and bromide salts were once a major medical sedative, before replacement by shorter-acting drugs. They retain niche uses as antiepileptics.\n\nBromine was discovered independently by two chemists, Carl Jacob Löwig and Antoine Balard, in 1825 and 1826, respectively.\n\nLöwig isolated bromine from a mineral water spring from his hometown Bad Kreuznach in 1825. Löwig used a solution of the mineral salt saturated with chlorine and extracted the bromine with diethyl ether. After evaporation of the ether a brown liquid remained. With this liquid as a sample of his work he applied for a position in the laboratory of Leopold Gmelin in Heidelberg. The publication of the results was delayed and Balard published his results first.\n\nBalard found bromine chemicals in the ash of seaweed from the salt marshes of Montpellier. The seaweed was used to produce iodine, but also contained bromine. Balard distilled the bromine from a solution of seaweed ash saturated with chlorine. The properties of the resulting substance were intermediate between those of chlorine and iodine; thus he tried to prove that the substance was iodine monochloride (ICl), but after failing to do so he was sure that he had found a new element, and named it muride, derived from the Latin word \"muria\" for brine.\n\nAfter the French chemists Louis Nicolas Vauquelin, Louis Jacques Thénard, and Joseph-Louis Gay-Lussac approved the experiments of the young pharmacist Balard, the results were presented at a lecture of the Académie des Sciences and published in \"Annales de Chimie et Physique\". In his publication, Balard states that he changed the name from \"muride\" to \"brôme\" on the proposal of M. Anglada. \"Brôme\" (bromine) derives from the Greek βρωμος (stench). Other sources claim that the French chemist and physicist Joseph-Louis Gay-Lussac suggested the name \"brôme\" for the characteristic smell of the vapors. Bromine was not produced in large quantities until 1858, when the discovery of salt deposits in Stassfurt enabled its production as a by-product of potash.\nApart from some minor medical applications, the first commercial use was the daguerreotype. In 1840, bromine was discovered to have some advantages over the previously used iodine vapor to create the light sensitive silver halide layer in daguerreotypy.\n\nPotassium bromide and sodium bromide were used as anticonvulsants and sedatives in the late 19th and early 20th centuries, but were gradually superseded by chloral hydrate and then by the barbiturates. In the early years of the First World War, bromine compounds such as xylyl bromide were used as poison gas.\n\nBromine is the third halogen, being a nonmetal in group 17 of the periodic table. Its properties are thus similar to those of fluorine, chlorine, and iodine, and tend to be intermediate between those of the two neighbouring halogens, chlorine and iodine. Bromine has the electron configuration [Ar]3d4s4p, with the seven electrons in the fourth and outermost shell acting as its valence electrons. Like all halogens, it is thus one electron short of a full octet, and is hence a strong oxidising agent, reacting with many elements in order to complete its outer shell. Corresponding to periodic trends, it is intermediate in electronegativity between chlorine and iodine (F: 3.98, Cl: 3.16, Br: 2.96, I: 2.66), and is less reactive than chlorine and more reactive than iodine. It is also a weaker oxidising agent than chlorine, but a stronger one than iodine. Conversely, the bromide ion is a weaker reducing agent than iodide, but a stronger one than chloride. These similarities led to chlorine, bromine, and iodine together being classified as one of the original triads of Johann Wolfgang Döbereiner, whose work foreshadowed the periodic law for chemical elements. It is intermediate in atomic radius between chlorine and iodine, and this leads to many of its atomic properties being similarly intermediate in value between chlorine and iodine, such as first ionisation energy, electron affinity, enthalpy of dissociation of the X molecule (X = Cl, Br, I), ionic radius, and X–X bond length. The volatility of bromine accentuates its very penetrating, choking, and unpleasant odour.\n\nAll four stable halogens experience intermolecular van der Waals forces of attraction, and their strength increases together with number of electrons among all homonuclear diatomic halogen molecules. Thus, the melting and boiling points of bromine are intermediate between those of chlorine and iodine. As a result of the increasing molecular weight of the halogens down the group, the density and heats of fusion and vaporisation of bromine are again intermediate between those of chlorine and iodine, although all their heats of vaporisation are fairly low (leading to high volatility) thanks to their diatomic molecular structure. The halogens darken in colour as the group is descended: fluorine is a very pale yellow gas, chlorine is greenish-yellow, and bromine is a reddish-brown volatile liquid that melts at −7.2 °C and boils at 58.8 °C. (Iodine is a shiny black solid.) This trend occurs because the wavelengths of visible light absorbed by the halogens increase down the group. Specifically, the colour of a halogen, such as bromine, results from the electron transition between the highest occupied antibonding \"π\" molecular orbital and the lowest vacant antibonding \"σ\" molecular orbital. The colour fades at low temperatures, so that solid bromine at −195 °C is pale yellow.\n\nLike solid chlorine and iodine, solid bromine crystallises in the orthorhombic crystal system, in a layered lattice of Br molecules. The Br–Br distance is 227 pm (close to the gaseous Br–Br distance of 228 pm) and the Br···Br distance between molecules is 331 pm within a layer and 399 pm between layers (compare the van der Waals radius of bromine, 195 pm). This structure means that bromine is a very poor conductor of electricity, with a conductivity of around 5 × 10 Ω cm just below the melting point, although this is better than the essentially undetectable conductivity of chlorine.\n\nAt a pressure of 55 GPa (roughly 540,000 times atmospheric pressure) bromine undergoes an insulator-to-metal transition. At 75 GPa it changes to a face-centered orthorhombic structure. At 100 GPa it changes to a body centered orthorhombic monatomic form.\n\nBromine has two stable isotopes, Br and Br. These are its only two natural isotopes, with Br making up 51% of natural bromine and Br making up the remaining 49%. Both have nuclear spin 3/2− and thus may be used for nuclear magnetic resonance, although Br is more favourable. The relatively 1:1 distribution of the two isotopes in nature is helpful in identification of bromine containing compounds using mass spectroscopy. Other bromine isotopes are all radioactive, with half-lives too short to occur in nature. Of these, the most important are Br (\"t\" = 17.7 min), Br (\"t\" = 4.421 h), and Br (\"t\" = 35.28 h), which may be produced from the neutron activation of natural bromine. The most stable bromine radioisotope is Br (\"t\" = 57.04 h). The primary decay mode of isotopes lighter than Br is electron capture to isotopes of selenium; that of isotopes heavier than Br is beta decay to isotopes of krypton; and Br may decay by either mode to stable Se or Kr.\n\nBromine is intermediate in reactivity between chlorine and iodine, and is one of the most reactive elements. Bond energies to bromine tend to be lower than those to chlorine but higher than those to iodine, and bromine is a weaker oxidising agent than chlorine but a stronger one than iodine. This can be seen from the standard electrode potentials of the X/X couples (F, +2.866 V; Cl, +1.395 V; Br, +1.087 V; I, +0.615 V; At, approximately +0.3 V). Bromination often leads to higher oxidation states than iodination but lower or equal oxidation states to chlorination. Bromine tends to react with compounds including M–M, M–H, or M–C bonds to form M–Br bonds.\n\nThe simplest compound of bromine is hydrogen bromide, HBr. It is mainly used in the production of inorganic bromides and alkyl bromides, and as a catalyst for many reactions in organic chemistry. Industrially, it is mainly produced by the reaction of hydrogen gas with bromine gas at 200–400 °C with a platinum catalyst. However, reduction of bromine with red phosphorus is a more practical way to produce hydrogen bromide in the laboratory:\n\nAt room temperature, hydrogen bromide is a colourless gas, like all the hydrogen halides apart from hydrogen fluoride, since hydrogen cannot form strong hydrogen bonds to the large and only mildly electronegative bromine atom; however, weak hydrogen bonding is present in solid crystalline hydrogen bromide at low temperatures, similar to the hydrogen fluoride structure, before disorder begins to prevail as the temperature is raised. Aqueous hydrogen bromide is known as hydrobromic acid, which is a strong acid (p\"K\" = −9) because the hydrogen bonds to bromine are too weak to inhibit dissociation. The HBr/HO system also involves many hydrates HBr·\"n\"HO for \"n\" = 1, 2, 3, 4, and 6, which are essentially salts of bromine anions and hydronium cations. Hydrobromic acid forms an azeotrope with boiling point 124.3 °C at 47.63 g HBr per 100 g solution; thus hydrobromic acid cannot be concentrated beyond this point by distillation.\n\nUnlike hydrogen fluoride, anhydrous liquid hydrogen bromide is difficult to work with as a solvent, because its boiling point is low, it has a small liquid range, its dielectric constant is low and it does not dissociate appreciably into HBr and ions – the latter, in any case, are much less stable than the bifluoride ions () due to the very weak hydrogen bonding between hydrogen and bromine, though its salts with very large and weakly polarising cations such as Cs and (R = Me, Et, Bu) may still be isolated. Anhydrous hydrogen bromide is a poor solvent, only able to dissolve small molecular compounds such as nitrosyl chloride and phenol, or salts with very low lattice energies such as tetraalkylammonium halides.\n\nNearly all elements in the periodic table form binary bromides. The exceptions are decidedly in the minority and stem in each case from one of three causes: extreme inertness and reluctance to participate in chemical reactions (the noble gases, with the exception of xenon in the very unstable XeBr); extreme nuclear instability hampering chemical investigation before decay and transmutation (many of the heaviest elements beyond bismuth); and having an electronegativity higher than bromine's (oxygen, nitrogen, fluorine, and chlorine), so that the resultant binary compounds are formally not bromides but rather oxides, nitrides, fluorides, or chlorides of bromine. (Nonetheless, nitrogen tribromide is named as a bromide as it is analogous to the other nitrogen trihalides.)\n\nBromination of metals with Br tends to yield lower oxidation states than chlorination with Cl when a variety of oxidation states is available. Bromides can be made by reaction of an element or its oxide, hydroxide, or carbonate with hydrobromic acid, and then dehydrated by mildly high temperatures combined with either low pressure or anhydrous hydrogen bromide gas. These methods work best when the bromide product is stable to hydrolysis; otherwise, the possibilities include high-temperature oxidative bromination of the element with bromine or hydrogen bromide, high-temperature bromination of a metal oxide or other halide by bromine, a volatile metal bromide, carbon tetrabromide, or an organic bromide. For example, niobium(V) oxide reacts with carbon tetrabromide at 370 °C to form niobium(V) bromide. Another method is halogen exchange in the presence of excess \"halogenating reagent\", for example:\nWhen a lower bromide is wanted, either a higher halide may be reduced using hydrogen or a metal as a reducing agent, or thermal decomposition or disproportionation may be used, as follows:\n\nMost of the bromides of the pre-transition metals (groups 1, 2, and 3, along with the lanthanides and actinides in the +2 and +3 oxidation states) are mostly ionic, while nonmetals tend to form covalent molecular bromides, as do metals in high oxidation states from +3 and above. Silver bromide is very insoluble in water and is thus often used as a qualitative test for bromine.\n\nThe halogens form many binary, diamagnetic interhalogen compounds with stoichiometries XY, XY, XY, and XY (where X is heavier than Y), and bromine is no exception. Bromine forms a monofluoride and monochloride, as well as a trifluoride and pentafluoride. Some cationic and anionic derivatives are also characterised, such as , , , , and . Apart from these, some pseudohalides are also known, such as cyanogen bromide (BrCN), bromine thiocyanate (BrSCN), and bromine azide (BrN).\n\nThe pale-brown bromine monofluoride (BrF) is unstable at room temperature, disproportionating quickly and irreversibly into bromine, bromine trifluoride, and bromine pentafluoride. It thus cannot be obtained pure. It may be synthesised by the direct reaction of the elements, or by the comproportionation of bromine and bromine trifluoride at high temperatures. Bromine monochloride (BrCl), a red-brown gas, quite readily dissociates reversibly into bromine and chlorine at room temperature and thus also cannot be obtained pure, though it can be made by the reversible direct reaction of its elements in the gas phase or in carbon tetrachloride. Bromine monofluoride in ethanol readily leads to the monobromination of the aromatic compounds PhX (\"para\"-bromination occurs for X = Me, Bu, OMe, Br; \"meta\"-bromination occurs for the deactivating X = –COEt, –CHO, –NO); this is due to heterolytic fission of the Br–F bond, leading to rapid electrophilic bromination by Br.\n\nAt room temperature, bromine trifluoride (BrF) is a straw-coloured liquid. It may be formed by directly fluorinating bromine at room temperature and is purified through distillation. It reacts explosively with water and hydrocarbons, but is a less violent fluorinating reagent than chlorine trifluoride. It reacts vigorously with boron, carbon, silicon, arsenic, antimony, iodine, and sulfur to give fluorides, and also reacts with most metals and their oxides: as such, it is used to oxidise uranium to uranium hexafluoride in the nuclear industry. Refractory oxides tend to be only partially fluorinated, but here the derivatives KBrF and BrFSbF remain reactive. Bromine trifluoride is a useful nonaqueous ionising solvent, since it readily dissociates to form and and thus conducts electricity.\n\nBromine pentafluoride (BrF) was first synthesised in 1930. It is produced on a large scale by direct reaction of bromine with excess fluorine at temperatures higher than 150 °C, and on a small scale by the fluorination of potassium bromide at 25 °C. It is a very vigorous fluorinating agent, although chlorine trifluoride is still more violent. Bromine pentafluoride explodes on reaction with water and fluorinates silicates at 450 °C.\n\nAlthough dibromine is a strong oxidising agent with a high first ionisation energy, very strong oxidisers such as peroxydisulfuryl fluoride (SOF) can oxidise it to form the cherry-red cation. A few other bromine cations are known, namely the brown and dark brown . The tribromide anion, , has also been characterised; it is analogous to triiodide.\n\nBromine oxides are not as well-characterised as chlorine oxides or iodine oxides, as they are all fairly unstable: it was once thought that they could not exist at all. Dibromine monoxide is a dark-brown solid which, while reasonably stable at −60 °C, decomposes at its melting point of −17.5 °C; it is useful in bromination reactions and may be made from the low-temperature decomposition of bromine dioxide in a vacuum. It oxidises iodine to iodine pentoxide and benzene to 1,4-benzoquinone; in alkaline solutions, it gives the hypobromite anion.\n\nSo-called \"bromine dioxide\", a pale yellow crystalline solid, may be better formulated as bromine perbromate, BrOBrO. It is thermally unstable above −40 °C, violently decomposing to its elements at 0 °C. Dibromine trioxide, \"syn\"-BrOBrO, is also known; it is the anhydride of hypobromous acid and bromic acid. It is an orange crystalline solid which decomposes above −40 °C; if heated too rapidly, it explodes around 0 °C. A few other unstable radical oxides are also known, as are some poorly characterised oxides, such as dibromine pentoxide, tribromine octoxide, and bromine trioxide.\n\nThe four oxoacids, hypobromous acid (HOBr), bromous acid (HOBrO), bromic acid (HOBrO), and perbromic acid (HOBrO), are better studied due to their greater stability, though they are only so in aqueous solution. When bromine dissolves in aqueous solution, the following reactions occur:\n\nHypobromous acid is unstable to disproportionation. The hypobromite ions thus formed disproportionate readily to give bromide and bromate:\n\nBromous acids and bromites are very unstable, although the strontium and barium bromites are known. More important are the bromates, which are prepared on a small scale by oxidation of bromide by aqueous hypochlorite, and are strong oxidising agents. Unlike chlorates, which very slowly disproportionate to chloride and perclorate, the bromate anion is stable to disproportionation in both acidic and aqueous solutions. Bromic acid is a strong acid. Bromides and bromates may comproportionate to bromine as follows:\n\nThere were many failed attempts to obtain perbromates and perbromic acid, leading to some rationalisations as to why they should not exist, until 1968 when the anion was first synthesised from the radioactive beta decay of unstable . Today, perbromates are produced by the oxidation of alkaline bromate solutions by fluorine gas. Excess bromate and fluoride are precipitated as silver bromate and calcium fluoride, and the perbromic acid solution may be purified. The perbromate ion is fairly inert at room temperature but is thermodynamically extremely oxidising, with extremely strong oxidising agents needed to produce it, such as fluorine or xenon difluoride. The Br–O bond in is fairly weak, which corresponds to the general reluctance of the 4p elements (especially arsenic, selenium, and bromine) to attain their maximum possible oxidation state, as they come after the scandide contraction characterised by the poor shielding afforded by the radial-nodeless 3d orbitals.\n\nLike the other carbon–halogen bonds, the C–Br bond is a common functional group that forms part of core organic chemistry. Formally, compounds with this functional group may be considered organic derivatives of the bromide anion. Due to the difference of electronegativity between bromine (2.96) and carbon (2.55), the carbon in a C–Br bond is electron-deficient and thus electrophilic. The reactivity of organobromine compounds resembles but is intermediate between the reactivity of organochlorine and organoiodine compounds. For many applications, organobromides represent a compromise of reactivity and cost.\n\nOrganobromides are typically produced by additive or substitutive bromination of other organic precursors. Bromine itself can be used, but due to its toxicity and volatility safer brominating reagents are normally used, such as \"N\"-bromosuccinimide. The principal reactions for organobromides include dehydrobromination, Grignard reactions, reductive coupling, and nucleophilic substitution.\n\nOrganobromides are the most common organohalides in nature, even though the concentration of bromide is only 0.3% of that for chloride in sea water, because of the easy oxidation of bromide to the equivalent of Br, a potent electrophile. The enzyme bromoperoxidase catalyzes this reaction. The oceans are estimated to release 1–2 million tons of bromoform and 56,000 tons of bromomethane annually.\nAn old qualitative test for the presence of the alkene functional group is that alkenes turn brown aqueous bromine solutions colourless, forming a bromohydrin with some of the dibromoalkane also produced. The reaction passes through a short-lived strongly electrophilic bromonium intermediate. This is an example of a halogen addition reaction.\n\nBromine is significantly less abundant in the crust than fluorine or chlorine, comprising only 2.5 parts per million of the Earth's crustal rocks, and then only as bromide salts. It is the forty-sixth most abundant element in Earth's crust. It is significantly more abundant in the oceans, resulting from long-term leaching. There, it makes up 65 parts per million, corresponding to a ratio of about one bromine atom for every 660 chlorine atoms. Salt lakes and brine wells may have higher bromine concentrations: for example, the Dead Sea contains 0.4% bromide ions. It is from these sources that bromine extraction is mostly economically feasible.\n\nThe main sources of bromine are in the United States and Israel. The element is liberated by halogen exchange, using chlorine gas to oxidise Br to Br. This is then removed with a blast of steam or air, and is then condensed and purified. Today, bromine is transported in large-capacity metal drums or lead-lined tanks that can hold hundreds of kilograms or even tonnes of bromine. The bromine industry is about one-hundredth the size of the chlorine industry. Laboratory production is unnecessary because bromine is commercially available and has a long shelf life.\n\nA wide variety of organobromine compounds are used in industry. Some are prepared from bromine and others are prepared from hydrogen bromide, which is obtained by burning hydrogen in bromine.\n\nBrominated flame retardants represent a commodity of growing importance, and make up the largest commercial use of bromine. When the brominated material burns, the flame retardant produces hydrobromic acid which interferes in the radical chain reaction of the oxidation reaction of the fire. The mechanism is that the highly reactive hydrogen radicals, oxygen radicals, and hydroxy radicals react with hydrobromic acid to form less reactive bromine radicals (i.e., free bromine atoms). Bromine atoms may also react directly with other radicals to help terminate the free radical chain-reactions that characterise combustion.\n\nTo make brominated polymers and plastics, bromine-containing compounds can be incorporated into the polymer during polymerisation. One method is to include a relatively small amount of brominated monomer during the polymerisation process. For example, vinyl bromide can be used in the production of polyethylene, polyvinyl chloride or polypropylene. Specific highly brominated molecules can also be added that participate in the polymerisation process For example, tetrabromobisphenol A can be added to polyesters or epoxy resins, where it becomes part of the polymer. Epoxys used in printed circuit boards are normally made from such flame retardant resins, indicated by the FR in the abbreviation of the products (FR-4 and FR-2). In some cases the bromine containing compound may be added after polymerisation. For example, decabromodiphenyl ether can be added to the final polymers.\n\nA number of gaseous or highly volatile brominated halomethane compounds are non-toxic and make superior fire suppressant agents by this same mechanism, and are particular effective in enclosed spaces such as submarines, airplanes, and spacecraft. However, they are expensive and their production and use has been greatly curtailed due to their effect as ozone-depleting agents. They are no longer used in routine fire extinguishers, but retain niche uses in aerospace and military automatic fire-suppression applications. They include bromochloromethane (Halon 1011, CHBrCl), bromochlorodifluoromethane (Halon 1211, CBrClF), and bromotrifluoromethane (Halon 1301, CBrF).\n\nSilver bromide is used, either alone or in combination with silver chloride and silver iodide, as the light sensitive constituent of photographic emulsions.\n\nEthylene bromide was an additive in gasolines containing lead anti-engine knocking agents. It scavenges lead by forming volatile lead bromide, which is exhausted from the engine. This application accounted for 77% of the bromine use in 1966 in the US. This application has declined since the 1970s due to environmental regulations (see below).\n\nPoisonous bromomethane was widely used as pesticide to fumigate soil and to fumigate housing, by the tenting method. Ethylene bromide was similarly used. These volatile organobromine compounds are all now regulated as ozone depletion agents. The Montreal Protocol on Substances that Deplete the Ozone Layer scheduled the phase out for the ozone depleting chemical by 2005, and organobromide pesticides are no longer used (in housing fumigation they have been replaced by such compounds as sulfuryl fluoride, which contain neither the chlorine or bromine organics which harm ozone). Before the Montreal protocol in 1991 (for example) an estimated 35,000 tonnes of the chemical were used to control nematodes, fungi, weeds and other soil-borne diseases.\n\nIn pharmacology, inorganic bromide compounds, especially potassium bromide, were frequently used as general sedatives in the 19th and early 20th century. Bromides in the form of simple salts are still used as anticonvulsants in both veterinary and human medicine, although the latter use varies from country to country. For example, the U.S. Food and Drug Administration (FDA) does not approve bromide for the treatment of any disease, and it was removed from over-the-counter sedative products like Bromo-Seltzer, in 1975. Commercially available organobromine pharmaceuticals include the vasodilator nicergoline, the sedative brotizolam, the anticancer agent pipobroman, and the antiseptic merbromin. Otherwise, organobromine compounds are rarely pharmaceutically useful, in contrast to the situation for organofluorine compounds. Several drugs are produced as the bromide (or equivalents, hydrobromide) salts, but in such cases bromide serves as an innocuous counterion of no biological significance.\n\nOther uses of organobromine compounds include high-density drilling fluids, dyes (such as Tyrian purple and the indicator bromothymol blue), and pharmaceuticals. Bromine itself, as well as some of its compounds, are used in water treatment, and is the precursor of a variety of inorganic compounds with an enormous number of applications (e.g. silver bromide for photography). Zinc–bromine batteries are hybrid flow batteries used for stationary electrical power backup and storage; from household scale to industrial scale.\n\nA 2014 study suggests that bromine (in the form of bromide ion) is a necessary cofactor in the biosynthesis of collagen IV, making the element essential to basement membrane architecture and tissue development in animals. Nevertheless, no clear deprivation symptoms or syndromes have been documented. In other biological functions, bromine may be non-essential but still beneficial when it takes the place of chlorine. For example, in the presence of hydrogen peroxide, HO, formed by the eosinophil, and either chloride or bromide ions, eosinophil peroxidase provides a potent mechanism by which eosinophils kill multicellular parasites (such as, for example, the nematode worms involved in filariasis) and some bacteria (such as tuberculosis bacteria). Eosinophil peroxidase is a haloperoxidase that preferentially uses bromide over chloride for this purpose, generating hypobromite (hypobromous acid), although the use of chloride is possible. \n\nAlthough α-haloesters are generally thought of as highly reactive, and therefore, toxic intermediates in organic synthesis, mammals, including humans, cats, and rats, appear to biosynthesize traces of an α-bromoester, 2-octyl 4-bromo-3-oxobutanoate, which is found in their cerebrospinal fluid and appears to play a yet unclarified role in inducing REM sleep. Human neutrophils use myeloperoxidase, H2O2, and Br(-) to brominate deoxycytidine, which is a mutagen, so transhalogenation reactions with bromine may be responsible for some human cancers. Marine organisms are the main source of organobromine compounds, and it is in these organisms that the essentiality of bromine is on much firmer ground. More than 1600 such organobromine compounds were identified by 1999. The most abundant is methyl bromide (CHBr), of which an estimated 56,000 tonnes is produced by marine algae each year. The essential oil of the Hawaiian alga \"Asparagopsis taxiformis\" consists of 80% bromoform. Most of such organobromine compounds in the sea are made by the action of a unique algal enzyme, vanadium bromoperoxidase.\n\nThe bromide anion is not very toxic: a normal daily intake is 2 to 8 milligrams. However, high levels of bromide chronically impair the membrane of neurons, which progressively impairs neuronal transmission, leading to toxicity, known as bromism. Bromide has an elimination half-life of 9 to 12 days, which can lead to excessive accumulation. Doses of 0.5 to 1 gram per day of bromide can lead to bromism. Historically, the therapeutic dose of bromide is about 3 to 5 grams of bromide, thus explaining why chronic toxicity (bromism) was once so common. While significant and sometimes serious disturbances occur to neurologic, psychiatric, dermatological, and gastrointestinal functions, death from bromism is rare. Bromism is caused by a neurotoxic effect on the brain which results in somnolence, psychosis, seizures and delirium.\n\nElemental bromine is toxic and causes chemical burns on human flesh. Inhaling bromine gas results in similar irritation of the respiratory tract, causing coughing, choking, and shortness of breath, and death if inhaled in large enough amounts. Chronic exposure may lead to frequent bronchial infections and a general deterioration of health. As a strong oxidising agent, bromine is incompatible with most organic and inorganic compounds. Caution is required when transporting bromine; it is commonly carried in steel tanks lined with lead, supported by strong metal frames. The Occupational Safety and Health Administration (OSHA) of the United States has set a permissible exposure limit (PEL) for bromine at a time-weighted average (TWA) of 0.1 ppm. The National Institute for Occupational Safety and Health (NIOSH) has set a recommended exposure limit (REL) of TWA 0.1 ppm and a short-term limit of 0.3 ppm. The exposure to bromine immediately dangerous to life and health (IDLH) is 3 ppm. Bromine is classified as an extremely hazardous substance in the United States as defined in Section 302 of the U.S. Emergency Planning and Community Right-to-Know Act (42 U.S.C. 11002), and is subject to strict reporting requirements by facilities which produce, store, or use it in significant quantities.\n"}
{"id": "46180649", "url": "https://en.wikipedia.org/wiki?curid=46180649", "title": "Buffalo Box", "text": "Buffalo Box\n\nA curb box (also known as a valve box, buffalo box, or b-box) is a vertical cast iron sleeve, accessible from the public way, housing the shut-off valve (curb cock or curb stop) for a property's water service line. It is typically located between a building and the district's water main lines and usually consists of a metal tube with a removable or sliding lid, allowing access to the turn-key within. It typically serves as the point denoting the separation of utility-maintained and privately-maintained water facilities.\n\nA curb box is often called a \"Buffalo Box\", because it originated in Buffalo, New York.\n"}
{"id": "4277390", "url": "https://en.wikipedia.org/wiki?curid=4277390", "title": "Cam-in-block", "text": "Cam-in-block\n\nThe cam-in-block valvetrain layout of piston engines is one where the camshaft is placed within the cylinder block, usually beside and slightly above the crankshaft in a straight engine or directly above the crankshaft in the V of a V engine. This contrasts with an overhead camshaft (OHC) design which places the camshafts within the cylinder head and drives the valves directly or through short rocker arms. \n\nPlacing the camshaft inside the engine block has a long history in its use in valve-in-block engines, in straight and V configurations, the Ford flathead being exemplary of the type. Pushrod overhead valve engines with the cam in the block were long used in Chevrolet and Buick straight engines from the 1930s through the mid-1950s and in various similar six-cylinder engines until the extensive employment of the V6 configuration in the 1980s. \n\nThere are three main cam-in-block designs:\n\nL-head (\"flathead\") refers to the pushrod valvetrain configuration in which the valves are placed in the engine block beside the pistons. The design was common on early engine designs, but has since fallen from use.\n\nGenerally L-head engines use a small chamber on one side of the cylinder to carry the valves. This has a number of advantages, primarily in that it makes the cylinder head much simpler. It also means that the valve can be operated by pushing directly up on it, as opposed to needing some sort of mechanical arrangement to push the valves down. It may also lead to slightly easier cooling, as the valves and operating rods are out of the way of the cylinder, making a cooling jacket simpler to construct (but see below). The line of intakes along the side of the engine lead to the name L-head, due to the cylinders having the shape of an upside-down L. This configuration is also known as sidevalve, as the valves are located be\"side\" the cylinders.\n\nOn the downside, the L-head engine also requires the airflow to make at least a 90° turn to enter the cylinder, which makes it less efficient; colloquially it's said that such an engine has poorer \"breathing\". Breathing was not greatly emphasized in past production cars because engines could not run long and reliably at high speed due to other factors. This was a minor concern given the benefits in simplicity.\n\nAlthough L-head inline 4 and 6-cylinder engines were frequently used for automobiles, tractors, etc., the best known L-head automotive engine is the early 20th century Ford V-8, which has both sets of valves (intake and exhaust) located on the inside of the \"Vee,\" and which are all operated by a single camshaft located above the crankshaft. The exhaust follows a lengthy path to leave the engine. This virtually guarantees that the engine will need an unusually large coolant radiator to avoid overheating under sustained heavy use. A flathead design in a V engine, with the air intake/fuel system and all of the exhaust and intake valves inside of the \"V\" requires that the exhaust gas be passed between the cylinders to outside of the V to the exhaust system. Exhaust heat is thus passed to the coolant (as it exits the engine between the cylinders). In the Ford V-8 flathead design, manufactured from 1932 through 1952, the center exhaust port on the outside of the block exhausts the gasses from two cylinders, exacerbating the high heat problem. This \"very hot in the middle\" problem makes this particular engine prone to heat-related stress and cracks in the cylinder block. In line engine exhaust gas exits the block more directly and does not cross between the cylinders and is a more temperature-stable design. Whenever exhaust ports and valves are in the cylinder head, exhausting heat has far less time to heat the coolant, and such engines are more durable under high load conditions and a similar sized engine will require less coolant radiator capacity than a flathead V-8. \n\nDue to the heating and efficiency problems, L-head engines fell from high power uses such as aircraft engines fairly quickly, prior to World War I. They lived on for some time in the automotive world and were used in the World War II Jeep, for instance. L-heads are no longer used in automobile engines, although they remain in common use for small-engine applications in lawnmowers and generators. Because of their heat-retaining design, the size of valves and the compression ratio are limited (the valve/combustion chamber is away from the piston top typically creating a larger combustion space--a lower compression ratio), which in turn reduces available power and economy. Not all L-heads are cam-in-block engines; the location of the camshaft varies in this layout.\n\nT-head flathead engines have a crossflow layout, with the exhaust valves on the opposite side from the inlet valves. A single-cylinder T-head engine needs only a single camshaft; multi-cylinder T-heads need two.\n\nThe I-head design is one in which the entry and exit valves and ports are contained in the cylinder head. It was developed by the Scottish-American David Dunbar Buick. It employed pushrod-actuated valves parallel to the pistons and is still in use today in some designs (notably several engines produced by General Motors and Chrysler). \n\nIt has several advantages over L- and F-head designs, but the most notable is the fact that the intake charge and exhaust gases have a more direct path into and out of the combustion chambers, increasing power, improving fuel efficiency and reducing noxious exhaust emissions.\n\nThe F-head layout (not to be confused with flathead) can be thought of as a combination of L-head and I-head: the intake manifold and its valves are located atop the cylinders (in the cylinder head, as in an I-head design) and are operated by pushrods, but the exhaust manifold and its valves are located beside the cylinders (in the block, as in an L-head design). The exhaust valves are either roughly or exactly parallel with the pistons; their faces point upwards and they are not operated by pushrods, but by direct contact with a lifter contacting the camshaft. Reverse variation of F-head with side intake and in head exhaust were also made- the Ford V8 overhead exhaust valve conversions to flathead engines were to decrease the overheating under load problems in commercial service. The Indian/Henderson 4-cylinder motorcycle engine family used both designs- the overhead exhaust was again an overheating consideration design. \n\nThis was a more expensive engine design. Its advantages over competing L-head engines included more power from its higher compression, better intake mixture flow, less susceptibility to pinging, and greater reliability from its cooling of the exhaust valve and its spring (and having half the number of pushrods of an OHV engine). With only one valve in the head, and one in the block, larger valves can be used than in an OHV engine, to offset the poorer airflow of a side exhaust valve.\n\nFor years the British motor car firms Rolls-Royce and Rover used this arrangement. From 1927 to 1929, the American firm Hudson used a 6-cylinder engine of this form as well, but this engine is not to be confused with that of the race-winning Hudsons of the 1950s. The last major use was the Willys Hurricane engine, used in civilian Jeeps in the 1950s and 1960s. It was replaced by the I-head design.\n\n"}
{"id": "20294783", "url": "https://en.wikipedia.org/wiki?curid=20294783", "title": "Charles Grafton Page", "text": "Charles Grafton Page\n\nCharles Grafton Page (in Salem, Massachusetts January 25, 1812 – May 5, 1868 in Washington, D.C.) was an American electrical experimenter and inventor, physician, patent examiner, patent advocate, and professor of chemistry.\n\nLike his more famous contemporaries Michael Faraday and Joseph Henry, Page began his career as an astute natural philosopher who developed innovative work with natural phenomena through direct observation and experimenting. Toward the later part of their careers, the science of the day had moved on to a more mathematical emphasis in which these scientists did not participate.\n\nThrough his exploratory experiments and distinctive inventions, Page developed a deep understanding of electromagnetism. He applied this understanding in the service of the US Patent Office, in support of other inventors, and in pursuing his own ill-fated dream of electromagnetic locomotion. His work had a lasting impact on telegraphy and in the practice and politics of patenting scientific innovation, challenging the rising scientific elitism that maintained 'the scientific do not patent'.\n\nCharles Grafton Page was born to Captain Jere Lee Page and Lucy Lang Page on January 25, 1812 in Salem, Massachusetts. Having eight siblings, four of each gender, he was the only one of five sons to pursue a career into mature adulthood. One of his brothers died in infancy. Brother George died from typhoid at age sixteen, brother Jery perished on a sea expedition to the Caribbean at age twenty-five, and Henry, afflicted by poliomyelitis, was not able to support himself. In writing to Charles Grafton during his final voyage, Jery expressed the family’s hope for his success: \"You are the only classical Page in our book.\"\n\nPage's curiosity about electricity was evident from childhood. At age nine, he climbed on top of his parents' house with a fire-shovel in an attempt to catch electricity during a thunderstorm. At age ten, he built an electrostatic machine that he used to shock his friends. At sixteen, Page developed the \"portable electrophorus,\" which served as the foundation for his first published article in the \"American Journal of Science\"(Page, 1834).\n\nOther early interests, including botany, entomology (Page, 1836b), and floriculture, contributed to his scientific training and later avocations.\n\nAfter graduating from medical school, Page continued to reside in his parents' Salem home and opened a small medical practice. In a well-stocked lab that he set up there, he experimented with electricity, demonstrated effects that no one had observed before, and improvised original apparatus that amplified these effects.\n\nWhen his father retired from a successful career as a sea captain in trade with East India, Page joined his family in relocating to rural Virginia outside Washington DC. \nPage married Priscilla Sewall Webster in 1844. Priscilla was the younger sister of the wife of a Washington physician, Harvey Lindsly, who was among Page's colleagues. One son died in infancy. The couple brought up three sons and two daughters. Their oldest daughter, Emelyn or Emmie, died less than a year before Page's own death. Their youngest son, Harvey Lindsly Page (1859–1934), was named for his uncle. He was a famous American architect and inventor, of San Antonio, Texas. http://www.tshaonline.org/handbook/online/articles/fpa75\n\nPage pursued undergraduate studies at Harvard College from 1828–1832, studying chemistry under Professor John White Webster. A classmate at Salem Latin School who also attended college and medical school with him, Henry Wheatland described Page as popular, fun-loving, athletic, a fine singer and \"a loved companion\". Page participated in organizing a college chemical club where he demonstrated electricity and other phenomena. After receiving an M.D. from Harvard Medical School in 1836, he practiced medicine and gave public lectures on chemistry in Salem.\n\nWhen Page moved to northern Virginia in 1838, he continued his experimental research and set up a medical practice which he sustained for several years.\n\nPage served as a patent examiner in the United States Patent Office in Washington, D.C. for two periods: 1842-1852 and 1861-1868. He became senior patent examiner during his first term. During the years intervening, he took up a business as a patent agent or solicitor to help other inventors secure patents, established and edited the short-lived \"The American Polytechnic Journal\", and pursued his own interests in electromagnetism, floriculture and other areas. As a patent agent, Page handled up to 50 successful patents a year, including patents for Eben Norton Horsford, Walter Hunt and others. Page's 1861 return to the Patent Agency as an examiner occurred in the wake of numerous dismissals of patent office employees under the new Administration of Abraham Lincoln.\n\nPage figured as a key witness in the Morse v. O'Reilly telegraph lawsuit of 1848. However, when Morse sought an extension of his patent on telegraph apparatus twelve years later, Page refuted Morse's role as inventor and was perhaps influential in the extensions' denial.\n\nFrom 1844-1849, Page was Professor of Chemistry and [Pharmacy] in the Medical Department at Columbian College in Washington, D.C. (now George Washington University).\nHe held other public roles such as that of advising the choice of stone to be used in constructing the Smithsonian Institution and the Washington Monument to the committees in charge of these projects.\n\nThroughout his life, Page published more than one-hundred articles over the course of three distinct periods: the late 1830s, the mid-1840s, and the early 1850s. The first period (1837–1840) was especially crucial in developing his analytic skills. Over 40 of his articles appeared in the \"American Journal of Science\" edited by Benjamin Silliman; some of these were reprinted at the time in William Sturgeon’s \"Annals of Electricity, Magnetism, and Chemistry\" printed in Great Britain. The \"Royal Society Catalogue of Scientific Papers\" (1800–1863 volume) records many of Page's papers, however this listing is incomplete, as is that provided in (Post, 1976a, p. 207-213).\n\nWhile still a medical student at Harvard, Page conducted a ground-breaking experiment which demonstrated the presence of electricity in an arrangement of a spiral conductor that no one had tried before. His experiment was a response to a short paper by Joseph Henry, announcing that a strong electric shock was obtained from a ribbon strip of copper, spiralled up between fabric insulation, at the moment when battery current stopped running in this conductor. These strong shocks manifested the electrical property of self-inductance which Faraday had identified in researches published prior to Henry's, building on his own landmark discovery of electromagnetic induction. Page seemed to be unaware of \nFaraday's analysis.\n\nPage's innovation was to construct a spiral conductor having cups filled with mercury (element) as electrical connectors that were placed at various positions along its length. He then connected one terminal from an electrochemical cell battery to the inner cup of the spiral, and put the other battery terminal into some other cup of the spiral. The direct battery current flowed through the spiral, from cup to cup. He held a metal wand in each hand, and put these wands into the same two cups as where the battery terminals went — or any other pair of cups. When an assistant removed one of the battery terminals, stopping the current from going in the spiral, Page received a shock. He reported stronger shocks when his hands covered more of the spiral's length than where direct battery current went. He even felt shocks from parts of the spiral where no direct battery current passed. He used acupuncture needles, pierced into his fingers, to amplify his sense of shock. \n\nWhile Page advocated the use of this shocking device as a medical treatment, an early form of electrotherapy, his own interest lay in its heightening of electrical tension, or voltage above that of the low voltage battery input, and in its other electrical behaviors. Page went on to improve the instrument, giving it the name 'Dynamic Multiplier'.\n\nIn order for Page's instrument to produce the shock, the battery current had to be stopped. In order to experience another shock, the battery had to be started again, and then stopped. Page invented the first interrupters, to provide a repeatable means of connecting and disconnecting the circuit. In these devices, electrical flow is started and stopped as a rocking or rotary motion lifts electrical contacts out of a mercury pool. An electric motor effect is responsible for the continued operation of the switch.\n\nCrucial to Page's research with the spiral conductor was his capacity to explore and question the unknown, where the physical effects were enigmatic and the 'received theories' inadequate. Page did not provide an explanation for what he found, yet he extended and amplified the apparatus and its unexpected behaviors. A recent reconstruction of Page's experiment corroborates the central role of ambiguity in his work, finding:\n\nPage's publication about his spiral instrument was well received in the American science community and in England, putting him into the upper ranks of American science at the time.\n\nBritish experimenter William Sturgeon reprinted Page's article in his journal \"Annals of Electricity\". Sturgeon provided an analysis of the electromagnetic effect involved; Page drew on and expanded Sturgeon's analysis in his own later work. Sturgeon devised coils that were adaptations of Page's instrument, where battery current flowed through one, inner, segment of a coil, and electrical shock was taken from the entire length of a coil.\n\nThrough the input from Sturgeon, as well as his own continuing researches, Page developed coil instruments that were the foundation for the eventual induction coil. These instruments had two wires. One wire, termed the \"primary\", carried battery current; a shock was taken across the ends of the other much longer wire, termed the \"secondary\" (see transformer). The primary wire was wound concentrically over an iron core; the secondary was wound over that. Page developed a deep understanding of the underlying behaviors. In Page's published account of his coil, he termed it \nand its contact breaker the 'Compound Electro-Magnet and \nElectrotome'. Page's patent model for this coil is on display at the National Museum of American History.\n\nIn a subsequent experiment with a spiral conductor, Page mounted it rigidly between the poles of a suspended horseshoe magnet. When current stopped flowing in the spiral, a tone could be heard from the magnet, which Page termed 'galvanic music'. Thirty years later, Alexander Graham Bell cited Page's galvanic music as an important precedent for his development of telephony.\n\nAn astute observer and exploratory experimenter, Page invented many other electromagnetic devices. Some of these involved the electromagnetic motor effect in original ways. Many prototypes devised by Page were turned into products manufactured and marketed by Boston instrument-maker Daniel Davis, Jr., the first American to specialize in magnetic philosophical instruments.\n\nWhile consulting with Samuel F.B. Morse and Alfred Lewis Vail on the development of [telegraph] apparatus and techniques, Page contributed to the adoption of suspended wires using a ground return, designed a signal receiver magnet and tested a magneto as a source to substitute for the battery.\n\nDuring the 1840s, Page developed what he termed the Axial Engine. This instrument used an electromagnetic solenoid coil to draw an iron rod into its hollow interior. The rod's displacement opened a switch that stopped current from flowing in the coil; then being unattracted, the rod reverted out of the coil, and this cycle repeated again. The resulting reciprocating motion of the rod back and forth, into and out of the coil, was converted to rotary motion by the mechanism. After demonstrating uses of this engine to run saws and pumps, Page successfully petitioned the U.S. Senate for funds to produce an electromagnetic locomotive, based on this design.\n\nWith these funds plus personal resources that took him into debt, Page built and tested the first full-sized electromagnetic locomotive, preceded only by the 1842 battery-powered model-sized \"Galvani\" of Scottish inventor Robert Davidson. Along the way, Page constructed a series of motors, revisions of the axial engine having different dimensions and mechanical features, which he tested thoroughly. The motor operated on large electrochemical cells, acid batteries having as electrodes zinc and costly platinum, with fragile clay diaphragms between the cells. Page's 1850 American Association for the Advancement of Science presentation about his progress impressed Joseph Henry, Benjamin Silliman and other leading scientists. \n\nOn April 29, 1851, Page boosts its motors from 8 to 20 HP power. With two engines, he drives a 10-ton locomotive with a top speed of 30 km/h. He travels the route from Washington to Bladenburg in 19 minutes. Page conducted a full test, intending to run the 21,000 pound locomotive from Washington DC to Baltimore and back with passengers on board. Problems immediately arose. High voltage sparks, resulting from the very effect Page had investigated with the spiral conductor, broke through the electrical insulation of the electrical coils, resulting in short circuits. Many of the battery's fragile clay dividers cracked on starting up; others broke down subsequently. Page and his mechanic Ari Davis struggled to make repairs and keep the locomotive running. With some periods of steady running, the silent engine (by comparison to a steam locomotive) traveled 5 miles to Bladensburg, Maryland, attaining a top speed of 19 miles per hour. Page prudently reversed direction there, for what was an arduous, calamity-laced return to the National Capitol.\n\nThe failures of Page's electromagnetic locomotive test run were cautionary to other inventors who eventually found other means than batteries to produce electrically driven locomotion. Before Page began his attempt, work such as that of James Prescott Joule had generated a general consensus among scientists that \"the battery powered motor was a hopelessly impractical device\". Page had disregarded those findings. He himself never gave up believing in the practical potential of \nhis design.\n\nA century and a half after Page's locomotive, the technology of Battery electric vehicles is demonstrating applications in numerous transportation contexts.\n\nComfortable himself in public performance as a popular lecturer and singer, being skilled in ventriloquism as well, Page was astute in detecting the misuse of performative acts in defrauding a gullible public. One class of fraudulent schemes prevalent at the time involved communications with spirits by means of rapping sounds, the motion of a table, or other such signs produced in the vicinity of the perpetrator-medium. The sounds and motions were attributed to occult forces and forms of electricity. The Fox sisters, of Rochester New York, made these claims notorious by exhibiting in public and private settings, while collecting money from their audiences.\n\nInvestigating some of these performers in person, Page produced a book that exposes various means of deception they employed (Page, 1853a). He described his analysis of these techniques during a sitting with the Fox sisters. Each time a critical observer peered under the table around which the sisters were seated, the spirit rapping ceased; whenever the observer sat upright, the sounds recommenced. Page asked to have the spirit sounds displayed elsewhere than via the table. One sister climbed into a wardrobe closet. Page identified where her long dress (concealing a stick or other apparatus) contacted the wardrobe. Through his expert knowledge of ventriloquism, Page detected how this performer was misdirecting the viewer's attention away from the actual source of the sound while building expectations to suppose the sound came from elsewhere than the source. However the trick was \"poorly done\" and the girl could not control it so as to produce any spirit communication.\n\nGoing on to reveal other fraudulent practices, Page addressed the relationship at work between performer and audience by which both functioned as perpetrators:\nPage's efforts to expose these frauds at their human roots stems in part from his keen concern for furthering the public understanding of science and their proficient use of its findings and benefits. In this undertaking, Page allied with contemporary Michael Faraday and other scientists throughout history who have sought to debunk the unscrupulous applications of pseudoscience upon a willing and gullible public.\n\nAs with the challenge to spiritualism described above, Page's scientific undertakings brought him into public arenas where politics and controversy held sway. Eloquent, combative, keen-minded and persistent, Page made his commitments known. Increasingly, Page's self-chosen and sometimes self-serving commitments diverged from the norms of behavior sanctioned by society and the elitism of the emerging professionalist trend in science. The resulting tarnish to Page's reputation impacted him during his lifetime and contributed to the longstanding historical neglect of his scientific work and personal story, thereby reducing general understanding of the complexity of the American experience in science.\n\nA tension early to arise in his career as patent examiner was that of the conflict of interest between the privileged information he had regarding applicants' patents, and his private consulting with particular inventors on the side. Following his appearance in the 1848 Morse v O'Reilly lawsuit over the telegraph, Page took a more careful stance in his role as patent examiner. Thereafter, he refrained from transmitting such privileged information to rival patent applicants.\n\nHowever, the well-paid public post of patent examiner put the occupants continually under scrutiny by politicians, scientists, and aspiring inventors. Both Congress and the executive branch exerted control and influence over policy and practices in the patent office.\n\nIn the early years of the United States Patent and Trademark Office, a patent examiner was expected to be highly trained, knowledgeable in all the sciences, informed on current and past technology. Page was an exemplar of this ideal.\n\nAs Page continued in the job, the number of patents submitted to the agency increased sharply, while the number of patents granted was the same or less, and the number of patent examiners was unchanged. Inventors seeking patents, becoming incensed about decisions made against them, coalesced into a lobby with a voice projected through the journal \"Scientific American\". This lobby advocated \"liberalization\" — more leniency in the granting of patents, giving the inventor the \"benefit of the doubt\"— and argued against the scientific research being sponsored by the Smithsonian Institution under Joseph Henry.\n\nHenry took a hard line, decrying inventors' \"futile attempts to innovate and improve\". The elite professionalized science that Henry was building up through the Smithsonian and other organizations treated as low status the having or seeking of a patent; patents were not considered a contribution to science. While Page set out to show that gaining patents was genuine scientific work, he fell out of favor with the scientific establishment. His friendship with Henry petered out, and Page was no longer held in high regard as part of elite science.\n\nPage shifted in his position on the granting of patents. As an examiner of patents, he was scrupulous and fair. Through his own experience as an inventor and association with other inventors, he allied with their concerns. On his resignation from the patent agency, Page used the journal he founded and edited as a forum to critique and even lambast the agency and policies which he had upheld for 10 years prior. \nFollowing the example of Samuel Morse, who developed the telegraph to commercial viability through assistance from federal government funds, Page sought a similar level of support for his electromagnetically powered locomotive. He found a political ally in Thomas Hart Benton, senator from Missouri. Benton's passionate rhetoric on behalf of Page's vision was instrumental in securing unanimous support for a Senate allocation of $20,000 to fund Page's project through the Department of Navy. By the end of that year (1849), Page reported to the Navy that he was collaborating on the project with a mechanic, Ari Davis, the brother of Daniel Davis Jr., but had nothing yet to show. In print, inventor Thomas Davenport (inventor) challenged the expenditure of public funds on Page's project, claiming that motors he had already invented and built were equal to the task. Page defused that objection by publishing a statement about his unique device.\n\nMore troubles ensued for the project. Running low on cash, Page asked for more. Speaking in the Senate in the summer of 1850, Benton presented Page's attainment of a force an order of magnitude greater than what the same battery had output under his initial trials. Benton upped the stakes by requesting funds for Page to develop an electromagnetically powered ship of war. This second petition met with serious opposition in the Senate. Senator Henry Stuart Foote countered that Page had not proved substantial progress or benefits from his work. Senator Jefferson Finis Davis objected to the appropriation of government funds to one inventor, while other inventors such as Thomas Davenport went unsupported. Both the US Senate and House nixed any further funds for Page's project. In order to prepare the locomotive for its 1851 trial run, Page went over $6000 into debt. In the wake of the failed public test of this locomotive, Page faced a critical press. Gaining no assistance from the world of finance, he emerged from the debacle in \"desperate straights, financially and emotionally\".\n\nThe American Civil War wreaked a further devastating impact on Page's scientific work and legacy. In 1863, Union soldiers stationed in the area of Page's home, broke into his laboratory as a random, unprovoked act of violence. His equipment, inventions and laboratory notebooks were destroyed. Some other inventions by Page which he had donated to the Smithsonian Institution were destroyed by a fire there in 1865. As a result of these destructive events, very few of Page's handmade devices exist today. With little remaining of his experimental work and notes, Page's many contributions have slipped from the view of most historians.\n\nSuffering debt, terminal illness and isolation from the mainstream scientific community by his last years, Page contrived one final effort to secure credit and status for his achievements. In 1867, he petitioned the United States Congress for a retrospective patent on his inventions of the late 1830s: the spiral conductor, the circuit breakers, the double helical coil. The granting of such a patent transgressed such policies as that an invention in widespread public use for decades can not be patented, and that an employee of the Patent office can not hold a patent. Page circumvented these policies by appealing to nationalism. To support his argument, he published anonymously a lengthy, closely researched yet self-promoting book titled \"The American Claim to the Induction Coil and its electrostatic Developments\"(1867b).\n\nBy the 1860s, the induction coil was becoming a prominent instrument of physics research. Instrument-makers in America, Great Britain and the European continent contributed in developing the construction and operation of induction coils. Premiere among these instrument makers was Heinrich Daniel Ruhmkorff, who in 1864 received from Emperor Napoleon III the prestigious Volta Prize along with a 50,000 franc award for his 'invention' of the induction coil. Page maintained that the devices he developed in the 1830s were not markedly different from the induction coil and that other American inventors had filled in with improvements that were better than anything made by Ruhmkorff — and alleging that Ruhmkorff had plagiarized the coil of another American instrument-maker, Edward Samuel Ritchie.\n\nA special act passed by the U.S. House and Senate, and signed by President Andrew Johnson authorized what was later dubbed \"The Page Patent\". Page died a few weeks later, in May 1868. Instead of dying with him, the Page patent went on to play a major role in the politics and economics of the telegraph industry. Page's lawyer and heirs successfully argued that the patent covered the mechanisms involved in \"all known forms of telegraphy\". An interest in the patent was sold to the Western Union Co; together Western Union and the Page heirs reaped lucrative benefits. Page's patent secured a life 'in style' for his widow and heirs. Although he was no longer living, it figured as yet another violation, on his part, of the behavior code under the emerging professionalization of science of the day, under which science was to be conducted for its own sake, without accruing apparent political or financial gain.\n\n\n\n\n"}
{"id": "9210947", "url": "https://en.wikipedia.org/wiki?curid=9210947", "title": "Conjugated fatty acid", "text": "Conjugated fatty acid\n\nConjugated fatty acids are polyunsaturated fatty acids in which at least one pair of double bonds are separated by only one single bond. An example of a conjugated fatty acid is the conjugated linoleic acid. Conjugated fatty acids may confer health benefits ranging from the prevention of hypertension to protection against certain forms of cancer, although more research is needed to confirm such effects.\n\nThe conjugation results in delocalization of electrons along the double-bonded carbons.\n"}
{"id": "23432134", "url": "https://en.wikipedia.org/wiki?curid=23432134", "title": "David Adler Lectureship Award in the Field of Materials Physics", "text": "David Adler Lectureship Award in the Field of Materials Physics\n\nThe David Adler Lectureship Award in the Field of Materials Physics is a prize that has been awarded annually by the American Physical Society since 1988. The recipient is chosen for \"an outstanding contributor to the field of materials physics, who is noted for the quality of his/her research, review articles and lecturing.\" The prize is named after physicist David Adler with contributions to the endowment by friends of David Adler and Energy Conversion Devices, Inc. The prize includes a $5,000 honorarium.\n\nSource: American Physical Society\n\n"}
{"id": "46411077", "url": "https://en.wikipedia.org/wiki?curid=46411077", "title": "Detonator crimping pliers", "text": "Detonator crimping pliers\n\nDetonator crimping pliers, cap crimping pliers or detonator crimping tool is a common tool when working with explosives. The construction is very similar to the construction of any other multi-tool or pliers. The specific feature is the presence of one groove on each of the opposing jaws. When the jaws are open, the groves will appear as a semi circle with an elliptical extension. When fully closed the two grooves will form a cylindrical shape.\n\nWhen a detonator is delivered, it is sometimes delivered as a piece of metal with nothing connected to it. To function, the detonator must be mated with the correct initiation source. The correct initiation source could for example be tar-fuse for time delay, or a Shock tube detonator to name a few.\n\nThe rear part of a detonator is usually made of a rigid but malleable aluminium alloy, sometimes supported by a relatively soft brass or copper structure to aid in reliably giving a strong and watertight seal without damaging the initiation source. A pair of detonator crimping pliers mate the detonator with the initiation source by allowing the detonator's back end to be inserted into the large hole present when the pliers is in the non-compressed position. When force is applied, the jaws will move closer to each other crimping the neck of the detonator. This shows that the function is just a simple crimping tool, adapted for one specific item.\n\nEach type of detonators have an instruction/information folder written by the manufacturer or the distributor. In it are clear instructions on how to crimp that specific type/model. Most detonators contain relatively sensitive explosive compound meant to initiate the staged detonation train. Crimping a detonator in the wrong place or with improper tools may cause it to detonate.\n"}
{"id": "458042", "url": "https://en.wikipedia.org/wiki?curid=458042", "title": "Dovetail joint", "text": "Dovetail joint\n\nA dovetail joint or simply dovetail is a joinery technique most commonly used in woodworking joinery (carpentry) including furniture, cabinets. log buildings and traditional timber framing. Noted for its resistance to being pulled apart (tensile strength), the dovetail joint is commonly used to join the sides of a drawer to the front.\nA series of 'pins' cut to extend from the end of one board interlock with a series of 'tails' cut into the end of another board. The pins and tails have a trapezoidal shape. Once glued, a wooden dovetail joint requires no mechanical fasteners.\n\nThe Dovetail joint technique probably pre-dates written history. Some of the earliest known examples of the dovetail joint are in ancient Egyptian furniture entombed with mummies dating from First Dynasty, as well as the tombs of Chinese emperors. The dovetail design is an important method of distinguishing various periods of furniture.\n\nIn Europe the dovetail joint is sometimes called a swallowtail joint, a culvertail joint, or a fantail joint.\n\nThe dovetail joint is very strong because of the way the 'tails' and 'pins' are shaped. This makes it difficult to pull the joint apart and virtually impossible when glue is added. This type of joint is used in box constructions such as drawers, jewellery boxes, cabinets and other pieces of furniture where strength is required. It is a difficult joint to make manually, requiring skilled workmanship. There are different types of dovetail joints. The joint is strong especially when used with glue.\n\nThe angle of slope varies according to the wood used, purpose of joint and type of work. Typically the slope is 1:6 for softwoods and a shallower 1:8 slope for hardwoods. Often a slope of 1:7 is used as a compromise. However, a different slope does not affect the strength of the joint in different types of wood.\n\nThe image at the top of this page shows a 'through dovetail' (also known as 'plain dovetail') joint, where the end grain of both boards is visible when the joint is assembled. Through dovetails are common in carcass and box construction. Traditionally, the dovetails would have often been covered by a veneer. However, dovetails have become a signature of craftsmanship and are generally considered a feature, so they are rarely concealed in contemporary work. When used in drawer construction, a through (or blind, mitred, or lapped) dovetail joint is sometimes referred to as an \"English dovetail.\"\n\nCraftsmen use a 'half-blind dovetail' when they do not want the end grain visible from the front of the joint. The tails fit into mortises in the ends of the board that is the front of the item, hiding their ends.\n\nHalf-blind dovetails are commonly used to fasten drawer fronts to drawer sides. This is an alternative to the practice of attaching false fronts to drawers constructed using through dovetails.\n\nThe 'secret mitred dovetail' joint (also called a 'mitred blind dovetail', 'full-blind dovetail', or 'full-blind mitred dovetail') is used in the highest class of cabinet and box work. It offers the strength found in the dovetail joint but is totally hidden from both outside faces by forming the outer edge to meet at a 45-degree angle while hiding the dovetails internally within the joint.\n\nThe mitred corner dovetail joint is very similar in design, but it has just a single dovetail and is used for picture frames and other similar joins.\n\nThe secret double-lapped dovetail is similar to the secret mitred dovetail, but presents a very thin section of end grain on one edge of the joint. Used for carcass and box construction to hide the dovetails completely from view.\n\nThe sliding dovetail is a method of joining two boards at right angles, where the intersection occurs within the field of one of the boards, that is not at the end. This joint provides the interlocking strength of a dovetail. Sliding dovetails are assembled by sliding the tail into the socket. It is common to slightly taper the socket, making it slightly tighter towards the rear of the joint, so that the two components can be slid together easily but the joint becomes tighter as the finished position is reached. Another method to implement a tapered sliding dovetail would be to taper the tail instead of the socket. When used in drawer construction, a \"stopped sliding dovetail\" that does not extend across the full width of the board is sometimes referred to as a \"French dovetail\".\n\nUsed for:\n\nDovetails are most commonly, but not exclusively, used in woodworking. Other areas of use are:\n\n"}
{"id": "53977963", "url": "https://en.wikipedia.org/wiki?curid=53977963", "title": "Dynamical dimensional reduction", "text": "Dynamical dimensional reduction\n\nDynamical dimensional reduction or spontaneous dimensional reduction is the apparent reduction in the number of spacetime dimensions as a function of the distance scale, or conversely the energy scale, with which spacetime is probed. At least within the current level of experimental precision, our universe has three dimensions of space and one of time. However, the idea that the number of dimensions may increase at extremely small length scales was first proposed more than a century ago, and is now fairly commonplace in theoretical physics. Contrary to this, a number of recent results in quantum gravity suggest the opposite behavior, a dynamical reduction of the number of spacetime dimensions at small length scales. \nThe phenomenon of dimensional reduction has now been reported in a number of different approaches to quantum gravity. String theory, causal dynamical triangulations, renormalization group approaches, noncommutative geometry, loop quantum gravity and Horava-Lifshitz gravity all find that the dimensionality of spacetime appears to decrease from approximately 4 on large distance scales to approximately 2 on small distance scales. \n\nThe evidence for dimensional reduction has come mainly, although not exclusively, from calculations of the spectral dimension. The spectral dimension is a measure of the effective dimension of a manifold at different resolution scales. Early numerical simulations within the causal dynamical triangulation (CDT) approach to quantum gravity found a spectral dimension of 4.02 ± 0.10 at large distances and 1.80 ± 0.25 at small distances. This result created significant interest in dimensional reduction within the quantum gravity community. A more recent study of the same point in the parameter space of CDT found consistent results, namely 4.05 ± 0.17 at large distances and 1.97 ± 0.27 at small distances.\n\nCurrently, there is no consensus on the correct theoretical explanation for the mechanism of dimensional reduction. \n\nThe ubiquity and consistency of dimensional reduction in quantum gravity has driven the search for a theoretical understanding of this phenomenon. Currently, there exist few proposed explanations for the observation of dimensional reduction. \n\nOne proposal is that of scale invariance. There is growing evidence that gravity may be nonperturbatively renormalizable as described by the asymptotic safety program, which requires the existence of a non-Gaussian fixed point at high energies towards which the couplings defining the theory flow. At such a fixed point gravity must be scale invariant, and hence Newton's constant must be dimensionless. Only in 2-dimensional spacetime is Newton's constant dimensionless, and so in this scenario going to higher energies and hence flowing towards the fixed point should correspond to the dimensionality of spacetime reducing to the value 2. This explanation is not entirely satisfying as it does not explain why such a fixed point should exist in the first place.\n\nA second possible explanation for dimensional reduction is that of asymptotic silence. General relativity exhibits so-called asymptotic silence in the vicinity of a spacelike singularity, which is the narrowing or focusing of light cones close to the Planck scale leading to a causal decoupling of nearby spacetime points. In this scenario, each point has a preferred spatial direction, and geodesics see a reduced (1 + 1)-dimensional spacetime.\n\nDimensional reduction implies a deformation or violation of Lorentz invariance and typically predicts an energy dependent speed of light. Given such radical consequences, an alternative proposal is that dimensional reduction should not be taken literally, but should instead be viewed as a hint of new Planck scale physics.\n"}
{"id": "21079741", "url": "https://en.wikipedia.org/wiki?curid=21079741", "title": "Electric vehicle network", "text": "Electric vehicle network\n\nAn electric vehicle network is an infrastructure system of charging stations and battery swap station to recharge electric vehicles. Many government, car manufacturers, and charging infrastructure providers sought to create networks. , Estonia remained the only country to have completed a nationwide public electric charging network. the largest fast-charging location was in Shanghai on the Tesla Supercharger network, with fifty charging stalls.\n\nThe Go Electric Stations is a global station navigation system. The project currently includes free smartphone apps via Next Charge and provides free data and information to users and providers regarding stations all around the world. Station activation from the tap of a smartphone and real time availability information can be discovered along with a detailed payment option system and navigational routing maps. The website also offers an exclusive PlugShare program which allows users to share their plug with fellow EV owners.\n\nThe PlugSurfing community is a community based charging station locator. PlugSurfing is merging static and realtime availability charging station information and crowd sources information through mobile apps and other devices. This way PlugSurfing responds to the needs of the electric driver.\n\nThe OpenChargeMap project is an open source database with a public API for sharing and distributing charging location information globally. Information in this system is gathered both manually and automatically from a variety of data sources. The project aims to provide globally relevant data freely to other application developers and navigation providers.\n\nThe EV Charger Maps website is a volunteer run effort coordinated by EV Charger News that catalogs EV charging station information across the U.S. It contains information targeted for real-world use by electric vehicle owners.\n\nThe LEMnet internet database is operated by Park & Charge. The database not only provides locations of the Park & Charge stations but also everyone is free to register their stations. As of August 2011 the database lists more than 3100 active charging locations in Europe. This includes some of the 190 locations (March 2011) of the Drehstromnetz initiative which is specialized on privately owned 400 V 3-phase charging stations (German \"Drehstromnetz\" means \"3-phase network\") and the 200 (May 2010) charging stations of the RWE Mobility infrastructure which also offers 400 V 3-phase charging. The LEMnet database does not include all of the 2500 charging stations (March 2010) listed in the ElektroTankstellen internet database operated by the Austrian \"Eurosolar\" initiative even that there is some overlap in data. All of them offer KMZ map files and POI collection files for navigation systems enhanced with information required for electric vehicle owners to find a public charging point in Europe.\n\nIn Spain Alargador offers an editable map of EV recharge points in which everybody who wants to share his electric outlet can do it easily. All data is downloadable freely to most common GPS navigators and mapping software. Another source is ElectroMaps to find a station in Spain.\n\nPOD Point provides one of the UK's largest public charging networks with over 1,500 stations. The live charging station map provides the address, charging rate, connector type and availability of public charging stations. In 2015, they launched the Open Charge public network with smartphone-enabled charging points that are reliable and easy to use.\n\nPlugShare is a crowdsourced map of public, private and residential charging locations. The site uses Google Maps to provide a map of charging locations and their own database to filter by charging type. Public chargers, private chargers, and residential charging locations are listed. The service provides an app for iOS and Android which allows users to locate chargers near their current location. An account is needed to view private persons' charging locations, as these locations are located at the homes or businesses of Plugshare members.\n\nSun Country Highway includes a map of 1000 of its chargers across Canada and the US.\n\n\n\nThe Renault–Nissan Alliance has made agreements to promote emission-free mobility in France, Israel, Portugal, Denmark and the U.S. state of Tennessee.\nNissan plans to install 200-volt level 2 charging stations at 2,200 Nissan dealers in Japan, and level 3 fast charging stations at 200 dealers.\n\nTesla Motors, in March 2009, announced that they are \"working with a government-affiliated partner to set up battery changing stations at various locations\" to service their Model S platform cars.\nThe first were unveiled September 24, 2012\nAs of 17 April 2016, Tesla currently operates 3,644 superchargers in 616 stations worldwide, and Tesla announced on 31 March 2016 plans to double the size of the Supercharger network by 2017. Superchargers are a free service for Tesla Model S and Model X owners.\n\nAustralia currently has thirteen electric vehicle charging stations across Sydney, Melbourne and Canberra from Coulomb Technologies. They opened in 2010 and 2011. One charge point from ECOtality has been installed in the car park at 140 William Street in Melbourne CBD with Exigency providing project management and metering. ChargePoint has expanded its service to eight cities by 2012 (Perth 3, Adelaide 5, Melbourne 10, Canberra 2, Sydney 8, Brisbane 6, Townsville 3, Hobart 1).\n\nConstruction of infrastructure (charging spots and battery switching stations) had been proposed by Better Place for the major cities Melbourne, Sydney and Brisbane.\nAustralia would have become the third country in the world to have an electric car network in a bid to run the country's 15 million cars on batteries powered by green energy under a plan announced in October 2008. Better Place filed for bankruptcy in Israel on May 25, shortly after pulling out of Australia.\nThe original plan to deploy as much as 200,000 charging stations was stopped in January 2013, after just 20 public charge spots had been installed.\n\nIn May 2011 CO2 Smart has completed a fast recharge network in test city Perth. Electromotive has provided 11 dual-headed IEC-compatible fast-charge stations at 32A to be used with the CO2 Smart of the test fleet. In the test drive the European connectors have been preferred over the American connectors since Australia (like Europe) does have three-phase power (at 415 V) in most home locations. The fast-charge outlets connect with a special 8-pin IEC-compatible round connector integrating single-phase and three-phase power The project of the University of Western Australia was continued with 23 public charging stations available by September 2012 featuring Type 2 connectors at 32 A.\n\nUnited States EV manufacturer Tesla Motors formally launched in Australia in December 2014, announcing their intention to build their supercharger network along the highway between Melbourne, Canberra and Sydney by the end of 2015, and extending to Brisbane by the end of 2016.\n\nIn 2012, a series of free public electric vehicle charging stations were installed along the main route of the Trans-Canada Highway by a private company, Sun Country Highway, permitting electric vehicle travel across the entire length, as demonstrated by the company's president in a publicity trip in a Tesla Roadster. this made it the longest electric vehicle ready highway in the world. The same company also partnered with Canadian rural hardware retailer Peavey Mart to add free public charging stations to its 29 stores across Western Canada and includes chargers located at Best Western hotels in Canada and the US on its online map of EV charging stations. the company's total network was over 700 chargers with plans to reach 1000 chargers by year end.\n\nFrom 2011 to 2014, the City of Vancouver installed \npublicly accessible Level 2 charging stations in a variety of locations, including community centres, shopping malls, curbside, and other locations throughout the city.\nIn 2008, the city changed the Building Bylaw to require 20% of parking stalls in apartments and condos, and all stalls in houses to be electric vehicle ready.\nIn 2013, the bylaw was updated so that 10% of stalls in mixed-use and commercial buildings are also ready for electric vehicles.\n\nIn a March 2016 news release,\nthe Government of British Columbia stated that the \nCEV Program\ninvestments have supported over 550 public Level 2 charging stations, and 30 DC fast charging stations.\n\nAs of June 2016, Brazil had about 90-100 charging stations.\n\nChina's first large electric charging station for electric vehicles—the Tangshan Nanhu EV Charging Station – was put into service on March 31, 2010. Five cities in northern Hebei province – Tangshan, Zhangjiakou, Qinhuangdao, Langfang and Chengde – want to build three charging stations and 100 charging poles in 2010.\n\nShandong is the province with most car manufacturers in China. The province planned to start in May 2010 with a charging station for 45 cars. According to China's State Grid Corporation, 75 electric vehicle charging stations are planned in 27 cities across China by the end of 2010. Additionally 6,209 charging posts and some battery replacement stations had been planned for 2010. The State Grid Corporation China announced success in distributing 7,031 charge poles in Hangzhou-Jinhua and wants to add 211 additional charging poles in 2011 along with 173 charging stations.\n\nChina is planning on installing 10 million electric vehicle charging stations by 2020. In the 12th Five Year Plan (2011–2015) China wants to deploy 2,351 charge and replacement power stations and 220,000 charge spots. The reason is to get rid of crude oil imports which makes for 54 percent of the oil usage and cars accounted for 40 percent of national oil consumption (2010).\n\nThe AVERE / European Association for Battery, Hybrid and Fuel Cell Electric Vehicles was founded back in 1978 and is a member of the World Electric Vehicle Association. AVERE is also the parent organization of CITELEC / Association of European Cities interested in Electric Vehicles and Eurelectric. The European Commission has funded the \"Green Cars Initiative\" since November 2008. In March 2011, the European Commission along with forty two partners from the industries, utilities, electric car manufacturers, municipalities, universities and technology and research institutions founded the \"Green eMotion\" initiative funded with €41.8 million under the Seventh Research and Development Framework Programme. The defined goal is to provide an interoperable electromobility framework to align the ongoing regional and national electromobility initiatives. At the same time the partners unveiled the \"Transport 2050\" plan which includes the aim to half the number of conventionally fuelled cars in cities by 2030 and phase them out by 2050.\n\nIn the second position paper (March 2011) of the European Automobile Manufacturers Association it is recommended to equip public charging stations with IEC 62196 Type 2 Mode 3 connectors with transitional solutions to be allowed up to 2017. Nevertheless, multiple socket types (IEC 60309-2 Mode 2 types, IEC 62196 Mode 3 types, Chademo and standard home socket outlets Mode 2) have been deployed already. Politics have called for single European-wide standard and in case of a market failure the EU will define the infrastructure side requirements by law in 2013. As expected from lobbying the European Commission has proposed in January 2013 to only use the Type 2 connector type as the single standard to end prior uncertainty about the charging station equipment in Europe. Common standards for electric charging points across Europe must be designed and implemented by December 2015.\n\nPower supplier ČEZ has announced to have 50 recharging stations to be ready by the end of 2011. By June 2012 the company had 14 public and 6 private charging stations installed with more to come in Mlada Boleslav at the Skoda facilities. These charging stations use a combination of 230 V mains connector (Type E) at 16 A and a 400 V three-phase Mennekes connector (Type 2) at 16 A or 32 A.\n\nThere are 2 major charge point operators in Denmark, E.on are operating mostly fast chargers, only installing rapids at freeways, while Clever is installing both fast and rapids in city centers.\n\nBoth E.on and Clever is taking part in installing rapid chargers at freeway lay-bys, with Clever installing 4 of them and E.on a total of 20 at 10 different locations.\n\nBesides E.on and Clever, local energy companies are installing free-to-use charge points, often only consisting of a CEE plug, so the users have to bring their own EVSE box.\n\nInfrastructure was planned by Better Place and has been installed by Coulomb Technologies for Copenhagen. Denmark has enacted policies that create a tax differential between zero-emission vehicles and traditional cars to accelerate the transition to electric cars. Better Place had announced the network to be complete in December 2012, however the stations and chargers have been switched off due to the bankruptcy of Better Place Danmark A/S in June 2013. By April 2013 the network had consisted of 700 public charging spots, 18 battery switch stations and 8 fast charger stations.\n\nIn 2013 E.on bought the charge points from Better Place and restarted the network, without the battery swap system.\n\nNorway has a tradition in building electric vehicles based on the Think Car. It is popular in Southern Norway (Oslo), Southern Sweden (Gothenburg) and Eastern Denmark (Copenhagen). The concept of the \"Move About\" project will provide 60 new Think cars in a test including charging stations in 50 towns in the area until 2013. The MoveAbout concept is actually derived from a car sharing system where cars are not offered for purchase but for leasing.\n\nEstonia became the first country to complete the deployment of a nationwide electric car charging network, and , is the only country with such geographical coverage. The Estonian network has the highest concentration of DC chargers in Europe.\n\nThe Estonian government and Kredex launched the charging station network project in 2011 in cooperation with ABB, funded partially by the Mitsubishi Corporation. The nationwide electric car charging network officially opened with 165 fast chargers on 20 February 2013. These chargers were installed in all urban settlements with more than 5,000 inhabitants. In addition, chargers are installed on the all major roads at intervals of no more than . That makes it possible to reach every point within the country without a supply interruption. All of the \"Terra 51\" CHAdeMO-type DC chargers are fast-charging, only needing between 15 and 30 minutes to fully charge a car's battery.\n\nIn France, Électricité de France (EDF) and Toyota are installing recharging points for PHEVs on roads, streets and parking lots.\n\nThe Renault–Nissan Alliance and the largest French electric utility, Electricite de France (EDF) have signed an agreement to promote emission-free mobility in France. The move aims at offering all-electric volume vehicles from 2011 — including a countrywide network of battery charging stations. The partner Vinci Autoroutes has announced to rebuild 738 car parks along motorways with at least 5 parking lots for charging electric vehicles – construction will start at the end of 2011 and the full extent will be reached in 2013.\n\nThe Environment Ministry of France, led by Jean-Louis Borloo has announced the goal to install 400,000 charging points in France by 2015. Jean-Louis Borloo has assigned 1.5 billion Euros in 2009 to support research and preparations for the first part of the electric vehicle network with 75,000 charging stations.\n\nMeanwhile, the pilot project in Paris has started with the introduction of 100 Z.E. cars. The map of charging stations can be downloaded from the city website. There are 101 locations with 178 charging points across the town and its suburbs (May 2010). The charging points have either Schuko-like sockets (Type E / 2P+T) or a Marechal plug on spiral cord where both variants are rated at 230 V/16 A (mains).\n\nSchneider Electric supports test drives in France with its charging stations that include a Type 3 (EV Plug Alliance) connector. In Strasbourg 100 Toyota Prius were tested with 135 recharging spots beginning Q1 2010 (Type 3 single-phase). In the suburbs of Paris there will be 300 recharging spots to be installed in Q1 2011. In the \"Projet Klébér\" the Strasbourg vehicle fleet may use the charging stations of EnBW in Mannheim, Karlsruhe, Stuttgart and vice versa. In Yvelines near Paris the fleet test SAVE (Seine Aval Véhicule Électrique) was started in April 2011 – until September 2011 a number of 200 charging stations will be built.\n\nThe Monaco government has sketched a plan to run a fleet test in 2011 including 300 charging stations and 3 fast-charge stations.\n\nThe Renault–Nissan group – including EDF – has enlarged its scope with partnering to the Italian utility Enel and Spanish utility Endesa in March 2010. Renault–Nissan offers a broader range by providing 60 all-electric vehicles – the Kangoo Express Z.E. and the Renault Fluence Z.E – to the new pilot project \"E-Moving\" in Italy. The project will start to install 270 charge points in the Lombardy region (including the cities of Milan and Brescia) up to June 2010. This \"E-Moving\" network will contain 150 public charging stations to be put up until the end of 2010.\n\nThe Italian Enel company had an early agreement with Daimler to run a test with their Smart line of cars. Enel has started the \"emobility Italy\" program in cooperation with Daimler in 2008 – this program will put up 400 public and private charging stations in Rome, Milan and Pisa with charging stations to be built since September 2010. The project was supposed to start in 2011 with a test run going for 48 months – since 5. April 2012 the Smart drive E-Mobility program is ready. The 100 public charging stations in Rome are built with a three-phase Type 2 Mennekes connector while the additional 50 home charging stations are built with a single-phase Type 3 Scame connector.\n\nThe E-Move charging stations around Bolzano allow for a test drive of connecting solar panels directly to light vehicles for charging.\n\nThe Zero Emission City Parma is a regional project with a 9 million Euro funding to create 300 charging stations along with 900 electric vehicles until 2015. The project is expected to go fully operational by the end of 2012, with 300 points of charge installed and 400/450 electric vehicles circulating.\n\nGermany has four major transmission system operators (50Hertz, Amprion, TenneT, TransnetBW). They try to set themselves into the position to sell electricity power to electric vehicle owners by becoming also the operators of the upcoming electric vehicle networks. To that avail, they offered partnerships to the German car makers, where they provided charging stations for field tests.\n\nCarmaker Daimler AG and utility RWE AG are running a joint electric car and charging station test project in the German capital, Berlin, called \"E-Mobility Berlin.\". They have set up 60 charging stations in Berlin (September 2009) and are in the process of extending the system to include 500 charging stations. Daimler has provided for 100 Smart electric drive cars to the project. The second phase started in November 2010. The RWE subsidiary \"RWE Mobility\" has created cooperations with the automobilist club ADAC, car rental service Sixt and car park provider APCOA to equip all locations with charging stations. since mid of 2009. Renault joined the RWE Mobility program in September 2009 whereby the project goals of erecting charging stations were enlarged to mid of 2011 Renault's partner Nissan has joined the RWE-mobility program on 21. June 2010 announcing that RWE will create a network of 1000 charging stations until the end of the year 2010 focusing on the Berlin and Rhein-Ruhr region. On 28. August a cooperation with fuel retailer PKN Orlen (owning 2700 gas stations in Poland, Czech Republic and Germany) was announced – they are starting to equip 30 gas stations in Hamburg with charging points for electric vehicles. The current list of RWE-mobility charging stations contains 500 locations in Germany, 50 locations in the Netherlands, 11 in Poland and Austria plus a few stations in other neighbouring countries – also RWE has switched all of its charging stations to Type 2 sockets.\n\nCarmaker BMW and utility Vattenfall run a joint electric car and charging test project called \"MINI E\" in the German capital, Berlin. They are in the process of erecting 50 charging stations and the project lends 50 BMW Mini cars to citizens. The project started in June 2009 and a second phase has been started in December 2009. Up to June 2011 there were 42 public charge points by Vattenfall in Berlin and the company is in the process of building 50 public charge points in Hamburg. While the earliest charging stations were using CEEplus sockets the newer charging stations are built to Type 2 Mode 3 sockets.\n\nCarmaker VW and utility E.ON run a joint electric car and charging station test project in the German capital, Berlin and in Wolfsburg. The \"Electric Mobility Fleet Test\" was started as a research project with mostly partners in German universities using the VW hybrid cars (to be tested in 2010). E.ON has later joined also in the MINI E project providing the infrastructure in Munich which was started in Juli 2009. erecting an initial series of 11 charging stations (May 2010) enlarging it continuously (21 locations in December 2010). The region test in Munich has been extended by BMW i3 and BMW i8 prototypes (project i) as well as Audi e-tron models (project eflott) in 2011. E.ON has announced to provide the eflott project with 200 public charging stations the Munich region.\n\nCarmaker Daimler, the utility EnBW and the government of Baden-Württemberg have announced on 18. June 2010 to enlarge the \"Landesinitiative Elektromobilität\" program with the \"e-mobility Baden-Württemberg\" project that includes erecting 700 charging stations in the state until the end of 2011. Additionally there will be 200 electric vehicles added to the test including some electric trucks. The government of Baden-Württemberg has assigned 28.5 million Euros to support EV research up to 2014. Meanwhile, EnBW has sponsored 500 E-Bikes in the Elektronauten project in 2010 which can use 13 charging stations in the Stuttgart region. EnBW has claimed to offer 250 charging stations for the Elektronauten 500 project in May 2011 although the map has not been updated. Bosch has developed a new charging station type for EnBW that is capable for 63A – the station was certified on 11. April 2011 by DEKRA and EnBW has announced to install 260 charge stations in the following weeks for MeRegioMobil project in Stuttgart and Karlsruhe. In November 2011 the car2go project announced to go to Stuttgart in 2012 – EnBW reassured to have 500 charging spots ready in time with the roll out of the car2go vehicles in the second half of 2012.\n\nThe German government has announced to support a fleet of 1 million electric cars in Germany by 2020. There are 500 million Euros assigned to the Federal Ministry of Economics and Technology (Germany) to support research and pilot projects in Germany. The ministry has created a dedicated coordination office in the \"Gemeinsame Geschäftsstelle Elektromobilität der Bundesregierung (GGEMO)\" (\"Joint Agency for Electric Mobility (of the Federal Government)\") which was opened in February 2010. The GGEMO has coordinated a partnership program with the German car industry named \"Nationale Plattform Elektromobilität (NPE)\" inaugurated on May 3, 2010, in the German Chancellery.\n\nThe NPE partnership is supposed to detail the plans for network evolution. The technical standardization part is mostly concentrated in the Deutsche Kommission Elektrotechnik (DKE) of the Association for Electrical, Electronic and Information Technologies (VDE) – the \"Standardization Overview on E-Mobility\" shows a wide range of efforts from electric grid management to the charging station infrastructure to the car charger electronics. The NPE partnership has published an interim report on November 30, 2010, showing a test fleet of 2800 electric vehicles and 2500 charging stations in 8 test regions.\n\nThe German government has announced that it will not install a rebate system for the introduction of electric cars but that it will reshape the legal provisions to quickly create a charging station network in Germany. Bernd Pischetsrieder (formerly Volskwagen) points to studies saying that most of the current buyers of electric cars did already own multiple cars so that a rebate plan would merely come out as a subvention of a consumer class that can afford the expense anyway.\n\nThe VDE E-mobility congress on the subject was held in Leipzig on 8./9. November 2010. During the congress a large consumer study was precented that showed some 64 percent want to buy an electric car. The study did also look at the requirements to the charging process – 51 percent of consumers in Germany expect a car to be charged in less than 2 hours, up to 4 hours is acceptable to 60 percent of consumers. 64 percent of consumers expect to charge in their own garage, 21 percent want to frequent a central charging station while casual charging in parking lots of shops and company grounds is expected by a mere 6 and 4 percent respectively. The maximum travel distance shows mixed results – while 53 percent say that 300 km is enough there are also 31 percent who like to travel 450 to 1000 km until required to recharge.\n\nThe interim report of the NPE partnership classifies electric vehicles in 3 categories, all-electric city cars, family cars and light trucks with an electric range for city transport. Development is sketched in phases 2010-2013, 2014–2017, 2018–2020 and post-2020 with the government goal to get 1 million electric cars up to 2020 and 6 million electric cars up to 2030 (for comparison there are 44 million cars in Germany in 2010). Batteries are not expected to show great advancements in terms of capacity but the safety will increase and the prices will fall to 250-300 €/kWh in the 2018-2020 time frame. In the post-2020 time frame new battery types are expected – instead of lithium-ion the fourth generation batteries will be introduced to the mobility market including lithium-air, lithium-sulfur and zinc-air batteries. As for charging stations a wide network of fast-charging points is considered possible with 22 kW (400 V 32 A) stations to be introduced in 2010-2013 and 44 kW (400 V 63 A) stations to be introduced in 2014-2017. For the time beyond 2020 there is an expectation of charging stations at 60 kW (400 V DC 150 A) allowing to charge the standard 20kWh battery pack to 80% in less than 10 minutes whereas this station type requires integration with smart grid technology and a strict worldwide standard (including SAE procedures). The \"early adoptors\" of electric vehicles are identified to be from the middle class owning multiple cars as well as owning a garage – the existence of a public network of charging stations is considered to be not (sic!) a prerequisite for market introduction in the first phases. Instead government funds should back the investments in privately owned charging stations for example with faster tax write off and cheap credits from the government KfW bank.\n\nA preliminary review of the Mercedes / RWE test drive in the smart ed project shows the importance of vehicle-to-grid communications in charging stations as an incentive to charge at night times. While most US households own a garage even for small cars the situation is different in Central Europe where public charging stations are needed.\n\nThe charging station network in Switzerland is derived from research in solar cars. In 1992 the government decided to support a charging station network. The network has since extended to neighbouring countries – in 2010 the Park & Charge network in Switzerland, Germany and Austria did encompass 500 charging locations, additionally there a few charging locations in the Netherlands and Italy.\n\nPlug’n Roll: the smart EV charging network in Switzerland.\n\nIn 2009–2010 the Irish Government, and electric utility Electricity Supply Board (ESB) entered into tripartite agreements with a number of major electric vehicle manufacturers (Renault–Nissan\nMitsubishi Motors\nToyota\n, and\nPSA Peugeot Citroën\n) to promote the uptake of electric vehicles in Ireland.\n\nThe Irish government has instituted a package of measures, including a €5,000 grant (US$7,158) to assist with purchasing the vehicle, exemption from vehicle registration tax, and accelerated capital allowances to promote electric vehicle purchase.\nIn 2013, the Irish government withdrew the EV vehicle registration tax exemption and replaced it with a €5,000 discount on the tax. New conditions were also added to the SEAI EV Grant which reclassified private EV purchases via Hire Purchase or car loan as a commercial purchase, effectively reducing the EV grant to €3,500 for all non-cash buyers. As a result of these changes, EV sales fell in 2013 to only 58 units.\n\nESB is providing the charging network, which will be made up of 46 fast-charging (50 kW DC) stations located at intervals on inter-urban national primary routes, 1,500 medium-speed(22 kW AC) public charging points distributed across all towns with population over 1500, and home chargers (3.6 kW 1Φ, 16A) at no cost to the first 2,000 grant qualifying electric car owners.\n\nThe first station of the charger network was commissioned in August 2010. At the end of 2011 the charging station map shows 50 AC charging places plus 10 DC stations – the AC chargers will be built to Type 2 sockets however some older charging spots still need to be rebuilt. As of 2014 all 46 CHAdeMO fast chargers are operational and are slowly being replaced by tri-standard units capable of CHAdeMO, CCS and 44 kW AC power.\n\nAmsterdam announced it will set up 200 charging stations by 2012. In the first step the city will put up 100 stations from Coulomb Technologies in cooperation with Dutch utility Nuon and grid company Alliander. The project \"Amsterdam Elektrisch\" project includes 100 street-side charging stations plus 100 charging stations at car parks The first one was put up on 6. November 2009, the 100th street-side charging station became operational on 4. March 2011, with also over 100 charging stations at car parks. In April 2011, the City of Amsterdam announced the expansion of the street-side charging network with another 1000 charging stations, to be installed by Essent and a joint venture of Nuon and Heijmans.\n\nThe Dutch government created the \"Formula E Team,\" a working group collaborating with local governments, private companies and research institutes to create national and regional electric vehicle initiatives. The Foundation E-Laad.nl has the ambitious plan to put up 10,000 charging points by 2012. The Dutch government and the regional grid companies help Foundation E-laad.nl to put up a charging station network adding 65 million Euro investment support in the timeframe 2009 to 2011. The point of 500 charging stations (distributed over 125 communities) was reached on 24. June. The point of 1000 charging stations was reached on 8. December 2011, 1500 on 2. May 2012. and 2500 on 22. August 2013.\n\nAccording to the roadmap of Formula E-Team the office has been created and the first RFI has started in August 2010; the results will be published in early November for comments and proposals with a definite guide for the infrastructure to be published in March 2011. The integration tests will run in mid of 2011 and the back office system for the networked charging stations to go live in late 2011 along with the \"Charge Authority Board\" for further development. On 19. July 2010 the Formula E-Team has resolved that charge points in the Netherlands will be equipped with Type 2 Mode 3 sockets, based on a decision by providers from 9. April 2010 that will replace the earlier 5-pin CEE red sockets.\n\nThe Netherlands is one of the first European markets for the Nissan Leaf;\nIt is also the first European country to adopt stations for the \"level 3\" fast-charging supported by the Leaf. Epyon has unveiled the first charging station at a gasoline station in Leeuwarden, in the northern province of Friesland.\n\nRWE and the \"Green Stream Cluster\" have started in June 2010 to put up a network of 130 charging stations in Warsaw. The Grean Stream Cluster project will run until mid of 2011. The Green Stream Cluster will put up overall 330 charging stations in five cities: Warsaw, Gdansk, Katowice, Kraków and Mielec. \"Ekoenergetyka-Zachod\" works on an electric vehicle network in the western cities of Zielona Gora (Grünberg), Sulechow, Pila (Schneidemühl) und Sieradz.\n\nRenault–Nissan have signed a contract with MOBIE.Tech that was started back in 2008. There shall be 1300 new charging stations and 50 fastcharge stations within the 2011 timeframe.\n\nThe government wants to enlarge to renewable energy sector up to 60% and usage of electric vehicles is considered an important strategy to cut dependency on imports. The MOBI.E network has installed 100 charging stations and it is deploying 1300 charging stations as well as 50 fast-charge stations in 25 cities up to June 2011. The MOBI.E stations work with magnetic stripe card and bills are sent to the cell phone – the government hopes to export the concept to other countries.\n\nAn overview of available charging stations is provided by polni.si, the biggest providers are Dravske Elektrarne Maribor, Elektro Celje, Elektro Gorenjska, Elektro Ljubljana, Elektro Maribor, Elektro Primorska and Petrol. The municipal works Elektro Ljubljana provides a number of public charging stations in the elektro-crpalke network based on 400 V/32 A type or the domestic socket type (Schuko).\n\nThe Alargador map has 190 (August 2010) contributors of charging stations in Spain and Portugal (May 2010). They are mainly hotels, camping sites, restaurants, commercial centers, foundations, associations, EV dealers and private individuals with most systems operated manually.\n\nIn Madrid, Spain, a trial project will convert 30 former telephone boxes into charging points for electric cars. They are considered suitable, since telephone boxes are generally located at the roadside and are already connected to the electricity supply network. They would form part of a planned network of 546 charging points in Madrid, Barcelona and Seville, subsidised by the Spanish Government. The charging grid is created for the MOVELE pilot project of the Institute for Diversification and Saving of Energy (Instituto para la Diversificación y Ahorro de la Energía, IDAE) that is also providing for 2,000 electric vehicles to the field test. The Spanish government has committed itself to have 1 million electric vehicles (fully electric and hybrid cars) in Spain by 2014.\n\nThe Chairman of Endesa, Borja Prado, together with a former mayor of Madrid, Alberto Ruiz Gallardón, and the Chairman of Telefónica, César Alierta, have the phone booth in Madrid which can also be used for recharging electric vehicles. Reserved parking spaces will be located next to this and all other booths set up in Metropolitan areas where users will be able to park their EVs and recharge at no cost once they have obtained their free \"zero emissions\" pre-paid card from the Madrid city council.\n\nThe \"Live Barcelona\" map (sponsored by the Barcelona city council, the Energy state department of Catalonia, utility Endesa, car maker Seat) lists 138 charging spots in Barcelona with 55 of them functional (February 2011).\n\nIn September 2011 Endesa signed agreements with Mitsubishi, Renault–Nissan and the Japanese Chademo Foundation on the promotion of fast-charge stations. Endesa will hold the Chademo Europe chair. As a consequence, Endesa will deploy two types of public charging spots – conventional charging (16 A, 230 V AC, Schuko type) and rapid charging (125 A, 400 V DC, Chademo type). In October 2011 Endesa ordered 53 rapid charging stations to be built by GE in strategic places in Spain.\n\nThe Galicia region is creating a research cluster (Clúster de Empresas de Automoción de Galicia / Ceaga). The infrastructure side (Plan Mobega – Plan de movilidad eléctrica de Galicia) includes the implementation of a network of multifunctional electromobility stations located at rent-a-car stations. The current installation includes 7 Multifunctional Electromobility stations which are located in the main metropolitan areas of Galicia and a fleet of 28 electric vehicles. The project was started in September 2011 and will continue until Enero 2013.\n\nÉlectricité de France is partnering with Elektromotive, Ltd. to install 250 new charging points over six months from October 2007 in London and elsewhere in the UK. By November 2011 there are 687 Electrobay charging stations (200 in London) and it plans to build 4000 charging points throughout 2012. Elektromotive has provided 400 public access charge points to the \"Charge your Car\" network of One NorthEast in 2010 and it has installed more than 120 charge points across Scotland.\n\nThe Renault–Nissan Alliance and UK company Elektromotive, a provider of electric vehicle recharging stations, are collaborating in the Partnership for Zero-Emission-Mobility, with the aim of accelerating the installation of charging networks for plug-in vehicles in cities. The Alliance and Elektromotive have signed a Memorandum of Understanding.\n\nA fleet of electric cars and charge points will be rolled out across Coventry (England) as part of a multimillion-pound pilot project.\n\nThe Department for Transport (DfT) announced in April 2009 that £230 million would be allocated to incentivise the market uptake of EVs in the UK. The scheme will become operational in 2011 and each EV purchaser could receive a rebate of between £2,000 -£5,000. Electric vehicles are exempt from purchase and annual vehicle tax. From April 2010, purchasers of an average new car (Band G) will pay a one off £155 showroom tax and an annual vehicle tax of £155.EVs are tax free.\n\nOn the 25 February 2010, London, the North East region and Milton Keynes were selected to be the lead places for electric vehicle infrastructure. In total, their plans will result in over 2,500 charge points in the first year and over 11,000 in the next three years, at a variety of publicly accessible car parks, transport hubs and workplaces.\n\nThe London mayor called for an E-revolution in March 2009 and he presented the \"Electric Delivery Plan for London\" in May 2009. The plan projects 25,000 charging points London by 2015 including 500 on-street, 2000 off-street in car-parks and 22,000 privately owned locations. London itself will buy 1000 electric vehicles up to 2015. Owners of an electric car will not need to pay the Congestion Charge for the city of London being worth up to £1,700 a year. At that point (May 2009) London already had 100 charge points in public places which will be increased to 250 by 2012. Beginning 2011 20% of new lots in car parks must have access to a charging outlet. Additionally, the parking in the Westminster boroughs will be free for electric vehicles saving the user up to £6,000 a year and a flat rate of £200 electricity cost is charged for the usage of public outlets in Westminster.\n\nAs of February 2011 the \"Source London\" project has contracted Siemens to build a network of public charging stations in London. At least 1,300 charging points will be installed by the end of 2013 in public locations and streets across the Capital. Transport for London (TfL) has also finalised a contract that will see Siemens manage the operation of the network and registration of drivers. The deal between TfL and Siemens will see Siemens run the Source London back office to March 2014 at no cost. Up to July 2011 there were 180 charging stations. and in November 2011 more than 200. The number of charging stations reached 790 in October 2012 with plans to increase that to 1300 in 2013. The goal of 1300 publicly accessible charging stations was met on 16 May 2013.\n\nIn July 2011 a charity called \"Zero Carbon World\" announced their \"Charge Points Everywhere\" network, their aim is to help business's install free to use charge points on their business's with the implicit understanding the person using the charge point will use the services of that business. The connections themselves are standard 32 and 13amp connectors and the inclusion of the 32amp connector means that car with powerful chargers such as Tesla can charge much faster than with the 13a connectors on the majority of chargers On February 15, 2012, the alliance announce to donate 1000 charging stations for free adding up on the existing 76 charging stations that are already deployed.\n\nThe \"Plugged-In Places\" program of the Department of Transport offers grants for charging station networks in the United Kingdom. The development plan identifies 8 regions to be in a strategic focus – Central Scotland, the East of England, Greater Manchester, Milton Keynes, the North East of England and Northern Ireland – with a target of 8500 chargepoints. Following the ACEA position paper the government program favours moving to a dedicated recharging connector of Type 2 Mode 3. Referring to the PIP program an open tender in Newcastle upon Tyne identifies the goal to have 75% of the charging stations to offer Type 2 Mode 3 sockets including to switch over existing charging stations to that type.\n\nIsrael has enacted policies that create a tax differential between zero-emission vehicles and traditional cars, to accelerate the transition to electric cars.\n\nBetter Place began to build its first electric vehicle network in Israel in conjunction with French car-maker, Renault. The company conducted its first market tests in Israel, Denmark and Hawaii because their small size also made them suitable as test markets. Better Place opened its first functional charging station in Israel the first week of December 2008 at Cinema City in Pi-Glilot, and additional stations were planned in Tel Aviv, Haifa, Kfar Saba, Holon, and Jerusalem.\n\nIn March 2011 Better Place presented a detailed plan for network construction, including 40 battery swap stations and 400 charging stations across Israel. 200 locations were said to be under construction or planned at the end of 2011, but that goal was not reached. On 26 May 2013, Better Place filed for bankruptcy in Israel, having terminated its projects in most markets.\n\nGnrgy, originally a producer of mobile charging solutions, entered the market as an alternative to Betterplace. On 29 February 2012 it partnered with Pango, provider of parking billing solutions, to set up a series of charging stations throughout Israel.\n\nInfrastructure is planned by Better Place and Nissan for Yokohama.\n\nInfrastructure is planned by Robert Bosch and Keppel Energy for Singapore \n\nInfrastructure has been installed by Coulomb Technologies in Arizona; California – San Francisco, San Jose, Walnut Creek, and Sonoma; Colorado; Washington, D.C.; Florida; Chicago, Illinois; Massachusetts; Detroit, Michigan; Minneapolis, Minnesota; New York City; Cary, North Carolina; Ohio; Portland, Oregon; Nashville, Tennessee; Texas; Seattle, Washington; Wisconsin.\nGilbarco Veeder-Root are partnering with Coulomb to advance public charging facilities. Gilbarco exhibited Coulomb Technologies' Smartlet Charging Station at the National Association of Convenience Stores (NACS) show in October 2008.\nAt the end of 2008, Coulomb Technologies planned to roll out five curbside charging stations in downtown San Jose that drivers can access through a prepaid plan. The company was working with entities in Las Vegas Nevada, New York and Florida to do something similar there. Coulomb Technologies has announced to provide 1000 free public charging stations until December 2010. They also plan to expand its \"ChargePoint America network\" to 4600 free home and public level-2 charging stations until October 2011 in nine regions: Austin, Texas; Detroit, Michigan; Los Angeles, California; New York, New York; Orlando, Florida; Sacramento, California; the San Jose/San Francisco Bay Area, California; Redmond, Washington; and Washington DC. The $37 million ChargePoint America program is made possible by a $15M grant funded by the American Recovery and Reinvestment Act through the Transportation Electrification Initiative administered by the Department of Energy. So far 149 stations are operational according to the ChargePoint map, 51 stations are in California. New York joins the ChargePoint network building more than 100 charging stations in public places until October 2011. In April 2012 the first milestone of the Chargepoint America program has been reached with Colulomb Technologies having delivered 2400 public and commercial charging stations, the actual installation of its Level 2 (240 V 30 A) stations in the 10 participating regions will continue.\n\nInfrastructure is planned by Better Place for Hawaii, Oregon, and California – the San Francisco Bay area, Sacramento, San Jose, Los Angeles, San Diego, and the highway and freeway corridors between them.\n\nOther companies that are building charging stations throughout the U.S. are ECOtality and SolarCity \n\nIn the initial phase of \"The EV Project\" of ECOtality there are 11 participating cities: Phoenix (AZ), Tucson (AZ), San Diego (CA), Portland (OR), Eugene (OR), Salem (OR), Corvallis (OR), Seattle (WA), Nashville (TN), Knoxville (TN) and Chattanooga (TN). The contract for the \"EV Project\" was signed on October 1, 2009, with the US Department of Energy and it includes 8,300 Level 2 chargers installed in owner's homes; 6,350 Level 2 chargers installed in commercial and public locations; and 310 Level 3 DC fast-chargers. The EV project will run for 36 months. The public charging stations will be put up beginning in summer 2010. Texas has joined the EV Project in July 2010. San Diego will take a share of 1,500 public charging stations and 1,000 home base charging points. The first milestone of The EV Project has been reached in April 2012.\n\nPortland General Electric installs 12 electric vehicle charging stations in Portland and Salem, Oregon till September 2008 and it has installed 20 charging stations by 2010 as part of a demonstration project to develop the transportation infrastructure needed to support electric vehicles and plug-in cars.\n\nNRG Energy has announced to create a network of 50 charging stations in northern Texas under the \"EVgo\" brand. In March 2012 the company announced to build a network of 200 fast-charging stations in California over the next four years. By Dec. 30, 2015 EVgo has installed over 1,000 chargers in over 25 markets. NRG EVgo has developed partnerships to build infrastructure and offer complimentary charging with Nissan, BMW and Ford.\n\nDrivers can now plug in at two park-and-ride lots in King County, which includes Seattle. The county plans to add sockets at three garages under construction. There are about 30 reliable sites in the Seattle area to plug in. Most are free, others charge the same as parking a gas-powered car -- $7 an hour. In recent months, the cities of Edmonds and Lacey invited drivers to plug in their electric vehicles at free public stations.\n\nIn Virginia, with the participation of the Town of Wytheville, and several businesses, Plugless Power inductive charging stations began field testing in March 2010. South Carolina has unveiled its \"Plug in Carolina\" program including 100 public charging stations in December 2010 In San Antonio, TX, a downtown church (Travis Park United Methodist Church) made Level 1 charging available in its parking lot in 2009.\n\nThe DBEDT ministry of Hawaii had a state rebate program \"EV Ready Grant\" that was funded by the American Recovery and Reinvestment Act – the program was offering $4500 for a full-speed commercially available electric vehicle and $500 for electric vehicle chargers. The \"EV Ready Grant\" program is followed by the \"EV Ready Rebate\" program offering 20% of the purchase price with a maximum of $4500 for a full-speed commercially available electric vehicle and 30% of the purchase prices with a maximum of $500 for electric vehicle chargers. Charging equipment is expected follow the standards including SAE J1772. The designated Transportation Working Group expects 200 charging stations to be available in 2010 In February 2012 it was announced to have Betterplace activate its multi-island network of 130 charging stations (Oahu, Maui, Kauai and the Big Island). The Hawaii rebate program is being continued with having reached a score of 372 funded vehicles and 246 chargers, and by April 2012 approximately 220 charging stations have been installed as part of the EV Ready Grant Program. The Hawaii station database lists the 200 public charging stations in 80 locations that were available up to March 2012, about 140 have been installed by BetterPlace.\n\nIn California the car maker Tesla has put up 18 public charging stations. Within the SF Bay Area Activities & Coalition has identified 109 locations to put up public charging stations beginning 2009 based on funding by ARRA. The last California \"ZEV Program Review symposium\" was held on 23. September 2009, the next one is scheduled for late summer 2010. In the past there had been a charging station network to support the General Motors EV1 that had installed 500 public charging station.\n\nThe U.S. Department of Energy offers a list of locations of the available alternative fuel infrastructure. The historic trend summary (1992–2010) shows a total of 541 electric charging locations by 2010 which had been still lower than the peak count of 873 charging locations in 2002. As of April 2014 the total count of public electric charge stations had increased to 7904.\n\nIn January 2016, UTE opened the first charging station in Montevideo, exclusive for taxis.\n\nIn December 2017, UTE and Ancapopened a charging station network that connects Colonia del Sacramento, Rosario, Puntas de Valdez, Montevideo, San Luis and Punta del Este, with stations every 65 km. The stations at the Carrasco Airport and Colonia have 43 kW, whereas the other stations have 22 kW.\n\n"}
{"id": "8314359", "url": "https://en.wikipedia.org/wiki?curid=8314359", "title": "Energy tax", "text": "Energy tax\n\nAn energy tax is a tax that increases the price of energy (Fisher \"et al\"., 1996, p. 416). Arguments in favour of energy taxes have included the pursuit of macroeconomic objectives, e.g., fiscal deficit reduction in the 1990s, as well as environmental benefits, i.e., reduced pollution (Nellor, 1994, p. 1). A weakness of energy taxes is that they impose a burden (or cost) in the form of reduced economic output and employment (p. 19).\n\nIn 1993, then President Bill Clinton proposed a BTU tax. A BTU tax is a type of energy tax (Baron, 1997, p. 14). The tax would have taxed all fuel sources based on their heat content except for wind, solar, and geothermal. It was never adopted. The BTU tax passed the House, but was rejected by the Senate in light of the lobbying effort mobilized against its adoption. The rejected proposal was watered down, as the Clinton administration tried to salvage their efforts by offering to exempt manufacturers and base the tax on the cost rather than the heat content of energy. Many of the House Democrats who voted for the tax and who lost their seats in the 1994 midterm election, blamed their loss on their vote for the BTU tax. Getting \"BTU'd\" became Beltway slang at the time for those who lost reelection by voting for the controversial proposal.\n\nOn 7 November 2006, citizens of Boulder, Colorado (a city with roughly 100,000 residents situated in the foothills of the Rocky Mtns) voted in favor of initiative 202, the Boulder Climate Action Plan Tax. That marks the first time in the nation that a municipal government has imposed an energy tax directly upon its residents to combat global warming. It applies to energy consumption with deductions for carbon-neutral and renewable energy sources (such as Xcel Energy’s WindSource).\n\nThe tax appears on consumer's energy bills and is used to fund the city’s Office of Environmental Affairs that is in charge of programs designed to reduce Boulder's carbon footprint.\n\nThe CAP tax is to generate roughly $1 million annually. The City Council has the authority to increase the rates as needed. As of October 2009, the rate is assigned as follows:\n\n"}
{"id": "17904373", "url": "https://en.wikipedia.org/wiki?curid=17904373", "title": "Eolica Sarichioi Wind Farm", "text": "Eolica Sarichioi Wind Farm\n\nThe Eolica Sarichioi Wind Farm is a proposed wind power project in Sarichioi, Tulcea County, Romania. It will consist of eight individual wind farms connected together. It will have 51 individual wind turbines with a nominal output of around 2 MW which will deliver up to 102 MW of power, enough to power over 66,700 homes, with a capital investment required of approximately US$110 million.\n"}
{"id": "26446980", "url": "https://en.wikipedia.org/wiki?curid=26446980", "title": "Ghana Nuclear Society", "text": "Ghana Nuclear Society\n\nThe Ghana Nuclear Society (GNS) is a nonprofit organization that advocates for the introduction of nuclear energy in Ghana. It is headquartered at the Ghana Atomic Energy Commission (GAEC) in Accra. With the establishment of The Ghana Nuclear Society, Ghana has joined the league of those countries with National Nuclear Societies. Its head office is located at the Ghana Atomic Energy Commission (GAEC) in Accra. The current national president is Prof. John Justice Fletcher. The society is not for science inclined persons alone.\n\nThe Ghana Nuclear Society received its certificate of incorporation on 13 May 2008. The society, which operates under the motto \"Nuclear for Sustainable Energy Development,\" has an eight-member Advisory Panel that consults with the Board of Directors, which is made up of 13 persons and four members from the National and Student Chapter Executives.\n\nThe society has created public information programs on nuclear matters, and it has produced seminars, educational outreach programs and interactive media presentations on local radio and television stations. It also publishes a newsletter that outlines issues relating to nuclear energy.\n\nIt is planning educational programs at the SAMBEL Academy and the GAEC, and at primary schools located near the Graduate School of Nuclear and Allied Sciences at Atomic, Kwabeyna.\nSOME MILESTONES IN GNS HISTORY\n\n\nTo enhance public acceptance and awareness of the Nuclear Power Option, the society organized a three-day conference under the theme: “Energy Security for Accelerated Development of the African Region”. This conference hoped to promote the acceptance of the Nuclear Power in Africa by bringing together nuclear power vendors, reactor manufactures, scientists and experts in the field to share knowledge with those in Africa.\n\n"}
{"id": "21045177", "url": "https://en.wikipedia.org/wiki?curid=21045177", "title": "Hardeland hydroelectric power station", "text": "Hardeland hydroelectric power station\n\nHardeland hydroelectric power station is a power plant in Etne in western Norway. The site is owned and operated by Haugaland Kraft.\n\nThe station uses water from two sets of reservoirs. Hardeland H is a 400 m vertical fall from Løkjelsvatnet, while Hardeland K uses Grindheimsvatnet, Ilsvatnet, Basurde-/Krokavatnet and Hjørnås via a 305 m fall from the Hjørnås lake.\n\nHardeland is connected to the grid with a 22 kV line to Litledalen. The site uses three pelton wheels with 14 MW generators, and average yearly output is 123 GWh.\n\n"}
{"id": "1024214", "url": "https://en.wikipedia.org/wiki?curid=1024214", "title": "Heliostat", "text": "Heliostat\n\nA heliostat (from \"helios\", the Greek word for \"sun\", and \"stat\", as in stationary) is a device that includes a mirror, usually a plane mirror, which turns so as to keep reflecting sunlight toward a predetermined target, compensating for the sun's apparent motions in the sky. The target may be a physical object, distant from the heliostat, or a direction in space. To do this, the reflective surface of the mirror is kept perpendicular to the bisector of the angle between the directions of the sun and the target as seen from the mirror. In almost every case, the target is stationary relative to the heliostat, so the light is reflected in a fixed direction. According to contemporary sources the heliostata, as it was called at first, was invented by Willem 's Gravesande (1688-1742). Other contenders are Giovanni Alfonso Borelli (1608-1679) and Daniel Gabriel Fahrenheit (1686-1736).\n\nNowadays, most heliostats are used for daylighting or for the production of concentrated solar power, usually to generate electricity. They are also sometimes used in solar cooking. A few are used experimentally, or to reflect motionless beams of sunlight into solar telescopes. Before the availability of lasers and other electric lights, heliostats were widely used to produce intense, stationary beams of light for scientific and other purposes.\n\nMost modern heliostats are controlled by computers. The computer is given the latitude and longitude of the heliostat's position on the earth and the time and date. From these, using astronomical theory, it calculates the direction of the sun as seen from the mirror, e.g. its compass bearing and angle of elevation. Then, given the direction of the target, the computer calculates the direction of the required angle-bisector, and sends control signals to motors, often stepper motors, so they turn the mirror to the correct alignment. This sequence of operations is repeated frequently to keep the mirror properly oriented.\n\nLarge installations such as solar-thermal power stations include fields of heliostats comprising many mirrors. Usually, all the mirrors in such a field are controlled by a single computer.\n\nThere are older types of heliostat which do not use computers, including ones that are partly or wholly operated by hand or by clockwork, or are controlled by light-sensors. These are now quite rare.\n\nHeliostats should be distinguished from solar trackers or sun-trackers that point directly at the sun in the sky. However, some older types of heliostat incorporate solar trackers, together with additional components to bisect the sun-mirror-target angle.\n\nA siderostat is a similar device which is designed to follow a fainter star, rather than the sun.\n\nIn a solar-thermal power plant, like those of The Solar Project or the PS10 plant in Spain, a wide field of heliostats focuses the sun's power onto a single collector to heat a medium such as water or molten salt. The medium travels through a heat exchanger to heat water, produce steam, and then generate electricity through a steam turbine.\n\nA somewhat different arrangement of heliostats in a field is used at experimental solar furnaces, such as the one at Odeillo, in France. All the heliostat mirrors send accurately parallel beams of light into a large paraboloidal reflector which brings them to a precise focus. The mirrors have to be located close enough to the axis of the paraboloid to reflect sunlight into it along lines parallel to the axis, so the field of heliostats has to be narrow. A closed loop control system is used. Sensors determine if any of the heliostats is slightly misaligned. If so, they send signals to correct it.\n\nIt has been proposed that the high temperatures generated could be used to split water producing hydrogen sustainably.\n\nSmaller heliostats are used for daylighting and heating. Instead of many large heliostats focusing on a single target to concentrate solar power (as in a solar power tower plant), a single heliostat usually about 1 or 2 square meters in size reflects non-concentrated sunlight through a window or skylight. A small heliostat, installed outside on the ground or on a building structure like a roof, moves on two axes (up/down and left/right) in order to compensate for the constant movement of the sun. In this way, the reflected sunlight stays fixed on the target (e.g. window).\n\nGenzyme Center, corporate headquarters of Genzyme Corp. in Cambridge, Massachusetts, uses heliostats on the roof to direct sunlight into its12-story atrium.\n\nIn a 2009 article, Bruce Rohr suggested that small heliostats could be used like a solar power tower system. Instead of occupying hundreds of acres, the system would fit in a much smaller area, like the flat rooftop of a commercial building, he said. The proposed system would use the power in sunlight to heat and cool a building or to provide input for thermal industrial processes like processing food. The cooling would be performed with an absorption chiller. Mr. Rohr proposed that the system would be “more reliable and more cost-effective per square meter of reflective area” than large solar power tower plants, in part because it would not be sacrificing 80 percent of the power collected in the process of converting it to electricity.\n\nHeliostat costs represent 30-50% of the initial capital investment for solar power tower power plants depending on the energy policy and economic framework in the location country. It is of interest to design less expensive heliostats for large-scale manufacturing, so that solar power tower power plants may produce electricity at costs more competitive to conventional coal or nuclear power plants costs.\n\nBesides cost, percent solar reflectivity (i.e. albedo) and environmental durability are factors that should be considered when comparing heliostat designs.\n\nOne way that engineers and researchers are attempting to lower the costs of heliostats is by replacing the conventional heliostat design with one that uses fewer, lighter materials. A conventional design for the heliostat's reflective components utilizes a second surface mirror. The sandwich-like mirror structure generally consists of a steel structural support, an adhesive layer, a protective copper layer, a layer of reflective silver, and a top protective layer of thick glass. This conventional heliostat is often referred to as a glass/metal heliostat. Alternative designs incorporate recent adhesive, composite, and thin film research to bring about materials costs and weight reduction. Some examples of alternative reflector designs are silvered polymer reflectors, glass fiber reinforced polyester sandwiches (GFRPS), and aluminized reflectors. Problems with these more recent designs include delamination of the protective coatings, reduction in percent solar reflectivity over long periods of sun exposure, and high manufacturing costs.\n\nThe movement of most modern heliostats employs a two-axis motorized system, controlled by computer as outlined at the start of this article. Almost always, the primary rotation axis is vertical and the secondary horizontal, so the mirror is on an alt-azimuth mount.\n\nOne simple alternative is for the mirror to rotate around a polar aligned primary axis, driven by a mechanical, often clockwork, mechanism at 15 degrees per hour, compensating for the earth's rotation relative to the sun. The mirror is aligned to reflect sunlight along the same polar axis in the direction of one of the celestial poles. There is a perpendicular secondary axis allowing occasional manual adjustment of the mirror (daily or less often as necessary) to compensate for the shift in the sun's declination with the seasons. The setting of the drive clock can also be occasionally adjusted to compensate for changes in the Equation of Time. The target can be located on the same polar axis that is the mirror's primary rotation axis, or a second, stationary mirror can be used to reflect light from the polar axis toward the target, wherever that might be. This kind of mirror mount and drive is often used with solar cookers, such as Scheffler reflectors. For this application, the mirror can be concave, so as to concentrate sunlight onto the cooking vessel.\n\nThe alt-azimuth and polar-axis alignments are two of the three orientations for two-axis mounts that are, or have been, commonly used for heliostat mirrors. The third is the target-axis arrangement in which the primary axis points toward the target at which sunlight is to be reflected. The secondary axis is perpendicular to the primary one. Heliostats controlled by light-sensors have used this orientation. A small arm carries sensors that control motors that turn the arm around the two axes, so it points toward the sun. (Thus this design incorporates a solar tracker.) A simple mechanical arrangement bisects the angle between the primary axis, pointing to the target, and the arm, pointing to the sun. The mirror is mounted so its reflective surface is perpendicular to this bisector. This type of heliostat was used for daylighting prior to the availability of cheap computers, but after the initial availability of sensor control hardware.\n\nThere are heliostat designs which do not require the rotation axes to have any exact orientation. For example, there may be light-sensors close to the target which send signals to motors so that they correct the alignment of the mirror whenever the beam of reflected light drifts away from the target. The directions of the axes need be only approximately known, since the system is intrinsically self-correcting. However, there are disadvantages, such as that the mirror has to be manually realigned every morning and after any prolonged cloudy spell, since the reflected beam, when it reappears, misses the sensors, so the system cannot correct the orientation of the mirror. There are also geometrical problems which limit the functioning of the heliostat when the directions of the sun and the target, as seen from the mirror, are very different. Because of the disadvantages, this design has never been commonly used, but some people do experiment with it.\n\nTypically, the heliostat mirror moves at a rate that is 1/2 the angular motion of the sun. There is another arrangement that satisfies the definition of a heliostat yet has a mirror motion that is 2/3rd of the motion of the sun.\n\nMany other types of heliostat have also occasionally been used. In the very earliest heliostats, for example, which were used for daylighting in ancient Egypt, servants or slaves kept the mirrors aligned manually, without using any kind of mechanism. (There are places in Egypt where this is done today, for the benefit of tourists. In the movie The Fifth Element an Egyptian boy holds a mirror to illuminate a wall inside a cave for a fictional archaeologist.) Elaborate clockwork heliostats were made during the 19th Century which could reflect sunlight to a target in any direction using only a single mirror, minimizing light losses, and which automatically compensated for the sun's seasonal movements. Some of these devices are still to be seen in museums, but they are not used for practical purposes today. Amateurs sometimes come up with \"ad hoc\" designs which work approximately, in some particular location, without any theoretical justification. An essentially limitless number of such designs are possible.\n\n\n"}
{"id": "52900176", "url": "https://en.wikipedia.org/wiki?curid=52900176", "title": "International Nuclear Societies Council", "text": "International Nuclear Societies Council\n\nThe International Nuclear Society Council (INSC), founded on 11 November 1990 by the INSG (International group of Nuclear Societies), is a non-governmental organisation made up of Nuclear Societies from all over the world that \"acts as a global forum for nuclear societies to discuss and establish common aims and goals\".\n\n\n"}
{"id": "1018670", "url": "https://en.wikipedia.org/wiki?curid=1018670", "title": "Kago", "text": "Kago\n\nA is a type of litter used as a means of human transportation by the non-samurai class in feudal Japan and into the Meiji period.\n\nThe basket of the kago was about three feet long and it was attached to bamboo uprights which were suspended by a large overhead single crossbeam. A roof of some type covered the top and screens could be used to cover the sides as protection from sun or rain. The kago was carried by a team of four men who took turns carrying the kago on their shoulders, five or six miles could be traveled in one hour. One man would support the weight of the large overhead pole at each end and walked until he tired and switched with a rested carrier. The kago should not be confused with the more elaborate \"\" which were used by the samurai class and wealthy individuals.\n"}
{"id": "23702710", "url": "https://en.wikipedia.org/wiki?curid=23702710", "title": "Kappa Crucis", "text": "Kappa Crucis\n\nKappa Crucis (κ Cru) can refer to:\n\n"}
{"id": "4989754", "url": "https://en.wikipedia.org/wiki?curid=4989754", "title": "Khushab Nuclear Complex", "text": "Khushab Nuclear Complex\n\nKhushab Nuclear Complex is a plutonium production nuclear reactor and heavy water complex situated 30 km south of the town of Jauharabad in Khushab District, Punjab, Pakistan.\n\nThe heavy water and natural uranium reactors at Khushab are a central element of Pakistan's program to produce plutonium and tritium for use in compact nuclear warheads. Khushab Nuclear Complex, like that at Kahuta, is not subject to International Atomic Energy Agency inspections.\n\nFour currently operating reactors have capacities variously reported at between 40 MWth to 50 MWth, and as high as 70 MWth. In total, they are estimated to be capable of producing 44 kg of weapons grade plutonium annually. Plutonium production and nuclear reprocessing facilities are being expanded at Khushab, New Labs and Chashma.\n\nPakistan's first indigenous nuclear reactor was commissioned at Khushab in March 1996. The Khushab Nuclear Complex was conceived and planned by the then chairman of the Pakistan Atomic Energy Commission (PAEC), Munir Ahmad Khan, who began work on the 50 MWth Khushab-I reactor and heavy water plant in 1986. He appointed nuclear engineer Sultan Bashiruddin Mahmood and Dr. N.A. Javed, both from the PAEC, as the Project-Directors for the reactor and the heavy water plant respectively. According to a Pakistani press report this reactor began operating in early 1998.\n\nBased on the success of these projects and the experience and capability gained during their construction, onsite construction work on the second unit began around 2001 or 2002. In February 2010 Prime Minister Yousaf Raza Gillani and senior military officers attended a ceremony at the Khushab complex for what is believed to be the completion of the second reactor. There has been little to no government comment on the complex or other aspects of the nuclear weapons program since the late 1990s.\n\nJudging by external appearance all but the first reactor are similar or identical in design.\n\n\nThe heavy water plant is estimated to be able to produce between 50 and 100 tons of heavy water per year.\n\n\n"}
{"id": "11548767", "url": "https://en.wikipedia.org/wiki?curid=11548767", "title": "Kudurru of Gula", "text": "Kudurru of Gula\n\nThe Kudurru of Gula is a boundary stone (Kudurru) for the Babylonian goddess Gula. Gula is the goddess of healing. It is from the 14th century - 13th century BC Kassite Babylonia, and is located at the Louvre.\n\nThe \"Kudurru of Gula\" shows Gula seated on her chair with her dog adjacent. Another side of the kudurru has registers representing symbols of gods, and also sections of cuneiform text.\n\n\n"}
{"id": "5635233", "url": "https://en.wikipedia.org/wiki?curid=5635233", "title": "Lignocellulosic biomass", "text": "Lignocellulosic biomass\n\nLignocellulose refers to plant dry matter (biomass), so called lignocellulosic biomass. It is the most abundantly available raw material on the Earth for the production of biofuels, mainly bio-ethanol. It is composed of carbohydrate polymers (cellulose, hemicellulose), and an aromatic polymer (lignin). These carbohydrate polymers contain different sugar monomers (six and five carbon sugars) and they are tightly bound to lignin. Lignocellulosic biomass can be broadly classified into virgin biomass, waste biomass and energy crops. Virgin biomass includes all naturally occurring terrestrial plants such as trees, bushes and grass. Waste biomass is produced as a low value byproduct of various industrial sectors such as agriculture (corn stover, sugarcane bagasse, straw etc.) and forestry (saw mill and paper mill discards). Energy crops are crops with high yield of lignocellulosic biomass produced to serve as a raw material for production of second generation biofuel; examples include switch grass(\"Panicum virgatum\") and Elephant grass.\n\nMany crops are of interest for their ability to provide high yields of biomass and can be harvested multiple times each year. These include poplar trees and \"Miscanthus giganteus\". The premier energy crop is sugarcane, which is a source of the readily fermentable sucrose and the lignocellulosic by-product bagasse.\n\nLignocellulosic biomass is the feedstock for the pulp and paper industry. This energy-intensive industry focuses on the separation of the lignin and cellulosic fractions of the biomass.\n\nLignocellulosic biomass, in the form of wood fuel, has a long history as a source of energy. Since the middle of the 20th century, the interest of biomass as a precursor to \"liquid\" fuels has increased. To be specific, the fermentation of lignocellulosic biomass to ethanol is an attractive route to fuels that supplements the fossil fuels. Biomass is a carbon-neutral source of energy: Since it comes from plants, the combustion of lignocellulosic ethanol produces no net carbon dioxide into the earth’s atmosphere. Aside from ethanol, many other lignocellulose-derived fuels are of potential interest, including butanol, dimethylfuran, and gamma-Valerolactone.\n\nOne barrier to the production of ethanol from biomass is that the sugars necessary for fermentation are trapped inside the lignocellulose. Lignocellulose has evolved to resist degradation and to confer hydrolytic stability and structural robustness to the cell walls of the plants. This robustness or \"recalcitrance\" is attributable to the crosslinking between the polysaccharides (cellulose and hemicellulose) and the lignin via ester and ether linkages. Ester linkages arise between oxidized sugars, the uronic acids, and the phenols and phenylpropanols functionalities of the lignin. To extract the fermentable sugars, one must first disconnect the celluloses from the lignin, and then use acid or enzymatic methods to hydrolyze the newly freed celluloses to break them down into simple monosaccharides. Another challenge to biomass fermentation is the high percentage of pentoses in the hemicellulose, such as xylose, or wood sugar. Unlike hexoses such as glucose, pentoses are difficult to ferment. The problems presented by the lignin and hemicellulose fractions are the foci of much contemporary research.\n\nA large sector of research into the exploitation of lignocellulosic biomass as a feedstock for bio-ethanol focuses particularly on the fungus \"Trichoderma reesei\", known for its cellulolytic abilities. Multiple avenues are being explored including the design of an optimised cocktail of cellulases and hemicellulases isolated from \"T. reesei\", as well as genetic-engineering-based strain improvement to allow the fungus to simply be placed in the presence of lignocellulosic biomass and break down the matter into -glucose monomers. Strain improvement methods have led to strains capable of producing significantly more cellulases than the original QM6a isolate; certain industrial strains are known to produce up to 100g of cellulase per litre of fungus thus allowing for maximal extraction of sugars from lignocellulosic biomass. These sugars can then be fermented, leading to bio-ethanol.\n"}
{"id": "45241275", "url": "https://en.wikipedia.org/wiki?curid=45241275", "title": "Lithosphere-Asthenosphere boundary", "text": "Lithosphere-Asthenosphere boundary\n\nThe Lithosphere-Asthenosphere boundary (LAB) represents a mechanical difference between layers in Earth’s inner structure. Earth’s inner structure can be described both chemically (crust, mantle, core) and mechanically. The Lithosphere-Asthenosphere boundary (referred to as the LAB by geophysicists) lies between Earth's cooler, rigid lithosphere and the warmer, ductile asthenosphere. The actual depth of the boundary is still a topic of debate and study, although it is known to vary according to the environment.\n\nThe LAB is determined from the differences in the lithosphere and asthenosphere including, but not limited to, differences in grain size, chemical composition, thermal properties, and extent of partial melt; these are factors that affect the rheological differences in the lithosphere and asthenosphere.\n\nThe lithosphere-asthenosphere boundary represents a rheological boundary. Colder temperatures at Earth's shallower depths affect the viscosity and strength of the lithosphere. Colder material in the lithosphere resists flow while the \"warmer\" material in the asthenosphere contributes to its lower viscosity. The increase in temperature with increasing depth is known as the geothermal gradient and is gradual within the rheological boundary layer. The lithosphere is the portion of the thermal boundary layer commonly defined by its purely conductive heat transport. Throughout the rheological boundary, the geotherm gradually transitions from the conductive nature of the lithospheric geotherm to the convective (adiabatic) nature of the underlying asthenosphere.\nSometimes other definitions of the asthenosphere and LAB are used, e.g. seismic asthenosphere is a layer of low velocity and high attenuation of the seismic waves. Moreover, the rheology of the rocks depends also on the stress tensor. It means that asthenosphere could be of thermal and/or mechanical origin.\n\nThe LAB is often observed and imaged via signal processing techniques and seismic waves. Seismic tomographic studies suggests that the LAB is not determined by a purely thermal model, but rather it is affected by the presence of partial melt material in the asthenosphere. Evidence from converted seismic phases indicates a sharp decrease in shear-wave velocity 90–110 km below continental crust. Recent seismological studies indicate a 5 to 10 percent reduction in shear-wave velocity in the depth range of 35 to 120 km beneath ocean basins. The seismic discontinuity often associated with this sharp contrast in wave velocity and presence of partial melt is known as the Gutenberg discontinuity or \"G\" to many geophysicists. The Gutenberg discontinuity coincides with the expected LAB depth in many studies and has also been found to become deeper under older crust, thus supporting the suggestion that the discontinuity is closely interrelated to the LAB.\n\nBeneath oceanic crust, the LAB ranges anywhere from 50 to 140 km in depth except at mid-ocean ridges where the LAB is no deeper than the depth of the new crust being created. Seismic evidence shows that oceanic plates do thicken with age. This would suggest that the Lithosphere-Asthenosphere boundary underneath oceanic lithosphere also deepens with plate age. Data from ocean seismometers indicate a sharp age-dependent LAB beneath the Pacific and Philippine plates and has been interpreted as evidence for a thermal control of oceanic-lithosphere thickness.\n\nThe continental lithosphere contains an ancient, stable part known as the craton. The LAB is particularly difficult to study in these regions and evidence suggests that the lithosphere within this old part of the continent is at it thickest and even appears to exhibit large variations in thickness beneath the cratons, thus supporting the theory that lithosphere thickness and LAB depth are age-dependent. Depths of the LAB beneath these regions (also known as shields and platforms) are estimated to be between 200 and 250 km deep.\nBeneath Phanerozoic continental crust, the LAB is roughly 100 km deep.\n"}
{"id": "53937915", "url": "https://en.wikipedia.org/wiki?curid=53937915", "title": "Ljungström air preheater", "text": "Ljungström air preheater\n\nLjungström air preheater is an air preheater invented by the Swedish engineer Fredrik Ljungström (1875-1964). The patent was achieved in 1930.\n\nEven in a modern utility boiler provides up to 20 percent of the total heat transfer in the boiler process, but only represents 2 percent of the investment. \n\nThe factory and workshop activities and laboratories in Lidingö would remain throughout the 1920s, with some 70 personnel. In the 1930s it was used a film studio, and was finally demolished in the 1970s to give space for new industry premises.\n\nWith Fredrik Ljungström's technology of the air preheater implemented in a vast amount of modern power stations around the world until this day with total attributed worldwide fuel savings estimated to 4,960,000,000 tons of oil, \"few inventions have been as successful in saving fuel as the Ljungström Air Preheater\".\n\nIn 1995, the Ljungström air preheater was distinguished as the 44th International Historic Mechanical Engineering Landmark by the American Society of Mechanical Engineers.\n\n"}
{"id": "182734", "url": "https://en.wikipedia.org/wiki?curid=182734", "title": "Magnetohydrodynamic drive", "text": "Magnetohydrodynamic drive\n\nA magnetohydrodynamic drive or MHD accelerator is a method for propelling vehicles using only electric and magnetic fields with no moving parts, accelerating an electrically conductive propellant (liquid or gas) with magnetohydrodynamics. The fluid is directed to the rear and as a reaction, the vehicle accelerates forward.\n\nThe first studies examining MHD in the field of marine propulsion date back to the early 1960s.\n\nFew large-scale working prototypes have been built, as marine MHD propulsion remains impractical due to its low efficiency, limited by the low electrical conductivity of seawater. Increasing current density is limited by Joule heating and water electrolysis in the vicinity of electrodes, and increasing the magnetic field strength is limited by the cost, size and weight (as well as technological limitations) of electromagnets and the power available to feed them.\n\nStronger technical limitations apply to air-breathing MHD propulsion (where ambient air is ionized) that is still limited to theoretical concepts and early experiments.\n\nPlasma propulsion engines using magnetohydrodynamics for space exploration have also been actively studied as such electromagnetic propulsion offers high thrust and high specific impulse at the same time, and the propellant would last much longer than chemical rockets.\n\nThe working principle involves the acceleration of an electrically conductive fluid (which can be a liquid or an ionized gas called a plasma) by the Lorentz force, resulting from the cross product of an electric current (motion of charge carriers accelerated by an electric field applied between two electrodes) with a perpendicular magnetic field. The Lorentz force accelerates all charged particles (positive and negative species) in the same direction whatever their sign, and the whole fluid is dragged through collisions. As a reaction, the vehicle is put in motion in the opposite direction.\n\nThis is the same working principle as an electric motor (more exactly a linear motor) except that in an MHD drive, the solid moving rotor is replaced by the fluid acting directly as the propellant. As all electromagnetic devices, an MHD accelerator is reversible: if the ambient working fluid is moving relatively to the magnetic field, charge separation induces an electric potential difference that can be harnessed with electrodes: the device then acts as a power source with no moving part transforming the kinetic energy of the incoming fluid into electricity, called an MHD generator.\n\nAs the Lorentz force in an MHD converter does not act on a single isolated charged particle nor on electrons in a solid electrical wire, but on a continuous charge distribution in motion, it is a \"volumetric\" (body) force, a force per unit volume:\n\nwhere f is the \"force density\" (force per unit volume), \"ρ\" the charge density (charge per unit volume), E the electric field, J the current density (current per unit area) and B the magnetic field.\n\nMHD thrusters are classified in two categories according to the way the electromagnetic fields operate:\n\nAs induction MHD accelerators are electrodeless, they do not exhibit the common issues related to conduction systems (especially Joule heating, bubbles and redox from electrolysis) but need much more intense peak magnetic fields to operate. Since one of the biggest issues with such thrusters is the limited energy available on-board, induction MHD drives have not been developed out of the laboratory.\n\nBoth systems can put the working fluid in motion according to two main designs:\n\nInternal flow systems concentrate the MHD interaction in a limited volume, preserving stealth characteristics. External field systems on the contrary have the ability to act on a very large expanse of surrounding water volume with higher efficiency and the ability to decrease drag, increasing the efficiency even further.\n\nMHD is attractive for underwater military applications because it has no moving parts, which means that a good design might be silent, stealthy, reliable, and efficient. Additionally, the MHD design eliminates many of the wear and friction pieces of the drivetrain with a directly driven propeller by an engine.\nThe major problem with MHD is that with current technologies, it is more expensive, and much slower, than a propeller driven by an engine.\nThe extra expense is from the large generator that must be driven by an engine. Such a large generator is not required when an engine directly drives a propeller.\n\nThe first prototype, a 3-meter (10-feet) long submarine called EMS-1, was designed and tested in 1966 by Stewart Way, a professor of mechanical engineering at the University of California, Santa Barbara. Way, on leave from his job at Westinghouse Electric, assigned his senior year undergraduate students to build the operational unit. This MHD submarine operated on batteries delivering power to electrodes and electromagnets, which produced a magnetic field of 0.015 tesla. The cruise speed was about 0.4 meter per second (15 inches per second) during the test in the bay of Santa Barbara, California, in accordance with theoretical predictions.\n\nLater, a Japanese prototype, the 3.6-meter long \"ST-500\", achieved speeds of up to 0.6 m/s in 1979.\n\nIn 1991, the world's first full-size prototype \"Yamato 1\" was completed in Japan after 6 years of R&D by the Ship & Ocean Foundation (later known as the Ocean Policy Research Foundation). The ship successfully carried a crew of ten plus passengers at speeds of up to 15 km/h in Kobe Harbour in June 1992.\n\nSmall-scale ship models were later built and studied extensively in the laboratory, leading to successful comparisons between the measurements and the theoretical prediction of ship terminal speeds.\n\nMilitary research about underwater MHD propulsion included high-speed torpedoes, remotely operated underwater vehicles (ROV), autonomous underwater vehicles (AUV), up to larger ones such as submarines.\n\nFirst studies of the interaction of plasmas with hypersonic flows around vehicles date back to the late 1950s, with the concept of a new kind of thermal protection system for space capsules during high-speed reentry. As low-pressure air is naturally ionized at such very high velocities and altitude, it was thought to use the effect of a magnetic field produced by an electromagnet to replace thermal ablative shields by a \"magnetic shield\". Hypersonic ionized flow interacts with the magnetic field, inducing eddy currents in the plasma. The current combines with the magnetic field to give Lorentz forces that oppose the flow and detach the bow shock wave further ahead of the vehicle, lowering the heat flux which is due to the brutal recompression of air behind the stagnation point. Such passive flow control studies are still ongoing, but a large-scale demonstrator has yet to be built.\n\nActive flow control by MHD force fields on the contrary involves a direct and imperious action of forces to locally accelerate or slow down the airflow, modifying its velocity, direction, pressure, friction, heat flux parameters, in order to preserve materials and engines from stress, allowing hypersonic flight. It is a field of magnetohydrodynamics also called magnetogasdynamics, magnetoaerodynamics or magnetoplasma aerodynamics, as the working fluid is the air (a gas instead of a liquid) ionized to become electrically conductive (a plasma).\n\nAir ionization is achieved at high altitude (electrical conductivity of air increases as atmospheric pressure reduces according to Paschen's law) using various techniques: high voltage electric arc discharge, RF (microwaves) electromagnetic glow discharge, laser, e-beam or betatron, radioactive source… with or without seeding of low ionization potential alkali substances (like caesium) into the flow.\n\nMHD studies applied to aeronautics try to extend the domain of hypersonic planes to higher Mach regimes: \n\nThe Russian project Ayaks (Ajax) is an example of MHD-controlled hypersonic aircraft concept. A US program also exists to design a hypersonic MHD bypass system, the Hypersonic Vehicle Electric Power System (HVEPS). A working prototype was completed in 2017 under development by General Atomics and the University of Tennessee Space Institute, sponsored by the US Air Force Research Laboratory. These projects aim to develop MHD generators feeding MHD accelerators for a new generation of high-speed vehicles. Such MHD bypass systems are often designed around a scramjet engine, but easier to design turbojets are also considered, as well as subsonic ramjets.\n\nSuch studies covers a field of resistive MHD with magnetic Reynolds number ≪ 1 using nonthermal weakly ionized gases, making the development of demonstrators much more difficult to realize than for MHD in liquids. \"Cold plasmas\" with magnetic fields are subject to the electrothermal instability occurring at a critical Hall parameter, which makes full-scale developments difficult.\n\nMHD propulsion has been considered as the main propulsion system for both marine and space ships since there is no need to produce lift to counter the gravity of Earth in water (due to buoyancy) nor in space (due to weightlessness), which is ruled out in the case of flight in the atmosphere.\n\nNonetheless, considering the current problem of the electric power source solved (for example with the availability of a still missing multi-megawatt compact fusion reactor), one could imagine future aircraft of a new kind silently powered by MHD accelerators, able to ionize and direct enough air downward to lift several tonnes. As external flow systems can control the flow over the whole wetted area, limiting thermal issues at high speeds, ambient air would be ionized and radially accelerated by Lorentz forces around an axisymmetric body (shaped as a cylinder, a cone, a sphere…), the entire airframe being the engine. Lift and thrust would arise as a consequence of a pressure difference between the upper and lower surfaces, induced by the Coandă effect. In order to maximize such pressure difference between the two opposite sides, and since the most efficient MHD converters (with a high Hall effect) are disk-shaped, such MHD aircraft would be preferably flattened to take the shape of a biconvex lens. Having no wings nor airbreathing jet engines, it would share no similarities with conventional aircraft, but it would behave like a helicopter whose rotor blades would have been replaced by a \"purely electromagnetic rotor\" with no moving part, sucking the air downward. Such concepts of flying MHD disks have been developed in the peer review literature from the mid 1970s mainly by physicists Leik Myrabo with the Lightcraft, Subrata Roy with the Wingless Electromagnetic Air Vehicle (WEAV), and Jean-Pierre Petit, who showed such MHD aerodynes should take the shape of a concave saucer to be able to confine the plasma to the wall.\n\nThese futuristic visions have been advertised in the media although they still remain beyond the reach of modern technology.\n\nA number of experimental methods of spacecraft propulsion are based on magnetohydrodynamics. As this kind of MHD propulsion involves compressible fluids in the form of plasmas (ionized gases) it is also referred to as magnetogasdynamics or magnetoplasmadynamics.\n\nIn such electromagnetic thrusters, the working fluid is most of the time ionized hydrazine, xenon or lithium. Depending on the propellant used, it can be seeded with alkali such as potassium or caesium to improve its electrical conductivity. All charged species within the plasma, from positive and negative ions to free electrons, as well as neutral atoms by the effect of collisions, are accelerated in the same direction by the Lorentz \"body\" force, which results from the combination of a magnetic field with an orthogonal electric field (hence the name of \"cross-field accelerator\"), these fields not being in the direction of the acceleration. This is a fundamental difference with ion thrusters which rely on electrostatics to accelerate only positive ions using the Coulomb force along a high voltage electric field.\n\nFirst experimental studies involving cross-field plasma accelerators (square channels and rocket nozzles) date back to the late 1950s. Such systems provide greater thrust and higher specific impulse than conventional chemical rockets and even modern ion drives, at the cost of a higher required energy density.\n\nSome devices also studied nowadays besides cross-field accelerators include the magnetoplasmadynamic thruster sometimes referred to as the \"Lorentz Force Accelerator\" (LFA), and the electrodeless Pulsed inductive thruster (PIT).\n\nEven today, these systems are not ready to be launched in space as they still lack a suitable compact power source offering enough energy density (such as hypothetical fusion reactors) to feed the power-greedy electromagnets, especially pulsed inductive ones. The rapid ablation of electrodes under the intense thermal flow is also a concern. For these reasons, studies remain largely theoretical and experiments are still conducted in the laboratory, although over 60 years have passed since the first research in this kind of thrusters.\n\n\"Oregon,\" a ship in the Oregon Files series of books by author Clive Cussler, has a magnetohydrodynamic drive. This allows the ship to turn very sharply and brake instantly, instead of gliding for a few miles. In \"Valhalla Rising,\" Clive Cussler writes the same drive into the powering of Captain Nemo's \"Nautilus.\"\n\nThe film adaptation of \"The Hunt for Red October\" popularized the magnetohydrodynamic drive as a \"caterpillar drive\" for submarines, a nearly undetectable \"silent drive\" intended to achieve stealth in submarine warfare. In reality, the current traveling through the water would create gases and noise, and the magnetic fields would induce a detectable magnetic signature. In the novel from which the film was adapted, the caterpillar that \"Red October\" used was actually a pumpjet of the so-called \"tunnel drive\" type (the tunnels provided acoustic camouflage for the cavitation from the propellers).\n\nIn the Ben Bova novel, \"The Precipice,\" the ship where some of the action took place, \"Starpower 1,\" built to prove that exploration and mining of the Asteroid Belt was feasible and potentially profitable, had a magnetohydrodynamic drive mated to a fusion power plant.\n\n\n"}
{"id": "5948933", "url": "https://en.wikipedia.org/wiki?curid=5948933", "title": "Mewbourne College of Earth and Energy", "text": "Mewbourne College of Earth and Energy\n\nThe Mewbourne College of Earth and Energy is the earth science unit at the University of Oklahoma in Norman. Currently, the school has an enrollment of 931 students, of which 728 are undergraduates and 203 are graduates. \n\nThe College was chartered on January 1, 2006 under a reorganization. The College includes two schools: the Mewbourne School of Petroleum and Geological Engineering and the ConocoPhillips School of Geology and Geophysics, and the Oklahoma Geological Survey (a state agency mandated by the Oklahoma Constitution).\n\nThe main offices for the College are located in Sarkeys Energy Center on the northeast corner of campus.\n\nOn November 2, 2007, it was announced that the College would be renamed the Mewbourne College of Earth and Energy in order to honor Curtis Mewbourne, a resident of Tyler, Texas, a 1958 OU graduate in petroleum engineering and one of the college's most prolific donors (he is the namesake for the School of Petroleum and Geological Engineering). In 1982, he had endowed the Curtis Mewbourne Professorship in Petroleum Engineering. Then he made a $6 million gift in 2000 to the School of Petroleum and Geological Engineering. OU renamed the college in recognition of Melbourne's support. In November 2000, he challenged other alumni and supporters of the college to endow undergraduate scholarships and graduate fellowships for students in petroleum engineering, geological engineering, geology and geophysics. He said he would match the gifts made between then and March 2008.\n\n\nDavid Deming, professor at the University of Oklahoma, and global warming critic, later was transferred to the College of Arts and Sciences.\n\n"}
{"id": "27625957", "url": "https://en.wikipedia.org/wiki?curid=27625957", "title": "Microwave burn", "text": "Microwave burn\n\nMicrowave burns are burn injuries caused by thermal effects of microwave radiation absorbed in a living organism. In comparison with radiation burns caused by ionizing radiation, where the dominant mechanism of tissue damage is internal cell damage caused by free radicals, the primary damage mechanism of microwave radiation is by heat.\n\nMicrowave damage can manifest with a delay; pain or signs of skin damage can show some time after microwave exposure.\n\nThe depth of penetration depends on the frequency of the microwaves and the tissue type. The Active Denial System (\"pain ray\") is a less-lethal directed energy weapon that employs a microwave beam at 95 GHz; a two-second burst of the 95 GHz focused beam heats the skin to a temperature of 130 °F (54 °C) at a depth of 1/64th of an inch (0.4 mm) and is claimed to cause skin pain without lasting damage. Conversely, lower frequencies penetrate deeper; at 5.8 GHz (3.2  mm) the depth most of the energy is dissipated in the first millimeter of the skin; the 2.45 GHz frequency microwaves commonly used in microwave ovens can deliver energy deeper into the tissue; the generally accepted value is 17 mm for muscle tissue.\n\nAs lower frequencies penetrate deeper into the tissue, and as there are fewer nerve endings in deeper-located parts of the body, the effects of the radio frequency waves (and the damage caused) may not be immediately noticeable. The lower frequencies at high power densities present a significant risk.\n\nThe microwave absorption is directed by the dielectric constant of the tissue. At 2.5 GHz, this ranges from about 5 for adipose tissue to about 56 for the cardiac muscle. As the speed of electromagnetic waves is proportional to the reciprocal of the square root of the dielectric constant, the resulting wavelength in the tissue can drop to a fraction of the wavelength in air; e.g. at 10 GHz the wavelength can drop from 3 cm to about 3.4 mm.\n\nThe layers of the body can be approximated as a thin layer of epidermis, dermis, adipose tissue (subcutaneous fat), and muscle tissue. At dozens of gigahertz, the radiation is absorbed in the top fraction to top few millimeters of skin. Muscle tissue is a much more efficient absorber than fat, so at lower frequencies that can penetrate sufficiently deep, most energy gets deposited there. In a homogeneous medium, the energy/depth dependence is an exponential curve with the exponent depending on the frequency and tissue. For 2.5 GHz, the first millimeter of muscle tissue absorbs 11% of the heat energy, the first two millimeters together absorb 20%. For lower frequencies, the attenuation factors are much lower, the achievable heating depths are higher, and the temperature gradient within the tissue is lower.\n\nThe tissue damage depends primarily on the absorbed energy and the tissue sensitivity; it is a function of the microwave power density (which depends on the distance from the source and its power output), frequency, absorption rate in the given tissue, and the tissue sensitivity. Tissues with high water (resp. electrolyte) content show higher microwave absorption.\n\nThe degree of the tissue damage depends on both the achieved temperature and the length of exposure. For short times, higher temperatures can be tolerated.\n\nThe damage can be spread over a large area, when the source is a relatively distant energy radiator, or a very small (though possibly deep) area, when the body comes to a direct contact with the source (e.g. a wire or a connector pin).\n\nThe epidermis has high electrical resistance for lower frequencies; at higher frequencies, the energy penetrates through by capacitive coupling. Damage to epidermis has low extent unless the epidermis is very moist. The characteristic depth for lower-frequency microwave injury is about 1 cm. The heating rate of adipose tissue is much slower than of muscle tissue. Frequencies in millimeter wave range are absorbed in the topmost layer of skin, rich in thermal sensors. At lower frequencies, between 1–10 GHz, most of the energy is however absorbed in deeper layers; the threshold for cellular injury there lies at 42 °C while the pain threshold is at 45 °C, so a subjective perception may not be a reliable indicator of a harmful level of exposure at those frequencies.\n\nExposure to frequencies common in domestic and industrial sources rarely leads to significant skin damage; in such cases, the damage tends to be limited to upper limbs. Significant injury with erythema, blisters, pain, nerve damage and tissue necrosis can occur even with exposures as short as 2–3 seconds. Due to the deep penetration of these frequencies, the skin may be minimally affected and show no signs of damage, while muscles, nerves, and blood vessels may be significantly damaged. Sensory nerves are particularly sensitive to such damage; cases of persistent neuritis and compression neuropathy were reported after significant microwave exposures.\n\nMicrowave burns show some similarities with electrical burns, as the tissue damage is deep rather than superficial. Adipose tissue shows less degree of damage than muscles and other water-rich tissues. (In contrast, radiant heat, contact burns and chemical burns damage subcutaneous adipose tissue to higher extent than deeper muscle tissue.) Full-thickness biopsy of the area between burned and unburned skin shows layers of more and less damaged tissue (\"tissue sparing\"), layers of undamaged fat between damaged muscles; a pattern that is not present in conventional thermal or chemical burns. Cells subjected to electrical burns show microscopic nuclear streaming on histology examination; this feature is not present with microwave burns. Microwaves also deposit more energy to areas with low blood supply and to tissue interfaces.\n\nHot spots may be formed in the tissue, with a consequent higher absorption of microwave energy and even higher temperature achieved, with localized necrosis of the affected tissue following. Sometimes, the affected tissue can even be charred.\n\nMuscle tissue destruction can lead to myoglobinuria, with renal failure following in severe cases; this is similar to burns from electric current. Urinalysis and serum CPK, BUN and creatine tests are used to check for this condition.\n\nCases of severe conjunctivitis were reported after technicians looked into powered waveguides.\n\nMicrowave-induced cataracts have been reported. Experiments on rabbits and dogs, mostly in the UHF range of frequencies, shown that the ocular effects are confined to eyelids and conjuctiva (as e.g. anterior segment keratitis or iritis). Cataracts were observed at several workers exposed to radiofrequency radiation, but in some of the cases the cause was unrelated to the RF exposure and in the other cases the evidence was incomplete or inconclusive. Some sources however mention incidence of microwave-related injuries of ocular lens and retina and the possibility of thermal effects to cause cataracts or focal tissue burns (incl. keratitis).\n\nFor the near field 2.45 GHz frequency, the minimum power density to cause cataracts in rabbits was found to be 150 mW/cm for 100 minutes; a retrolental temperature of 41 °C was necessary to be achieved. When the eye temperature was kept low by external cooling, cataracts were not produced by higher field intensities; that supports the hypothesis of a thermal mechanism being involved.\n\nSensory nerves are particularly sensitive to microwave damage. Cases of persistent neuritis and compression neuropathy were reported after significant microwave exposures.\n\nWhen the temperature of the brain is raised to or above 42 °C, the blood–brain barrier permeability increases.\n\nA neuropathy due to peripheral nerve lesion, without visible external burns, can occur when the nerve is subjected to microwaves of sufficient power density. The damage mechanism is believed to be thermal. Radiofrequency waves and ultrasound can be used for temporary blocking of peripheral nerves during neurosurgical operations.\n\nThe thermal effects of microwaves can cause testicular degeneration and lower sperm count.\n\nPulmonary burn can be present when lungs are exposed; chest x-ray is used for diagnosing.\n\nExposure of abdomen may lead to bowel obstruction due to stenosis of the affected bowel; flat and upright abdominal x-ray is used to check for this condition.\n\nHousehold microwave ovens have shielding around the inside of the oven that prevents microwaves from leaking out, as well as safety interlocks that prevent the oven from operating when the door is open. Therefore, burns due to direct exposure to microwave energy (as opposed to touching hot food) should not occur under normal circumstances.\n\nThere are several cases of child abuse where an infant or child has been placed in a microwave oven. The typical feature of such injuries are well-defined burns on the skin nearest to the microwave emitter, and histology examination shows higher damage extent in tissues with high content of water (e.g. muscles) than in tissues with less water (e.g. adipose tissue).\n\nOne such case involved a teenage babysitter who admitted to having placed a child in the microwave oven for approximately sixty seconds. The child developed a third degree burn to the back, measuring 5 inches x 6 inches. The babysitter later took the child to the emergency department, where multiple skin grafts were placed on the back. There were no signs of lasting emotional, cognitive or physical effects. CT scan of the head was normal, and there were no cataracts.\n\nAnother case involved a five-week-old female infant that had multiple full-thickness burns totaling 11% of the body surface area. The mother claimed the infant had been near a microwave oven, but not inside it. The infant survived but required amputations of parts of one leg and one hand.\n\nAlso, there have been three alleged infant deaths caused by microwave ovens. In all these cases, the babies were placed within microwaves and died of subsequent injuries.\n\nA case of nerve damage by an exposure to radiation from a malfunctioning 600 watt microwave oven, operated for five seconds with the door open, with both arms and hands exposed, was reported. During exposure, there was a pulsating, burning sensation in all fingers. Erythema appeared on the back sides of both hands and arms. Four years later, denervation of median nerve, ulnar nerve, and radial nerve in both arms was shown on an electromyography test.\n\nThe first microwave oven injury was reported in 1973. Two women operated a microwave oven in a department store snack bar. After several years, the oven showed a malfunction manifesting by burning the food. The first woman noticed burning sensations in her fingers and very little pain or tenderness when nearby to the operating oven. A small lesion appeared on her left index finger, near the base of the fingernail. In the next four weeks, three fingers of her right hand became affected as well. Transverse ridging and deformations close to the nail base appeared on her fingernails. After five months since the initial symptoms, she visited a doctor; the examination found no abnormalities other than the nails. Topical steroid cream used over six weeks led to gradual improvement. The second woman experienced nail deformation at the same time as the first one, with the same clinical findings. The oven was returned to the manufacturer before the involvement of the doctor, and the amount of leakage could not be assessed.\n\nOn July 29, 1977, H.F., a 51-year-old teacher, was attempting to remove a casserole dish from her new 600-watt microwave oven. The oven signaled the end of the heating cycle, but the light and the cooking blower were on. During retrieval of the dish, she inserted two thirds of her bare forearms into the oven, for a total time of about five seconds. The oven was still operating. She felt \"hot pulsating sensation\" and burning in fingers and fingernails and a sensation of \"needles\" over the exposed areas. Jabbing pain, swelling, and red-orange discoloration of dorsal sides of both hands and forearms appeared shortly afterwards. The next day she sought medical help. Since then, she has undergone treatment with oral and topical cortisone, Grenz rays, ultrasound, and later acupuncture, without relief. Symptoms persisted, including high sensitivity to radiant heat (sun, desk lamp, etc.) and growing intolerance to pressure of clothes and to touch in hands and forearms. Neurological examinations in 1980 and 1981 did not yield a definite diagnosis. Neuronal latencies were within norm. Electromyography discovered denervation in the median nerve, ulnar nerve, and radial nerve on both arms. Severe reduction of number of sweat glands in the finger pulps, in comparison with a random control, was also found. The injury was determined to be caused by the full power of the magnetron; the pulsating sensation was caused either by the stirrer (a mechanical mirror distributing the microwave beam across the oven space to prevent formation of hot and cold spots), or by the arterial pulsation in combination with increased nerve sensitivity. Damage to the A beta fibers, A delta fibers, and group C nerve fibers was the cause of the burning sensation. The increased hypersensitivity to radiant heat is caused by the damage to the A beta, A delta, and polymodal nociceptors (the group C fibers); this damage is induced by a single-time overheating of the skin to 48.5–50 °C, and the resulting sensitivity persists for a long time. Degeneration of the alpha motor neurons is also caused by the exposure to heat and radiation. Most of the major nerve trunks were not affected. Damage to the A beta fibers (located in the skin), discovered by the two-point discrimination test, is permanent; the Pacinian corpuscles, Meissner corpuscles, and Merkel nerve endings, which degenerated after denervation, do not regenerate. The sympathetic nervous system was involved as well; the reduction in active sweat glands was caused by destruction of their innervation, the initial edema and reddening was also caused by sympathetic nerve damage.\n\nIn 1983, a 35-year-old male was heating a sandwich in a microwave oven at work. After opening the door, the magnetron did not shut off and his right hand was exposed to microwave radiation as he retrieved the sandwich. After exposure, his hand was pale and cold; 30 minutes later the man presented himself to a doctor, with paresthesia in all fingers and the hand still pale and cold. An Allen's test showed a return to normal color after 60 seconds (normal is 5 seconds). By 60 minutes after exposure the hand was normal again, and the patient was discharged without treatment. A week later there was no paresthesia, motor weakness nor sensory deficit.\n\nAn engineer replaced a woodpecker-damaged feed horn of a high-power microwave antenna, a 15-meter dish at an Earth station of a television network, using a cherry picker. After finishing, he sent his technician to power up the transmitter, and attempted to lower the cherry picker down. The engine failed and the engineer was stuck next to the antenna, outside of its main lobe but well within the first sidelobe. The technician, unaware that the engineer was still close to the antenna, powered it up. The engineer was exposed to an intense microwave field for about three minutes, until the error was realized. There were no immediate symptoms; the next morning the engineer detected blood and solid matter in his urine, and visited a doctor, who found blood in stool and massive bowel adhesions. The engineer's medical problems lasted for many years.\n\nDielectric heating (diathermy) is used in medicine; the frequencies used typically lie in the ultrasonic, shortwave, and microwave ranges. Careless application, especially when the patient has implanted metal conductors (e.g. cardiostimulator leads), can cause burns of skin and deeper tissues and even death.\n\nMicrowave damage to tissues can be intentionally exploited as a therapeutic technique, e.g. radiofrequency ablation and radiofrequency lesioning. Controlled destruction of tissue is performed for treatment of arrhythmia. Microwave coagulation can be used for some kinds of surgeries, e.g., stopping bleeding after a severe liver injury.\n\nMicrowave heating seems to cause more damage to bacteria than equivalent thermal-only heating. However food reheated in a microwave oven typically reaches lower temperature than classically reheated, therefore pathogens are more likely to survive.\n\nMicrowave heating of blood, e.g. for transfusion, is contraindicated, as it can cause hemolysis and hyperkalemia.\n\nMicrowave heating is one of the methods for inducing hyperthermia for hyperthermia therapy.\n\nHigh-energy microwaves are used in neurobiology experiments to kill small laboratory animals (mice, rats) in order to fix brain metabolites without the loss of anatomical integrity of the tissue. The instruments used are designed to focus most of the power to the animal's head. The unconsciousness and death is nearly instant, occurring in less than one second, and the method is the most efficient one to fix brain tissue chemical activity. A 2.45 GHz, 6.5 kW source will heat the brain of a 30 g mouse to 90 °C in about 325 milliseconds; a 915 MHz, 25 kW source will heat the brain of a 300 g rat to the same temperature in a second. Special devices designed or modified for this purpose have to be used; use of kitchen-grade microwave ovens is condemned.\n\nSafety limits exist for microwave exposure. The U.S. Occupational Safety and Health Administration defines energy density limit for exposure periods of 0.1 hours or more to 10 mW/cm; for shorter periods the limit is 1 mW-hr/cm with limited excursions above 10 mW/cm. The U.S. Food and Drug Administration (FDA) standard for microwave oven leakage puts limit to 5 mW/cm at 2 inches from the oven's surface.\n\nFor 5.8 GHz, exposure to 30 mW/cm causes increase of facial skin temperature by 0.48 °C, corneal surface heats by 0.7 °C, and the temperature of retina is estimated to increase by 0.08–0.03 °C.\n\nExposure of skin to microwaves can be perceived as a sensation of heat or pain. Due to lower penetration of higher frequencies, perception threshold is lower for higher frequencies as more energy is dissipated closer to the body surface. When the entire face is exposed to 10 GHz microwaves, the feeling of heat is evoked at energy densities of 4–6 mW/cm for 5 or more seconds, or about 10 mW/cm for a half second. Experiments on six volunteers exposed to 2.45 GHz microwaves shown perception thresholds on forearm skin to be at the average of 25–29 mW/cm, ranging from 15.40 to 44.25 mW/cm. The sensation was indistinguishable from heat delivered by infrared radiation, though the infrared radiation required about five times lower energy density. Pain threshold for 3 GHz was demonstrated to range from 0.83–3.1 W/cm for 9.5 cm of exposed area, depending on length of the exposure; other source says the dependence is not directly on the power density and exposure length, but primarily on the critical skin temperature.\n\nMicrowave energy can be focused by metal objects in the vicinity of the body or when implanted. Such focusing and resultant increased heating can significantly lower the perception, pain and damage thresholds. Metal-framed glasses perturb microwave fields between 2–12 GHz; individual components were found to be resonant between 1.4 and 3.75 GHz.\n\nA security guard with a metal plate in his leg experienced heating of the plate when patrolling near tropospheric scatter transmitter antennas; he had to be removed from their vicinity.\n\nIn the 30–300 GHz band, dry clothing may serve as an impedance transformer, facilitating more efficient energy coupling to the underlying skin.\n\nPulsed microwave radiation can be perceived by some workers as a phenomenon called \"microwave hearing\"; the irradiated personnel perceive auditory sensations of clicking or buzzing. The cause is thought to be thermoelastic expansion of portions of auditory apparatus. The auditory system response occurs at least from 200 MHz to at least 3 GHz. In the tests, repetition rate of 50 Hz was used, with pulse width between 10–70 microseconds. The perceived loudness was found to be linked to the peak power density instead of average power density. At 1.245 GHz, the peak power density for perception was below 80 mW/cm. The generally accepted mechanism is rapid (but minuscule, in the range of 10 °C) heating of brain by each pulse, and the resulting pressure wave traveling through skull to cochlea.\n\nSome vacuum tubes present in microwave installations tend to generate bremsstrahlung x-rays. Magnetrons and especially hydrogen thyratrons tend to be the worst offenders.\n\nAs the energy of radio frequency waves and microwaves is insufficient to directly disrupt individual chemical bonds in small or stable molecules, the effects are considered limited to thermal. Energy densities that are not sufficient to overheat the tissues are not shown to cause lasting damage. To clarify, the deep-red lightbulb in a black-and-white photographic darkroom produces a higher-energy form of radiation than microwaves. Like a microwave, this lightbulb can burn, particularly if touched, but the burn is only possible due to too much heat. A study of 20,000 radar technicians of the US Navy, who were chronically exposed to high levels of microwave radiation, did not detect increased incidence of cancer. Recent epidemiologic evidence also led to the consensus that exposure to electromagnetic fields, e.g. along power lines, did not raise incidence of leukemia or other cancers.\n\nA common myth among radar and microwave communication workers is that the exposure of the genital area to microwaves renders a man sterile for about a day. The power density necessary for this effect is however sufficient to also cause permanent damage.\n"}
{"id": "53959142", "url": "https://en.wikipedia.org/wiki?curid=53959142", "title": "Miller and Lents", "text": "Miller and Lents\n\nMiller and Lents, Ltd. is a petroleum consulting company based in Houston, Texas. The firm provides services including reserves certifications, audits, and independent evaluations. They prepare evaluations according to the standards of the United States Securities and Exchange Commission (SEC) Regulation S-X and the Petroleum Resources Management System (PRMS) published by the Society of Petroleum Engineers (SPE).\n\nChairman: Robert Oberst.\nPresident: Roy “Lee” Comer.\nSenior Vice Presidents: Gregory Armes, Leslie Fallon, Gary Knapp, Guy Miller and Katie Reinaker.\n\nMiller and Lents, Ltd. prepares reserves estimates by applying both SEC and SPE-PRMS standards. These estimates include the assessment of developed and undeveloped reserves and classification according to Proved, Probable, Possible, Contingent, and Prospective Resources definitions.\n\nThey also evaluate relevant economic parameters and creates financial reports for the United States Securities and Exchange Commission (SEC), the London Stock Exchange (LSE), and the Alternative Investment Market (AIM); cash flow projections; forecasts of future prices; and estimates of Fair Market Value.\n\nThey perform geologic studies including: seismic studies, structural studies, stratigraphic studies, subsurface mapping, field development studies, and reservoir characterization.\n\nIn addition, they perform petrophysical analyses such as log analysis and core analysis studies.\n\nMiller and Lents, Ltd. provides services to domestic and international clients, with a significant portion of their business coming from clients operating in Russia. In addition to evaluations for clients operating in Russia, Miller and Lents, Ltd. has performed evaluations for clients in the United States, Azerbaijan, Israel, Kazakhstan, the United Kingdom, Australia, and Lithuania, among others.\n\nIn 1948, J. R. Butler and Martin Miller formed an oil and gas consulting partnership known as J. R. Butler and Company. Max Lents, who was not a partner at the beginning of J. R. Butler and Company, was considered as an original founding partner when he joined the firm a year later. The company name then changed to Butler, Miller and Lents.\n\nIn 1970 its name was changed to Butler, Miller and Lents, Ltd., at which time it became a Subchapter S Corporation.\n\nIn 1976, the name of the firm was changed to its current name, Miller and Lents, Ltd. after J. R. Butler exchanged his interest in Butler, Miller and Lents, Ltd.\n\nIn addition to founding Miller and Lents, Ltd., Max Lents and Martin Miller made significant contributions to the field of petroleum engineering. They introduced the Miller-Lents Permeability Distribution which aids in describing the permeability of heterogeneous reservoirs and provides a “better match with actual field performance when applied to cycling operations in gas condensate reservoirs.”\n\nSteiber was a Petroleum Engineer with Miller and Lents, Ltd. from 1974 to 2004. He made significant contributions to the field of Petroleum Engineering and the practice of Oil and Gas Well log analysis. In his paper “The Distribution of Shale in Sandstones and its Effect upon Porosity,” co-authored by E.C. Thomas in 1975, he introduced the Thomas-Steiber Diagram which is still commonly used for log analysis today.\n"}
{"id": "19052", "url": "https://en.wikipedia.org/wiki?curid=19052", "title": "Molybdenum", "text": "Molybdenum\n\nMolybdenum is a chemical element with symbol Mo and atomic number 42. The name is from Neo-Latin \"molybdaenum\", from Ancient Greek \"\", meaning lead, since its ores were confused with lead ores. Molybdenum minerals have been known throughout history, but the element was discovered (in the sense of differentiating it as a new entity from the mineral salts of other metals) in 1778 by Carl Wilhelm Scheele. The metal was first isolated in 1781 by Peter Jacob Hjelm.\n\nMolybdenum does not occur naturally as a free metal on Earth; it is found only in various oxidation states in minerals. The free element, a silvery metal with a gray cast, has the sixth-highest melting point of any element. It readily forms hard, stable carbides in alloys, and for this reason most of world production of the element (about 80%) is used in steel alloys, including high-strength alloys and superalloys.\n\nMost molybdenum compounds have low solubility in water, but when molybdenum-bearing minerals contact oxygen and water, the resulting molybdate ion is quite soluble. Industrially, molybdenum compounds (about 14% of world production of the element) are used in high-pressure and high-temperature applications as pigments and catalysts.\n\nIn its pure form, molybdenum is a silvery-grey metal with a Mohs hardness of 5.5, and a standard atomic weight of 95.95 g/mol. It has a melting point of ; of the naturally occurring elements, only tantalum, osmium, rhenium, tungsten, and carbon have higher melting points. It has one of the lowest coefficients of thermal expansion among commercially used metals. The tensile strength of molybdenum wires increases about 3 times, from about 10 to 30 GPa, when their diameter decreases from ~50–100 nm to 10 nm.\n\nMolybdenum is a transition metal with an electronegativity of 2.16 on the Pauling scale. It does not visibly react with oxygen or water at room temperature. Weak oxidation of molybdenum starts at ; bulk oxidation occurs at temperatures above 600 °C, resulting in molybdenum trioxide. Like many heavier transition metals, molybdenum shows little inclination to form a cation in aqueous solution, although the Mo cation is known under carefully controlled conditions.\n\nThere are 35 known isotopes of molybdenum, ranging in atomic mass from 83 to 117, as well as four metastable nuclear isomers. Seven isotopes occur naturally, with atomic masses of 92, 94, 95, 96, 97, 98, and 100. Of these naturally occurring isotopes, only molybdenum-100 is unstable.\n\nMolybdenum-98 is the most abundant isotope, comprising 24.14% of all molybdenum. Molybdenum-100 has a half-life of about 10 y and undergoes double beta decay into ruthenium-100. Molybdenum isotopes with mass numbers from 111 to 117 all have half-lives of approximately 150 ns. All unstable isotopes of molybdenum decay into isotopes of niobium, technetium, and ruthenium.\n\nAs also noted below, the most common isotopic molybdenum application involves molybdenum-99, which is a fission product. It is a parent radioisotope to the short-lived gamma-emitting daughter radioisotope technetium-99m, a nuclear isomer used in various imaging applications in medicine.\nIn 2008, the Delft University of Technology applied for a patent on the molybdenum-98-based production of molybdenum-99.\n\nMolybdenum forms chemical compounds in oxidation states from -II to +VI. Higher oxidation states are more relevant to its terrestrial occurrence and its biological roles, mid-level oxidation states are often associated with metal clusters, and very low oxidation states are typically associated with organomolybdenum compounds. Mo and W chemistry shows strong similarities. The relative rarity of molybdenum(III), for example, contrasts with the pervasiveness of the chromium(III) compounds. The highest oxidation state is seen in molybdenum(VI) oxide (MoO), whereas the normal sulfur compound is molybdenum disulfide MoS.\n\nFrom the perspective of commerce, the most important compounds are molybdenum disulfide () and molybdenum trioxide (). The black disulfide is the main mineral. It is roasted in air to give the trioxide:\n\nThe trioxide, which is volatile at high temperatures, is the precursor to virtually all other Mo compounds as well as alloys. Molybdenum has several oxidation states, the most stable being +4 and +6 (bolded in the table at left).\n\nMolybdenum(VI) oxide is soluble in strong alkaline water, forming molybdates (MoO). Molybdates are weaker oxidants than chromates. They tend to form structurally complex oxyanions by condensation at lower pH values, such as [MoO] and [MoO]. Polymolybdates can incorporate other ions, forming polyoxometalates. The dark-blue phosphorus-containing heteropolymolybdate P[MoO] is used for the spectroscopic detection of phosphorus. The broad range of oxidation states of molybdenum is reflected in various molybdenum chlorides:\nMolybdenum(VI) chloride MoCl is not known, although the molybdenum hexafluoride is well characterized.\n\nLike chromium and some other transition metals, molybdenum forms quadruple bonds, such as in Mo(CHCOO) and [MoCl], which also has a quadruple bond.\n\nThe oxidation state 0 is possible with carbon monoxide as ligand, such as in molybdenum hexacarbonyl, Mo(CO).\n\nMolybdenite—the principal ore from which molybdenum is now extracted—was previously known as molybdena. Molybdena was confused with and often utilized as though it were graphite. Like graphite, molybdenite can be used to blacken a surface or as a solid lubricant. Even when molybdena was distinguishable from graphite, it was still confused with the common lead ore PbS (now called galena); the name comes from Ancient Greek \"\", meaning \"lead\". (The Greek word itself has been proposed as a loanword from Anatolian Luvian and Lydian languages).\n\nAlthough (reportedly) molybdenum was deliberately alloyed with steel in one 14th-century Japanese sword (mfd. ca. 1330), that art was never employed widely and was later lost. In the West in 1754, Bengt Andersson Qvist examined a sample of molybdenite and determined that it did not contain lead and thus was not galena.\n\nBy 1778 Swedish chemist Carl Wilhelm Scheele stated firmly that molybdena was (indeed) neither galena nor graphite. Instead, Scheele correctly proposed that molybdena was an ore of a distinct new element, named \"molybdenum\" for the mineral in which it resided, and from which it might be isolated. Peter Jacob Hjelm successfully isolated molybdenum using carbon and linseed oil in 1781.\n\nFor the next century, molybdenum had no industrial use. It was relatively scarce, the pure metal was difficult to extract, and the necessary techniques of metallurgy were immature. Early molybdenum steel alloys showed great promise of increased hardness, but efforts to manufacture the alloys on a large scale were hampered with inconsistent results, a tendency toward brittleness, and recrystallization. In 1906, William D. Coolidge filed a patent for rendering molybdenum ductile, leading to applications as a heating element for high-temperature furnaces and as a support for tungsten-filament light bulbs; oxide formation and degradation require that molybdenum be physically sealed or held in an inert gas. In 1913, Frank E. Elmore developed a froth flotation process to recover molybdenite from ores; flotation remains the primary isolation process.\n\nDuring World War I, demand for molybdenum spiked; it was used both in armor plating and as a substitute for tungsten in high speed steels. Some British tanks were protected by 75 mm (3 in) manganese steel plating, but this proved to be ineffective. The manganese steel plates were replaced with much lighter molybdenum steel plates allowing for higher speed, greater maneuverability, and better protection. The Germans also used molybdenum-doped steel for heavy artillery, like in the super-heavy howitzer Big Bertha, because traditional steel melts at the temperatures produced by the propellant of the one ton shell. After the war, demand plummeted until metallurgical advances allowed extensive development of peacetime applications. In World War II, molybdenum again saw strategic importance as a substitute for tungsten in steel alloys.\n\nMolybdenum is the 54th most abundant element in the Earth's crust and the 25th most abundant element in its oceans, with an average of 10 parts per billion; it is the 42nd most abundant element in the Universe. The Russian Luna 24 mission discovered a molybdenum-bearing grain (1 × 0.6 µm) in a pyroxene fragment taken from Mare Crisium on the Moon. The comparative rarity of molybdenum in the Earth's crust is offset by its concentration in a number of water-insoluble ores, often combined with sulfur in the same way as copper, with which it is often found. Though molybdenum is found in such minerals as wulfenite (PbMoO) and powellite (CaMoO), the main commercial source is molybdenite (MoS). Molybdenum is mined as a principal ore and is also recovered as a byproduct of copper and tungsten mining.\n\nThe world's production of molybdenum was 250,000 tonnes in 2011, the largest producers being China (94,000 t), the United States (64,000 t), Chile (38,000 t), Peru (18,000 t) and Mexico (12,000 t). The total reserves are estimated at 10 million tonnes, and are mostly concentrated in China (4.3 Mt), the US (2.7 Mt) and Chile (1.2 Mt). By continent, 93% of world molybdenum production is about evenly shared between North America, South America (mainly in Chile), and China. Europe and the rest of Asia (mostly Armenia, Russia, Iran and Mongolia) produce the remainder.\n\nIn molybdenite processing, the ore is first roasted in air at a temperature of . The process gives gaseous sulfur dioxide and the molybdenum(VI) oxide:\n\nThe oxidized ore is then usually extracted with aqueous ammonia to give ammonium molybdate:\nCopper, an impurity in molybdenite, is less soluble in ammonia. To completely remove it from the solution, it is precipitated with hydrogen sulfide. Ammonium molybdate converts to ammonium dimolybdate, which is isolated as a solid. Heating this solid gives molybdenum trioxide:\nCrude trioxide can be further purified by sublimation at .\n\nMetallic molybdenum is produced by reduction of the oxide with hydrogen:\n\nThe molybdenum for steel production is reduced by the aluminothermic reaction with addition of iron to produce ferromolybdenum. A common form of ferromolybdenum contains 60% molybdenum.\n\nMolybdenum had a value of approximately $30,000 per tonne as of August 2009. It maintained a price at or near $10,000 per tonne from 1997 through 2003, and reached a peak of $103,000 per tonne in June 2005. In 2008, the London Metal Exchange announced that molybdenum would be traded as a commodity.\n\nHistorically, the Knaben mine in southern Norway, opened in 1885, was the first dedicated molybdenum mine. It was closed in 1973 but was reopened in 2007. and now produces of molybdenum disulfide per year. Large mines in Colorado (such as the Henderson mine and the Climax mine) and in British Columbia yield molybdenite as their primary product, while many porphyry copper deposits such as the Bingham Canyon Mine in Utah and the Chuquicamata mine in northern Chile produce molybdenum as a byproduct of copper mining.\n\nAbout 86% of molybdenum produced is used in metallurgy, with the rest used in chemical applications. The estimated global use is structural steel 35%, stainless steel 25%, chemicals 14%, tool & high-speed steels 9%, cast iron 6%, molybdenum elemental metal 6%, and superalloys 5%.\n\nMolybdenum can withstand extreme temperatures without significantly expanding or softening, making it useful in environments of intense heat, including military armor, aircraft parts, electrical contacts, industrial motors, and filaments.\n\nMost high-strength steel alloys (for example, 41xx steels) contain 0.25% to 8% molybdenum. Even in these small portions, more than 43,000 tonnes of molybdenum are used each year in stainless steels, tool steels, cast irons, and high-temperature superalloys.\n\nMolybdenum is also valued in steel alloys for its high corrosion resistance and weldability. Molybdenum contributes corrosion resistance to type-300 stainless steels (specifically type-316) and especially so in the so-called superaustenitic stainless steels (such as alloy AL-6XN, 254SMO and 1925hMo). Molybdenum increases lattice strain, thus increasing the energy required to dissolve iron atoms from the surface. Molybdenum is also used to enhance the corrosion resistance of ferritic (for example grade 444) and martensitic (for example 1.4122 and 1.4418) stainless steels.\n\nBecause of its lower density and more stable price, molybdenum is sometimes used in place of tungsten. An example is the 'M' series of high-speed steels such as M2, M4 and M42 as substitution for the 'T' steel series, which contain tungsten. Molybdenum can also be used as a flame-resistant coating for other metals. Although its melting point is , molybdenum rapidly oxidizes at temperatures above making it better-suited for use in vacuum environments.\n\nTZM (Mo (~99%), Ti (~0.5%), Zr (~0.08%) and some C) is a corrosion-resisting molybdenum superalloy that resists molten fluoride salts at temperatures above . It has about twice the strength of pure Mo, and is more ductile and more weldable, yet in tests it resisted corrosion of a standard eutectic salt (FLiBe) and salt vapors used in molten salt reactors for 1100 hours with so little corrosion that it was difficult to measure.\n\nOther molybdenum-based alloys that do not contain iron have only limited applications. For example, because of its resistance to molten zinc, both pure molybdenum and molybdenum-tungsten alloys (70%/30%) are used for piping, stirrers and pump impellers that come into contact with molten zinc.\n\n\n\nMolybdenum is an essential element in most organisms. In fact a scarcity of molybdenum in the Earth's early oceans may have strongly influenced evolution of eukaryotic life (which includes all plants and animals).\n\nAt least 50 molybdenum-containing enzymes have been identified, mostly in bacteria. those enzymes include aldehyde oxidase, sulfite oxidase and xanthine oxidase. With one exception, Mo in proteins is bound by molybdopterin to give the molybdenum cofactor.\n\nIn terms of function, molybdoenzymes catalyze the oxidation and sometimes reduction of certain small molecules in the process of regulating nitrogen, sulfur, and carbon. In some animals, and in humans, the oxidation of xanthine to uric acid, a process of purine catabolism, is catalyzed by xanthine oxidase, a molybdenum-containing enzyme. The activity of xanthine oxidase is directly proportional to the amount of molybdenum in the body. However, an extremely high concentration of molybdenum reverses the trend and can act as an inhibitor in both purine catabolism and other processes. Molybdenum concentration also affects protein synthesis, metabolism, and growth.\n\nMo is as a component in most nitrogenases. Among molybdoenzymes, nitrogenases are unique in lacking the molybdopterin. Nitrogenases catalyze the production of ammonia from atmospheric nitrogen:\nThe biosynthesis of the FeMoco active site is highly complex.\n\nMolybdate is transported in the body as MoO.\n\nMolybdenum is an essential trace dietary element. Four mammalian Mo-dependent enzymes are known, all of them harboring a pterin-based molybdenum cofactor (Moco) in their active site: sulfite oxidase, xanthine oxidoreductase, aldehyde oxidase, and mitochondrial amidoxime reductase. People severely deficient in molybdenum have poorly functioning sulfite oxidase and are prone to toxic reactions to sulfites in foods. The human body contains about 0.07 mg of molybdenum per kilogram of body weight, with higher concentrations in the liver and kidneys and lower in the vertebrae. Molybdenum is also present within human tooth enamel and may help prevent its decay.\n\nAcute toxicity has not been seen in humans, and the toxicity depends strongly on the chemical state. Studies on rats show a median lethal dose (LD) as low as 180 mg/kg for some Mo compounds. Although human toxicity data is unavailable, animal studies have shown that chronic ingestion of more than 10 mg/day of molybdenum can cause diarrhea, growth retardation, infertility, low birth weight, and gout; it can also affect the lungs, kidneys, and liver. Sodium tungstate is a competitive inhibitor of molybdenum. Dietary tungsten reduces the concentration of molybdenum in tissues.\n\nLow soil concentration of molybdenum in a geographical band from northern China to Iran results in a general dietary molybdenum deficiency, and is associated with increased rates of esophageal cancer. Compared to the United States, which has a greater supply of molybdenum in the soil, people living in those areas have about 16 times greater risk for esophageal squamous cell carcinoma.\n\nMolybdenum deficiency has also been reported as a consequence of non-molybdenum supplemented total parenteral nutrition (complete intravenous feeding) for long periods of time. It results in high blood levels of sulfite and urate, in much the same way as molybdenum cofactor deficiency. However (presumably since pure molybdenum deficiency from this cause occurs primarily in adults), the neurological consequences are not as marked as in cases of congenital cofactor deficiency.\n\nA congenital molybdenum cofactor deficiency disease, seen in infants, is an inability to synthesize molybdenum cofactor, the heterocyclic molecule discussed above that binds molybdenum at the active site in all known human enzymes that use molybdenum. The resulting deficiency results in high levels of sulfite and urate, and neurological damage.\n\nHigh levels of molybdenum can interfere with the body's uptake of copper, producing copper deficiency. Molybdenum prevents plasma proteins from binding to copper, and it also increases the amount of copper that is excreted in urine. Ruminants that consume high levels of molybdenum suffer from diarrhea, stunted growth, anemia, and achromotrichia (loss of fur pigment). These symptoms can be alleviated by copper supplements, either dietary and injection. The effective copper deficiency can be aggravated by excess sulfur.\n\nCopper reduction or deficiency can also be deliberately induced for therapeutic purposes by the compound ammonium tetrathiomolybdate, in which the bright red anion tetrathiomolybdate is the copper-chelating agent. Tetrathiomolybdate was first used therapeutically in the treatment of copper toxicosis in animals. It was then introduced as a treatment in Wilson's disease, a hereditary copper metabolism disorder in humans; it acts both by competing with copper absorption in the bowel and by increasing excretion. It has also been found to have an inhibitory effect on angiogenesis, potentially by inhibiting the membrane translocation process that is dependent on copper ions. This is a promising avenue for investigation of treatments for cancer, age-related macular degeneration, and other diseases that involve a pathologic proliferation of blood vessels.\n\nIn 2000, the then U.S. Institute of Medicine (now the National Academy of Medicine, NAM) updated its Estimated Average Requirements (EARs) and Recommended Dietary Allowances (RDAs) for molybdenum. If there is not sufficient information to establish EARs and RDAs, an estimate designated Adequate Intake (AI) is used instead.\n\nAn AI of 2 micrograms (μg) of molybdenum per day was established for infants up to 6 months of age, and 3 μg/day from 7 to 12 months of age, both for males and females. For older children and adults, the following daily RDAs have been established for molybdenum: 17 μg from 1 to 3 years of age, 22 μg from 4 to 8 years, 34 μg from 9 to 13 years, 43 μg from 14 to 18 years, and 45 μg for persons 19 years old and older. All these RDAs are valid for both sexes. Pregnant or lactating females from 14 to 50 years of age have a higher daily RDA of 50 μg of molybdenum.\n\nAs for safety, the NAM sets tolerable upper intake levels (ULs) for vitamins and minerals when evidence is sufficient. In the case of molybdenum, the UL is 2000 μg/day. Collectively the EARs, RDAs, AIs and ULs are referred to as Dietary Reference Intakes (DRIs).\n\nThe European Food Safety Authority (EFSA) refers to the collective set of information as Dietary Reference Values, with Population Reference Intake (PRI) instead of RDA, and Average Requirement instead of EAR. AI and UL defined the same as in United States. For women and men ages 15 and older the AI is set at 65 μg/day. Pregnant and lactating women have the same AI. For children aged 1–14 years, the AIs increase with age from 15 to 45 μg/day. The adult AIs are higher than the U.S. RDAs, but on the other hand, the European Food Safety Authority reviewed the same safety question and set its UL at 600 μg/day, which is much lower than the U.S. value.\n\nFor U.S. food and dietary supplement labeling purposes, the amount in a serving is expressed as a percent of Daily Value (%DV). For molybdenum labeling purposes 100% of the Daily Value was 75 μg, but as of May 27, 2016 it was revised to 45 μg. A table of the old and new adult Daily Values is provided at Reference Daily Intake. The original deadline to be in compliance was July 28, 2018, but on September 29, 2017 the Food and Drug Administration (FDA) released a proposed rule that extended the deadline to January 1, 2020 for large companies and January 1, 2021 for small companies.\n\nAverage daily intake varies between 120 and 240 μg/day, which is higher than dietary recommendations. Pork, lamb, and beef liver each have approximately 1.5 parts per million of molybdenum. Other significant dietary sources include green beans, eggs, sunflower seeds, wheat flour, lentils, cucumbers, and cereal grain.\n\nMolybdenum dusts and fumes, generated by mining or metalworking, can be toxic, especially if ingested (including dust trapped in the sinuses and later swallowed). Low levels of prolonged exposure can cause irritation to the eyes and skin. Direct inhalation or ingestion of molybdenum and its oxides should be avoided. OSHA regulations specify the maximum permissible molybdenum exposure in an 8-hour day as 5 mg/m. Chronic exposure to 60 to 600 mg/m can cause symptoms including fatigue, headaches and joint pains. At levels of 5000 mg/m, molybdenum is immediately dangerous to life and health.\n\n\n"}
{"id": "13323279", "url": "https://en.wikipedia.org/wiki?curid=13323279", "title": "NEK EAD", "text": "NEK EAD\n\nNatsionalna Elektricheska Kompania EAD (NEK) ( ; ) is a single-owned joint-stock electric company headquartered in Sofia, Bulgaria. Bulgarian Energy Holding is the holder of the capital of NEK. \n\nThe main company's activities are the generation of electrical energy, purchase and sale of electrical energy, import and export of electrical energy. NEK is the owner of 30 hydro and pumped storage power plants with a total installed capacity of 2713 MW. Most of the hydropower is generated within four hydropower cascades: Belmeken-Sestrimo-Chaira; Batak, Vacha, and Dolna Arda. All are used to cover peak loads, and to regulate the grid system.\n\nNEK is a holder of hydro (HPP&PS) generation license, electricity trading license and public power supply license, all issued by SEWRC.\n\nNEK, through its branch Dams and Cascades, manages 40 dams and the total capacity of the storage reservoirs operated by NEK represents 50.1% of the total controlled water resources of the country. .\n"}
{"id": "54364819", "url": "https://en.wikipedia.org/wiki?curid=54364819", "title": "Oil regeneration", "text": "Oil regeneration\n\nOil regeneration - is extraction of contaminants from oil in order to restore its original properties to be used equally with fresh oils.\n\nAging is a result of physical and chemical processes that change oil during storage and use in machines and mechanisms.\nThe main cause of aging is exposure to high temperatures and contact with air that leads to oxidation, decomposition, polymerization and condensation of hydrocarbons. Another cause of aging is contamination with metal particles, water and dust. Their accumulation leads to build up of slurries, resinous and asphaltic compounds, coke, soot, various salts and acids in the oils.\nThe oil in which aging process occurs, cannot fully perform its functions. Therefore, it is either replaced with new oil or regenerated.\n\nPhysical methods of regeneration do not change the chemical properties of oil. They remove only mechanical impurities (metal particles, sand, dust, as well as tar, asphalt and coke-like substances, water).\nRegeneration by physical methods include:\n\nPhysicochemical methods are based on the use of coagulants and adsorbents. Coagulants promote the coarsening and precipitation of fine-dispersed asphalt-resinous substances in oil. Adsorbents selectively absorb organic and inorganic compounds. These methods remove asphalt and resinous compounds, emulsified and dissolved water from oil. Adsorptive treatment with bleaching clays neutralizes free acid in acid-treated oil, unstable oxidized and sulphurized products as well as traces of sulphonic acid. In addition, clay treatment leads to higher resistance to oil oxidation at high temperatures and increased colour stability. This process is used in clay polishing plants for waste oil re-refining and transformer oil regeneration systems for the reclamation of old transformer oil to as-new condition.\n\nChemical methods of regeneration remove asphalt, silicic, acidic, some hetero-organic compounds and water from oils. These methods are based on the interaction of contaminating substances in oil with special reagents introduced into them. The compounds formed as a result of these chemical reactions are then easily removed from oil. Chemical methods include acid and alkaline refining, drying with calcium sulphate or reduction with metal hydrides.\n\nIn practice, to achieve a complete regeneration of oil using only one method is difficult. Therefore, a combination of different approaches are often used.\nThe choice of methods is influenced by factors such as the nature and origin of aging products, the requirements for regenerated oils, the amount of collected waste oil.\n\n"}
{"id": "18705770", "url": "https://en.wikipedia.org/wiki?curid=18705770", "title": "Q and R Holes", "text": "Q and R Holes\n\nThe Q and R Holes are a series of concentric sockets which currently represent the earliest known evidence for a stone structure on the site of Stonehenge.\n\nBeneath the turf and just inside the later Sarsen Circle are a double arc of buried stoneholes, the only surviving evidence of the first stone structure (possibly a double stone circle) erected within the centre of Stonehenge (Figs.1 & 2) and currently regarded as instigating the period known as Stonehenge Phase 3i. This phase tentatively began as early as 2600 BC, although recent radiocarbon dates from samples retrieved from one of the sockets in 2008 during excavations by Darvill and Wainwright suggest a date of around 2400 to 2300 BC. The final report is yet to be published, but some interesting results follow from the partial excavation of Q Hole 13 where 'associations with Beaker pottery' where noted. Although first encountered by William Hawley in the 1920s, it was Richard Atkinson who formally identified and named these irregular settings in 1954: ‘In choosing this designation, I had in mind John Aubrey’s frequent use, as a marginal note…of the phrase ‘quaere quot’ – ‘inquire how many’ – which seemed appropriate to the occasion.’ Their place at the beginning of the stone monument phase has been recognized from their stratigraphic relationships: in places they were cut through by both the settings of the later and still partly surviving Bluestone Circle and also by a stonehole dug for one of the uprights of the Sarsen Circle.\nThe diameter of the outer (Q) circuit is c. 26.2 m and that of the inner (R) is, 22.5 m; with an average spacing between the paired stone settings of 1.5 m. These trench-like intrusions are roughly 2 m long and 1 m wide, set radially and slightly enlarged at each end to provide paired stone sockets to a depth of around 0.6 m, the intervening strip generally re-filled with chalk rubble. Atkinson described them as being ‘dumb-bell’ shaped, although not all were of this form. The bases of some sockets bore ‘the impressions…of heavy stones’ some with ‘minute chips of dolerite [i.e. bluestone] embedded. While this does not imply that \"only\" bluestones were used in the Q and R structure he found no evidence for sarsens. His accounts make it clear that he believed the sockets to have exclusively located bluestones 'presumably the same stones that are still at Stonehenge'.\n\nAtkinson estimated that if the Q and R Holes originally formed a complete circle that 38 pairs would have been present, although recent computer-modeling shows that there is actually room for 40. The Q and R Holes not only represent the foundation cuts for the first central stone construction, but they also were to include several additional stone settings on the northeast. This modified group face the midsummer sunrise with a possible reciprocal stone aligned on the midwinter sunset. This is the first evidence for any unambiguous alignment at Stonehenge (the solstice axis). The analysis of the spacing between the Q and R array, and that of the modified (inset) portal group (Fig.3) imply a shift from an angular splay of 9 degrees (i.e. 40 settings) to 12 degrees, the same as that of the later 30 Sarsen Circle. How long the bluestones remained in the Q and R settings before they were removed (if indeed this early structure was ever completed) is not known. However the dates suggested from the 2008 excavation (above) implies the Q & R arrays were perhaps no earlier than 2,400 BC, presenting a challenge to the recently accepted Late Neolithic date for the construction of the iconic sarsen monument.\n"}
{"id": "3192041", "url": "https://en.wikipedia.org/wiki?curid=3192041", "title": "Radiophobia", "text": "Radiophobia\n\nRadiophobia is an obsessive fear of ionizing radiation, in particular, fear of X-rays. While in some cases radiation may be harmful (i.e. radiation-induced cancer, and acute radiation syndrome), the effects of poor information, understanding, or a traumatic experience may cause unnecessary or even irrational fear. The term is also used in a non-medical sense to describe the opposition to the use of nuclear technology (i.e. nuclear power) arising from concerns disproportionately greater than actual risks would merit.\n\nThe term was used in a paper entitled \"Radio-phobia and radio-mania\" presented by Dr Albert Soiland of Los Angeles in 1903. In the 1920s, the term was used to describe people who were afraid of radio broadcasting and receiving technology. In 1931, radiophobia was referred to in \"The Salt Lake Tribune\" as a \"fear of loudspeakers\", an affliction that Joan Crawford was reported as suffering. The term \"radiophobia\" was also printed in Australian newspapers in the 1930s and 1940s, assuming a similar meaning. The 1949 poem by Margarent Mercia Baker entitled \"Radiophobia\" laments the intrusion of advertising into radio broadcasts. The term remained in use with its original association with radios and radio broadcasting during the 1940s and 1950s. \n\nDuring the 1950s and 1960s, the Science Service associated the term with fear of gamma radiation and the medical use of x-rays. A Science Service article published in several American newspapers proposed that \"radiophobia\" could be attributed to the publication of information regarding the \"genetic hazards\" of exposure to ionising radiation by the National Academy of Sciences in 1956.\n\nIn a newspaper column published in 1970, Dr Harold Pettit MD wrote:\"A healthy respect for the hazards of radiation is desirable. When atomic testing began in the early fifties, these hazards were grossly exaggerated, producing a new psychological disorder which has been called \"radiophobia\" or \"nuclear neurosis\".\n\nMarch 1, 1954, the operation Castle Bravo testing of a then, first of its kind, experimental thermonuclear \"Shrimp\" device, overshot its predicted yield of 4–6 megatons and instead produced 15 megatons; this resulted in an unanticipated amount of \"Bikini snow\" or visible particles of nuclear fallout being produced, fallout which caught the Japanese fishing boat the Daigo Fukuryū Maru or \"Lucky Dragon\" in its plume, even though it was fishing outside the initially predicted ~5 megaton fallout area which had been cordoned off for the Castle Bravo test. Approximately 2 weeks after the test and fallout exposure, the 23-member fishing crew began to fall ill, with acute radiation sickness, largely brought on by beta burns that were caused by direct contact between the \"Bikini snow\" fallout and their skin, through their practice of scooping the \"Bikini snow\" into bags with their bare hands. One member of the crew, Kuboyama Aikichi the boat's chief radioman, died 7 months later, on September 23, 1954. It was later estimated that about a hundred fishing boats were contaminated to some degree by fallout from the test. Inhabitants of the Marshall Islands were also exposed to fallout, and a number of islands had to be evacuated.\n\nThis incident, due to the era of secrecy around nuclear weapons, created widespread fear of uncontrolled and unpredictable nuclear weapons, and also of radioactively contaminated fish affecting the Japanese food supply. With the publication of Joseph Rotblat's findings that the contamination caused by the fallout from the Castle Bravo test was nearly a thousand times greater than that stated officially, outcry in Japan reached such a level that the incident was dubbed by some as \"a second Hiroshima\". To prevent the subsequent strong anti-nuclear movement from turning into an anti-American movement, the Japanese and U.S. governments agreed on compensation of 2 million dollars for the contaminated fishery, with the surviving 22 crew men receiving about ¥ 2 million each, ($5,556 in 1954, $ in 2018)\n\nThe surviving crew members, and their family, would later experience prejudice and discrimination, as local people thought that radiation was contagious.\n\nThe Castle Bravo test and the new fears of radioactive fallout inspired a new direction in art and cinema. The Godzilla films, beginning with Ishirō Honda's landmark 1954 film \"Gojira\", are strong metaphors for post-war radiophobia. The opening scene of Gojira echoes the story of the Daigo Fukuryū Maru, from the initial distant flash of light to survivors being found with radiation burns. Although he found the special effects unconvincing, Roger Ebert stated that the film was \"an important one\" and \"properly decoded, was the Fahrenheit 9/11 of its time.\"\n\nA year after the Castle Bravo test, Akira Kurosawa examined one person's unreasoning terror of radiation and nuclear war in his 1955 film \"I Live in Fear\". At the end of the film, the foundry worker who lives in fear has been declared incompetent by his family, but the possible partial validity of his fears has transferred over to his doctor.\n\nNevil Shute's 1957 novel \"On the Beach\" depicts a future just six years later, based on the premise that a nuclear war has released so much radioactive fallout that all life in the Northern Hemisphere has been killed. The novel is set in Australia, which, along with the rest of the Southern Hemisphere, awaits a similar and inevitable fate. Helen Caldicott describes reading the novel in adolescence as 'a formative event' in her becoming part of the anti-nuclear movement.\n\nIn the former Soviet Union many patients with negligible radioactive exposure after the Chernobyl disaster displayed extreme anxiety about low level radiation exposure, and therefore developed many psychosomatic problems, with an increase in fatalistic alcoholism also being observed. As Japanese health and radiation specialist Shunichi Yamashita noted: \n\nThe term \"radiation phobia syndrome\" was introduced in 1987. by L. A. Ilyin and O. A. Pavlovsky in their report \"Radiological consequences of the Chernobyl accident in the Soviet Union and measures taken to mitigate their impact\".\n\nThe author of \"Chernobyl Poems\" Lyubov Sirota wrote in her poem \"Radiophobia\":\nIs this only—a fear of radiation? <br>\nPerhaps rather—a fear of wars? <br>\nPerhaps—the dread of betrayal, <br>\nCowardice, stupidity, lawlessness?\n\nThe term has been criticized by Adolph Kharash, Science Director at the Moscow State University because, he writes, It treats the normal impulse to self-protection, natural to everything living, your moral suffering, your anguish and your concern about the fate of your children, relatives and friends, and your own physical suffering and sickness as a result of delirium, of pathological perversion\n\nHowever, it must be noted that the psychological phobia of radiation in sufferers may not coincide with an actual life-threatening exposure to an individual or their children. Radiophobia refers only to a display of anxiety disproportionate to the actual quantity of radiation one is exposed to, with, in many cases, radiation exposure values equal to, or not much higher than, that which individuals are naturally exposed to every day from background radiation. Anxiety following a response to an actual life-threatening level of exposure to radiation is not considered to be radiophobia, nor misplaced anxiety, but a normal, appropriate response.\n\nMarvin Goldman is an American doctor who provided commentary to newspapers claiming that radiophobia had taken a larger toll than the fallout itself had, and that radiophobia was to blame. \n\nFollowing the accident, journalists mistrusted many medical professionals (such as the spokesman from the UK National Radiological Protection Board), and in turn encouraged the public to mistrust them.\n\nThroughout the European continent, in nations where abortion is legal, many requests for induced abortions, of otherwise normal pregnancies, were obtained out of fears of radiation from Chernobyl; including an excess number of abortions of healthy human fetuses in Denmark in the months following the accident.\nIn Greece, following the accident there was panic and false rumors which led to many obstetricians initially thinking it prudent to interrupt otherwise wanted pregnancies and/or were unable to resist requests from worried pregnant mothers over fears of radiation, within a few weeks misconceptions within the medical profession were largely cleared up, although worries persisted in the general population. Although it was determined that the effective dose to Greeks would not exceed 1 mSv (0.1 rem), a dose much lower than that which could induce embryonic abnormalities or other non-stochastic effects, there was an observed 2500 excess of otherwise wanted pregnancies being terminated, probably out of fear in the mother of some kind of perceived radiation risk.\n\nA \"slightly\" above the expected number of requested induced abortions occurred in Italy, were upon request, \"a week of reflection\" and then a 2 to 3 week \"health system\" delay usually occur before the procedure.\n\nThe term \"radiophobia\" is also sometimes used in the arguments against proponents of the conservative LNT concept (Linear no-threshold response model for ionizing radiation) of radiation security proposed by the U.S. National Council on Radiation Protection and Measurements (NCRP) in 1949. The \"no-threshold\" position effectively assumes, from data extrapolated from the atomic bombings on Hiroshima and Nagasaki, that even negligible doses of radiation increase ones risk of cancer linearly as the exposure increases from a value of 0 up to high dose rates. This is a controversial model as the LNT model therefore suggests that radiation exposure from naturally occurring background radiation, the radiation exposure from flying at high altitudes in airplanes, as well as lying next to loved ones for extended periods and the eating of bananas, which are also weakly naturally radioactive (both mostly due to Potassium-40, a naturally occurring radioactive material required for human life) all increase one's chance of cancer.\n\nMoreover, the lack of strong evidence supporting the LNT model, a model created from extrapolation from atomic bomb exposure, and not hard experimental evidence at low doses, has made the model controversial. As no irrefutable link between radiation induced negative health effects from low doses, in both human and other mammal exposure experiments, has been found.\n\nOn the contrary, many very low dose radiation exposure experiments find positive (hormetic) health effects at low doses of radiation, therefore the conservative LNT model when applied to low dose exposure remains controversial within the scientific community.\n\nAfter the Fukushima disaster, the German news magazine \"Der Spiegel\" reported that Japanese residents were suffering from radiophobia. British medical scientist Geraldine Thomas has also attributed suffering of the Japanese to radiophobia in interviews and formal presentations. Four years after the event The New York Times reported that ″about 1,600 people died from the stress of the evacuation″. The forced evacuation of 154,000 people ″was not justified by the relatively moderate radiation levels″, but it was ordered because ″the government basically panicked″.\n\nRadiation, most commonly in the form of X-rays, is used frequently in society in order to produce positive outcomes. The primary use of radiation in healthcare is in the use of radiography for radiographic examination or procedure, and in the use of radiotherapy in the treatment of cancerous conditions. Radiophobia can be a fear which patients experience before and after either of these procedures, it is therefore the responsibility of the healthcare professional at the time, often a Radiographer or Radiation Therapist, to reassure the patients about the stochastic and deterministic effects of radiation on human physiology. Advising patients and other irradiated persons of the various radiation protection measures that are enforced, including the use of lead-rubber aprons, dosimetry and Automatic Exposure Control (AEC) is a common method of informing and reassuring radiophobia sufferers.\n\nSimilarly, in industrial radiography there is the possibility of persons to experience radiophobia when radiophobia sufferers are near industrial radiographic equipment.\n\n\n"}
{"id": "12902187", "url": "https://en.wikipedia.org/wiki?curid=12902187", "title": "Rajasthan Atomic Power Station", "text": "Rajasthan Atomic Power Station\n\nThe Rajasthan Atomic Power Station (RAPS; also \"Rajasthan Atomic Power Project\" - RAPP) is located at Rawatbhata in the state of Rajasthan, India.\n\nThe construction of the Douglas Point Nuclear Generating Station Canada began in 1961 with a CANDU (CANada Deuterium Uranium) pressurised heavy water reactor (PHWR) capable of producing 220 MW of electricity. Two years after construction of the Rajasthan Power Project (RAPP) commenced, with two similar reactors built in the state of Rajasthan. Ten years later, in 1973 RAPS-1 was put into service. In 1974 after India conducted Smiling Buddha, its first nuclear weapons test Canada stopped their support of the project, delaying the commissioning of RAPS-2 until 1981.\n\nIn the context of the Indian atomic program, two more PHWR with an output of 220 MW each were built. They cost around 570 million dollars. RAPS-3 became critical on 24 December 1999, RAPS-4 became critical on 3 November 2000. Commercial operations began on 1 June 2000 for unit 3, and on 23 December 2000 for unit 4.\n\nTwo more reactors (RAPS-5 and RAPS-6) with 220 MWe have also been built, with unit 5 beginning commercial operation on 4 February 2010, and unit 6 on 31 March 2010.\n\nTwo of the new Indian-designed 700 MWe series of reactor (RAPP-7 and RAPP-8) are under construction at Rajasthan.\n\nIn November 2012, the International Atomic Energy Agency (IAEA) intensively audited over several weeks two reactors at the Rajasthan Atomic Power Station for safety. It has concluded that the reactors are among the best in the world, the indigenously made 220 MW atomic plants can withstand a Fukushima type of accident, even suggesting that the \"safety culture is strong in India\" and that India emerged a winner with a high global safety rank.\n\nFirst concrete for unit 7 was poured on 18 July 2011,\nwith commercial operation expected by 2016. \nThe two reactors will cost an estimated Rs 123.2 billion (US$2.6 billion).\n\nBy 2003 RAPS-1 had experienced numerous problems due to leaks, cracks in the end-shield and turbine blade failures, had undergone repairs and appeared to be generating 100 MW electricity, with RAPS-2 reportedly generating 200 MW.\n\nOn 29 August 2006, a 90% iron meteorite weighing 6.8 kilograms fell in Kanvarpura village, near the power station. The Deputy Director-General (western region) of the Geological Survey of India, R.S. Goyal, said that devastation on an \"unimaginable scale\" would have ensued had the object struck the station.\n\nIn June 2012, 38 workers were exposed to tritium when a welding operation went wrong inside the protected environment of the reactor.\n\n\n"}
{"id": "20793410", "url": "https://en.wikipedia.org/wiki?curid=20793410", "title": "Rotational Brownian motion", "text": "Rotational Brownian motion\n\nRotational Brownian motion is the random change in the orientation of a polar molecule due to collisions with other molecules. It is an important element of theories of dielectric materials. \n\nThe polarization of a dielectric material is a competition between torques due to the imposed electric field, which tend to align the molecules, and collisions, which tend to destroy the alignment. The theory of rotational Brownian motion allows one to calculate the net result of these two competing effects, and to predict how the permittivity of a dielectric material depends on the strength and frequency of the imposed electric field. \n\nRotational Brownian motion was first discussed by Peter Debye, who applied Einstein's theory of translational Brownian motion to the rotation of molecules having permanent electric dipoles. Debye ignored inertial effects and assumed that the molecules were spherical, with an intrinsic, fixed dipole moment. He derived expressions for the dielectric relaxation time and for the permittivity. These formulae have been successfully applied to many materials. However, Debye's expression for the permittivity predicts that the absorption tends toward a constant value when the frequency of the applied electric field becomes very large—the \"Debye plateau\". This is not observed; instead, the absorption tends toward a maximum and then declines with increasing frequency.\n\nThe breakdown in Debye's theory in these regimes can be corrected by including inertial effects; allowing the molecules to be non-spherical; including dipole-dipole interactions between molecules; etc. These are computationally very difficult problems and rotational Brownian motion is a topic of much current research interest.\n\n\n\n"}
{"id": "10702544", "url": "https://en.wikipedia.org/wiki?curid=10702544", "title": "Russian floating nuclear power station", "text": "Russian floating nuclear power station\n\nFloating nuclear power stations (Russian: плавучая атомная теплоэлектростанция малой мощности, ПАТЭС ММ - lit. floating combined heat and power (CHP) low-power nuclear station) are vessels designed by Rosatom.\nThey are self-contained, low-capacity, floating nuclear power plants. \nThe stations are to be mass-produced at shipbuilding facilities and then towed to the destination ports of the cities and towns experiencing deficit of power due to industrialization.\n\nThe work on such projects dates back to MH-1A in the United States, which was built in the 1960s into the hull of a World War II Liberty Ship; however, the Rosatom project is the first floating nuclear power plant intended for mass production. The initial plan was to manufacture at least seven of the vessels by 2015.\n\nIn 2000 the project for a floating power station began when the Ministry for Atomic Energy of the Russian Federation (Rosatom) chose Severodvinsk in Arkhangelsk Oblast as the construction site, Sevmash was appointed as general contractor. \nConstruction of the first power station, the \"Akademik Lomonosov\", started on 15 April 2007 at the Sevmash Submarine-Building Plant in Severodvinsk. \nHowever, in August 2008 construction works were transferred to the Baltic Shipyard in Saint Petersburg, which is also responsible for the construction of future vessels. \n\"Akademik Lomonosov\" was launched on 1 July 2010, at a cost of 6 billion rubles (232 m$).\nIn 2015 construction of a second vessel starting in 2019 was announced by Russia’s state nuclear corporation Rosatom.\n\nThe floating nuclear power station is a non-self propelled vessel. \nIt has length of , width of , height of , and draught of . The vessel has a displacement of 21,500 tonnes and a crew of 69 people.\n\nEach vessel of this type has two modified KLT-40 naval propulsion reactors together providing up to 70 MW of electricity or 300 MW of heat, or cogeneration of electricity and heat for district heating, enough for a city with a population of 200,000 people. It could also be modified as a desalination plant producing 240,000 cubic meters of fresh water a day.\nSmaller modification of the plant will be fitted with two ABV-6M reactors with the electrical power around 18 MWe (megawatts of electricity).\n\nThe much larger VBER-300 917 MW thermal or 325 MWe and the slightly larger RITM-200 55 MWe reactors have both been considered as a potential energy source for these floating nuclear power stations.\n\nThe hull and sections of vessels are to be built by the Baltic Shipyard in Saint Petersburg. Reactors are designed by OKBM Afrikantov and assembled by Nizhniy Novgorod Research and Development Institute Atomenergoproekt (both part of Atomenergoprom). The reactor vessels are produced by Izhorskiye Zavody. \nKaluga Turbine Plant supplies the turbo-generators.\n\nThe floating power stations need to be refueled every three years while saving up to 200,000 metric tons of coal and 100,000 tons of fuel oil a year. The reactors are supposed to have a lifespan of 40 years. Every 12 years, the whole plant will be towed home and overhauled at the wharf where it was constructed. The manufacturer will arrange for the disposal of the nuclear waste and maintenance is provided by the infrastructure of the Russian nuclear industry. Thus, virtually no radiation traces are expected at the place where the power station produced its energy.\n\nEnvironmental groups are concerned that floating plants will be more vulnerable to accidents and terrorism than land-based stations. They point to a history of naval and nuclear accidents in Russia and the former Soviet Union, including the Chernobyl disaster of 1986.\nRussia does have 50 years of experience operating a fleet of nuclear-powered icebreakers that are also used for scientific and Arctic tourism expeditions. The Russians have commented that a nuclear reactor that sinks, such as the similar reactor on the submarine in the Kursk submarine disaster, can be raised and probably put back into operation.\n\nFloating nuclear power stations are planned to be used mainly in the Russian Arctic. Five of these will be used by Gazprom for offshore oil and gas field development and for operations on the Kola and Yamal peninsulas. Other locations include Dudinka on the Taymyr Peninsula, Vilyuchinsk on the Kamchatka Peninsula and Pevek on the Chukchi Peninsula. In 2007, Rosatom signed an agreement with the Sakha Republic to build a floating plant for its northern parts, using smaller ABV reactors.\n\nAccording to Rosatom, 15 countries, including China, Indonesia, Malaysia, Algeria, Namibia, Cape Verde and Argentina, have shown interest in hiring such a device. It has been estimated that 75% of the world’s population live within 100 miles of a port city.\n\n\n"}
{"id": "11528924", "url": "https://en.wikipedia.org/wiki?curid=11528924", "title": "Smokeless fuel", "text": "Smokeless fuel\n\nSmokeless fuel means fuel which does not produce visible smoke when burned. The term is usually applied to solid fuels, such as:\n\n\n"}
{"id": "2916199", "url": "https://en.wikipedia.org/wiki?curid=2916199", "title": "Snedding", "text": "Snedding\n\nSnedding is the process of stripping the side shoots and buds from the length of a branch or shoot, usually of a tree or woody shrub. This process is most commonly performed during hedge laying and prior to the felling of trees on plantations ready for cropping.\n\nThe verb, \"to sned\", analogous to today's limbing, was also used by woodcutters in Scotland to refer to the process of removing branches from felled trees. Whether using an axe, a chainsaw or a billhook, the relative difficulty of snedding was a key measure of the difficulty of the job as a whole.\n\nThe word comes from the Scandinavian \"snäddare\", meaning a smooth log via the Old English \"snaedan\".\n\nSnedding can also describe a form of pruning when only some shoots will be removed, or when removing the leafy top from root crops (particularly turnips).\n\n"}
{"id": "13480124", "url": "https://en.wikipedia.org/wiki?curid=13480124", "title": "Spinodal decomposition", "text": "Spinodal decomposition\n\nSpinodal decomposition is a mechanism for the rapid unmixing of a mixture of liquids or solids\n\nSpinodal decomposition can be contrasted with nucleation and growth. There the initial formation of the microscopic clusters involves a large free energy barrier, and so can be very slow, and may occur as little as once in the initial phase, not throughout the phase, as happens in spinodal decomposition.\n\nSpinodal decomposition is of interest for two primary reasons. In the first place, it is one of the few phase transition in solids for which there is a complete quantitative theory. The reason for this is the inherent simplicity of the reaction. Since there is no thermodynamic barrier to the reaction inside of the spinodal region, the decomposition is determined solely by diffusion. Thus, it can be treated purely as a diffusional problem, and many of the characteristics of the decomposition can be described by an approximate analytical solution to the general diffusion equation.\n\nIn contrast, theories of nucleation and growth have to invoke the thermodynamics of fluctuations. The diffusional problem involved in the growth of the nucleus is far more difficult to solve because it is unrealistic to linearize the diffusion equation.\n\nFrom a more practical standpoint, spinodal decomposition provides a means of producing a very finely dispersed microstructure that can significantly enhance the physical properties of the material.\n\nIn the early 1940s, Bradley reported the observation of sidebands around the Bragg peaks of the X-ray diffraction pattern from a Cu-Ni-Fe alloy that had been quenched and then annealed inside the miscibility gap. Further observations on the same alloy were made by Daniel and Lipson, who demonstrated that the sidebands could be explained by a periodic modulation of composition in the <100> directions. From the spacing of the sidebands, they were able to determine the wavelength of the modulation, which was of the order of 100 angstroms.\n\nThe growth of a composition modulation in an initially homogeneous alloy implies uphill diffusion, or a negative diffusion coefficient. Becker and Dehlinger had already predicted a negative diffusivity inside the spinodal region of a binary system. But their treatments could not account for the growth of a modulation of a particular wavelength, such as was observed in the Cu-Ni-Fe alloy. In fact, any model based on Fick's law yields a physically unacceptable solution when the diffusion coefficient is negative.\n\nThe first explanation of the periodicity was given by Mats Hillert in his 1955 Doctoral Dissertation at MIT. Starting with a regular solution model, he derived a flux equation for one-dimensional diffusion on a discrete lattice. This equation differed from the usual one by the inclusion of a term which allowed for the effect on the driving force of the interfacial energy between adjacent interatomic planes that differed in composition. Hillert solved the flux equation numerically and found that inside the spinodal it yielded a periodic variation of composition with distance. Furthermore, the wavelength of the modulation was of the same order as that observed in the Cu-Ni-Fe alloys.\n\nBuilding on Hillert's work, a more flexible continuum model was subsequently developed by John W. Cahn and John Hilliard, who included the effects of coherency strains as well as the gradient energy term. The strains are significant in that they dictate the ultimate morphology of the decomposition in anisotropic materials.\n\nA metastable phase lies at a local but not global minimum in free energy, and is resistant to small fluctuations. J. Willard Gibbs described two criteria for a metastable phase: that it must remain stable against a small change over a large area, and that it must remain stable against a large change over a small area.\n\nGradient energies associated with even the smallest of compositional fluctuations can be evaluated using an approximation introduced by Ginzburg and Landau in order to describe magnetic field gradients in superconductors. This approach allows one to approximate the\nenergy associated with a concentration gradient formula_1C. Thus, as a result of series expansions with respect to ( c – c ), this energy can be expressed in the form κ(formula_1C)\n\n\nThe vector derivative of a scalar field \"f\" is called the gradient, and it can be represented as:\n\nCahn & Hilliard used such an approximation to evaluate the free energy of a small volume of non-uniform isotropic solid solution as follows:\n\nor:\n\nwhere:\n\nThe κ(formula_1C)\nterm, is a measure of the free energy of a composition gradient and is strongly dependent on local composition. (The constant κ is related to derivatives of the free energy with respect to composition.) The interfacial energy associated with this compositional gradient therefore increases with the square of formula_1C.\n\nSince we shall be concerned with testing the stability of an initially homogeneous solution to infinitesimal composition (or density) fluctuations, the gradients will also be infinitesimal and the second term will be completely sufficient to describe the contribution from the incipient 'surfaces\" (between regions differing in composition). Higher order gradient energy terms\nwill be negligible, except at very large gradients. We may also expand \"f\" (c) about the average composition c as follows:\n\nThe difference in free energy per unit volume (or free energy density) between the initial homogeneous solution and one with a composition given by:\n\nis given by:\n\nNote that both terms are quadratic in the amplitude, so the stability criterion is initially independent of amplitude.\n\nThus, \"ΔF\" is positive if the second derivative of the free energy with respect to composition (hereafter referred to as \"f\"\" ) is positive, because the contribution of the surface energy\nin the second term is always positive. In this case, the system is stable against all infinitesimal fluctuations in composition since the formation of such fluctuations would result in an increase in the free energy of the system.\n\nIn contrast, if \"f\"\" is negative, then \"ΔF\" is negative when:\n\nThe formation of fluctuations can therefore be accompanied by a decrease in the free energy of the system within this region provided the scale or wavelength of the fluctuation is large enough. Within this context, such gradual changes in composition maintain small values for the gradient term formula_1C.\n\nCahn and Hilliard formulated a theory for the amplification (or attenuation) of an arbitrary composition fluctuation by considering, with Debye, the Fourier components of the composition rather than the composition itself. Thus, for a concentration fluctuation:\n\none obtains for the change in free energy on forming fluctuations:\n\nThe \"solution is then unstable\" (\"ΔF\" < 0) for all fluctuations of wave number \"β\" smaller than a critical wave number \"β\" given by:\n\nor \"for all fluctuations of wavelength λ = 2π/β which are longer than a critical wavelength\" given by:\n\nFrom these equations, it is seen that the incipient surface energy, reflected in the gradient energy term, prevents the solution from decomposing on too small a scale. This concept was first introduced by Hillert, and shows that as the spinodal is approached, the critical wavelength approaches infinity.\nThis type of phase transformation is known as spinodal decomposition, and can be illustrated on a phase diagram exhibiting a miscibility gap. Thus, phase separation occurs whenever a material transitions into the unstable region of the phase diagram. The boundary of the unstable region, sometimes referred to as the binodal or coexistence curve, is found by performing a common tangent construction of the free-energy diagram. Inside the binodal is a region called the spinodal, which is found by determining where the curvature of the free-energy curve is negative. The binodal and spinodal meet at the critical point. It is when a material is moved into the spinodal region of the phase diagram that spinodal decomposition can occur.\nThe free energy curve is plotted as a function of composition for a temperature below the convolute temperature, T. Equilibrium phase compositions are those corresponding to the free energy minima. Regions of negative curvature (∂f/∂c < 0 ) lie within the inflection points of the curve (∂f/∂c = 0 ) which are called the spinodes. Their locus as a function of temperature defines the spinodal curve. For compositions within the spinodal, a homogeneous solution is unstable against infinitesimal fluctuations in density or composition, and there is no thermodynamic barrier to the growth of a new phase. Thus, the spinodal represents the limit of physical and chemical stability.\n\nTo reach the spinodal region of the phase diagram, a transition must take the material through the binodal region or the critical point. Often phase separation will occur via nucleation during this transition, and spinodal decomposition will not be observed. To observe spinodal decomposition, a very fast transition, often called a \"quench\", is required to move from the stable to the spinodally unstable region of the phase diagram.\n\nIn some systems, ordering of the material leads to a compositional instability and this is known as a \"conditional spinodal\", e.g. in the feldspars.\n\nThe mathematical theory of spinodal decomposition is based largely on the development of a generalized diffusion equation.\n\nA diffusion equation relates a spontaneous flux of material to a gradient in composition. Fundamental thermodynamic principles dictate that in order for the flux to be spontaneous, it must be associated with a net decrease in the free energy of the system. Consider the following diffusion equation relating the flux of two species ( J and J ) to the gradient of the chemical potential difference:\n\nAs pointed out by Cahn, this equation can be considered as a phenomenological definition of the mobility M, which must by definition be positive.\n\nIt consists of the ratio of the flux to the local gradient in chemical potential.\n\nThe quantity ( \"μ - μ\" ) is the change in free energy when we reversibly add a unit amount of A atoms ( ΔF = + \"μ\" ) and simultaneously remove an equal number of B atoms ( ΔF = - \"μ\" ). This term may include factors such as composition, compositional gradients, stresses, and magnetic fields. For a homogeneous system:\n\nThe quantity \"f\" is the free energy of that number of lattice points in the crystal which initially occupied a unit volume. Substituting,\n\nand defining the interdiffusion coefficient \"D\" by:\n\nWe can then define the interdiffusion coefficient \"D\" as follows:\n\nNote that since M must always be positive, \"D\" takes its sign from the sign of f\", which is negative within the spinodal. This has often been referred to as \"uphill diffusion\".\n\nThe above derivation of the diffusion coefficient is valid for concentration gradients that are so small that, for all practical purposes, each atom finds itself in surroundings which are similar to that which it would have in a homogeneous material of identical composition. If, however, concentration gradients are so large that within the range of interaction of an atom the average concentration has changed appreciably, then the atom will be aware of its inhomogeneous environment. This leads to a change in its chemical potential, and for fluids:\n\nSubstitution yields:\n\nBy taking the divergence, we obtain the new diffusion equation:\n\nAlternatively, since:\n\nthe flux equation can be written as:\n\nFor a system in equilibrium, the chemical potentials, and hence their difference, are constant throughout the system. Thus this equation for the flux satisfies the physical requirement that the net flux should go to zero as equilibrium is approached. For the time dependence of the composition we obtain on differentiation:\n\nComparing this equation with the usual statement of Fick's second law\n\nit is seen that the mobility is related to the interdiffusion coefficient by the following:\n\nIt then follows from the solution to be described next that a particular solution to this new diffusion equation is given by:\n\nin which c is the average composition and \"A(β,t)\" is the amplitude of the Fourier component of wavenumber β at time t. In terms of the initial amplitude at time zero:\n\nwhere \"R(β)\" is an amplification factor given by:\n\nFor most crystalline solid solutions, there is a variation of lattice parameter with composition. If the lattice of such a solution is to remain coherent in the presence of a composition modulation, mechanical work has to be done in order to strain the rigid lattice structure. The maintenance of coherency thus affects the driving force for diffusion.\n\nConsider a crystalline solid containing a one-dimensional composition modulation along the x-direction. We calculate the elastic strain energy for a cubic crystal by estimating the work required to deform a slice of material so that it can be added coherently to an existing slab of cross-sectional area. We will assume that the composition modulation is along the x' direction and, as indicated, a prime will be used to distinguish the reference axes from the standard axes of a cubic system (that is, along the <100>).\nLet the lattice spacing in the plane of the slab be \"a\" and that of the undeformed slice \"a\". If the slice is to be coherent after addition of the slab, it must be subjected to a strain ε in the \" z' \" and \" y' \" directions which is given by:\n\nIn the first step, the slice is deformed hydrostatically in order to produce the required strains to the \" z' \" and \" y' \" directions. We use the linear compressibility of a cubic system 1 / ( c + 2 c ) where the c's are the elastic constants. The stresses required to produce a hydrostatic strain of δ are therefore given by:\n\nThe elastic work per unit volume is given by:\n\nwhere the ε's are the strains. The work performed per unit volume of the slice during the first step is therefore given by:\n\nIn the second step, the sides of the slice parallel to the x' direction are clamped and the stress in this direction is relaxed reversibly. Thus, ε = ε = 0. The result is that:\n\nThe net work performed on the slice in order to achieve coherency is given by:\n\nor\n\nThe final step is to express c in terms of the constants referred to the standard axes. From the rotation of axes, we obtain the following:\n\nwhere l, m, n are the direction cosines of the x' axis and, therefore the direction cosines of the composition modulation. Combining these, we obtain the following:\n\nThe existence of any shear strain has not been accounted for. Cahn considered this problem, and concluded that shear would be absent for modulations along <100>, <110>, <111> and that for other directions the effect of shear strains would be small. It then follows that the total elastic strain energy of a slab of cross-sectional area A is given by:\n\nWe next have to relate the strain δ to the composition variation. Let a be the lattice parameter of the unstrained solid of the average composition c. Using a Taylor's series expansion about c yields the following:\n\nin which\n\nwhere the derivatives are evaluated at c. Thus, neglecting higher order terms, we have:\n\nSubstituting, we obtain:\n\nThis simple result indicates that the strain energy of a composition modulation depends only on the amplitude and is independent of the wavelength. For a given amplitude, the strain energy W is proportional to Y. Let us consider a few special cases.\n\nFor an isotropic material:\n\nso that:\n\nThs equation can also be written in terms of Young's modulus E and Poissons's ratio υ using the standard relationships:\n\nSubstituting, we obtain the following:\n\nFor most metals, the left hand side of this equation\n\nis positive, so that the elastic energy will be a minimum for those directions that minimize the term: lm + mn + ln. By inspection, those are seen to be <100>. For this case:\n\nthe same as for an isotropic material. At least one metal (molybdenum) has an anisotropy of opposite sign. In this case, the directions for minimum W will be those that maximize the directional cosine function. These directions are <111>, and\n\nAs we will see, the growth rate of the modulations will be a maximum in the directions that minimize Y. These directions therefore determine the morphology and structural characteristics of the decomposition in cubic solid solutions.\n\nRewriting the diffusion equation and including the term derived for the elastic energy yields the following:\n\nor\n\nwhich can alternatively be written in terms of the diffusion coefficient D as:\n\nThe simplest way of solving this equation is by using the method of Fourier transforms.\n\nThe motivation for the Fourier transform comes from the study of a Fourier series. In the study of a Fourier series, complicated periodic functions are written as the sum of simple waves mathematically represented by sines and cosines. Due to the properties of sine and cosine it is possible to recover the amount of each wave in the sum by an integral. In many cases it is desirable to use Euler's formula, which states that \"e\" = cos 2\"πθ\" + \"i\" sin 2\"πθ\", to write Fourier series in terms of the basic waves \"e\", with the distinct advantage of simplifying many unwieldy formulas.\n\nThe passage from sines and cosines to complex exponentials makes it necessary for the Fourier coefficients to be complex valued. The usual interpretation of this complex number is that it gives you both the amplitude (or size) of the wave present in the function and the phase (or the initial angle) of the wave. This passage also introduces the need for negative \"frequencies\". (E.G. If θ were measured in seconds then the waves \"e\" and \"e\" would both complete one cycle per second—but they represent different frequencies in the Fourier transform. Hence, frequency no longer measures the number of cycles per unit time, but is closely related.)\n\nIf A(β) is the amplitude of a Fourier component of wavelength λ and wavenumber β = 2π/λ the spatial variation in composition can be expressed by the Fourier integral:\n\nin which the coefficients are defined by the inverse relationship:\n\nSubstituting, we obtain on equating coefficients:\n\nThis is an ordinary differential equation that has the solution:\n\nin which \"A(β)\" is the initial amplitude of the Fourier component of wave wavenumber β and \"R(β)\" defined by:\n\nor, expressed in terms of the diffusion coefficient D:\n\nIn a similar manner, the new diffusion equation:\n\nhas a simple sine wave solution given by:\n\nwhere R(β) is obtained by substituting this solution back into the diffusion equation as follows:\n\nFor solids, the elastic strains resulting from (in)coherency add terms to the amplification factor R(β) as follows:\n\nwhere, for isotropic solids:\n\nwhere E is Young's modulus of elasticity, υ is Poisson's ratio, and η is the linear strain per unit composition difference. For anisotropic solids, the elastic term depends on direction in a manner which can be predicted by elastic constants and how the lattice parameters vary with composition. For the cubic case, Y is a minimum for either (100) or (111) directions, depending only on the sign of the elastic anisotropy.\n\nThus, by describing any composition fluctuation in terms of its Fourier components, Cahn showed that a solution would be unstable with respect to sinusoidal fluctuations of a critical wavelength. By relating the elastic strain energy to the amplitudes of such fluctuations, he formalized the wavelength or frequency dependence of the growth of such fluctuations, and thus introduced the principle of selective amplification of Fourier components of certain wavelengths. The treatment yields the expected mean particle size or wavelength of the most rapidly growing fluctuation.\n\nThus, the amplitude of composition fluctuations should grow continuously until a metastable equilibrium is reached with a preferential amplification of components of particular wavelengths. The kinetic amplification factor \"R\" is negative when the solution is stable to the fluctuation, zero at the critical wavelength, and positive for longer wavelengths—exhibiting a maximum at exactly formula_74 times the critical wavelength.\n\nConsider a homogeneous solution within the spinodal. It will initially have a certain amount of fluctuation from the average composition which may be written as a Fourier integral. Each Fourier component of that fluctuation will grow or diminish according to its wavelength.\n\nBecause of the maximum in \"R\" as a function of wavelength, those components of the fluctuation with formula_74 times the critical wavelength will grow fastest and will dominate. This \"principle of selective amplification\" depends on the initial presence of these wavelengths but does not critically depend on their exact amplitude relative to other wavelengths (if the time is large compared with (1/R). It does not depend on any additional assumptions, since different wavelengths can coexist and do not interfere with one another.\n\nLimitations of this theory would appear to arise from this assumption and the absence of an expression formulated to account for irreversible processes during phase separation which may be associated with internal friction and entropy production. In practice, frictional damping is generally present and some of the energy is transformed into thermal energy. Thus, the amplitude and intensity of a one-dimensional wave decreases with distance from the source, and for a three-dimensional wave the decrease will be greater.\n\nIn the spinodal region of the phase diagram, the free-energy can be lowered by allowing the components to separate, thus increasing the relative concentration of a component material in a particular region of the material. The concentration will continue to increase until the material reaches the stable part of the phase diagram. Very large regions of material will change their concentration slowly due to the amount of material which must be moved. Very small regions will shrink away due to the energy cost in maintaining an interface between two dissimilar component materials.\n\nTo initiate a homogeneous quench a control parameter, such as temperature, is abruptly and globally changed. For a binary mixture of formula_76-type and formula_77-type materials, the Landau free-energy\n\nis a good approximation of the free-energy near the critical point and is often used to study homogeneous quenches. The mixture concentration formula_79 is the density difference of the mixture components, the control parameters which determine the stability of the mixture are formula_76 and formula_77, and the interfacial energy cost is determined by formula_82.\n\nDiffusive motion often dominates at the length-scale of spinodal decomposition. The equation of motion for a diffusive system is\n\nwhere formula_84 is the diffusive mobility, formula_85 is some random noise such that formula_86, and the chemical potential formula_87 is derived from the Landau free-energy:\n\nWe see that if formula_89, small fluctuations around formula_90 have a negative effective diffusive mobility and will grow rather than shrink. To understand the growth dynamics, we disregard the fluctuating currents due to formula_91, linearize the equation of motion around formula_92 and perform a Fourier transform into formula_93-space. This leads to\n\nwhich has an exponential growth solution:\n\nSince the growth rate formula_96 is exponential, the fastest growing angular wavenumber\n\nwill quickly dominate the morphology. We now see that spinodal decomposition results in domains of the characteristic length scale called the \"spinodal length\":\n\nThe growth rate of the fastest growing angular wave number is\n\nwhere formula_100 is known as the \"spinodal time\".\n\nThe spinodal length and spinodal time can be used to nondimensionalize the equation of motion, resulting in universal scaling for spinodal decomposition.\n\n"}
{"id": "164596", "url": "https://en.wikipedia.org/wiki?curid=164596", "title": "Storm track", "text": "Storm track\n\nStorm tracks are the relatively narrow zones in seas and oceans where storms travel driven by the prevailing winds. \n\nThe Atlantic and Pacific have storm tracks along which most Atlantic or Pacific extratropical cyclones or tropical cyclones travel. The storm tracks usually begin in the westernmost parts of Atlantic and Pacific, where the large temperature contrasts between land and sea cause cyclones to form, particularly in winter. Surface friction cause these cyclones to quickly fill up and decay as soon as they reach land at the eastern end of the basins, accounting for the easternmost edges of the storm tracks.\n\nStorm tracks can shift position, causing important climatic patterns. As an example, during La Niña the Atlantic storm track shifts north causing droughts in Israel, while during El Niño it shifts south bringing heavy rains to the same region.\n\nAnother example of a storm track is the circumpolar storm track in the Antarctic, however land-sea contrasts play no role in its formation.\n\nGiven a grid point field of geopotential height, storm tracks can be visualized by contouring its average standard deviation, after the data has been band-pass filtered.\n\n\n"}
{"id": "15298053", "url": "https://en.wikipedia.org/wiki?curid=15298053", "title": "Stranded costs", "text": "Stranded costs\n\nIn discussions of electric power generation deregulation, stranded costs represent a public utility's existing infrastructure investments that may become redundant after substantial changes in regulatory or market conditions. An incumbent electric power utility will have made substantial investments over the years and will carry debt. The whole-life cost of electricity includes payments on this debt.\n\nAs technology improves, with all else equal, the cost of generating electricity falls. A new entrant to the market, unencumbered by debt, can build a modern plant and generate electricity at a lower cost than existing providers. Logical customers leave the incumbent utility for the new entrant, reducing the incumbent's revenue and spreading its debt payments across fewer remaining customers.\n\nThe problem is often caused by overlong depreciation schedules for capital investments by utilities, presuming that regulatory and market conditions would not change substantially.\n\nSolutions to the stranded costs problem include assigning a portion of the incumbent utility's debt to the new entrant as a condition of entry, or charging all customers in the market area a \"stranded cost recovery fee\". In some cases, a government may assume a portion of an incumbent utility's debt and assign it to the public debt, thus freeing the incumbent to compete more efficiently against new entrants.\n\n"}
{"id": "23609986", "url": "https://en.wikipedia.org/wiki?curid=23609986", "title": "Sustained yield", "text": "Sustained yield\n\nAccording to the Multiple Use, Sustained Yield Act of 1960 (P.L. 86-517), sustained yield (from the national forests) means the “achievement and maintenance in perpetuity of a high level of annual or regular periodic output of the various renewable resources of the national forests without impairment of the productivity of the land.”\n\n"}
{"id": "55624933", "url": "https://en.wikipedia.org/wiki?curid=55624933", "title": "Tired mountain syndrome", "text": "Tired mountain syndrome\n\nTired mountain syndrome is a condition in which underground nuclear testing fractures and weakens rock, increasing permeability and the risk of release of radionuclides and radioactive contamination of the environment. Locations said to have undergone the syndrome include the French Polynesian island of Moruroa, Rainier Mesa in the United States, the Dnepr 1 nuclear test site on the Kola Peninsula in Russia, possibly Mount Lazarev in the Novaya Zemlya Test Site in Russia, and Mount Mantap in North Korea.\n\n"}
{"id": "44078902", "url": "https://en.wikipedia.org/wiki?curid=44078902", "title": "Upstream contamination", "text": "Upstream contamination\n\nUpstream contamination by floating particles is a counterintuitive phenomenon in fluid dynamics. When pouring water from a higher container to a lower one, particles floating in the latter can climb upstream into the upper container. A definitive explanation is still lacking: experimental and computational evidence indicates that the contamination is chiefly driven by surface tension gradients, however the phenomenon is also affected by the dynamics of swirling flows that remain to be fully investigated.\n\nThe phenomenon was first observed in 2008 by the Argentinian \nS. Bianchini during mate tea preparation, while studying Physics \nat the University of Havana.\n\nIt rapidly attracted the interest of Prof. A. Lage, who performed,\nwith Bianchini, a series of controlled experiments. Later on\nProf. E. Altshuler completed the trio in Havana, which resulted in the\nDiploma thesis of Bianchini and \na short original paper posted in the web arxiv and commented as a surprising fact in some online journals.\n\nBianchini's Diploma thesis showed that the phenomenon could be reproduced \nin a controlled laboratory setting using mate leaves or chalk powder as contaminants, \nand that temperature gradients (hot in the top, cold in the bottom) were not necessary \nto generate the effect. The research also showed that surface tension was \na key element to the explanation through the so-called Marangoni effect, which was \nsuggested by two facts: (a) both mate and \nchalk lowered the surface tension of water, and (b) if an industrial surfactant was\nadded on the upper reservoir, the upstream motion of particles would stop.\n\nAfter a talk by A. Lage at the First Workshop on Complex Matter Physics\nin Havana (MarchCOMeeting'2012), Prof. T Shinbrot (Rutgers University)\ngot interested in the subject. Together with student T. Siu, \nCuban results were confirmed and expanded with new experiments and numerical\nsimulations at Rutgers,\nwhich resulted in a joint peer-reviewed paper.\n\nLater on, the phenomenon has been confirmed independently by others.\nWhether it is caused solely by surface tension gradients or depends also on dynamical \nbehaviors of the falling water still remains as an open question.\n\nVideos of the effect are available on YouTube.\n\nThe phenomenon of upstream contamination could be relevant to industrial and biotechnological processes, and may be\nconnected even to movements of the protoplasm. It could imply that some of the \"good practices\" in industrial and biotechnological procedures need revision.\n"}
{"id": "53258493", "url": "https://en.wikipedia.org/wiki?curid=53258493", "title": "Vacuum interrupter", "text": "Vacuum interrupter\n\nIn electrical engineering, a vacuum interrupter is a switch which uses electrical contacts in a vacuum. It is the core component of medium-voltage circuit-breakers, generator circuit-breakers & high-voltage circuit-breakers. Separation of the electrical contacts results in a metal vapour arc, which is quickly extinguished. Vacuum interrupters are widely used in utility power transmission systems, power generation unit, and power-distribution systems for railway, arc furnace application & industrial plants.\n\nSince the arc is contained within the interrupter, switchgear using vacuum interrupters are very compact compared with switchgear using air, SF or oil as arc-suppression medium. Vacuum interrupters can be used for circuit-breakers and load switches. Circuit-breaker vacuum interrupters are primarily used in the power sector in substation and power-generation facilities, and load-switching vacuum interrupters are used for power-grid end users.\n\nThe use of a vacuum for switching electrical currents was motivated by the observation that a one-centimeter gap in an X-ray tube could withstand tens of thousands of volts. Although some vacuum switching devices were patented during the 19th century, they were not commercially available. In 1926, a group led by Royal Sorensen at the California Institute of Technology investigated vacuum switching and tested several devices; fundamental aspects of arc interruption in a vacuum were investigated. Sorenson presented the results at an AIEE meeting that year, and predicted the switches' commercial use. In 1927, General Electric purchased the patent rights and began commercial development. The Great Depression and the development of oil-filled switchgear caused the company to reduce development work, and little commercially-important work was done on vacuum power switichgear until the 1950s.\n\nIn 1956, H. Cross revolutionized the high-frequency-circuit vacuum switch produced by Jenning and produced a vacuum switch with a rating of 15 kV at 200 A. Five years later, General Electric produced the first vacuum circuit breakers with a rated voltage of 15 kV at short-circuit breaking currents of 12.5 kA. In 1966, devices were developed with a rated voltage of 15 kV and short-circuit breaking currents of 25 and 31.5 kA. After the 1970s, vacuum switches began to replace the minimal-oil switches in medium-voltage switchgear. in the early 1980s, SF6 switches & breakers were also gradually replaced by vacuum technology in medium-voltage application.\n\nAs of 2018, a single-break vacuum circuit breaker has reached 145 kV and breaking current has reached 200 kA.\n\nManufacturers of vacuum interrupters have included Siemens, General Electric (GE), Westinghouse Electric (WH), Jucro Electric, Cooper Electric, Jenning ELectric, the General Electric Company\n(later acquired by Alstom), Areva and AEG. Toshiba, Meidensha, Fuji Electric, Hitachi and Mitsubishi products have a longitudinal magnetic-field structure.\n\nVacuum interrupters may be classified by enclosure type, by application, and by voltage class. \nExperimental, radio-frequency, and early power-switching vacuum interrupters had glass enclosures. More recently, vacuum interrupters for power switchgear are made with ceramic envelopes.\n\nApplications & uses include circuit-breakers, generator circuit-breaker, load switches, motor contactors, and reclosers. Special-purpose vacuum interrupters are also manufactured, such as those used in transformer tap changers, or in electrical arc furnace.\n\nResearches and investigations in the early 1990s allow the employment of vacuum switching technology for generator applications. Generator switching applications are well-known for their higher strains on interrupting devices such as high fault current of high asymmetry or high and steep transient recovery voltage, the standard IEC/IEEE 62271-37-013 (former and still valid IEEE C37.013, 1997) was introduced to address such requirements on circuit breakers used in generator applications.\n\nVacuum circuit breakers can be qualified as a generator circuit breakers according to IEC/IEEE 62271-37-013. Compared to circuit breakers using other quenching media (such as SF6, air-blast or minimum oil), vacuum circuit breakers have the advantages of: \n\nA vacuum interrupter generally has one fixed and one moving contact, a flexible bellows to allow movement of that contact, and arc shields enclosed in a hermetically-sealed glass, ceramic or metal housing with a high vacuum. The moving contact is connected by a flexible braid to the external circuit, and is moved by a mechanism when the device is required to open or close. Since air pressure tends to close the contacts, the operating mechanism must hold the contacts open against the closing force of air pressure on the bellows.\n\nThe interrupter's enclosure is made of glass or ceramic. Hermetic seals ensure that the interrupter vacuum is maintained for the life of the device. The enclosure must be impermeable to gas, and must not give off trapped gas. The stainless-steel bellows isolates the vacuum inside the interrupter from the external atmosphere and moves the contact within a specified range, opening and closing the switch.\n\nA vacuum interrupter has shields around the contacts and at the ends of the interrupter, preventing any contact material vaporized during an arc from condensing on the inside of the vacuum envelope. This would reduce the insulation strength of the envelope, ultimately resulting in the arcing of the interrupter when open. The shield also helps control the shape of the electric-field distribution inside the interrupter, which contributes to a higher open-circuit voltage rating. It helps absorb some of the energy produced in the arc, increasing a device's interrupting rating.\n\nThe contacts carry the circuit current when closed, forming the terminals of the arc when open. They are made of a variety of materials, depending on the vacuum interrupter's use and design for long contact life, rapid recovery of voltage withstand rating, and control of over-voltage due to current chopping.\n\nAn external operating mechanism drives the moving contact, which opens and closes the connected circuit. The vacuum interrupter includes a guide sleeve to control the moving contact and protect the sealing bellows from twisting, which would drastically shorten its life.\n\nAlthough some vacuum-interrupter designs have simple butt contacts, contacts are generally shaped with slots, ridges, or grooves to improve their ability to break high currents. Arc current flowing through the shaped contacts generate magnetic forces on the arc column, which cause the arc contact spot to move rapidly over the surface of the contact. This reduces contact wear due to erosion by an arc, which melts the contact metal at the point of contact.\n\nOnly few manufacturer of vacuum interrupters worldwide produce the contact material itself. The basic raw materials copper and chrome are combined to a powerful contact material by means of the arc-melting procedure. The resulting raw parts are processed to RMF or AMF contact discs, whereby the slotted AMF discs are deburred at the end. Contact materials require the following:\n\n\nIn circuit breakers, vacuum-interrupter contact materials are primarily a 50-50 copper-chromium alloy. They may be made by welding a copper-chrome alloy sheet on the upper and lower contact surfaces over a contact seat made of oxygen-free copper. Other materials, such as silver, tungsten and tungsten compounds, are used in other interrupter designs. The vacuum interrupter's contact structure has a great influence on its breaking capacity, electrical durability and level of current chopping.\n\nThe vacuum interrupter bellows allows the moving contact to be operated from outside the interrupter enclosure, and must maintain a long-term high vacuum over the expected operating life of the interrupter. The bellows is made of stainless steel with a thickness of 0.1 to 0.2 mm. Its fatigue life is affected by heat conducted from the arc.\n\nTo enable them to meet the requirements for high endurance in real practice, the bellows are regularly subjected to an endurance test every three months. The test is carried out in a fully automatic test cabin with the travels adjusted to the respective type.\n\nBellows lifetime are over 30,000 CO operation cycles.\n\nA vacuum interrupter uses a high vacuum to extinguish the arc between a pair of contacts. As the contacts move apart, current flows through a smaller area. There is a sharp increase in resistance between the contacts, and the temperature at the contact surface increases rapidly until the occurrence of electrode-metal evaporation. At the same time, the electric field is very high across the small contact gap. The breakdown of the gap produces a vacuum arc. As the alternating current is forced to pass through zero thanks to the arc resistance, and the gap between the fixed and moving contacts widens, the conductive plasma produced by the arc moves away from the gap and becomes non-conductive. The current is interrupted. u\n\nAMF & RMF contacts have spiral (or radial) slots cut into their faces. The shape of the contacts produces magnetic forces which move the arc spot over the surface of the contacts, so the arc does not remain in one place for very long. The arc is evenly distributed over the contact surface to maintain a low arc voltage and to reduce contact erosion.\n\nComponents of the vacuum interrupter must be thoroughly cleaned before assembly, since contaminants could emit gas into the vacuum envelope. To ensure a high breakdown voltage, components are assembled in a cleanroom where dust is strictly controlled.\n\nAfter having finished and cleaned the surfaces by electroplating and performed an optical inspection of the surface consistency of all single parts, the interrupter is assembled. The high-vacuum solder is applied at the joints of the components, the parts are aligned, and the interrupters are fixed. As cleanliness during assembly is especially important, all operations are done under air-conditioned clean-room conditions – in this way we can guarantee a constantly high quality of the interrupters and maximum possible ratings up to 100 kA according to IEC/IEEE 62271-37-013.\n\nSub-assemblies of vacuum interrupters were initially assembled and brazed together in a hydrogen atmosphere furnace. A tube connected to the interrupter's interior was used to evacuate the interrupter with an external vacuum pump while the interrupter was maintained at about . Since the 1970s, interrupter sub-components have been assembled in a high-vacuum brazing furnace by a combined brazing-and-evacuation process. Tens (or hundreds) of bottles are processed in one batch, using a high-vacuum furnace which heats them at temperatures up to 900 °C and a pressure of 10-6 mbar. Thus, the interrupters fulfill the quality requirement “sealed for lifetime”. Thanks to the fully automatic production process, the high quality can be constantly reproduced at any time\n\nThen, the evaluation of the interrupters by means of the X-ray procedure is used to verify the positions as well as the completeness of the internal components, and the quality of the brazing points. It ensures the high quality of vacuum interrupters.\n\nDuring forming, the definitive internal dielectric strength of the vacuum interrupter is established with gradually increasing voltage, and this is verified by a subsequent lightning impulse voltage test. Both operations are done with higher values than those specified in the standards, as evidence of the quality of the vacuum interrupters. This is the prerequisite for a long endurance and a high availability.\n\nDue to their manufacturing process, vacuum interrupters are proved to be \"sealed for lifetime\". This avoids the need for monitoring systems or tightness tests as stated in the IEEE std C37.100.1 on paragraph 6.8.3.\n\nUnder certain circumstances, the vacuum circuit breaker can force the current in the circuit to zero before the natural zero (and reversal of current) in the alternating-current circuit. If interrupter operation timing is unfavorable with respect to the AC-voltage waveform (when the arc is extinguished but the contacts are still moving and ionization has not yet dissipated in the interrupter), the voltage may exceed the gap's withstand voltage. This can re-ignite the arc, causing abrupt transient currents. In either case, oscillation is introduced into the system which may result in significant overvoltage. Vacuum-interrupter manufacturers address these concerns by selecting contact materials and designs to minimize current chopping. To protect equipment from overvoltage, vacuum switchgears usually include surge arresters.\n\nNowadays, with very low current chopping, vacuum circuit breaker will not induce any overvoltage which could have a negative impact on the insulation of surrounding equipment.\n"}
{"id": "40554399", "url": "https://en.wikipedia.org/wiki?curid=40554399", "title": "Warana Power Co-operative", "text": "Warana Power Co-operative\n\nWarana Power Co-operative legally known as Shree Tatyasaheb Kore Warana Sahakari Navashkti Sanstha Ltd. Warananagar, is a cooperative organization generating electric power from non-conventional, natural clean energy resources, such as hydro and biomass through the co-operative sector. These members are other co-operative organizations of Warana Group. They have also proposed to generate electric energy from wind, solar, tidal and geothermal, and later from other conventional energy resources.\n\nWarana Group (Warana Nagar) is situated on the banks of the Warana River, and lies in a green valley about from the city of Kolhapur, and about from Mumbai. The transformation of Warana from a barren to its current prosperous and fertile region began with the setting up of a cooperative sugar factory near the village of Kodoli in 1959. The Warana Power Co-operative idea of generating energy through co-operative organization was grown in the mind of Vinay Kore; and in January 2005, Warana Power Co-operative had been established. To start, Warana Power Co-operative took up 6 small hydro power projects form the state government of Maharashtra under the BOT (build–operate–transfer) policy.\n\nThe hydro power projects, all located in Kolhapur district, Maharashtra, India, are as follows:\nPresent Total Proposed power generation is 10.6MW through Hydro Power\n\nWarana co-generation project of 44MW has also started under the Warana Power Co-operative, which is attached with Warana Sugar. Bagasse, a waste product of sugar cane, will be used as fuel in the co-generation plant for the 150-day crushing period of the sugar factory; and for remaining 150 days, imported coal or biomass such as sugarcane trash, cotton stalk, rice husk. Julie Flora is apparently known Babool Tree in Hindi; need to link to actual plant name. --> sawdust, etc. will be used.\n"}
{"id": "12701068", "url": "https://en.wikipedia.org/wiki?curid=12701068", "title": "Water conservation order", "text": "Water conservation order\n\nA water conservation order is a legal ruling to protect aspects of water bodies. It may be to protect the quantity of the water itself or for any issues relating to the water body as a whole.\n\nIn New Zealand, a Water Conservation Order is used to protect the natural, cultural and recreational values of any water body. Water Conservation Orders came about as a result of lobbying by a group of stakeholders in the late seventies. At that time rivers were managed through the Water & Soil Conservation Act, which was administered by an appointed statutory body (NWASCA) serviced by the Ministry of Works. The engineers of the Ministry of Works argued that there was no need to legislate further as the Act contained provision for setting Minimum Flows.\n\nThere are currently 14 separate Water Conservation Orders:\n\nA water conservation order has been proposed for the Hurunui River in the South Island.\n\nIrrigation New Zealand, the national body representing agricultural irrigators and the irrigation industry, opposes water conservation orders. Irrigation NZ considers they no longer have relevance, they lock up the water resource and they may bankrupt the nation.\n\n"}
