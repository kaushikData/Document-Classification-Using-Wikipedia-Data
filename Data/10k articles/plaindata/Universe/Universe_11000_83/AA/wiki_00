{"id": "35020007", "url": "https://en.wikipedia.org/wiki?curid=35020007", "title": "2012 Hawaii hailstorm", "text": "2012 Hawaii hailstorm\n\nOn the morning of March 9, 2012, a particularly devastating and long-lived hailstorm hit the Hawaiian islands of Oahu and Lanai. The hailstorm was produced by a supercell thunderstorm. This event produced the largest hailstone ever recorded in Hawaii since records began in 1950. The hailstone was measured at long, tall, and wide. National Weather Service meteorologist Tom Birchard stated that the event was \"unprecedented.\"\n\nIn addition to spectacular early-morning lightning storms and flooding from the four feet (3.3 meters) of rainfall received, a tornadic waterspout formed off the coast of Oahu during the morning of March 9, 2012. Non-supercullar waterspouts are not uncommon (the State of Hawaii records an average of one waterspout/tornado per year), this mesocyclone-induced waterspout tracked inland for 1.5 miles, becoming an EF0 tornado that caused minor damage to the Enchanted Lakes subdivision of Kailua at 7:10 am Hawaiian-Aleutian Time.\n\n"}
{"id": "4548", "url": "https://en.wikipedia.org/wiki?curid=4548", "title": "Blizzard", "text": "Blizzard\n\nA blizzard is a severe snowstorm characterized by strong sustained winds of at least and lasting for a prolonged period of time—typically three hours or more. A ground blizzard is a weather condition where snow is not falling but loose snow on the ground is lifted and blown by strong winds. Blizzards can have an immense size and usually stretch to hundreds or thousands of kilometres.\n\nIn the United States, the National Weather Service defines a blizzard as a severe snow storm characterized by strong winds causing blowing snow that results in low visibilities. The difference between a blizzard and a snowstorm is the strength of the wind, not the amount of snow. To be a blizzard, a snow storm must have sustained winds or frequent gusts that are greater than or equal to with blowing or drifting snow which reduces visibility to or less and must last for a prolonged period of time—typically three hours or more.\n\nWhile severe cold and large amounts of drifting snow may accompany blizzards, they are not required. Blizzards can bring whiteout conditions, and can paralyze regions for days at a time, particularly where snowfall is unusual or rare.\n\nA severe blizzard has winds over , near zero visibility, and temperatures of or lower. In Antarctica, blizzards are associated with winds spilling over the edge of the ice plateau at an average velocity of .\n\nGround blizzard refers to a weather condition where loose snow or ice on the ground is lifted and blown by strong winds. The primary difference between a ground blizzard as opposed to a regular blizzard is that in a ground blizzard no precipitation is produced at the time, but rather all the precipitation is already present in the form of snow or ice at the surface.\n\nThe Australia Bureau of Meteorology describes a blizzard as, \"Violent and very cold wind which is laden with snow, some part, at least, of which has been raised from snow covered ground.\"\nThe \"Oxford English Dictionary\" concludes the term \"blizzard\" is likely onomatopoeic, derived from the same sense as \"blow, blast, blister, and bluster\"; the first recorded use of it for weather dates to 1829, when it was defined as a \"violent blow\". It achieved its modern definition by 1859, when it was in use in the western United States. The term became common in the press during the harsh winter of 1880–81.\n\nIn the United States, storm systems powerful enough to cause blizzards usually form when the jet stream dips far to the south, allowing cold, dry polar air from the north to clash with warm, humid air moving up from the south.\n\nWhen cold, moist air from the Pacific Ocean moves eastward to the Rocky Mountains and the Great Plains, and warmer, moist air moves north from the Gulf of Mexico, all that is needed is a movement of cold polar air moving south to form potential blizzard conditions that may extend from the Texas Panhandle to the Great Lakes and Midwest. A blizzard also may be formed when a cold front and warm front mix together and a blizzard forms at the border line.\n\nAnother storm system occurs when a cold core low over the Hudson Bay area in Canada is displaced southward over southeastern Canada, the Great Lakes, and New England. When the rapidly moving cold front collides with warmer air coming north from the Gulf of Mexico, strong surface winds, significant cold air advection, and extensive wintry precipitation occur.\n\nLow pressure systems moving out of the Rocky Mountains onto the Great Plains, a broad expanse of flat land, much of it covered in prairie, steppe and grassland, can cause thunderstorms and rain to the south and heavy snows and strong winds to the north. With few trees or other obstructions to reduce wind and blowing, this part of the country is particularly vulnerable to blizzards with very low temperatures and whiteout conditions. In a true whiteout there is no visible horizon. People can become lost in their own front yards, when the door is only away, and they would have to feel their way back. Motorists have to stop their cars where they are, as the road is impossible to see.\n\nA nor'easter is a macro-scale storm that occurs off the New England and Atlantic Canada coastlines. It gets its name from the direction the wind is coming from. The usage of the term in North America comes from the wind associated with many different types of storms some of which can form in the North Atlantic Ocean and some of which form as far south as the Gulf of Mexico. The term is most often used in the coastal areas of New England and Atlantic Canada. This type of storm has characteristics similar to a hurricane. More specifically it describes a low-pressure area whose center of rotation is just off the coast and whose leading winds in the left-forward quadrant rotate onto land from the northeast. High storm waves may sink ships at sea and cause coastal flooding and beach erosion. Notable nor'easters include The Great Blizzard of 1888, one of the worst blizzards in U.S. history. It dropped of snow and had sustained winds of more than that produced snowdrifts in excess of . Railroads were shut down and people were confined to their houses for up to a week. It killed 400 people, mostly in New York.\n\nThe 1972 Iran Blizzard, which caused 4,000 reported deaths, was the deadliest blizzard in recorded history. Dropping as much as of snow, it completely covered 200 villages. After a snowfall lasting nearly a week, an area the size of Wisconsin was entirely buried in snow.\n\nThe winter of 1880–1881 is widely considered the most severe winter ever known in parts of the United States. Many children—and their parents—learned of \"The Snow Winter\" through the children's book \"The Long Winter\" by Laura Ingalls Wilder, in which the author tells of her family's efforts to survive. The snow arrived in October 1880 and blizzard followed blizzard throughout the winter and into March 1881, leaving many areas snowbound throughout the entire winter. Accurate details in Wilder's novel include the blizzards' frequency and the deep cold, the Chicago and North Western Railway stopping trains until the spring thaw because the snow made the tracks impassable, the near-starvation of the townspeople, and the courage of her future husband Almanzo and another man, who ventured out on the open prairie in search of a cache of wheat that no one was even sure existed.\n\nThe October blizzard brought snowfalls so deep that two-story homes had snow up to the second floor windows. No one was prepared for the deep snow so early in the season and farmers all over the region were caught before their crops had even been harvested, their grain milled, or with their fuel supplies for the winter in place. By January the train service was almost entirely suspended from the region. Railroads hired scores of men to dig out the tracks but it was a wasted effort: As soon as they had finished shoveling a stretch of line, a new storm arrived, filling up the line and leaving their work useless.\n\nThere were no winter thaws and on February 2, 1881, a second massive blizzard struck that lasted for nine days. In the towns the streets were filled with solid drifts to the tops of the buildings and tunneling was needed to secure passage about town. Homes and barns were completely covered, compelling farmers to tunnel to reach and feed their stock.\n\nWhen the snow finally melted in late spring of 1881, huge sections of the plains were flooded. Massive ice jams clogged the Missouri River and when they broke the downstream areas were ravaged. Most of the town of Yankton, in what is now South Dakota, was washed away when the river overflowed its banks.\n\nThe Storm of the Century, also known as the Great Blizzard of 1993, was a large cyclonic storm that formed over the Gulf of Mexico on March 12, 1993, and dissipated in the North Atlantic Ocean on March 15. It is unique for its intensity, massive size and wide-reaching effect. At its height, the storm stretched from Canada towards Central America, but its main impact was on the United States and Cuba. The cyclone moved through the Gulf of Mexico, and then through the Eastern United States before moving into Canada. Areas as far south as northern Alabama and Georgia received a dusting of snow and areas such as Birmingham, Alabama, received up to with hurricane-force wind gusts and record low barometric pressures. Between Louisiana and Cuba, hurricane-force winds produced high storm surges across northwestern Florida, which along with scattered tornadoes killed dozens of people. In the United States, the storm was responsible for the loss of electric power to over 10 million customers. It is purported to have been directly experienced by nearly 40 percent of the country's population at that time. A total of 310 people, including 10 from Cuba, perished during this storm. The storm cost $6 to $10 billion in damages.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "25313593", "url": "https://en.wikipedia.org/wiki?curid=25313593", "title": "Brechfa Forest Wind Farm", "text": "Brechfa Forest Wind Farm\n\nBrechfa Forest West Wind Farm is a wind farm in Brechfa Forest in Carmarthenshire in south west Wales. Construction of the wind farm began in November 2016 and power generation commenced in January 2018.\n\nInnogy Renewables UK Ltd. proposed that the wind farm would be built in Strategic Search Area G, created as part of the Welsh Government's TAN 8 policy, published in 2005. Brechfa Forest West Wind Farm, had a proposed capacity of greater than 50 MW; thus the application was submitted to the Infrastructure Planning Commission. \n\nThe Brechfa Forest West development, located near the Afon Pib valley, consists of 28 turbines with a maximum tip height of 145 metres. The turbines have a generating capacity of around 57.4 MW. The plans were opposed by local activists.\n\nAs the Infrastructure Planning Commission ceased to exist in April 2012, the Brechfa Forest West application fell to the National Infrastructure Planning section of the Planning Inspectorate to produce a recommendation for the Secretary of State at DECC. They recommended approval, and DECC confirmed that development consent for Brechfa Forest West was granted on 12 March 2013.\n\nCarmarthenshire County Council Planning Committee approved the planning application for Brechfa Forest East at their 17 December 2013 meeting.\n\nConstruction on the Brechfa Forest West wind farm started in November 2016, and all turbines were operational in June 2018.\n\n\n\n"}
{"id": "46889405", "url": "https://en.wikipedia.org/wiki?curid=46889405", "title": "CSG Standard", "text": "CSG Standard\n\nThe CSG Standard is a voluntary carbon market certification standard, which describes the principles, processes and rules for the issuance of CSG Standard carbon credits. The CSG Standard is designed to finance small-scale climate protection projects, allowing voluntary carbon market participants to support local initiatives by purchasing carbon credits.\n\nVoluntary carbon market certification standards, also known as carbon offset schemes, define the rules of verification, measurement, tracking and monitoring of greenhouse gas emission units (so-called carbon credits) generated by climate protection projects . Several such standards exist. Projects running under voluntary carbon market certification standards (e.g. Verified Carbon Standard, Gold Standard, Social Carbon, VER+, The Voluntary Offset Standard) currently in use around the world must meet three additionality criteria:\n\n• Legal additionality: a project cannot meet this criterion if it is only realised in order to comply with legal requirements. However, it must comply with all relevant standards and legislation;\n\n• Environmental additionality: a project may not cause large-scale negative impact on the environment;\n\n• Financial additionality: a project may only be implemented if it could not be realised without the profits made through selling carbon credits. (https://web.archive.org/web/20150706102935/http://www.wwf.org.uk/filelibrary/pdf/carbon_offset_long.pdf )\n\n\nDepending on the type of a particular climate protection project, a Project Design Document (PDD) is prepared either based on existing methodology or after developing new methodology, which is then submitted to the Technical Advisory Panel (TAP), consisting of external experts, for approval. The Project Design Document includes a detailed description of both the technical and business plan for a project, the methodology used, monitoring procedures and the expected environmental impacts of the project. Following the third-party verification of the project, carbon credits may be issued through carbonregistry.com, an online registration system for the administration of various carbon credit transactions. Carbon credits sold through the system will be used by buyers to offset their emissions, and at the same time credits are “retired”, i.e. withdrawn from circulation.\n\nhttp://csgstandard.com/\n\nhttp://carbonregistry.com/\n\nhttp://www.fenykorkozosseg.hu/\n\nhttp://fenykorkozosseg.hu/fenykor-karbon-kredit\n"}
{"id": "1505330", "url": "https://en.wikipedia.org/wiki?curid=1505330", "title": "Carbon Trust", "text": "Carbon Trust\n\nThe Carbon Trust is a for-mission company that helps governments, organisations and companies reduce their carbon emissions and become more resource efficient. Its stated mission is to accelerate the move to a sustainable, low carbon economy. \n\nThe Carbon Trust helps reduce carbon emissions and increase resource efficiency through providing specialist help, support and advice. It has operations and projects across the globe, supporting companies and organisations wherever help is needed.\n\nThe Carbon Trust looks at current and future sustainability challenges and works with business and organisations to develop sustainable strategies to deliver savings.\n\nThe Carbon Trust provides voluntary carbon certification services and carbon labelling schemes – it verifies organisation and product carbon footprint data and provides marks of quality to organisations to demonstrate standards have been met.\n\nThe Carbon Trust works with governments, innovators and corporates with the aim of accelerating the commercialisation of low carbon technologies, and leads projects to deliver commercial partnerships and develop low carbon technologies. It is particularly active in the areas of offshore wind, marine energy, fuel cell development and industrial energy efficiency. One such project is the Offshore Wind Accelerator, which is aimed at reducing the cost of wind power through projects focused in the North Sea. The Offshore Wind Accelerator is a partnership between industry and governments.\n\nThe Carbon Trust provides analysis on sustainability issues to help businesses, investors and policy makers with their roles in reducing carbon and saving energy. It works with companies and governments across the world.\n\nThe Carbon Trust runs a series of environmental standards that certify measurement and reduction. Currently these cover carbon, water and waste and have been awarded to hundreds of leading companies and organisations across the world.\n\nIn June 2008 the Carbon Trust introduced the Carbon Trust Carbon Standard to address what it describes as business greenwash. The Carbon Trust Carbon Standard is only awarded to companies and organisations who measure and reduce their carbon emissions year on year. Examples of organisations who have held the Carbon Standard include Sky, Aldi, Eurotunnel, Bupa, Pricewaterhousecoopers, Samsung Electronics, Angus Council, Capital & Regional, O2, RWE npower, Credit Suisse, Scottish Government and the UK DECC.\n\nIn February 2013 the Carbon Trust introduced the Carbon Trust Water Standard to recognise those companies reducing their water use year on year. The first four companies to receive the Water Standard were Sainsbury's, Coca-Cola Enterprises Ltd, Sunlight Services Group, and Branston.\n\nIn July 2013 the Carbon Trust introuduced the Carbon Trust Waste Standard. In November 2013 the waste standard was awarded to the first wave of organisations, which included the Football Association, Renishaw plc, Whitbread, Pricewaterhousecoopers and AkzoNobel Decorative Paints. These last three became the first in the world to gain the triple crown of reaching the carbon, water and waste standard.\n\nIn 2015 the Carbon Trust launched the Carbon Trust Supply Chain Standard to look at carbon footprints across the supply chain. It is the world’s first independent certification for organisations that are measuring, managing and reducing greenhouse gas (CO2e) emissions in their supply chains.\n\nIn 2016 the Carbon Trust launched the Carbon Trust Zero Waste to Landfill Standard.\n\nThe Carbon Trust helps companies to measure the carbon emissions associated with their products (embodied emissions) and also provides a label for these products carbon footprint. Measuring the embodied emissions of products enables reductions to be identified and achieved across the supply chain. The label demonstrates a commitment by the product owner to reduce that footprint every two years. The Carbon Reduction Label was introduced in March 2007.\n\nExamples of products that have featured their carbon footprint are Kingsmill bread, Quorn foods, Evian water, Silver Spoon sugar, Walkers crisps, a range of own brand products in Tesco supermarkets, Halifax (HBOS) bank accounts, Dyson airblades, Marshalls building products, Quaker oats, Lafarge cement, and Pompeian Olive Oil.\n\nThe standards behind carbon labelling are now formally recognised through the PAS 2050 developed by the Carbon Trust in conjunction with BSI and Defra. This methodology is now gaining international acceptance following its launch in October 2008.\n\nThe Carbon Trust also supports the development and deployment of low carbon technologies and is actively engaged in the fuel cell, wave energy, wind energy, solar energy, biomass and biofuels sectors. It has a particularly active role in the Offshore Wind sector through its Offshore Wind Accelerator programme.\n\nIn April 2016 it launched its Green Business Fund offering a range of services for SMEs in Great Britain including up to £5,000 capital contribution towards buying energy efficient equipment. It is also involved in other finance schemes including interest-free loans for small and medium-sized enterprises available for energy-efficient equipment in Wales.\n\n"}
{"id": "41776538", "url": "https://en.wikipedia.org/wiki?curid=41776538", "title": "Catastrophe: Risk and Response", "text": "Catastrophe: Risk and Response\n\nCatastrophe: Risk and Response is a 2004 book by the economist Richard Posner, in which the author advocates the use of a cost–benefit framework to address potential major disasters such as runaway global warming and planet-obliterating asteroids.\n\n\n"}
{"id": "1430901", "url": "https://en.wikipedia.org/wiki?curid=1430901", "title": "Chlorine trifluoride", "text": "Chlorine trifluoride\n\nChlorine trifluoride is an interhalogen compound with the formula ClF. This colorless, poisonous, corrosive, and extremely reactive gas condenses to a pale-greenish yellow liquid, the form in which it is most often sold (pressurized at room temperature). The compound is primarily of interest as a component in rocket fuels, in plasmaless cleaning and etching operations in the semiconductor industry, in nuclear reactor fuel processing, and other industrial operations.\n\nIt was first reported in 1930 by Ruff and Krug who prepared it by fluorination of chlorine; this also produced ClF and the mixture was separated by distillation.\nClF is approximately T-shaped, with one short bond (1.598 Å) and two long bonds (1.698 Å). This structure agrees with the prediction of VSEPR theory, which predicts lone pairs of electrons as occupying two equatorial positions of a hypothetic trigonal bipyramid. The elongated Cl-F axial bonds are consistent with hypervalent bonding.\n\nPure ClF is stable to 180 °C in quartz vessels; above this temperature it decomposes by a free radical mechanism to its constituent elements.\n\nReactions with many metals give chlorides and fluorides; phosphorus yields phosphorus trichloride (PCl) and phosphorus pentafluoride (PF); and sulfur yields sulfur dichloride (SCl) and sulfur tetrafluoride (SF). ClF also violently reacts with water, oxidizing it to give oxygen or, in controlled quantities, oxygen difluoride (OF), as well as hydrogen fluoride and hydrogen chloride:\n\nIt will also convert many metal oxides to metal halides and oxygen or oxygen difluoride.\n\nOne of the main uses of ClF is to produce uranium hexafluoride, UF, as part of nuclear fuel processing and reprocessing, by the fluorination of uranium metal:\n\nThe compound can also dissociate under the scheme:\n\nIn the semiconductor industry, chlorine trifluoride is used to clean chemical vapour deposition chambers. It has the advantage that it can be used to remove semiconductor material from the chamber walls without the need to dismantle the chamber. Unlike most of the alternative chemicals used in this role, it does not need to be activated by the use of plasma since the heat of the chamber is enough to make it decompose and react with the semiconductor material.\n\nChlorine trifluoride has been investigated as a high-performance storable oxidizer in rocket propellant systems and it may be used as such someday. Handling concerns, however, severely limit its use. John Drury Clark summarized the difficulties:\n\nIt is, of course, extremely toxic, but that's the least of the problem. It is hypergolic with every known fuel, and so rapidly hypergolic that no ignition delay has ever been measured. It is also hypergolic with such things as cloth, wood, and test engineers, not to mention asbestos, sand, and water—with which it reacts explosively. It can be kept in some of the ordinary structural metals—steel, copper, aluminum, etc.—because of the formation of a thin film of insoluble metal fluoride that protects the bulk of the metal, just as the invisible coat of oxide on aluminum keeps it from burning up in the atmosphere. If, however, this coat is melted or scrubbed off, and has no chance to reform, the operator is confronted with the problem of coping with a metal-fluorine fire. For dealing with this situation, I have always recommended a good pair of running shoes.\n\nUnder the code name N-Stoff (\"substance N\"), chlorine trifluoride was investigated for military applications by the Kaiser Wilhelm Institute in Nazi Germany not long before the start of World War II. Tests were made against mock-ups of the Maginot Line fortifications, and it was found to be an effective combined incendiary weapon and poison gas. From 1938, construction commenced on a partly bunkered, partly subterranean 31.76 km munitions factory, the Falkenhagen industrial complex, which was intended to produce 90 tonnes of N-Stoff per month, plus sarin. However, by the time it was captured by the advancing Red Army in 1945, the factory had produced only about 30 to 50 tonnes, at a cost of over 100 German Reichsmark per kilogram. N-Stoff was never used in war.\n\nClF is a very strong oxidizing and fluorinating agent. It is extremely reactive with most inorganic and organic materials, such as glass, and will initiate the combustion of many otherwise non-flammable materials without any ignition source. These reactions are often violent, and in some cases explosive. Vessels made from steel, copper, or nickel resist the attack of the material due to formation of a thin layer of insoluble metal fluoride, but molybdenum, tungsten, and titanium form volatile fluorides and are consequently unsuitable. Any equipment that comes into contact with chlorine trifluoride must be scrupulously cleaned and then passivated, because any contamination left may burn through the passivation layer faster than it can re-form. Chlorine trifluoride has also been known to corrode materials otherwise known to be non-corrodible such as iridium, platinum, and gold.\n\nThe power to surpass the oxidizing ability of oxygen leads to corrosivity against oxide-containing materials often thought as incombustible. Chlorine trifluoride and gases like it have been reported to ignite sand, asbestos, and other highly fire-retardant materials. It will also ignite the ashes of materials that have already been burned in oxygen. In an industrial accident, a spill of 900 kg of chlorine trifluoride burned through 30 cm of concrete and 90 cm of gravel beneath. Other than the use of nitrogen and noble gases, no known fire control/suppression methods are capable of suppressing this oxidation, so the surrounding area must be flooded with nitrogen or helium or simply kept cool until the reaction ceases. The compound reacts with water-based suppressors, and oxidizes even in the absence of atmospheric oxygen, rendering traditional atmosphere-displacement suppressors such as CO and halon ineffective. It ignites glass on contact.\n\nExposure to larger amounts of chlorine trifluoride, as a liquid or as a gas, ignites living tissue. The hydrolysis reaction with water is violent and exposure results in a thermal burn. The products of hydrolysis are mainly hydrofluoric acid and hydrochloric acid, usually released as acidic steam or vapor due to the highly exothermic nature of the reaction. Hydrofluoric acid is corrosive to human tissue, is absorbed through skin, selectively attacks bone, interferes with nerve function, and causes often-fatal fluorine poisoning. Although hydrochloric acid is much less toxic to humans, it is often more corrosive than hydrofluoric acid.\n\n\n\n"}
{"id": "40674284", "url": "https://en.wikipedia.org/wiki?curid=40674284", "title": "Curtright field", "text": "Curtright field\n\nIn theoretical physics, the Curtright field (named after Thomas Curtright) is a tensor quantum field of mixed symmetry, whose gauge-invariant dynamics are dual to those of the general relativistic graviton in higher (\"D\">4) spacetime dimensions. Or at least this holds for the linearized theory.\nFor the full nonlinear theory, less is known. Several difficulties arise when interactions of mixed symmetry fields are considered, but at least in situations involving an infinite number of such fields (notably string theory) these difficulties are not insurmountable. \n\nIn four spacetime dimensions, the field is not dual to the graviton, if massless, but it can be used to describe \"massive\", pure spin 2 quanta. Similar descriptions exist for other massive higher spins, in \"D\"≥4. \n\nThe simplest example of the linearized theory is given by a rank three Lorentz tensor formula_1 whose indices carry the permutation symmetry of the Young diagram corresponding to the integer partition 3=2+1. That is to say, formula_2 and formula_3 where indices in square brackets are totally antisymmetrized. The corresponding field strength for formula_1 is\nformula_5 This has a nontrivial trace formula_6 where formula_7 is the Minkowski metric with signature (+,−,−...).\n\nThe action for formula_1 in \"D\" spacetime dimensions is bilinear in the field strength and its trace.\nThis action is gauge invariant, assuming there is zero net contribution from any boundaries, while the field strength itself is not. The gauge transformation in question is given by\nwhere \"S\" and \"A\" are arbitrary symmetric and antisymmetric tensors, respectively.\n\nAn infinite family of mixed symmetry gauge fields arises, formally, in the zero tension limit of string theory, especially if \"D\">4. Such mixed symmetry fields can also be used to provide alternate local descriptions for massive particles, either in the context of strings with nonzero tension, or else for individual particle quanta without reference to string theory.\n\n"}
{"id": "34231047", "url": "https://en.wikipedia.org/wiki?curid=34231047", "title": "Data furnace", "text": "Data furnace\n\nThe data furnace is a proposed method of heating residential homes or offices by running computers in them, which release considerable amounts of waste heat. Data furnaces can theoretically be cheaper than storing computers in huge data centers because the higher cost of electricity in residential areas (when compared to industrial zones) can be offset by charging the home owner for the heat that the data center gives off. Some large companies that store and process thousands of gigabytes of data believe that data furnaces could be cheaper because there would be little to no overhead costs. The cost of a traditional data storage center is up to around $400 per server, whereas the overhead cost per server of a home data furnace is around $10. Individuals had already begun using computers as a heat source by 2011.\n\nThe first kind of data furnace (DF) could be a low cost seasonal DF. This kind of DF would use an existing broadband connection to perform delay-tolerant jobs such as content indexing or the processing of large sets of scientific data. The server will only come on and start heating and processing when the house needs heat. The second kind of DF would be the low bandwidth neighborhood DF. This option can provide faster computations as it can run at all times, but this increases the risk of overheating. To get around this problem there may be vents to the outside added to the server racks to get rid of some of the unneeded heat. The third option would be an eco-friendly urban DF. This option, much like the second, runs year round and can vent excess heat to the outside. This would be an advantage for service providers to expand into urban areas more quickly, so long as the applications scale to the number of servers. This option causes a new challenge, because since it runs year round, the cost of electricity to run the servers cannot be offset by billing the home owners for the heat that they use as it will be little to none.\n\nFor a data furnace heating water, the heating needs to be at least 56°C/ 133°F to prevent the development of pathogens while limiting the risks of skin. Regarding space heating radiators, a temperature of 50-60°C/122-140°F is suitable for a radiator embedding processors as long as the heating surface is of significant size to dissipate the heat.\n\nThere are concerns about the security of these servers, as they would be stored on private properties, unmonitored. Unlike traditional data centers that are constantly monitored, data furnaces should be treated as the most insecure environment for data storage. For the best security, each server would have a device to prevent tampering. Furthermore, all of the data on these servers would have to be encrypted so that no one except the person requesting the data would have access to it.\n\nA few companies around the world are commercialising this concept around the world. A German company Cloud&Heat offers hot water heated by a distributed data center installed in the premises. French company Qarnot computing developed a radiator that heats with embedded processors and sells the computing power generated.\n\n"}
{"id": "8102919", "url": "https://en.wikipedia.org/wiki?curid=8102919", "title": "European Network of Transmission System Operators for Electricity", "text": "European Network of Transmission System Operators for Electricity\n\nENTSO-E, the European Network of Transmission System Operators, represents 43 electricity transmission system operators (TSOs) from 36 countries across Europe, thus extending beyond EU borders. ENTSO-E was established and given legal mandates by the EU’s Third Package for the Internal Energy Market in 2009, which aims at further liberalising the gas and electricity markets in the EU.\n\nOn 27 June 2008, 36 European electricity transmission system operators (TSOs) signed in Prague a declaration of intent to create the ENTSO-E. ENTSO-E was established on 19 December 2008 in Brussels by 42 TSOs as a successor of six regional associations of the electricity transmission system operators. ENTSO-E became operational on 1 July 2009. The former associations ETSO, ATSOI, UKTSOA, NORDEL, UCTE and BALTSO became a part of the ENTSO-E, while still offering data by their predecessors for public interest.\n\nCreation of ENTSO-E was initiated by the adoption of the European Union third legislative package on the gas and electricity markets. In 2003, the European Commission conducted a sector inquiry concerning the competition of electricity market in six European countries. Examining competition in these countries, the final report stated serious issues to be solved. It was noticed that the integration between member state's markets is still insufficient. Additionally, the absence of transparently available market information was assessed. As a result, the third legislative package on the EU gas and electricity markets was adopted by the European Commission in September 2007.\n\nAccording to its website, \"ENTSO-E promotes closer cooperation across Europe’s TSOs to support the implementation of EU energy policy and achieve Europe’s energy & climate policy objectives, which are changing the very nature of the power system. The main objectives of ENTSO-E centre on the integration of renewable energy sources (RES) such as wind and solar power into the power system, and the completion of the internal energy market (IEM), which is central to meeting the European Union’s energy policy objectives of affordability, sustainability and security of supply. [...] ENTSO-E aims to be the focal point for all technical, market and policy issues relating to TSOs and the European network, interfacing with power system users, EU institutions, regulators and national governments.\"\n\nTSOs are responsible for the bulk transmission of electric power on the main high voltage electric networks. TSOs provide grid access to the electricity market players (i.e., generating companies, traders, suppliers, distributors, and directly connected customers) according to non-discriminatory and transparent rules. In many countries, TSOs are in charge of the development of the grid infrastructure, too. TSOs in the European Union internal electricity market are entities operating independently from the other electricity market players (unbundling)\n\nENTSO-E contains 43 TSOs from 36 countries.\n\nThe geographical area covered by ENTSO-E’s member TSOs is divided into five synchronous areas and two isolated systems (Cyprus and Iceland). Synchronous areas are groups of countries that are connected via their respective power systems. The system frequency (50 Hz, with usually very minor deviations) is synchronous within each area, and a disturbance at one single point in the area will be registered across the entire zone. Individual synchronous areas are interconnected through direct current interconnectors.\n\nThe benefits of synchronous areas include pooling of generation capacities, common provisioning of reserves, both resulting in cost-savings, and mutual assistance in the event of disturbances, resulting in cheaper reserve power costs (for instance in case of a disturbance or outage).\n\nThe Third Energy Package and Regulation (EC) No 714/2009 on conditions for access to the network for cross-border exchanges in electricity regulation stipulate ENTSO-E’s tasks and responsibilities. Regulation (EU) 838/2010 on guidelines relating to the inter-TSO compensation mechanism sets out the methodology by which TSOs receive compensation for the costs incurred in hosting cross-border flows of electricity. Regulation (EU) 347/2013 on guidelines for trans-European energy infrastructure defines European Projects of Common Interest (PCIs) identifies ENTSO-E's ten-year network development plan (TYNDP) as the basis for the selection of PCIs. ENTSO-E is also mandated to develop a corresponding cost–benefit methodology for the assessment of transmission infrastructure projects.\n\nThe Transparency Regulation (EU) No. 543/2013 on submission and publication of data in electricity markets makes it mandatory for European Member State data providers and owners to submit fundamental information related to electricity generation, load, transmission, balancing, outages, and congestion management for publication through the ENTSO-E Transparency Platform.\n\nThe ten-year network development plan 2016 (TYNDP) is drafted by ENTSO-E, in close cooperation with stakeholders, under scrutiny of ACER and is finally adopted by the European Commission. It is the only existing pan-European network development plan. It is the basis for the selection of EU projects of common interest (PCIs). The list of PCIs is not fixed by ENTSO-E and is subject to a different process led by the European Commission and EU Member States.\n\nThe TYNDP is updated every two years. For inclusion in the TYNDP, each project, whether transmission or storage, has to go through a cost–benefit analysis. The benefit analysis methodology is developed by ENTSO-E in consultation with stakeholders and adopted by the European Commission. It assesses projects against socio-economic and environmental criteria.\n\nENTSO-E publishes summer and winter generation outlooks, well as a long-term system adequacy forecast, the Scenario Outlook & Adequacy Forecast (SO&AF). The seasonal outlooks assess if there is enough generation to cover supply and highlight possibilities for neighbouring countries to contribute to the generation/demand balance in critical situations in a specific country. The SO&AF analyses system adequacy on the long-term and is connected to investment decisions.\n\nENTSO-E’s network codes are binding pan-European rules drafted by ENTSO-E in consultation with stakeholders, with guidance from ACER. Network codes are grouped in three areas: \nThe drafting and adoption process of network codes is defined by the Third Package. ACER develops a framework guideline setting the policy choices for each code. On this basis, the codes are drafted by ENTSO-E in consultation with stakeholders. After ACER’s opinion and recommendation for adoption, each code is submitted to the European Commission for approval through the Comitology process, i.e., to be voted on by Member State representatives and thus to become EU law, directly binding and implemented across all Member States.\n\nENTSO-E’s Central Information Transparency Platform provides free access to fundamental data and information on pan-European wholesale energy generation, transmission, and consumption.\n\nENTSO-E’s R&D Roadmap provides the ENTSO-E vision on grid projects to be carried out by TSOs to meet EU objectives. The roadmap is supported by the annual R&D Implementation Plan, which combines both top-down and bottom-up approaches in meeting the requirements of the roadmap. ENTSO-E publishes annually a R&D Monitoring Report that assesses the progress of TSO-related R&D work.\n\nENTSO-E is an international non-profit association (AISBL) established according to Belgian law. ENTSO-E is financed by its members. The TSOs contribute to the budget according to the number of countries and the population served.\n\nThe highest body of ENTSO-E is the Assembly, which is composed of representatives at CEO level of all the currently 43 members. The ENTSO-E Board is elected every two years from the overall membership and through the Assembly. It includes 12 representatives. The president, vice president, and committee chairs are invited to board meetings. The board coordinates the committees and LRG work and implements Assembly decisions.\n\nENTSO-E has established four specialized committees composed of managers from member TSOs. Each committee leads a number of regional groups and working groups.\n\nAt the same level as the four committees, the transversal Legal & Regulatory Group advises all ENTSO-E bodies on legal and regulatory issues. In addition, expert groups on data, network codes implementation, and EU affairs provide specific expertise and work products to the association.\n\nENTSO-E’s Secretariat is based in Brussels. It is headed by the secretary-general and represents ENTSO-E to the European institutions, regulators, and stakeholders.\n\n\n\n"}
{"id": "31456567", "url": "https://en.wikipedia.org/wiki?curid=31456567", "title": "Flow conditioning", "text": "Flow conditioning\n\nFlow conditioning ensures that the “real world” environment closely resembles the “laboratory” environment for proper performance of inferential flowmeters like orifice, turbine, coriolis, ultrasonic etc.\n\nBasically, Flow in pipes can be classified as follows –\n\nFlow conditioners shown in fig.(a) can be grouped into following three types –\n\nStraightening devices such as honeycombs and vanes inserted upstream of the flow meter can reduce the length of straight pipe required. However, they produce only marginal improvements in measurement accuracy and may still require significant length of straight pipe, which a cramped installation site may not permit.\n\nNatural gas that carries a lot of liquids with it is known as wet gas whereas natural gas that is produced without liquid is known dry gas. Dry gas is also treated as to remove all liquids. The effect of flow conditioning for various popular meters which is used in gas measurement is explained below.\n\nThe most important as well as most difficult to measure aspects of flow measurement are flow conditions within a pipe upstream of a meter. Flow conditions mainly refer to the flow velocity profile, irregularities in the profile, varying turbulence levels within the flow velocity or turbulence intensity profile, swirl and any other fluid flow characteristics which will cause the meter to register flow different than that expected. It will change the value from the original calibration state referred to as reference conditions that are free of installation effects.\n\nInstallation effects such as insufficient straight pipe, exceptional pipe roughness or smoothness, elbows, valves, tees and reducers causes the flow conditions within the pipe to vary from the reference conditions. How these installation effects impact the meter is very important since devices which create upstream installation effects are common components of any standard metering design. Flow Conditioning refers to the process of artificially generating a reference, fully developed flow profile and is essential to enable accurate measurement while maintaining a cost-competitive meter standard design. The meter calibration factors are valid only of geometric and dynamic similarity exists between the metering and calibration conditions. In fluid mechanics, this is commonly referred to as the Law of Similarity.\n\nThe principle of Law of Similarity is used extensively for theoretical and experimental fluid machines. With respect to calibration of flowmeters, the Law of Similarity is the foundation for flow measurement standards. To satisfy the Law of Similarity, the central facility concept requires geometric and dynamic similarity between the laboratory meter and the installed conditions of this same meter over the entire custody transfer period. This approach assumes that the selected technology does not exhibit any significant sensitivity to operating or mechanical variations between calibrations. The meter factor determined at the time of calibration is valid if both dynamic and geometric similarity exists between the field installation and the laboratory installation of the artifact. \nA proper manufacturer’s experimental pattern locates sensitive regions to explore, measure and empirically adjust. The manufacturer’s recommended correlation method is a rational basis for performance prediction provided the physics do not change. For instance, the physics are different between subsonic and sonic flow. To satisfy the Law of Similarity the in situ calibration concept requires geometric and dynamic similarity between the calibrated meter and the installed conditions of this same meter over the entire custody transfer period. This approach assumes that the selected technology does not exhibit any significant sensitivity to operating or mechanical variations between calibrations. The meter factor determined at the time of calibration is valid if both dynamic and geometric similarity exists in the “field meter installation” over the entire custody transfer period.\n\nThe most commonly used description of flow conditions within the pipe is the flow velocity profile. Fig.(1) shows the typical flow velocity profile for natural gas measurement. The shape of the flow velocity profile is given by the following equation, formula_1 ---- (1) \nThe value of n determines the shape of the flow velocity profile. The eq.(1) can be used to determine the flow profile's shape within the pipe by fitting a curve to experimentally measured velocity data. In 1993, the transverse flow velocities were being measured within the high pressure natural gas environment using hot wire technology to accomplish the data fit. A fully developed flow profile was used as the reference state for meter calibration and determination of Coefficient of Discharge (Cd). For Reynolds Number formula_2 to formula_3 n is approximately 7.5; for Re of formula_3, n is approximately 10.0 where a fully developed profile in a smooth pipe was assumed. Since n is a function of Reynolds Number and friction factor, more accurate values of n can be estimated by using the eq.(2),\nformula_5 ---- (2) \nWhere, f is friction factor. A good estimate of a fully developed velocity profile can be used for those without adequate equipment to actually measure the flow velocities within the pipe. The following straight pipe equivalent length in eq.(3) was utilized to ensure a fully developed flow profile exists.\nformula_6 ---- (3) \nIn eq.(3) the pipe lengths required is significant, hence we need some devices that can able to condition the flow over a shorter pipe length allowing metering packages to be cost competitive and accurate. Here the velocity flow profile is generally three-dimensional. Normally the description requires no axial orientation indication if the profile is asymmetric and if it does exists, then axial orientation with respect to some suitable plane of reference is required. Asymmetry exists downstream of installation effects such as elbows or tees. Usually, the velocity flow profile is described on two planes 90° apart. Using the latest software technology a full pipe cross sectional description of the velocity profile is possible provided sufficient data points are given.\n\nThe second description of the flow field state within the pipe is the turbulence intensity. According to an experiment in 1994, the metering errors may exist even when the velocity flow profile is fully developed with perfect pipe flow conditions. Conversely, it was found zero metering error at times when the velocity profile was not fully developed. Hence this behavior was referred to the turbulence intensity of the gas flow that can cause metering bias error. This behavior accounts in part for the less than adequate performance of the conventional tube bundle.\n\nThe third description of the flow field's state is swirl. Swirl is the tangential flow component of the velocity vector. The velocity profile should be referred to as the axial velocity profile. As the velocity vector can be resolved into three mutually orthogonal components, the velocity profile only represents the axial component of velocity. fig.(2) showing the Swirl Angle which explains the definition of flow swirl and swirl angle. Note that swirl is usually referenced to full body rotation (that which the full pipeline flow follows one axis of swirl). In real pipeline conditions, such as downstream of elbows two or more mechanisms of swirl may be present.\n\nThe condition of a flow can affect the performance and accuracy of devices that measure the flow.\n\nThe basic orifice mass flow equation provided by API 14.3 and ISO 5167 is given as,\nformula_7 ----(4)\nWhere, \nformula_8 = Mass flow \nformula_9 = Coefficient of discharge \nformula_10 = Velocity of approach factor \nY = Expansion factor \nd = orifice diameter \nformula_11 = density of the fluid \nformula_12 = differential pressure \nNow to use the eq.(4), the flow field entering the orifice plate must be free of swirl and exhibit a fully developed flow profile. API 14.3 (1990) and ISO standards determined the Coefficient of Discharge by completing numerous calibration tests where the indicated mass flow was compared to the actual mass flow to determine coefficient of discharge. In all testing the common requirement was a fully developed flow profile entering the orifice plate. Accurate standard compliant meter designs must therefore ensure that a swirl free, fully developed flow profile is impinging on the orifice plate. There are numerous methods available to accomplish this. These methods are commonly known as “flow conditioning”.\nThe first installation option is to revert to no flow conditioning, but adequate pipe lengths must be provided by the eq.(2) mentioned above. This generally makes the manufacturing costs for a flow measurement facility unrealistic due to excessively long meter tubes; Imagine meter tubes 75 diameters long.\nThe second and most well known option is the 19-tube tube-bundle flow conditioner. The majority of flow installations in North America contain the tube bundle. With the help of hot wire, pitot tube and laser-based computerized measurement systems which allow detailed measurement of velocity profile and turbulence intensity; we know that the tube bundle does not provide fully developed flow. Therefore, this device is causing biased orifice flow measurement. As a result of these recent findings, few tube bundles are specified for flow measurement and reduce the use of such device. Numerous references are available providing performance results indicating less than acceptable meter performance when using the conventional 19-tube test bundle. The individual results should be reviewed to ascertain details such as beta ratio, meter tube lengths, Re and test conditions. \nThe general indications are that the conventional tube bundle will cause the orifice installation to over register flow values up to 1.5% when the tube bundle is 1 pipe diameter to approximately 11 pipe diameters from the orifice plate. This is caused by a flat velocity profile that creates higher differential pressures than with a fully developed profile. There is a crossover region from approximately 10 to 15 pipe diameters where the error band is approximately zero. Then a slight under-registration of flows occurs for distances between approximately 15 to 25 pipe diameters. This is due to a peaked velocity profile that creates lower differential pressures than a fully developed profile. At distances greater than 25 pipe diameters the error asymptotes to zero. Fig.(3) showing the Conventional Tube Bundle Performance explaining typical characteristic behavior of the popular 19 tube, tube-bundle. An additional drawback of the conventional 19 tube, tube bundle is variation in sizing.\nThe conventional tube bundle provides errors very much dependent on installation details, that is, the elbows on and out of plane, tees, valves and distances from the last pipe installation to the conditioner and conditioner to the orifice plate. These errors have a great significance. Therefore, the latest findings regarding conventional tube bundle performance should be reviewed prior to meter station design and installation.\nThe final installation option for orifice metering is perforated plate flow conditioners. There is a variety of perforated plates have entered the market. These devices generally are designed to rectify the drawbacks of the conventional tube bundle (accuracy and repeatability insufficiency). The reader is cautioned to review the performance of the chosen perforated plate carefully prior to installation. A flow conditioner performance test guideline should be utilized to determine performance. The key elements of a flow conditioner test are -\n\nThe turbine meter is available in various manufacturer's configurations of a common theme; turbine blades and rotor configured devices. These devices are designed such that when a gas stream passes through them they will spin proportionally to the amount of gas passing over the blades in a repeatable fashion. Accuracy is then ensured by completion of a calibration, indicating the relationship between rotational speed and volume, at various Reynolds Numbers. The fundamental difference between the orifice meter and the turbine meter is the flow equation derivation. The orifice meter flow calculation is based on fluid flow fundamentals (a 1st Law of Thermodynamics derivation utilizing the pipe diameter and vena contracta diameters for the continuity equation). Deviations from theoretical expectation can be assumed under the Coefficient of Discharge. Thus, one can manufacture an orifice meter of known uncertainty with only the measurement standard in hand and access to a machine shop. The need for flow conditioning, and hence, a fully developed velocity flow profile is driven from the original determination of Cd which utilized fully developed or 'reference profiles' as explained above.\n\nConversely, the turbine meter operation is not rooted deeply in fundamentals of thermodynamics. This is not to say that the turbine meter is in any way an inferior device. There are sound engineering principles providing theoretical background. It is essentially an extremely repeatable device that is then assured accuracy via calibration. The calibration provides the accuracy. It is carried out in good flow conditions (flow conditions free of swirl and a uniform velocity flow profile) this is carried out for every meter manufactured. Deviations from the as-calibrated conditions would be considered installation effects, and the sensitivity of the turbine meter to these installation effects is of interest. The need for flow conditioning is driven from the sensitivity of the meter to deviations from as calibrated conditions of swirl and velocity profile.\nGenerally, recent research indicates that turbine meters are sensitive to swirl but not to the shape of the velocity profile. A uniform velocity profile is recommended, but no strict requirements for fully developed flow profiles are indicated. Also, no significant errors are evident when installing single or dual rotor turbine meters downstream of two elbows out-of-plane without flow conditioning devices.\n\nDue to the relative age of the technology, it may be beneficial to discuss the operation of the multipath ultrasonic meter to illustrate the effects of flow profile distortion and swirl. There are various types of flow measurements utilizing high frequency sound. The custody transfer measurement devices available today utilize the time of travel concept. The difference in time of flight with the flow is compared to the time of flight against the flow. This difference is used to infer average flow velocity on the sound path. Fig.(5) showing the Ultrasonic Meter sound path no flow which illustrates this concept.\n\nThe resulting flow equation for the mean velocity experienced by the sound path is given by,\nformula_20 ----(5)\nThe case of no flow gives the actual path of the sound when there is zero flow (by equating eq.(5) to zero). In case of theoretical flow profile, say a uniform velocity flow profile where the no-slip condition on the pipe walls is not applied, Fig.(6) shows Ultrasonic Meter sound path - uniform velocity profile which illustrates the resultant sound path.\nA theoretical derivation of the Mean velocity equation for this sound path becomes much more complicated. In case of a perfect fully developed real velocity profile of Ultrasonic meter which is shown in Fig.(7) indicating a possible sound path as a result of an installation in a real flow. \nHere a mathematical derivation for this Ultrasonic meter is also becomes very complicated. Developing a robust flow algorithm to calculate the mean flow velocity for the sound path can be quite complicated. Now add to this; sound path reflection from the pipe wall, multipaths to add degrees of freedom, swirl and departure from axisymmetric fully developed flow profile and the problem of integrating the actual velocity flow profile to yield volume flow rate can be an accomplishment. Hence the real performance of ultrasonic meters downstream of perturbations, and the need for calibrations is required.\n\nCoriolis meter shown in fig.(8) is very accurate in single-phase conditions but inaccurate to measure two-phase flows. It poses a complex fluid structure interaction problem in case of two-phase operation. There is a scarcity of theoretical models available to predict the errors reported by Coriolis meter in aforementioned conditions. Flow conditioners make no effect on meter accuracy while using wet gas due to the annular flow regime, which is not highly affected by flow conditioners. In single-phase conditions, Coriolis meter gives accurate measurement even in presence of severe flow disturbances. There is no need for flow conditioning before the meter to obtain accurate readings from it, which would be the case in other metering technologies like orifice and turbine. On the other hand, in two-phase flows, the meter consistently gives negative errors. The use of flow conditioners clearly affects the reading of the meter in aerated liquids. This phenomenon can be used to get fairly accurate estimate of flow rate in low gas volume fraction liquid flows.\n\nFlow conditioning makes a huge effect on the accuracy of liquid turbine meter which results in flow disturbances. These effects are mainly caused by debris on strainer screens, for various upstream piping geometries and different types of flow conditioners.\nThe effectiveness of a flow conditioner can be indicated by the following two key measurements:\n\n"}
{"id": "8127497", "url": "https://en.wikipedia.org/wiki?curid=8127497", "title": "Fusion torch", "text": "Fusion torch\n\nA fusion torch is a technique for utilizing the high-temperature plasma of a fusion reactor to break apart other materials (especially waste materials) and convert them into a few reusable and saleable elements. It was invented in 1968 by Bernard J. Eastlund and William C. Gough while they were program managers of the controlled thermonuclear research program of the United States Atomic Energy Commission (AEC). The basic concept was to impinge the plasma leaking from fusion reactors onto solids or liquids, vaporizing, dissociating and ionizing the materials, then separating the resulting elements into separate bins for collection. Other applications of fusion plasmas such as generation of UV and optical light, and generation of hydrogen fuel, were also described in their associated 1969 paper.\n\nThe process began with a tokamak, a doughnut-shaped magnetic “bottle”, containing plasma and unwanted material. This combination would result in a pool of electrons and nuclei which in turn would cause the tokamak to overflow and transfer the plasma into an outlet. This plasma then passes through a series of metal plates, differing in particular temperatures, all arranged in descending order. The atoms of elements pass over the plates that have a boiling point that is above their own. Eventually, the atoms encounter plates where the temperature is lower than their boiling point. This makes them stick onto the plate. The plates then work as a distillation system which sorts the plasma into its constituent elements. These pure elements can then be reused.\n\nIn the paper \"The Fusion Torch – Closing the Cycle from Use to Reuse\", Bernard J. Eastlund and William C. Gough defined Population (food), Entropy (resources, energy, pollution), and War (human needs and behavior) as three traps that could hamper the advancement of mankind. In terms of energy needs they estimated that by the year 2000 will need 140,000 megawatts of electrical capacity. They also speculated that the fusion torch concept would be useful for the separation of uranium from reactor fuel element material.\n\nAlthough the fusion torch will help in disposal of pollution and waste and make it available for reuse, there is also a problem that arises. The process of separating elements uses a lot of energy, and therefore creates a lot of heat. With the rise of the standards of living, all this heat that is created from using the fusion torch will be released into the atmosphere. Such a large amount of heat could cause the surface temperature of the earth to rise. This could eventually lead to severe climate modifications and put a limit on world population and standards of living. However, the heat generated from the fusion torch and fusion can be \"contained\" if the system is brought to Break-even temperature. Therefore, becoming self-sustaining.\n\n"}
{"id": "430790", "url": "https://en.wikipedia.org/wiki?curid=430790", "title": "Gauge boson", "text": "Gauge boson\n\nIn particle physics, a gauge boson is a force carrier, a bosonic particle that carries any of the fundamental interactions of nature, commonly called forces. Elementary particles, whose interactions are described by a gauge theory, interact with each other by the exchange of gauge bosons—usually as virtual particles.\n\nAll known gauge bosons have a spin of 1. Therefore, all known gauge bosons are vector bosons.\n\nGauge bosons are different from the other kinds of bosons: first, fundamental scalar bosons (the Higgs boson); second, mesons, which are composite bosons, made of quarks; third, larger composite, non-force-carrying bosons, such as certain atoms.\n\nThe Standard Model of particle physics recognizes four kinds of gauge bosons: photons, which carry the electromagnetic interaction; W and Z bosons, which carry the weak interaction; and gluons, which carry the strong interaction.\n\nIsolated gluons do not occur because they are color-charged and subject to color confinement.\n\nIn a quantized gauge theory, gauge bosons are quanta of the gauge fields. Consequently, there are as many gauge bosons as there are generators of the gauge field. In quantum electrodynamics, the gauge group is \"U\"(1); in this simple case, there is only one gauge boson. In quantum chromodynamics, the more complicated group \"SU\"(3) has eight generators, corresponding to the eight gluons. The three W and Z bosons correspond (roughly) to the three generators of \"SU\"(2) in GWS theory.\n\nFor technical reasons involving gauge invariance, gauge bosons are described mathematically by field equations for massless particles. Therefore, at a naïve theoretical level, all gauge bosons are required to be massless, and the forces that they describe are required to be long-ranged. The conflict between this idea and experimental evidence that the weak and strong interactions have a very short range requires further theoretical insight.\n\nAccording to the Standard Model, the W and Z bosons gain mass via the Higgs mechanism. In the Higgs mechanism, the four gauge bosons (of \"SU\"(2)×\"U\"(1) symmetry) of the unified electroweak interaction couple to a Higgs field. This field undergoes spontaneous symmetry breaking due to the shape of its interaction potential. As a result, the universe is permeated by a nonzero Higgs vacuum expectation value (VEV). This VEV couples to three of the electroweak gauge bosons (the Ws and Z), giving them mass; the remaining gauge boson remains massless (the photon). This theory also predicts the existence of a scalar Higgs boson, which has been observed in experiments at the LHC.\n\nThe Georgi-Glashow model predicts additional gauge bosons named X and Y bosons. The hypothetical X and Y bosons mediate interactions between quarks and leptons, hence violating conservation of baryon number and causing proton decay. Such bosons would be even more massive than W and Z bosons due to symmetry breaking. Analysis of data collected from such sources as the Super-Kamiokande neutrino detector has yielded no evidence of X and Y bosons.\n\nThe fourth fundamental interaction, gravity, may also be carried by a boson, called the graviton. In the absence of experimental evidence and a mathematically coherent theory of quantum gravity, it is unknown whether this would be a gauge boson or not. The role of gauge invariance in general relativity is played by a similar symmetry: diffeomorphism invariance.\n\nW' and Z' bosons refer to hypothetical new gauge bosons (named in analogy with the Standard Model W and Z bosons).\n\n\n"}
{"id": "7919098", "url": "https://en.wikipedia.org/wiki?curid=7919098", "title": "Grammage", "text": "Grammage\n\nGrammage and basis weight, in the pulp and paper and the fabric industries, are the areal density of a paper or fabric product, that is, its mass per unit of area. \nTwo ways of expressing grammage are commonly used:\n\nIn the metric system, the mass per unit area of all types of paper and paperboard is expressed in terms of grams per square meter (g/m). This quantity is commonly called \"grammage\" in both English and French (ISO 536), though printers in most English-speaking countries still refer to the \"weight\" of paper.\n\nTypical office paper has 80 g/m, therefore a typical A4 sheet ( m) weighs 5 g. The unofficial unit symbol \"gsm\" instead of the standard \"g/m\" is also widely encountered in English speaking countries.\n\nTypically grammage is measured in paper mill on-line by a quality control system and verified by laboratory measurement.\n\nIn countries that use US paper sizes, a less direct measure known as \"basis weight\" is used in addition to or instead of grammage. The basis weight of paper is the density of paper expressed in terms of the mass of a ream of given dimensions and a sheet count. In the US system, the weight is specified in avoirdupois pounds and the sheet count of a paper ream is usually 500 sheets. However, the mass specified is not the mass of the ream that is sold to the customer. Instead, it is the mass of the uncut \"basis ream\" in which the sheets have some larger size. Often, this is a size used during the manufacturing process before the paper was cut to the dimensions in which it is sold. So, to compute the mass per area, one must know\n\nThe standard dimensions and sheet count of a ream vary according to the type of paper. These \"uncut\" basis sizes are not normally labelled on the product, are not formally standardized, and therefore have to be guessed or inferred somehow from trading practice. Historically, this convention is the product of pragmatic considerations such as the size of a sheet mold.\n\nBy using the same basis sheet size for the same type of paper, consumers can easily compare papers of differing brands. Twenty pound bond paper is always lighter and thinner than 32 pound bond, no matter what its cut size. And 20 pound bond \"letter size\" and 20 pound bond \"legal size\" papers are the same weight paper having different cut size.\n\nHowever, a sheet of common copy paper that has a basis weight of does not have the same mass as the same size sheet of coarse paper (newsprint). In the former case, the standard ream is 500 sheets of paper, and in the latter, 500 sheets of paper. Here are some basic ream sizes for various types of paper. Units are inches except where noted.\n\nSheets can be cut into four sheets, a standard for business stationery known conventionally as \"letter sized paper\". So, the ream became commonly used. The book-paper ream developed because such a size can easily be cut into sixteen 6 by book sized sheets without significant waste.\n\nEarly newsprint presses printed sheets in size, and so the ream dimensions for newsprint became , with 500 sheets to a ream. Newsprint was made from ground wood pulp, and ground wood hanging paper (wallpaper) was made on newsprint machines. Newsprint was used as wrapping paper, and the first paper bags were made from newsprint. The newsprint ream standard also became the standard for packaging papers, even though in packaging papers kraft pulp rather than ground wood was used for greater strength.\n\nPaper weight is sometimes stated using the \"#\" symbol. For example, \"20#\" means \"20 pounds per basis ream of 500 sheets\". When the density of a ream of paper is given in pounds, it is often accompanied by its \"M weight\". The M weight is the weight (in pounds) of 1000 cut sheets. Paper suppliers will often charge by M weight, since it is always consistent within a specific paper size, and because it allows a simple weight calculation for shipping charges.\n\nFor example, a 500-sheet ream of 20# copy paper may be specified \"10 M\". 1000 cut sheets (or two reams) will weigh , half of the four reams of cut paper resulting from the 20# basis ream of paper.\n\nPaper thickness, or caliper, is a common measurement specified and required for certain printing applications. Since a paper's density is typically not directly known or specified, the thickness of any sheet of paper cannot be calculated by any method. Instead, it is measured and specified separately as its caliper. However, paper thickness for most typical business papers might be similar across comparable brands. If thickness is not specified for a paper in question, it must be either measured or guessed based on a comparable paper's specification.\n\nCaliper is usually measured in micrometres (µm), or in the US also in mils (1 mil = 0.001 in = 25.4 µm). Commonly, 20-lb bond ranges between roughly in thickness.\n\n\n"}
{"id": "1187522", "url": "https://en.wikipedia.org/wiki?curid=1187522", "title": "Hemingray Glass Company", "text": "Hemingray Glass Company\n\nThe Hemingray Glass Company was an American glass manufacturing company. The company was founded by Robert Hemingray and Ralph Gray in Cincinnati in 1848. In its early years the company went through numerous and frequent name changes, including Gray & Hemingray; Gray, Hemingray & Bros.; Gray, Hemingray & Brother; Hemingray Bros. & Company and R. Hemingray & Company before incorporating into the Hemingray Glass Company, Inc in 1870. The Hemingray company had factories in Cincinnati, Ohio and Covington, Kentucky with main production in Muncie, Indiana. Though Hemingray was best known for its telegraph insulators, the company produced many other glass items including bottles, fruit jars, pressed glass dishes, tumblers, battery jars, fishbowls, lantern globes, and oil lamps. In 1933, the company was sold to the Owens-Illinois Glass Company but production remained in Muncie under the Hemingray name.\n\nThe main plant in Muncie shut down in 1966 and insulator production ceased. The complex is now the used by Gerdau Ameristeel, a steel production company headquartered in Brazil.\n\nHemingray was best known for producing telegraph insulators. To give an overview of the large variety of styles produced, the following table contains the twenty most common. There are two numbers given in this table: the Consolidated Design (CD) number and the style number. The CD number is from a classification system developed by collectors that refers to the shape of the insulator and is completely independent from the Hemingray Glass Company. However the style number (or name) was assigned by Hemingray to each insulator. Due to slight modifications in design over years of production single styles can span multiple CD numbers.\n\nBrookfield Glass Company\n\nInsulator (electrical)\n\n"}
{"id": "1702398", "url": "https://en.wikipedia.org/wiki?curid=1702398", "title": "Hubbard model", "text": "Hubbard model\n\nThe Hubbard model is an approximate model used, especially in solid-state physics, to describe the transition between conducting and insulating systems. The Hubbard model, named after John Hubbard, is the simplest model of interacting particles in a lattice, with only two terms in the Hamiltonian (see example below): a kinetic term allowing for tunneling (\"hopping\") of particles between sites of the lattice and a potential term consisting of an on-site interaction. The particles can either be fermions, as in Hubbard's original work, or bosons, when the model is referred to as either the \"Bose–Hubbard model\" or the \"boson Hubbard model\".\n\nThe Hubbard model is a good approximation for particles in a periodic potential at sufficiently low temperatures that all the particles are in the lowest Bloch band, as long as any long-range interactions between the particles can be ignored. If interactions between particles on different sites of the lattice are included, the model is often referred to as the \"extended Hubbard model\".\n\nThe model was originally proposed (in 1963) to describe electrons in solids and has since been the focus of particular interest as a model for high-temperature superconductivity. More recently, the Bose–Hubbard model has been used to describe the behavior of ultracold atoms trapped in optical lattices. Recent ultracold atom experiments have also realised the original, fermionic Hubbard model in the hope that such experiments could yield its phase diagram.\n\nFor electrons in a solid, the Hubbard model can be considered as an improvement on the tight-binding model, which includes only the hopping term. For strong interactions, it can give qualitatively different behavior from the tight-binding model and correctly predicts the existence of so-called Mott insulators, which are prevented from becoming conducting by the strong repulsion between the particles.\n\nThe Hubbard model is based on the tight-binding approximation from solid-state physics. In the tight-binding approximation, electrons are viewed as occupying the standard orbitals of their constituent atoms, and then \"hopping\" between atoms during conduction. Mathematically, this is represented as a \"hopping integral\", or \"transfer integral\", between neighboring atoms, which can be viewed as the physical principle that creates electron bands in crystalline materials, due to overlapping between atomic orbitals. The width of the band depends upon the overlapping amplitude. However, the more general band theories do not consider interactions between electrons explicitly. They consider the interaction of a single electron with the potential of nuclei and other electrons in an average way only. By formulating conduction in terms of the hopping integral, however, the Hubbard model is able to include the so-called \"on-site repulsion\", which stems from the Coulomb repulsion between electrons at the same atomic orbitals. This sets up a competition between the hopping integral, which is a function of the distance and angles between neighboring atoms, and the on-site Coulomb repulsion, which is not considered in the usual band theories. The Hubbard model can therefore explain the transition from metal to insulator in certain metal oxides as they are heated by the increase in nearest-neighbor spacing, which reduces the \"hopping integral\" to the point where the on-site potential is dominant. Similarly, this can explain the transition from conductor to insulator in systems such as rare-earth pyrochlores as the atomic number of the rare-earth metal increases, because the lattice parameter increases (or the angle between atoms can also change — see Crystal structure) as the rare-earth element atomic number increases, thus changing the relative importance of the hopping integral compared to the on-site repulsion.\n\nThe hydrogen atom has only one electron, in the so-called \"s\" orbital, which can either be spin up (formula_1) or spin down (formula_2). This orbital can be occupied by at most two electrons, one with spin up and one down (see Pauli exclusion principle).\n\nNow, consider a 1D chain of hydrogen atoms. Under band theory, we would expect the 1s orbital to form a continuous band, which would be exactly half-full. The 1D chain of hydrogen atoms is thus predicted to be a conductor under conventional band theory.\n\nBut now consider the case where the spacing between the hydrogen atoms is gradually increased. At some point we expect that the chain must become an insulator.\n\nExpressed in terms of the Hubbard model, on the other hand, the Hamiltonian is now made up of two components. The first component is the hopping integral. The hopping integral is typically represented by the letter formula_3 because it represents the kinetic energy of electrons hopping between atoms. The second term in the Hubbard model is then the on-site repulsion formula_4 that represents the potential energy arising from the charges on the electrons. Written out in second quantization notation, the Hubbard Hamiltonian then takes the form\n\nwhere formula_6 represents summation over nearest-neighbor lattice sites. Note that both \"t\" and \"U\" are positive quantities. formula_7 is the spin-density operator for spin formula_8 on formula_9-th site. The total density operator is formula_10 and occupation of formula_9-th site for the wavefunction formula_12 is formula_13.\n\nIf we consider the Hamiltonian without the contribution of the second term, we are simply left with the tight binding formula from regular band theory.\n\nWhen the second term is included, however, we end up with a more realistic model that also predicts a transition from conductor to insulator as the inter-atomic spacing is increased. In the limit where the spacing is infinite (or if we ignore the first term), the chain simply resolves into a set of isolated magnetic moments. Additionally, when there are some contributions from the first term, but the material remains an insulator, the overlap integral provides for exchange interactions between neighboring magnetic moments, which may lead to a variety of interesting magnetic correlations, such as ferromagnetic, antiferromagnetic, etc. depending on the exact solutions of the model. The one-dimensional Hubbard model was solved by Lieb and Wu using the Bethe ansatz. Essential progress has been achieved in the 1990s: a hidden symmetry was discovered, and the scattering matrix, correlation functions, thermodynamic and quantum entanglement were evaluated.\n\nAlthough the Hubbard model is useful in describing systems such as a 1D chain of hydrogen atoms, it is important to note that in more complex systems there may be other effects that the Hubbard model does not consider. In general, insulators can be divided into Mott–Hubbard type insulators (see Mott insulator) and charge-transfer insulators.\n\nConsider the following description of a Mott–Hubbard insulator:\n\nThis can be seen as analogous to the Hubbard model for hydrogen chains, where conduction between unit cells can be described by a transfer integral.\n\nHowever, it is possible for the electrons to exhibit another kind of behavior:\n\nThis is known as charge transfer and results in charge-transfer insulators. Note that this is quite different from the Mott–Hubbard insulator model because there is no electron transfer between unit cells, only within a unit cell.\n\nBoth of these effects may be present and competing in complex ionic systems.\n\nThe fact that the Hubbard model has not been solved analytically in arbitrary dimensions has led to intense research into numerical methods for these strongly correlated electron systems.\n\nExact treatment of the Hubbard model at absolute zero is possible using the Lanczos algorithm, which produces static and dynamic properties of the system. This method requires the storing of three vectors of the size of the number of states, which limits the number of sites in the lattice to about 20 on currently available hardware. With projector and finite-temperature auxiliary-field Monte Carlo, two statistical methods exist that also can provide an exact solution. For low temperatures and large lattice sizes convergence problems appear that lead to an exponential growth of computational effort due to the so-called sign problem.\n\nThe Hubbard model can also be studied within dynamical mean-field theory (DMFT). This scheme maps the Hubbard Hamiltonian onto a single-site impurity model, a mapping that is formally exact only in infinite dimensions and in finite dimensions corresponds to the exact treatment of all purely local correlations only. DMFT allows one to compute the local Green's function of the Hubbard model for a given formula_4 and a given temperature. Within DMFT, one can compute the evolution of the spectral function and observe the appearance of the upper and lower Hubbard bands as correlations increase.\n\n\n"}
{"id": "19984224", "url": "https://en.wikipedia.org/wiki?curid=19984224", "title": "Hódmezővásárhely Wind Farm", "text": "Hódmezővásárhely Wind Farm\n\nThe Hódmezővásárhely Wind Farm is an under construction wind power project in Csongrád County, Hungary. It will have 14 individual wind turbines with a nominal output of around 2 MW which will deliver up to 28 MW of power, enough to power over 11,077 homes, with a capital investment required of approximately US$45 million.\n\n"}
{"id": "13122821", "url": "https://en.wikipedia.org/wiki?curid=13122821", "title": "IEA-ECBCS Annex 48 : Heat Pumping and Reversible Air Conditioning", "text": "IEA-ECBCS Annex 48 : Heat Pumping and Reversible Air Conditioning\n\nIn June 2006, the IEA Energy in Buildings and Communities Programme (EBC, formerly ECBCS) Executive Committee decided to launch the three-year working phase of the Annex 48 on Heat pumping and reversible air conditioning.\n\n\nSubstituting a heat pump to a boiler may save more than 50% of primary energy, if electricity is produced by a modern gas-steam power plant (and even more if a part of that electricity is produced from a renewable source). “Heat Pumping” is probably today one of the quickest and safest solutions to save energy and to reduce CO emission. Most of air-conditioned commercial buildings offer attractive retrofit opportunities, because:\nThe retrofit of an existing building and, even more, the design of a new one should take all possibilities of heat pumping into consideration, in such a way to make air conditioning as “reversible” as possible. Different techniques are already available, but a recent survey of monitoring results established in Germany made still appear a lot of faults, lacks of optimisation and surprisingly low COP (coefficient of performance) after, at least, one year of operation. It appears that the many mistakes would not have been discovered without monitoring. It also appears that these mistakes and disappointing results are mainly due to a lack of good understanding of the dynamic behaviour of the systems at design stage, a lack of simulation work, a lack of instrumentation, for satisfactory commissioning, optimal control and fault detection.\n\nThe aim of this project is to promote the most efficient combinations of heating and cooling techniques in air conditioning.\n\n\nThese goals will be achieved by performing five different subtasks, whose content is very\nbriefly summarized hereunder:\n\n\n\n\n\n\n\n\n"}
{"id": "619351", "url": "https://en.wikipedia.org/wiki?curid=619351", "title": "Ignitron", "text": "Ignitron\n\nAn ignitron is a type of gas-filled tube used as a controlled rectifier and dating from the 1930s. Invented by Joseph Slepian while employed by Westinghouse, Westinghouse was the original manufacturer and owned trademark rights to the name \"Ignitron\". Ignitrons are closely related to mercury-arc valves but differ in the way the arc is ignited. They function similarly to thyratrons; a triggering pulse to the igniter electrode turns the device \"on\", allowing a high current to flow between the cathode and anode electrodes. After it is turned on, the current through the anode must be reduced to zero to restore the device to its nonconducting state. They are used to switch high currents in heavy industrial applications.\n\nAn ignitron is usually a large steel container with a pool of mercury in the bottom that acts as a cathode during operation. A large graphite or refractory metal cylinder, held above the pool by an insulated electrical connection, serves as the anode. An igniting electrode (called the \"ignitor\"), made of a refractory semiconductor material such as silicon carbide, is briefly pulsed with a high current to create a puff of electrically conductive mercury plasma. The plasma rapidly bridges the space between the mercury pool and the anode, permitting heavy conduction between the main electrodes. At the surface of the mercury, heating by the resulting arc liberates large numbers of electrons which help to maintain the mercury arc. The mercury surface thus serves as the cathode, and current is normally only in one direction. Once ignited, an ignitron will continue to pass current until either the current is externally interrupted or the voltage applied between cathode and anode is reversed.\n\nIgnitrons were long used as high-current rectifiers in major industrial and utility installations where thousands of amperes of AC current must be converted to DC, such as aluminum smelters. Ignitrons were used to control the current in electric welding machines. Large electric motors were also controlled by ignitrons used in gated fashion, in a manner similar to modern semiconductor devices such as silicon controlled rectifiers and triacs. Many electric locomotives used them in conjunction with transformers to convert high voltage AC from the overhead lines to relatively low voltage DC for the traction motors. The Pennsylvania Railroad's E44 freight locomotives carried on-board ignitrons, as did the Russian freight locomotive. For many modern applications, ignitrons have been replaced by solid state alternatives.\n\nBecause they are far more resistant to damage due to overcurrent or back-voltage, ignitrons are still manufactured and used in preference to semiconductors in some installations. For example, specially constructed \"pulse rated\" ignitrons are still used in certain pulsed power applications. These devices can switch hundreds of kiloamperes and hold off as much as 50kV. The anodes in these devices are often fabricated from a refractory metal, usually molybdenum, to handle reverse current during ringing (or oscillatory) discharges without damage. Pulse rated ignitrons usually operate at very low duty cycles. They are often used to switch high energy capacitor banks during electromagnetic forming, electrohydraulic forming, or for emergency short-circuiting of high voltage power sources (\"crowbar\" switching).\nAlthough the basic principles of how the arc is formed, along with many aspects of construction, are very similar to other types of mercury-arc valves, ignitrons differ from other mercury-arc valves in that the arc is ignited each time a conduction cycle is started, and then extinguished when the current falls below a critical threshold.\n\nIn other types of mercury-arc valve, the arc is ignited just once when the valve is first energised, and thereafter remains permanently established, alternating between the main anode(s) and a low-power \"auxiliary anode\" or \"keep-alive circuit\". Moreover, control grids are required in order to adjust the timing of the start of conduction.\n\nThe action of igniting the arc at a controlled time, each cycle, allows the ignitron to dispense with the auxiliary anode and control grids required by other mercury-arc valves. However, a disadvantage is that the ignition electrode must be positioned very accurately, just barely touching the surface of the mercury pool, which means that ignitrons must be installed very accurately within a few degrees of an upright position.\n\n\n"}
{"id": "53384718", "url": "https://en.wikipedia.org/wiki?curid=53384718", "title": "John Goss-Custard", "text": "John Goss-Custard\n\nDr John D. Goss-Custard is a British behavioural ecologist; he was one of the first scientists to carry out field work on foraging behaviour making use of optimising models, specifically the optimal diet model. After completing a BSc degree in Zoology at the University of Bristol, he moved to the University of Aberdeen to carry out research for a PhD degree, which he was awarded in 1966. The University of Aberdeen awarded him its DSc degree in 1987.\n\nGoss-Custard's PhD was based on the study of foraging in the Common Redshank. Subsequently, he worked at the Centre for Ecology and Hydrology's Furzebrook Research Station at Wareham, Dorset, leading an extensive project on the foraging of overwintering Eurasian Oystercatchers on the estuary of the River Exe. This project led to one of the first uses of agent-based modelling to predict ecological relationships in an extended landscape; the model, developed for the Exe estuary, was subsequently tested successfully on the Wash. This work was surveyed in a book that he edited.\n\nGoss-Custard retired from his post at CEH in 2002. Although he did not hold a substantive university post, Goss-Custard held an honorary position at the University of Exeter for many years, and is currently a Visiting Professor at the University of Bournemouth. He co-supervised PhD degrees with colleagues at the University of Exeter and also the University of Oxford.\n\n"}
{"id": "27410636", "url": "https://en.wikipedia.org/wiki?curid=27410636", "title": "KazKuat", "text": "KazKuat\n\nKazKuat is a state-owned organization established in February 2005 to accelerate the development of the domestic hydro-electric sector in Kazakhstan. KazKuat’s main goal is to construct and modernise electric power plants in the country The company is responsible for overlooking the building of the $81m Kerbulak Power Station on the Ili River to increase the capacity of the Kapshagai plant by 160 MW to provide peak-load power during the winter. KazKuat is also responsible for the modernisation plans, estimated at a cost of $47.9m, of Shardarin Power Station and a $270m project on the Bulak Power Station on the Irtysh River.\n"}
{"id": "36387873", "url": "https://en.wikipedia.org/wiki?curid=36387873", "title": "Kinetic Inhibitor", "text": "Kinetic Inhibitor\n\nKinetic Inhibitors are a new and evolving technology of a class of Low Dosage Hydrate Inhibitors (LDHI) that are polymers and copolymers (or a mix thereof). The most common of which is polyvinylcaprolactam (PVCap). These inhibitors are primarily utilized to retard the formation of clathrate hydrates. This problem becomes most prominent in flow lines when hydrocarbons and water flow through a line. The pressure and cold temperatures that could be exposed to the flow lines provides an environment in which clathrate hydrates can form and plug up the flow line. The inhibitors generally slow the formation of the hydrates enough so the fluid reaches storage without causing blockage.\n\nThere may be variations on the types and structure of the polymer used as the inhibitor. However, in general, while the gas hydrate is forming, the cavity where the hydrocarbon usually resides. Instead, the alkyl group penetrates the cavity followed by the carbonyl group of the amide group hydrogen bonding to the surface of the hydrate, thus preventing the formation of hydrates.\n\nThese inhibitors chemically retard the formation of clathrate hydrates. Dosage of such inhibitors in the fluid is usually .3% to .5% by weight of the fluid, compared to 10% to 50% by weight for thermodynamic inhibitors for prevention of clathrate formation. Due to this and other factors, kinetic inhibitors are more cost effective than thermodynamic inhibitors. However, if the fluid is in a static environment (i.e. packer fluid), kinetic inhibitors will have limited effect as hydrates will still form over sufficient time. At this point the only viable option is thermodynamic inhibitors.\n\n"}
{"id": "26781283", "url": "https://en.wikipedia.org/wiki?curid=26781283", "title": "Luhansk power station", "text": "Luhansk power station\n\nLuhansk power station (, formerly Luhanskaya GRES) is a thermal power station north of Shchastia, near Luhansk, Ukraine. It was built between 1950 and 1956 and its first generator was connected to the grid on 30 September 1956. In 1957, 4 turbines and 7 boilers went in service. In 1958, it was completed.\n\nBetween 1979 and 2004, Luhansk power station was modernized. It consists today of 4 units with 200 MW, 3 units of 175 MW and 1 unit of 100 MW. The generator building of Luhansk power station, which has 3 tall chimneys, is long.\n\nOn 3 September 2014, Ukrainian Aidar Battalion commander Sergei Melnychuk announced that they have mined the plant (which gives electricity to 98% of Luhansk Oblast), and that they will blow it up if the separatist forces advanced. On 17 September, part of the plant exploded due to combat in the zone, while firefighters couldn't act due to the fire and the mines planted.\n\n"}
{"id": "28901537", "url": "https://en.wikipedia.org/wiki?curid=28901537", "title": "Mechanical testing", "text": "Mechanical testing\n\nMechanical testing is an umbrella term that covers a wide range of tests, which can be divided broadly into two types:\n\nThere exists a large number of tests, many of which are standardized, to determine the various mechanical properties of materials. In general, such tests set out to obtain geometry-independent properties; i.e. those intrinsic to the bulk material. In practice this is not always feasible, since even in tensile tests, certain properties can be influenced by specimen size and/or geometry. Here is a listing of some of the most common tests:\n\n\n"}
{"id": "22049795", "url": "https://en.wikipedia.org/wiki?curid=22049795", "title": "Mundra Port Coal Terminal", "text": "Mundra Port Coal Terminal\n\nMundra Port Coal Terminal is the world's biggest coal importing terminal. It can handle 40 million tonnes of coal annually. It was built at a cost of Rs 2000 crore. It is located at Mundra Port, the largest private port in India.\n"}
{"id": "46217160", "url": "https://en.wikipedia.org/wiki?curid=46217160", "title": "México Pelágico", "text": "México Pelágico\n\nMéxico Pelágico is a 2014 Mexican documentary film. It was written by Jerónimo Prieto, who also directed it. It portrays the life of the open ocean of Mexico through the eyes of young conservationists.\n\nIt received the Director's Award at the 12th San Francisco International Ocean Film Festival in early 2015, and an audience award for \"Best Environmental Film\" at the 2015 Vail Film Festival.\n\n"}
{"id": "33554027", "url": "https://en.wikipedia.org/wiki?curid=33554027", "title": "Newfoundland Highland forests", "text": "Newfoundland Highland forests\n\nThe Newfoundland Highland forests are an ecoregion located in Newfoundland, Canada. Part of the Taiga, the total area of the region is 4,031,999 acres (1,631,692 hectares).\n\nThe terrain of this region is mostly between 300 and 700 meters above sea level. It is characterized by steep, rugged Palaeozoic and Precambrian rock, commonly bare or ridged.\n\nThe winters are snowy and cold, and the summers are cool. The region receives between 1,000 and 1,400 millimeters mean annual precipitation.\n\nMean annual temperature: 4°C\nMean summer temperature range: 11.5°C to 12°C\nMean winter temperature range: -3.5°C to -4°C.\n\nThe region contains boreal forests with dwarf black spruce (\"Picea mariana\") and balsam fir (\"Abies balsamea\"), dwarf kalmia (\"Kalmia polifolia\"), and various mosses. Various mixed evergreen and deciduous shrubs can be found in exposed areas.\n\nThe Arctic hare (\"Lepus arcticus\") is found in this region. It is their southernmost limit to their range.\n\nOther species include:\n\nThe region is threatened by an increase in harvest of wood for lumber and the pulp and paper industry. Higher elevations are mostly threatened by mining interests and granite quarrying. Further threats come from high all-terrain vehicle traffic, which affect some areas.\n\n80 to 90 percent of the region is considered to be intact. Large areas of the region are protected. These are:\n\n\n"}
{"id": "11022629", "url": "https://en.wikipedia.org/wiki?curid=11022629", "title": "Nickel–hydrogen battery", "text": "Nickel–hydrogen battery\n\nA nickel–hydrogen battery (NiH or Ni–H) is a rechargeable electrochemical power source based on nickel and hydrogen. It differs from a nickel–metal hydride (NIMH) battery by the use of hydrogen in gaseous form, stored in a pressurized cell at up to 1200 psi (82.7 bar) pressure. The Nickel–hydrogen battery was patented on Feb 25, 1971 by Alexandr Ilich Kloss and Boris Ioselevich Tsenter in the United States.\n\nNiH cells using 26% potassium hydroxide (KOH) as an electrolyte have shown a service life of 15 years or more at 80% depth of discharge (DOD)\nThe energy density is 75 Wh/kg, 60 Wh/dm specific power 220 W/kg.<ref name=\"NASA/CR—2001-210563/PART2 -Pag.10\">NASA/CR—2001-210563/PART2 -Pag.10 </ref> The open-circuit voltage is 1.55 V, the average voltage during discharge is 1.25 V.\n\nWhile the energy density is only around one third as that of a lithium battery, the distinctive virtue of the nickel–hydrogen battery is its long life: the cells handle more than 20,000 charge cycles with 85% energy efficiency and 100% faradaic efficiency.\n\nNiH rechargeable batteries possess properties which make them attractive for the energy storage of electrical energy in satellites and space probes. For example, the ISS, Mercury Messenger, Mars Odyssey and the Mars Global Surveyor are equipped with nickel–hydrogen batteries. The Hubble Space Telescope, when its original batteries were changed in May 2009 more than 19 years after launch, led with the highest number of charge and discharge cycles of any NiH battery in low earth orbit.\n\nThe development of the nickel hydrogen battery started in 1970 at Comsat and was used for the first time in 1977 aboard the U.S. Navy's Navigation technology satellite-2 (NTS-2). Currently, the major manufacturers of nickel-hydrogen batteries are Eagle-Picher Technologies and Johnson Controls, Inc.\n\nThe nickel-hydrogen battery combines the positive nickel electrode of a nickel-cadmium battery and the negative electrode, including the catalyst and gas diffusion elements, of a fuel cell. During discharge, hydrogen contained in the pressure vessel is oxidized into water while the nickel oxyhydroxide electrode is reduced to nickel hydroxide. Water is consumed at the nickel electrode and produced at the hydrogen electrode, so the concentration of the potassium hydroxide electrolyte does not change. As the battery discharges, the hydrogen pressure drops, providing a reliable state of charge indicator. In one communication satellite battery, the pressure at full charge was over 500 pounds/square inch (3.4 MPa), dropping to only about 15 PSI (0.1 MPa) at full discharge.\n\nIf the cell is over-charged, the oxygen produced at the nickel electrode reacts with the hydrogen present in the cell and forms water; as a consequence the cells can withstand overcharging as long as the heat generated can be dissipated.\n\nThe cells have the disadvantage of relatively high self-discharge rate, i.e. chemical reduction of Ni(III) into Ni(II) in the cathode:\n\nNiOOH + 0.5 H2 = Ni(OH)2.\n\nwhich is proportional to the pressure of hydrogen in the cell; in some designs, 50% of the capacity can be lost after only a few days' storage. Self-discharge is less at lower temperature.\nCompared with other rechargeable batteries, a nickel-hydrogen battery provides good specific energy of 55-60 watthours/kg, and very long cycle life (40,000 cycles at 40% DOD) and operating life (> 15 years) in satellite applications. The cells can tolerate overcharging and accidental polarity reversal, and the hydrogen pressure in the cell provides a good indication of the state of charge. However, the gaseous nature of hydrogen means that the volume efficiency is relatively low (60-100 Wh/L for an IPV (individual pressure vessel) cell), and the high pressure required makes for high-cost pressure vessels.\n\nThe positive electrode is made up of a dry sintered porous nickel plaque, which contains nickel hydroxide. The negative hydrogen electrode utilises a teflon-bonded platinum black catalyst at a rather high loading of 7 mg/cm2, the separator is knit zirconia cloth(ZYK-15 Zircar) Asbestos was used in the past.\n\nThe Hubble replacement batteries are produced with a wet slurry process where a binder agent and powdered metallic materials are molded and heated to boil off the liquid.\n\n\n\n\n\n"}
{"id": "57657930", "url": "https://en.wikipedia.org/wiki?curid=57657930", "title": "Ocean Nuclear", "text": "Ocean Nuclear\n\nOcean Nuclear (Chinese: 海核能源) is a financial services provider for the nuclear energy industry. It provides capital market services for energy projects worldwide and has negotiated nuclear infrastructure projects in more than 20 countries. The company is a member of the World Nuclear Association (WNA). \n\nOcean Nuclear is currently raising $5bn to fund infrastructure projects in nuclear energy. \n\nOcean Nuclear co-organises the Global Nuclear Investment Summit (GNIS). The first was held in Beijing in January 2018. The second took place in London in June 2018, in partnership with the Financial Times. \n"}
{"id": "37715967", "url": "https://en.wikipedia.org/wiki?curid=37715967", "title": "Offshore crane shock absorber", "text": "Offshore crane shock absorber\n\nAn offshore crane shock absorber is a gas spring with hydraulic damping used to reduce dynamic loads during offshore lifting. The lifting capacity of the offshore crane can be increased significantly during lifting in high sea states if a shock absorber is fitted. \nOffshore lifting capacity is governed by classification society rules. The fundamental rule is that the design load should not be exceeded. The design load is given by:\nformula_1\n\nWhere: \nformula_2 - Crane design load\nformula_3 - Dynamic coefficient\nformula_4 - Safe working load\nformula_5 - Acceleration of gravity \n\nThe classification societies require that formula_6 ≥1.3 for offshore cranes. This means that formula_4 cannot exceed formula_8 even for platform lifts. \n\nTo calculate the dynamic load the classification societies use energy equations. The kinetic energy of the load is expressed as:\nformula_9\nWhere: \nformula_10 - Kinetic energy of the load \nformula_11 - Relative velocity between crane hook and load \n\nEnergy absorbed by the crane is given by:\nformula_12\nWhere: \nformula_13 - Spring energy stored in crane structure\nformula_14 - Stiffness of offshore crane\nformula_15 - Vertical displacement of crane hook\n\nThe pring force is given by:\nformula_16\nCombining the two previous equations yields an alternative description of the energy stored in the crane structure:\nformula_17\nDuring an offshore lift the crane does not start to absorb energy from the load before the force in the crane wire exceeds the static weight of the load, which means that we can write the energy absorption of the crane as:\nformula_18\n\nSetting formula_13 equal to formula_10:\nformula_21\nAnd solving for formula_3: \nformula_23\n\nThe dynamic factor will always be larger than 1 because there will always be a velocity difference between the crane hook and the load during offshore lifts. The value of formula_3 determines the lifting capacity as a function of formula_11 which is dependent on the significant wave height.\nIf the offshore crane has a shock absorber mounted it will absorb energy according to:\nformula_26\nWhere: \nformula_27 - Energy absorbed by shock absorber\nformula_28 - Stroke utilization (safety factor), standard value 0.9\nformula_29 - Efficiency of shock absorber, usually possible to get close to 0.9\nformula_30 - Stroke length\n\nAdding this to the energy balance yields:\nformula_31\n\nAgain solving for ψ:\n\nformula_32\n\nThe plot to the right shows the effect of the shock absorber stroke length.\n\nThe relative velocity is defined by several classification societies as:\n\nformula_33\n\nWhere:\n\nformula_34 - Maximum crane lifting velocity at this SWL \nformula_35 - Vertical velocity of the deck which the load is placed on\nformula_36 - Vertical velocity of the crane tip due to wave motion\n\nformula_34 has to be gathered from crane user manual or be measured. The deck velocity can be estimated from Table 1.\n\nThe crane tip velocity, formula_38, can be estimated by Table 2.\n\nThese estimations are conservative and for large ships/rigs they may deviate significantly from reality.\nA plot of the dynamic factor versus significant wave height for a crane mounted on a semisubmersible and lifting from supply boat is shown above to the right.\n"}
{"id": "38610659", "url": "https://en.wikipedia.org/wiki?curid=38610659", "title": "Ogishima Solar Power Plant", "text": "Ogishima Solar Power Plant\n\nThe Ogishima Solar Power Plant (扇島太陽光発電所) is a 13 MW solar photovoltaic power station located on the waterfront in Kawasaki. It is the second, and the largest solar plant built by Tepco, and was completed on December 19, 2011. In the first year of operation, it produced 15,059 MWh, a capacity factor of 0.13, which was about 10% greater than anticipated. An unusual feature of the plant is that the panels are mounted at a fixed angle of 10°, instead of the 30°, which would normally be considered optimal for this latitude.\n\n"}
{"id": "55439100", "url": "https://en.wikipedia.org/wiki?curid=55439100", "title": "Pavement light", "text": "Pavement light\n\nPavement lights (UK), vault lights (US), floor lights, or sidewalk prisms are flat-topped walk-on skylights, usually set into pavement (sidewalks) or floors to let sunlight into the space below. They often use anidolic lighting prisms to throw the light sideways under the building. They declined in popularity with the advent of cheap electric lighting. \n\nSome cities are systematically removing historic sidewalk lights; others are restoring them. Pavement lights have been used in a few new architectural designs. \n\nSidewalk prisms are a method of daylighting basements, and are able to serve as a sole source of illumination during the day. At night, lighting in the basements beneath produces a glowing sidewalk. Vault lights may be used to make subterranean space useful. They are more common in city centers, dense, high-rent areas where space is valuable. Historically, landlords took an interest in improving not only the floor area ratio, but the amount of space that was naturally lit, on the grounds that this was profitable. Occupiers valued daylight not only as a way of saving on artificial lighting costs (which were higher historically), but also as a way to let premises remain cooler in summer, and a way to save on ventilation costs (if using gas lighting rather than arc lamps or early incandescent lights).\n\nPavement lights and related products were historically marketed as a way of saving on artificial lighting costs and making space more usable and pleasant. Modern studies of similar daylighting technology provide evidence for those claims.\n\nVault lights also are used in floors under glass roofs, for example in Budapest's historic Párizsi udvar and New York's mostly-demolished old Pennsylvania Station. Vault lights also could be set into the basement floor, underneath other vault lights, creating a double-deck arrangement, which would light the subbasement. Manhole covers and coalhole covers with lighting elements were also made. Some steps have vault lights set into the vertical stair risers.\n\nOlder cities and smaller centers around the world have or have had pavement lights. Most such lights are approximately a century old, although lights are being installed in some new construction.\n\nA basement that extends below a sidewalk or pavement is called an areaway, a sidewalk vault, or a hollow sidewalk. In some cities, these areaways were created by the raising of the street level to combat floods, and in some cases they form an (often now abandoned) underground tunnel network. To light these spaces, sidewalks incorporated gratings, which were a trip hazard and let water and street dirt as well as light into the basement. Replacing the open gratings with glass was an obvious improvement. \n\nSidewalk prisms developed from deck prisms, which were used to let light through the decks of ships. The earliest pavement light (Rockwell, 1834) used a single large round glass lens set in an iron frame. The large lens was directly exposed to traffic, and if the lens broke, a large hole was left in the pavement, which was potentially unsafe for pedestrians.\n\nThaddeus Hyatt corrected these faults with his \"Hyatt light\" of 1854. Many small lenses (\"bull's-eyes\") were set in a wrought-iron frame, (later cast iron), and the frame included raised nubs around each lens to improve traction in wet weather and to protect them from damage and wear. Even if all the lenses were broken out, the panel would still be safe to walk on.\n\nIn the 1930s, London authorities ruled that glass sections could not be larger than 100 mm by 100 mm. Modern glass floors are made of laminated and toughened glass pavers, which can be substantially larger. They have an upper protective layer that can be replaced if it becomes chipped or cracked. The top surface of the pavers may also be chosen and treated to improve traction.\nWrought iron, cast iron, and stainless steel frames have all been used. Reinforced concrete slabs began to replace iron frames in the 1890s in New York. Benefits claimed included less condensation (due to the lower thermal conductivity) and a less slippery surface when wet. Concrete panels may be pre-cast or cast in-situ. \n\nLate concrete panels often were made with metal-framed \"armored prisms\", which were intended to prevent breakage and make replacing individual prisms easier. The glass is not cast into the concrete but caulked into the frame. Rather than chiselling out the old glass, the glass can be popped out of the frame.\n\nTranslucent concrete has also been proposed as a floor material. This would essentially make it a vault light with very small (fiberoptic) lighting elements. It also innately redirects the light from the angle of incidence to an angle ~parallel to the optical fibers (usually, perpendicular to the surface of the concrete).\n\nThe transparent elements may be referred to as prisms or lenses (depending on shape), or as jewels.\n\nThe glass in many old pavement lights is now either purple or straw-colored. This is a side-effect of the manufacturing process. Pure silica glass is transparent, but older glass manufacture often used silica from sand, which contains iron and other impurities. Iron produces a greenish tint in the finished glass. To remove this effect, a \"decolorizer\" such as manganese dioxide (\"glassmakers' soap\") was added during the manufacture of the glass.\n\nWhen exposed to ultraviolet light, the manganese slowly \"solarizes\", turning purple, which is why many existing sidewalk prisms are now purple. WWI increased demand for manganese in the US and cut off the supply of high-grade ore from Germany, so selenium dioxide was used as a decolorizer instead. Selenium also solarizes, but to a straw color.\n\nReplacement glass that has been tinted purple deliberately, in order to match the current colour, has been used in some historic restoration projects.\n\nIn 1871 London, Hayward Brothers patented their \"semi-prism\": changing the shape of the glass by adding pendant prisms to the underside reflects the light sideways, allowing it to light the area under the main building. The pendant shapes were right-angle (\"half\") prisms, which reflected all incoming light sideways. The horizontal ridges protruding from the top of the prism let it be set into an opening in an iron or cement grating.\n\nSome cast glass pendant prisms have flat portions to shed light directly below, as well as throwing it sideways under the main body of the building (see image). Some prisms were made with multiple pendant prisms, either as a Fresnel-lens-like sheet of identical prisms (\"multi\") or a sheet of dissimilar prisms that could distribute the light (\"three-way\" etc.).\n\nThe precise angles at which the prisms refracted or reflected light was important. An installation would generally consist of multiple different prescriptions of prism, chosen either by an on-site expert contractor or by a layman using standard algorithms. This also would diffuse the light somewhat, as would the rough glass surfaces (the lenses are translucent, not transparent).\n\nLarger castings are more expensive, not only because they use more glass, but because they take longer to cool. Modern glass floors use laminate sheet glass some centimeters (more than an inch) thick; it often is transparent. \nSynthetic resin composites (such as fiberglass), as well as plastics such as Lexan, have been proposed to replace missing prism lights. Translucent decking panels made of fiberglass are often used for balconies which would otherwise shade the windows below them. Peel-and-stick prism films recently have come on the market, with acrylic micro-prisms that internally reflect light somewhat like glass pendant prisms.\n\nIn some cases, a second vertical curtain of prisms was installed under the building sill. These were analogous to the prism transoms used over above-ground windows and doors. The light could be bent in two stages and used to daylight the whole basement.\n\nThe areaway under a sidewalk light usually has a masonry wall separating it from the soil under the street, although it may extend partly under the street. Support for the vault light frames varies. Steel cross-beams supported by columns are common in older buildings; metal decks are common in newer ones.\n\nSome modern pavement lights are quite different from historic ones, so restoration and replacement may use different techniques and parts.\n\nA few companies now manufacture and sell vault lights, either as glass-only, prefab panels, or installation. Construction methods and prices vary widely. Historically, glass lenses were standardized by each manufacturer; some modern manufacturers produce standardized prisms. Some firms also supply replacement glass castings to order. Cost varies greatly; shapes needing complicated articulated moulds are more expensive.\n\nModern caulking materials are used for caulking in replacement glass. Broken and damaged frames can be patched, re-welded, or re-cast. Generally speaking, restoration requires only simple tools and technology.\n\nPromptly repairing sidewalk cracks, and avoiding de-icers that will corrode metal, helps keep the supporting structure dry and in good repair. Keeping a sidewalk light watertight does not cost much in time or materials. Vaults generally last many decades, and many extant vaults are more than a century old.\n\nDespite their reusability and repairability, old panels often are landfilled. However, the city of Victoria, Canada is stockpiling removed pavement light panels for future restoration projects. Often, individual broken sidewalk prisms are not replaced, but instead, the opening is filled with concrete or other opaque materials, such as metal, wood, and asphalt.\n\nWhen a building is renovated, vault lights may be removed or concreted over. For instance, the floor of New York's mostly-demolished old Pennsylvania Station was made of vault lights, to let light through the concourse floor onto the platforms. The undersides of the lights can still be seen, but the tops have been concreted over (see images).\n\nWhile some cities have preservation measures for vault lights, others actively remove them and fill areaways. Sometimes the outside appearance of the lights is retained while filling the areaway and setting the lights in a concrete pad, removing their daylighting function. Some areaways are \"mothballed\"; that is, filled with gravel that could later be removed.\n\nAreaways are used in some cities as a convenient place to run utilities, which may make the cities reluctant to give areaways legal protection. In some cases, utility construction leads to areaways being filled.\n\nThe load-bearing strength of vault lights varies widely with span, construction, and state of repair. Some damaged vaults may not be able to support a fire engine, which a sidewalk vault in sound condition should be able to do.\n\nDamp areaways may corrode the steel load-bearing elements supporting the pavement roof. Moisture may come from leakage from above or from groundwater from below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "29878101", "url": "https://en.wikipedia.org/wiki?curid=29878101", "title": "Pedro Arrojo-Agudo", "text": "Pedro Arrojo-Agudo\n\nPedro Arrojo-Agudo is a Spanish physicist, economist and environmentalist, a professor at the University of Zaragoza. He was awarded the Goldman Environmental Prize in 2003, for his contributions to conservation of water.\n"}
{"id": "29419999", "url": "https://en.wikipedia.org/wiki?curid=29419999", "title": "Pin-point method (ecology)", "text": "Pin-point method (ecology)\n\nThe pin-point method (or point-intercept method) is used for non-destructive measurements of plant cover and plant biomass. \n\nIn a pin-point analysis, a frame (or a transect) with a fixed grid pattern is placed above the vegetation. A pin is inserted vertically through one of the grid points into the vegetation and will typically touch a number of plants. The number of times the pin touches different plant species is then recorded. This procedure is repeated at each grid point. Vertical rulers connected to the frame are used to prevent horizontal drift of the pins and to measure the height of vegetation hit by the pins.\n"}
{"id": "41869945", "url": "https://en.wikipedia.org/wiki?curid=41869945", "title": "Port Bonython oil spill", "text": "Port Bonython oil spill\n\nThe Port Bonython oil spill occurred on 30 August 1992, when the fuel tank of the tanker \"Era\" was pierced by the bow of the tugboat \"Turmoil\" during berthing operations in upper Spencer Gulf, South Australia. Wind and swell were high and 296 tonnes of bunker fuel were released into Spencer Gulf. The incident resulted in 500 oiled birds and damage to 15 km of mangrove and seagrass habitat south-west of Port Pirie.\n\nThe spill occurred at 10.52am, as the \"Era \"was berthing at the Port Bonython wharf in rough seas. The vessel had arrived to load oil for the Kwinana refinery. The \"Era\" was chartered by BP and the tugboat involved, \"Turmoil\", was property of the Adelaide Steamship Company. Also present were the tugboat \"Taminga\" and the line-boat \"Wanilla\". A lug on the \"Turmoil\"'s bow pierced the hull of the tanker, leaving a 20 cm hole in the hull and rupturing her fuel tank. The 94,287 deadweight tonne vessel was listed 5 degrees to slow the flow of heavy bunker oil, and the leak was eventually stopped at 2.12pm. A total of 296 tonnes of heavy fuel oil had escaped into the sea.\n\nThe slick was described in the official report as \"a moderate sheen with streaks of heavy dark oil moving round the stern of the vessel.\" Tug boats were driven through the slick in an attempt to break it up, and the spraying of chemical dispersant commenced soon after the spill occurred. 400 litres of Corexit 7764 and 4200 litres of Corexit 9527 were supplied by Santos.\n\nThe absence of an appropriate crane at the jetty made loading \"Turmoil\" with dispersant at Port Bonython impossible. Instead, the tugboat proceeded to Whyalla to load, while the dispersant was transported there by road. The Santos-owned Shark Cat \"Tregalana\" loaded in Santos' boat harbor. The other vessels present\" \"were unable to assist due to the \"Wanilla\" requiring towing by the \"Taminga.\" A line had fouled its propeller during the berthing of the \"Era\".\n\nThree different chemical dispersants were used: Corexit 7764, Corexit 9527 and Ardrox 6120. Strong north-westerly winds of up to 25 knots combined with tide and current initially transported the oil slick eastwards, towards Port Germein. The high winds prevented the early use of containment booms.\n\nAssisted by the Department of Environment and Planning, volunteers prepared for the possibility of a 'massive clean-up'.\n\nThe 20 cm hole in the fuel tank of the \"Era\" was temporarily repaired, and the tanker was loaded in preparation to sail for Kwinana, Western Australia in the evening. The Department of Marine and Harbors announced that the oil spill posed no threat to wildlife. At this time, only 30 tonnes of oil was left at the sea's surface. The slick was estimated to be dense, 500 m long and 100 m wide.\n\nThe \"Era\" departed Port Bonython for Kwinana, Western Australia at 5.30am, laden with 22,990 tonnes of crude oil from the Cooper Basin.\n\nThe spill had attracted national news media attention. A visible 10-20 tonne slick remained on the water's surface after the majority of the oil had been sprayed with chemical dispersant. The dispersal of the slick involved 50 people, five boats, a helicopter and spotter aircraft.\n\n25 Special Emergency Services (SES) personnel were placed on standby. Staff from the Australian Marine Oil Spill Centre (AMOSC) were present to supervise the use of booms to channel the oil, and some oil was recovered by this method. At this time, there had been no reports of dead fish or birds. About 10 volunteers were ready at Port Pirie to handle any oiled wildlife. Council workers were also dispatched from the Mount Remarkable District Council to assist.\n\nProfessional fishermen, including the 39-member Spencer Gulf Prawn Fishermen's Association called for an independent inquiry into the incident and a review of all berthing procedures. Spokesperson Mick Puglisi stated that he believed the \"Era \"should never have berthed in such extreme weather.\n\nA Department of Marine and Harbors spokesman said that it was unlikely that anyone would be charged under the Marine Pollution by Oil and Noxious Substances Act.\n\nLocal pilot Syd Cheesman said that he had seen places where \"the oil is on the bottom and the water is on top of it.\" He also described the slick as covering \"an extensive area\".\n\nAbout 10 tonnes of oil had washed into mangroves overnight. A slick 15 km long had been seen drifting 1 km off Port Pirie. A makeshift animal hospital was established in Port Pirie. Small boats searched at first light for oiled birds.\n\nThe visible oil was described by David Gray of the Australian Maritime Safety Authority as 'a sheen on the water'. Some of this sheen impacted the mangroves and a number of tidal creeks to the south-west of Port Pirie.\n\nThe estimated number of impacted birds was revised from 100 birds to up to 500 birds as the search and recovery efforts began. Rescuers expressed concern about the birds' ingestion of oil as a consequence of preening.\n\nThe oil had settled in the mangroves south-west of Port Pirie. More than 500 birds had now been affected and 20 dead birds had been recorded. Prawn fishermen and Fisheries officers collected samples of prawn larvae and seabed sediment for analysis. Fisherman David Wilks participated in the bird rescue and recovery and described the scene:\n\n\"As the tides recede, oil is plastered black all over the mangroves, oil is pouring out of the creeks with the tides and there's a film of oil left behind which is seeping into the seagrass beds.\"\n\nTeams of volunteers scrubbed oiled birds overnight and prepared them for transport to the RSPCA in Lonsdale. Treated birds included pelicans, cormorants, grebes and herons. More than a dozen dead birds were collected on this day. Most of the dead birds recovered were cormorants. Fauna rescue researcher Ms Erna Walraven was intended to receive the dead birds in Sydney for further study. National Parks and Wildlife put a call out for more flat-bottomed boats and experienced boat operators to assist.\n\nSenior Environment and Planning Department officer Brian Wagstaff said the spill posed no long or medium-term ecological risk. He stated: \"There won't be a die-back of mangroves, although there may be some loss of leaves... it's the birds that are the main concern.\"\n\nAt the time of the spill, environmental scientist Doug Reilly warned of the risk the slick posed to important breeding grounds of western king prawns. He also raised concerns about the potential threat chemical dispersant could pose to marine ecosystems. Mangrove and seagrass habitats were impacted, as were native birds and the local fishing industry.\n\n10 days after the spill, the South Australian Fishing Industry Council stated that 25 families had lost their income in the short term due to the spill. The Council stated that if the spill was found to have caused long-term damage to fishing grounds, millions of dollars in compensation would be sought from whoever was deemed liable.\n\nTwo inquiries into the incident reached the conclusion that it was an unforeseeable accident for which no-one was to blame.\n\nThe vessel continued to sail as the \"Era\" until 1997, when it was renamed \"Frixos\". It was decommissioned in 2010. Its final resting place was the Gadani ship-breaking yard northwest of Karachi, Pakistan.\n</div>\n\n"}
{"id": "3488128", "url": "https://en.wikipedia.org/wiki?curid=3488128", "title": "Reynolds transport theorem", "text": "Reynolds transport theorem\n\nIn differential calculus, the Reynolds transport theorem (also known as the Leibniz–Reynolds transport theorem), or in short Reynolds' theorem, is a three-dimensional generalization of the Leibniz integral rule which is also known as differentiation under the integral sign.\nThe theorem is named after Osborne Reynolds (1842–1912). It is used to recast derivatives of integrated quantities and is useful in formulating the basic equations of continuum mechanics.\n\nConsider integrating over the time-dependent region that has boundary , then taking the derivative with respect to time:\nIf we wish to move the derivative within the integral, there are two issues: the time dependence of , and the introduction of and removal of space from due to its dynamic boundary. Reynolds' transport theorem provides the necessary framework.\n\nReynolds' transport theorem can be expressed as follows:\n\nin which is the outward-pointing unit normal vector, is a point in the region and is the variable of integration, and are volume and surface elements at , and is the velocity of the area element (\"not\" the flow velocity). The function may be tensor-, vector- or scalar-valued. Note that the integral on the left hand side is a function solely of time, and so the total derivative has been used.\n\nIn continuum mechanics, this theorem is often used for material elements. These are parcels of fluids or solids which no material enters or leaves. If is a material element then there is a velocity function , and the boundary elements obey\nThis condition may be substituted to obtain:\n\nIf we take to be constant with respect to time, then and the identity reduces to\n\nas expected. (This simplification is not possible if the flow velocity is incorrectly used in place of the velocity of an area element.)\n\nThe theorem is the higher-dimensional extension of differentiation under the integral sign and reduces to that expression in some cases. Suppose is independent of and , and that is a unit square in the -plane and has limits and . Then Reynolds transport theorem reduces to\n\nwhich, up to swapping and , is the standard expression for differentiation under the integral sign.\n\n\n\n"}
{"id": "6458206", "url": "https://en.wikipedia.org/wiki?curid=6458206", "title": "Rummy-nose tetra", "text": "Rummy-nose tetra\n\nThe rummy-nose tetra (\"Hemigrammus rhodostomus\") is a species of tropical freshwater characin fish originating in South America, popular among fishkeepers as an aquarium fish. One of many small tetras belonging to the same genus, it is on average 5 cm (2 in) long when fully grown, and is a long established favourite among tropical fishkeepers. The fish is one of several very similar species including \"Hemigrammus bleheri\", and \"Petitella georgiae\", and it is possible that more recently collected specimens available in the aquarium trade are members of one or other of these similar species. The common name applied to most of these fishes is \"rummy-nose tetra\", though other common names are in circulation (such as \"firehead tetra\" for \"H. bleheri\", according to FishBase).\n\nThe rummy-nose tetra is a torpedo-shaped fish, whose basal body colour is a translucent silvery colour suffused in some specimens with a greenish tint: iridescent green scales are frequently seen adorning the fish at the point where the fontanel (a part of the head roughly corresponding to the forehead in humans) meets the body. The fins are hyaline, with the exception of the tail fin, this fin being adorned with black and white horizontal stripes, variable in number, but usually comprising one central black stripe in the central portion of the tail fin, with two horizontal black stripes upon each caudal fin lobe, the spaces between the stripes being white, the total count of black stripes being five. The head is a deep red colour, iridescent in lustre, with the red continuing into the iris of the eye, and some red colouration extends beyond the operculum or gill plate into the anteriormost section of the body proper. Some specimens classified as \"Hemigrammus rhodostomus\" possess three black tail stripes instead of five, and some specimens classified as \"Petitella georgiae\" have a black stripe in the caudal peduncle extending forwards into the body, surmounted above by a thin iridescent gold line: however, whether these features are reliable determinants of species identity has yet to be fully ascertained.\n\nMale and female individuals exhibit no obvious visual differences, other than increased fullness of the body outline in ripe females.\n\nThe three different species of fish thus known as rummy-nose tetras have the following distributions:\n\nAll three species of rummy-nose tetra inhabit river stretches whose water chemistry is mineral-deficient (soft), acidic, and frequently tainted with humic acid decay products from leaf litter upon the river substrates (known as 'blackwaters' because of their appearance). Aquatic plants are sometimes present in these stretches of water, though the upper reaches of the Rio Negro are less densely populated with aquatic flora than the other rivers due to shading from the rainforest canopy. The fishes preferentially inhabit the middle and mid-lower water regions.\n\nThe details of aquarium maintenance for all three species are more or less identical: the rummy-nose tetra is a schooling fish, which exhibits tight schooling behaviour both in the wild and in the aquarium. Consequently, these fishes should be maintained in groups of no fewer than six individuals, with larger numbers being preferable where space permits-these tetras preferring about a 25 gallon/80 litre aquarium. All three species are lovers of warm aquarium water, the temperature range for maintenance being 24 °C to 31 °C, with the fishes sometimes requiring temperatures as high as 33 °C for breeding. Consequently, compatibility of these fishes with cooler water fishes is contra-indicated: for example, panda corydoras would be a bad choice of companion as these fishes prefer lower temperatures, and there is little overlap in the temperature ranges of the two species. Many fish can be kept with rummy-nose tetras, some including smaller gourami, tetras, barbs, danios, Australian rainbows, and various catfish, such as Ancistrus. These fish cannot be kept with African cichlids as they have very different water parameters.\n\nThe water chemistry preferred by these fishes, as might be inferred readily from that of the wild habitat, is soft, acidic water (hardness no higher than 6° dH and pH around 6.4–7.0 is \"definitely\" preferred if possible) though for maintenance purposes, the pH of the aquarium water can range from 5.6 to 7.4. However, if captive reproduction is to be attempted, the rummy-nose tetra \"needs\" soft, acidic water, as \"Tropical Fish Hobbyist\" magazine recently reported that research work upon these fishes indicates that high levels of calcium ions in the water induce sterility in these fishes. A planted aquarium is welcomed by these fishes, particularly if the plants include fine-leaved species such as \"Cabomba\" and \"Myriophyllum\".\n\nFeeding presents relatively few problems, as the fishes will eagerly devour a range of prepared as well as live fish foods. In common with numerous other tetras, these fishes are particularly fond of live bloodworms (these are the aquatic larvae of a midge belonging to the genus \"Chironomus\") and will also devour live \"Daphnia\" avidly. Unlike those tetra species which adapt to surface feeding in the aquarium, rummy-nose tetras are not considered likely to add live mosquito larvae to their diet in the aquarium, though instances where these fishes discover and enjoy this food are possible: usually, rummy-nose tetras prefer to take their foods in the middle and lower regions of the aquarium.\n\nLifespan for the rummy-nose tetra in the aquarium is usually 5 to 6 years with careful maintenance. Exceptional specimens can live for more than 8 years.\n\nThe fish is interesting in that it can act as a \"mine canary\" in an aquarium, alerting the aquarist to potential pollution problems in an aquarium. When levels of certain metabolic wastes (ammonia, nitrites and nitrates) exceed critical levels, the intense ruby-red colour of the fish's facial area becomes pale. The fishes also become pale in appearance immediately after the disturbance that takes place in the aquarium following a water change, but in this instance, once clean water has been supplied, the intense deep red colour returns. \"Persistent\" paleness of the head is to be taken as a sign that water chemistry parameters in the maintenance aquarium are in need of adjustment, and that pollutant levels are becoming dangerous for the inhabitants.\n\nRummy-nose tetras present serious challenges even to experienced aquarists from the standpoint of breeding, primarily due to two factors: the likelihood of sterility ensuing if the prospective parents are maintained in water with too high a level of dissolved calcium ions, and the slow growth rate of the fry. An additional problem is that gender differentiation is difficult by visual inspection alone, making pair selection partly a matter of luck unless an obviously gravid female is available for selection. Again, identical remarks apply to all three species listed above.\n\nThe breeding aquarium for the rummy-nose tetra, in addition, needs to be sterilised prior to use, as the fish produces eggs that are notoriously sensitive to bacterial and fungal infection. Use of an antifungal agent is strongly advised once spawning is completed in order to prevent various fungi from attacking the eggs.\n\nNeedless to say, given the above remarks about likely sterility if the fishes are maintained in water whose chemical parameters are incorrect, prospective parents \"must\" be maintained in soft, acidic water throughout their lives if they are to remain capable of reproduction. Failure to do so will destroy any chances of success from the very start. Furthermore, it is highly advisable to filter the water of the breeding aquarium over peat, or alternatively use one of the commercially available 'Blackwater Tonic' additives to provide the necessary chemical environment conducive to reproduction. Parent fishes should, in addition, be conditioned heavily with copious live foods to bring them into prime breeding condition.\n\nThough the fishes prefer to spawn among fine leaved plants, one problem that the aquarist faces in this regard is that the majority of fine leaved plants available in the aquarium prefer high light levels (\"Cabomba\" is a particular case in point) while the rummy-nose tetra prefers to spawn under subdued lighting conditions. Workarounds for this include the use of Java moss (a plant that thrives even in very low light levels, and is an ideal spawning medium for many fishes) or the use of synthetic alternatives (\"spawning mops\" made of nylon are typically deployed where a suitable natural plant is unavailable).\n\nThe parents should be introduced to the breeding aquarium up to 7 days before spawning, fed heavily with live foods, and kept under subdued lighting. In addition, the parents tend to prefer to spawn under quiet conditions, and thus the aquarium should be sited away from areas of busy human traffic. The temperature should be slowly raised to 32 °C, and sometimes 33 °C may be needed depending upon the individual specimens. Spawning is difficult to observe, taking place as it does under subdued lighting, and while chase sequences followed by the adoption of a side-by-side position by the parents amid the provided spawning media may be taken as indication that spawning is indeed taking place, this is by no means certain. The judicious use of a low-power flashlight to observe spawning may be helpful in determining if eggs are actually being produced.\n\nThe species is not noted as a particularly egregious egg eater (unlike, for example, the lemon tetra—see that article for notes on egg-eating behaviour in characins) but it is still advisable to remove the parents once spawning is completed. At this stage, antifungal agents to protect the eggs should be added, and indeed are vital in the case of this difficult, sensitive species.\n\nOnce spawning is completed, it is advisable to keep the aquarium under dim lighting conditions until the eggs have hatched and the fry are free-swimming. While rummy-nose tetras do not need their eggs to be kept in total darkness as is the case for neon and cardinal tetras, the eggs are known to exhibit some degree of photosensitivity, and subdued lighting is highly advisable during egg development in the breeding aquarium.\n\nFertile rummy-nose tetra eggs take approximately 72 to 96 hours to hatch at 32 °C. The fry spend a further 24 to 48 hours absorbing the yolk sac, whereupon they become free-swimming. At this stage, the fry should be fed with infusoria or a special egglayer fry food, and frequent partial water changes (around 10% of the aquarium volume every 24 to 48 hours) initiated.\n\nHaving overcome the challenge of persuading the fishes to spawn, the aquarist soon discovers that the rummy-nose tetra presents another hurdle to successful captive reproduction—the fry are among the slowest growing of \"all\" characins, and indeed among the slowest-growing of all popular aquarium fishes. Infusoria and other similar foods are required for the fry for a \"minimum\" of three weeks, and it is not unknown for the fishes to take as long as \"twelve weeks\" to migrate to larger foods, growth rates being particularly sensitive to temperature. Success at raising fry to a size where they are capable of taking larger food morsels is much more likely to be achieved if the fry are kept at temperatures above 30 °C for the first 3 months of life: even then, attrition rates due to the appearance of diseases among the fishes (most frequently bacterial infections) can be severe.\n\nIt can take as long as 6 months to raise fry to juvenile sizes where they are capable of eating live daphnia on a regular basis. During this time, they are likely to be sensitive to sudden changes in water chemistry, and managing pollutants in the fry aquarium is made all the more difficult by the need to maintain low mineral content in the aquarium water during development in order to prevent sterility affecting the fishes—the buffering capacity of the aquarium is likely to be low as a result of the low concentration of bicarbonate ions that is needed in addition to the low concentration of calcium ions. Taking all of these factors into account, the fish is a major breeding challenge for the aquarist, and success may depend at least partly upon luck.\n\n\n\n"}
{"id": "53072804", "url": "https://en.wikipedia.org/wiki?curid=53072804", "title": "Search for the Super Battery", "text": "Search for the Super Battery\n\nSearch for the Super Battery: Discover the Powerful World of Batteries is a 2017 American documentary film about energy storage and how it may help provide an environmentally friendly, or green, future. The basic mechanism of batteries, including lithium-ion types, is described. The benefits and limitations of various batteries are also presented. Details of seeking a much safer, more powerful, longer-lasting and less expensive battery, a so-called \"super battery\", is discussed. The broad importance of energy storage devices, in mobile phones and automobiles, and in the overall electric grid system of the United States, is examined in detail.\n\nThe documentary film is narrated by Jay O. Sanders and includes the following participants (alphabetized by last name):\n\nAccording to David Templeton of the Pittsburgh Post-Gazette, the program \"walks the viewer through the science of how batteries work, returning to that theme time and again to explain variations in design to create cheaper, safer, longer-lasting batteries and energy-storage systems.\" Notable discoveries, featured in the program, Templeton reports, are a safe battery \"made with saltwater electrolytes\", as well as a safe battery \"made of plastics that can use lithium metal rather than a traditional lithium ion to produce longer-lasting, safe power.\" Vicky Hallet of the Washington Post writes that lithium-ion batteries \"gained widespread popularity because of their ability to pack a lot of energy into a lightweight package.\" However, such batteries - due to the thermal runaway properties of the varieties of lithium-content rechargeable cells that use lithium cobalt oxide in their positive electrodes - can potentially burst into flames. The program shows several possible ways to make batteries safer. Hallet reports that the program presents an important notion: \"Batteries are evolving to do more, and do it safely. It’s powerful stuff.\"\n\n"}
{"id": "48540448", "url": "https://en.wikipedia.org/wiki?curid=48540448", "title": "Shidao Bay Nuclear Power Plant", "text": "Shidao Bay Nuclear Power Plant\n\nShidao Bay Nuclear Power Plant (), commonly known as Shidaowan, is a nuclear power plant in Shandong province, China. The site is located near the Xiqianjia village in Ningjin subdistrict, Rongcheng, Weihai, Shandong. The plant is located about south of Rongcheng City, northwest of Shidao, and southeast of Weihai City.\n\nThe plant will have the first fourth-generation nuclear reactors in the world: the HTR-PM, a high-temperature gas-cooled reactor (HTGR) concept. The plant will ultimately have ten 210 MW (megawatts electrical) units of this type. Each unit is made of two HTR-PM reactors driving a single 210 MW steam turbine.\n\nThe plant will also host the construction of two 1,500 MW CAP1400 pressurized water reactors, a design based on the AP1000 jointly developed by Westinghouse and the State Nuclear Power Technology Corporation (SNPTC).\n\nShidao Bay nuclear power plant is a joint venture by China Huaneng Group, China Nuclear Engineering & Construction Group, and Tsinghua University. The total investment of 100 billion yuan (US$15.7 billion) and the 20-year construction plan makes it China's largest planned nuclear project.\n\nIn 2005, China announced its intention to scale up the HTR-10 experimental reactor for commercial power generation. \nThe first two 250-MWt High-Temperature Reactor-Pebble-bed Modules (HTR-PM) will be installed at Shidao Bay, and together drive a steam turbine generating 200 MWe.\n\nOriginally to be started in 2011, the project was postponed after the incident at Fukushima Daiichi Nuclear Power Plant in Japan in March 2011. \nIn 2009, it was planned to be finished in 2013. \nConstruction finally began at the end of 2012, with the pouring of concrete basemat occurring in April 2014. \nThe vessel was installed in 2016. \nIt was expected to begin operating around 2017, later postponed to 2018.\n\nIn December 2017, the pressure vessel head was installed.\n\nAn updated larger power plant, HTR-PM600, is planned with a capacity of 600 MWe using 6 HTR-PM reactor units.\n\n"}
{"id": "35023447", "url": "https://en.wikipedia.org/wiki?curid=35023447", "title": "Single- and double-acting cylinders", "text": "Single- and double-acting cylinders\n\nReciprocating engine cylinders are often classified by whether they are single- or double-acting, depending on how the working fluid acts on the piston.\n\nA single-acting cylinder in a reciprocating engine is a cylinder in which the working fluid acts on one side of the piston only. A single-acting cylinder relies on the load, springs, other cylinders, or the momentum of a flywheel, to push the piston back in the other direction. Single-acting cylinders are found in most kinds of reciprocating engine. They are almost universal in internal combustion engines (e.g. petrol and diesel engines) and are also used in many external combustion engines such as Stirling engines and some steam engines. They are also found in pumps and hydraulic rams.\n\nA double-acting cylinder is a cylinder in which the working fluid acts alternately on both sides of the piston. In order to connect the piston in a double-acting cylinder to an external mechanism, such as a crank shaft, a hole must be provided in one end of the cylinder for the piston rod, and this is fitted with a gland or \"stuffing box\" to prevent escape of the working fluid. Double-acting cylinders are common in steam engines but unusual in other engine types. Many hydraulic and pneumatic cylinders use them where it is needed to produce a force in both directions. A double-acting hydraulic cylinder has a port at each end, supplied with hydraulic fluid for both the retraction and extension of the piston. A double-acting cylinder is used where an external force is not available to retract the piston or it can be used where high force is required in both directions of travel.\n\nSteam engines normally use double-acting cylinders. However, early steam engines, such as atmospheric engines and some beam engines were single-acting. These often transmitted their force through the beam by means of chains and an \"arch head\", as only a tension in one direction was needed.\n\nWhere these were used for pumping mine shafts and only had to act against a load in one direction, single-acting designs remained in use for many years. The main impetus towards double-acting cylinders came when James Watt was trying to develop a rotative beam engine, that could be used to drive machinery via an output shaft. With a single-cylinder engine, a double-acting cylinder gave a smoother power output. The high-pressure engine, as developed by Richard Trevithick, used double-acting pistons and became the model for most steam engines afterwards.\n\nSome of the later steam engines, the high-speed steam engines, used single-acting pistons of a new design. The crosshead became part of the piston, and there was no longer any piston rod. This was for similar reasons to the internal combustion engine, as avoiding the piston rod and its seals allowed a more effective crankcase lubrication system.\n\nSmall models and toys often use single-acting cylinders for the above reason but also to reduce manufacturing costs.\n\nIn contrast to steam engines, nearly all internal combustion engines have used single-acting cylinders.\n\nTheir pistons are usually trunk pistons, where the gudgeon pin joint of the connecting rod is within the piston itself. This avoids the crosshead, piston rod and its sealing gland, but it also makes a single-acting piston almost essential. This, in turn, has the advantage of allowing easy access to the bottom of the piston for lubricating oil, which also has an important cooling function. This avoids local overheating of the piston and rings.\n\nSmall petrol two-stroke engines, such as for motorcycles, use crankcase compression rather than a separate supercharger or scavenge blower. This uses both sides of the piston as working faces, the lower side of the piston acting as a piston compressor to compress the inlet charge ready for the next stroke. The piston is still considered as single-acting, as only one of these faces \"produces\" power.\n\nSome early gas engines, such as Lenoir's original engines, from around 1860, were double-acting and followed steam engines in their design.\n\nInternal combustion engines soon switched to single-acting cylinders. This was for two reasons: as for the high-speed steam engine, the high force on each piston and its connecting rod was so great that it placed large demands upon the bearings. A single-acting piston, where the direction of the forces was consistently compressive along the connecting rod, allowed for tighter bearing clearances. Secondly the need for large valve areas to provide good gas flow, whilst requiring a small volume for the combustion chamber so as to provide good compression, monopolised the space available in the cylinder head. Lenoir's steam engine-derived cylinder was inadequate for the petrol engine and so a new design, based around poppet valves and a single-acting trunk piston appeared instead.\nExtremely large gas engines were also built as blowing engines for blast furnaces, with one or two extremely large cylinders and powered by the burning of furnace gas. These, particularly those built by Körting, used double-acting cylinders. Gas engines require little or no compression of their charge, in comparison to petrol or compression-ignition engines, and so the double-acting cylinder designs were still adequate, despite their narrow, convoluted passageways.\n\nDouble-acting cylinders have been infrequently used for internal combustion engines since, although Burmeister & Wain made 2-stroke cycle double-acting (2-SCDA) diesels for marine propulsion before 1930. The first, of 7,000 hp, was fitted in the British MV \"Amerika\" (United Baltic Co.) in 1929. The two B&W SCDA engines fitted to the in 1937 produced 24,000 hp each.\n\nIn 1935 the US submarine USS \"Pompano\" was ordered as part of the \"Perch\" class Six boats were built, with three different diesel engine designs from different makers. \"Pompano\" was fitted with H.O.R. (Hooven-Owens-Rentschler) 8-cylinder double-acting engines that were a licence-built version of the MAN auxiliary engines of the cruiser \"Leipzig\". Owing to the limited space available within the submarines, either opposed-piston or, in this case, double-acting engines were favoured for being more compact. \"Pompano\"s engines were a complete failure and were wrecked during trials before even leaving the Mare Island Navy Yard. \"Pompano\" was laid up for eight months until 1938 while the engines were replaced. Even then the engines were regarded as unsatisfactory and were replaced by Fairbanks-Morse engines in 1942. While \"Pompano\" was still being built, the \"Salmon\" class submarines were ordered. Three of these were built by Electric Boat, with a 9-cylinder development of the H.O.R. engine. Although not as great a failure as \"Pompano\"s engines, this version was still troublesome and the boats were later re-engined with the same single-acting General Motors 16-248 V16 engines as their sister boats. Other Electric Boat constructed submarines of the \"Sargo\" and \"Seadragon\" classes were also built with these 9-cylinder H.O.R. engines, but later re-engined.\n\nSee main Hydraulic cylinder article.\n"}
{"id": "7110145", "url": "https://en.wikipedia.org/wiki?curid=7110145", "title": "Spectra Shield", "text": "Spectra Shield\n\nSpectra Shield is a composite material (specifically, an ultra-high-molecular-weight polyethylene (UHMWPE) fiber) used in bulletproof vests and vehicle armour. It is manufactured by Honeywell.\n\nOther popular fibers with similar uses are aramid (Kevlar or Twaron) and Dyneema (another UHMWPE).\n\n"}
{"id": "1295284", "url": "https://en.wikipedia.org/wiki?curid=1295284", "title": "Staged combustion cycle", "text": "Staged combustion cycle\n\nThe staged combustion cycle (sometimes known as topping cycle or preburner cycle) is a power cycle of a bipropellant rocket engine. In the staged combustion cycle, propellant flows through multiple combustion chambers, and is thus combusted in stages. The main advantage relative to other rocket engine power cycles is high fuel efficiency, measured through specific impulse, while its main disadvantage is engineering complexity.\n\nTypically, propellant flows through two kinds of combustion chambers; the first called \"preburner\" and the second called \"main combustion chamber\". In the preburner, a small portion of propellant is combusted, and the over-pressure produced is used to drive the turbopumps that feed the engine with propellant. In the main combustion chamber, the propellants are combusted completely to produce thrust.\n\nThe fuel efficiency of the staged combustion cycle is in part a result of all propellant ultimately flowing to the main combustion chamber; contributing to thrust. The staged combustion cycle is sometimes referred to as closed cycle, as opposed to the gas generator, or open cycle where a portion of propellant never reaches the main combustion chamber. The engineering complexity is partly a result of the preburner exhaust of hot and highly pressurized gas which, particularly when oxidizer-rich, produces extremely harsh conditions for turbines and plumbing.\n\nStaged combustion (\"\") was first proposed by Alexey Isaev in 1949. The first staged combustion engine was the S1.5400 (11D33) used in the Soviet planetary rocket, designed by Melnikov, a former assistant to Isaev. About the same time (1959), Nikolai Kuznetsov began work on the closed cycle engine NK-9 for Korolev's orbital ICBM, GR-1. Kuznetsov later evolved that design into the NK-15 and NK-33 engines for the unsuccessful Lunar N1 rocket.\nThe non-cryogenic NO/UDMH engine RD-253 using staged combustion was developed by Valentin Glushko circa 1963 for the Proton rocket.\n\nAfter the abandonment of the N-1, Kuznetsov was ordered to destroy the NK-33 technology, but instead he warehoused dozens of the engines. In the 1990s, Aerojet was contacted and eventually visited Kuznetsov's plant. Upon meeting initial skepticism about the high specific impulse and other specifications, Kuznetsov shipped an engine to the US for testing. Oxidizer-rich staged combustion had been considered by American engineers, but deemed impossible.\nThe Russian RD-180 engine also employs a staged-combustion rocket engine cycle. Lockheed Martin began purchasing the RD-180 in circa 2000 for the Atlas III and later, the V, rockets. The purchase contract was subsequently taken over by United Launch Alliance (ULA), the Lockheed-Martin successor company after 2006, and ULA continues to fly the Atlas V with RD-180 engines as of 2018.\n\nThe first laboratory staged-combustion test engine in the West was built in Germany in 1963, by Ludwig Boelkow.\n\nHydrogen peroxide/kerosene fueled engines such as the British Gamma of the 1950s may use a closed-cycle process by catalytically decomposing the peroxide to drive turbines \"before\" combustion with the kerosene in the combustion chamber proper. This gives the efficiency advantages of staged combustion, while avoiding major engineering problems.\n\nThe Space Shuttle Main Engine is another example of a staged combustion engine, and the first to use liquid oxygen and liquid hydrogen. Its counterpart in the Soviet shuttle was the RD-0120, similar in specific impulse, thrust, and chamber pressure specification to the SSME, but with some differences that reduced complexity and cost at the expense of increased engine weight.\n\nSeveral variants of the staged combustion cycle exist. Preburners that burn a small portion of oxidizer with a full flow of fuel are called \"fuel-rich\", while preburners that burn a small portion of fuel with a full flow of oxidizer are called \"oxidizer-rich\". The RD-180 has an oxidizer-rich preburner, while the RS-25 has two fuel-rich preburners. The SpaceX Raptor has both oxidizer-rich and fuel-rich preburners, a design called \"full-flow staged combustion\".\n\nStaged combustion designs can be either \"single-shaft\" or \"twin-shaft\". In the single-shaft design, one set of preburner and turbine drives both propellant turbopumps. Examples include the Energomash RD-180 and the Blue Origin BE-4. In the twin-shaft design, the two propellant turbopumps are driven by separate turbines, which are in turn driven by the outflow of either one or separate preburners. Examples of twin-shaft designs include the Rocketdyne RS-25, the JAXA LE-7, and the Raptor. Relative to a single-shaft design, the twin-shaft design requires an additional set of preburner and turbine, but allows for individual control of the two turbopumps.\n\nIn addition to the propellant turbopumps, staged combustion engines often require smaller boost pumps so to prevent both preburner backflow and turbopump cavitation. For example, the RD-180 and RS-25 use boost pumps driven by tap-off and expander cycles, as well as pressurized tanks, to incrementally increase propellant pressure prior to entering the preburner.\n\nFull-flow staged combustion (FFSC) is a twin-shaft staged combustion cycle that uses both oxidizer-rich and fuel-rich preburners. The cycle allows full flow of both propellants through the turbines; hence the name. The fuel turbopump is driven by the fuel-rich preburner, and the oxidizer turbopump is driven by the oxidizer-rich preburner.\n\nBenefits of the full-flow staged combustion cycle include turbines that run cooler and at lower pressure, due to increased mass flow, leading to a longer engine life and higher reliability. As an example, up to 25 flights were anticipated for an engine design studied by the DLR (German Aerospace Center) in the frame of the SpaceLiner project. Further, the full-flow cycle eliminates the need for an interpropellant turbine seal normally required to separate oxidizer-rich gas from the fuel turbopump or fuel-rich gas from the oxidizer turbopump, thus improving reliability.\n\nSince the use of both fuel and oxidizer preburners results in full gasification of each propellant before entering the combustion chamber, FFSC engines belong to a broader class of rocket engines called gas-gas engines. Full gasification of components leads to faster chemical reactions in the combustion chamber, which improves performance.\n\nPotential disadvantages of the full-flow staged combustion cycle includes increased engineering complexity of two preburners, relative to a single-shaft staged combustion cycle, as well as an increased parts count.\n\nOnly three full-flow staged combustion rocket engines have ever progressed sufficiently to be tested on test stands; the Soviet Energomash RD-270 project in the 1960s, SpaceX's Raptor engine first test-fired in September 2016. , and the US government-funded Aerojet Rocketdyne Integrated powerhead demonstration project in the mid-2000s.\n\n\n\n\n\n\n"}
{"id": "26437521", "url": "https://en.wikipedia.org/wiki?curid=26437521", "title": "Standard Gibbs free energy of formation", "text": "Standard Gibbs free energy of formation\n\nThe standard Gibbs free energy of formation of a compound is the change of Gibbs free energy that accompanies the formation of 1 mole of a substance in its standard state from its constituent elements in their standard states (the most stable form of the element at 1 bar of pressure and the specified temperature, usually 298.15 K or 25 °C).\n\nThe table below lists the Standard Gibbs function of formation for several elements and chemical compounds and is taken from Lange's Handbook of Chemistry. Note that all values are in kJ/mol. Far more extensive tables can be found in the CRC Handbook of Chemistry and Physics and the NIST JANAF tables. The NIST Chemistry WebBook (see link below) is an online resource that contains standard enthalpy of formation for various compounds along with the standard absolute entropy for these compounds from which the Standard Gibbs Free Energy of Formation can be calculated.\n\n\n\n"}
{"id": "21815513", "url": "https://en.wikipedia.org/wiki?curid=21815513", "title": "The Garden (2008 film)", "text": "The Garden (2008 film)\n\nThe Garden is a 2008 American documentary film directed by Scott Hamilton Kennedy. It tells the story of the now demolished South Central Farm; a community garden and urban farm located in Los Angeles, California. \"The Garden\" details the plight of the South Central Farmers, a mostly Latinx community of farmers who organized and worked on the farm. After a suspected back room deal, the land upon which the farm operated was sold from the city back to the original owner, Richard Horowitz. He then decided he did not want to allow the farmers to use it anymore. Despite efforts to keep their farm, the South Central Farmers were evicted and their garden was bulldozed. The film was nominated for an Academy Award for Best Documentary Feature on 22 January 2009.\n\n\"The Garden\" includes appearances by Danny Glover, Daryl Hannah, and Antonio Villaraigosa.\n\n\"The Garden\" was nominated for Best Documentary Feature in the 81st Academy Awards.\n\nThe International Documentary Association nominated it for the Pare Lorentz Award.\n\nIt won the Grand Jury Award from the 2008 Silverdocs Documentary Festival.\n\n\n"}
{"id": "10026140", "url": "https://en.wikipedia.org/wiki?curid=10026140", "title": "Tropical cyclone seasonal forecasting", "text": "Tropical cyclone seasonal forecasting\n\nTropical cyclone seasonal forecasting is the process of predicting the number of tropical cyclones in one of the world's seven tropical cyclone basins during a particular tropical cyclone season. In the north Atlantic Ocean, one of the most widely publicized annual predictions comes from the Tropical Meteorology Project at Colorado State University. These reports are written by Philip J. Klotzbach and William M. Gray.\n\nSince 1984, Dr William M. Gray and his associates at the Colorado State University has issued a seasonal forecast, that has aimed to predict the number of tropical storms and hurricanes that will develop within the Atlantic basin during the upcoming season amongst other factors. The forecasts were initially issued ahead of time for June and August.\n\nAfter the active 2005 Atlantic hurricane season, Dr Gray decided to allow Philip J. Klotzbach to take the primary responsibility for the project’s seasonal, monthly and landfall probability forecasts effective with the first forecast for the 2006 Atlantic hurricane season.\nAhead of each season several national meteorological services issue forecasts of how many tropical cyclones will form during a season and/or how many tropical cyclones will affect a particular country. Examples include the United Kingdom's Met Office which issues a in May/June of the number of tropical storms for the upcoming Atlantic hurricane season, while the Philippine Atmospheric, Geophysical and Astronomical Service tries to predict how many tropical cyclones will move into its area of responsibility.\n\nIn August 1998, the United States Climate Prediction Center in conjunction with the National Hurricane Center and the Hurricane Research Division issued a tropical cyclone outlook, which accurately predicted that there would be an above-normal number of tropical storms and hurricanes in the Atlantic between August and October. The NOAA centres subsequently started to issue an outlook that gave a general guide to the expected overall activity within the Atlantic Ocean.\n\nAhead of the 2003 Pacific hurricane season, the NOAA forecasters decided to start issuing an experimental tropical cyclone outlook for the Eastern Pacific, which was designed not to be updated during the mid-season. As a result of both the 2003 and 2004 outlooks being successful, the predictions became an operational product during 2005.\n\nNOAA is also one of the contributors to New Zealand's National Institute of Water & Atmospheric Research Tropical Cyclone Outlook, through its National Weather Service forecast offices in the region and the Climate Prediction Center.\n\nNew Zealand's National Institute of Water & Atmospheric Research (NIWA) and collaborating agencies including the Meteorological Service of New Zealand and Pacific Island National Meteorological Services issue the \"Island Climate Update Tropical Cyclone Outlook\" for the Pacific. This forecast attempts to predict how many tropical cyclones and severe tropical cyclones will develop within the Southern Pacific between 135°E and 120°W as well as how many will affect a particular island nation. The Fiji Meteorological Service while collaborating with NIWA and partners also publishes its own seasonal forecast but for the South Pacific basin between 160°E and 120°W. Since the start of the 2009–10 season, the Australian Bureau of Meteorology's National Climate Center has publicly released a forecast for the Australian region which focused on the broadscale aspects of the cyclone season, and forecasted how likely it was that a subregion was to see activity above the average as well as how many tropical cyclones may occur within the basin and each of its subregions. However ahead of the 2011–12 season the NCC stopped forecasting publicly how many tropical cyclones may occur in a certain region and just forecasted how likely it was that a subregion was to see activity above the average.\n\n\n"}
{"id": "35214401", "url": "https://en.wikipedia.org/wiki?curid=35214401", "title": "Whaleman Foundation", "text": "Whaleman Foundation\n\nThe Whaleman Foundation is a non-profit, marine conservation organization based in Lahaina, Hawaii in the United States. It advocates for the protection of cetaceans (whales, dolphins, and porpoises) and their habitats.\n\nWhaleman was founded by Jeff Pantukhoff. Hayden Panettiere is the spokesperson for the foundation. She has promoted the Foundations' \"Save the Whales Again!\" campaign since 2008.\n"}
{"id": "48476299", "url": "https://en.wikipedia.org/wiki?curid=48476299", "title": "Willwind Wind Farm", "text": "Willwind Wind Farm\n\nThe Willwind Wind Farm (also sometimes referred to as Willpita Wind Farm) is a small onshore wind farm built near the village of Bithugalgama, in the Ratnapura District of Sri Lanka. The wind farm is owned and operated by . The facility consists of seven wind turbines measuring approximately each.\n\n"}
