{"id": "11947062", "url": "https://en.wikipedia.org/wiki?curid=11947062", "title": "Albert Kahn Associates", "text": "Albert Kahn Associates\n\nAlbert Kahn Associates is an architectural design firm in Detroit, Michigan with a second office located in Miami, Florida. It was established in 1895 and is still active today. Recent work includes being awarded third place in the Virtual Modeling Stage of NASA’s 3D-Printed Habitat Competition (as of July 2018) for their work on martian habitats, and also creating the world's largest penguin conservation center, Polk Penguin Conservation Center. In earlier years, It introduced a new technology in industrial building involving a unique reinforced concrete method referred to as the Kahn System of construction using proprietary patented reinforcement steel manufactured by Trussed Concrete Steel Company. The building of automobile factories and other types of factories were revolutionized from wooden timber framing construction. Besides being an advanced technology in strength that led to wider open interior spaces, it featured a high degree of fire resistance and larger window space for light. The firm started by Albert Kahn built factories for Chrysler for over a decade, Ford Automobile for 30 years and Packard Automobile for 35 years. Other important clients of the firm were Republic Steel and General Motors. The firm was awarded a $40 million contract to build a tractor factory in Russia in 1928. The firm's output was over a million dollars worth of work per week by 1929. By 1939, the firm designed 19 percent of all industrial buildings in the United States and had designed some $800 million of buildings worldwide. \n\nAlbert Kahn established the Detroit firm in 1895 and was its first and only employee then. In 1896, Kahn took on two partners: George Nettleton and Alexander Trowbridge. All three were architects at the time with the architectural firm Mason & Rice in Detroit. The firm they opened in January 1896 was initially called \"Nettleton, Kahn, and Trowbridge\". At the time there was an abundance of design jobs available. Mason & Rice even referred some of their work to Kahn's new firm when they had more than they could handle.\n\nThe firm's first large design job in Detroit was to design Children's Hospital of Michigan. Its next major project came in 1898 from James E. Scripps to design an elaborate library with an art gallery. The Scripps Library and Museum was located adjacent to Scripps' mansion home on Trumbull Avenue at Grand River Avenue. The dynamics of the firm changed when Trowbridge took a position at Cornell University in 1897 and Nettleton died in 1900. The firm then hired designers with good reputations and an ability to work as a team with other designers. Ernest Wilby, a young Englishman from Canada, was their key designer working for the firm at the time. This new team then collaborated with the Mason & Rice architectural firm to design the Palms Apartments, a building with English architecture on Jefferson Avenue at Rivard Street in Detroit. It was one of the first buildings to use reinforced concrete as a major construction material.\n\nThe firm changed its name in 1901 to \"Albert Kahn, architect; Ernest Wilby, associate\", and had its first industrial design the same year. Joseph Boyer had hired the firm to design a factory that made pneumatic tools. Boyer then asked the firm to design a building to house the Burroughs Adding Machine Company, where modern mechanical calculators were made. Boyer in 1902 introduced Kahn to Henry Joy, head of the Packard Motor Car company. Kahn's firm received several non-industrial jobs from Joy, including remodeling Joy's home. Joy additionally had the firm design automobile factory buildings.\n\nBetween 1903–05, the firm designed the first nine Packard Automotive Plant buildings using traditional wooden designs that had spans no longer than . Packard produced over 700 automobiles a year by 1905, so an expansion to the factory facility was in order. The team decided to use reinforced concrete to build the new factory space. With Albert's younger brother Julius as the key architect and engineer, Building Number 10 was constructed with the Kahn bar and other reinforced concrete products. These were manufactured at the Trussed Concrete Steel Company (aka \"Truscon\"), which provided spans. This new technology for concrete changed the way American automobile factories were built. This building was built with uniquely designed steel beam reinforced concrete for reinforcement that used the Kahn bar with winged tabs on the steel bar edges that were bent back at 45 degrees to resist and counter tension stresses. This was the first time reinforced concrete was used for automobile factory construction in the United States. The new state of the art design concept featured larger open interior spaces than the old mill-framed wooden factory buildings. The building was 322 feet long by 60 feet wide (about 98 meters by 18 meters). This new factory concept of building with reinforced concrete lead to Albert Kahn Associates doing all the aviation factories for the government during World War I.\n\nIn 1906, the firm designed factory buildings for the Pierce-Arrow Motor Car Company in Buffalo, New York. In 1908 the Highland Park Ford Plant was designed by the firm. It was similar to the Packard factory just a few miles away, however different from the Pierce-Arrow factory buildings that were of one-story only and had saw-tooth roofs with skylights. The Ford factory building had four stories.\nIt was however, of reinforced concrete design using the patented Kahn System of his brother Julius, just like the Packard and Pierce-Arrow factory buildings. The Ford plant was 860 feet long and 75 feet wide. The steel reinforced concrete building designs were fireproof. The \"Model factory\" featured what was referred to as Truscon's \"Daylight System\" because of the many large windows.\n\nHenry Ford had the firm design his River Rouge automobile factory that started construction in 1917. At the time Ford decided to make a ship factory for the United States Navy and had the firm design B Building. This was called the Eagle Plant because it built Eagle Boats that were antisubmarine chasers that destroyed German submarines. It was 100 feet (30 meters) high by 300 feet (91 meters) wide by 1700 feet (518 meters) long. When World War I ended this factory plant was converted to make Ford automobiles.\n\nThe firm designed over 50 factories between 1921 and 1929 and had a substantial quantity of work not related to industrial buildings. It had over 400 on the staff by 1930 and was producing a million dollars worth of construction a week. The firm never hired an architectural college graduate until 1935. Going into the twenty-first century the firm name is Albert Kahn Associates and has branches in Birmingham, Alabama, and São Paulo, Brasil. The firm designed about two thousand structures between 1895 and 1945 of which twenty-six were buildings for the University of Michigan.\n\nThe firm had a staff of 300–400 people in the 1930s, including about 40 secretaries, stenographers, typists, and file clerks. There were around 175 architectural designers and draftsmen, 80–90 mechanical and electrical engineers, 40–50 field superintendents, and about 30 specification writers. The chief administrator was Albert Kahn and his assistants were his brothers Julius, Moritz and Louis. The staff was increased to 600 people at the time of World War II. The firm ramped up for war plants to make tanks and other war related equipment.\nThe Technical Division had departments that designed the projects which included electrical, plumbing, heating and air-conditioning.\nThis division was responsible for the functional design of the building. Key important design elements for factories included \"straight-line\" production capability, flexibility in interior floor designs to accommodate various changing production methods, generous column spacing to give maximum open floor space, high ceilings, strong floors to carry heavy loads, and with good lighting and ventilation.\n\nThe Executive Division was responsible for the management of the projects. Business-like management was required for concerns of economics, speed, obtaining reliable sub-contractors, adequate supervision, and complete accurate drawings.\n\nBelow are some of the buildings designed by Albert Kahn Associates:\n\n\n\n"}
{"id": "1524630", "url": "https://en.wikipedia.org/wiki?curid=1524630", "title": "Alternatives to the Standard Higgs Model", "text": "Alternatives to the Standard Higgs Model\n\nThe Alternative models to the Standard Higgs Model are models which are considered by many particle physicists to solve some of Higgs boson's existing problems. Two of the most currently researched models are quantum triviality, and Higgs hierarchy problem.\n\nIn particle physics, elementary particles and forces give rise to the world around us. Physicists explain the behaviors of these particles and how they interact using the Standard Model—a widely accepted framework believed to explain most of the world we see around us. Initially, when these models were being developed and tested, it seemed that the mathematics behind those models, which were satisfactory in areas already tested, would also forbid elementary particles from having any mass, which showed clearly that these initial models were incomplete. In 1964 three groups of physicists almost simultaneously released papers describing how masses could be given to these particles, using approaches known as symmetry breaking. This approach allowed the particles to obtain a mass, without breaking other parts of particle physics theory that were already believed reasonably correct. This idea became known as the Higgs mechanism, and later experiments confirmed that such a mechanism does exist—but they could not show exactly \"how\" it happens.\n\nThe simplest theory for how this effect takes place in nature, and the theory that became incorporated into the Standard Model, was that if one or more of a particular kind of \"field\" (known as a Higgs field) happened to permeate space, and if it could interact with elementary particles in a particular way, then this would give rise to a Higgs mechanism in nature. In the basic Standard Model there is one field and one related Higgs boson; in some extensions to the Standard Model there are multiple fields and multiple Higgs bosons.\n\nIn the years since the Higgs field and boson were proposed as a way to explain the origins of symmetry breaking, several alternatives have been proposed that suggest how a symmetry breaking mechanism could occur without requiring a Higgs field to exist. Models which do not include a Higgs field or a Higgs boson are known as Higgsless models. In these models, strongly interacting dynamics rather than an additional (Higgs) field produce the non-zero vacuum expectation value that breaks electroweak symmetry.\n\nA partial list of proposed alternatives to a Higgs field as a source for symmetry breaking includes:\n\n\n\n"}
{"id": "151816", "url": "https://en.wikipedia.org/wiki?curid=151816", "title": "Bountiful Harvest", "text": "Bountiful Harvest\n\nBountiful Harvest is a book by University of Houston economics professor Thomas R. DeGregori, debunking what he calls \"anti-science environmental activists\", and arguing for the employment modern agricultural innovations such as bioengineered foods, which he claims have increased life expectancy and crop yields, and generally improved human well-being.\n\nThe \"AgBiotech Reporter\" called DeGregori's book \"an ideal handbook for anyone who wants to understand the opponents of progress.\"\n\n"}
{"id": "29578326", "url": "https://en.wikipedia.org/wiki?curid=29578326", "title": "Cell encapsulation", "text": "Cell encapsulation\n\nCell microencapsulation technology involves immobilization of the cells within a polymeric semi-permeable membrane that permits the bidirectional diffusion of molecules such as the influx of oxygen, nutrients, growth factors etc. essential for cell metabolism and the outward diffusion of waste products and therapeutic proteins. At the same time, the semi-permeable nature of the membrane prevents immune cells and antibodies from destroying the encapsulated cells regarding them as foreign invaders.\n\nThe main motive of cell encapsulation technology is to overcome the existing problem of graft rejection in tissue engineering applications and thus reduce the need for long-term use of immunosuppressive drugs after an organ transplant to control side effects.\n\nIn 1933 Vincenzo Bisceglie made the first attempt to encapsulate cells in polymer membranes. He demonstrated that tumor cells in a polymer structure transplanted into pig abdominal cavity remained viable for a long period without being rejected by the immune system.\n\nThirty years later in 1964, the idea of encapsulating cells within ultra thin polymer membrane microcapsules so as to provide immunoprotection to the cells was then proposed by Thomas Chang who introduced the term \"artificial cells\" to define this concept of bioencapsulation. He suggested that these artificial cells produced by a drop method not only protected the encapsulated cells from immunorejection but also provided a high surface-to-volume relationship enabling good mass transfer of oxygen and nutrients.\nTwenty years later, this approach was successfully put into practice in small animal models when alginate-polylysine-alginate (APA) microcapsules immobilizing xenograft islet cells were developed. The study demonstrated that when these microencapsulated islets were implanted into diabetic rats, the cells remained viable and controlled glucose levels for several weeks.\nHuman trials utilising encapsulated cells were performed in 1998. Encapsulated cells expressing a cytochrome P450 enzyme to locally activate an anti-tumour prodrug were used in a trial for advanced, non-resectable pancreatic cancer. Approximately a doubling of survival time compared to historic controls was demonstrated.\n\nQuestions could arise as to why the technique of encapsulation of cells is even required when therapeutic products could just be injected at the site. An important reason for this is that the encapsulated cells would provide a source of sustained continuous release of therapeutic products for longer durations at the site of implantation. Another advantage of cell microencapsulation technology is that it allows the loading of non-human and genetically modified cells into the polymer matrix when the availability of donor cells is limited. Microencapsulation is a valuable technique for local, regional and oral delivery of therapeutic products as it can be implanted into numerous tissue types and organs. For prolonged drug delivery to the treatment site, implantation of these drug loaded artificial cells would be more cost effective in comparison to direct drug delivery. Moreover, the prospect of implanting artificial cells with similar chemical composition in several patients irrespective of their leukocyte antigen could again allow reduction in costs.\n\nThe potential of using cell microencapsulation in successful clinical applications can be realized only if several requirements encountered during the development process are optimized such as the use of an appropriate biocompatible polymer to form the mechanically and chemically stable semi-permeable matrix, production of uniformly sized microcapsules, use of an appropriate immune-compatible polycations cross-linked to the encapsulation polymer to stabilized the capsules, selection of a suitable cell type depending on the situation.\n\nThe use of the best biomaterial depending on the application is crucial in the development of drug delivery systems and tissue engineering. The polymer alginate is very commonly used due to its early discovery, easy availability and low cost but other materials such as cellulose sulphate, collagen, chitosan, gelatin and agarose have also been employed.\n\nSeveral groups have extensively studied several natural and synthetic polymers with the goal of developing the most suitable biomaterial for cell microencapsulation. Extensive work has been done using alginates which are regarded as the most suitable biomaterials for cell microencapsulation due to their abundance, excellent biocompatibility and biodegradability properties. Alginate is a natural polymer which can be extracted from seaweed and bacteria with numerous compositions based on the isolation source.\n\nAlginate is not free from all criticism. Some researchers believe that alginates with high-M content could produce an inflammatory response and an abnormal cell growth while some have demonstrated that alginate with high-G content lead to an even higher cell overgrowth and inflammatory reaction in vivo as compared to intermediate-G alginates.\nEven ultrapure alginates may contain endotoxins, and polyphenols which could compromise the biocompatibility of the resultant cell microcapsules. It has been shown that even though purification processes successfully lower endotoxin and polyphenol content in the processed alginate, it is difficult to lower the protein content and the purification processes could in turn modify the properties of the biomaterial. Thus it is essential that an effective purification process is designed so as to remove all the contaminants from alginate before it can be successfully used in clinical applications.\n\nResearchers have also been able to develop alginate microcapsules with an altered form of alginate with enhanced biocompatibility and higher resistance to osmotic swelling. \nAnother approach to increasing the biocompatibility of the membrane biomaterial is through surface modification of the capsules using peptide and protein molecules which in turn controls the proliferation and rate of differentiation of the encapsulated cells. One group that has been working extensively on coupling the amino acid sequence Arg-Gly-Asp (RGD) to alginate hydrogels demonstrated that the cell behavior can be controlled by the RGD density coupled on the alginate gels. Alginate microparticles loaded with myoblast cells and functionalized with RGD allowed control over the growth and differentiation of the loaded cells. \nAnother vital factor that controls the use of cell microcapsules in clinical applications is the development of a suitable immune-compatible polycation to coat the otherwise highly porous alginate beads and thus impart stability and immune protection to the system. Poly-L-lysine is the most commonly used polycation but its low biocompatibility restricts the successful clinical use of these PLL formulated microcapsules which attract inflammatory cells thus inducing necrosis of the loaded cells. Studies have also shown that alginate-PLL-alginate (APA) microcapsules demonstrate low mechanical stability and short term durability. Thus several research groups have been looking for alternatives to PLL and have demonstrated promising results with poly-L-ornithine and poly(methylene-co-guanidine) hydrochloride by fabricating durable microcapsules with high and controlled mechanical strength for cell encapsulation.\n\nSeveral groups have also investigated the use of chitosan which is a naturally derived polycation as a potential replacement for PLL to fabricate alginate-chitosan (AC) microcapsules for cell delivery applications. However, studies have also shown that the stability of this AC membrane is again limited and one group demonstrated that modification of this alginate-chitosan microcapsules with genipin, a naturally occurring iridoid glucosid from gardenia fruits, to form genipin cross-linked alginate-chitosan (GCAC) microcapsules could augment stability of the cell loaded microcapsules.\n\nCollagen, a major protein component of the ECM, provides support to tissues like skin, cartilage, bones, blood vessels and ligaments and is thus considered a model scaffold or matrix for tissue engineering due to its properties of biocompatibility, biodegradability and ability to promote cell binding. This ability allows chitosan to control distribution of cells inside the polymeric system. Thus, Type-I collagen obtained from animal tissues is now successfully being used commercially as tissue engineered biomaterial for multiple applications. Collagen has also been used in nerve repair and bladder engineering. Immunogenicity has limited the applications of collagen. Gelatin has been considered as an alternative for that reason.\n\nGelatin is prepared from the denaturation of collagen and many desirable properties such as biodegradability, biocompatibility, non-immunogenity in physiological \nenvironments, and easy processability make this polymer a good choice for tissue engineering applications. It is used in engineering tissues for the skin, bone and cartilage and is used commercially for skin replacements.\n\nChitosan is a polysaccharide composed of randomly distributed β-(1-4)-linked D-glucosamine (deacetylated unit) and N-acetyl-D-glucosamine (acetylated unit). It is derived from the N-deacetylation of chitin and has been used for several applications such as drug delivery, space-filling implants and in wound dressings. However, one drawback of this polymer is its weak mechanical properties and is thus often combined with other polymers such collagen to form a polymer with stronger mechanical properties for cell encapsulation applications.\n\nAgarose is a polysaccharide derived from seaweed used for nanoencapsulation of cells and the cell/agarose suspension can be modified to form microbeads by reducing the temperature during preparation. However, one drawback with the microbeads so obtained is the possibility of cellular protrusion through the polymeric matrix wall after formation of the capsules.\n\nCellulose sulphate is derived from cotton and, once processed appropriately, can be used as a biocompatible base in which to suspend cells. When the poly-anionic cellulose sulphate solution is immersed in a second, poly-cationic solution (e.g. pDADMAC), a semi-permeable membrane is formed around the suspended cells as a result of gelation between the two poly-ions. Both mammalian cell lines and bacterial cells remain viable and continue to replicate within the capsule membrane in order to fill-out the capsule. As such, in contrast to some other encapsulation materials, the capsules can be used to grow cells and act as such like a mini-bioreactor. The biocompatible nature of the material has been demonstrated by observation during studies using the cell-filled capsules themselves for implantation as well as isolated capsule material. Capsules formed from cellulose sulphate have been successfully used, showing safety and efficacy, in clinical and pre-clinical trials in both humans and animals, primarily as anti-cancer treatments, but also exploring possible uses for gene therapy or antibody therapies. Using cellulose sulphate it has been possible to manufacture encapsulated cells as a pharmaceutical product at large scale and fulfilling Good Manufacturing Process (cGMP) standards. This was achieved by the company Austrianova in 2007.\n\nThe use of an ideal high quality biomaterial with the inherent properties of biocompatibility is the most crucial factor that governs the long term efficiency of this technology. An ideal biomaterial for cell encapsulation should be one that is totally biocompatible, does not trigger an immune response in the host and does not interfere with cell homeostasis so as to ensure high cell viability. However, one major limitation has been the inability to reproduce the different biomaterials and the requirements to obtain a better understanding of the chemistry and biofunctionality of the biomaterials and the microencapsulation system. Several studies demonstrate that surface modification of these cell containing microparticles allows control over the growth and cellular differentiation. of the encapsulated cells.\n\nOne study proposed the use of zeta potential which measures the electric charge of the microcapsule as a means to predict the interfacial reaction between microcapsule and the surrounding tissue and in turn the biocompatibility of the delivery system.\n\nA fundamental criterion that must be established while developing any device with a semi-permeable membrane is to adjust the permeability of the device in terms of entry and exit of molecules. It is essential that the cell microcapsule is designed with uniform thickness and should have a control over both the rate of molecules entering the capsule necessary for cell viability and the rate of therapeutic products and waste material exiting the capsule membrane. Immunoprotection of the loaded cell is the key issue that must be kept in mind while working on the permeability of the encapsulation membrane as not only immune cells but also antibodies and cytokines should be prevented entry into the microcapsule which in fact depends on the pore size of the biomembrane.\nIt has been shown that since different cell types have different metabolic requirements, thus depending on the cell type encapsulated in the membrane the permeability of the membrane has to be optimized. Several groups have been dedicated towards the study of membrane permeability of cell microcapsules and although the role of permeability of certain essential elements like oxygen has been demonstrated, the permeability requirements of each cell type are yet to be determined.\n\nSodium Citrate is used for degradation of alginate beads after encapsulation of cells. In order to determine viability of the cells or for further experimentation. Concentrations of approximately 25mM are used to dissolve the alginate spheres and the solution is spun down using a centrifuge so the sodium citrate can be removed and the cells can be collected.\n\nIt is essential that the microcapsules have adequate membrane strength (mechanical stability) to endure physical and osmotic stress such as during the exchange of nutrients and waste products. The microcapsules should be strong enough and should not rupture on implantation as this could lead to an immune rejection of the encapsulated cells. For instance, in the case of xenotransplantation, a tighter more stable membrane would be required in comparison to allotransplantation. Also, while investigating the potential of using APA microcapsules loaded with bile salt hydrolase (BSH) overproducing active Lactobacillus plantarum 80 cells, in a simulated gastro intestinal tract model for oral delivery applications, the mechanical integrity and shape of the microcapsules was evaluated. It was shown that APA microcapsules could potentially be used in the oral delivery of live bacterial cells. However, further research proved that the GCAC microcapsules possess a higher mechanical stability as compared to APA microcapsules for oral delivery applications. Martoni et al. were experimenting with bacteria-filled capsules that would be taken by mouth to reduce serum cholesterol. \nThe capsules were pumped through a series of vessels simulating the human GI tract to determine how well the capsules would survive in the body. Extensive research into the mechanical properties of the biomaterial to be used for cell microencapsulation is necessary to determine the durability of the microcapsules during production and especially for in vivo applications where a sustained release of the therapeutic product over long durations is required.\nvan der Wijngaart et al. grafted a solid, but permeable, shell around the cells to provide increased mechanical strength.\n\nSodium Citrate is used for degradation of alginate beads after encapsulation of cells. In order to determine viability of the cells or for further experimentation. Concentrations of approximately 25mM are used to dissolve the alginate spheres and the solution is spun down using a centrifuge so the sodium citrate can be removed and the cells can be collected.\n\n\nDroplet-based microfluidics can be used to generate microparticles with repeatable size.\n\n\nEletrospraying is used to create alginate spheres by pumping an alginate solution through a needle. A source of high voltage usually provided by a clamp attached to the needle is used to generate an electric potential with the alginate falling from the needle tip into a solution that contains a ground. Calcium chloride is used as cross linking solution in which the generated capsules drop into where they harden after approximately 30 minutes. Beads are formed from the needle due to charge and surface tension.\n\nThe diameter of the microcapsules is an important factor that influences both the immune response towards the cell microcapsules as well as the mass transport across the capsule membrane. Studies show that the cellular response to smaller capsules is much lesser as compared to larger capsules and in general the diameter of the cell loaded microcapsules should be between 350-450 µm so as to enable effective diffusion across the semi-permeable membrane.\n\nThe cell type chosen for this technique depends on the desired application of the cell microcapsules. The cells put into the capsules can be from the patient (autologous cells), from another donor (allogeneic cells) or from other species (xenogeneic cells). The use of autologous cells in microencapsulation therapy is limited by the availability of these cells and even though xenogeneic cells are easily accessible, danger of possible transmission of viruses, especially porcine endogenous retrovirus to the patient restricts their clinical application, and after much debate several groups have concluded that studies should involve the use of allogeneic instead of xenogeneic cells. Depending on the application, the cells can be genetically altered to express any required protein. However, enough research has to be carried out to validate the safety and stability of the expressed gene before these types of cells can be used.\n\nThis technology has not received approval for clinical trial because of the high immunogenicity of cells loaded in the capsules. They secrete cytokines and produce a severe inflammatory reaction at the implantation site around the capsules, in turn leading to a decrease in viability of the encapsulated cells. One promising approach being studied is the administration of anti-inflammatory drugs to reduce the immune response produced due to administration of the cell loaded microcapsules. Another approach which is now the focus of extensive research is the use of stem cells such as mesenchymal stem cells for long term cell microencapsulation and cell therapy applications in hopes of reducing the immune response in the patient after implantation. Another issue which compromises long term viability of the microencapsulated cells is the use of fast proliferating cell lines which eventually fill up the entire system and lead to decrease in the diffusion efficiency across the semi-permeable membrane of the capsule. A solution to this could be in the use of cell types such as myoblasts which do not proliferate after the microencapsulation procedure.\n\nProbiotics are increasingly being used in numerous dairy products such as ice cream, milk powders, yoghurts, frozen dairy desserts and cheese due to their important health benefits. But, low viability of probiotic bacteria in the food still remains a major hurdle. The pH, dissolved oxygen content, titratable acidity, storage temperature, species and strains of associative fermented dairy product organisms and concentration of lactic and acetic acids are some of the factors that greatly affect the probiotic viability in the product. As set by Food and Agriculture Organization (FAO) of the United Nations and the World Health Organization (WHO), the standard in order to be considered a health food with probitic addition, the product should contain per gram at least 10-10 cfu of viable probiotic bacteria. It is necessary that the bacterial cells remain stable and healthy in the manufactured product, are sufficiently viable while moving through the upper digestive tract and are able to provide positive effects upon reaching the intestine of the host.\n\nCell microencapsulation technology has successfully been applied in the food industry for the encapsulation of live probiotic bacteria cells to increase viability of the bacteria during processing of dairy products and for targeted delivery to the gastrointestinal tract.\n\nApart from dairy products, microencapsulated probiotics have also been used in non-dairy products, such as TheresweetTM which is a sweetener. It can be used as a convenient vehicle for delivery of encapsulated \"Lactobacillus\" to the intestine although it is not itself a dairy product.\n\nThe potential of using bioartificial pancreas, for treatment of diabetes mellitus, based on encapsulating islet cells within a semi permeable membrane is extensively being studied by scientists. These devices could eliminate the need for of immunosuppressive drugs in addition to finally solving the problem of shortage of organ donors. The use of microencapsulation would protect the islet cells from immune rejection as well as allow the use of animal cells or genetically modified insulin-producing cells. It is hoped that development of these islet encapsulated microcapsules could prevent the need for the insulin injections needed several times a day by type 1 diabetic patients. The Edmonton protocol involves implantation of human islets extracted from cadaveric donors and has shown improvements towards the treatment of type 1 diabetics who are prone to hypoglycemic unawareness. However, the two major hurdles faced in this technique are the limited availability of donor organs and with the need for immunosuppresents to prevent an immune response in the patient's body.\n\nSeveral studies have been dedicated towards the development of bioartificial pancreas involving the immobilization of islets of Langerhans inside polymeric capsules. The first attempt towards this aim was demonstrated in 1980 by Lim et al. where xenograft islet cells were encapsulated inside alginate polylysine microcapsules and showed significant in vivo results for several weeks. It is envisaged that the implantation of these encapsulated cells would help to overcome the use of immunosuppressive drugs and also allow the use of xenograft cells thus obviating the problem of donor shortage.\n\nThe polymers used for islet microencapsulation are alginate, chitosan, polyethylene glycol (PEG), agarose, sodium cellulose sulfate and water-insoluble polyacrylates with alginate and PEG being commonly used polymers. \nWith successful in vitro studies being performed using this technique, significant work in clinical trials using microencapsulated human islets is being carried out. In 2003, the use of alginate/PLO microcapsules containing islet cells for pilot phase-1 clinical trials was permitted to be carried out at the University of Perugia by the Italian Ministry of Health. In another study, the potential of clinical application of PEGylation and low doses of the immunosuppressant cyclosporine A were evaluated. The trial which began in 2005 by Novocell, now forms the phase I/II of clinical trials involving implantation of islet allografts into the subcutaneous site. However, there have been controversial studies involving human clinical trials where Living Cell technologies Ltd demonstrated the survival of functional xenogeneic cells transplanted without immunosuppressive medication for 9.5 years. However, the trial received harsh criticism from the International Xenotransplantation Association as being risky and premature.\nHowever, even though clinical trials are under way, several major issues such as biocompatibility and immunoprotection need to be overcome.\n\nPotential alternatives to encapsulating isolated islets (of either allo- or xenogeneic origin) are also being explored. Using sodium cellulose sulphate technology from Austrianova Singapore an islet cell line was encapsulated and it was demonstrated that the cells remain viable and release insulin in response to glucose. In pre-clinical studies, implanted, encapsulated cells were able to restore blood glucose levels in diabetic rats over a period of 6 months.\n\nThe use of cell encapsulated microcapsules towards the treatment of several forms of cancer has shown great potential. One approach undertaken by researchers is through the implantation of microcapsules containing genetically modified cytokine secreting cells. An example of this was demonstrated by Cirone et al. when genetically modified IL-2 cytokine secreting non-autologous mouse myoblasts implanted into mice showed a delay in the tumor growth with an increased rate of survival of the animals. However, the efficiency of this treatment was brief due to an immune response towards the implanted microcapsules. \nAnother approach to cancer suppression is through the use of angiogenesis inhibitors to prevent the release of growth factors which lead to the spread of tumors. The effect of implanting microcapsules loaded with xenogenic cells genetically modified to secrete endostatin, an antiangiogenic drug which causes apoptosis in tumor cells, has been extensively studied. However, this method of local delivery of microcapsules was not feasible in the treatment of patients with many tumors or in metastasis cases and has led to recent studies involving systemic implantation of the capsules.\n\nIn 1998, a murine model of pancreatic cancer was used to study the effect of implanting genetically modified cytochrome P450 expressing feline epithelial cells encapsulated in cellulose sulfate polymers for the treatment of solid tumors. The approach demonstrated for the first time the application of enzyme expressing cells to activate chemotherapeutic agents. On the basis of these results, an encapsulated cell therapy product, NovaCaps, was tested in a phaseI/II clinical trial for the treatment of pancreatic cancer in patients and has recently been designated by the European medicines agency (EMEA) as an orphan drug in Europe. A further phase I/II clinical trial using the same product confirmed the results of the first trial, demonstrating an approximate doubling of survival time in patients with stage IV pancreatic cancer. In all of these trials using cellulose sulphate, in addition to the clear anti-tumour effects, the capsules were well tolerated and there were no adverse reactions seen such as immune response to the capsules, demonstrating the biocompatible nature of the cellulose sulphate capsules. In one patient the capsules were in place for almost 2 years with no side effects.\n\nThese studies show the promising potential application of cell microcapsules towards the treatment of cancers. However, solutions to issues such as immune response leading to inflammation of the surrounding tissue at the site of capsule implantation have to be researched in detail before more clinical trials are possible.\n\nNumerous studies have been dedicated towards the development of effective methods to enable cardiac tissue regeneration in patients after ischemic heart disease. An emerging approach to answer the problems related to ischemic tissue repair is through the use of stem cell-based therapy. However, the actual mechanism due to which this stem cell-based therapy has generative effects on cardiac function is still under investigation. Even though numerous methods have been studied for cell administration, the efficiency of the number of cells retained in the beating heart after implantation is still very low. A promising approach to overcome this problem is through the use of cell microencapsulation therapy which has shown to enable a higher cell retention as compared to the injection of free stem cells into the heart.\n\nAnother strategy to improve the impact of cell based encapsulation technique towards cardiac regenerative applications is through the use of genetically modified stem cells capable of secreting angiogenic factors such as vascular endothelial growth factor (VEGF) which stimulate neovascularization and restore perfusion in the damaged ischemic heart. An example of this is shown in the study by Zang et al. where genetically modified xenogeneic CHO cells expressing VEGF were encapsulated in alginate-polylysine-alginate microcapsules and implanted into rat myocardium. It was observed that the encapsulation protected the cells from an immunorespone for three weeks and also led to an improvement in the cardiac tissue post-infarction due to increased angiogenesis.\n\nThe use of monoclonal antibodies for therapy is now widespread for treatment of cancers and inflammatory diseases. Using cellulose sulphate technology, scientists have successfully encapsulated antibody producing hybridoma cells and demonstrated subsequent release of the therapeutic antibody from the capsules. The capsules containing the hybridoma cells were used in pre-clinical studies to deliver neutralising antibodies to the mouse retrovirus FrCasE, successfully preventing disease.\n\nMany other medical conditions have been targeted with encapsulation therapies, especially those involving a deficiency in some biologically derived protein. One of the most successful approaches is an external device that acts similarly to a dialysis machine, only with a reservoir of pig hepatocytes surrounding the semipermeable portion of the blood-infused tubing. This apparatus can remove toxins from the blood of patients suffering severe liver failure. Other applications that are still in development include cells that produce Ciliary-derived neurotrophic factor for the treatment of ALS and Huntington's Disease, Glial-derived neurotrophic factor for Parkinson's Disease, Erythropoietin for Anemia, and HGH for Dwarfism. In addition, monogeneic diseases such as haemophilia, Gaucher's disease and some Mucopolysaccharide disorders could also potentially be targeted by encapsulated cells expressing the protein that is otherwise lacking in the patient.\n"}
{"id": "1916848", "url": "https://en.wikipedia.org/wiki?curid=1916848", "title": "Central Electricity Board", "text": "Central Electricity Board\n\nIn 1925 Lord Weir chaired a committee that proposed the creation of the Central Electricity Board (CEB) to link the UK’s most efficient power stations with consumers\nvia a ‘national gridiron’.\n\nThe United Kingdom Central Electricity Board was set up under The Electricity (Supply) Act 1926 to standardise the nation's electricity supply. At that time, the industry consisted of more than 600 electricity supply companies and local authority undertakings, and different areas operated at different voltages and frequencies (including DC in some places). The board's first chairman was Andrew Duncan.\n\nThe CEB established the UK's first synchronised AC grid, running at 132 kilovolts and 50 Hertz, which by 1933 was a collection of local grids, with emergency interlinks, covering most of England. This started operating as a national system, the National Grid, in 1938. The CEB ceased to exist when the grid was nationalised by the Electricity Act 1947 and taken over by the British Electricity Authority.\n\nThe CEB coexisted with the Electricity Commissioners, an industry regulator responsible to the Ministry of Transport.\n\n"}
{"id": "291436", "url": "https://en.wikipedia.org/wiki?curid=291436", "title": "Chemiluminescence", "text": "Chemiluminescence\n\nChemiluminescence (also chemoluminescence) is the emission of light (luminescence), as the result of a chemical reaction. There may also be limited emission of heat. Given reactants A and B, with an excited intermediate ◊,\n\nFor example, if [A] is luminol and [B] is hydrogen peroxide in the presence of a suitable catalyst we have:\n\nwhere:\n\n\nThe decay of this excited state[◊] to a lower energy level causes light emission. In theory, one photon of light should be given off for each molecule of reactant. This is equivalent to Avogadro's number of photons per mole of reactant. In actual practice, non-enzymatic reactions seldom exceed 1% Q, quantum efficiency.\n\nIn a chemical reaction, reactants collide to form a transition state, the enthalpic maximum in a reaction coordinate diagram, which proceeds to the product. Normally, reactants form products of lesser chemical energy. The difference in energy between reactants and products, represented as formula_1, is turned into heat, physically realized as excitations in the vibrational state of the normal modes of the product. Since vibrational energy is generally much greater than the thermal agitation, it rapidly disperses in the solvent through molecular rotation. This is how exothermic reactions make their solutions hotter. In a chemiluminescent reaction, the direct product of the reaction is an excited electronic state. This state then decays into an electronic ground state and emits light through either an allowed transition (analogous to fluorescence) or a forbidden transition (analogous to phosphorescence), depending partly on the spin state of the electronic excited state formed.\n\nChemiluminescence differs from fluorescence or phosphorescence in that the electronic excited state is the product of a chemical reaction rather than of the absorption of a photon. It is the antithesis of a photochemical reaction, in which light is used to drive an endothermic chemical reaction. Here, light is \"generated\" from a chemically exothermic reaction. The chemiluminescence might be also induced by an electrochemical stimulus, in this case is called electrochemiluminescence.\n\nA standard example of chemiluminescence in the laboratory setting is the luminol test. Here, blood is indicated by luminescence upon contact with iron in hemoglobin. When chemiluminescence takes place in living organisms, the phenomenon is called bioluminescence. A light stick emits light by chemiluminescence.\n\nChemiluminescence in aqueous system is mainly caused by redox reactions.\n\n\n\nIn chemical kinetics, \"infrared chemiluminiscence\" (IRCL) refers to the emission of infrared photons from vibrationally excited product molecules immediately after their formation. The intensities of infrared emission lines from vibrationally excited molecules are used to measure the populations of vibrational states of product molecules.\n\nThe observation of IRCL was developed as a kinetic technique by John Polanyi, who used it to study the attractive or repulsive nature of the potential energy surface for gas-phase reactions. In general the IRCL is much more intense for reactions with an attractive surface, indicating that this type of surface leads to energy deposition in vibrational excitation. In contrast reactions with a repulsive potential energy surface lead to little IRCL, indicating that the energy is primarily deposited as translational energy.\n\nEnhanced chemiluminescence is a common technique for a variety of detection assays in biology. A horseradish peroxidase enzyme (HRP) is tethered to an antibody that specifically recognizes the molecule of interest. This enzyme complex then catalyzes the conversion of the enhanced chemiluminescent substrate into a sensitized reagent in the vicinity of the molecule of interest, which on further oxidation by hydrogen peroxide, produces a triplet (excited) carbonyl, which emits light when it decays to the singlet carbonyl. Enhanced chemiluminescence allows detection of minute quantities of a biomolecule. Proteins can be detected down to femtomole quantities, well below the detection limit for most assay systems.\n\n\nChemiluminescence has been applied by forensic scientists to solve crimes. In this case, they use luminol and hydrogen peroxide. The iron from the blood acts as a catalyst and reacts with the luminol and hydrogen peroxide to produce blue light for about 30 seconds. Because only a small amount of iron is required for chemiluminescence, trace amounts of blood are sufficient.\n\nIn biomedical research, the protein that gives fireflies their glow and its co-factor, luciferin, are used to produce red light through the consumption of ATP. This reaction is used in many applications, including the effectiveness of cancer drugs that choke off a tumor's blood supply. This form of bioluminescence imaging allows scientists to test drugs in the pre-clinical stages cheaply.\nAnother protein, aequorin, found in certain jellyfish, produces blue light in the presence of calcium. It can be used in molecular biology to assess calcium levels in cells. What these biological reactions have in common is their use of adenosine triphosphate (ATP) as an energy source. Though the structure of the molecules that produce luminescence is different for each species, they are given the generic name of luciferin. Firefly luciferin can be oxidized to produce an excited complex. Once it falls back down to a ground state a photon is released. It is very similar to the reaction with luminol.\n\nMany organisms have evolved to produce light in a range of colors. At the molecular level, the difference in color arises from the degree of conjugation of the molecule, when an electron drops down from the excited state to the ground state. Deep sea organisms have evolved to produce light to lure and catch prey, as camouflage, or to attract others. Some bacteria even use bioluminescence to communicate. The common colors for the light emitted by these animals are blue and green because they have shorter wavelength than red and can transmit more easily in water.\n\nChemiluminescence is different from fluorescence. Hence the application of fluorescent proteins such as Green fluorescent protein is not a biological application of chemiluminescence.\n\n"}
{"id": "28283406", "url": "https://en.wikipedia.org/wiki?curid=28283406", "title": "Cloud fraction", "text": "Cloud fraction\n\nCloud fraction is the percentage of each pixel in satellite imagery or each gridbox in a weather or climate model that is covered with clouds. A cloud fraction of one means the pixel is completely covered with clouds, while a cloud fraction of zero represents a totally cloud free pixel. Cloud fraction is important for the modeling of downward radiation.\n"}
{"id": "20125479", "url": "https://en.wikipedia.org/wiki?curid=20125479", "title": "Collision cascade", "text": "Collision cascade\n\nA collision cascade (also known as a displacement cascade or a displacement spike) is a set of nearby adjacent energetic (much higher than ordinary thermal energies) collisions of atoms induced by an energetic particle in a solid or liquid.\n\nIf the maximum atom or ion energies in a collision cascade are higher than the threshold displacement energy of the material (tens of eVs or more), the collisions can permanently displace atoms from their lattice sites and produce defects. The initial energetic atom can be, e.g., an ion from a particle accelerator, an atomic recoil produced by a passing high-energy neutron, electron or photon, or be produced when a radioactive nucleus decays and gives the atom a recoil energy.\n\nThe nature of collision cascades can vary strongly depending on the energy and mass of the recoil/incoming ion and density of the material (stopping power).\n\nWhen the initial recoil/ion mass is low, and the material where the cascade occurs has a low density (i.e. the recoil-material combination has a low stopping power), the collisions between the initial recoil and sample atoms occur rarely, and can be understood well as a sequence of independent binary collisions between atoms. This kind of a cascade can be theoretically well treated using the binary collision approximation (BCA) simulation approach. For instance, H and He ions with energies below 10 keV can be expected to lead to purely linear cascades in all materials.\n\nThe most commonly used BCA code SRIM can be used to simulate linear collision cascades in disordered materials for all ion in all materials up to ion energies of 1 GeV. Note, however, that SRIM does not treat effects such as damage due to electronic energy deposition or damage produced by excited electrons. The nuclear and electronic stopping powers used are averaging fits to experiments, and are thus not perfectly accurate either. The electronic stopping power can be readily included in binary collision approximation or molecular dynamics (MD) simulations. In MD simulations they can be included either as a frictional force or in a more advanced manner by also following the heating of the electronic systems and coupling the electronic and atomic degrees of freedom. However, uncertainties remain on what is the appropriate low-energy limit of electronic stopping power or electron-phonon coupling is.\n\nIn linear cascades the set of recoils produced in the sample can be described as a sequence of recoil generations depending on how many collision steps have passed since the original collision: primary knock-on atoms (PKA), secondary knock-on atoms (SKA), tertiary knock-on atoms (TKA), etc. Since it is extremely unlikely that all energy would be transferred to a knock-on atom, each generation of recoil atoms has on average less energy than the previous, and eventually the knock-on atom energies go below the threshold displacement energy for damage production, at which point no more damage can be produced.\n\nWhen the ion is heavy and energetic enough, and the material is dense, the collisions between the ions may occur so near to each other that they can not be considered independent of each other. In this case the process becomes a complicated process of many-body interactions between hundreds and tens of thousands of atoms, which can not be treated with the BCA, but can be modelled using molecular dynamics methods.\n\nComputer simulation-based animations of collision cascades in the heat spike regime are available on YouTube.\n\nTypically, a heat spike is characterized by the formation of a transient underdense region in the center of the cascade, and an overdense region around it. After the cascade, the overdense region becomes interstitial defects, and the underdense region typically becomes a region of vacancies.\n\nIf the kinetic energy of the atoms in the region of dense collisions is recalculated into temperature (using the basic equation E = 3/2·N·kT), one finds that the kinetic energy in units of temperature is initially of the order of 10,000 K. Because of this, the region can be considered to be very hot, and is therefore called a \"heat spike\" or \"thermal spike\" (the two terms are usually considered to be equivalent). The heat spike cools down to the ambient temperature in 1–100 ps, so the \"temperature\" here does not correspond to thermodynamic equilibrium temperature. However, it has been shown that after about 3 lattice vibrations, the kinetic energy distribution of the atoms in a heat spike has the Maxwell–Boltzmann distribution, making the use of the concept of temperature somewhat justified. Moreover, experiments have shown that a heat spike can induce a phase transition which is known to require a very high temperature, showing that the concept of a (non-equilibrium) temperature is indeed useful in describing collision cascades.\n\nIn many cases, the same irradiation condition is a combination of linear cascades and heat spikes. For example, 10 MeV Cu ions bombarding Cu would initially move in the lattice in a linear cascade regime, since the nuclear stopping power is low. But once the Cu ion would slow down enough, the nuclear stopping power would increase and a heat spike would be produced. Moreover, many of the primary and secondary recoils of the incoming ions would likely have energies in the keV range and thus produce a heat spike.\n\nFor instance, for copper irradiation of copper, recoil energies of around 5–20 keV are almost guaranteed to produce heat spikes. At lower energies, the cascade energy is too low to produce a liquid-like zone. At much higher energies, the Cu ions would most likely lead initially to a linear cascade, but the recoils could lead to heat spikes, as would the initial ion once it has slowed down enough. The concept \"subcascade breakdown threshold energy\" signifies the energy above which a recoil in a material is likely to produce several isolated heat spikes rather than a single dense one.\n\nSwift heavy ions, i.e. MeV and GeV heavy ions which produce damage by a very strong electronic stopping, can also be considered to produce thermal spikes in the sense that they lead to strong lattice heating and a transient disordered atom zone. However, at least the initial stage of the damage might be better understood in terms of a Coulomb explosion mechanism. Regardless of what the heating mechanism is, it is well established that swift heavy ions in insulators typically produce ion tracks forming long cylindrical damage zones of reduced density.\n\nTo understand the nature of collision cascade, it is very important to know the associated time scale. The ballistic phase of the cascade, when the initial ion/recoil and its primary and lower-order recoils have energies well above the threshold displacement energy, typically lasts 0.1– 0.5 ps. If a heat spike is formed, it can live for some 1–100 ps until the spike temperature has cooled down essentially to the ambient temperature. The cooling down of the cascade occurs via lattice heat conductivity and by electronic heat conductivity after the hot ionic subsystem has heated up the electronic one via electron-phonon coupling. Unfortunately the rate of electron-phonon coupling from the hot and disordered ionic system is not well known, as it can not be treated equally to the fairly well known process of transfer of heat from hot electrons to an intact crystal structure. Finally, the relaxation phase of the cascade, when the defects formed possibly recombine and migrate, can last from a few ps to infinite times, depending on the material, its defect migration and recombination properties, and the ambient temperature.\n\nSince the kinetic energies in a cascade can be very high, it can drive the material locally far outside thermodynamic equilibrium. Typically this results in defect production. The defects can be, e.g., point defects such as \nFrenkel pairs, ordered or disordered dislocation loops, stacking faults, or amorphous zones. Prolonged irradiation of many materials can lead to their full amorphization, an effect which occurs regularly during the ion implantation doping of silicon chips.\n\nThe defects production can be harmful, such as in nuclear fission and fusion reactors where the neutrons slowly degrade the mechanical properties of the materials, or a useful and desired materials modification effect, e.g., when ions are introduced into semiconductor quantum well structures to speed up the operation of a laser. or to strengthen carbon nanotubes.\n\nA curious feature of collision cascades is that the final amount of damage produced may be much less than the number of atoms initially affected by the heat spikes. Especially in pure metals, the final damage production after the heat spike phase can be orders of magnitude smaller than the number of atoms displaced in the spike. On the other hand, in semiconductors and other covalently bonded materials the damage production is usually similar to the number of displaced atoms. Ionic materials can behave like either metals or semiconductors with respect to the fraction of damage recombined.\n\nCollision cascades in the vicinity of a surface often lead to sputtering, both in the linear spike and heat spike regimes. Heat spikes near surfaces also frequently lead to crater formation. This cratering is caused by liquid flow of atoms, but if the projectile size above roughly 100,000 atoms, the crater production mechanism switches to the same mechanism as that of macroscopic craters produced by bullets or asteroids.\n\nThe fact that lots of atoms are displaced by a cascade means that ions can be used to deliberately mix materials, even for materials that are normally thermodynamically immiscible. This effect is known as ion beam mixing.\n\nThe non-equilibrium nature of irradiation can also be used to drive materials out of thermodynamic equilibrium, and thus form new kinds of alloys.\n\n"}
{"id": "6015", "url": "https://en.wikipedia.org/wiki?curid=6015", "title": "Crystal", "text": "Crystal\n\nA crystal or crystalline solid is a solid material whose constituents (such as atoms, molecules, or ions) are arranged in a highly ordered microscopic structure, forming a crystal lattice that extends in all directions. In addition, macroscopic single crystals are usually identifiable by their geometrical shape, consisting of flat faces with specific, characteristic orientations. The scientific study of crystals and crystal formation is known as crystallography. The process of crystal formation via mechanisms of crystal growth is called crystallization or solidification.\n\nThe word \"crystal\" derives from the Ancient Greek word (), meaning both \"ice\" and \"rock crystal\", from (), \"icy cold, frost\".\n\nExamples of large crystals include snowflakes, diamonds, and table salt. Most inorganic solids are not crystals but polycrystals, i.e. many microscopic crystals fused together into a single solid. Examples of polycrystals include most metals, rocks, ceramics, and ice. A third category of solids is amorphous solids, where the atoms have no periodic structure whatsoever. Examples of amorphous solids include glass, wax, and many plastics.\n\nDespite the name, lead crystal, crystal glass, and related products are \"not\" crystals, but rather types of glass, i.e. amorphous solids.\n\nCrystals are often used in pseudoscientific practices such as crystal therapy, and, along with gemstones, are sometimes associated with spellwork in Wiccan beliefs and related religious movements.\n\nThe scientific definition of a \"crystal\" is based on the microscopic arrangement of atoms inside it, called the crystal structure. A crystal is a solid where the atoms form a periodic arrangement. (Quasicrystals are an exception, see below.)\n\nNot all solids are crystals. For example, when liquid water starts freezing, the phase change begins with small ice crystals that grow until they fuse, forming a \"polycrystalline\" structure. In the final block of ice, each of the small crystals (called \"crystallites\" or \"grains\") is a true crystal with a periodic arrangement of atoms, but the whole polycrystal does \"not\" have a periodic arrangement of atoms, because the periodic pattern is broken at the grain boundaries. Most macroscopic inorganic solids are polycrystalline, including almost all metals, ceramics, ice, rocks, etc. Solids that are neither crystalline nor polycrystalline, such as glass, are called \"amorphous solids\", also called glassy, vitreous, or noncrystalline. These have no periodic order, even microscopically. There are distinct differences between crystalline solids and amorphous solids: most notably, the process of forming a glass does not release the latent heat of fusion, but forming a crystal does.\n\nA crystal structure (an arrangement of atoms in a crystal) is characterized by its \"unit cell\", a small imaginary box containing one or more atoms in a specific spatial arrangement. The unit cells are stacked in three-dimensional space to form the crystal.\n\nThe symmetry of a crystal is constrained by the requirement that the unit cells stack perfectly with no gaps. There are 219 possible crystal symmetries, called crystallographic space groups. These are grouped into 7 crystal systems, such as cubic crystal system (where the crystals may form cubes or rectangular boxes, such as halite shown at right) or hexagonal crystal system (where the crystals may form hexagons, such as ordinary water ice).\n\nCrystals are commonly recognized by their shape, consisting of flat faces with sharp angles. These shape characteristics are not \"necessary\" for a crystal—a crystal is scientifically defined by its microscopic atomic arrangement, not its macroscopic shape—but the characteristic macroscopic shape is often present and easy to see.\n\nEuhedral crystals are those with obvious, well-formed flat faces. Anhedral crystals do not, usually because the crystal is one grain in a polycrystalline solid.\n\nThe flat faces (also called facets) of a euhedral crystal are oriented in a specific way relative to the underlying atomic arrangement of the crystal: they are planes of relatively low Miller index. This occurs because some surface orientations are more stable than others (lower surface energy). As a crystal grows, new atoms attach easily to the rougher and less stable parts of the surface, but less easily to the flat, stable surfaces. Therefore, the flat surfaces tend to grow larger and smoother, until the whole crystal surface consists of these plane surfaces. (See diagram on right.)\n\nOne of the oldest techniques in the science of crystallography consists of measuring the three-dimensional orientations of the faces of a crystal, and using them to infer the underlying crystal symmetry.\n\nA crystal's habit is its visible external shape. This is determined by the crystal structure (which restricts the possible facet orientations), the specific crystal chemistry and bonding (which may favor some facet types over others), and the conditions under which the crystal formed.\n\nBy volume and weight, the largest concentrations of crystals in the Earth are part of its solid bedrock. Crystals found in rocks typically range in size from a fraction of a millimetre to several centimetres across, although exceptionally large crystals are occasionally found. , the world's largest known naturally occurring crystal is a crystal of beryl from Malakialina, Madagascar, long and in diameter, and weighing .\n\nSome crystals have formed by magmatic and metamorphic processes, giving origin to large masses of crystalline rock. The vast majority of igneous rocks are formed from molten magma and the degree of crystallization depends primarily on the conditions under which they solidified. Such rocks as granite, which have cooled very slowly and under great pressures, have completely crystallized; but many kinds of lava were poured out at the surface and cooled very rapidly, and in this latter group a small amount of amorphous or glassy matter is common. Other crystalline rocks, the metamorphic rocks such as marbles, mica-schists and quartzites, are recrystallized. This means that they were at first fragmental rocks like limestone, shale and sandstone and have never been in a molten condition nor entirely in solution, but the high temperature and pressure conditions of metamorphism have acted on them by erasing their original structures and inducing recrystallization in the solid state.\n\nOther rock crystals have formed out of precipitation from fluids, commonly water, to form druses or quartz veins.\nThe evaporites such as halite, gypsum and some limestones have been deposited from aqueous solution, mostly owing to evaporation in arid climates.\n\nWater-based ice in the form of snow, sea ice and glaciers is a very common manifestation of crystalline or polycrystalline matter on Earth. A single snowflake is a single crystal or a collection of crystals, while an ice cube is a polycrystal.\n\nMany living organisms are able to produce crystals, for example calcite and aragonite in the case of most molluscs or hydroxylapatite in the case of vertebrates.\n\nThe same group of atoms can often solidify in many different ways. Polymorphism is the ability of a solid to exist in more than one crystal form. For example, water ice is ordinarily found in the hexagonal form Ice I, but can also exist as the cubic Ice I, the rhombohedral ice II, and many other forms. The different polymorphs are usually called different \"phases\".\n\nIn addition, the same atoms may be able to form noncrystalline phases. For example, water can also form amorphous ice, while SiO can form both fused silica (an amorphous glass) and quartz (a crystal). Likewise, if a substance can form crystals, it can also form polycrystals.\n\nFor pure chemical elements, polymorphism is known as allotropy. For example, diamond and graphite are two crystalline forms of carbon, while amorphous carbon is a noncrystalline form. Polymorphs, despite having the same atoms, may have wildly different properties. For example, diamond is among the hardest substances known, while graphite is so soft that it is used as a lubricant.\n\nPolyamorphism is a similar phenomenon where the same atoms can exist in more than one amorphous solid form.\n\nCrystallization is the process of forming a crystalline structure from a fluid or from materials dissolved in a fluid. (More rarely, crystals may be deposited directly from gas; see thin-film deposition and epitaxy.)\n\nCrystallization is a complex and extensively-studied field, because depending on the conditions, a single fluid can solidify into many different possible forms. It can form a single crystal, perhaps with various possible phases, stoichiometries, impurities, defects, and habits. Or, it can form a polycrystal, with various possibilities for the size, arrangement, orientation, and phase of its grains. The final form of the solid is determined by the conditions under which the fluid is being solidified, such as the chemistry of the fluid, the ambient pressure, the temperature, and the speed with which all these parameters are changing.\n\nSpecific industrial techniques to produce large single crystals (called \"boules\") include the Czochralski process and the Bridgman technique. Other less exotic methods of crystallization may be used, depending on the physical properties of the substance, including hydrothermal synthesis, sublimation, or simply solvent-based crystallization.\n\nLarge single crystals can be created by geological processes. For example, selenite crystals in excess of 10 meters are found in the Cave of the Crystals in Naica, Mexico. For more details on geological crystal formation, see above.\n\nCrystals can also be formed by biological processes, see above. Conversely, some organisms have special techniques to \"prevent\" crystallization from occurring, such as antifreeze proteins.\n\nAn \"ideal\" crystal has every atom in a perfect, exactly repeating pattern. However, in reality, most crystalline materials have a variety of crystallographic defects, places where the crystal's pattern is interrupted. The types and structures of these defects may have a profound effect on the properties of the materials.\n\nA few examples of crystallographic defects include vacancy defects (an empty space where an atom should fit), interstitial defects (an extra atom squeezed in where it does not fit), and dislocations (see figure at right). Dislocations are especially important in materials science, because they help determine the mechanical strength of materials.\n\nAnother common type of crystallographic defect is an impurity, meaning that the \"wrong\" type of atom is present in a crystal. For example, a perfect crystal of diamond would only contain carbon atoms, but a real crystal might perhaps contain a few boron atoms as well. These boron impurities change the diamond's color to slightly blue. Likewise, the only difference between ruby and sapphire is the type of impurities present in a corundum crystal.\nIn semiconductors, a special type of impurity, called a dopant, drastically changes the crystal's electrical properties. Semiconductor devices, such as transistors, are made possible largely by putting different semiconductor dopants into different places, in specific patterns.\n\nTwinning is a phenomenon somewhere between a crystallographic defect and a grain boundary. Like a grain boundary, a twin boundary has different crystal orientations on its two sides. But unlike a grain boundary, the orientations are not random, but related in a specific, mirror-image way.\n\nMosaicity is a spread of crystal plane orientations. A mosaic crystal is supposed to consist of smaller crystalline units that are somewhat misaligned with respect to each other.\n\nIn general, solids can be held together by various types of chemical bonds, such as metallic bonds, ionic bonds, covalent bonds, van der Waals bonds, and others. None of these are necessarily crystalline or non-crystalline. However, there are some general trends as follows.\n\nMetals are almost always polycrystalline, though there are exceptions like amorphous metal and single-crystal metals. The latter are grown synthetically. (A microscopically-small piece of metal may naturally form into a single crystal, but larger pieces generally do not.) Ionic compound materials are usually crystalline or polycrystalline. In practice, large salt crystals can be created by solidification of a molten fluid, or by crystallization out of a solution. Covalently bonded solids (sometimes called covalent network solids) are also very common, notable examples being diamond and quartz. Weak van der Waals forces also help hold together certain crystals, such as crystalline molecular solids, as well as the interlayer bonding in graphite. Polymer materials generally will form crystalline regions, but the lengths of the molecules usually prevent complete crystallization—and sometimes polymers are completely amorphous.\n\nA quasicrystal consists of arrays of atoms that are ordered but not strictly periodic. They have many attributes in common with ordinary crystals, such as displaying a discrete pattern in x-ray diffraction, and the ability to form shapes with smooth, flat faces.\n\nQuasicrystals are most famous for their ability to show five-fold symmetry, which is impossible for an ordinary periodic crystal (see crystallographic restriction theorem).\n\nThe International Union of Crystallography has redefined the term \"crystal\" to include both ordinary periodic crystals and quasicrystals (\"any solid having an essentially discrete diffraction diagram\").\n\nQuasicrystals, first discovered in 1982, are quite rare in practice. Only about 100 solids are known to form quasicrystals, compared to about 400,000 periodic crystals known in 2004. The 2011 Nobel Prize in Chemistry was awarded to Dan Shechtman for the discovery of quasicrystals.\n\nCrystals can have certain special electrical, optical, and mechanical properties that glass and polycrystals normally cannot. These properties are related to the anisotropy of the crystal, i.e. the lack of rotational symmetry in its atomic arrangement. One such property is the piezoelectric effect, where a voltage across the crystal can shrink or stretch it. Another is birefringence, where a double image appears when looking through a crystal. Moreover, various properties of a crystal, including electrical conductivity, electrical permittivity, and Young's modulus, may be different in different directions in a crystal. For example, graphite crystals consist of a stack of sheets, and although each individual sheet is mechanically very strong, the sheets are rather loosely bound to each other. Therefore, the mechanical strength of the material is quite different depending on the direction of stress.\n\nNot all crystals have all of these properties. Conversely, these properties are not quite exclusive to crystals. They can appear in glasses or polycrystals that have been made anisotropic by working or stress—for example, stress-induced birefringence.\n\n\"Crystallography\" is the science of measuring the crystal structure (in other words, the atomic arrangement) of a crystal. One widely used crystallography technique is X-ray diffraction. Large numbers of known crystal structures are stored in crystallographic databases.\n\n"}
{"id": "2618773", "url": "https://en.wikipedia.org/wiki?curid=2618773", "title": "DEMOnstration Power Station", "text": "DEMOnstration Power Station\n\nDEMO (DEMOnstration Power Station) is a proposed nuclear fusion power station that is intended to build upon the ITER experimental nuclear fusion reactor. The objectives of DEMO are usually understood to lie somewhere between those of ITER and a \"first of a kind\" commercial station, sometimes referred to as PROTO.\n\nWhile there is no clear international consensus on exact parameters or scope, the following parameters are often used as a baseline for design studies: DEMO should produce at least 2 gigawatts of fusion power on a continuous basis, and it should produce 25 times as much power as required for breakeven. DEMO's design of 2 to 4 gigawatts of thermal output will be on the scale of a modern electric power station.\n\nTo achieve its goals, DEMO must have linear dimensions about\n15% larger than ITER, and a plasma density about 30% greater than ITER. As a prototype commercial fusion reactor, DEMO could make fusion energy available by 2033. It is estimated that subsequent commercial fusion reactors could be built for about a quarter of the cost of DEMO.\n\nThe following timetable was presented at the IAEA Fusion Energy Conference in 2004 by Christopher Llewellyn Smith:\n\nIn 2012 European Fusion Development Agreement (EFDA) presented a roadmap to fusion power with a plan showing the dependencies of DEMO activities on ITER and IFMIF.\n\nThis 2012 roadmap was intended to be updated in 2015 and 2019, but EFDA was superseded by EUROfusion in 2013 and the roadmap was not further updated. It can be expected the roadmap will be at least 10 years postponed, according to schedule of ITER.\n\nWhen deuterium and tritium fuse, the two nuclei come together to form a resonant state which splits to form in turn a helium nucleus (an alpha particle) and a high-energy neutron.\nDEMO will be constructed once designs which solve the many problems of current fusion reactors are engineered. These problems include: containing the plasma fuel at high temperatures, maintaining a great enough density of reacting ions, and capturing high-energy neutrons from the reaction without melting the walls of the reactor.\n\n\nOnce fusion has begun, high-energy neutrons at about 160,000,000,000 kelvins will flood out of the plasma along with X-rays, neither being affected by the strong magnetic fields. Since neutrons receive the majority of the energy from the fusion, they will be the reactor's main source of thermal energy output. The ultra-hot helium product at roughly 40,000,000,000 kelvins will remain behind (temporarily) to heat the plasma, and must make up for all the loss mechanisms (mostly bremsstrahlung X-rays from electron collisions) which tend to cool the plasma rather quickly.\n\n\nThe DEMO project is planned to build upon and improve the concepts of ITER. Since it is only proposed at this time, many of the details, including heating methods and the method for the capture of high-energy neutrons, are still undetermined.\n\nAll aspects of DEMO were discussed in detail in a 2009 document by the Euratom-UKAEA Fusion Association.\nFour conceptual designs PPCS A,B,C,D were studied. Challenges identified included:\n\nIn the 2012 timeline the conceptual design should be completed in 2020.\n\nWhile fusion reactors like ITER and DEMO will produce neither transuranic nor fission product wastes, which together make up the bulk of the nuclear wastes produced by fission reactors, some of the components of the ITER and DEMO reactors will become radioactive due to neutrons impinging upon them. It is hoped that plasma facing materials will be developed so that wastes produced in this way will have much shorter half lives than the waste from fission reactors, with wastes remaining harmful for less than one century. Development of these materials is the prime purpose of the International Fusion Materials Irradiation Facility. The process of manufacturing tritium currently produces long-lived waste, but both ITER and DEMO will produce their own tritium, dispensing with the fission reactor currently used for this purpose.\n\nPROTO is a proposal for a beyond-DEMO experiment, part of European Commission long-term strategy for research of fusion energy. PROTO would act as a prototype power station, taking in any remaining technology refinements, and demonstrating electricity generation on a commercial basis. It is only expected after DEMO, beyond 2050, and may or may not be a second part of DEMO/PROTO experiment.\n\n"}
{"id": "1013923", "url": "https://en.wikipedia.org/wiki?curid=1013923", "title": "Datum reference", "text": "Datum reference\n\nA datum reference or just datum is some important part of an object—such as a point, line, plane, hole, set of holes, or pair of surfaces—that serves as a reference in defining the geometry of the object and (often) in measuring aspects of the actual geometry to assess how closely they match with the nominal value, which may be an ideal, standard, average, or desired value. For example, on a car's wheel, the lug nut holes define a bolt circle that is a datum from which the location of the rim can be defined and measured. This matters because the hub and rim need to be concentric to within close limits (or else the wheel will not roll smoothly). The concept of datums is used in many fields, including carpentry, metalworking, needlework, geometric dimensioning and tolerancing (GD&T), aviation, surveying, and others. \n\nIn carpentry, an alternative, more common name is \"face side\" and \"face edge\". The artisan nominates two straight edges on a workpiece as the \"datum edges\", and they are marked accordingly. One convention is to mark the first datum edge with a single slanted line (/) and the second with double lines (//). For most work, the datum references of the workpiece need to be square. If necessary they may be cut, planed or filed to make them so. In subsequent marking out, all measurements are then taken from either of the two datum references.\n\nIn aviation, an aircraft is designed to operate within a specified range of weight and (chiefly longitudinal) balance; an airman is responsible for determining these factors for each flight under his or her command. This requires the calculation of moment for each variable mass in the aircraft (fuel, passengers, cargo, etc.), by multiplying its weight by its distance from a datum reference. The datum for light airplanes is usually the engine firewall or the tip of the spinner, but in all cases it is a fixed plane perpendicular to the aircraft's longitudinal axis, and specified in its operating handbook.\n\nAn engineering datum used in geometric dimensioning and tolerancing is a feature on an object used to create a reference system for measurement.\nIn engineering and drafting, a \"datum\" is a reference point, surface, or axis on an object against which measurements are made.\n\nThese are then referred to by one or more 'datum references' which indicate measurements that should be made with respect to the corresponding datum feature .\n\nIn geometric dimensioning and tolerancing, datum reference frames are typically 3D. Datum reference frames are used as part of the feature control frame to show where the measurement is taken from. A typical datum reference frame is made up of three planes. For example, the three planes could be one \"face side\" and two \"datum edges\". These three planes are marked A, B and C, where A is the face side, B is the first datum edge, and C is the second datum edge. In this case, the datum reference frame is A/B/C. A/B/C is shown at the end of feature control frame to show from where the measurement is taken. (See the ASME standard Y14.5M-2009 for more examples and material modifiers.)\n\nThe engineer selects A/B/C based on the dimensional function of the part. The datums should be functional per the ASME standard. Typically, a part is required to fit with other parts. So, the functional datums are chosen based on how the part attaches. Note: Typically, the functional datums are not used to manufacture the part. The manufacturing datums are typically different from the functional datums to save cost, improve process speed, and repeatability. A tolerance analysis may be needed in many cases to convert between the functional datums and the manufacturing datums. Computer software can be purchased for dimensional analysis. A trained engineer is required to run the software.\n\nThere are typically 6 degrees of freedom that need to be considered by the engineer before choosing which feature is A, B, or C. For this example, A is the primary datum, B is the secondary, and C is the tertiary datum. The primary datum controls the most degrees of freedom. The tertiary datum controls the least degrees of freedom. For this example, of a block of wood, Datum A controls 3 degrees of freedom, B controls 2 degrees of freedom, and C controls 1 degree of freedom. 3+2+1 = 6, all 6 degrees of freedom are considered.\n\nThe 6 degrees of freedom in this example are 3 translation and 3 rotation about the 3D coordinate system. Datum A controls 3: translation along the Z axis, rotation about the x axis, and rotation about the y axis. Datum B controls 2: translation along the y axis and rotation about the z axis. Finally, Datum C controls 1 degree of freedom, namely the translation along the x axis.\n\n"}
{"id": "22300890", "url": "https://en.wikipedia.org/wiki?curid=22300890", "title": "Discotic liquid crystal", "text": "Discotic liquid crystal\n\nDiscotic liquid crystals are mesophases formed from disc-shaped molecules known as \"discotic mesogens\". These phases are often also referred to as columnar phases. Discotic mesogens are typically composed of an aromatic core surrounded by flexible alkyl chains. The aromatic cores allow charge transfer in the stacking direction through the π conjugate systems. The charge transfer allows the discotic liquid crystals to be electrically semiconductive along the stacking direction. Applications have been focusing on using these systems in photovoltaic devices, organic light emitting diodes (OLED), and molecular wires. Discotics have also been suggested for use in compensation films, for LCD displays.\n\nDiscotic liquid crystals have similar potential to the conducting polymers for their use in photovoltaic cells, they have the same technical challenges of low conductivity and sensitivity to UV damage as the polymer designs. However one advantage is the self-healing properties of the discotic mesogens. So far the photovoltaic applications have been limited, using a perylene and hexabenzocoronene mesogens in a simple two layer systems has only resulted in ~2% power efficiency.\n\nSo far the study into discotic liquid crystals for light emitting diodes are still in its infancy, but there have been some examples produced; a triphenylene and perylene-mesogen combination can be used to make a red LED. The self-assembly properties make them more desirable for manufacturing purposes when producing commercial electronics, than the currently used small molecule crystals in Sony’s new OLED displays. Also they have the added benefit of the self-healing properties that both the small molecule and the polymers lack as conductors, potentially being beneficial for longevity OLED products.\n"}
{"id": "1322118", "url": "https://en.wikipedia.org/wiki?curid=1322118", "title": "Distribution transformer", "text": "Distribution transformer\n\nA distribution transformer or service transformer is a transformer that provides the final voltage transformation in the electric power distribution system, stepping down the voltage used in the distribution lines to the level used by the customer. \nThe invention of a practical efficient transformer made AC power distribution feasible; a system using distribution transformers was demonstrated as early as 1882.\n\nIf mounted on a utility pole, they are called pole-mount transformers. If the distribution lines are located at ground level or underground, distribution transformers are mounted on concrete pads and locked in steel cases, thus known as a distribution tap pad-mount transformers.\n\nDistribution transformers normally have ratings less than 200 kVA, although some national standards can allow for units up to 5000 kVA to be described as distribution transformers. Since distribution transformers are energized for 24 hours a day (even when they don't carry any load), reducing iron losses has an important role in their design. As they usually don't operate at full load, they are designed to have maximum efficiency at lower loads. To have a better efficiency, voltage regulation in these transformers should be kept to a minimum. Hence they are designed to have small leakage reactance.\n\nDistribution transformers are classified into different categories based on certain factors such as:\n\nDistribution transformers are normally located at a service drop, where wires run from a utility pole or underground power lines to a customer's premises. They are often used for the power supply of facilities outside settlements, such as isolated houses, farmyards or pumping stations at voltages below 30 kV. Another application is the power supply of the overhead wire of railways electrified with AC. In this case single phase distribution transformers are used.\n\nThe number of customers fed by a single distribution transformer varies depending on the number of customers in an area. Several homes may be fed off a single transformer in urban areas; rural distribution may require one transformer per customer. A large commercial or industrial complex will have multiple distribution transformers. In urban areas and neighborhoods where the primary distribution lines run underground, padmount transformers, transformers in locked metal enclosures mounted on a concreted pad, are used. Many large buildings have electric service provided at primary distribution voltage. These buildings have customer-owned transformers in the basement for step-down purposes. \n\nDistribution transformers are also found in the power collector networks of wind farms, where they step up power from each wind turbine to connect to a substation that may be several miles (kilometres) distant.\n\nBoth pole-mount and pad-mount transformers convert the high 'primary' voltage of the overhead or underground distribution lines to the lower 'secondary' voltage of the distribution wires inside the building. The primaries use the three-phase system. Main distribution lines always have three wires, while smaller \"laterals\" (close to the customer) may include one or two phases, used to serve all customers with single-phase power. If three-phase service is desired, one must have a three-phase supply. Primaries provide power at the standard distribution voltages used in the area; these range from as low as 2300 volts to about 35,000 volts depending on local distribution practice and standards; often 11,000 V (50 Hz systems) and 13,800 V (60 Hz systems) are used but many other voltages are standard.\n\nThe high voltage primary windings are brought out to bushings on the top of the case. \nThe transformer is always connected to the primary distribution lines through protective fuses and disconnect switches. For pole-mounted transformers this usually takes the form of a 'fused cutout'. An electrical fault causes the fuse to melt, and the device drops open to give a visual indication of trouble. It can also be manually opened while the line is energized by lineworkers using insulated hot sticks. In some cases completely self protected transformers are used, which have a circuit breaker built in, so a fused cutout isn't needed.\n\nThe low voltage secondary windings are attached to three or four terminals on the transformer's side. \n\nHigher secondary voltages, such as 480 volts, are sometimes required for commercial and industrial uses. Some industrial customers require three-phase power at secondary voltages. To provide this, three-phase transformers can be used. In the US, which uses mostly single phase transformers, three identical single phase transformers are often wired in a \"transformer bank\" in either a wye or delta connection, to create a three phase transformer.\n\nDistribution transformers are made using a core made from laminations of sheet steel stacked and either glued together with resin or banded together with steel straps. Where large numbers of transformers are made to standard designs, a wound C-shaped core is economic to manufacture. A steel strip is wrapped around a former, pressed into shape and then cut into two C-shaped halves, which are re-assembled on the copper windings.\n\nThe primary coils are wound from enamel coated copper or aluminum wire and the high current, low voltage secondaries are wound using a thick ribbon of aluminum or copper. The windings are insulated with resin-impregnated paper. The entire assembly is baked to cure the resin and then submerged in a powder coated steel tank which is then filled with transformer oil (or other insulating liquid), which is inert and non-conductive. The transformer oil cools and insulates the windings, and protects the transformer winding from moisture, which will float on the surface of the oil. The tank is temporarily depressurized to remove any remaining moisture that would cause arcing and is sealed against the weather with a gasket at the top.\n\nFormerly, distribution transformers for indoor use would be filled with a polychlorinated biphenyl (PCB) liquid. Because these liquids persist in the environment and have adverse effects on animals, they have been banned. Other fire-resistant liquids such as silicones are used where a liquid-filled transformer must be used indoors. Certain vegetable oils have been applied as transformer oil; these have the advantage of a high fire point and are completely biodegradable in the environment.\n\nPole-mounted transformers often include accessories such as surge arresters or protective fuse links. A self-protected transformer includes an internal fuse and surge arrester; other transformers have these components mounted separately outside the tank. Pole-mounted transformers may have lugs allowing direct mounting to a pole, or may be mounted on crossarms bolted to the pole. Aerial transformers, larger than around 75 kVA, may be mounted on a platform supported by one or more poles. A three-phase service may use three identical transformers, one per phase.\n\nTransformers designed for below-grade installation can be designed for periodic submersion in water.\n\nDistribution transformers may include an off-load tap changer to allow slight adjustment of the ratio between primary and secondary voltage, to bring the customer voltage within the desired range on long or heavily loaded lines.\n\nPad-mounted transformers have secure locked and bolted grounded metal enclosures to discourage unauthorized access to live internal parts. The enclosure may also include fuses, isolating switches, load-break bushings, and other accessories as described in technical standards. Pad-mounted transformers for distribution systems typically range from around 100 to 2000 kVA, although some larger units are also used.\n\n\n"}
{"id": "52753112", "url": "https://en.wikipedia.org/wiki?curid=52753112", "title": "Ecoleasing", "text": "Ecoleasing\n\nEcoleasing is a system in which goods (mainly from the technical cycle, i.e. appliances, ...) are rented to a client for a certain period of time after which he returns the goods so the company that made it can recycle the materials.\n\nThe term ecoleasing has been used by William McDonough and Michael Braungart in their book Cradle_to_Cradle: Remaking the Way We Make Things. It is used to distinct itself from regular leasing in that:\n\n\nEcoleasing can for instance be done with TV's. Instead of the consumer purchasing the TV, by ecoleasing it, he is entitled to say 10 000 service hours. After this, he can send the TV back to the company.\n\n"}
{"id": "8864153", "url": "https://en.wikipedia.org/wiki?curid=8864153", "title": "Energy policy of the European Union", "text": "Energy policy of the European Union\n\nAlthough the European Union has legislated in the area of energy policy for many years, the concept of introducing a mandatory and comprehensive European Union energy policy was only approved at the meeting of the informal European Council on 27 October 2005 at Hampton Court. The EU Treaty of Lisbon of 2007 legally includes solidarity in matters of energy supply and changes to the energy policy within the EU. Prior to the Treaty of Lisbon, EU energy legislation has been based on the EU authority in the area of the common market and environment. However, in practice many policy competencies in relation to energy remain at national member state level, and progress in policy at European level requires voluntary cooperation by members states.\n\nIn 2007, the EU was importing 82% of its oil and 57% of its gas, which then made it the world's leading importer of these fuels. Only 3% of the uranium used in European nuclear reactors was mined in Europe. Russia, Canada, Australia, Niger and Kazakhstan were the five largest suppliers of nuclear materials to the EU, supplying more than 75% of the total needs in 2009. In 2015, the EU imports 53% of the energy it consumes.\nIn January 2014, the EU agreed to a 40% emissions reduction by 2030, compared to 1990 levels, and a 27% renewable energy target. The target is the most ambitious of any region in the world, and is expected to provide 70,000 full-time jobs and cut €33bn in fossil fuel imports.\n\nIn 2015, the Framework Strategy for Energy Union is launched as one of the European Commission's 10 Priorities.\n\nThe Energy Union Strategy is a project of the European Commission to coordinate the transformation of European energy supply. It was launched in February 2015, with the aim of providing secure, sustainable, competitive, affordable energy.\n\nDonald Tusk, President of the European Council, introduced the idea of an energy union when he was Prime Minister of Poland. Eurocommissioner Vice President Maroš Šefčovič called the Energy Union the biggest energy project since the European Coal and Steel Community. The EU's reliance on Russia for its energy, and the annexation of Crimea by Russia have been cited as strong reasons for the importance of this policy.\n\nThe European Council concluded on 19March 2015 that the EU is committed to building an Energy Union with a forward-looking climate policy on the basis of the Commission's framework strategy, with five priority dimensions:\nThe strategy includes a minimum 10% electricity interconnection target for all member states by 2020, which the Commission hopes will put downward pressure onto energy prices, reduce the need to build new power plants, reduce the risk of black-outs or other forms of electrical grid instability, improve the reliability of renewable energy supply, and encourage market integration.\n\nEU Member States agreed 25 January 2018 on the Commission's proposal to invest €873 million in clean energy infrastructure. The projects are financed by CEF (Connecting Europe Facility).\n\nThe possible principles of Energy Policy for Europe were elaborated at the Commission's green paper \"A European Strategy for Sustainable, Competitive and Secure Energy\" on 8 March 2006. As a result of the decision to develop a common energy policy, the first proposals, \"Energy for a Changing World\" were published by the European Commission, following a consultation process, on 10 January 2007.\n\nIt is claimed that they will lead to a 'post-industrial revolution', or a low-carbon economy, in the European Union, as well as increased competition in the energy markets, improved security of supply, and improved employment prospects. The Commission's proposals have been approved at a meeting of the European Council on 8 and 9 March 2007.\n\nKey proposals include:\n\n\nMany underlying proposals are designed to limit global temperature changes to no more than 2 °C above pre-industrial levels, of which 0.8 °C has already taken place and another 0.5–0.7 °C is already committed. 2 °C is usually seen as the upper temperature limit to avoid 'dangerous global warming'. Due to only minor efforts in global Climate change mitigation it is highly likely that the world will not be able to reach this particular target. The EU might then not only be forced to accept a less ambitious global target. Because the planned emissions reductions in the European energy sector (95% by 2050) are derived directly from the 2 °C target since 2007, the EU will have to revise its energy policy paradigm.\n\nIn 2014, negotiations about binding EU energy and climate targets until 2030 are set to start.\nEuropean Parliament voted in February 2014 in favour of binding 2030 targets on renewables, emissions and energy efficiency: a 40% cut in greenhouse gases, compared with 1990 levels; at least 30% of energy to come from renewable sources; and a 40% improvement in energy efficiency.\n\nThe FP7 research program only reserved a moderate amount of funding for energy research, although energy has recently emerged as one of the key issues of the European Union. A large part of FP7 energy funding is also devoted to fusion research, a technology that will not be able to help meet European climate and energy objectives until beyond 2050. The European Commission tried to redress this shortfall with the SET plan.\n\nThe Steering Group on the implementation of the Strategic Energy Technologies Plan (SET Plan) on 26 June 2008 will set the agenda for an EU energy technology policy. It will enhance the coordination of national and European research and innovation efforts to position the EU in the forefront of the low-carbon technologies markets.\n\nThe SET plan initiatives:\n\nThe budget for the SET plan is estimated at €71 billion.\n\nThe IEA raised its concern that demand-side technologies do not feature at all in the six priority areas of the SET Plan.\n\nThe EU resolution is available in EUR-Lex\n\nThe European Energy Research Alliance (EERA) is founded by the leading research institutes in the European Union (EU), to expand and optimise EU energy research capabilities through the sharing of world-class national facilities and the joint realisation of national and European programmes. This new Research Alliance will be a key actor of the EU Strategic Energy Technology Plan (SET Plan) and will contribute to accelerate the development of new low carbon technologies for EU to move toward a low carbon economy. As of May 2017,Nils Anders Røkke from SINTEF is the EERA Chairman.\n\nThe climate and energy policy is a choice between democracy and autocracy. The choice is one between patronage-based oil-and-gas oligarchies on the one hand, and adaptive and innovative low-carbon economy on the other.\n\nUnder the requirements of the Directive on Electricity Production from Renewable Energy Sources, which entered into force in October 2001, the member states are expected to meet \"indicative\" targets for renewable energy production. Although there is significant variation in national targets, the average is that 22% of electricity should be generated by renewables by 2010 (compared to 13,9% in 1997). The European Commission has proposed in its Renewable Energy Roadmap21 a binding target of increasing the level of renewable energy in the EU's overall mix from less than 7% today to 20% by 2020.\n\nEurope spent €406 billion in 2011 and €545 billion in 2012 on importing fossil fuels. This is around three times more than the cost of the Greek bailout up to 2013. In 2012, wind energy avoided €9.6 billion of fossil fuel costs. EWEA recommends binding renewable energy target to support in replacing fossil fuels with wind energy in Europe by providing a stable regulatory framework. In addition, it recommends setting a minimum emission performance standard for all new-build power installations.\n\nThe EU promotes electricity market liberalisation and security of supply through Directive 2009/72/EC.\n\nThe 2004 Gas Security Directive has been intended to improve security of supply in the natural gas sector.\n\nAt the Heiligendamm Summit in June 2007, the G8 acknowledged an EU proposal for an international initiative on energy efficiency tabled in March 2007, and agreed to explore, together with the International Energy Agency, the most effective means to promote energy efficiency internationally. A year later, on 8 June 2008, the G8 countries, China, India, South Korea and the European Community decided to establish the International Partnership for Energy Efficiency Cooperation, at the Energy Ministerial meeting hosted by Japan in the frame of the 2008 G8 Presidency, in Aomori.\n\nBuildings account for around 40% of EU energy requirements and have been the focus of several initiatives. From 4 January 2006, the 2002 Directive on the energy performance of buildings requires member states to ensure that new buildings, as well as large existing buildings undergoing refurbishment, meet certain minimum energy requirements. It also requires that all buildings should undergo 'energy certification' prior to sale, and that boilers and air conditioning equipment should be regularly inspected.\n\nAs part of the EU's SAVE Programme, aimed at promoting energy efficiency and encouraging energy-saving behaviour, the Boiler Efficiency Directive specifies minimum levels of efficiency for boilers fired with liquid or gaseous fuels. Originally, from June 2007, all homes (and other buildings) in the UK would have to undergo Energy Performance Certification before they are sold or let, to meet the requirements of the European Energy Performance of Buildings Directive (Directive 2002/91/EC).\n\nCarbon dioxide emissions from transport have risen rapidly in recent years, from 21% of the total in 1990 to 28% in 2004.\n\nEU policies include the voluntary ACEA agreement, signed in 1998, to cut carbon dioxide emissions for new cars sold in Europe to an average of 140 grams of /km by 2008, a 25% cut from the 1995 level. Because the target was unlikely to be met, the European Commission published new proposals in February 2007, requiring a mandatory limit of 130 grams of /km for new cars by 2012, with 'complementary measures' being proposed to achieve the target of 120 grams of /km that had originally been expected.\n\nIn the area of fuels, the 2001 Biofuels Directive requires that 5,75% of all transport fossil fuels (petrol and diesel) should be replaced by biofuels by 31 December 2010, with an intermediate target of 2% by the end of 2005. In February 2007 the European Commission proposed that, from 2011, suppliers will have to reduce carbon emissions per unit of energy by 1% a year from 2010 levels, to result in a cut of 10% by 2020 Stricter fuel standards to combat climate change and reduce air pollution.\n\nAirlines can be charged for their greenhouse gas emissions on flights to and from Europe according to a court ruling in October 2011.\n\nThe European Union Emission Trading Scheme, introduced in 2005 under the 2003 Emission Trading Directive, sets national caps on greenhouse gas emissions for power plants and other large point sources.\n\nA further area of energy policy has been in the area of consumer goods, where energy labels were introduced to encourage consumers to purchase more energy-efficient appliances.\n\nBeyond the bounds of the European Union, EU energy policy has included negotiating and developing wider international agreements, such as the Energy Charter Treaty, the Kyoto Protocol, the post-Kyoto regime and a framework agreement on energy efficiency; extension of the EC energy regulatory framework or principles to neighbours (Energy Community, Baku Initiative, Euro-Med energy cooperation) and the emission trading scheme to global partners; the promotion of research and the use of renewable energy.\n\nThe EU-Russia energy cooperation will be based on a new comprehensive framework agreement within the post-Partnership and Cooperation Agreement (PCA), which will be negotiated in 2007. The energy cooperation with other third energy producer and transit countries is facilitated with different tools, such as the PCAs, the existing and foreseen Memorandums of Understanding on Energy Cooperation (with Ukraine, Azerbaijan, Kazakhstan and Algeria), the Association Agreements with Mediterranean countries, the European Neighbourhood Policy Action Plans; Euromed energy cooperation; the Baku initiative; and the EU-Norway energy dialogue. For the cooperation with African countries, a comprehensive Africa-Europe Energy partnership would be launched at the highest level, with the integration of Europe's Energy and Development Policies.\n\nFor ensuring efficient follow-up and coherence in pursuing the initiatives and processes, for sharing information in case of an external energy crisis, and for assisting the EU's early response and reactions in case of energy security threats, the network of energy correspondents in the Member States was established in early 2007. After the Russian-Ukrainian Gas Crisis of 2009 the EU decided that the existing external measures regarding gas supply security should be supplemented by internal provisions for emergency prevention and response, such as enhancing gas storage and network capacity or the development of the technical prerequisites for reverse flow in transit pipelines.\n\nDocuments leaked in late-2016 reveal that a confidential European Union impact assessment analyzes four scenarios for paring back the 'priority dispatch' system afforded to renewable generation in many countries. Priority dispatch is mandated under the Renewable Energy Directive 2009/28/EC which expires in 2020. The assessment concludes that removing priority dispatch could increase carbon emissions by 45million to 60million tonnes per annum or up to 10%, with the aim of making European energy generators more flexible and cost-competitive.\n\nIn 2013, a two-year investigation by the European Commission concluded that Chinese solar panel exporters were selling their products in Europe up to 88% below market prices, backed by state subsidies. In response, the European Council imposed tariffs on solar imported from China at an average rate of 47.6% beginning the 6th June that year. \n\nThe Commission reviewed these measures in December 2016 and proposed to extend them for two years until March 2019. However, in January 2017, 18 out of 28 EU member states voted in favor of shortening the extension period. In February 2017, the Commission announced its intention to extend its anti-dumping measures for a reduced period of 18 months.\n\nThe European Union is also active in the areas of energy research, development and promotion, via initiatives such as CEPHEUS (ultra-low energy housing), and programs under the umbrella titles of SAVE (energy saving) ALTENER (new and renewable energy sources), STEER (transport) and COOPENER (developing countries). Through Fusion for Energy, the EU is participating in the ITER project.\n\nIn a poll carried out for the European Commission in October and November 2005, 47% of the citizens questioned in the 27 countries of the EU (including the 2 states that joined in 2007) were in favour of taking decisions on key energy policy issues at a European level. 37% favoured national decisions and 8% that they be tackled locally.\n\nA similar survey of 29,220 people in March and May 2006 indicated that the balance had changed in favour of national decisions in these areas (42% in favour), with 39% backing EU policy making and 12% preferring local decisions. There was significant national variation with this, with 55% in favour of European level decision making in the Netherlands, but only 15% in Finland.\n\nA comprehensive public opinion survey was performed in May and June 2006. The authors propose following conclusions:\n\n\n\n\n"}
{"id": "2446430", "url": "https://en.wikipedia.org/wiki?curid=2446430", "title": "Eno (drug)", "text": "Eno (drug)\n\nEno is an over-the-counter antacid brand, produced by GlaxoSmithKline. Its main ingredients are sodium bicarbonate, and citric acid; it is sometimes used for cooking, too. \n\nEno was first marketed by James Crossley Eno (1827–1915). Legend says that his idea for the product arose while he was working at the pharmacy of an infirmary in Newcastle where Dennis Embleton worked, and that Embleton often prescribed an effervescent drink made by mixing sodium bicarbonate and citric acid in water, and that Eno adopted this beverage from Embleton. However, Eno opened a pharmacy where he made the mixture in 1852, a year before Embleton came to work at the infirmary, and such fruit salt mixtures were common at the time. \n\nEno gave away his branded mixture to sea captains at the port, and in this way Eno's became a brand known around the world. By 1865 he had to move to a bigger facility, and he formally founded the company Eno's \"Fruit Salt\" Works in 1868. In 1878 Eno moved the business to Hatcham where the factory employed 50 people by 1884.\n\nEno was advertised heavily, as all patent medicines were at that time. In 1883 it was advertised as a cure for cholera and in 1892 for \"keeping blood pure and free from disease\", prevention of diarrhea, and many other conditions. By 1928 the company had factories in England, Canada, France, Germany, Spain, and the US; that year the company was acquired by International Proprietaries, a company that had been established by Canadian businessman Harold F. Ritchie. International Proprietaries was eventually renamed Eno, and in 1938 the business was bought by Beecham for its products as well as its international marketing force. As the pharmaceutical industry transitioned from selling cure-all patent medicines to selling drugs in the 1950s, Eno was one of a handful of products that were retained in the industry.\n"}
{"id": "17481271", "url": "https://en.wikipedia.org/wiki?curid=17481271", "title": "Fluorine", "text": "Fluorine\n\nFluorine is a chemical element with symbol F and atomic number 9. It is the lightest halogen and exists as a highly toxic pale yellow diatomic gas at standard conditions. As the most electronegative element, it is extremely reactive, as it reacts with almost all other elements, except for helium and neon.\n\nAmong the elements, fluorine ranks 24th in universal abundance and 13th in terrestrial abundance. Fluorite, the primary mineral source of fluorine which gave the element its name, was first described in 1529; as it was added to metal ores to lower their melting points for smelting, the Latin verb \"\" meaning \"flow\" gave the mineral its name. Proposed as an element in 1810, fluorine proved difficult and dangerous to separate from its compounds, and several early experimenters died or sustained injuries from their attempts. Only in 1886 did French chemist Henri Moissan isolate elemental fluorine using low-temperature electrolysis, a process still employed for modern production. Industrial production of fluorine gas for uranium enrichment, its largest application, began during the Manhattan Project in World War II.\n\nOwing to the expense of refining pure fluorine, most commercial applications use fluorine compounds, with about half of mined fluorite used in steelmaking. The rest of the fluorite is converted into corrosive hydrogen fluoride en route to various organic fluorides, or into cryolite which plays a key role in aluminium refining. Organic fluorides have very high chemical and thermal stability; their major uses are as refrigerants, electrical insulation and cookware, the last as PTFE (Teflon). Pharmaceuticals such as atorvastatin and fluoxetine also contain fluorine, and the fluoride ion inhibits dental cavities, and so finds use in toothpaste and water fluoridation. Global fluorochemical sales amount to more than US$15 billion a year.\n\nFluorocarbon gases are generally greenhouse gases with global-warming potentials 100 to 20,000 times that of carbon dioxide. Organofluorine compounds persist in the environment due to the strength of the carbon–fluorine bond. Fluorine has no known metabolic role in mammals; a few plants synthesize organofluorine poisons that deter herbivores.\n\nFluorine atoms have nine electrons, one fewer than neon, and electron configuration 1s2s2p: two electrons in a filled inner shell and seven in an outer shell requiring one more to be filled. The outer electrons are ineffective at nuclear shielding, and experience a high effective nuclear charge of 9 − 2 = 7; this affects the atom's physical properties.\n\nFluorine's first ionization energy is third-highest among all elements, behind helium and neon, which complicates the removal of electrons from neutral fluorine atoms. It also has a high electron affinity, second only to chlorine, and tends to capture an electron to become isoelectronic with the noble gas neon; it has the highest electronegativity of any element. Fluorine atoms have a small covalent radius of around 60 picometers, similar to those of its period neighbors oxygen and neon.\n\nThe bond energy of difluorine is much lower than that of either or and similar to the easily cleaved peroxide bond; this, along with high electronegativity, accounts for fluorine's easy dissociation, high reactivity, and strong bonds to non-fluorine atoms. Conversely, bonds to other atoms are very strong because of fluorine's high electronegativity. Unreactive substances like powdered steel, glass fragments, and asbestos fibers react quickly with cold fluorine gas; wood and water spontaneously combust under a fluorine jet.\n\nReactions of elemental fluorine with metals require varying conditions. Alkali metals cause explosions and alkaline earth metals display vigorous activity in bulk; to prevent passivation from the formation of metal fluoride layers, most other metals such as aluminium and iron must be powdered, and noble metals require pure fluorine gas at 300–450 °C (575–850 °F). Some solid nonmetals (sulfur, phosphorus) react vigorously in liquid air temperature fluorine. Hydrogen sulfide and sulfur dioxide combine readily with fluorine, the latter sometimes explosively; sulfuric acid exhibits much less activity, requiring elevated temperatures.\n\nHydrogen, like some of the alkali metals, reacts explosively with fluorine. Carbon, as lamp black, reacts at room temperature to yield fluoromethane. Graphite combines with fluorine above 400 °C (750 °F) to produce non-stoichiometric carbon monofluoride; higher temperatures generate gaseous fluorocarbons, sometimes with explosions. Carbon dioxide and carbon monoxide react at or just above room temperature, whereas paraffins and other organic chemicals generate strong reactions: even fully substituted haloalkanes such as carbon tetrachloride, normally incombustible, may explode. Although nitrogen trifluoride is stable, nitrogen requires an electric discharge at elevated temperatures for reaction with fluorine to occur, due to the very strong triple bond in elemental nitrogen; ammonia may react explosively. Oxygen does not combine with fluorine under ambient conditions, but can be made to react using electric discharge at low temperatures and pressures; the products tend to disintegrate into their constituent elements when heated. Heavier halogens react readily with fluorine as does the noble gas radon; of the other noble gases, only xenon and krypton react, and only under special conditions.\n\nAt room temperature, fluorine is a gas of diatomic molecules, pale yellow when pure (sometimes described as yellow-green). It has a characteristic pungent odor detectable at 20 ppb. Fluorine condenses into a bright yellow liquid at −188 °C (−306 °F), a transition temperature similar to those of oxygen and nitrogen.\n\nFluorine has two solid forms, α- and β-fluorine. The latter crystallizes at −220 °C (−364 °F) and is transparent and soft, with the same disordered cubic structure of freshly crystallized solid oxygen, unlike the orthorhombic systems of other solid halogens. Further cooling to −228 °C (−378 °F) induces a phase transition into opaque and hard α-fluorine, which has a monoclinic structure with dense, angled layers of molecules. The transition from β- to α-fluorine is more exothermic than the condensation of fluorine, and can be violent.\n\nOnly one isotope of fluorine occurs naturally in abundance, the stable isotope . It has a high magnetogyric ratio and exceptional sensitivity to magnetic fields; because it is also the only stable isotope, it is used in magnetic resonance imaging. Seventeen radioisotopes with mass numbers from 14 to 31 have been synthesized, of which is the most stable with a half-life of 109.77 minutes. Other radioisotopes have half-lives less than 70 seconds; most decay in less than half a second. The isotopes and undergo β decay and electron capture, lighter isotopes decay by proton emission, and those heavier than undergo β decay (the heaviest ones with delayed neutron emission). Two metastable isomers of fluorine are known, , with a half-life of 162(7) nanoseconds, and , with a half-life of 2.2(1) milliseconds.\n\nAmong the lighter elements, fluorine's abundance value of 400 ppb (parts per billion) – 24th among elements in the universe – is exceptionally low: other elements from carbon to magnesium are twenty or more times as common. This is because stellar nucleosynthesis processes bypass fluorine, and any fluorine atoms otherwise created have high nuclear cross sections, allowing further fusion with hydrogen or helium to generate oxygen or neon respectively.\n\nBeyond this transient existence, three explanations have been proposed for the presence of fluorine:\n\nFluorine is the thirteenth most common element in Earth's crust at 600–700 ppm (parts per million) by mass. Elemental fluorine in Earth's atmosphere would easily react with atmospheric water vapor, precluding its natural occurrence; it is found only in combined mineral forms, of which fluorite, fluorapatite and cryolite are the most industrially significant. Fluorite or fluorspar (), colorful and abundant worldwide, is fluorine's main source; China and Mexico are the major suppliers. The U.S. led extraction in the early 20th century but ceased mining in 1995. Although fluorapatite (Ca(PO)F) contains most of the world's fluorine, its low mass fraction of 3.5% means that most of it is used as a phosphate. In the U.S. small quantities of fluorine compounds are obtained via fluorosilicic acid, a phosphate industry byproduct. Cryolite (), once used directly in aluminium production, is the rarest and most concentrated of these three minerals. The main commercial mine on Greenland's west coast closed in 1987, and most cryolite is now synthesized.\n\nOther minerals such as topaz contain fluorine. Fluorides, unlike other halides, are insoluble and do not occur in commercially favorable concentrations in saline waters. Trace quantities of organofluorines of uncertain origin have been detected in volcanic eruptions and geothermal springs. The existence of gaseous fluorine in crystals, suggested by the smell of crushed antozonite, is contentious; a 2012 study reported the presence of 0.04% by weight in antozonite, attributing these inclusions to radiation from the presence of tiny amounts of uranium.\n\nIn 1529, Georgius Agricola described fluorite as an additive used to lower the melting point of metals during smelting. He penned the Latin word \"fluorés\" (\"fluor,\" flow) for fluorite rocks. The name later evolved into \"fluorspar\" (still commonly used) and then \"fluorite\". The composition of fluorite was later determined to be calcium difluoride.\n\nHydrofluoric acid was used in glass etching from 1720 onwards. Andreas Sigismund Marggraf first characterized it in 1764 when he heated fluorite with sulfuric acid, and the resulting solution corroded its glass container. Swedish chemist Carl Wilhelm Scheele repeated the experiment in 1771, and named the acidic product \"fluss-spats-syran\" (fluorspar acid). In 1810, the French physicist André-Marie Ampère suggested that hydrogen and an element analogous to chlorine constituted hydrofluoric acid. Sir Humphry Davy proposed that this then-unknown substance be named \"fluorine\" from fluoric acid and the \"-ine\" suffix of other halogens. This word, with modifications, is used in most European languages; Greek, Russian, and some others (following Ampère's suggestion) use the name \"ftor\" or derivatives, from the Greek φθόριος (\"phthorios\", destructive). The New Latin name \"fluorum\" gave the element its current symbol F; Fl was used in early papers.\n\nInitial studies on fluorine were so dangerous that several 19th-century experimenters were deemed \"fluorine martyrs\" after misfortunes with hydrofluoric acid. Isolation of elemental fluorine was hindered by the extreme corrosiveness of both elemental fluorine itself and hydrogen fluoride, as well as the lack of a simple and suitable electrolyte. Edmond Frémy postulated that electrolysis of pure hydrogen fluoride to generate fluorine was feasible and devised a method to produce anhydrous samples from acidified potassium bifluoride; instead, he discovered that the resulting (dry) hydrogen fluoride did not conduct electricity. Frémy's former student Henri Moissan persevered, and after much trial and error found that a mixture of potassium bifluoride and dry hydrogen fluoride was a conductor, enabling electrolysis. To prevent rapid corrosion of the platinum in his electrochemical cells, he cooled the reaction to extremely low temperatures in a special bath and forged cells from a more resistant mixture of platinum and iridium, and used fluorite stoppers. In 1886, after 74 years of effort by many chemists, Moissan isolated elemental fluorine.\n\nIn 1906, two months before his death, Moissan received the Nobel Prize in Chemistry, with the following citation:\n\nThe Frigidaire division of General Motors (GM) experimented with chlorofluorocarbon refrigerants in the late 1920s, and Kinetic Chemicals was formed as a joint venture between GM and DuPont in 1930 hoping to market Freon-12 () as one such refrigerant. It replaced earlier and more toxic compounds, increased demand for kitchen refrigerators, and became profitable; by 1949 DuPont had bought out Kinetic and marketed several other Freon compounds. Polytetrafluoroethylene (Teflon) was serendipitously discovered in 1938 by Roy J. Plunkett while working on refrigerants at Kinetic, and its superlative chemical and thermal resistance lent it to accelerated commercialization and mass production by 1941.\n\nLarge-scale production of elemental fluorine began during World War II. Germany used high-temperature electrolysis to make tons of the planned incendiary chlorine trifluoride and the Manhattan Project used huge quantities to produce uranium hexafluoride for uranium enrichment. Since is as corrosive as fluorine, gaseous diffusion plants required special materials: nickel for membranes, fluoropolymers for seals, and liquid fluorocarbons as coolants and lubricants. This burgeoning nuclear industry later drove post-war fluorochemical development.\n\nFluorine has a rich chemistry, encompassing organic and inorganic domains. It combines with metals, nonmetals, metalloids, and most noble gases, and usually assumes an oxidation state of −1. Fluorine's high electron affinity results in a preference for ionic bonding; when it forms covalent bonds, these are polar, and almost always single.\n\nAlkali metals form ionic and highly soluble monofluorides; these have the cubic arrangement of sodium chloride and analogous chlorides. Alkaline earth difluorides possess strong ionic bonds but are insoluble in water, with the exception of beryllium difluoride, which also exhibits some covalent character and has a quartz-like structure. Rare earth elements and many other metals form mostly ionic trifluorides.\n\nCovalent bonding first comes to prominence in the tetrafluorides: those of zirconium, hafnium and several actinides are ionic with high melting points, while those of titanium, vanadium, and niobium are polymeric, melting or decomposing at no more than 350 °C (660 °F). Pentafluorides continue this trend with their linear polymers and oligomeric complexes. Thirteen metal hexafluorides are known, all octahedral, and are mostly volatile solids but for liquid and , and gaseous . Rhenium heptafluoride, the only characterized metal heptafluoride, is a low-melting molecular solid with pentagonal bipyramidal molecular geometry. Metal fluorides with more fluorine atoms are particularly reactive.\n\nHydrogen and fluorine combine to yield hydrogen fluoride, in which discrete molecules form clusters by hydrogen bonding, resembling water more than hydrogen chloride. It boils at a much higher temperature than heavier hydrogen halides and unlike them is fully miscible with water. Hydrogen fluoride readily hydrates on contact with water to form aqueous hydrogen fluoride, also known as hydrofluoric acid. Unlike the other hydrohalic acids, which are strong, hydrofluoric acid is a weak acid at low concentrations. However, it can attack glass, something the other acids cannot do.\n\nBinary fluorides of metalloids and p-block nonmetals are generally covalent and volatile, with varying reactivities. Period 3 and heavier nonmetals can form hypervalent fluorides.\n\nBoron trifluoride is planar and possesses an incomplete octet. It functions as a Lewis acid and combines with Lewis bases like ammonia to form adducts. Carbon tetrafluoride is tetrahedral and inert; its group analogues, silicon and germanium tetrafluoride, are also tetrahedral but behave as Lewis acids. The pnictogens form trifluorides that increase in reactivity and basicity with higher molecular weight, although nitrogen trifluoride resists hydrolysis and is not basic. The pentafluorides of phosphorus, arsenic, and antimony are more reactive than their respective trifluorides, with antimony pentafluoride the strongest neutral Lewis acid known.\n\nChalcogens have diverse fluorides: unstable difluorides have been reported for oxygen (the only known compound with oxygen in an oxidation state of +2), sulfur, and selenium; tetrafluorides and hexafluorides exist for sulfur, selenium, and tellurium. The latter are stabilized by more fluorine atoms and lighter central atoms, so sulfur hexafluoride is especially inert. Chlorine, bromine, and iodine can each form mono-, tri-, and pentafluorides, but only iodine heptafluoride has been characterized among possible interhalogen heptafluorides. Many of them are powerful sources of fluorine atoms, and industrial applications using chlorine trifluoride require precautions similar to those using fluorine.\n\nNoble gases, having complete electron shells, defied reaction with other elements until 1962 when Neil Bartlett reported synthesis of xenon hexafluoroplatinate; xenon difluoride, tetrafluoride, hexafluoride, and multiple oxyfluorides have been isolated since then. Among other noble gases, krypton forms a difluoride, and radon and fluorine generate a solid suspected to be radon difluoride. Binary fluorides of lighter noble gases are exceptionally unstable: argon and hydrogen fluoride combine under extreme conditions to give argon fluorohydride. Helium and neon have no long-lived fluorides, and no neon fluoride has ever been observed; helium fluorohydride has been detected for milliseconds at high pressures and low temperatures.\n\nThe carbon–fluorine bond is organic chemistry's strongest, and gives stability to organofluorines. It is almost non-existent in nature, but is used in artificial compounds. Research in this area is usually driven by commercial applications; the compounds involved are diverse and reflect the complexity inherent in organic chemistry.\n\nThe substitution of hydrogen atoms in an alkane by progressively more fluorine atoms gradually alters several properties: melting and boiling points are lowered, density increases, solubility in hydrocarbons decreases and overall stability increases. Perfluorocarbons, in which all hydrogen atoms are substituted, are insoluble in most organic solvents, reacting at ambient conditions only with sodium in liquid ammonia.\n\nThe term \"perfluorinated compound\" is used for what would otherwise be a perfluorocarbon if not for the presence of a functional group, often a carboxylic acid. These compounds share many properties with perfluorocarbons such as stability and hydrophobicity, while the functional group augments their reactivity, enabling them to adhere to surfaces or act as surfactants; Fluorosurfactants, in particular, can lower the surface tension of water more than their hydrocarbon-based analogues. Fluorotelomers, which have some unfluorinated carbon atoms near the functional group, are also regarded as perfluorinated.\n\nPolymers exhibit the same stability increases afforded by fluorine substitution (for hydrogen) in discrete molecules; their melting points generally increase too. Polytetrafluoroethylene (PTFE), the simplest fluoropolymer and perfluoro analogue of polyethylene with structural unit ––, demonstrates this change as expected, but its very high melting point makes it difficult to mold. Various PTFE derivatives are less temperature-tolerant but easier to mold: fluorinated ethylene propylene replaces some fluorine atoms with trifluoromethyl groups, perfluoroalkoxy alkanes do the same with trifluoromethoxy groups, and Nafion contains perfluoroether side chains capped with sulfonic acid groups. Other fluoropolymers retain some hydrogen atoms; polyvinylidene fluoride has half the fluorine atoms of PTFE and polyvinyl fluoride has a quarter, but both behave much like perfluorinated polymers.\n\nMoissan's method is used to produce industrial quantities of fluorine, via the electrolysis of a potassium fluoride/hydrogen fluoride mixture: hydrogen and fluoride ions are reduced and oxidized at a steel container cathode and a carbon block anode, under 8–12 volts, to generate hydrogen and fluorine gas respectively. Temperatures are elevated, KF•2HF melting at and being electrolyzed at . KF, which acts as catalyst, is essential since pure HF cannot be electrolyzed. Fluorine can be stored in steel cylinders that have passivated interiors, at temperatures below ; otherwise nickel can be used. Regulator valves and pipework are made of nickel, the latter possibly using Monel instead. Frequent passivation, along with the strict exclusion of water and greases, must be undertaken. In the laboratory, glassware may carry fluorine gas under low pressure and anhydrous conditions; some sources instead recommend nickel-Monel-PTFE systems.\n\nWhile preparing for a 1986 conference to celebrate the centennial of Moissan's achievement, Karl O. Christe reasoned that chemical fluorine generation should be feasible since some metal fluoride anions have no stable neutral counterparts; their acidification potentially triggers oxidation instead. He devised a method which evolves fluorine at high yield and atmospheric pressure:\n\nChriste later commented that the reactants \"had been known for more than 100 years and even Moissan could have come up with this scheme.\" As late as 2008, some references still asserted that fluorine was too reactive for any chemical isolation.\n\nFluorite mining, which supplies most global fluorine, peaked in 1989 when 5.6 million metric tons of ore were extracted. Chlorofluorocarbon restrictions lowered this to 3.6 million tons in 1994; production has since been increasing. Around 4.5 million tons of ore and revenue of US$550 million were generated in 2003; later reports estimated 2011 global fluorochemical sales at $15 billion and predicted 2016–18 production figures of 3.5 to 5.9 million tons, and revenue of at least $20 billion. Froth flotation separates mined fluorite into two main metallurgical grades of equal proportion: 60–85% pure metspar is almost all used in iron smelting whereas 97%+ pure acidspar is mainly converted to the key industrial intermediate hydrogen fluoride.\n\nAt least 17,000 metric tons of fluorine are produced each year. It costs only $5–8 per kilogram as uranium or sulfur hexafluoride, but many times more as an element because of handling challenges. Most processes using free fluorine in large amounts employ \"in situ\" generation under vertical integration.\n\nThe largest application of fluorine gas, consuming up to 7,000 metric tons annually, is in the preparation of for the nuclear fuel cycle. Fluorine is used to fluorinate uranium tetrafluoride, itself formed from uranium dioxide and hydrofluoric acid. Fluorine is monoisotopic, so any mass differences between molecules are due to the presence of or , enabling uranium enrichment via gaseous diffusion or gas centrifuge. About 6,000 metric tons per year go into producing the inert dielectric for high-voltage transformers and circuit breakers, eliminating the need for hazardous polychlorinated biphenyls associated with devices. Several fluorine compounds are used in electronics: rhenium and tungsten hexafluoride in chemical vapor deposition, tetrafluoromethane in plasma etching and nitrogen trifluoride in cleaning equipment. Fluorine is also used in the synthesis of organic fluorides, but its reactivity often necessitates conversion first to the gentler , , or , which together allow calibrated fluorination. Fluorinated pharmaceuticals use sulfur tetrafluoride instead.\n\nAs with other iron alloys, around 3 kg (6.5 lb) metspar is added to each metric ton of steel; the fluoride ions lower its melting point and viscosity. Alongside its role as an additive in materials like enamels and welding rod coats, most acidspar is reacted with sulfuric acid to form hydrofluoric acid, which is used in steel pickling, glass etching and alkane cracking. One-third of HF goes into synthesizing cryolite and aluminium trifluoride, both fluxes in the Hall–Héroult process for aluminium extraction; replenishment is necessitated by their occasional reactions with the smelting apparatus. Each metric ton of aluminium requires about 23 kg (51 lb) of flux. Fluorosilicates consume the second largest portion, with sodium fluorosilicate used in water fluoridation and laundry effluent treatment, and as an intermediate en route to cryolite and silicon tetrafluoride. Other important inorganic fluorides include those of cobalt, nickel, and ammonium.\n\nOrganofluorides consume over 20% of mined fluorite and over 40% of hydrofluoric acid, with refrigerant gases dominating and fluoropolymers increasing their market share. Surfactants are a minor application but generate over $1 billion in annual revenue. Due to the danger from direct hydrocarbon–fluorine reactions above −150 °C (−240 °F), industrial fluorocarbon production is indirect, mostly through halogen exchange reactions such as Swarts fluorination, in which chlorocarbon chlorines are substituted for fluorines by hydrogen fluoride under catalysts. Electrochemical fluorination subjects hydrocarbons to electrolysis in hydrogen fluoride, and the Fowler process treats them with solid fluorine carriers like cobalt trifluoride.\n\nHalogenated refrigerants, termed Freons in informal contexts, are identified by R-numbers that denote the amount of fluorine, chlorine, carbon, and hydrogen present. Chlorofluorocarbons (CFCs) like R-11, R-12, and R-114 once dominated organofluorines, peaking in production in the 1980s. Used for air conditioning systems, propellants and solvents, their production was below one-tenth of this peak by the early 2000s, after widespread international prohibition. Hydrochlorofluorocarbons (HCFCs) and hydrofluorocarbons (HFCs) were designed as replacements; their synthesis consumes more than 90% of the fluorine in the organic industry. Important HCFCs include R-22, chlorodifluoromethane, and R-141b. The main HFC is R-134a with a new type of molecule HFO-1234yf, a Hydrofluoroolefin (HFO) coming to prominence owing to its global warming potential of less than 1% that of HFC-134a.\n\nAbout 180,000 metric tons of fluoropolymers were produced in 2006 and 2007, generating over $3.5 billion revenue per year. The global market was estimated at just under $6 billion in 2011 and was predicted to grow by 6.5% per year up to 2016. Fluoropolymers can only be formed by polymerizing free radicals.\n\nPolytetrafluoroethylene (PTFE), sometimes called by its DuPont name Teflon, represents 60–80% by mass of the world's fluoropolymer production. The largest application is in electrical insulation since PTFE is an excellent dielectric. It is also used in the chemical industry where corrosion resistance is needed, in coating pipes, tubing, and gaskets. Another major use is in PFTE-coated fiberglass cloth for stadium roofs. The major consumer application is for non-stick cookware. Jerked PTFE film becomes expanded PTFE (ePTFE), a fine-pored membrane sometimes referred to by the brand name Gore-Tex and used for rainwear, protective apparel, and filters; ePTFE fibers may be made into seals and dust filters. Other fluoropolymers, including fluorinated ethylene propylene, mimic PTFE's properties and can substitute for it; they are more moldable, but also more costly and have lower thermal stability. Films from two different fluoropolymers replace glass in solar cells.\n\nThe chemically resistant (but expensive) fluorinated ionomers are used as electrochemical cell membranes, of which the first and most prominent example is Nafion. Developed in the 1960s, it was initially deployed as fuel cell material in spacecraft and then replaced mercury-based chloralkali process cells. Recently, the fuel cell application has reemerged with efforts to install proton exchange membrane fuel cells into automobiles. Fluoroelastomers such as Viton are crosslinked fluoropolymer mixtures mainly used in O-rings; perfluorobutane (CF) is used as a fire-extinguishing agent.\n\nFluorosurfactants are small organofluorine molecules used for repelling water and stains. Although expensive (comparable to pharmaceuticals at $200–2000 per kilogram), they yielded over $1 billion in annual revenues by 2006; Scotchgard alone generated over $300 million in 2000. Fluorosurfactants are a minority in the overall surfactant market, most of which is taken up by much cheaper hydrocarbon-based products. Applications in paints are burdened by compounding costs; this use was valued at only $100 million in 2006.\n\nAbout 30% of agrichemicals contain fluorine, most of them herbicides and fungicides with a few crop regulators. Fluorine substitution, usually of a single atom or at most a trifluoromethyl group, is a robust modification with effects analogous to fluorinated pharmaceuticals: increased biological stay time, membrane crossing, and altering of molecular recognition. Trifluralin is a prominent example, with large-scale use in the U.S. as a weedkiller, but it is a suspected carcinogen and has been banned in many European countries. Sodium monofluoroacetate (1080) is a mammalian poison in which two acetic acid hydrogens are replaced with fluorine and sodium; it disrupts cell metabolism by replacing acetate in the citric acid cycle. First synthesized in the late 19th century, it was recognized as an insecticide in the early 20th, and was later deployed in its current use. New Zealand, the largest consumer of 1080, uses it to protect kiwis from the invasive Australian common brushtail possum. Europe and the U.S. have banned 1080.\n\nPopulation studies from the mid-20th century onwards show topical fluoride reduces dental caries. This was first attributed to the conversion of tooth enamel hydroxyapatite into the more durable fluorapatite, but studies on pre-fluoridated teeth refuted this hypothesis, and current theories involve fluoride aiding enamel growth in small caries. After studies of children in areas where fluoride was naturally present in drinking water, controlled public water supply fluoridation to fight tooth decay began in the 1940s and is now applied to water supplying 6 percent of the global population, including two-thirds of Americans. Reviews of the scholarly literature in 2000 and 2007 associated water fluoridation with a significant reduction of tooth decay in children. Despite such endorsements and evidence of no adverse effects other than mostly benign dental fluorosis, opposition still exists on ethical and safety grounds. The benefits of fluoridation have lessened, possibly due to other fluoride sources, but are still measurable in low-income groups. Sodium monofluorophosphate and sometimes sodium or tin(II) fluoride are often found in fluoride toothpastes, first introduced in the U.S. in 1955 and now ubiquitous in developed countries, alongside fluoridated mouthwashes, gels, foams, and varnishes.\n\nTwenty percent of modern pharmaceuticals contain fluorine. One of these, the cholesterol-reducer atorvastatin (Lipitor), made more revenue than any other drug until it became generic in 2011. The combination asthma prescription Seretide, a top-ten revenue drug in the mid-2000s, contains two active ingredients, one of which – fluticasone – is fluorinated. Many drugs are fluorinated to delay inactivation and lengthen dosage periods because the carbon–fluorine bond is very stable. Fluorination also increases lipophilicity because the bond is more hydrophobic than the carbon–hydrogen bond, and this often helps in cell membrane penetration and hence bioavailability.\n\nTricyclics and other pre-1980s antidepressants had several side effects due to their non-selective interference with neurotransmitters other than the serotonin target; the fluorinated fluoxetine was selective and one of the first to avoid this problem. Many current antidepressants receive this same treatment, including the selective serotonin reuptake inhibitors: citalopram, its isomer escitalopram, and fluvoxamine and paroxetine. Quinolones are artificial broad-spectrum antibiotics that are often fluorinated to enhance their effects. These include ciprofloxacin and levofloxacin. Fluorine also finds use in steroids: fludrocortisone is a blood pressure-raising mineralocorticoid, and triamcinolone and dexamethasone are strong glucocorticoids. The majority of inhaled anesthetics are heavily fluorinated; the prototype halothane is much more inert and potent than its contemporaries. Later compounds such as the fluorinated ethers sevoflurane and desflurane are better than halothane and are almost insoluble in blood, allowing faster waking times.\n\nFluorine-18 is often found in radioactive tracers for positron emission tomography, as its half-life of almost two hours is long enough to allow for its transport from production facilities to imaging centers. The most common tracer is fluorodeoxyglucose which, after intravenous injection, is taken up by glucose-requiring tissues such as the brain and most malignant tumors; computer-assisted tomography can then be used for detailed imaging.\n\nLiquid fluorocarbons can hold large volumes of oxygen or carbon dioxide, more so than blood, and have attracted attention for their possible uses in artificial blood and in liquid breathing. Because fluorocarbons do not normally mix with water, they must be mixed into emulsions (small droplets of perfluorocarbon suspended in water) to be used as blood. One such product, Oxycyte, has been through initial clinical trials. These substances can aid endurance athletes and are banned from sports; one cyclist's near death in 1998 prompted an investigation into their abuse. Applications of pure perfluorocarbon liquid breathing (which uses pure perfluorocarbon liquid, not a water emulsion) include assisting burn victims and premature babies with deficient lungs. Partial and complete lung filling have been considered, though only the former has had any significant tests in humans. An Alliance Pharmaceuticals effort reached clinical trials but was abandoned because the results were not better than normal therapies.\n\nFluorine is not essential for humans or other mammals; small amounts may be beneficial for bone strength, but this has not been definitively established. As there are many environmental sources of trace fluorine, the possibility of a fluorine deficiency could apply only to artificial diets. Natural organofluorines have been found in microorganisms and plants but not animals. The most common is fluoroacetate, which is used as a defense against herbivores by at least 40 plants in Africa, Australia and Brazil. Other examples include terminally fluorinated fatty acids, fluoroacetone, and 2-fluorocitrate. An enzyme that binds fluorine to carbon – adenosyl-fluoride synthase – was discovered in bacteria in 2002.\n\nElemental fluorine is highly toxic to living organisms. Its effects in humans start at concentrations lower than hydrogen cyanide's 50 ppm and are similar to those of chlorine: significant irritation of the eyes and respiratory system as well as liver and kidney damage occur above 25 ppm, which is the immediately dangerous to life and health value for fluorine. Eyes and noses are seriously damaged at 100 ppm, and inhalation of 1,000 ppm fluorine will cause death in minutes, compared to 270 ppm for hydrogen cyanide.\n\nHydrofluoric acid is a contact poison with greater hazards than many strong acids like sulfuric acid even though it is weak: it remains neutral in aqueous solution and thus penetrates tissue faster, whether through inhalation, ingestion or the skin, and at least nine U.S. workers died in such accidents from 1984 to 1994. It reacts with calcium and magnesium in the blood leading to hypocalcemia and possible death through cardiac arrhythmia. Insoluble calcium fluoride formation triggers strong pain and burns larger than 160 cm (25 in) can cause serious systemic toxicity.\n\nExposure may not be evident for eight hours for 50% HF, rising to 24 hours for lower concentrations, and a burn may initially be painless as hydrogen fluoride affects nerve function. If skin has been exposed to HF, damage can be reduced by rinsing it under a jet of water for 10–15 minutes and removing contaminated clothing. Calcium gluconate is often applied next, providing calcium ions to bind with fluoride; skin burns can be treated with 2.5% calcium gluconate gel or special rinsing solutions. Hydrofluoric acid absorption requires further medical treatment; calcium gluconate may be injected or administered intravenously. Using calcium chloride – a common laboratory reagent – in lieu of calcium gluconate is contraindicated, and may lead to severe complications. Excision or amputation of affected parts may be required.\n\nSoluble fluorides are moderately toxic: 5–10 g sodium fluoride, or 32–64 mg fluoride ions per kilogram of body mass, represents a lethal dose for adults. One-fifth of the lethal dose can cause adverse health effects, and chronic excess consumption may lead to skeletal fluorosis, which affects millions in Asia and Africa. Ingested fluoride forms hydrofluoric acid in the stomach which is easily absorbed by the intestines, where it crosses cell membranes, binds with calcium and interferes with various enzymes, before urinary excretion. Exposure limits are determined by urine testing of the body's ability to clear fluoride ions.\n\nHistorically, most cases of fluoride poisoning have been caused by accidental ingestion of insecticides containing inorganic fluorides. Most current calls to poison control centers for possible fluoride poisoning come from the ingestion of fluoride-containing toothpaste. Malfunctioning water fluoridation equipment is another cause: one incident in Alaska affected almost 300 people and killed one person. Dangers from toothpaste are aggravated for small children, and the Centers for Disease Control and Prevention recommends supervising children below six brushing their teeth so that they do not swallow toothpaste. One regional study examined a year of pre-teen fluoride poisoning reports totaling 87 cases, including one death from ingesting insecticide. Most had no symptoms, but about 30% had stomach pains. A larger study across the U.S. had similar findings: 80% of cases involved children under six, and there were few serious cases.\n\nThe Montreal Protocol, signed in 1987, set strict regulations on chlorofluorocarbons (CFCs) and bromofluorocarbons due to their ozone damaging potential (ODP). The high stability which suited them to their original applications also meant that they were not decomposing until they reached higher altitudes, where liberated chlorine and bromine atoms attacked ozone molecules. Even with the ban, and early indications of its efficacy, predictions warned that several generations would pass before full recovery. With one-tenth the ODP of CFCs, hydrochlorofluorocarbons (HCFCs) are the current replacements, and are themselves scheduled for substitution by 2030–2040 by hydrofluorocarbons (HFCs) with no chlorine and zero ODP. In 2007 this date was brought forward to 2020 for developed countries; the Environmental Protection Agency had already prohibited one HCFC's production and capped those of two others in 2003. Fluorocarbon gases are generally greenhouse gases with global-warming potentials (GWPs) of about 100 to 10,000; sulfur hexafluoride has a value of around 20,000. An outlier is HFO-1234yf which is a new type of refrigerant called a Hydrofluoroolefin (HFO) and has attracted global demand due to its GWP of 4 compared to 1,430 for the current refrigerant standard HFC-134a.\n\nOrganofluorines exhibit biopersistence due to the strength of the carbon–fluorine bond. Perfluoroalkyl acids (PFAAs), which are sparingly water-soluble owing to their acidic functional groups, are noted persistent organic pollutants; perfluorooctanesulfonic acid (PFOS) and perfluorooctanoic acid (PFOA) are most often researched. PFAAs have been found in trace quantities worldwide from polar bears to humans, with PFOS and PFOA known to reside in breast milk and the blood of newborn babies. A 2013 review showed a slight correlation between groundwater and soil PFAA levels and human activity; there was no clear pattern of one chemical dominating, and higher amounts of PFOS were correlated to higher amounts of PFOA. In the body, PFAAs bind to proteins such as serum albumin; they tend to concentrate within humans in the liver and blood before excretion through the kidneys. Dwell time in the body varies greatly by species, with half-lives of days in rodents, and years in humans. High doses of PFOS and PFOA cause cancer and death in newborn rodents but human studies have not established an effect at current exposure levels.\n\n"}
{"id": "8948181", "url": "https://en.wikipedia.org/wiki?curid=8948181", "title": "Garter spring", "text": "Garter spring\n\nA garter spring is a coiled steel spring that is connected at each end to create a circular shape, and is used in oil seals, shaft seals, belt-driven motors, and electrical connectors. Compression garter springs exert outward radial forces, while extension garter springs exert inward radial forces. The manufacturing process is not much different from the creation of regular coiled springs, with the addition of joining the ends together. Like most other springs, garter springs are typically manufactured with either carbon steel or stainless steel wire.\n\nCompression garter springs are a type of coiled spring that exerts outward radial forces away from the center. They are typically made up of a thick steel wire with large coils; compression springs need to be able to handle very large loads while being able to return to their natural extended position. Compression springs store potential energy when they are compressed (length of spring decreases), and exert kinetic energy when released. Compression garter springs use this principle to withstand forces acting on it from outside. They may be placed inside a circular object to maintain the object's circular shape. This is similar to squeezing a rubber ball; the ball will contract when squeezed but will return to its natural state once the external pressure is released.\n\nExtension garter springs are on the opposite side of the spring spectrum. Although they are also a type of coiled spring, extension garter springs exert inward radial forces that move toward the center. Extension springs store potential energy in their extended form and want to contract. Thinner wire and a greater number of coils allow extension springs to be able to contract quickly, which is essential when dealing with pressurized fluids and gases. Extension garter springs act against forces from the center, so they may be placed on the outside of a circular object to maintain the object's circular shape. They act similar to a bracelet, which is extended to fit around the hand and then snaps back into shape on the wrist. Extension garter springs are more common than compression garter springs because they use less material (smaller circumference and thinner wire) and they respond to changes quicker and more efficiently.\n\nThere are four main stages for the production of steel garter springs. The first step is to cut and coil reels of steel wire to produce normal coiled springs. The strength of the spring is proportional to the thickness of the wire. Compression springs are coiled in such a way that the coils are more spaced apart, while extension springs have no space between the coils.\n\nThe second step is to join each end of the spring to form the garter spring's unique circular shape. This can be accomplished through a few different ways:\nThe third stage is heat treating, which prevents the spring from being too brittle to function. Heat treating involves placing the spring in an oven at high temperature for a predetermined amount of time, and then letting it cool slowly.\n\nThe fourth stage is applying the finishing touches to the spring, which may include grinding (flattening the ends of the spring), shot peening (shooting tiny steel balls at the spring to harden the wire further), setting (permanently fixing the length and pitch of the spring), coating (electroplating or applying paint or rubber to the surface to prevent corrosion), and packaging.\n\nCarbon steel wire is typically used for garter springs due to its affordable price and usability, in comparison to stainless steel. Carbon steel springs tend to have very high yield strengths, and are able to return to their original shape when temporarily deformed. The carbon content in carbon steel wires range from 0.50 to 0.95 percent. This relatively small amount of carbon is enough to improve the toughness of the spring. The close proximity to oil and high-pressure engines mean heat treated garter springs are essential for enduring temperatures over 100 °C (212 °F). However, carbon steel is not suitable for highly corrosive environments; stainless steel would be a more viable option. Stainless steel differs from carbon steel in the amount of chromium present; stainless steel has between 10.5% to 11% chromium by mass, while carbon steel has about 1%.\n\nMost garter springs are used for oil seals and shaft seals. Since they are able to withstand forces from all directions, garter springs are effective at handling changes in volume, pressure, temperature, and viscosity.\n\n"}
{"id": "30851140", "url": "https://en.wikipedia.org/wiki?curid=30851140", "title": "Greenhouse gas footprint", "text": "Greenhouse gas footprint\n\nThe Greenhouse gas footprint, or GHG footprint, refers to the amount of greenhouse gases that are emitted during the creation of products or services.\n\nHuman activities are one of the main causes of greenhouse gas. These increase the earth's temperature and are emitted from fossil fuel usage in electricity and other byproducts of manufacturing. The major effects mainly consist of climate changes, such as extreme precipitation and acidification and warming of oceans. Climate change has been occurring since the start of the Industrial Revolution in the 1820s. Due to humans' heavy reliance on fossil fuels, energy usage, and constant deforestation, the amount of greenhouse gas in the atmosphere is increasing, which makes reducing a greenhouse gas footprint harder to achieve. However, there are several ways to reduce one's greenhouse gas footprint, such as using more energy efficient household appliances, increase usage of fuel efficient cars, and saving electricity.\n\nGreenhouse gases (GHGs) are gases that increase the temperature of the Earth due to their absorption of infrared radiation. Although some emissions are natural, the rate of which they are being produced has increased because of humans. These gases are emitted from fossil fuel usage in electricity, in heat and transportation, as well as being emitted as byproducts of manufacturing. The most common GHGs are carbon dioxide (CO), methane (CH), nitrous oxide (NO), and many fluorinated gases. A greenhouse gas footprint is the numerical quantity of these gases that a single entity emits. The calculations can be computed ranging from a single person to the entire world. A GHG footprint can be calculated on the US Environmental Protection Agency’s (EPA) website. After entering data, a number with units of pounds of CO will be given. The EPA use pounds of CO equivalence to determine the greenhouse gas footprint.\n\nAs early as the 1820s the investigation on climate change was in full swing. Joseph Fourier believed that light from the sun can enter the atmosphere, but cannot leave nearly as easily. He sought to prove that air can absorb the infrared radiation and will be given back to the Earth’s surface. Later in 1859, John Tyndall discovered that water vapor and CO trap heat waves given by the sun. In 1896, Svante Arrhenius tried to prove that it would take thousands of years for the industrial production of CO to raise the Earth’s temperature 5-6°C. But this idea was met with much skepticism throughout the early 20th century because it was oversimplified. In the mid 20th century, it was concluded that there was a 10% increase in carbon dioxide in the atmosphere over the 19th century, which resulted in slight warming. It was at this time that people believed the emissions of CO will increase exponentially in the future and the oceans would absorb any surplus of greenhouse gases. In 1956, Gilbert N. Plass concluded that greenhouse gas emissions will have an effect on the Earth’s temperature and argued that dismissing GHG emissions would be a mistake. Soon after, scientists from all sectors began to work together to figure out the mystery of GHG emissions and their effects. As technology advanced, it was in the 1980s that there was proof of a rise in CO levels. An ice core, captured through drilling, provided clear evidence that carbon dioxide levels have risen.\n\nAlthough some production of greenhouse gases is natural, human activity has increased the production substantially. Major industrial sources of greenhouse gasses are power plants, residential buildings, and road transportation, as well as energy industry processes and losses, iron and steel manufacturing, coal mining, and chemical and petrochemical industries. Changes in the environment also contribute the increase in greenhouse gas emission such as, deforestation, forest degradation and land use, livestock, agricultural soils and water, and wastewater. China is the largest contributor of greenhouse gas, causing up 30% of the total emissions. The United States contributes 15%, followed by the EU with 9%, then India with 7%, Russia with 5%, Japan with 4% , and other miscellaneous countries making up the remaining 30%.\n\nAlthough carbon dioxide (CO) is the most prevalent gas, it is not the most damaging. Carbon dioxide is essential to life because animals release it during cellular respiration when they breathe and plants use it for photosynthesis. Carbon dioxide is released naturally by decomposition, ocean release and respiration. Humans contribute an increase of carbon dioxide emissions by burning fossil fuels, deforestation, and cement production.\n\nMethane (CH) is largely released by coal, oil, and natural gas industries. Although methane is not mass produced like carbon dioxide, it is still very prevalent. Methane is more harmful than carbon dioxide because it traps heat better than CO. Methane is a main component in natural gas. Recently industries as well as consumers have been using natural gas because they believe that it is better for the environment since it contains less CO. However, this is not the case because methane is actually more harmful to the environment.\n\nNitrous oxide (NO) is released by fuel combustion, most of which comes from coal fired power plants, agricultural and industrial activities.\n\nFluorinated gases include hydroflucarbons (HFCs), perfluorocarbons (PFCs), sulfur hexafluoride (SF), and nitrogen trifluoride (NF). These gases have no natural source and are solely products of human activity. The biggest cause of these sources is the usage of ozone depleting substances; such as refrigerants, aerosol, propellants, foam blowing agents, solvents, and fire retardants.\n\nThe production of all of these gases contributes to one's GHG footprint. The more that these gases are produced, the higher the GHG footprint.\n\nSince the Industrial Revolution, greenhouse gas emissions have increased immensely. As of 2017, the carbon dioxide (CO) levels are 142%, of what they were pre-industrial revolution. Methane is up 253% and nitrous oxide is 121% of pre-industrial levels. The energy driven consumption of fossil fuels has made GHG emissions rapidly increase, causing the Earth's temperature to rise. In the past 250 years, human activity such as, burning fossil fuels and cutting down carbon-absorbing forests, have contributed greatly to this increase. In the last 25 years alone, emissions have increased by more than 33%, most of which comes from carbon dioxide, accounting for three-fourths of this increase.\n\nDifferent GHGs last different amounts of time in the atmosphere. For example, fluorinated gases can last from a few weeks to a few thousand years in the atmosphere, whereas nitrous oxide can last for over a century. However, methane is somewhere in the middle, lasting a little over a decade. Carbon dioxide's lifespan cannot be calculated exactly because it does not disappear, but is either used by plants or absorbed by the ocean. There is a possibility that some greenhouse gases have been in the atmosphere since the beginning of the twentieth century, when the first signs of an increase of these gases arose.\n\nGreenhouse gases may cause severe and irreversible impacts on people and ecosystems. The warming caused by GHG emissions is effectively irreversible over multi-century time scales. Risks of GHG footprints are evaluated based on the interaction of projected damages in an earth system climate change effected by choice of emission scenarios. A lot of the effects are extreme weather changes, which include, cold winter extremes, increase in annual mean precipitation, and year round reductions in arctic sea ice. There are a lot of key risks that span over sectors and regions. This includes severe ill-health and disrupted livelihoods, breaking down of infrastructure, and risk loss of ecosystems. The disrupted livelihoods and severe ill-health includes storm surges and coastal and inland flooding. The breaking down of infrastructure includes the networks and critical services. There is also a risk of loss of biodiversity, ecosystem goods, functions, and services.\n\nIt is likely that heat waves will occur more often and last longer as a result of climate change and an increase of the carbon cycle. Climate change and the carbon cycle will be amplified because of GHG emissions. Extreme precipitation events will become more intense and frequent. The global mean surface temperature change per trillion tonnes of carbon emitted as CO2 is likely in range of .8 to 2.5 degrees Celsius. This is known as a transient climate response to cumulative carbon emissions (TCRE).\nThe extreme precipitation will cause the oceans to warm and acidify. The southern ocean will increase the most in temperature. The global mean sea levels will rise due to the extreme precipitation. Near-surface permafrost in high northern latitudes will reduce resulting in sea level rise not being uniform across regions. There will be a rise in about 95% of ocean area. The sea level rise depends on the pathway of CO emissions not only the cumulative total. In some regions, the future sea level will be at an extreme by 2100. 70 percent of coastlines are projected to have sea level change within more or less than 20 percent of global mean.\n\nIn order to decrease CO emissions, the reliance of fossil fuels must be lowered. These fuels are producing lots of CO across all forms of their usage. Alternatively, renewable sources are cleaner for the environment. Capturing CO from power plants will also reduce emissions.\n\nHousehold energy conservation measures include increasing insulation in construction, using fuel-efficient vehicles and ENERGY STAR appliances, and unplugging electrical items when not in use.\n\nReducing methane emissions can be accomplished in several ways. Capturing CH emissions from coal mines and landfills, are two ways of reducing these emissions. Manure management and livestock operations is another possible solution. Motor vehicles use fossil fuels, which produces CO2, but fossil fuels also produce CH4 as a byproduct. Thus, better technology for these vehicles to avoid leakage would be very beneficial.\n\nNitrous oxide (NO) is often given off as a byproduct in various ways. Nylon production and fossil fuel usage are two ways that NO is given off as a byproduct. Thus, improving technology for nylon production and the gathering of fossil fuels would greatly reduce nitrous oxide emissions. Also, many fertilizers have a nitrogenous base. A decrease in usage of these fertilizers, or changing their components, are more ways to reduce NO emissions.\n\nAlthough fluorinated gases are not produced on a massive scale, they have the worst effect on the environment. A reduction of fluorinated gas emissions can be done in many ways. Many industries that emit these gases can capture or recycle them. These same industries can also invest in more advanced technology that will not produce these gases. A reduction of leakage within power grids and motor vehicles will also decrease the emissions of fluorinated gases. There are also many air conditioning systems that emit fluorinated gases, thus an update in technology would decrease these emissions.\n\nThere are many simple changes that can be made to the everyday lifestyle of a person that would reduce their GHG footprint. Reducing energy consumption within a household can include lowering one’s dependence on air conditioning and heating, using CFL light bulbs, choosing ENERGY STAR appliances, recycling, using cold water to wash clothes, and avoiding a dryer. Another adjustment would be to use a motor vehicle that is fuel-efficient as well as reducing reliance on motor vehicles. Motor vehicles produce many GHGs, thus an adjustment to one’s usage will greatly affect a GHG footprint.\n\n"}
{"id": "4373936", "url": "https://en.wikipedia.org/wiki?curid=4373936", "title": "Heat deflection temperature", "text": "Heat deflection temperature\n\nThe heat deflection temperature or heat distortion temperature (HDT, HDTUL, or DTUL) is the temperature at which a polymer or plastic sample deforms under a specified load. This property of a given plastic material is applied in many aspects of product design, engineering and manufacture of products using thermoplastic components.\n\nThe heat distortion temperature is determined by the following test procedure outlined in ASTM D648. The test specimen is loaded in three-point bending in the edgewise direction. The outer fiber stress used for testing is either 0.455 MPa or 1.82 MPa, and the temperature is increased at 2 °C/min until the specimen deflects 0.25 mm. This is similar to the test procedure defined in the ISO 75 standard.\n\nLimitations that are associated with the determination of the HDT is that the sample is not thermally isotropic and, in thick samples in particular, will contain a temperature gradient. The HDT of a particular material can also be very sensitive to stress experienced by the component which is dependent on the component’s dimensions. The selected deflection of 0.25 mm (which is 0.2% additional strain) is selected arbitrarily and has no particular physical significance.\n\nAn injection molded plastic part is considered \"safe\" to remove from its mold once it is near or below the HDT. This means that part deformation will be held within acceptable limits after removal. The molding of plastics by necessity occurs at high temperatures (routinely 200 °C or higher) due to the low viscosity of plastics in fluid form (this issue can be addressed to some extent by the addition of plasticizers to the melt). Once plastic is in the mold, it must be cooled to a temperature to which little or no dimensional change will occur after removal. In general, plastics do not conduct heat well and so will take quite a while to cool to room temperature. One way to mitigate this is to use a cold mold (thereby increasing heat loss from the part). Even so, the cooling of the part to room temperature can limit the mass production of parts.\n\nChoosing a resin with a higher heat deflection temperature can allow manufacturers to achieve a much faster molding process than they would otherwise while maintaining dimensional changes within certain limits.\n\n"}
{"id": "29224451", "url": "https://en.wikipedia.org/wiki?curid=29224451", "title": "Herbfield", "text": "Herbfield\n\nHerbfields are plant communities dominated by herbaceous plants, especially forbs and grasses. They are found where climatic conditions do not allow large woody plants to grow, such as in subantarctic and alpine tundra environments. Herbfield is defined in New South Wales (Australia) government legislation as native vegetation that predominantly does not contain an over-storey or a mid-storey and where ground cover is dominated by non-grass species. The New Zealand Department of Conservation has described herbfield vegetation as that in which the cover of herbs in the canopy is 20–100%, and in which herb cover is greater than that of any other growth form, or of bare ground.\n\nVarious kinds of herbfield include:\n\n"}
{"id": "14053983", "url": "https://en.wikipedia.org/wiki?curid=14053983", "title": "Hundhammerfjellet Wind Farm", "text": "Hundhammerfjellet Wind Farm\n\nHundhammerfjellet Wind Farm is a wind farm located in Nærøy, Norway. It has 17 windmills with output between 1.66 and 3.5 MW, delivered from Norwegian manufacturer Scanwind (14 turbines, one more has been decommissioned), Vestas (one V66) and Enercon (one E-70 2.0 MW and one E-70 2.3 MW). The farm is owned by Nord-Trøndelag Elektrisitetsverk and has been completed in December 2009 with the installation of the ENERCON E-70 2.3 MW.\n"}
{"id": "40133011", "url": "https://en.wikipedia.org/wiki?curid=40133011", "title": "Kavşaktepe Dam", "text": "Kavşaktepe Dam\n\nThe Kavşaktepe Dam is a gravity dam under construction on the Ortasu River (a tributary of the Hezil River) in Uludere district of Şırnak Province, southeast Turkey. Under contract from Turkey's State Hydraulic Works, Be-Ha-Se Insaat began construction on the dam in 2008 and a completion date has not been announced.\n\nThe reported purpose of the dam is water storage and it can also support a 1.57 MW hydroelectric power station in the future. Another purpose of the dam which has been widely reported in the Turkish press is to reduce the freedom of movement of Kurdistan Workers' Party (PKK) militants. Blocking and flooding valleys in close proximity to the Iraq–Turkey border is expected to help curb cross-border PKK smuggling and deny caves in which ammunition can be stored. A total of 11 dams along the border; seven in Şırnak Province and four in Hakkâri Province were implemented for this purpose. In Şırnak they are the Silopi, Şırnak, Uludere and Balli Dams downstream of the Kavşaktepe Dam and the Musatepe and Çetintepe Dams upstream on the Ortasu River. In Hakkari are the Gölgeliyamaç (since cancelled) and Çocuktepe Dams on the Güzeldere River and the Aslandağ and Beyyurdu Dams on the Bembo River.\n\n"}
{"id": "45035872", "url": "https://en.wikipedia.org/wiki?curid=45035872", "title": "Krughütte Solar Park", "text": "Krughütte Solar Park\n\nThe Krughütte Solar Park is a 29.1-megawatt (MW) photovoltaic power station in Eisleben, Germany.\n\nThe solar farm is located in the state of Saxony-Anhalt and was developed and constructed by German project developers SRU Solar AG, Berga and Parabel AG, Berlin, who continue to operate the farm. It was constructed on the site of a former copper mine, and at over is one of the largest projects in the region.\n\n"}
{"id": "10767147", "url": "https://en.wikipedia.org/wiki?curid=10767147", "title": "Lightning rocket", "text": "Lightning rocket\n\nA lightning rocket is a rocket device, generally about the size of a man, that trails behind a conductor, such as a fine copper wire or other medium that is conductive, to conduct lightning charges to the ground. Lightning strikes derived from this process are called \"triggered lightning.\"\n\nA conducting lightning rod which is grounded and positioned alongside the launch tube in communication with the conductive path to thereby control the time and location of a lightning strike from the thundercloud. The conductor trailed by the rocket can be either a physical wire, or column of ionized gas produced by the engine. A lightning rocket using solid propellant may have cesium salts added, which produces a conductive path when the exhaust gases are discharged from the rocket. In a liquid propellant rocket a solution of calcium chloride is used to form the conductive path.\n\nThe system consists of a specially designed launch pad with lightning rods and conductors attached. The launch pad is either controlled wirelessly or via pneumatic line to the control station to prevent the discharge traveling to the control equipment. The fine copper wire (more recently reinforced with kevlar) is attached to the ground and plays out from the rocket as it ascends. The initial strike follows this wire and is as a result unusually straight. As the wire is vaporized by the initial strike subsequent strikes are more angular in nature following the ionization trail of the initial strike. Rockets of this type are used for both lightning research and lightning control.\n\nThe Betts \"lightning rocket\", patented by Robert E. Betts in 2003, consists of a rocket launcher that is in communication with a detection device that measures the presence of electrostatic and ionic change in close proximity to the rocket launcher that also fires the rocket. This system is designed to control the time and the location of a lightning strike. As the rocket flies to the thundercloud this liquid is expelled aft forming a column in the air of particles that are more electrically conductive than the surrounding air. In a similar fashion to the system employing a solid propellant as the conductive producer this conductive path conducts a lightning strike to ground to thereby control the time and location of a lightning strike from the thundercloud.\n\n"}
{"id": "40620408", "url": "https://en.wikipedia.org/wiki?curid=40620408", "title": "List of ecoregions in El Salvador", "text": "List of ecoregions in El Salvador\n\nThis is a list of ecoregions in El Salvador as defined by the World Wildlife Fund and the Freshwater Ecoregions of the World database.\n\n\n\n\n\n\n"}
{"id": "40902766", "url": "https://en.wikipedia.org/wiki?curid=40902766", "title": "List of electricity sectors", "text": "List of electricity sectors\n\nThis is a list of electric power industry sectors in every country or region around the world.\n\n\n\n\n\n\n\n\n"}
{"id": "32509090", "url": "https://en.wikipedia.org/wiki?curid=32509090", "title": "Mahatma Gandhi Kalwakurthy lift irrigation scheme", "text": "Mahatma Gandhi Kalwakurthy lift irrigation scheme\n\nMahatma Gandhi Kalwakurthy lift irrigation project (KLIP) is a lift irrigation project on River Krishna located in Mahbubnagar district in Telangana, India. The lift canal starts from the back waters of Srisailam Dam near Kollapur. The gravity driven, 100 kilometer canal provides cultivation for nearly 4,00,000 acres in 300 villages located in constituencies of Kollapur, Wanaparthy, Nagarkurnool, Kalwakurthy, Jadcherla and Achampet. \nThe Kalwakurthy Lift Irrigation Scheme foundation stone was laid way back in 1984. It was started again in 2005 but delayed again. In August 2014 the work started with a spending of ₹2100 crores and completed in October 2017 and water was released by irrigation Minister, Harish Rao. It took a little over three decades to be completed.\n\nThe original design for the project was 25 TMC water capacity, but later it was increased to 40 TMC to reach the prescribed target. This project provides water to the most drought prone areas in Mahbubnagar. The water is sourced from Krishna river, by lifting water 300 meters above river level and channeling into the reservoir. \n\nAchampet Branch Canal will be expanded by a length of 14 km, creating an additional ayacut of 15,000 acres (10,000 acres in Uppunuthala mandal and 5,000 acre) in Achampet mandal.\n\nAround 51 balancing reservoirs with 20 TMC water capacity are proposed under MGKLIS for assured water supply to the ayacut. They are scheduled to be completed by June 2019.\n\nThe Buddaram Peddavagu Lake is proposed to be converted into a reservoir in 2018. It was conceptualised and conceived way back in 2008, due to delays it is completing in June 2019. With this reservoir, the entire parched lands under Wanaparthy district will get water for irrigation. This would also ensure continuous flow of water\n\nThis project has three lifts, first lift in Eluru (Kollapur mandal) second in Jonnalaboguda village and third lift in Gudipally village. There are small reservoirs yet to be constructed to store the water which is lifted from the backwaters of the Srisailam. This project serves water through canals which will fill the lakes of all the villages and towns present in the five constituencies of Mahabubnagar district.\n\nABB partnered for industrial-strength water pumping technology with Andritz AG, to deploy five of its massive and high-capacity 30-megawatt (MW), 11-kilovolt (kV) motors, each capable of pumping 23,000 litres per second. ABB also supplied substation equipment, transformers, and a digitally-enabled supervisory control and data acquisition (SCADA) monitoring system that ensures a steady and reliable drawing of power from the grid and smooth pumping of water into the reservoir.\n\n2. - Kalwakurthy’s two more lifts commissioned\n"}
{"id": "5732885", "url": "https://en.wikipedia.org/wiki?curid=5732885", "title": "Marcel Roche", "text": "Marcel Roche\n\nMarcel Roche Dugand (August 15, 1920 in Caracas, Venezuela – May 3, 2003 in Miami, USA) was a physician, scientist and scientific leader.\n\nHe was born into a wealthy family of French origin. His father, Luis Roche was a well known urbanist. His secondary education was conducted in Paris, France, graduating in 1938. Following this, he moved to the USA and got a Bachelor of Science degree at St. Joseph’s College, in Philadelphia, following by studies in medicine at Johns Hopkins Medical School, in Baltimore. After graduation in 1946, he specialized in endocrinology and nuclear medicine. Before returning to Venezuela in 1951, he carried out biomedical research for some time at the New York Institute of Public Health. \n\nIn Venezuela, Dr. Roche started several pioneering works as an Assistant Professor of the Central University of Venezuela on goitre, hookworm infections and nutritional deficiencies and anaemias, especially among the poor and aboriginal people. \n\nHe was founder and director of the Institute of Medical Research at the Central University, and in 1958 he also became the Secretary General of the Venezuelan Association for the Advancement of Science. Other institutions directed by him were the Institute of Neurology and Brain Investigation, reorganized in 1959 as the Venezuelan Institute for Scientific Research after succeeding Dr. Humberto Fernandez Moran in 1958. During his tenure, Dr. Roche became interested and supported the development of anthropology and the study the history and sociology of science.\n\nHe was founder and director of the Venezuelan National Council of Scientific Investigation and the magazine Intersciencia, as well as being involved in the publishing of several other scientific periodicals. Dr. Roche was also a pioneer in the area of public understanding of science and a pioneer in the production of TV programs and documentary films on many science subjects. He was very active in promoting science to the public and participated in many national and international organizations promoting science. Dr. Roche was an advisor to the World Health Organization (WHO), UNESCO, a Governor of the International Atomic Energy Agency (1958-1960), and was a Member and President of the Council of the University of the United Nations in Tokyo, and Secretary of the Third World Academy of Sciences. \n\nHe received many honours and degrees from Belgium, Germany, France, the United States, India and Brazil. He won the Kalinga Prize in 1987 from UNESCO for his work.\n\n"}
{"id": "2537332", "url": "https://en.wikipedia.org/wiki?curid=2537332", "title": "Massicot", "text": "Massicot\n\nMassicot is lead (II) oxide mineral with an orthorhombic lattice structure.\n\nLead(II) oxide (formula: PbO) can occur in one of two lattice formats, orthorhombic and tetragonal. The tetragonal form is called litharge. PbO can be changed from massicot to litharge (or vice versa) by controlled heating and cooling. At room temperature massicot forms soft (Mohs hardness of 2) yellow to reddish-yellow, earthy, scaley masses which are very dense, with a specific gravity of 9.64. Massicot can be found as a natural mineral, though it is only found in minor quantities. In bygone centuries it was mined. Nowadays massicot arises during industrial processing of lead and lead oxides, especially in the glass industry, which is the biggest user of PbO.\n\nThe definition of massicot as orthorhombic PbO dates from the 1840s, but the substance massicot and the name massicot has been in use since the late medieval era. There is some evidence that the ancient Romans used the substance.\n\nIt may occur as an oxidation product of other lead-bearing minerals such as galena, bournonite, boulangerite, either naturally or in industrial processing. When massicot is found in a natural environment, some other minerals that may be found with it may include cerussite, litharge, minium, wulfenite, oxides of antimony and limonite.\n\n"}
{"id": "1031302", "url": "https://en.wikipedia.org/wiki?curid=1031302", "title": "Neoplan AN440", "text": "Neoplan AN440\n\nThe Neoplan Transliner was a series of related public transport single-decker bus models introduced by Neoplan USA in 1981. It was able to compete with the Rapid Transit Series, Flxible Metro, Gillig Phantom and the Orion I in the early 1980s.\n\nThe Transliner was available in a wide variety of body styles. The Transliners could be ordered in 26', 30', 35', 40', and 60' lengths. Standard (high), completely low, or semi-low (with steps to access the rear section after the rear doors) floors were available.\n\nNeoPlan offered a variety of engines, both diesel and CNG fueled. Depending upon the model, Detroit Diesel 40 or 50 series or the 6V92TA; Cummins ISM, C-series, or ISL; or CAT C-9 engines could also be had. Most Transliners featured Allison B400 or B500 \"World Transmissions\". However, ZF (4HP590) and Voith transmissions were also available on some models.\n\n\nIn late 1983 through 1985, Neoplan delivered an order for over 1000 buses for the state of Pennsylvania. By 1989, the largest transportation network in Pennsylvania, SEPTA, had 1092 Neoplan AN440s in service. However over the years as they were heavily replaced mainly by New Flyer low-floor buses, that were placed in service since between 2001 and 2006, While the remaining 35 Neoplan AN440s (EZ models) were sent to the SEPTA Frankford Depot, following after the retirement of the 1978 AM General Trolleybuses that retire in 2006, as they served as the temporary replacement, they were retired on June 20, 2008, following after the final delivery of the New Flyer trolleybuses was completed. SEPTA ordered 155 Neoplan AN460s from 1998-2000 to replace their aging Volvo B10M articulated buses and about 60 AN460 high floor buses are in service until 2013, they were being replaced by Novabus LFS Artic articulated buses, as of 2016, after delivering the last remaining set of Novabus Artics, all of the remaining AN460 buses were retired.\n\nIn the 1980s, WMATA, also known as Metro, and PAT Port Authority Transit in Pittsburgh, Pennsylvania complained about frame problems with their Neoplan coaches. However, today, they both run new Neoplan buses — WMATA with the AN460, a 60-foot articulated version, and PAT with AN460s and AN440LFs, the low floor version of these buses.\n\nHowever, the AN440 and AN460 models ordered by the San Francisco Municipal Railway proved troublesome. Among the problems were insufficient, excessively noisy cooling fans (sounds reached up to 90 dB), faulty transmissions, maintenance intensive brake systems (needing service every 5,000 miles), and cracking frames. The problems were compounded when Neoplan eventually refused to fix the problems, instead choosing to repossess the remaining spare parts and abandon its overhaul yard located in San Francisco.\n\nSelf-supporting monocoque steel construction made of seamless square steel tubes, electrically welded. Exterior roof and side wall panels are of double galvanized steel, sealed to the skeleton with a combination of spot welding and gluing. Stainless steel wheelhousings and stepwells.\n\nSeating capacity for up to 46 passengers.\n\nV Drive:\n\nDetroit Diesel Series 50, DDEC III, 275HP\n\nCummins L10, Bravo Phase III 275HP\n\nCummins L10 Bravo Phase III, CNG / LNG optional\n\nT-Drive:\n\nDetroit Diesel Series 50, DDEC III, 275 to 315HP\n\nCummins L10, Phase III 275 to 300HP\n\nCummins L10 CNG / LNG optional\n\nV-Drive:\n\nAllison V731 or VR731 3 speed with integral retarder\n\nZF HP590 5 speed\n\nVoith D883\n\nT-Drive:\n\nAllison World Transmission B400 / B500, Retarder optional\n\nFront: Rockwell 17100 Series, IFS hubs\n\nRear(V-Drive): Rockwell 50738 Series with ratios to provide top speeds between 55 and 65 MPH.\n\nRear (T-Drive): Rockwell 61100 Series with ratios to provide top speeds between 55 and 70 MPH.\n\nDual circuit air brakes\n\nBendix \"S\" cam with automatic slack adjusters and spring type parking brake.\n\nAnti-lock brake systems and tractiona control optional\n\nThe 40-foot buses were made for over 50 transportation networks all over the United States. The bus networks that have or have had them in service are MBTA in Boston, Massachusetts; SEPTA in Philadelphia, Pennsylvania; WMATA in Washington, DC; ABQ RIDE in Albuquerque, New Mexico and LACMTA in Los Angeles, California.\n\nSan Francisco Municipal Railway (SF MUNI) ordered Neoplan AN440s and AN460s to replace their aging bus fleet during 2001-2004, but they are unique in that they have rear windows, with the air conditioning unit mounted on the roof.\n\nBi-State Development Agency, dba Metro, has a small fleet of Neoplan Buses for its MetroBus service, and since March 30, 2009, has retired them.\n\n\n"}
{"id": "5534701", "url": "https://en.wikipedia.org/wiki?curid=5534701", "title": "Nuclear power by country", "text": "Nuclear power by country\n\nNuclear power plants currently operate in 31 countries. \nMost are in Europe, North America, East Asia and South Asia.\nThe United States is the largest producer of nuclear power, while France has the largest share of electricity generated by nuclear power.\nIn 2010, before the Fukushima Daiichi nuclear disaster, it was reported that an average of about 10 nuclear reactors were expected to become operational per year, although according to the World Nuclear Association, of the 17 civilian reactors planned to become operational between 2007 and 2009, only five actually came on stream. \nGlobal nuclear electricity generation in 2012 was at its lowest level since 1999.\nChina has the fastest growing nuclear power program with 28 new reactors under construction, and a considerable number of new reactors are also being built in India, Russia and South Korea. \nAt the same time, at least 100 older and smaller reactors will \"most probably be closed over the next 10–15 years\".\n\nSome countries operated nuclear reactors in the past but have currently no operating nuclear plants. \nAmong them, Italy closed all of its nuclear stations by 1990 and nuclear power has since been discontinued because of the 1987 referendums on which Italians voted.\nLithuania, Kazakhstan and Armenia are planning to reintroduce nuclear power in the future.\n\nSeveral countries are currently operating nuclear power plants but are planning a nuclear power phase-out.\nThese are Belgium, Germany, Spain, and Switzerland. \nOther countries, like Netherlands, Sweden, and Taiwan are also considering a phase-out. \nAustria never started to use its first nuclear plant that was completely built. \n\nDue to financial, political and technical reasons, Cuba, Libya, North Korea, and Poland never completed the construction of their first nuclear plants, and Australia, Azerbaijan, Georgia, Ghana, Ireland, Kuwait, Oman, Peru, Singapore, and Venezuela never built their planned first nuclear plants.\n\nOf the 31 countries in which nuclear power plants operate, only France, Slovakia, Ukraine, Belgium, and Hungary use them as the source for a majority of the country's electricity supply.\nOther countries have significant amounts of nuclear power generation capacity. By far the largest nuclear electricity producers are the United States with 805 647 GWh of nuclear electricity in 2017, followed by France with 381 846 GWh. As of December 2017 448 reactors with a net capacity of 391 721 MWe are operational and 59 reactors with net capacity of 60 460 MWe are under construction, of those 18 reactors with 19 016 MWe in China. \n\n\n"}
{"id": "3548289", "url": "https://en.wikipedia.org/wiki?curid=3548289", "title": "Penning mixture", "text": "Penning mixture\n\nA Penning mixture, named after Frans Michel Penning, is a mixture of gases used in electric lighting or displaying fixtures. Although the popular phrase for the most common of these is a neon lamp, it is more efficient to have the glass tube filled not with pure neon, but with a Penning mixture, which is defined as a mixture of one inert gas with a minute amount of another gas, one that has lower ionization voltage than the main constituent (or constituents). \n\nThe other gas, called a quench gas, has to have lower ionization potential than the first excited state of the noble gas. The energy of the excited, but neutral, noble gas atoms then can ionize the quench gas particles by energy transfer via collisions; known as the Penning effect.\n\nA very common Penning mixture of about 98–99.5% of neon with 0.5–2% of argon is used in some neon lamps, especially those rated at 110 volts. The mixture is easier to ionize than either neon or argon alone, and lowers the striking voltage at which the tube becomes conductive and starts producing light. The optimal level of argon is about 0.25%, but some of it gets adsorbed onto the borosilicate glass used for the tubes, so higher concentrations are used to take the losses into account; higher argon content is used in higher-power tubes, as hotter glass adsorbs more argon. The argon changes the color of the \"neon light\", making it slightly more yellowish. The gas mixtures used in nixie tubes often also included a small amount of mercury vapor, which glowed blue.\n\nA Penning mixture of neon and argon is also used as a starter gas in sodium vapor lamps, where it is responsible for the faint reddish glow before the sodium emission begins.\n\nThe Penning mixture used in plasma displays is usually helium or neon with small percentage of xenon, at several hundred torr.\n\nPenning mixtures with the formulas of argon-xenon, neon-argon, argon-acetylene, and xenon-TMA are used as filler gases in gaseous ionization detectors.\n\nOther kinds of Penning mixture include helium-xenon.\n\n"}
{"id": "37528172", "url": "https://en.wikipedia.org/wiki?curid=37528172", "title": "Plug-in electric vehicles in Norway", "text": "Plug-in electric vehicles in Norway\n\nThe fleet of plug-in electric vehicles in Norway is the largest per capita in the world. In March 2014, Norway became the first country where one in every 100 passenger cars on the road was a plug-in electric; the market penetration passed 5% in December 2016, and attained 10% in October 2018. The Norwegian plug-in car segment market share has been the highest in the world for several years, achieving 39.2% in 2017, up from 29.1% in 2016. In December 2017, the plug-in segment reached 50% of new monthly registrations for the first time ever.\n\nThe stock of light-duty plug-in electric vehicles registered in Norway totaled almost 275,000 units at the end of September 2018, consisting of 183,637 all-electric passenger cars and vans, and 89,922 plug-in hybrids, including a significant number of used imports from neighboring countries. , the country has the largest European stock of light-duty plug-in vehicles, and the fourth largest in the world after China, the U.S. and Japan. The fleet of electric cars is one of the cleanest in the world since 98% of the electricity generated in the country comes from hydropower. In 2017, and as a result of its fast growing EV adoption, Norway was able to achieve its climate target for average fleet emissions (85 g/km) for new passenger cars three years earlier than pledged.\n\nThe adoption and deployment of zero emission vehicles in Norway has been driven by policy, and actively supported by the government since the 1990s. Among the existing public incentives, all-electric cars and vans are exempt from all non-recurring vehicle fees, including purchase taxes, and 25% VAT on purchase, making electric car purchase price competitive with conventional cars. Also, a tax reduction for plug-in hybrids went into effect starting in July 2013. In April 2015, after achieving the initial goal of 50,000 pure electric vehicles on the road, a decision was made to keep the existing incentives through 2017, and Parliament agreed to reduced and phase out some of the incentives beginning in 2018. Also local authorities were granted the right to decide whether electric cars can park for free and use public transport lanes. In 2016, through its National Transport Plan 2018-2029 (NTP), a goal was set for all sales of new cars, urban buses and light commercial vehicles by 2025 to be zero emission vehicles.\n\nAs a result of the successful policies implemented to promote EV adoption, the stock of electric vehicles in Norway has increased rapidly, resulting in several unintended consequences, and raising several complaints and criticism. These include: high public subsidies as compared to the value of the reduced carbon footprint of electric vehicles; the possibility of traffic congestion in some of Oslo's bus lanes due to the increasing number of electric cars; the loss of revenue for some ferry operators due to the large number of electric cars exempted from payment; and the shortage of parking spaces for owners of conventional cars due to preference to electric cars (although this was actually the intended policy).\n\nThe adoption and deployment of electric vehicles in Norway, particularly zero emission vehicles, has been driven by policy, and since 1990, actively supported by the Norwegian government. The Parliament of Norway set the goal to reach 50,000 zero emission vehicles by 2018. Among the existing incentives, all-electric cars and utility vans are exempt in Norway from all non-recurring vehicle fees, including purchase taxes, which are extremely high for ordinary cars, and 25% VAT on purchase, together making electric car purchase price competitive with conventional cars. As an example, by early 2013 the price of the top selling Nissan Leaf is 240,690 kroner (around ) while the purchase price of the 1.3-lt Volkswagen Golf is 238,000 kroner (about ). Electric vehicles are also exempt from the annual road tax, all public parking fees, and toll payments, as well as being able to use bus lanes. These incentives are in effect until the end of 2017 or until the 50,000 EV target is achieved.\n\nSales of plug-in hybrids have had a much smaller market penetration than pure electric car sales. Plug-in hybrids are not eligible for the same tax exemptions and other government incentives enacted for electric cars. Because the Norwegian tax system levies higher taxes to heavier vehicles, plug-in hybrids are more expensive than equivalent gasoline and diesel-powered cars due to the extra weight of the battery pack and its additional electric components. Beginning on 1 July 2013, the existing weight deduction for conventional hybrids and plug-in hybrids of 10% was increased to 15% for PHEVs. The weight deduction was increased to 26% effective since January 2015. This fiscal incentive combined with a broader range of models available in the market resulted in record sales of plug-in hybrids in 2015, with almost 8,000 new units registered, up from about 1,700 in 2014. The plug-in hybrid market share rose to 5.2%, up from just over 1% in 2014, and from 4.2% in September 2015 to 13.9% in September 2016.\n\nThe government set identifying letters (\"EL\", \"EK\" and \"EV\") for use on license plates of electric vehicles to facilitate the enforcement of EV incentives and perks such a free parking, free passage through toll booths etc. Correspondingly, hydrogen vehicles have \"HY\" as identifying letters. There is no equivalent for plug-in hybrids.\n\nAs the number of registered electric cars and vans reached 60,000 units by July 2015, and because the \"EL\" prefix was set to end at \"EL 99999\" (most vehicles in the country have five-digit registration numbers between 10000 and 99999), the Norwegian Public Roads agency opted for the prefix \"EK\" for the second series of plates, to signify \"elektrisk kjøretøy\", Norwegian for electric vehicle. The \"EV\" prefix was set aside for future electric cars. \n\nIn July 2016, the first electric vehicles registered with the new \"EK\" series were on the road. About 90,000 pure electric vehicles have been registered by the end of August 2016, depleting the \"EL\" prefix plates. Just over two years later, in the summer of 2018, the \"EK\" series was depleted, and the prefix \"EV\" plates were deployed.\n\nIn September 2013, the Norwegian Parliament approved, as part of the revised 2014 budget, an exemption from the 25% VAT for leasing electric vehicles effective on 1 January 2014. However, , the exemption had not gone into effect because the Minister of Finance decided to deferred the measure, pending a formal consultation with the EFTA Surveillance Authority (ESA) to ensure that the VAT exemption for leasing was not in violation of the European Economic Area (EEA) Agreement. The government's loss of revenue due to the still not implemented leasing exemption is estimated at about 47 million kroner (around ) per year. One Member of Parliament has criticized the government for the delay. He argued that the initial VAT exemption for all electric vehicles was never approved in ESA. In addition, an ESA spokesman confirmed that the Government has not sent any request , nor has the agency received any complaints about Norway's original EV tax exemption. The MP said he would demand that the decision be implemented when Parliament meets in October 2014. The consultation was presented to ESA in November 2014, and the authority ruled in April 2015 that the implementation of the VAT exemption on leasing of electric vehicles and electric vehicle batteries is in line the EEA Agreement, since the goal is to reduce greenhouse gas emissions. The approval from ESA initially applies until the end of 2017, but the government can apply for an extension if the zero rate for VAT is kept. The exemption for leasing of electric vehicles went into effect in July 2015.\n\nThe target of 50,000 electric cars on Norwegian roads was reached on 20 April 2015, more than two years earlier than expected. The milestone was commemorated by the Norwegian Electric Vehicle Association in Drammen where the 50,000th electric car registered, a Tesla Model S, was granted the license plate \"EL 60000.\" The special electric vehicle series began with \"EL 10000.\" By reaching a stock of 50,000 electric cars, the market penetration of pure electric vehicles reached 2% of all passenger cars registered in Norway. The milestone of 100,000 light-duty battery electric vehicles was achieved in December 2016, representing about 10% of all pure electric cars that have been sold worldwide. According to the Norwegian Electric Vehicle Association, if the country wants to reach the ambitious climate goals set by the Parliament, the next goal is to have 400,000 battery electric vehicles by 2020.\n\nAs one of the criteria to end the incentives was achieved, , no decision was made by the authorities about the reintroduction of the 25% VAT on purchase of electric vehicles. Among the options considered by the government were to introduce the tax in a step-wise fashion. However, Prime Minister Erna Solberg announced the government decided not to make any changes about the electric car benefits in the 2015 budget.\n\nIn early March 2015, negotiations began among parties represented in the Parliament to define the future of all motor vehicles and fuel taxes. The Liberal Party wanted all the benefits to continue beyond the established quota. The Ministry of Finance also made a comprehensive review of all motor vehicle taxes. The two purchase tax exemptions cost the government about 3 billion kroner (around ) in lost revenue just in 2014, and up to 4 billion kroner (around ) if all the other benefits are accounted for. In May 2015, the Government decided to keep the existing incentives through 2017, and the political parties in Parliament agreed to reduced and phase out some of the incentives. Beginning in January 2018, electric car owners will be required to pay half of the yearly road license fee and the full rate as of 2020. The value-added tax (VAT) exemption for electric cars was scheduled to end in 2018, but replaced by a new scheme, which may be subjected to a ceiling that could be reduced as technology develops. The agreement also gave local authorities the right to decide whether electric cars can park for free and use public transport lanes.\n\nIn March 2016, the Ministry of Transport issued new regulations for parking in locations with access to the general public. The new parking regulations, that went into effect on January 1, 2017, terminated the free parking for zero-emission vehicles, but established that Municipalities are allowed to introduce payment exemption for electric and hydrogen powered motor vehicles on municipal parking locations. , the city councils of Trondheim and Tønsberg decided to introduce full payment for EVs from 2017; the cities of Bodø and Tromsø introduced payment for parking in downtown but exempted parking outside the city's center; and the cities of Oslo, Mandal and Drammen decided to keep free parking for zero-emission vehicles. , 24 out of 58 major municipalities kept the free parking for EVs. Among the 34 municipalities that terminated the benefit, six kept different variants of partial free parking.\n\nFor the 2017 National Budget, the Government proposed to extend the VAT exemption for zero-emission vehicles until 2020. It also put forward a national rule to keep a maximum tax rate of zero-emission vehicles equal to half the value charged to conventional cars. The budget proposal also included an adjustment to exempt plug-in hybrids from the higher taxes levied to heavier vehicles, and instead, to charge taxes based on their fuel economy under the New European Driving Cycle (NEDC). Until 2016, all plug-in hybrids had a weight allowance of 26% regardless of their all-electric range or fuel efficiency.\n\nIn October 2017, the National Government proposed in its 2018 Budget a new tax for EVs based on weight. This proposal would affect mainly bigger electric car models, all Tesla vehicles, and in particular the Tesla Model X. After some public controversy, the proposal was dropped. Nevertheless, Tesla's sales surge at the end of 2017 for consumers' fear of the introduction of the new tax.\n\nIn February 2016, the government opened for public discussion until 1 July 2016 the proposed National Transport Plan 2018-2029 (NTP). The plan explains that the transportation sector accounts for emissions of about 16.5 million tons of , which is about one third of the total greenhouse gas emissions produced domestically in Norway. And road traffic, including both private cars and heavy vehicles, account for about 10 million tons of . The NTP set policies and actions to reduce greenhouse gas emissions from private cars, trucks, ships, aircraft and construction equipment by about one half until 2030.\n\nIn order to achieve this objective, among others, the NTP sets the goal for all sales of new cars, urban buses and light commercial vehicles in 2025 to be zero emission vehicles, this is, all-electric and hydrogen vehicles. By 2030, heavy-duty vans, 75% of new long-distance buses, and 50% of new trucks must be zero emission vehicles. Also, by 2030, 40% of all ships in short sea shipping should be using biofuels or be low- or zero-emission ships. The proposed strategy states that until zero-emission vehicles take over, all internal combustion engine cars sold be plug-in hybrids, and wherever possible, biofuels must be used. Also, government agencies should as far as possible make use of biofuels, low- and zero-emission technologies in private and hired vehicles and vessels. The plan also calls to support the deployment of zero emission vehicles, but also for the reduction of the existing incentives, and proposes to invest more in public transport, walking and cycling.\n\nSince 2013 several complaints and criticism have arisen due to the rapid increase in the number of electric vehicles on the roads as a result of the existing incentives to promote EV adoption, and particularly about the non-fiscal incentives. These include high public subsidies as compared to the value of the reduced carbon footprint of electric vehicles; the travel mode shift by people who buy an electric vehicle as a second car instead of taking buses and trains; the potential traffic congestion in Oslo's bus lanes due to the increasing number of electric cars; the loss of revenue for some ferry operators due to the large number of electric cars exempted from payment; and shortage of parking spaces for owners of conventional cars due to preference to electric cars and lack of a cap on parking time.\n\nAccording to the results of a study published by Reuters in March 2013, prepared by Bjart Holtsmark, an analyst of Statistics Norway, the tax exemptions on the purchase of an electric car are worth almost in comparison to the fully taxed price of a regular internal combustion engine car, which is equivalent to a year over a car's lifetime (8 years). The value of the toll exemption for driving into Oslo are worth per year, the free parking is worth per year, and electric cars avoid other charges worth a year. Without adding value to the benefit of driving in bus lanes, the annual benefit of owning an electric car in Oslo is estimated at per car, per year. The analysis used a Toyota Prius Plug-in Hybrid as the benchmark vehicle. Mr. Holtsmark also pointed out that \"by encouraging people who can afford it to buy a second car instead of taking buses and trains, the electric car scheme may ironically be aggravating environmental problems and causing traffic jams.\"\n\nThe Norwegian project \"Grønn bil\" (Green Car) disputed these figures because they consider the analysis is based on unrealistic assumptions. The group argues that the analysis used a very short total vehicle lifespan of 7.8 years, while Norway's' average is closer to 18 years; it is very unlikely that a vehicle can be parked in Oslo between 1,875 hours and 3,000 hours per year to save the estimated considering the existing time limits for parking; and the typical EV owner drives around per year, not the implicit in the analysis. Using what they consider more realistic assumptions, \"Grønn bil\" estimates that the annual benefit of owning an electric car in Oslo is estimated at per car, per year, 40% of Holtsmark's estimation. They also found that the cost per tonne of emissions reduced is , not the estimated by Holtsmark.\n\nIn December 2013 the newspaper \"Budstikka\" conducted an informal test to measure the difference in travel time between an electric car and a gasoline-powered car during the morning rush hour on a stretch of road between the suburban municipality of Asker in Akershus, and Skøyen, a neighborhood of Oslo. The electric car completed the trip in 19 minutes using the bus lane while it took 51 minutes for the conventional car traveling in the normal lanes. Around noon, the same trip took the electric car just 13 minutes.\n\n\"Budstikka\" also did a count of the number of vehicles traveling in the bus lane during the rush hour on December 3, 2013. The newspaper found that a total of 829 vehicles used the bus lane between 7:30 and 8:30 a.m., of which, 618 vehicles were electric cars (74.5%). Buses accounted for only 7.5% of the traffic in the bus lane, and taxis, two-wheelers and mini-buses made up the rest. The Deputy Director of the Institute of Transport Economics (TØI) explained that the normal capacity of a highway lane is between 1,800 and 2,000 vehicles per hour, but because of the ramp entrances and exits, and the buses maneuvering in and out the bus lane to do their stops, the traffic flow starts to become troublesome when the number of vehicles in the bus lane is about 1,000 vehicles per hour. Although by December 2013 the traffic is approaching this limit, TØI's Deputy Director did not want to predict when this critical situation will occur. The Manager of the green car advocate group Grønn Bil warned that \"if the only reason people bought an electric car is to drive in the bus lane, they will probably be disappointed sooner or later.\"\n\nBy mid-2014, bus drivers in some parts of Oslo begun complaining about the delays caused by the ever-increasing number of electric cars. An interviewed bus driver expressed his concern that the electric cars \"can create a vicious circle - tired of being stuck in traffic, bus users could be tempted to buy an electric car themselves, worsening the congestion problem.\" According to the Norwegian Public Roads Administration, , electric cars represent 85% of traffic in the bus lanes during rush hours. As of late August 2014, Norwegian ministers are under increasing pressure to reduce the non-financial incentives and tax breaks for electric cars in order to reduce a rising congestion problem, but no decision has been made by the central government authorities. The success of electric car adoption was unexpected, as the authorities planned to keep the incentives in place until the end of 2017, or until they reach 50,000 units. At the pace of sales reached during 2014, the target of 50,000 EVs registered could be met by April or May 2015.\n\nAccording to local authorities from the city of Oslo, the negative effects on the bus lanes occur only at certain places and in certain times of the day, particularly at the Norwegian National Road E18, west of Oslo. The problems are concentrated at the exit and entry ramps that in the long term might have serious consequences for bus accessibility. All the involved agencies are monitoring the situation and Oslo's authorities consider that restrictions for EVs to access the bus lanes should be considered only when it becomes a major problem for the buses.\n\nAs part of the incentives to promote EV adoption, plug-in electric vehicles are exempted from payment of ferryboat fees, but only the car crosses for free, the driver and each passenger pay the ordinary fare. The accelerated growth of electric cars on some ferry routes has caused complaints from ferryboat operators due to the increasing loss on their farebox revenues. According to FosenNamsos Sea AS, an operator with four ferry lines servicing Hordaland county, during the first seven months of 2014 the number of electric vehicles riding the service Krokeidet-Hufthamar from Hordaland increased by 215% compared to the same period last year, for a total of 9,226 electric cars not paying the ferry fee though the end of July. The company argues that \"no one could foresee the tremendous growth of electric cars we see on some ferry routes, but the Austevoll satellite connection involves a significant loss of revenue for us.\"\n\nOn 1 June 2014, the company's home county of Sør-Trøndelag repealed the payment exemption for electric vehicles on the county ferries. The company has also requested to Transport of Hordaland a similar end of the exemption or some form of the income loss compensation. FosenNamsos Sea AS has argued that the financial burden should be on the government not the ferry operators. , the county of Hordaland had 5,016 registered electric cars, the second largest in Norway after Oslo. Hordaland transport authorities are studying the request but already have stated that the agency must follow state regulations for ferry rates and the regulations established for electric cars.\n\nAs an incentive to promote EV adoption, plug-in electric vehicles are exempted from public parking fees. Politicians in Trondheim, in Sør-Trøndelag county are complaining about the lack of parking spaces for owners of conventional cars due to preference to electric cars. The city has a five-hour time limit for electric cars to use street parking for free, but electric car owners who use their car to commute keep moving their cars during the day, and end up having free parking all day while they are at work. A City Council member noted that in many streets there are large numbers of plug-in electric vehicles parked all day, and sometimes there are more electric cars than regular cars. This situation makes it difficult to find parking for those who come to the city to shop. In addition, the municipality of Trondheim is losing revenue. The City Counselor wants to end the incentives electric car owners have to park downtown Trondheim all day long for free.\n\n, there are in the country 7,632 electric recharge points. The county with the most stations is Oslo with 1,996 points, followed by Akershus with 1,117, and Hordaland with 932. The Norwegian charging infrastructure includes 293 CHAdeMO quick charging points and 194 fast charging points at Tesla supercharger stations.\n\nIn June 2013, the Norwegian Electric Vehicle Association (\"Norsk Elbilforening\") conducted a survey among all-electric car owners, with a total of 1,858 respondents, representing over 15% of all the electric car owners in Norway. The study found that the typical electric car owner is a middle-aged family father with higher education and income, and he owns a Nissan Leaf as one of two cars. A total of 85% of the respondents had two or more cars in their household because they need a second car for longer journeys given the limited range current electric cars can provide. However, for everyday needs, the study found that one electric car is sufficient. Norwegians travel 42 km on average every day, mostly by car. Based on the survey, the study found that 15% of the electric car owners do their daily travel with just their one electric car. These users opt for public transport or car rental/sharing for longer trips, because high taxation on traditional cars in Norway makes it expensive to own a car.\n\nThe study also found that about half of the respondents to the 2013 survey own a Nissan Leaf the best-selling electric car in Norway and among the top five at the general model ranking. About 5% of the respondents had more than one electric car, as some owners kept their old electric car (such as Th!nk and Buddy) when they bought a new one. The survey showed that in most cases electric cars replaced a traditional car (87%), but also use of public transport (10%), and walking and bicycling (1%). Regarding everyday use of electric cars, the study found that owners use the electric car mostly for commuting to work (89.6%), shopping (88.4%) and driving to after work activities (77.0%). Other uses include delivery of children to school or kindergarten (40.9%) and for business purposes (40.2%). On the other hand, use of the electric car for holiday travel is very limited (11.7%).\n\nAs for the charging patterns, the 2013 survey found that 85% of the respondents could charge in their own garage or parking lot, and 10% had access to charging in the shared apartment building where they live. This means that 95% of the respondents were able to charge their electric cars at home during the night. The survey showed that 59% of the respondents had access to charging where they work, and 48% at public charging stations in the area they normally use the electric car.\n\nThe Norwegian Institute of Transport Economics conducted in March 2016 a survey among over 8,000 vehicle owners in Norway. The study's aim was to identified how the plug-in vehicles are used, why they are bought and how the technology is rated compared to owners of internal combustion engine (ICE) vehicles. The sample consisted of 3,111 all-electric vehicles (BEV) owners, 2,065 private plug-in hybrid (PHEV) owners, and 3,080 ICE vehicle owners. The study found that buyers of BEVs and PHEVs have different transport needs but both are motivated by economy of use and environment, whereas all-electric vehicle owners are also motivated by the free toll road incentive. The survey showed that normally diesel and gasoline vehicles are replaced with the purchase of a plug-in vehicle, but a larger share of battery electric vehicles become extra vehicles in households. BEV owners are younger, have more children, travel a longer distance to work and own more vehicles than other vehicle owners.\nMost BEV owners (71%) also own an ICEV, 4% a PHEV, and 4% more than one BEV. The remaining 21% only have one BEV. 46% of PHEV owners and 48% of ICEV owners belong to single vehicle household. The most multipurpose BEV, Tesla Model S, is twice as common in single BEV households as in households also owning ICEVs, and four times as common in households owning several BEVs. Based on the survey, the researchers found that plug-in hybrid owners in Norway drive on average 55% of their annual distance in charge-depleting or all-electric mode (EV mode), and the share goes up to about 63% for work trips. The share of electric travel is higher for trips to work and in the summer, and lower in the winter. The average plug-in hybrid user in the survey drives 60% of the total distance in EV mode in the summer and 53% in winter. The estimate for work trips is higher at 70% in the summer and 59% in winter. On the other hand, the study found that battery electric vehicles are driven more in total and in everyday traffic.\n\nAccording to the survey results, plug-in vehicles are mainly charged at home, whether in their garage or at an outdoor parking on the owner property, with 59% of BEV owners and 74% of PHEV owners charging this way. Only 6% of BEV owners and 5% of PHEV owners never charge at home. Charging at work is relatively common among BEV owners, 28% do it more than twice a week, 38% weekly. About 21% of PHEV owners do it at least weekly. Charging elsewhere is rare, but BEVs owners more frequently recharged at public charging stations and shopping centers than\nPHEV owners. Fast charging is used for irregular trips where users plan to use fast chargers to accomplish the trip or to solve a problem on the go. Most battery electric vehicle owners manage everyday life well and are satisfied with the vehicle which in combination with attractive local incentives not available to plug-in hybrid owners and other vehicle users.\n\nPeer-to-peer influences is particularly important to diffusion of battery electric vehicles being the biggest source of information leading to the purchase. Plug-in hybrid buyers received most information leading to the purchase from dealers and advertising material. The four reasons most frequently mentioned by the 89% of BEV owners who say they will buy a BEV again are economy of use, environmental performance, future proof technology, and the free usage of toll roads without paying. Less than 1% will not buy a BEV again. The reasons not to buy again are range and charging issues. The three main reasons why 80% of PHEV owners say they will buy one again are economy of use, environmental performance and that the technology is future proof. Only 2% will not buy a PHEV again. The main reasons not to buy again are the short range in all-electric mode and inability to use EV mode when it is cold.\n\nThe Norwegian Electric Vehicle Association (\"Norsk Elbilforening\") conducted a survey among all-electric car owners in June 2018, with a total of 9,520 respondents. The study found that 63% of Norwegian households with electric cars also have a fossil car or hybrid car, down from 70% in 2017. The survey also found that among respondents having only one car in the household, one third (32.4%) are electric car owners, up from 26.3% in 2017.\n\nThe government's initial goal of 50,000 pure electric vehicles on Norwegian roads was achieved by late April 2015. The stock of light-duty plug-in electric vehicles registered in Norway passed the 100,000 unit milestone in April 2016, and registrations of light-duty all-electric vehicles achieved the 100,000 unit milestone in December 2016. , there were 34 models of highway capable plug-in vehicles available in the Norwegian market, 19 plug-in hybrids and 15 all-electric cars and utility vans.\n\n, a total of 135,276 light-duty plug-in electric vehicles have been registered in Norway, allowing the country to have the largest European stock of plug-in cars and vans, and the fourth largest in the world after China, the U.S. and Japan. Of these, 50,834 plug-ins were registered in 2016 (37.6%), including 5,281 used imports. Norway was the top selling plug-in country market in Europe in 2016 with 45,492 new plug-in cars and vans registered, surpassing the Netherlands, Europe’s top market in 2015.\n\nAlso, Norway was the first country in the world to have all-electric cars topping the new car sales monthly ranking. From September 2013 to November 2016, nine times a plug-in electric car has topped the country's monthly ranking, four times the Tesla Model S, twice the Nissan Leaf, once the Mitsubishi Outlander P-HEV, once the Tesla Model X, and once the BMW i3. In March 2014, the Tesla Model S also broke the 28-year-old record for monthly sales of a single model regardless of its power source. , the Leaf ranked as the all-time best selling plug-in electric car in the country with 27,115 Leafs on Norwegian roads at the end of November 2016, followed by the Volkswagen e-Golf with 15,991 units, and the Tesla Model S with 11,615 units. These figures include used imports. These three models account for more than half of the registered stock of all-electric vehicles registered in the country. Norway is the largest European market for both the Nissan Leaf and the Tesla Model S.\n\n, and accounting for both new and used imports registrations, the Norwegian light-duty plug-in electric fleet consisted of 141,951 all-electric passenger cars and vans, and 67,171 plug-in hybrids. The registered plug-in stock includes almost 2,700 all-electric vans and about 24,500 used imported electric cars from neighboring countries.\n\n, the total plug-in electric registered stock included over 2,500 heavy quadricycles, such as the Kewet/Buddy (1,087), Th!nk City (1,120), and the REVAi (299). These city cars are entitled to the special \"EL\" licensed plate reserved for Norwegian electric vehicles.\n\n\nSales of used imports in Norway are significant, and , over 11,500 used plug-in vehicles imported from neighboring countries had been registered, mainly pure electric cars. Registrations of used all-electric cars totaled 2,086 units in 2013, 3,063 in 2014 and 5,122 in 2015. In addition, about 1,300 used electric cars were imported into Norway before 2013. By September 2014 most imports came from France, particularly the Nissan Leaf model. Just in 2015, Norwegians imported a total of 21,756 used cars in 2015, of which highly sought plug-in electric models topped the list of imported cars, the Nissan Leaf with 2,088 and the Kia Soul EV with 2,044. A total of 5,281 used imported electric cars were registered in 2016, up 3.1% from 2015, with registrations led by two popular plug-in models, the Kia Soul (2,494) and the Nissan Leaf (2,112).\n\n\nIn March 2014, Norway became the first country where over one in every 100 registered passenger cars is plug-in electric, out of a fleet of over 2.52 million passenger cars. All-electric vehicles reached a market penetration of 1.02% of the total registered passenger fleet, and for the entire plug-in electric segment the market penetration increases to 1.07% when plug-in hybrids are accounted for. In March 2015, the plug-in segment market penetration passed 2%, and the all-electric segment alone reached 2% of the country's 2.5 million registered passenger cars by late April 2015. The market penetration of the country's plug-in electric car segment passed 3% in December 2015. The all-electric segment achieved a market penetration of 3.5% in September 2016. , plug-in cars represented 5% of the all passenger cars on Norwegians roads, and the 10% milestone was achieved in October 2018.\n\n\nAlso, due to its population size, Norway is the country with the largest EV ownership per capita in the world, In 2013 the EV concentration reached four plug-in electric vehicles per 1,000 people in 2013, nine times higher than the U.S., the world's largest plug-in electric car market at the time. By July 2016, the market concentration had increased to 21.5 registered plug-in cars per 1,000 people, 3.6 times higher California's, the leading American market, and 14.2 times higher than the U.S. average concentration, then the world's largest country market.\n\n\nThe Norwegian plug-in electric car market share of new car sales is the highest in the world. The segment's market share rose from 1.6% in 2011, to 3.1% in 2012, and reached 5.6% in 2013. Only the Netherlands, with 5.34% in 2013, achieved a similar market share. In 2014 the overall plug-in car take rate climbed to 13.8%, and reached 22.4% in 2015. With a plug-in market share of 9.7% in 2015, the Netherlands had the world's second largest market share after Norway.\n\nA new record plug-in market share of 29.1% of new car sales was achieved in 2016. The all-electric segment register a market share of 15.7%, down from 17.1% in 2015, while the market share of plug-in hybrids rose to 13.4%. This result reflects a new trend in the Norwegian plug-in electric market that began in 2016, as annual sales and the market share of all-electric cars suffered a decline over the previous year, while the plug-in hybrid segment experienced significant growth. \n\nA new monthly record was set in December 2017, when the plug-in car segment achieved 50% of new registrations. The plug-in segment again set a record market share in 2017 with 39.2% of new passenger cars registered, with 20.8% for the all-electric segment and 18.4% for plug-in hybrids. Adding conventional hybrids, the electrified segment for the first time ever surpassed the combined annual registrations of cars only powered by gasoline or diesel, with a market share of 52.2% of new cars registrations in 2017.\n\nThe world's highest-ever monthly market share for the plug-in electric passenger segment in Norway, and in any country was achieved in September 2018 with a market share of 60.2% of new car registrations. Accounting for conventional hybrids, the electrified segment achieved a new record 71.5% market share.\n\n\nNorway was the first country in the world where plug-in electric cars have been listed among its top 10 best selling new cars in a given month, and the first one to have all-electric cars topping the new car sales monthly ranking. Since 2013, plug-in cars have topped the new car sales monthly ranking nine times. The Tesla Model S has been the top selling new car four times, twice in 2013, first in September and again in December, and one more time in March 2014, and again in March 2015. The Nissan Leaf has topped the monthly new car sales ranking twice, first in October 2013 and again in January 2014. Both the Nissan Leaf and the Tesla Model S were listed among the Norwegian top 20 best selling new cars in 2013, with the Leaf ranking third with 4,604 units and a 3.2% market share; and the Model S ranking 20th with a 1.4% market share of new car sales in 2013.\n\nIn March 2014, the Tesla Model S also broke the 28-year-old record for monthly sales of a single model regardless of its power source, with 1,493 units sold, surpassing the Ford Sierra, which sold 1,454 units in May 1986. In July 2016, when new car registrations are break down by type of powertrain, for the first time a plug-in hybrid, the Mitsubishi Outlander P-HEV, listed as the top selling new car. In September 2016, the Tesla Model X ranked as the top selling new car model in Norway when registrations are broken down by type of powetrain. The BMW i3 was the top selling new passenger car in November 2016.\n\nA total of 2,240 electric cars were sold in 2011, up from 722 in 2010. A total of 5,411 electric cars and vans were registered in the country at the end of 2011. Sales in 2011 were led by the Mitsubishi i-MiEV family with 1,477 units including 1,050 i-MiEVs, 217 Peugeot iOns and 210 Citroën C-Zeros, together representing 66% of electric car sales in Norway that year. All-time registrations were led by the Th!nk City with 1,216 units registered at the end of 2011, followed by the Kewet/Buddy with 1,125 units and the Mitsubishi i-MiEV with 1,050 units.\n\nAt the end of the first quarter of 2012 the Th!nk City (1,205 units) was surpassed as the all-time top selling electric car by the Mitsubishi i-MiEV (1,223), while registrations during this quarter were dominated by the Nissan Leaf with over 600 units registered. Registrations totaled 4,679 plug-in electric cars in 2012, including 318 plug-in hybrids and 59 electric vans. Plug-in electric-drive sales in 2012 represented a 3.1% market share of passenger car sales in the country, up from 1.6% in 2011. Registrations in 2012 included 300 imported used electric vehicles, representing 1.0% of total used imports in the country. Among the top selling countries of all-electric cars in 2012, Norway ranked 5th with a 7% market share of global EV sales.\n\nSales in 2012 were led by the Nissan Leaf with 2,487 units registered, including 189 imported used Leafs, and Leaf sales represented 53% of the plug-in segment sales that year. Cumulative sales reached 2,860 Leafs since its launch in September 2011, accounting for more than 5% of the Leaf's global sales. Norway was the first country in the world where an electric car ranked among the top 10 best selling cars, as the Nissan Leaf ranked 9th in October new car sales, and ended 2012 in the 13th place, representing a market share of 1.7% of all new car sales in the country, up from 0.3% in 2011. The other top selling models in 2012 were the Mitsubishi i-MiEV with 672 units (7 used imports), Citroën C-Zero 560 (47 used imports), and Peugeot iOn 477 (47 used imports), for a total of 1,709 i MiEV family cars registered. Since 2009, the i-MiEV family sold 3,147 new electric cars through December 2012.\n\nPlug-in electric car registrations totaled 10,769 units in 2013, of which used imports represented 20%. Total registrations included 387 plug-in hybrids and 355 all-electric light commercial vans, together representing 6.9% of total 2013 registrations, and reflecting the continued dominance of pure electric vehicles in the Norwegian market. The plug-in electric segment in Norway grew 129% from 2012 to 2013, achieving the second highest growth rate in the world after the Netherlands (338%).\n\nThe Nissan Leaf continued leading the Norwegian plug-in market with 4,604 new units sold in 2013, representing 58.4% of all plug-in car sales. The Tesla Model S ranked second with 1,986 units (25.2%), followed by the Volkswagen e-Up! with 580 units (7.4%). Since September 2011, a total of 7,275 new Leaf cars have been sold in the country through December 2013. Accounting for used Leafs imported from neighboring countries, of which, 1,608 units were registered during 2013, a total of 9,080 Leafs have been registered in Norway through December 2013, representing 9.4% of the 96,847 Leafs delivered worldwide through December 2013. The Toyota Prius Plug-in Hybrid was top selling plug-in hybrid in 2013 with 184 units, followed by the Opel Ampera and Volvo V60 Plug-in Hybrid, both with 94 units.\n\nTesla Model S deliveries began in Oslo on 7 August 2013, it was the first European retail delivery of a Model S. The first Model S was delivered to Frederic Hauge, a Norwegian environmental activist. Model S sales together with record Leaf sales, allowed the electric car segment to reach its best monthly sales and a record 6.0% market share of new passenger car sales in August 2013. Model S sales surged in September 2013, with a total of 616 units delivered, making the Tesla Model S the top selling car in Norway during this month, representing a market share of 5.1% of all the new cars sold in the country, and contributing to a record 8.6% market share for all-electric vehicle sales during September. The share climbs to 9.0% when plug-in hybrids and electric vans are accounted for. According to Reuters, the demand for the Model S is so high that there was a five-month waiting list, and as a result of the shortage, a used market has appeared.\n\nIn October 2013 an all-electric car was the best selling car in the country for a second month in a row. This time was the Nissan Leaf with 716 units sold, representing a 5.6% of new car sales. In December 2013, with 553 units sold and a 4.9% market share, the Model S was the top selling new car in the country for the second time in 2013. A total of 1,986 new Model S cars were sold through December 2013, allowing Tesla's electric car to rank as the second top selling electric vehicle in 2013 after the Nissan Leaf. According to Elon Musk, by the end of 2013 Norway became Tesla's largest per capita sales market for the Model S, together with Switzerland.\n\nA total of 23,390 plug-in electric vehicles were registered in Norway in 2014, consisting of 18,094 new all-electric cars, 3,063 used imported all-electric cars, 1,678 new plug-in hybrid cars and 555 new all-electric vans. Combined sales of new and used plug-in electric vehicles captured a 13.84% market share of total passenger car registrations in 2014. The new all-electric car segment reached a market share of 12.5%. New all-electric passenger car registrations were up 129.5% from 2013, and the plug-in hybrid segment grew 411.6% from a year earlier. Norway was the top selling European country with 18,649 passenger cars and utility vans registered, representing a third of all European all-electric car sales in 2014.\n\nIn January 2014, the Leaf topped for a second time the ranking of top selling new cars in Norway, with 650 units sold, representing a 5.7% of new car sales that month. Nissan Leaf registrations passed the 10,000 unit milestone in February 2014. The Model S topped the monthly sales ranking for a third time in March 2014, with 1,493 units sold, capturing a 10.8% market share of new car sales that month, and contributing to a record market share for the all-electric car segment of 20.3% of total new car sales. The monthly market share of the plug-in electric car segment set a new record in January 2014, 18.0% for all-electric cars and 3.1% for plug-in hybrids , for a combined market share of 21.1% of total new car registrations.\n\nA total of 2,056 Model S cars were sold during the first quarter of 2014, making the Model S the best selling new car in Norway during 2014 so far, capturing a 5.6% market share of new car sales. The Renault Zoe was officially launched in the Norwegian market in April 2014, and unlike other European countries, the Zoe is sold with the battery pack included.\n\nDuring the first half of 2014, the Model S, with 3,136 units sold, ranked as the second best selling new car in Norway with a market share of 4.3% of new car sales; and also was the top selling plug-in electric car, with a 33.5% share of the all-electric segment sales. The Leaf, with 2,665 units, ranked fourth among the top selling new cars, capturing a 3.7% market share of new car sales; and listed as the second top selling plug-in car after the Model S, with a share of 28.5% of the all-electric segment sales. The other top selling plug-in cars were the Volkswagen e-Up! with 1,551 units and 16.6% share of the all-electric segment; the BMW i3 with 1,159 units, including sales of the variant with the range-extender (REx) option, and captured a 12.4% share of the all-electric segment. The recently released Volkswagen e-Golf was the top selling plug-in electric car in July 2014 with 391 units sold and representing 34.4% of the Golf nameplate sales (1,136), which was country's top selling new car that month. The e-Golf was again the top selling plug-in electric car in August 2014 with 467 units sold, representing 43.4% of the Golf nameplate sales that month (1,075). In two months and a half a total of 925 Volkswagen e-Golf cars have been sold, surpassing initial Model S sales which delivered 805 units during its first two months in the Norwegian market.\n\nSales of plug-in hybrids increased significantly during the first half of 2014, with 856 units sold. Sales were driven by the Mitsubishi Outlander P-HEV with 818 units sold between January and June 2014, representing 95.6% of the Norwegian plug-in hybrid segment. Only 25 Volvo V60 Plug-in Hybrids, 21 Prius PHVs and 15 Amperas were sold during this period. The Outlander plug-in version represented almost 54% of the 1,523 Outlanders sold in Norway in the first half of 2014. The Outlander P-HEV passed the 1,000 unit mark in August 2014.\n\nPlug-in electric car sales in 2014 were led by the Nissan Leaf with 4,781 new registrations, followed by Tesla Model S with 4,040 units. The Leaf ended 2014 as the third top selling new car in Norway. The top selling plug-in hybrid in 2014 was the Mitsubishi Outlander P-HEV with 1,485 units sold, out of almost 1,700 plug-in hybrids sold in the country that year.\n\n, a total of 12,056 new Leafs had been sold in the country. In addition, there were 3,626 used imported Leafs registered in the country . With about 16,000 units registered including used imports, the Leaf ranks as the country's all-time top selling electric car, representing 39% of the country's all-electric registered fleet. The Tesla Model S, released in August 2013, ranks second with cumulative sales of 6,023 new units up until December 2014, with about 14% of the total registered plug-in electric vehicle stock. , Norway is the Model S largest overseas market, with an average of 436 sedans sold per month since August 2013.\n\nRecord registrations and the highest monthly market share ever were registered in March 2015, with 3,391 new all-electric cars sold that month representing 23.4% of new car sales, and 357 plug-in hybrids representing a market share of 2.52% that month, together reaching a combined PEV market share of 26.4%. In addition, a total of new 73 all-electric vans and 320 all-electric used import cars were registered in March 2015, raising total March registrations of light-duty plug-in vehicles to 4,141 units. March sales set another record, with three all-electric cars ranking as the top 3 selling new cars in the country, the Tesla Model S with 1,140 units, the Volkswagen e-Golf with 956 (out of a total of 1,421 units sold by the Golf nameplate), and the Nissan Leaf with 526.\n\nA total of 39,632 light-duty plug-in electric vehicles were registered in Norway in 2015, up from 23,408 in 2014 (69.3%). New plug-in sales totaled 34,455 units, consisting of 25,779 pure electric cars, 7,964 plug-in hybrids, and 712 all-electric utility vans. A total of 5,177 used imports were registered, consisting of 5,122 used imported pure electric cars and 55 vans. The combined sales of new plug-in cars reached a market share of 22.4% of all new passenger cars sold in 2015, with the all-electric car segment reaching 17.1%, up from 12.5% in 2014, while the plug-in hybrid segment reached 5.2%, up from 1% in 2014.\n\nThe VW e-Golf, with 8,943 units sold, was the best-selling plug-in electric car in Norway in 2015, representing 34.7% of the plug-in segment sales, ahead of the Tesla Model S (4,039) and the Nissan Leaf (3,189). The e-Golf variant represented 54.6% of total new VW Golf nameplate sales in the country in 2015. For the second year running, the Mitsubishi Outlander P-HEV was top selling plug-in hybrid in 2015 with 2,875 units, becoming the all-time top selling plug-in hybrid in the country, with 4,360 units registered since 2014. In 2015, the Outlander was followed by the Volkswagen Golf GTE with 2,000 units, and the Audi A3 e-tron with 1,684 units, together representing 84% of the plug-in hybrid segment sales in 2015.\n\n, the Nissan Leaf continued as the all-time best selling plug-in electric car in the country with a total of 15,245 new Leafs registered since 2011. In addition, a significant number of used imported Leafs from neighboring countries have been registered in the country, raising the stock of registered Leafs to over 20,000 units, meaning that more than 10% of Leafs sold in the world are on Norwegian roads by November 2015. Ranking second is the Volkswagen e-Golf, with 10,961 new units registered since 2014, followed by the Tesla Model S, with 10,062 new units registered in Norway through December 2015, representing about 10% of the Model S global sales.\n\nA total of 50,875 plug-in electric vehicles were registered in Norway in 2016, consisting of 24,222 new electric cars, 5,281 used imported all-electric cars, 20,663 new plug-in hybrid cars, 607 new all-electric vans, and 102 used imported all-electric vans. New light-duty plug-in registrations totaled 45,492 plug-in cars and vans registered. with new plug-in passenger car registrations were up 32% from 2015. Registrations of new plug-in cars reached a market share of 29.1% of all new passenger cars registered in 2016, with the all-electric car segment reaching 15.7%, down from 17.1% in 2015, and the plug-in hybrid segment climbed to 13.4%, up from 5.3% in 2015. When conventional hybrids sales are accounted for, the combined market share of the electric-drive segment achieved 40.2% of new passenger car sales in 2016.\n\nThe stock of light-duty plug-in electric vehicles registered in Norway passed the 100,000 unit milestone in April 2016, making Norway the country with the fourth largest stock of plug-in cars and vans in the world after China, the U.S. and Japan, and also the European country with largest stock of light-duty plug-in vehicles. Registrations of light-duty all-electric vehicles achieved the 100,000 unit milestone in December 2016, with three models, the Nissan Leaf, Tesla Model S and Volkswagen e-Golf, accounting for more than half of total stock of pure electric cars on Norwegian roads at the end of November 2016. Norway was the best selling plug-in country in Europe in 2016, surpassing the Netherlands, Europe’s top market in 2015.\n\nThe Outlander PHEV ended 2016 listed as the best selling plug-in car in Norway with 5,136 units sold, the first time ever a plug-in hybrid topped the Norwegian list of top selling plug-in electric cars. Ranking next were the Volkswagen e-Golf (4,705), Volkswagen Golf GTE (4,337), Nissan Leaf (4,162), and BMW i3 (3,953). Registrations of used imports were led by the Kia Soul (2,494) and the Nissan Leaf (2,112). When new car sales in 2016 are breakdown by powertrain or fuel, nine of the top ten best-selling models were electric-drive models: three plug-in hybrids, three battery electric cars, three conventional hybrids, and only one diesel-powered car.\n\nA record market share for the plug-in electric passenger segment was achieved in March 2016 with 33.5% of new car sales; the all-electric car segment had an 18.7% market share among new passenger cars, while the plug-in hybrid segment had a 14.8%. Also in March 2016, combined sales of the Golf plug-in variants totaled 1,216 units out of 1,411 new Golf nameplate units registered that month, representing 86.2% of the model total registrations.\n\nWhen new car registrations in July 2016 are broken down by type of powertrain, a total of five plug-in cars ranked among the top 10 best selling new cars in Norway that month, with the Mitsubishi Outlander P-HEV ranking for the first time as the top selling new car with 504 units registered in July 2016. Ranking next were the Volkswagen Golf GTE (412), Volkswagen Passat GTE (294), Volkswagen e-Golf (279), and Nissan Leaf (237). By the end of August 2016, about 90,000 pure electric vehicles have been registered in the country, including used imports, triggering the introduction of the new \"EK\" special license plate series dedicated to all-electric vehicles.\n\nThe VW Golf nameplate led new car registrations in September 2016 with 996 units, followed by the Tesla Model X with 601 and the BMW i3 with 520. However, when Golf family sales are broken down by each variant's powetrain, the all-electric e-Golf registered 392 units, the Golf GTE plug-in hybrid 358, and the internal combustion-powered Golf 242 units. Therefore, the Model X not only led sales in the plug-in electric segment, but also was the top selling new car model in September 2016. In addition, when models are ranked considering their powertrain, a total of five plug-in cars ranked among the top 10 best selling new cars in Norway that month. In addition to the Model X and the i3, the other top selling plug-in models were the Mitsubishi Outlander P-HEV (427), Volkswagen e-Golf (392), and Volkswagen Golf GTE (358). Again in November 2016, an electric car topped new cars sales in the country. The BMW i3 listed as the top selling new passenger car model with 1,014 units registered, capturing a market share of 7.7% of new car sales that month.\n\n, the Nissan Leaf remains as the all-time best selling plug-in electric car in the country with a total of 19,407 new Leafs registered since 2011. When used imported Leafs are accounted for, there were 27,115 Leafs on Norwegian roads at the end of November 2016. Ranking second is the VW e-Golf with 16,216 units registered followed by the Tesla Model S with 11,878 units. , the Outlander PHEV is the all-time top selling plug-in hybrid car with 9,499 new units registered since 2013.\n\nA record monthly market share for the plug-in electric passenger segment was achieved in January 2017 with 37.5% of new car sales; the plug-in hybrid segment had a 20.0% market share of new passenger cars, while the all-electric car segment had 17.5%. In January 2017 the electrified segment for the first time ever surpassed combined sales of cars with a diesel or gasoline engine. Sales of plug-in hybrids, all-electric cars and conventional hybrids achieved a market share of 51.4% of new car sales that month. Another market share record was set in December 2017, when the combined plug-in segment achieved 50% of new registrations, 27.6% for all-electric cars and 22.4% for plug-in hybrids. When conventional hybrids are accounted for, the electrified segment achieved a record 58.4% of new car registrations that month.\n\nA total of 71,682 plug-in electric vehicles were registered in Norway in 2017, consisting of 33,025 new electric cars (plus 55 new zero emissions hydrogen cars), 8,558 used imported all-electric cars, 29,236 new plug-in hybrid cars, 742 new all-electric vans, and 176 used imported all-electric vans. The all-electric segment market share was 20.8% and the plug-in hybrid car segment was 18.4%, for a combined market share of 39.2% of new passenger cars registered in 2017. Adding conventional hybrids, the electrified segment for the first time ever in any country surpassed annual registrations of cars only powered by gasoline or diesel, with a market share of 52.1% of new cars registrations in 2017.\n\nThe Volkswagen e-Golf was the best-selling plug-in electric car with 6,639 new units registered, followed by the BMW i3 with 5,035 (plus one unit with range-extender), and the Tesla Model X with 4,748 units. The Mitsubishi Outlander P-HEV ended 2017 with 4,067 new units registered, and rises to around 6,500 when used imports are accounted for. These sales results allow the Outlander P-HEV to rank as the top selling plug-in hybrid in 2017, and the Norwegian segment's top selling model for four years running (2014-2017). It is the all-time top selling PHEV with 13,566 new units sold .\n\nAmong the top 20 best selling new cars in Norway in 2017, half were plug-in passenger cars: six all-electric cars and four plug-in hybrids. The Volkswagen e-Golf was the country's top selling new car in 2017, and the BMW i3 the second best selling new car. The Tesla Model X ranked fourth overall. The complete Volkswagen Golf lineup was the top selling new car in 2017, completing ten consecutive years as the leading model in the Norwegian market, but 54.6% were electric models (e-Golf) and 20% were plug-in hybrids (Golf GTE), as a result, 3 out of 4 new VW Golf cars registered in 2017 had a plug. \n\nRegistration of used imports totaled 20,944 cars in 2017, mostly imported from European countries. Many of these imports were registered in their country of origin and shortly after shipped to Norway. A large proportion were electric cars, such as the Nissan Leaf and Kia Soul, and plug-in hybrids, mainly Mitsubishi Outlander. Accounting for total registrations, that is, including used imports, a total of 13 plug-in cars ranked among the top 20 best selling new cars in Norway in 2017, seven pure electrics and six plug-in hybrids. The list was completed by three conventional hybrids and three diesel-powered automobiles. Among the top 10, none was exclusively powered by an internal combustion engine, seven were plug-in cars and three conventional hybrids. The top four positions were occupied by plug-in models: VW e-Golf, Nissan Leaf, Mitsubishi Outlander P-HEV and BMW i3 all-electric. The Toyota RAV4 Hybrid was fifth.\n\n, two Norwegian cities are listed among the world's top 25 cities with the largest plug-in electric vehicle markets, accounting for 44% of the world’s stock of plug-in electric cars, Oslo with about 75,000 vehicles and Bergen with about 40,000. The top 25 list is led by Shanghai with cumulative sales of over 162,000 electric vehicles, followed by Beijing with 147,000 and Los Angeles with 143,000. Among the 25 cities, Bergen has the highest market share of the plug-in segment, with about 50% of new car sales in 2017, followed by Oslo with 40%.\n\nIn September 2018, the market share of all-electric cars reached 45.3% and plug-in hybrids 14.9%, for a combined market share of the plug-in car segment of 60.2% of new car registrations that month, becoming the world's highest-ever monthly market share for the plug-in electric passenger segment in Norway, and in any country. The market share for diesel-powered cars fell to 12.4% of new registrations and for gasoline cars was 16.1%. Accounting for conventional hybrids, the electrified segment achieved a record 71.5% market share in September 2018. The plug-in car market share continued above 55% in October and November 2018.\n\n, the Norwegian stock of light-duty plug-in electric vehicles registered in Norway totaled 273,559 units consisting of 183,637 all-electric passenger cars and vans, and 89,922 plug-in hybrids. Passenger cars totaled 2,728,043 units registered, of which, 178,521 (6.5%) were all-electric cars and 89,034 were plug-in hybrid cars (3.3%). Conventional hybrid cars totaled 103,108 units (3.8%), for the electrified segment to account for 13.7% of all registered passenger cars. In October 2018, Norway became the first country where 1 in every 10 passenger cars registered is a plug-in electric vehicle.\n\n, the Mitsubishi Outlander P-HEV remained as the top selling plug-in hybrid with 14,196 new units sold. , the Nissan Leaf continued to be the most popular model in the plug-in segment with 48,235 units registered, including over 15,000 used cars imported from neighboring countries, and representing over 25% of total all-electric car registrations in Norway.\n\nDuring the first quarter of 2016 gasoline-powered cars kept almost the same market share as 2015, while the share of diesel-powered cars declined by 8.9%, almost corresponding to the gain of the plug-in hybrid segment. In September 2016 the Norwegian electric-drive segment had achieved a record 47.8% market share of new cars sales that month.\n\nIn 2016 the more general category of hybrid electric cars, which in Norway includes plug-in hybrids, had a market share of 24.5% of new car sales, up from 12.4% in 2015. Accounting together the market shares of all-electric cars (15.7%), plug-in hybrids (13.4%), and conventional hybrids (11.1%), the Norwegian electric-drive segment achieved a record 40.2% market share of new cars sales in 2016. In contrast, the market share of new diesel-powered cars declined to 30.8% from 40.8% in 2015, and gasoline-powered cars had a 29.0% market share, slightly down from 29.6% in 2015. These trends indicate that the diesel segment, and in a lesser degree, the gasoline segment, are losing market share in favor of conventional hybrids and plug-in electric cars, particularly plug-in hybrids. Sales of plug-in cars were expected to overtake diesel-powered cars in Norway in 2017.\n\nIn January 2017 the electrified passenger car segment for the first time ever surpassed monthly combined sales of new cars with a conventional diesel or gasoline engine. Sales of plug-in hybrids, all-electric cars and conventional hybrids achieved a market share of 51.4% of new car sales. The diesel car segment excluding hybrids had a market share of 23.9%. A new record was set in December 2017, when the plug-in car segment achieved a monthly market share of 50% of new registrations, 27.6% for all-electric cars and 22.4% for plug-in hybrids, both individually surpassing the market share of the diesel car segment excluding hybrids (18.8%). Cars powered only by gasoline represented 22.7% of new car sales. The plug-in segment set a new record market share in 2017 with 39.2% of new passenger cars registered. The all-electric segment market share was 20.8% and the plug-in hybrid car segment was 18.4%, while the diesel-only segment was 23.1%, down from 30.8% in 2016, and gasoline-only was 24.7%, down from 29.0% in 2016. Adding conventional hybrids, the electrified segment for the first time ever in any country surpassed the combined annual registrations of cars only powered by gasoline or diesel, with a market share of 52.2% of new cars registrations in 2017.\n\nThe combined market share of the plug-in car segment in September 2018 rose to 60.2% of new car registrations, becoming the world's highest-ever monthly market share for the plug-in electric passenger segment in Norway, and in any country. The market share of all-electric cars was 45.3% and 14.9% for plug-in hybrids, while the market share for cars powered only by diesel fell to 12.4% and gasoline-only to 16.1%. Accounting for conventional hybrids, the electrified segment achieved a record 71.5% market share in September 2018.\n\nThe plug-in hybrid segment outsold all-electric cars for the first time ever in the month of April 2016. Registrations of new passenger plug-in vehicles during the first half of 2016 totaled 11,744 all-electric cars and 10,338 plug-in hybrids, with the all-electric car segment reaching a market share of 15.1%, down from 18.4% in the same period in 2015, while the plug-in hybrid segment reached a record 13.3%, up from 4.5% in 2015. These sales results for the first half of 2016 revealed a new trend in the Norwegian plug-in electric market. After years of spectacular growth, the market share of all-electric cars suffered a decline over the previous year, while the plug-in hybrid segment experienced significant growth. In terms of sales volume during the first half of 2016, for the first time plug-in hybrid registrations (10,338) were very close to all-electrics (11,744). Accounting for registrations during the first three quarters of 2016, plug-in hybrids grew nearly three-fold from the same period in 2016.\n\nAccording to the Norwegian Electric Vehicle Association this new trend is the result of uncertainty created by the government about the future incentives for zero-emission vehicles. Also, buyers have more models to choose from, as the number of plug-in hybrid cars available in the market has increased significantly, , there were 19 plug-in hybrid models in the market and 15 all-electric cars. Accounting for cumulative registrations between January and July 2016, four plug-in hybrids were listed among the top 10 selling plug-in electric cars, with a plug-in hybrid ranking for the first time ever in first place. As the trend of stronger plug-in hybrid sales continued in July 2016, the split between battery electric cars and plug-in hybrids almost reached parity, with 12,855 electric cars (51.3%) registered in the first seven months of 2016 compared to 12,203 plug-in hybrids registered in the same period (48.7%). In September 2016, the Norwegian Electric Vehicle Association proposed to the government to change the rules in the 2017 budget to limit the incentives for plug-in hybrids with insufficient all-electric range and luxury models featuring an electric powertrain with a small battery with the sole purpose of increasing power output.\n\nIn May 2018, European Federation for Transport and Environment, based on research by the EU Electromobility Platform, reported that there is evidence from Norway and other European countries suggesting that EV adoption is being hampered locally by insufficient supply of electric cars to match the existing level of demand. In the particular case of Norway, despite being a small country, demand is outstripping supply, and as a result, the waiting time for costumers wanting to buy a electric car is between 8 months and 2 years, while thousands have paid deposits to be on a waiting list for new models.\n\nIn October 2018, the Norwegian Electric Vehicle Association (\"Norsk Elbilforening\") reported that there is a supply shortage of some electric cars models already released, and also, there are many customers who already paid a deposit for reservation of a future model, for a combined total of about 30,000 electric car customers listed on different waiting lists.\n\n, the waiting list for models already released include the Tesla Model 3 (10,000), Hyundai Kona Electric (6,000), second generation Nissan Leaf (3,000), and Jaguar I-Pace (3,000). The waiting list for models slated for release in the Norwegian market in the near future include: the Audi e-tron quattro (6,300), Kia e-Niro (5,900), Porsche Taycan (2,300), Mercedes-Benz EQC (2,200), DS Crossback E-Tense (1,350), and BMW iX3 (1,000). Total paid reservations amount to more than 40,000 pre-orders, as some costumers signed in for more than one model. According to NRK estimates, the total amount paid by costumers in these waiting lists is about 400 million krone (about ). By monetary value, the waiting list is led by the Audi e-tron quattro and the Tesla Model 3.\n\nThe following table presents registrations of plug-in electric cars and utility vans by model per year since 2008 through December 2015. Figures between 2008 and 2013 correspond to the combined number of first registrations by year accounting for both, new and used imports. Figures for 2014 and 2015 correspond only to new car registrations. The breakdown between new and used import for the Nissan Leaf and the Kia Soul EV is shown because these are the two PEV models with the largest share of used import registrations (almost 70%).\n\nDespite having for many years the world's highest growth rates and highest penetration of EVs, Norway’s crude oil consumption for motor vehicles increased from 2013 to 2016, particularly dutiable diesel fuel. One factor in the slow decline of oil consumption is Norway’s relatively rapid population growth. Another factor is the significant number of gasoline- and diesel-powered vehicles still on the roads, with only 5% being plug-in electric passenger cars by the end of 2016.\n\nHowever, according to Forbes, government figures for the sales of petroleum products in 2017, show that for the first time since 2014 Norway's consumption of gasoline and diesel declined across the board in 2017. Motor gasoline sales declined by 2.9%, dutiable diesel fell by 2.7%, and duty-free diesel (used by agricultural equipment) declined by 2.6%. This decline follows oil sales that were flat in 2014, and then grew by 1% in 2015 and 3.2% in 2016.\n\nA 2009 European Union regulation set a mandatory average emissions target for new cars of 130 g/km, that were phased in between 2012 and 2015. A target of 95 g/km will apply from 2021. This regulation applies to the average fleet emissions of new passenger cars sold in the European Union and EEA member states. A car manufacturer who fails to comply has to pay an \"excess emissions premium\" for each car registered according with amount of g/km of exceedance. The average emissions level of a new car sold in 2017 was significantly below the 2015 target, at 118.5 grams of per km. Norway achieved in 2016 the European target set for 2021, with average emissions for all new passenger cars registered in 2016 of 93 g/km, down 7 g/km from 2015.\n\nIn order to reduce the country's greenhouse gas emissions, the Norwegian government pledged in 2012, among other measures, a target for the average fleet emission rate of new passenger cars of 85 g/km by 2020, 10 g/km lower than the European Commission's targets for 2021.\n\nAs a result of its fast growing EV market penetration, average fleet emissions have been falling in Norway from month to month. Average emissions for all new passenger cars registered in 2017 was 82 g/km, down from 93 g/km in 2016, and below the government's target of 85 grams set for 2020. With all-electric cars accounting for just over 5% of the total stock of passenger cars registered at the end of 2017, Norway achieved its transportation emissions target three years before the pledged deadline.\n\nIn September 2018, the average emissions from all first-time registered new passenger cars achieved a new record low of 55 g/km, down from 71 g/km in September 2017. An explanation of the new records, among other things, is found in the record sales of all-electric cars that took place in September 2018, when for the first time ever, zero emissions cars stood for almost half of passenger car sales for a month (45.3%). Such a large number of electric cars caused a record low average fleet emissions.\n\nSome car dealers in Norway have been importing new and used plug-in electric cars, in particular from Sweden, Denmark, Belgium, Netherlands, Germany, France, Italy and Spain. In the case of the Nissan Leaf, these dealers buy mostly new cars at a lower price than Norway's retail price thanks to the moderate demand for Leaf in other countries, where better price deals are offered. Then, the cars are sold in the Norwegian market up to 30,000 kr (~ ) cheaper than from Norway's dealerships. Official Norwegian dealers have raised questions about the kind of guarantee offered for the imports. Out of 1,412 all-electric cars registered in the country during the first quarter of 2013, 269 were used imports, representing 19% of all registrations during this quarter.\n\nIn September 2013, several French news outlets reported that according to the Norwegian newspaper Dagens Næringsliv, some car dealers in Norway have been buying electric cars in France and earning the (~ ) government subsidy. These cars are then imported to Norway and after discounting the freight costs, they are sold at a discount. Dagens Næringsliv cited the case of one dealer near Oslo with 70% of its electric car sales corresponding to vehicles imported from France, and with at least 40 Leafs imported, totaling ( ~ ) in benefits at a cost of the French taxpayers. These dealers are taking advantage of a loophole in the French law, which only requires to have an address in the country when buying a new car.\n\nAccording to Der Spiegel, by the early fourth quarter of 2015 the Kia Soul EV ranked as the top selling plug-in electric car in Germany during 2015 with 2,459 units sold, with almost 1,000 registered in October, nevertheless, there were actually only a few of them on German roads. At the time, about 1,400 Soul EVs had been shipped to Norway and sold as used cars, where availability of new Soul EVs is limited. According to the magazine, Kia Motors is registering the electric cars in Germany and then shipping them to Norway, which does not belong to the European Union, as a strategy to reduce the average fleet emissions of the entire Hyundai-Kia Group. This strategy allows the carmaker to comply with European Union regulations that mandate 130 grams of emission per km in 2015, and so they avoid to pay a fine of per year for each gram above the established average limit. According to German authorities this loophole is legal. A total of 2,044 Kia Soul EVs were imported to Norway as used cars during 2015.\n\n"}
{"id": "32321344", "url": "https://en.wikipedia.org/wiki?curid=32321344", "title": "Poland National Renewable Energy Action Plan", "text": "Poland National Renewable Energy Action Plan\n\nThe Poland National Renewable Energy Action Plan is the National Renewable Energy Action Plan (NREAP) for Poland. The plan was commissioned by the Directive 2009/28/EC which required Member States of the European Union to notify the European Commission with a road map. The report describes how Poland planned to achieve its legally binding target of a 15% share of energy from renewable sources in gross final consumption of energy by 2020. \nPoland national target for the share of renewable sources in gross final consumption of energy in 2020 is 15%. The expected total energy consumption in 2020 is 69,200 ktoe and hence the amount of energy from renewable sources in target year 2020 should be 10,380.5 ktoe. National Renewable Energy Action Plan sets a target of the share of renewable energies to be 19.13% in electricity sector, 17.05% in heating/cooling sector and 10.14% in transport sector by 2020.\n\na) Based on the Energy Act and secondary legislation to this Act, inter alia:\n\n\nb) Based on other legislation:\n\n\nc) Financial support to investments in RES provided in form of grants or borrowings and investment loans bearing low interest rate:\n\n\n\n"}
{"id": "23055", "url": "https://en.wikipedia.org/wiki?curid=23055", "title": "Potassium", "text": "Potassium\n\nPotassium is a chemical element with symbol K (from Neo-Latin \"kalium\") and atomic number 19. It was first isolated from potash, the ashes of plants, from which its name derives. In the periodic table, potassium is one of the alkali metals. All of the alkali metals have a single valence electron in the outer electron shell, which is easily removed to create an ion with a positive charge – a cation, which combines with anions to form salts. Potassium in nature occurs only in ionic salts. Elemental potassium is a soft silvery-white alkali metal that oxidizes rapidly in air and reacts vigorously with water, generating sufficient heat to ignite hydrogen emitted in the reaction, and burning with a lilac-colored flame. It is found dissolved in sea water (which is 0.04% potassium by weight), and is part of many minerals.\n\nPotassium is chemically very similar to sodium, the previous element in group 1 of the periodic table. They have a similar first ionization energy, which allows for each atom to give up its sole outer electron. That they are different elements that combine with the same anions to make similar salts was suspected in 1702, and was proven in 1807 using electrolysis. Naturally occurring potassium is composed of three isotopes, of which is radioactive. Traces of are found in all potassium, and it is the most common radioisotope in the human body.\n\nPotassium ions are vital for the functioning of all living cells. The transfer of potassium ions through nerve cell membranes is necessary for normal nerve transmission; potassium deficiency and excess can each result in numerous signs and symptoms, including an abnormal heart rhythm and various electrocardiographic abnormalities. Fresh fruits and vegetables are good dietary sources of potassium. The body responds to the influx of dietary potassium, which raises serum potassium levels, with a shift of potassium from outside to inside cells and an increase in potassium excretion by the kidneys.\n\nMost industrial applications of potassium exploit the high solubility in water of potassium compounds, such as potassium soaps. Heavy crop production rapidly depletes the soil of potassium, and this can be remedied with agricultural fertilizers containing potassium, accounting for 95% of global potassium chemical production.\n\nThe English name for the element \"potassium\" comes from the word \"potash\", which refers to an early method of extracting various potassium salts: placing in a \"pot\" the \"ash\" of burnt wood or tree leaves, adding water, heating, and evaporating the solution. When Humphry Davy first isolated the pure element using electrolysis in 1807, he named it \"potassium\", which he derived from the word potash.\n\nThe symbol \"K\" stems from \"kali\", itself from the root word \"alkali\", which in turn comes from \"\" \"al-qalyah\" \"plant ashes.\" In 1797, the German chemist Martin Klaproth discovered \"potash\" in the minerals leucite and lepidolite, and realized that \"potash\" was not a product of plant growth but actually contained a new element, which he proposed to call \"kali\". In 1807, Humphry Davy produced the element via electrolysis: in 1809, Ludwig Wilhelm Gilbert proposed the name \"Kalium\" for Davy's \"potassium\". In 1814, the Swedish chemist Berzelius advocated the name \"kalium\" for potassium, with the chemical symbol \"K\".\n\nThe English and French speaking countries adopted Davy and Gay-Lussac/Thénard's name Potassium, while the Germanic countries adopted Gilbert/Klaproth's name Kalium. The \"Gold Book\" of the International Union of Physical and Applied Chemistry has designated the official chemical symbol as K.\n\nPotassium is the second least dense metal after lithium. It is a soft solid with a low melting point, and can be easily cut with a knife. Freshly cut potassium is silvery in appearance, but it begins to tarnish toward gray immediately on exposure to air. In a flame test, potassium and its compounds emit a lilac color with a peak emission wavelength of 766.5 nanometers.\n\nNeutral potassium atoms have 19 electrons, one more than the extremely stable configuration of the noble gas argon. Because of this and its low first ionization energy of 418.8 kJ/mol, the potassium atom is much more likely to lose the last electron and acquire a positive charge than to gain one and acquire a negative charge (though negatively charged alkalide ions are not impossible). This process requires so little energy that potassium is readily oxidized by atmospheric oxygen. In contrast, the second ionization energy is very high (3052 kJ/mol), because removal of two electrons breaks the stable noble gas electronic configuration (the configuration of the inert argon). Potassium therefore does not form compounds with the oxidation state of +2 or higher.\n\nPotassium is an extremely active metal that reacts violently with oxygen in water and air. With oxygen it forms potassium peroxide, and with water potassium forms potassium hydroxide. The reaction of potassium with water is dangerous because of its violent exothermic character and the production of hydrogen gas. Hydrogen reacts again with atmospheric oxygen, producing water, which reacts with the remaining potassium. This reaction requires only traces of water; because of this, potassium and the liquid sodium-potassium (NaK) alloy are potent desiccants that can be used to dry solvents prior to distillation.\n\nBecause of the sensitivity of potassium to water and air, reactions with other elements are possible only in an inert atmosphere such as argon gas using air-free techniques. Potassium does not react with most hydrocarbons such as mineral oil or kerosene. It readily dissolves in liquid ammonia, up to 480 g per 1000 g of ammonia at 0 °C. Depending on the concentration, the ammonia solutions are blue to yellow, and their electrical conductivity is similar to that of liquid metals. In a pure solution, potassium slowly reacts with ammonia to form , but this reaction is accelerated by minute amounts of transition metal salts. Because it can reduce the salts to the metal, potassium is often used as the reductant in the preparation of finely divided metals from their salts by the Rieke method. For example, the preparation of magnesium by this method employs potassium as the reductant:\n\nThe only common oxidation state for potassium is +1. Potassium metal is a powerful reducing agent that is easily oxidized to the monopositive cation, . Once oxidized, it is very stable and difficult to reduce back to the metal.\n\nPotassium oxidizes faster than most metals and often forms oxides containing oxygen-oxygen bonds, as do all alkali metals except lithium. There are three possible oxides of potassium: potassium oxide (KO), potassium peroxide (KO), and potassium superoxide (KO); they contain three different oxygen-based ions: oxide (), peroxide (), and superoxide (). The latter two species, especially the superoxide, are rare and are formed only in reaction of very electropositive metals (Na, K, Rb, Cs, etc.) with oxygen; these species contain oxygen-oxygen bonds. All potassium-oxygen binary compounds are known to react with water violently, forming potassium hydroxide.\n\nPotassium hydroxide (KOH) is a very strong alkali, and up to 1.21 kg of it can dissolve in merely one liter of water. KOH reacts readily with carbon dioxide to produce potassium carbonate, and is used to remove traces of the gas from air.\n\nIn general, potassium compounds are highly ionic and, owing to the high hydration energy of the ion, have excellent water solubility. The main species in water solution are the aquated complexes where n = 6 and 7. The potassium ion is colorless in water and is very difficult to precipitate; possible precipitation methods include reactions with sodium tetraphenylborate, hexachloroplatinic acid, and sodium cobaltinitrite into potassium tetraphenylborate, potassium hexachloroplatinate, and potassium cobaltinitrite.\n\nThere are 24 known isotopes of potassium, three of which occur naturally: (93.3%), (0.0117%), and (6.7%). Naturally occurring has a half-life of 1.250×10 years. It decays to stable by electron capture or positron emission (11.2%) or to stable by beta decay (88.8%). The decay of to is the basis of a common method for dating rocks. The conventional K-Ar dating method depends on the assumption that the rocks contained no argon at the time of formation and that all the subsequent radiogenic argon () was quantitatively retained. Minerals are dated by measurement of the concentration of potassium and the amount of radiogenic that has accumulated. The minerals best suited for dating include biotite, muscovite, metamorphic hornblende, and volcanic feldspar; whole rock samples from volcanic flows and shallow instrusives can also be dated if they are unaltered. Apart from dating, potassium isotopes have been used as tracers in studies of weathering and for nutrient cycling studies because potassium is a macronutrient required for life.\n\nPotassium is formed in Supernovae by nucleosynthesis from lighter atoms. Potassium is principally created in Type II supernovae via an explosive oxygen-burning process. is also formed in s-process nucleosynthesis and the neon burning process.\n\nPotassium is the 20th most abundant element in the solar system and the 17th most abundant element by weight in the earth. It makes up about 2.6% of the weight of the earth's crust and is the seventh most abundant element in the crust. The potassium concentration in seawater is 0.39 g/L (0.039 wt/v%), about one twenty-seventh the concentration of sodium.\n\nPotash is primarily a mixture of potassium salts because plants have little or no sodium content, and the rest of a plant's major mineral content consists of calcium salts of relatively low solubility in water. While potash has been used since ancient times, it was not understood for most of its history to be a fundamentally different substance from sodium mineral salts. Georg Ernst Stahl obtained experimental evidence that led him to suggest the fundamental difference of sodium and potassium salts in 1702, and Henri Louis Duhamel du Monceau was able to prove this difference in 1736. The exact chemical composition of potassium and sodium compounds, and the status as chemical element of potassium and sodium, was not known then, and thus Antoine Lavoisier did not include the alkali in his list of chemical elements in 1789. For a long time the only significant applications for potash were the production of glass, bleach, soap and gunpowder as potassium nitrate. Potassium soaps from animal fats and vegetable oils were especially prized because they tend to be more water-soluble and of softer texture, and are therefore known as soft soaps. The discovery by Justus Liebig in 1840 that potassium is a necessary element for plants and that most types of soil lack potassium caused a steep rise in demand for potassium salts. Wood-ash from fir trees was initially used as a potassium salt source for fertilizer, but, with the discovery in 1868 of mineral deposits containing potassium chloride near Staßfurt, Germany, the production of potassium-containing fertilizers began at an industrial scale. Other potash deposits were discovered, and by the 1960s Canada became the dominant producer.\n\nPotassium \"metal\" was first isolated in 1807 by Sir Humphry Davy, who derived it from caustic potash (KOH, potassium hydroxide) by electrolysis of molten KOH with the newly discovered voltaic pile. Potassium was the first metal that was isolated by electrolysis. Later in the same year, Davy reported extraction of the metal sodium from a mineral derivative (caustic soda, NaOH, or lye) rather than a plant salt, by a similar technique, demonstrating that the elements, and thus the salts, are different. Although the production of potassium and sodium metal should have shown that both are elements, it took some time before this view was universally accepted.\n\nElemental potassium does not occur in nature because of its high reactivity. It reacts violently with water (see section Precautions below) and also reacts with oxygen. Orthoclase (potassium feldspar) is a common rock-forming mineral. Granite for example contains 5% potassium, which is well above the average in the Earth's crust. Sylvite (KCl), carnallite , kainite and langbeinite are the minerals found in large evaporite deposits worldwide. The deposits often show layers starting with the least soluble at the bottom and the most soluble on top. Deposits of niter (potassium nitrate) are formed by decomposition of organic material in contact with atmosphere, mostly in caves; because of the good water solubility of niter the formation of larger deposits requires special environmental conditions.\n\nPotassium is the eighth or ninth most common element by mass (0.2%) in the human body, so that a 60 kg adult contains a total of about 120 g of potassium. The body has about as much potassium as sulfur and chlorine, and only calcium and phosphorus are more abundant (with the exception of the ubiquitous CHON elements). Potassium ions are present in a wide variety of proteins and enzymes.\n\nPotassium levels influence multiple physiological processes, including\n\nPotassium homeostasis denotes the maintenance of the total body potassium content, plasma potassium level, and the ratio of the intracellular to extracellular potassium concentrations within narrow limits, in the face of pulsatile intake (meals), obligatory renal excretion, and shifts between intracellular and extracellular compartments.\n\nPlasma potassium is normally kept at 3.5 to 5.0 millimoles (mmol) [or milliequivalents (mEq)] per liter by multiple mechanisms. Levels outside this range are associated with an increasing rate of death from multiple causes, and some cardiac, kidney, and lung diseases progress more rapidly if serum potassium levels are not maintained within the normal range.\n\nAn average meal of 40-50 mmol presents the body with more potassium than is present in all plasma (20-25 mmol). However, this surge causes the plasma potassium to rise only 10% at most as a result of prompt and efficient clearance by both renal and extra-renal mechanisms.\n\nHypokalemia, a deficiency of potassium in the plasma, can be fatal if severe. Common causes are increased gastrintestinal loss (vomiting, diarrhea), and increased renal loss (diuresis). Deficiency symptoms include muscle weakness, paralytic ileus, ECG abnormalities, decreased reflex response; and in severe cases, respiratory paralysis, alkalosis, and cardiac arrhythmia.\n\nPotassium content in the plasma is tightly controlled by four basic mechanisms, which have various names and classifications. The four are 1) a reactive negative-feedback system, 2) a reactive feed-forward system, 3) a predictive or circadian system, and 4) an internal or cell membrane transport system. Collectively, the first three are sometimes termed the \"external potassium homeostasis system\"; and the first two, the \"reactive potassium homeostasis system\".\n\nRenal handling of potassium is closely connected to sodium handling. Potassium is the major cation (positive ion) inside animal cells [150 mmol/L, (4.8 g)], while sodium is the major cation of extracellular fluid [150 mmol/L, (3.345 g)]. In the kidneys, about 180 liters of plasma is filtered through the glomeruli and into the renal tubules per day. This filtering involves about 600 g of sodium and 33 g of potassium. Since only 1–10 g of sodium and 1–4 g of potassium are likely to be replaced by diet, renal filtering must efficiently reabsorb the remainder from the plasma.\n\nSodium is reabsorbed to maintain extracellular volume, osmotic pressure, and serum sodium concentration within narrow limits; potassium is reabsorbed to maintain serum potassium concentration within narrow limits. Sodium pumps in the renal tubules operate to reabsorb sodium. Potassium must be conserved also, but, because the amount of potassium in the blood plasma is very small and the pool of potassium in the cells is about thirty times as large, the situation is not so critical for potassium. Since potassium is moved passively in counter flow to sodium in response to an apparent (but not actual) Donnan equilibrium, the urine can never sink below the concentration of potassium in serum except sometimes by actively excreting water at the end of the processing. Potassium is excreted twice and reabsorbed three times before the urine reaches the collecting tubules. At that point, urine usually has about the same potassium concentration as plasma. At the end of the processing, potassium is secreted one more time if the serum levels are too high.\n\nWith no potassium intake, it is excreted at about 200 mg per day until, in about a week, potassium in the serum declines to a mildly deficient level of 3.0–3.5 mmol/L. If potassium is still withheld, the concentration continues to fall until a severe deficiency causes eventual death.\n\nThe potassium moves passively through pores in the cell membrane. When ions move through pumps there is a gate in the pumps on either side of the cell membrane and only one gate can be open at once. As a result, approximately 100 ions are forced through per second. Pores have only one gate, and there only one kind of ion can stream through, at 10 million to 100 million ions per second. The pores require calcium to open although it is thought that the calcium works in reverse by blocking at least one of the pores. Carbonyl groups inside the pore on the amino acids mimic the water hydration that takes place in water solution by the nature of the electrostatic charges on four carbonyl groups inside the pore.\n\nThe U.S. Institute of Medicine (IOM) sets Estimated Average Requirements (EARs) and Recommended Dietary Allowances (RDAs), or Adequate Intakes (AIs) for when there is not sufficient information to set EARs and RDAs. Collectively the EARs, RDAs, AIs and ULs are referred to as Dietary Reference Intakes. The AIs for potassium are: 400 mg of potassium for 0-6-month-old males, 700 mg of potassium for 7-12-month-old males, 3,000 mg of potassium for 1-3-year-old males, 3,800 mg of potassium for 4-8-year-old males, 4,500 mg of potassium for 9-13-year-old males, and 4,700 mg of potassium for males that are 14 years old and older.\nThe AIs for potassium are: 400 mg of potassium for 0-6-month-old females, 700 mg of potassium for 7-12-month-old females, 3,000 mg of potassium for 1-3-year-old females, 3,800 mg of potassium for 4-8-year-old females, 4,500 mg of potassium for 9-13-year-old females, and 4,700 mg of potassium for females that are 14 years old and older.\nThe AIs for potassium are: 4,700 mg of potassium for 14-50-year-old pregnant females; furthermore, 5,100 mg of potassium for 14-50-year-old lactating females. As for safety, the IOM also sets Tolerable upper intake levels (ULs) for vitamins and minerals, but for potassium the evidence was insufficient, so no UL established.\n\nMost Americans consume only half that amount per day.\n\nLikewise, in the European Union, in particular in Germany and Italy, insufficient potassium intake is somewhat common. However, the British National Health Service recommends a lower intake, saying that adults need 3,500 mg per day and that excess amounts may cause health problems such as stomach pain and diarrhoea.\n\nPotassium is present in all fruits, vegetables, meat and fish. Foods with high potassium concentrations include yam, parsley, dried apricots, milk, chocolate, all nuts (especially almonds and pistachios), potatoes, bamboo shoots, bananas, avocados, coconut water, soybeans, and bran.\n\nThe USDA lists tomato paste, orange juice, beet greens, white beans, potatoes, plantains, bananas, apricots, and many other dietary sources of potassium, ranked in descending order according to potassium content. A day's worth of potassium is in 5 plantains or 11 bananas.\n\nDiets low in potassium can lead to hypertension and hypokalemia.\n\nSupplements of potassium are most widely used in conjunction with diuretics that block reabsorption of sodium and water upstream from the distal tubule (thiazides and loop diuretics), because this promotes increased distal tubular potassium secretion, with resultant increased potassium excretion. A variety of prescription and over-the counter supplements are available. Potassium chloride may be dissolved in water, but the salty/bitter taste make liquid supplements unpalatable. Typical doses range from 10 mmol (400 mg), to 20 mmol (800 mg). Potassium is also available in tablets or capsules, which are formulated to allow potassium to leach slowly out of a matrix, since very high concentrations of potassium ion that occur adjacent to a solid tablet can injure the gastric or intestinal mucosa. For this reason, non-prescription potassium pills are limited by law in the US to a maximum of 99 mg of potassium.\n\nSince the kidneys are the site of potassium excretion, individuals with impaired kidney function are at risk for hyperkalemia if dietary potassium and supplements are not restricted. The more severe the impairment, the more severe is the restriction necessary to avoid hyperkalemia.\n\nA meta-analysis concluded that a 1640 mg increase in the daily intake of potassium was associated with a 21% lower risk of stroke. Potassium chloride and potassium bicarbonate may be useful to control mild hypertension.\n\nPotassium can be detected by taste because it triggers three of the five types of taste sensations, according to concentration. Dilute solutions of potassium ions taste sweet, allowing moderate concentrations in milk and juices, while higher concentrations become increasingly bitter/alkaline, and finally also salty to the taste. The combined bitterness and saltiness of high-potassium solutions makes high-dose potassium supplementation by liquid drinks a palatability challenge.\n\nPotassium salts such as carnallite, langbeinite, polyhalite, and sylvite form extensive evaporite deposits in ancient lake bottoms and seabeds, making extraction of potassium salts in these environments commercially viable. The principal source of potassium – potash – is mined in Canada, Russia, Belarus, Kazakhstan, Germany, Israel, United States, Jordan, and other places around the world. The first mined deposits were located near Staßfurt, Germany, but the deposits span from Great Britain over Germany into Poland. They are located in the Zechstein and were deposited in the Middle to Late Permian. The largest deposits ever found lie below the surface of the Canadian province of Saskatchewan. The deposits are located in the Elk Point Group produced in the Middle Devonian. Saskatchewan, where several large mines have operated since the 1960s pioneered the technique of freezing of wet sands (the Blairmore formation) to drive mine shafts through them. The main potash mining company in Saskatchewan is the Potash Corporation of Saskatchewan. The water of the Dead Sea is used by Israel and Jordan as a source of potash, while the concentration in normal oceans is too low for commercial production at current prices.\n\nSeveral methods are used to separate potassium salts from sodium and magnesium compounds. The most-used method is fractional precipitation using the solubility differences of the salts at different temperatures. Electrostatic separation of the ground salt mixture is also used in some mines. The resulting sodium and magnesium waste is either stored underground or piled up in slag heaps. Most of the mined potassium mineral ends up as potassium chloride after processing. The mineral industry refers to potassium chloride either as potash, muriate of potash, or simply MOP.\n\nPure potassium metal can be isolated by electrolysis of its hydroxide in a process that has changed little since it was first used by Humphry Davy in 1807. Although the electrolysis process was developed and used in industrial scale in the 1920s, the thermal method by reacting sodium with potassium chloride in a chemical equilibrium reaction became the dominant method in the 1950s.\n\nThe production of sodium potassium alloys is accomplished by changing the reaction time and the amount of sodium used in the reaction. The Griesheimer process employing the reaction of potassium fluoride with calcium carbide was also used to produce potassium.\n\nReagent-grade potassium metal costs about $10.00/pound ($22/kg) in 2010 when purchased by the tonne. Lower purity metal is considerably cheaper. The market is volatile because long-term storage of the metal is difficult. It must be stored in a dry inert gas atmosphere or anhydrous mineral oil to prevent the formation of a surface layer of potassium superoxide, a pressure-sensitive explosive that detonates when scratched. The resulting explosion often starts a fire difficult to extinguish.\n\nPotassium ions are an essential component of plant nutrition and are found in most soil types. They are used as a fertilizer in agriculture, horticulture, and hydroponic culture in the form of chloride (KCl), sulfate (), or nitrate (). Agricultural fertilizers consume 95% of global potassium chemical production, and about 90% of this potassium is supplied as KCl. The potassium content of most plants range from 0.5% to 2% of the harvested weight of crops, conventionally expressed as amount of . Modern high-yield agriculture depends upon fertilizers to replace the potassium lost at harvest. Most agricultural fertilizers contain potassium chloride, while potassium sulfate is used for chloride-sensitive crops or crops needing higher sulfur content. The sulfate is produced mostly by decomposition of the complex minerals kainite () and langbeinite (). Only a very few fertilizers contain potassium nitrate. In 2005, about 93% of world potassium production was consumed by the fertilizer industry.\n\nPotassium sodium tartrate (, Rochelle salt) is the main constituent of baking powder; it is also used in the silvering of mirrors. Potassium bromate () is a strong oxidizer (E924), used to improve dough strength and rise height. Potassium bisulfite () is used as a food preservative, for example in wine and beer-making (but not in meats). It is also used to bleach textiles and straw, and in the tanning of leathers.\n\nMajor potassium chemicals are potassium hydroxide, potassium carbonate, potassium sulfate, and potassium chloride. Megatons of these compounds are produced annually.\n\nPotassium hydroxide is a strong base, which is used in industry to neutralize strong and weak acids, to control pH and to manufacture potassium salts. It is also used to saponify fats and oils, in industrial cleaners, and in hydrolysis reactions, for example of esters.\n\nPotassium nitrate () or saltpeter is obtained from natural sources such as guano and evaporites or manufactured via the Haber process; it is the oxidant in gunpowder (black powder) and an important agricultural fertilizer. Potassium cyanide (KCN) is used industrially to dissolve copper and precious metals, in particular silver and gold, by forming complexes. Its applications include gold mining, electroplating, and electroforming of these metals; it is also used in organic synthesis to make nitriles. Potassium carbonate ( or potash) is used in the manufacture of glass, soap, color TV tubes, fluorescent lamps, textile dyes and pigments. Potassium permanganate () is an oxidizing, bleaching and purification substance and is used for production of saccharin. Potassium chlorate () is added to matches and explosives. Potassium bromide (KBr) was formerly used as a sedative and in photography.\n\nPotassium chromate () is used in inks, dyes, stains (bright yellowish-red color); in explosives and fireworks; in the tanning of leather, in fly paper and safety matches, but all these uses are due to the chemistry of the chromate ion, rather than the potassium ion.\n\nThere are thousands of uses of various potassium compounds. One example is potassium superoxide, , an orange solid that acts as a portable source of oxygen and a carbon dioxide absorber. It is widely used in respiration systems in mines, submarines and spacecraft as it takes less volume than the gaseous oxygen.\n\nAnother example is potassium cobaltinitrite, , which is used as artist's pigment under the name of Aureolin or Cobalt Yellow.\n\nThe stable isotopes of potassium can be laser cooled and used to probe fundamental and technological problems in quantum physics. The two bosonic isotopes possess convenient Feshbach resonances to enable studies requiring tunable interactions, while K is one of only two stable fermions amongst the alkali metals.\n\nAn alloy of sodium and potassium, NaK is a liquid used as a heat-transfer medium and a desiccant for producing dry and air-free solvents. It can also be used in reactive distillation. The ternary alloy of 12% Na, 47% K and 41% Cs has the lowest melting point of −78 °C of any metallic compound.\n\nMetallic potassium is used in several types of magnetometers.\n\nPotassium metal reacts violently with water producing potassium hydroxide (KOH) and hydrogen gas.\n\nThis reaction is exothermic and releases enough heat to ignite the resulting hydrogen in the presence of oxygen, possibly explosively splashing onlookers with potassium hydroxide, which is a strong alkali that destroys living tissue and causes skin burns. Finely grated potassium ignites in air at room temperature. The bulk metal ignites in air if heated. Because its density is 0.89 g/cm, burning potassium floats in water that exposes it to atmospheric oxygen. Many common fire extinguishing agents, including water, either are ineffective or make a potassium fire worse. Nitrogen, argon, sodium chloride (table salt), sodium carbonate (soda ash), and silicon dioxide (sand) are effective if they are dry. Some Class D dry powder extinguishers designed for metal fires are also effective. These agents deprive the fire of oxygen and cool the potassium metal.\n\nPotassium reacts violently with halogens and detonates in the presence of bromine. It also reacts explosively with sulfuric acid. During combustion, potassium forms peroxides and superoxides. These peroxides may react violently with organic compounds such as oils. Both peroxides and superoxides may react explosively with metallic potassium.\n\nBecause potassium reacts with water vapor in the air, it is usually stored under anhydrous mineral oil or kerosene. Unlike lithium and sodium, however, potassium should not be stored under oil for longer than six months, unless in an inert (oxygen free) atmosphere, or under vacuum. After prolonged storage in air dangerous shock-sensitive peroxides can form on the metal and under the lid of the container, and can detonate upon opening.\n\nBecause of the highly reactive nature of potassium metal, it must be handled with great care, with full skin and eye protection and preferably an explosion-resistant barrier between the user and the metal. Ingestion of large amounts of potassium compounds can lead to hyperkalemia, strongly influencing the cardiovascular system. Potassium chloride is used in the United States for lethal injection executions.\n\n\n"}
{"id": "13277180", "url": "https://en.wikipedia.org/wiki?curid=13277180", "title": "Priming (steam locomotive)", "text": "Priming (steam locomotive)\n\nPriming (foaming in North America) is a condition in the boiler of a steam locomotive in which water is carried over into the steam delivery. It may be caused by impurities in the water, which foams up as it boils, or simply too high a water level. It is harmful to the valves and pistons, as lubrication is washed away, and can be dangerous as any water collecting in the cylinders is not compressible and if trapped may fracture the cylinder head or piston.\n\nThe most frequent cause is running the locomotive with too high a level of water in the boiler and is most apparent when the regulator is opened sharply or steam demand is high. Thus, sensible locomotive management by the operators will help to prevent the occurrence. The phenomenon is particularly evident in areas of impure water, where boiled water creates a foam, or a mist of droplets, filling the space that collects steam at the top of the boiler, to be drawn down the steam collector pipe in the form of slugs of water. If boiler water is condensed and re-used, any oil or grease must be extracted as this form of contamination is particularly likely to give trouble.\n\nEarly designers fitted curved sheets below the steam collector pipe, but these were not successful as the whole of the steam space could contain foam. In districts where the feed water is unsuitable, blowdown valves (\"scum valves\"), either continuously working while the regulator is open or operated in conjunction with the boiler feed, are fitted. Valves at water level reduce surface scum; those towards the bottom of the boiler help remove precipitated solids. Other forms of prevention include the chemical treatment of water before it enters the boiler. In the event of priming (and also when steam is admitted through cold piping or into a cold cylinder) the operators need to open the cylinder cocks, which are designed to release trapped water. Once occurring, the problem can affect the level indicated in the boiler's gauge glass and for this reason is difficult to put right without reducing the water level to the extent that the firebox crown becomes dangerously exposed.\n\n\n"}
{"id": "24698826", "url": "https://en.wikipedia.org/wiki?curid=24698826", "title": "Reprocessed uranium", "text": "Reprocessed uranium\n\nReprocessed uranium (RepU) is the uranium recovered from nuclear reprocessing, as done commercially in France, the UK and Japan and by nuclear weapons states' military plutonium production programs. This uranium actually makes up the bulk of the material separated during reprocessing. Commercial LWR spent nuclear fuel contains on average (excluding cladding) only four percent plutonium, minor actinides and fission products by weight. Reuse of reprocessed uranium has not been common because of low prices in the uranium market of recent decades, and because it contains undesirable isotopes of uranium.\n\nGiven sufficiently high uranium prices, it is feasible for reprocessed uranium to be re-enriched and reused. A higher enrichment level is required to compensate for the U which is lighter than U and therefore concentrates in the enriched product.\nAlso, if fast breeder reactors ever come into commercial use, reprocessed uranium, like depleted uranium, will be usable in their breeding blankets.\n\nThere have been some studies involving the use of reprocessed uranium in CANDU reactors. CANDU is designed to use natural uranium as fuel; the U-235 content remaining in spent PWR/BWR fuel is typically greater than that found in natural uranium, which is about 0.72% U-235, allowing the re-enrichment step to be skipped. Fuel cycle tests also have included the DUPIC (Direct Use of spent PWR fuel In CANDU) fuel cycle, where used fuel from a Pressurized Water Reactor (PWR) is packaged into a CANDU fuel bundle with only physical reprocessing (cut into pieces) but no chemical reprocessing.\n\nAdvanced Fuel Cycle Cost Basis - Idaho National Laboratory\n"}
{"id": "173513", "url": "https://en.wikipedia.org/wiki?curid=173513", "title": "Road verge", "text": "Road verge\n\nA road verge is a strip of grass or plants, and sometimes also trees, located between a roadway (carriageway) and a sidewalk (pavement). Verges are known by dozens of other names, often quite regional; see Terminology, below.\n\nThe land is often public property, with maintenance usually being a municipal responsibility. Some municipal authorities, however, require that abutting property owners maintain their respective verge areas, as well as the adjunct footpaths or sidewalks.\n\nBenefits include visual aesthetics, increased safety and comfort of sidewalk users, protection from spray from passing vehicles, and a space for benches, bus shelters, street lights, and other public amenities. Verges are also often part of sustainability for water conservation or the management of urban runoff and water pollution and can provide useful wildlife habitat. Snow that has been ploughed off the street in colder climates often is stored in the area of the verge by default.\n\nIn the British Isles, verges are the last location of habitats for a range of flora. \n\nThe main disadvantage of a road verge is that the right-of-way must be wider, increasing the cost of the road. In some localities, a wider verge offers opportunity for later road widening, should the traffic usage of a road demand this. For this reason, footpaths are usually sited a significant distance from the curb.\nThe term \"verge\" has many synonyms and dialectal differences. Some dialects and idiolects lack a specific term for this area, instead using a circumlocution.\n\nTerms used include:\n\nIn urban and suburban areas, urban runoff from private and civic properties can be guided by grading and bioswales for rainwater harvesting collection and bioretention within the \"tree-lawn\" - parkway zone in rain gardens. This is done for reducing runoff of rain and domestic water: for their carrying waterborne pollution off-site into storm drains and sewer systems; and for the groundwater recharge of aquifers.\n\nIn some cities, such as Santa Monica, California, city code mandates specify: Parkways, the area between the outside edge of the sidewalk and the inside edge of the curb which are a component of the Public Right of Way (PROW) - that the landscaping should require little or no irrigation and the area produce no runoff. \n\nFor Santa Monica, another reason for this use of \"tree-lawns\" is to reduce current beach and Santa Monica Bay ocean pollution that is measurably higher at city outfalls. New construction and remodeling projects needing building permits require that landscape design submittals include garden design plans showing the means of compliance.\n\nIn some cities and counties, such as Portland, Oregon, street and highway departments are regrading and planting rain gardens in road verges to reduce boulevard and highway runoff. This practice can be useful in areas with either independent Storm sewers or combined storm and sanitary sewers, reducing the frequency of pollution, treatment costs, and released overflows of untreated sewage into rivers and oceans during rainstorms.\n\nIn some countries, the road verge can be a corridor of vegetation that remains after adjacent land has been cleared. Considerable effort in supporting conservation of the remnant vegetation is prevalent in Australia, where significant tracts of land are managed as part of the \"roadside conservation\" strategies by government agencies.\n\n\n"}
{"id": "32792727", "url": "https://en.wikipedia.org/wiki?curid=32792727", "title": "Robotic Refueling Mission", "text": "Robotic Refueling Mission\n\nThe Robotic Refueling Mission (RRM) is a NASA technology demonstration mission with equipment launches in both 2011 and 2013 to increase the technological maturity of in-space rocket propellant transfer technology by testing a wide variety of potential propellant transfer hardware, of both new and existing satellite designs.\n\nThe first phase of the mission was successfully completed in 2013. The second phase experiments continued in 2015.\n\nThe Robotic Refueling Mission was developed by the Satellite Servicing Capabilities Office at the Goddard Space Flight Center (GSFC). \nIt was planned to demonstrate the technology and tools to refuel satellites in orbit by robotic means. After the proof of concept, the long-term goal of NASA is to transfer the technology to the commercial sector.\n\nRRM was designed with four tools, each with electronics and two cameras and lights. Additionally, it had pumps and controllers and electrical systems such as electrical valves and sensors.\n\nThe RRM payload was transported to the Kennedy Space Center in early March 2011, where the GSFC team performed the final preparations for space flight.\nOnce up on the International Space Station, RRM was planned to be installed into the ELC-4. The Dextre robot was planned to be used in 2012 and 2013 during the refueling demonstration experiments.\n\nThe RRM phase 1 experiment platform was launched to the International Space Station (ISS) on 8 July 2011, transported by Space Shuttle \"Atlantis\" on STS-135, the 135th and final flight mission of the American Space Shuttle program.\n\nNASA successfully completed the phase 1 demonstration mission in January 2013, performing a series of robotic refuelings of satellite hardware that had not been designed for refueling . An extensive series of robotically-actuated propellant transfer experiments on the exposed facility platform of the International Space Station (ISS) were completed by the RRM equipment suite and the Canadarm/Dextre robotic arm combination.\n\nThe experiment suite included a number of propellant valves, nozzles and seals similar to those used on a wide variety commercial and U.S. government satellites, plus a series of four prototype tools that could be attached to the distal end of the Dextre robotic arm. Each tool was a prototype of a device that could be used by future satellite servicing missions to refuel spacecraft in orbit. RRM is the first in-space refueling demonstration using a platform of fuel valves and spacecraft plumbing representative of most existing satellites, which were not designed for refueling.\n\nPhase 2 of the RRM mission began in August 2013 with the launch of the phase 2 RRM hardware to the ISS aboard the Japanese H-II Transfer Vehicle 4 (HTV-4) for test operations expected to be carried out in 2014.\n\nThe Phase 2 hardware complement consists of:\n\nIn February 2014 the ground-based 'Remote Robotic Oxidizer Transfer Test' (RROxiTT) transferred nitrogen tetroxide (NTO) via a standard satellite-fueling valve at the satellite fuelling facility, Kennedy Space Center (KSC), using a robot controlled remotely from the Goddard Space Flight Centre, away in Greenbelt, Maryland.\n\nOn March 26, 2015 The RRM On-orbit Transfer Cage was loaded into the Kibo airlock and picked up by the JEM Robotic Arm who handed it off to Dextre for installation on the main module. \n\nOn April 30, 2015 The RRM On-Orbit Transfer Cage was installed on the main module and the Phase 1 hardware was removed and placed in the cage for disposal on HTV-4. The experiment was then activated that same day.\n\nFebruary 2016 the Phase 2 experiment was deactivated and all fuel and cooling lines were turned off in preparation for disposal of the RRM payload and its fuel on SpaceX Dragon CRS-10.\n\nOn February 23, 2017 The main module of the RRM experiment and the Phase 2 hardware were removed and stored in the trunk of SpaceX CRS-10 for disposal and the STP H5 experiment with Raven was activated beginning Phase 3.\n\nWith the completion of Phase 2 Phase 3 testing is underway with the launch of Raven on CRS-10. The new Phase 3 module will be delivered to the station in December 2018 on SpaceX CRS-16. \n\n\n"}
{"id": "48223695", "url": "https://en.wikipedia.org/wiki?curid=48223695", "title": "Rotating detonation engine", "text": "Rotating detonation engine\n\nA rotating detonation engine (RDE) is a proposed engine using a form of pressure gain combustion, where one or more detonations continuously travel around an annular channel. Although none are in production, computational simulations and experimental results have shown that the RDE has potential, and there is wide interest and research into the concept.\n\nTheoretically, detonative combustion, (i.e. that which happens at speeds above the speeds of sound), is more efficient than the conventional deflagrative combustion by as much as 25%. If this theoretical gain in efficiency can be realized, there would be a major fuel savings benefit. Because the combustion is supersonic, it can also more efficiently provide thrust at speeds above the speed of sound. \n\nThe disadvantages of the RDE include stability and noise.\n\nThe basic concept of an RDE is a detonation wave that travels around an annulus, which is a circular channel. First, fuel and oxidizer are injected into the channel, normally through small holes or slits. Initially, a detonation must be initiated in the fuel/oxidizer mixture by use of some form of ignitor. After the engine is started, however, the detonation is self-sustaining. The passing detonation ignites the fuel/oxidizer mixture, which releases the energy necessary to sustain the detonation. The combustion products expand out of the channel and are also pushed out of the channel by the incoming fuel and oxidizer. \n\nAlthough the RDE's design is similar to the pulse detonation engine (PDE), one aspect that makes the RDE superior is the fact that the waves constantly cycle around the chamber, while the PDE requires the chambers to be purged after each pulse.\n\nAn image of an RDE can be found on the American Institute of Aeronautics and Astronautics Pressure Gain Combustion Program Committee's Resources page.\n\nSeveral organisations have been working on RDE design. \n\nThe US Navy is pushing the development of this engine, but there is no known time of completion. Researchers at the Naval Research Laboratory (NRL) have been showing particular interest in detonation engines, such as the RDE, because they realize these engines have the capability to reduce the fuel consumption in their heavy vehicles. Several obstacles still remain to overcome in order to use the RDE in the field. NRL researchers are currently focusing on getting a better understanding of how the RDE works.\n\nSince 2010, Aerojet Rocketdyne has conducted over 520 tests of multiple configurations of a rotating detonation engine.\n\nResearchers at NASA also play a part in the development of the RDE. Daniel Paxson, an accredited scientist at the Glenn Research Center, has used simulations in computational fluid dynamics (CFD) to assess the RDE's detonation frame of reference and compares the performance with the PDE. He found that the results from his code of the RDE were shown to be in favor of those from a more complex code; thus, stating that the RDE can perform at least on the same level. Furthermore, he found that the performance of the RDE can be directly compared to the PDE as their performance were essentially the same.\n\nAccording to the Russian Vice Prime Minister Dmitry Rogozin, in mid-January 2018 NPO Energomash company successfully completed the initial test phase of the 2-ton class liquid propelant RDE and plans to develop larger ones for use in the space launch vehicles. \n\nOther experiments have used numerical procedures to better understand the flow-field of the RDE.\n\n"}
{"id": "4006161", "url": "https://en.wikipedia.org/wiki?curid=4006161", "title": "Sodium diuranate", "text": "Sodium diuranate\n\nSodium diuranate or yellow uranium oxide, NaUO·6HO, is a uranium salt also known as the yellow oxide of uranium. Sodium diuranate is commonly referred to by the initials SDU. Along with ammonium diuranate it was a component in early yellowcakes. The ratio of the two species is determined by process conditions; however, yellowcake is now largely a mix of uranium oxides. It was also used in porcelain dentures to give them a fluorescence similar to that of natural teeth and once used in pottery to produce ivory to yellow shades in glazes. It was added to these products as a mix with cerium oxide. The final uranium composition was from 0.008 to 0.1% by weight uranium with an average of about 0.02%. The practice appears to have stopped in the late 1980s.\n\nIn the classical procedure for extracting uranium, pitchblende is broken up and mixed with sulfuric and nitric acids. The uranium dissolves to form uranyl sulfate, and sodium hydroxide is added to make the uranium precipitate as sodium diuranate. This older method of extracting uranium from its uraninite ores has been replaced in current practice by such procedures as solvent extraction, ion exchange, and volatility methods.\n\nIn the past it was widely used to produce uranium glass or vaseline glass, the sodium salt dissolving easily into the silica matrix during the firing of the initial melt.\n\nThe alkaline process of milling uranium ores involves precipitating sodium uranate from the pregnant leaching solution to produce the semi-refined product referred to as yellowcake.\n\nSodium uranate may be obtained in the amorphous form by heating together urano-uranic oxide and sodium chlorate; or by heating sodium uranyl acetate or carbonate. The crystalline form is produced by adding the green oxide in small quantities to fused sodium chloride, or by dissolving the amorphous form in fused sodium chloride, and allowing crystallization to take place. It yields reddish-yellow to greenish-yellow prisms or leaflets.\n\n"}
{"id": "28138122", "url": "https://en.wikipedia.org/wiki?curid=28138122", "title": "Solar road stud", "text": "Solar road stud\n\nSolar road studs are flashing solar cell powered LED maintenance-free lighting devices used in road construction to delineate road edges and centrelines. Embedded in the road surface, they are an electronic improvement on the traditional cat's eyes in that they may give drivers more than a thirty-second reaction window compared with about 3 seconds for conventional reflective devices. The intense brightness of the LEDs makes them easily visible at distances of about 900 m under favourable conditions.\n\nAveraging about 100 mm square or 100 mm diameter and about 40 mm thick, units are extremely robust to avoid damage by passing vehicles, and are normally constructed of engineering plastics and polycarbonates. Use of solar road studs reduces the necessity of headlight main beams and the accompanying hazard of dazzling oncoming drivers. They are also more visible in rain and fog conditions where the old type retroreflectors and road markings are problematic. The solar cells charge batteries or capacitors during sunlit hours, over which period the flashing LEDs are turned off by a photoswitch.\n\n"}
{"id": "36498699", "url": "https://en.wikipedia.org/wiki?curid=36498699", "title": "Somali Airlines Flight 40", "text": "Somali Airlines Flight 40\n\nSomali Airlines Flight 40 was a scheduled domestic Somali Airlines flight on 20 July 1981 from Mogadishu to Hargeisa in Somalia. The aircraft crashed a few minutes after takeoff, and all 44 passengers and six crew on board were killed.\n\nOn 20 July 1981, Somali Airlines Flight 40, operated by a Fokker F27 Friendship, took off from Mogadishu's Mogadishu International Airport en route to Hargeisa International Airport in Hargeisa. It later returned to the Mogadishu airport for some repairs, before departing a second time. A few minutes after Flight 40 took off again, the aircraft entered an area of heavy rainfall. The flight crew subsequently lost control and crashed near the town of Balad. All 50 people on board were killed, the most fatalities in a single aircraft crash within Somali airspace.\n\nThe crash investigation determined that the aircraft had entered a spiral dive after encountering strong vertical gusts. Loads during the dive increased to approximately 5.76 g, exceeding the design stress limits of the Fokker F27 type and causing its right wing to separate. The flight crew were believed to have erred in taking off during known thunderstorm conditions.\n"}
{"id": "3722228", "url": "https://en.wikipedia.org/wiki?curid=3722228", "title": "Station Stones", "text": "Station Stones\n\nThe Station Stones are elements of the prehistoric monument of Stonehenge.\n\nOriginally there were four stones, resembling the four corners of a rectangle that straddles the inner sarsen circle, set just inside Stonehenge's surrounding bank. Two stood on earth mounds at opposing corners, one corner broadly in the north of the site and one in the south. The mounds are called the North and South barrows although they never contained burials. The ring ditches surrounding these barrows respect the presence of Stonehenge's encircling bank indicating that they postdate this feature. \n\nThe other two corners of the rectangle are occupied by the two surviving stones which are undressed sarsens. Their installation at the monument dates to sometime in Stonehenge phase 3, perhaps around 4,000 years ago. \n\nVarious astronomical alignments have been suggested for the stones, all involving other features at the site. As they cannot be said with certainty to have been contemporaneous with other stones or posts at Stonehenge, archaeoastronomical theories regarding their function have been treated with scepticism by mainstream archaeology. Although described as forming a rectangle, the two stones and the two stone settings can also be described as representing two opposite facets of an octagon. This suggests that they were laid out to a geometric plan and challenges the theory that the positions were astronomically determined. \n\n"}
{"id": "2951653", "url": "https://en.wikipedia.org/wiki?curid=2951653", "title": "Supercritical carbon dioxide", "text": "Supercritical carbon dioxide\n\nSupercritical carbon dioxide (s) is a fluid state of carbon dioxide where it is held at or above its critical temperature and critical pressure.\n\nCarbon dioxide usually behaves as a gas in air at standard temperature and pressure (STP), or as a solid called dry ice when frozen. If the temperature and pressure are both increased from STP to be at or above the critical point for carbon dioxide, it can adopt properties midway between a gas and a liquid. More specifically, it behaves as a supercritical fluid above its critical temperature () and critical pressure (), expanding to fill its container like a gas but with a density like that of a liquid.\n\nSupercritical is becoming an important commercial and industrial solvent due to its role in chemical extraction in addition to its low toxicity and environmental impact. The relatively low temperature of the process and the stability of also allows most compounds to be extracted with little damage or denaturing. In addition, the solubility of many extracted compounds in varies with pressure, permitting selective extractions.\n\nCarbon dioxide is gaining popularity among coffee manufacturers looking to move away from classic decaffeinating solvents, because of real or perceived dangers related to their use in food preparation. s is forced through the green coffee beans which are then sprayed with water at high pressure to remove the caffeine. The caffeine can then be isolated for resale (e.g. to the pharmaceutical or beverage manufacturers) by passing the water through activated charcoal filters or by distillation, crystallization or reverse osmosis. Supercritical carbon dioxide is used to remove organochloride pesticides and metals from agricultural crops without adulterating the desired constituents from the plant matter in the herbal supplement industry.\n\nSupercritical carbon dioxide can be used as a more environmentally friendly solvent for dry cleaning over traditional solvents such as hydrocarbons, including perchloroethylene.\n\nSupercritical carbon dioxide is used as the extraction solvent for creation of essential oils and other herbal distillates. Its main advantages over solvents such as hexane and acetone in this process are that it is non-toxic and non-flammable. Furthermore, separation of the reaction components from the starting material is much simpler than with traditional organic solvents. The can evaporate into the air or be recycled by condensation into a cold recovery vessel. Its advantage over steam distillation is that it operates at a lower temperature, which can separate the plant waxes from the oils.\n\nIn laboratories, s is used as an extraction solvent, for example for determining total recoverable hydrocarbons from soils, sediments, fly-ash and other media, and determination of polycyclic aromatic hydrocarbons in soil and solid wastes. Supercritical fluid extraction has been used in determining hydrocarbon components in water.\n\nProcesses that use s to produce micro and nano scale particles, often for pharmaceutical uses, are under development. The gas antisolvent process, rapid expansion of supercritical solutions and supercritical antisolvent precipitation (as well as several related methods) process a variety of substances into particles.\n\nDue to its ability to selectively dissolve organic compounds and assist the functioning of enzymes, s has been suggested as a potential solvent to support biological activity on Venus- or super-Earth-type planets.\n\nEnvironmentally beneficial, low-cost substitutes for rigid thermoplastic and fired ceramic are made using s as a chemical reagent. The s in these processes is reacted with the alkaline components of fully hardened hydraulic cement or gypsum plaster to form various carbonates. The primary byproduct is water.\n\nSupercritical carbon dioxide is used in the foaming of polymers. Supercritical carbon dioxide can saturate the polymer with solvent. Upon depressurization and heating the carbon dioxide rapidly expands, causing voids within the polymer matrix, i.e., creating a foam. Research is also ongoing at many universities in the production of microcellular foams using s.\n\nAn electrochemical carboxylation of a para-isobutylbenzyl chloride to ibuprofen is promoted under s.\n\nSupercritical is chemically stable, reliable, low-cost, non-toxic, non-flammable and readily available, making it a desirable candidate working fluid.\n\nThe unique properties of s present advantages for closed-loop power generation and can be applied to various power generation applications. Power generation systems that use traditional air Brayton and steam Rankine cycles can be upgraded to s to increase efficiency and power output.\n\nThe relatively new Allam power cycle uses sCO2 as the working fluid in combination with fuel and pure oxygen. The CO2 produced by combustion mixes with the sCO2 working fluid and a corresponding amount of pure CO2 must be removed from the process (for industrial use or sequestration). This process reduces atmospheric emissions to zero. \n\nIt presents interesting properties that promise substantial improvements in system efficiency. Due to its high fluid density, s enables extremely compact and highly efficient turbomachinery. It can use simpler, single casing body designs while steam turbines require multiple turbine stages and associated casings, as well as additional inlet and outlet piping. The high density allows for highly compact, microchannel-based heat exchanger technology.\n\nIn 2016, General Electric announced an s-based turbine that operated at 50% efficiency. In it the is heated to 700 °C. It requires less compression and allows heat transfer. It reaches full power in 2 minutes, whereas steam turbines need at least 30 minutes. The prototype generated 10 MW and is approximately 10% the size of a comparable steam turbine.\n\nFurther, due to its superior thermal stability and non-flammability, direct heat exchange from high temperature sources is possible, permitting higher working fluid temperatures and therefore higher cycle efficiency. And unlike two-phase flow, s’s single-phase nature eliminates the necessity of a heat input for phase change that is required for the water to steam conversion, thereby also eliminating associated thermal fatigue and corrosion.\n\nDespite the promise of substantially higher efficiency and lower capital costs, the use of s presents material selection and design issues. Materials in power generation components must display resistance to damage caused by high-temperature, oxidation and creep. Candidate materials that meet these property and performance goals include incumbent alloys in power generation, such as nickel-based superalloys for turbomachinery components and austenitic stainless steels for piping. Components within s Brayton loops suffer from corrosion and erosion, specifically erosion in turbomachinery and recuperative heat exchanger components and intergranular corrosion and pitting in the piping.\n\nTesting has been conducted on candidate Ni-based alloys, austenitic steels, ferritic steels and ceramics for corrosion resistance in s cycles. The interest in these materials derive from their formation of protective surface oxide layers in the presence of carbon dioxide, however in most cases further evaluation of the reaction mechanics and corrosion/erosion kinetics and mechanisms is required, as none of the materials meet the necessary goals.\n\nWork is underway to develop a s closed-cycle gas turbine to operate at temperatures near 550 °C. This would have implications for bulk thermal and nuclear generation of electricity, because the supercritical properties of carbon dioxide at above 500 °C and 20 MPa enable thermal efficiencies approaching 45 percent. This could increase the electrical power produced per unit of fuel required by 40 percent or more. Given the volume of carbon fuels used in producing electricity, the environmental impact of cycle efficiency increases would be significant.\n\nSupercritical is an emerging natural refrigerant, used in new, low carbon solutions for domestic heat pumps. Supercritical heat pumps are commercially marketed in Asia. EcoCute systems from Japan, developed by Mayekawa, develop high temperature domestic water with small inputs of electric power by moving heat into the system from the surroundings.\n\nSupercritical has been used since the 1980s to enhance recovery in mature oil fields.\n\n\"Clean coal\" technologies are emerging that could combine such enhanced recovery methods with carbon sequestration. Using gasifiers instead of conventional furnaces, coal and water is reduced to hydrogen gas, carbon dioxide and ash. This hydrogen gas can be used to produce electrical power In combined cycle gas turbines, is captured, compressed to the supercritical state and injected into geological storage, possibly into existing oil fields to improve yields. The unique properties of s ensure that it remains out of the atmosphere.\n\nSupercritical could be used as a working fluid in enhanced geothermal systems. Possible advantages compared to water include higher energy yield resulting from its lower viscosity, better chemical interaction, storage through fluid loss and higher temperature limit. As of 2011, the concept had not been tested in the field.\n\nSupercritical carbon dioxide is used in the production of silica, carbon and metal based aerogels. For example, silicon dioxide gel is formed and then exposed to s. When the goes supercritical, all surface tension is removed, allowing the liquid to leave the aerogel and produce nanometer sized pores.\n\nSupercritical is an alternative for terminal sterilization of biological materials and medical devices with combination of the additive peracetic acid (PAA). Supercritical does not sterilize the media, because it does not kill the spores of microorganisms. Moreover, this process is gentle, as the morphology, ultrastructure and protein profiles of inactivated microbes are preserved.\n\nSupercritical is used in certain industrial cleaning processes.\n\n\nMukhopadhyay M. \"Natural extracts using supercritical carbon dioxide\". United States: CRC Press, LLC; 2000 Free preview at Google Books\n"}
{"id": "3105122", "url": "https://en.wikipedia.org/wiki?curid=3105122", "title": "TERON (Tillage erosion)", "text": "TERON (Tillage erosion)\n\nTERON is a foundation dedicated to the assessment of tillage related erosion in Europe.\n\n\n"}
{"id": "20948660", "url": "https://en.wikipedia.org/wiki?curid=20948660", "title": "Turkish Airlines Flight 158", "text": "Turkish Airlines Flight 158\n\nTurkish Airlines Flight 158 was a scheduled domestic passenger flight from Istanbul Yeşilköy Airport to Ankara Esenboğa Airport, Turkey. On 16 January 1983, the aircraft operating the flight, a Boeing 727-200, landed about short of the runway at its destination airport in driving snow, broke up and caught fire. Of the 67 occupants on board, 47 perished.\n\nThe aircraft, a Boeing 727-2F2 with three Pratt & Whitney JT8D-15 turbofan jet engines, was built by Boeing with manufacturer serial number 21603/1389, and made its first flight in 1978.\n\nThe aircraft had 7 crew and 60 passengers on board. 47 passengers were killed, all the crew and 13 passengers survived the accident with injuries.\n"}
{"id": "8442921", "url": "https://en.wikipedia.org/wiki?curid=8442921", "title": "Zero Power Physics Reactor", "text": "Zero Power Physics Reactor\n\nThe Zero Power Physics Reactor or ZPPR (originally named Zero Power Plutonium Reactor) was a nuclear reactor located at the Idaho National Laboratory, Idaho, USA.\n\nZPPR ran only at extremely low power, for testing nuclear reactor designs. ZPPR was operated as a critical facility from April 18, 1969 until 1990.\n\n"}
