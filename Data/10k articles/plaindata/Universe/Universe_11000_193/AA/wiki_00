{"id": "27451282", "url": "https://en.wikipedia.org/wiki?curid=27451282", "title": "1997 Central European flood", "text": "1997 Central European flood\n\nThe 1997 Central European flood or the 1997 Oder Flood of the Oder and Morava river basins in July 1997 affected Poland, Germany, and the Czech Republic, taking the lives of about 114 people (in the Czech Republic and Poland) and causing material damages estimated at $4.5 billion (3.8 billion euros in the Czech Republic and Poland and 330 million euros in Germany). The flooding began in the Czech Republic, then spread to Poland and Germany. In Poland, where it was one of the most disastrous floods in the history of that country, it was named the Millennium Flood (\"Powódź tysiąclecia\"). The term was also used in Germany (\"Jahrtausendflut\"). The flood has also been referred to as the Great Flood of 1997.\n\nSouthwestern Poland and the northern Czech Republic experienced two periods of extensive rainfall, the first occurring 3–10 July and the second 17–22 July. The precipitation was caused by a Genoa low pressure system, which moved from northern Italy to Moravia and Poland. The unusual development occurred when the field of higher air pressure between the Azores Islands and Scandinavia was blocked. The center of the low pressure remained over southern Poland for a long period.\n\nThe precipitation was very high, measuring , and corresponded to several months' average rainfall over a few days. The waters rose 2–3 m above the previously recorded averages and were so high that they flooded over standing measurement poles. It was one of the heaviest rainfalls in the recorded world's history. It was dubbed the Millennium Flood because the likelihood of such a flood in a particular year was estimated at 0.1%.\n\nFlooding began on July 5 in the Czech Republic and spread to Poland on July 6. Those early floods were very rapid flash floods (water levels rose by up to four meters in half a day). In Poland, the first towns flooded were located around Głuchołazy, and were visited by Polish Prime Minister Włodzimierz Cimoszewicz on July 7. Flooding spread rapidly from Chałupki to Racibórz. In Kłodzko several buildings dating back a few hundred years (kamienica) collapsed; on 8 July the flood reached Krapkowice. In the second stage of the flood, the flood wave flowed down through the Oder river, submerging successive towns in the area. Left-bank Opole was flooded on July 10, Wrocław and Rybnik on July 12, and Głogów soon after. The rising waters slowed by the time they reached the Polish-German border (the Oder-Neisse line), allowing more time for preparations; the damages were thus much lower.\n\nOn 18 July, Polish president Aleksander Kwaśniewski declared a day of national mourning.\n\nWater levels recorded on the Oder river in the flood period:\n\nThe flood caused the deaths of 114 people (56 in Poland, 58 in the Czech Republic) and material damages estimated at $4.5 billion (3.8 billion euros in the Czech Republic and Poland and 330 million euros in Germany).\n\nIn Poland, it is estimated that 7,000 people lost all of their possessions. 9,000 private businesses were affected and 680,000 houses were damaged or destroyed. The flood also damaged 843 schools (100 destroyed), 4,000 bridges (45 destroyed), 14,400 km of roads, 2,000 km of railways. In total, 665,835 hectares were affected in Poland (an estimated 2% of Polish total territory). The losses were estimated at 63 billion Polish zloties (or US$2.3-3.5 billion at the 1997 levels). The town of Kłodzko sustained damages equivalent to 50 years of its annual budget.\n\nIn the Czech Republic, there were 50 fatalities (another source gives 60). 2151 flats and 48 bridges were destroyed. 538 villages and towns were affected. The losses were estimated at 63 billion Czech korunas. The town of Troubky was most severely affected.\n\nIn Germany there were no fatalities.\n\nGovernment responses in Czech Republic and Poland were criticized. The flood revealed various inadequacies in decision making and infrastructure, although the unprecedented magnitude of the disaster was seen by some as a mitigating factor.\n\nNumerous charities provided aid to those affected by the floods.\n\n\n"}
{"id": "17476975", "url": "https://en.wikipedia.org/wiki?curid=17476975", "title": "Annette Malm Justad", "text": "Annette Malm Justad\n\nAnnette Malm Justad (born 1958) is a Norwegian businessperson.\n\nAs education she has two master's degrees, one in technology management from the MIT Sloan School of Management and the Norwegian Institute of Technology, and another in chemical engineering from the Norwegian Institute of Technology. She was employed at Norsk Hydro, as Vice President and Fleet Manager at Norgas Carriers and Vice President of Yara International before she became CEO of Eitzen Maritime Services.\n\nMalm Justad is a member of the board of directors at Petroleum Geo-Services, and is a member of the board of Camillo Eitzen & Co and Aker American Shipping. She is a non-executive director of the Port of London Authority.\n\n"}
{"id": "36507868", "url": "https://en.wikipedia.org/wiki?curid=36507868", "title": "Artifact Puzzles", "text": "Artifact Puzzles\n\nArtifact Puzzles is a manufacturer of wooden jigsaw puzzles operating out of Menlo Park California in Silicon Valley. The business was founded in 2009 by University of Washington electrical engineering professor Maya Gupta, and was originally based in Seattle, Washington. Unlike traditional wooden jigsaw puzzles which are hand-cut by jigsaw, Artifact Puzzles laser-cuts 1/4\" thick 3-ply plywood.\n\nThe puzzle pieces are designed by an artist for each new puzzle, and do not follow a consistent style of cut. For example, some of their puzzles have traditional knob connectors, while others have piece connectors shape like clouds, hearts, bird feet, horse hooves, and ancient Greek symbols. Like traditional wooden jigsaw puzzles, most of their puzzles have \"whimsy pieces\", which are pieces shaped like recognizable objects. These pieces are designed to match the theme of each puzzle, and range from a cow jumping over a moon in one of their Daniel Merriam puzzles, to pieces shaped like ballerinas in their Degas puzzle. The company has designed and manufactures over 250 different puzzles of a broad range of art, with an unusually large selection of whimsical neo-surrealist art and 16th century art.\n"}
{"id": "47262372", "url": "https://en.wikipedia.org/wiki?curid=47262372", "title": "Ben R. Pollner", "text": "Ben R. Pollner\n\nBenjamin R. Pollner (born in 1941 in New York) is an American businessman. He is the founder of Taurus Petroleum, based in Geneva, created in 1993, and remains its chairman and chief executive officer.\n\nIn 1963, he graduated from Wagner College with a B.Sc. in Chemistry. Between 1965 and 1968 he attended the New York University Graduate School of Business Administration.\n\nBen Pollner held various positions in industrial companies until joining the Steuber Corporation in 1970 when he commenced his international marketing, transport and trading activities in the bulk petrochemical industry. In the 1990s, he worked for David B. Chalmers Jr at Bayoil Petroleum. Pollner is a member of a small group of top oil and mineral traders who were under the tutelage of one-time fugitive financier Marc Rich.\n\nPollner's company, Taurus Petroleum was responsible for over $4 billion in Oil-for-Food deals with Iraq. Under the program, oil was purchased from Iraq and resold to large refiners in exchange for food.\nPollner, through his company Taurus Petroleum, purchased the majority of the oil from two companies in Geneva, Switzerland, Fenar Petroleum and Alcon Petroleum (both registered in Liechtenstein in 1999). Fenar and Alcon were among the largest oil purchasers under the Oil-for-Food program exporting a combined $2.47 billion in crude oil. It was alleged that Pollner in fact, either controlled or owned both companies. Fenar and Alcon were alleged to have funneled $18.9 million in kickbacks to the regime of Saddam Hussein, in the form of surcharges on oil purchases. Taurus has denied paying the surcharges. In response to the investigation, Pollner stated: \"I did nothing in New York or the U.S. that would be considered illegal.\" Pollner was never indicted.\n\nTaurus Petroleum also functions as a broker of oil from Ecuador, working in conjunction with PetroChina.\n\nPollner has been a member of the American Petroleum Institute (API), the National Petrochemical and Refiners Association (NPRA) and the European Petrochemical Association.\n\nBen Pollner is active in various charitable functions including the T. Anthony Pollner Distinguished Professorship and FACES (Finding a Cure for Epilepsy and Seizures).\n\nHe was married to Alice Thorpe. They had three children, Edward J. Pollner (married to Lisa) of Fairfield, Connecticut, Amy P. Moritz (married to Sasha) of New York, NY and T. Anthony Pollner (who died in a motorcycle accident in 2001).\n"}
{"id": "1987878", "url": "https://en.wikipedia.org/wiki?curid=1987878", "title": "Bentwood", "text": "Bentwood\n\nBentwood objects are those made by wetting wood (either by soaking or by steaming), then bending it and letting it harden into curved shapes and patterns.\n\nIn furniture making this method is often used in the production of rocking chairs, cafe chairs, and other light furniture. The iconic No. 14 chair by Thonet is a well-known design based on the technique. The process is in widespread use for making casual and informal furniture of all types, particularly seating and table forms. It is also a popular technique in the worldwide production of furniture with frames made of heavy cane, which is commonly imported into European and Western shops.\nBentwood boxes are a traditional item made by the First Nations people of the North American west coast including the Haida, Gitxsan, Tlingit, Tsimshian, Sugpiaq, Unangax, Yup'ik, Inupiaq and Coast Salish. These boxes are generally made out of one piece of wood that is steamed and bent to form a box. Traditional uses of the boxes was varied and included storage of food goods, clothing and for burial. They were often without decoration while others were decorated elaborately. Today many are made for collectors and can be purchased from museums, gift shops and online sites as well as directly commissioned from the artists.\n\nThe Aleut or Unangan People of Alaska made hunting visors, called \"chagudax\", out of driftwood using the bentwood method. The visors were used by hunters who were in kayaks. They are said to help keep the sea spray off the face as well as improve hearing. They were often decorated with paints, beads, sea lion whiskers and ivory figurines. Andrew Gronholdt is credited with reviving the art of chagudax carving in the 1980s. Present day Unangan artists create chagudax for ceremonial purposes and offer them for sale to the public as well.\n\n\n"}
{"id": "35527682", "url": "https://en.wikipedia.org/wiki?curid=35527682", "title": "Bolshaya Kokshaga (nature reserve)", "text": "Bolshaya Kokshaga (nature reserve)\n\nBolshaya Kokshaga nature reserve (Госуда́рственный приро́дный запове́дник «Больша́я Кокша́га») — nature reserve in Kilemarsky District and Medvedevsky District, Mari-El, Russia.\n"}
{"id": "55000922", "url": "https://en.wikipedia.org/wiki?curid=55000922", "title": "Bursa Energy Museum", "text": "Bursa Energy Museum\n\nBursa Merinos Energy Museum () is a technology museum dedicated to electricity, which was established in 2012 in the defunct power plant of an abandoned textile factory in Bursa, northwestern Turkey.\n\nThe museum is situated at on Dr. Sadık Ahmet Boulevard in Osmangazi district.\n\nMerinos factory was one of the state-owned factories established in 1938. It was a textile factory using the wool of merino. With additions in 1944 and 1946 it became the biggest factory of its kind in Balkan and Mideastern countries. However, in 2004 the factory was closed within the scope of privatization program. Its land lot together with the infrastructure was handed over to the Bursa Metropolitan Municipality. \n\nThe factory had its own energy plant with a ground area of . On 7 September 2012, the municipality established an energy museum. In the museum, the role of electricity in the civilization is emphasized. The history of electricity in the World, in Turkey and in Bursa, the illumination gadgets and the production of electricity are shown by the photographs and animations.\n"}
{"id": "41570222", "url": "https://en.wikipedia.org/wiki?curid=41570222", "title": "Cheapium", "text": "Cheapium\n\nCheapium is a term used for several theoretically stable binary platinum-group combinations sifted from thousands of potential platinum-group compounds using supercomputers, databases, and algorithms. By using theories on how atoms interact with model chemical structures to automate new compounds testing, years of laboratory research and money can be potentially saved to discover cheaper (thus the name Cheapium) alternatives to expensive currently known platinum based compounds. The term was coined by materials scientist Stefano Curtarolo as a mixture of inexpensive elements (cheap) having chemical, physical or structural property equivalent to an expensive and rare element (Expensium).\n"}
{"id": "69837", "url": "https://en.wikipedia.org/wiki?curid=69837", "title": "Committee on Climate Change Science and Technology Integration", "text": "Committee on Climate Change Science and Technology Integration\n\nThe Committee on Climate Change Science and Technology Integration was created as part of the Clear Skies Initiative in February 2002 by George W. Bush, as a Cabinet-level effort to coordinate climate change science and technology research.\n\nThe White House says:\n"}
{"id": "390883", "url": "https://en.wikipedia.org/wiki?curid=390883", "title": "Electric locomotive", "text": "Electric locomotive\n\nAn electric locomotive is a locomotive powered by electricity from overhead lines, a third rail or on-board energy storage such as a battery or a supercapacitor.\n\nElectric locomotives with on-board fueled prime movers, such as diesel engines or gas turbines, are classed as diesel-electric or gas turbine-electric and not as electric locomotives, because the electric generator/motor combination serves only as a power transmission system.\n\nElectric locomotives benefit from the high efficiency of electric motors, often above 90% (not including the inefficiency of generating the electricity). Additional efficiency can be gained from regenerative braking, which allows kinetic energy to be recovered during braking to put power back on the line. Newer electric locomotives use AC motor-inverter drive systems that provide for regenerative braking. Electric locomotives are quiet compared to diesel locomotives since there is no engine and exhaust noise and less mechanical noise. The lack of reciprocating parts means electric locomotives are easier on the track, reducing track maintenance. Power plant capacity is far greater than any individual locomotive uses, so electric locomotives can have a higher power output than diesel locomotives and they can produce even higher short-term surge power for fast acceleration. Electric locomotives are ideal for commuter rail service with frequent stops. Electric locomotives are used on freight routes with consistently high traffic volumes, or in areas with advanced rail networks. Power plants, even if they burn fossil fuels, are far cleaner than mobile sources such as locomotive engines. The power can also come from clean or renewable sources, including geothermal power, hydroelectric power, nuclear power, solar power and wind turbines.\n\nThe chief disadvantage of electrification is the high cost for infrastructure: overhead lines or third rail, substations, and control systems. Public policy in the U.S. interferes with electrification: higher property taxes are imposed on privately owned rail facilities if they are electrified. The EPA regulates exhaust emissions on locomotive and marine engines, similar to regulations on car & freight truck emissions, in order to limit the amount of carbon monoxide, unburnt hydrocarbons, nitric oxides, and soot output from these mobile power sources. Because railroad infrastructure is privately owned in the U.S., railroads are unwilling to make the necessary investments for electrification. In Europe and elsewhere, railway networks are considered part of the national transport infrastructure, just like roads, highways and waterways, so are often financed by the state. Operators of the rolling stock pay fees according to rail use. This makes possible the large investments required for the technically and, in the long-term, also economically advantageous electrification.\n\nThe first known electric locomotive was built in 1837 by chemist Robert Davidson of Aberdeen, and it was powered by galvanic cells (batteries). Davidson later built a larger locomotive named \"Galvani\", exhibited at the Royal Scottish Society of Arts Exhibition in 1841. The seven-ton vehicle had two direct-drive reluctance motors, with fixed electromagnets acting on iron bars attached to a wooden cylinder on each axle, and simple commutators. It hauled a load of six tons at four miles per hour (6 kilometers per hour) for a distance of . It was tested on the Edinburgh and Glasgow Railway in September of the following year, but the limited power from batteries prevented its general use. It was destroyed by railway workers, who saw it as a threat to their job security.\n\nThe first electric passenger train was presented by Werner von Siemens at Berlin in 1879. The locomotive was driven by a 2.2 kW, series-wound motor, and the train, consisting of the locomotive and three cars, reached a speed of 13 km/h. During four months, the train carried 90,000 passengers on a 300-metre-long (984 feet) circular track. The electricity (150 V DC) was supplied through a third insulated rail between the tracks. A contact roller was used to collect the electricity.\nThe world's first electric tram line opened in Lichterfelde near Berlin, Germany, in 1881. It was built by Werner von Siemens (see Gross-Lichterfelde Tramway and Berlin Straßenbahn). Volk's Electric Railway opened in 1883 in Brighton. Also in 1883, Mödling and Hinterbrühl Tram opened near Vienna in Austria. It was the first in the world in regular service powered from an overhead line. Five years later, in the U.S. electric trolleys were pioneered in 1888 on the Richmond Union Passenger Railway, using equipment designed by Frank J. Sprague.\n\nMuch of the early development of electric locomotion was driven by the increasing use of tunnels, particularly in urban areas. Smoke from steam locomotives was noxious and municipalities were increasingly inclined to prohibit their use within their limits. The first electrically-worked underground line was the City and South London Railway, prompted by a clause in its enabling act prohibiting use of steam power. It opened in 1890, using electric locomotives built by Mather and Platt. Electricity quickly became the power supply of choice for subways, abetted by the Sprague's invention of multiple-unit train control in 1897. Surface and elevated rapid transit systems generally used steam until forced to convert by ordinance.\n\nThe first use of electrification on a main line was on a four-mile stretch of the Baltimore Belt Line of the Baltimore and Ohio Railroad (B&O) in 1895 connecting the main portion of the B&O to the new line to New York through a series of tunnels around the edges of Baltimore's downtown. Parallel tracks on the Pennsylvania Railroad had shown that coal smoke from steam locomotives would be a major operating issue and a public nuisance. Three Bo+Bo units were initially used, at the south end of the electrified section; they coupled onto the locomotive and train and pulled it through the tunnels. Railroad entrances to New York City required similar tunnels and the smoke problems were more acute there. A collision in the Park Avenue tunnel in 1902 led the New York State legislature to outlaw the use of smoke-generating locomotives south of the Harlem River after 1 July 1908. In response, electric locomotives began operation in 1904 on the New York Central Railroad. In the 1930s, the Pennsylvania Railroad, which had introduced electric locomotives because of the NYC regulation, electrified its entire territory east of Harrisburg, Pennsylvania.\n\nThe Chicago, Milwaukee, St. Paul and Pacific Railroad (the Milwaukee Road), the last transcontinental line to be built, electrified its lines across the Rocky Mountains and to the Pacific Ocean starting in 1915. A few East Coast lines, notably the Virginian Railway and the Norfolk and Western Railway, electrified short sections of their mountain crossings. However, by this point electrification in the United States was more associated with dense urban traffic and the use of electric locomotives declined in the face of dieselization. Diesels shared some of the electric locomotive's advantages over steam and the cost of building and maintaining the power supply infrastructure, which discouraged new installations, brought on the elimination of most main-line electrification outside the Northeast. Except for a few captive systems (e.g. the Black Mesa and Lake Powell), by 2000 electrification was confined to the Northeast Corridor and some commuter service; even there, freight service was handled by diesels. Development continued in Europe, where electrification was widespread. 1,500 V DC is still used on some lines near France and 25 kV 50 Hz is used by high-speed trains.\n\nThe first practical AC electric locomotive was designed by Charles Brown, then working for Oerlikon, Zürich. In 1891, Brown had demonstrated long-distance power transmission, using three-phase AC, between a hydro-electric plant at Lauffen am Neckar and Frankfurt am Main West, a distance of 280 km. Using experience he had gained while working for Jean Heilmann on steam-electric locomotive designs, Brown observed that three-phase motors had a higher power-to-weight ratio than DC motors and, because of the absence of a commutator, were simpler to manufacture and maintain. However, they were much larger than the DC motors of the time and could not be mounted in underfloor bogies: they could only be carried within locomotive bodies.\n\nIn 1894, Hungarian engineer Kálmán Kandó developed a new type 3-phase asynchronous electric drive motors and generators for electric locomotives. \nKandó's early 1894 designs were first applied in a short three-phase AC tramway in Evian-les-Bains (France), which was constructed between 1896 and 1898.\n\nIn 1918, Kandó invented and developed the rotary phase converter, enabling electric locomotives to use three-phase motors whilst supplied via a single overhead wire, carrying the simple industrial frequency (50 Hz) single phase AC of the high voltage national networks.\n\nIn 1896, Oerlikon installed the first commercial example of the system on the Lugano Tramway. Each 30-tonne locomotive had two motors run by three-phase 750 V 40 Hz fed from double overhead lines. Three-phase motors run at constant speed and provide regenerative braking, and are well suited to steeply graded routes, and the first main-line three-phase locomotives were supplied by Brown (by then in partnership with Walter Boveri) in 1899 on the 40 km Burgdorf—Thun line, Switzerland. The first implementation of industrial frequency single-phase AC supply for locomotives came from Oerlikon in 1901, using the designs of Hans Behn-Eschenburg and Emil Huber-Stockar; installation on the Seebach-Wettingen line of the Swiss Federal Railways was completed in 1904. The 15 kV, 50 Hz , 48 tonne locomotives used transformers and rotary converters to power DC traction motors.\nItalian railways were the first in the world to introduce electric traction for the entire length of a main line rather than just a short stretch. The 106 km Valtellina line was opened on 4 September 1902, designed by Kandó and a team from the Ganz works. The electrical system was three-phase at 3 kV 15 Hz. The voltage was significantly higher than used earlier and it required new designs for electric motors and switching devices. The three-phase two-wire system was used on several railways in Northern Italy and became known as \"the Italian system\". Kandó was invited in 1905 to undertake the management of Società Italiana Westinghouse and led the development of several Italian electric locomotives. During the period of electrification of the Italian railways, tests were made as to which type of power to use: in some sections there was a 3,600 V  Hz three-phase power supply, in others there was 1,500 V DC, 3 kV DC and 10 kV AC 45 Hz supply. After WW2, 3 kV DC power was chosen for the entire Italian railway system.\n\nA later development of Kandó, working with both the Ganz works and Societa Italiana Westinghouse, was an electro-mechanical converter, allowing the use of three-phase motors from single-phase AC, eliminating the need for two overhead wires. In 1923, the first phase-converter locomotive in Hungary was constructed on the basis of Kandó's designs and serial production began soon after. The first installation, at 16 kV 50 Hz, was in 1932 on the 56 km section of the Hungarian State Railways between Budapest and Komárom. This proved successful and the electrification was extended to Hegyeshalom in 1934.\nIn Europe, electrification projects initially focused on mountainous regions for several reasons: coal supplies were difficult, hydroelectric power was readily available, and electric locomotives gave more traction on steeper lines. This was particularly applicable in Switzerland, where almost all lines are electrified. An important contribution to the wider adoption of AC traction came from SNCF of France after World War II. The company had assessed the industrial-frequency AC line routed through the steep Höllental Valley, Germany, which was under French administration following the war. After trials, the company decided that the performance of AC locomotives was sufficiently developed to allow all its future installations, regardless of terrain, to be of this standard, with its associated cheaper and more efficient infrastructure. The SNCF decision, ignoring as it did the of high-voltage DC already installed on French routes, was influential in the standard selected for other countries in Europe.\n\nThe 1960s saw the electrification of many European main lines. European electric locomotive technology had improved steadily from the 1920s onwards. By comparison, the Milwaukee Road class EP-2 (1918) weighed 240 t, with a power of 3,330 kW and a maximum speed of 112 km/h; in 1935, German E 18 had a power of 2,800 kW, but weighed only 108 tons and had a maximum speed of 150 km/h. On 29 March 1955, French locomotive CC 7107 reached 331 km/h. In 1960 the SJ Class Dm 3 locomotives on Swedish Railways produced a record 7,200 kW. Locomotives capable of commercial passenger service at 200 km/h appeared in Germany and France in the same period. Further improvements resulted from the introduction of electronic control systems, which permitted the use of increasingly lighter and more powerful motors that could be fitted inside the bogies (standardising from the 1990s onwards on asynchronous three-phase motors, fed through GTO-inverters).\n\nIn the 1980s, development of very high-speed service brought further electrification. The Japanese Shinkansen and the French TGV were the first systems for which devoted high-speed lines were built from scratch. Similar programs were undertaken in Italy, Germany and Spain; in the United States the only new main-line service was an extension of electrification over the Northeast Corridor from New Haven, Connecticut, to Boston, Massachusetts, though new electric light rail systems continued to be built.\n\nOn 2 September 2006, a standard production Siemens electric locomotive of the Eurosprinter type ES64-U4 (ÖBB Class 1216) achieved , the record for a locomotive-hauled train, on the new line between Ingolstadt and Nuremberg. This locomotive is now employed largely unmodified by ÖBB to haul their Railjet which is however limited to a top speed of 230 km/h due to economic and infrastructure concerns.\n\nAn electric locomotive can be supplied with power from\n\nThe distinguishing design features of electric locomotives are:\n\nThe most fundamental difference lies in the choice of AC or DC. The earliest systems used DC, as AC was not well understood and insulation material for high voltage lines was not available. DC locomotives typically run at relatively low voltage (600 to 3,000 volts); the equipment is therefore relatively massive because the currents involved are large in order to transmit sufficient power. Power must be supplied at frequent intervals as the high currents result in large transmission system losses.\n\nAs AC motors were developed, they became the predominant type, particularly on longer routes. High voltages (tens of thousands of volts) are used because this allows the use of low currents; transmission losses are proportional to the square of the current (e.g. twice the current means four times the loss). Thus, high power can be conducted over long distances on lighter and cheaper wires. Transformers in the locomotives transform this power to a low voltage and high current for the motors.\nA similar high voltage, low current system could not be employed with direct current locomotives because there is no easy way to do the voltage/current transformation for DC so efficiently as achieved by AC transformers.\n\nAC traction still occasionally uses dual overhead wires instead of single phase lines. The resulting three-phase current drives induction motors, which do not have sensitive commutators and permit easy realisation of a regenerative brake. Speed is controlled by changing the number of pole pairs in the stator circuit, with acceleration controlled by switching additional resistors in, or out, of the rotor circuit. The two-phase lines are heavy and complicated near switches, where the phases have to cross each other. The system was widely used in northern Italy until 1976 and is still in use on some Swiss rack railways. The simple feasibility of a fail-safe electric brake is an advantage of the system, while speed control and the two-phase lines are problematic.\n\nRectifier locomotives, which used AC power transmission and DC motors, were common, though DC commutators had problems both in starting and at low velocities. Today's advanced electric locomotives use brushless three-phase AC induction motors. These polyphase machines are powered from GTO-, IGCT- or IGBT-based inverters. The cost of electronic devices in a modern locomotive can be up to 50% of the cost of the vehicle.\n\nElectric traction allows the use of regenerative braking, in which the motors are used as brakes and become generators that transform the motion of the train into electrical power that is then fed back into the lines. This system is particularly advantageous in mountainous operations, as descending locomotives can produce a large portion of the power required for ascending trains.\nMost systems have a characteristic voltage and, in the case of AC power, a system frequency. Many locomotives have been equipped to handle multiple voltages and frequencies as systems came to overlap or were upgraded. American FL9 locomotives were equipped to handle power from two different electrical systems and could also operate as diesel-electrics.\n\nWhile today's systems predominantly operate on AC, many DC systems are still in use – e.g. in South Africa and the United Kingdom (750 V and 1,500 V); Netherlands, Japan, Ireland (1,500 V); Slovenia, Belgium, Italy, Poland, Russia, Spain (3,000 V) and Washington DC (750 V).\n\nElectrical circuits require two connections (or for three phase AC, three connections). From the beginning, the track was used for one side of the circuit. Unlike model railroads the track normally supplies only one side, the other side(s) of the circuit being provided separately.\n\nRailways generally tend to prefer overhead lines, often called \"catenaries\" after the support system used to hold the wire parallel to the ground. Three collection methods are possible:\n\n\nOf the three, the pantograph method is best suited for high-speed operation. Some locomotives use both overhead and third rail collection (e.g. British Rail Class 92).\nIn Europe the recommended geometry and shape of pantographs are defined by standard EN 50367/IEC 60486\n\nThe original Baltimore and Ohio Railroad electrification used a sliding shoe in an overhead channel, a system quickly found to be unsatisfactory. It was replaced by a third rail, in which a pickup (the \"shoe\") rode underneath or on top of a smaller rail parallel to the main track, above ground level. There were multiple pickups on both sides of the locomotive in order to accommodate the breaks in the third rail required by trackwork. This system is preferred in subways because of the close clearances it affords.\n\nDuring the initial development of railroad electrical propulsion, a number of drive systems were devised to couple the output of the traction motors to the wheels. Early locomotives often used jackshaft drives. In this arrangement, the traction motor is mounted within the body of the locomotive and drives the jackshaft through a set of gears. This system was employed because the first traction motors were too large and heavy to mount directly on the axles. Due to the number of mechanical parts involved, frequent maintenance was necessary. The jackshaft drive was abandoned for all but the smallest units when smaller and lighter motors were developed,\n\nSeveral other systems were devised as the electric locomotive matured. The Buchli drive was a fully spring-loaded system, in which the weight of the driving motors was completely disconnected from the driving wheels. First used in electric locomotives from the 1920s, the Buchli drive was mainly used by the French SNCF and Swiss Federal Railways. The quill drive was also developed about this time and mounted the traction motor above or to the side of the axle and coupled to the axle through a reduction gear and a hollow shaft - the quill - flexibly connected to the driving axle. The Pennsylvania Railroad GG1 locomotive used a quill drive. Again, as traction motors continued to shrink in size and weight, quill drives gradually fell out of favour.\n\nAnother drive was the \"bi-polar\" system, in which the motor armature was the axle itself, the frame and field assembly of the motor being attached to the truck (bogie) in a fixed position. The motor had two field poles, which allowed a limited amount of vertical movement of the armature. This system was of limited value since the power output of each motor was limited. The EP-2 bi-polar electrics used by the Milwaukee Road compensated for this problem by using a large number of powered axles.\n\nModern electric locomotives, like their Diesel-electric counterparts, almost universally use axle-hung traction motors, with one motor for each powered axle. In this arrangement, one side of the motor housing is supported by plain bearings riding on a ground and polished journal that is integral to the axle. The other side of the housing has a tongue-shaped protuberance that engages a matching slot in the truck (bogie) bolster, its purpose being to act as a torque reaction device, as well as a support. Power transfer from motor to axle is effected by spur gearing, in which a pinion on the motor shaft engages a bull gear on the axle. Both gears are enclosed in a liquid-tight housing containing lubricating oil. The type of service in which the locomotive is used dictates the gear ratio employed. Numerically high ratios are commonly found on freight units, whereas numerically low ratios are typical of passenger engines.\n\nThe Whyte notation system for classifying steam locomotives is not adequate for describing the variety of electric locomotive arrangements, though the Pennsylvania Railroad applied classes to its electric locomotives as if they were steam. For example, the PRR GG1 class indicates that it is arranged like two 4-6-0 class G locomotives coupled back-to-back.\n\nUIC classification system was typically used for electric locomotives, as it could handle the complex arrangements of powered and unpowered axles and could distinguish between coupled and uncoupled drive systems.\n\nA battery-electric locomotive (or battery locomotive) is powered by on-board batteries; a kind of battery electric vehicle.\n\nSuch locomotives are used where a conventional diesel or electric locomotive would be unsuitable. Another use for battery locomotives is in industrial facilities where a combustion-powered locomotive (i.e., steam- or diesel-powered) could cause a safety issue due to the risks of fire, explosion or fumes in a confined space. Battery locomotives are preferred for mines where gas could be ignited by trolley-powered units arcing at the collection shoes, or where electrical resistance could develop in the supply or return circuits, especially at rail joints, and allow dangerous current leakage into the ground. Mine railways often use battery locomotives.\n\nThe first electric locomotive built in 1837 was a battery locomotive It was built by chemist Robert Davidson of Aberdeen, and it was powered by galvanic cells (batteries). Another early example was at the Kennecott Copper Mine, Latouche, Alaska, where in 1917 the underground haulage ways were widened to enable working by two battery locomotives of . In 1928, Kennecott Copper ordered four 700-series electric locomotives with on-board batteries. These locomotives weighed and operated on 750 volt overhead trolley wire with considerable further range whilst running on batteries. The locomotives provided several decades of service using Nickel-iron battery (Edison) technology. The batteries were replaced with lead-acid batteries, and the locomotives were retired shortly afterward. All four locomotives were donated to museums, but one was scrapped. The others can be seen at the Boone and Scenic Valley Railroad, Iowa, and at the Western Railway Museum in Rio Vista, California.\n\nThe Toronto Transit Commission previously operated a battery electric locomotive built by Nippon-Sharyo in 1968 and retired in 2009.\n\nLondon Underground regularly operates battery-electric locomotives for general maintenance work.\n\nElectrification is widespread in Europe. Due to higher density schedules, operating costs are more dominant with respect to the infrastructure costs than in the U.S. and electric locomotives have much lower operating costs than diesels. In addition, governments were motivated to electrify their railway networks due to coal shortages experienced during the First and Second World Wars.\n\nDiesel locomotives have less power compared to electric locomotives for the same weight and dimensions. For instance, the 2,200 kW of a modern British Rail Class 66 was matched in 1927 by the electric SBB-CFF-FFS Ae 4/7 (2,300 kW), which is lighter. However, for low speeds, tractive effort is more important than power. This is why diesel engines are competitive for slow freight traffic (as it is common in the U.S.) but not for passenger or mixed passenger/freight traffic like on many European railway lines, especially where heavy freight trains must be run at comparatively high speeds (80 km/h or more).\n\nThese factors led to high degrees of electrification in most European countries. In some countries like Switzerland, even electric shunters are common and many private sidings can be served by electric locomotives. During World War II, when materials to build new electric locomotives were not available, Swiss Federal Railways installed electric heating elements, fed from the overhead supply, in the boilers of some steam shunters to deal with the shortage of imported coal.\n\nRecent political developments in many European countries to enhance public transit have led to another boost for electric traction. High-speed trains like the TGV, ICE, AVE and Pendolino can only be run economically using electric traction and the operation of branch lines is usually less in deficit when using electric traction, due to cheaper and faster rolling stock and more passengers due to more frequent service and more comfort. In addition, gaps of un-electrified track are closed to avoid replacing electric locomotives by diesels for these sections. The necessary modernisation and electrification of these lines is possible due to financing of the railway infrastructure by the state.\n\nRussia and other countries of the former USSR have a mix of 3,300 V DC and 25 kV AC for historical reasons.\n\nThe special \"junction stations\" (around 15 over the former USSR - Vladimir, Mariinsk near Krasnoyarsk etc.) have wiring switchable from DC to AC. Locomotive replacement is essential at these stations and is performed together with the contact wiring switching.\n\nMost Soviet, Czech (the USSR ordered passenger electric locomotives from Skoda), Russian and Ukrainian locomotives can operate on AC or DC only. For instance, VL80 is an AC machine, with VL10 a DC version. There were some half-experimental small series like VL82, which could switch from AC to DC and were used in small amounts around the city of Kharkov in Ukraine. Also, the latest Russian passenger locomotive EP10 is dual-system.\n\nHistorically, 3,300 V DC was used for simplicity. The first experimental track was in Georgian mountains, then the suburban zones of the largest cities were electrified for EMUs - very advantageous due to much better dynamic of such a train compared to the steam one, which is important for suburban service with frequent stops. Then the large mountain line between Ufa and Chelyabinsk was electrified.\n\nFor some time, electric railways were only considered to be suitable for suburban or mountain lines. In around 1950, a decision was made (according to legend, by Joseph Stalin) to electrify the highly loaded plain prairie line of Omsk-Novosibirsk. After this, electrifying the major railroads at 3,000 V DC became mainstream.\n\n25 kV AC started in the USSR in around 1960, when the industry managed to build the rectifier-based AC-wire DC-motor locomotive (all Soviet and Czech AC locomotives were such; only the post-Soviet ones switched to electronically controlled induction motors). The first major line with AC power was Mariinsk-Krasnoyarsk-Tayshet-Zima; the lines in European Russia like Moscow-Rostov-on-Don followed.\n\nIn 1990s, some DC lines were rebuilt as AC to allow the usage of the huge 10 MWt AC locomotive of VL85. The line around Irkutsk is one of them. The DC locomotives freed by this rebuild were transferred to the St Petersburg region.\n\nThe Trans-Siberian Railway has been partly electrified since 1929, entirely since 2002. The system is 25 kV AC 50 Hz after the junction station of Mariinsk near Krasnoyarsk, 3,000 V DC before it, and train weights are up to 6,000 tonnes.\n\nElectric locomotives are used for passenger trains on Amtrak's Northeast Corridor between Washington, DC, and Boston, with a branch to Harrisburg, Pennsylvania, and on some commuter rail lines. Mass transit systems and other electrified commuter lines use electric multiple units, where each car is powered. All other long-distance passenger service and, with rare exceptions, all freight is hauled by diesel-electric locomotives.\n\nIn North America, the flexibility of diesel locomotives and the relative low cost of their infrastructure has led them to prevail except where legal or operational constraints dictate the use of electricity. An example of the latter is the use of electric locomotives by Amtrak and commuter railroads in the Northeast. New Jersey Transit New York corridor uses ALP-46 electric locomotives, due to the prohibition on diesel operation in Penn Station and the Hudson and East River Tunnels leading to it. Some other trains to Penn Station use dual-mode locomotives that can also operate off third-rail power in the tunnels and the station. Electric locomotives are planned for the California High Speed Rail system.\n\nDuring the steam era, some mountainous areas were electrified but these have been discontinued. The junction between electrified and non-electrified territory is the locale of engine changes; for example, Amtrak trains had extended stops in New Haven, Connecticut, as locomotives were swapped, a delay which contributed to the decision to electrify the New Haven to Boston segment of the Northeast Corridor in 2000.\n\nNo main-line railways in Canada use electric locomotives as of January 2011.\n\nAgence métropolitaine de transport (AMT) operates the ALP-45DP dual-mode electro-diesel locomotives for the Repentigny-Mascouche Line (AMT). They run as electric while in the poorly ventilated Mount Royal Tunnel, otherwise they run as diesel locomotives.\n\nToronto subway operates two Arva Industries electric locomotives in their work car fleet. It previous had Nippon Sharyo electric locomotives since 1968.\n\nThe rail system consists of the following ():\n\nElectrification systems used by the JR group, Japan's formerly state-owned operators, are 1,500 V DC and 20 kV AC for conventional lines and 25 kV AC for Shinkansen. Electrification at 600 V DC and 750 V DC are also seen in private lines. The frequency of the AC power supply is 50 Hz in Eastern Japan and 60 Hz in Western Japan.\n\nJapan has come close to complete electrification largely due to the relatively short distances and mountainous terrain, which make electric service a particularly economical investment. Additionally, the mix of freight to passenger service is weighted much more toward passenger service (even in rural areas) than in many other countries, and this has helped drive government investment into electrification of many remote lines.\n\nElectrification began in earnest for local railways in the 1920s and main lines electrification began following World War II using a universal 1,500 V DC standard and eventually, a 20 kV standard for rapid intercity main lines (often overlaying 1,500 V DC lines) and 25 kV AC for high-speed Shinkansen lines. Because most of the electrification infrastructure was destroyed in the war, the only variances to this standard with significant traffic are a few of the older subway lines in Tokyo and Osaka. The Tōkaidō Main Line, Japan's busiest line, completed electrification in 1956 and Tōkaidō Shinkansen was complete in 1964. By the mid 1970s, most main lines had been converted. During the 1970s and into the 1980s, when a fast-growing Japanese economy encouraged massive infrastructure spending, almost every line with any significant traffic was electrified. Though the massive debts incurred for these upgrades (along with the more publicised expense of Shinkansen expansions) led to the privatization and break-up of the national rail company. By the time of the breakup in 1987, electric service had penetrated to every line with significant traffic. In the 1990s, and 2000s, rural infrastructure was the focus of a lot of government stimulus funding and this included some rail electrification on infrequently used lines, and funding for expanding the Shinkansen network (which, as with all high speed trains, is electric). The latter was mostly in the form of loans rather than direct investment as in the former.\n\nKeretapi Tanah Melayu of Malaysia operates 25 kV AC electric multiple units, starting from their KTM Komuter in 1995. In December 2009, a fleet of new ETS arrived.\n\nThe Harbour line (Mumbai Suburban Railway) of the Mumbai Suburban Railway was the first railroad to be electrified (in 1925) followed by the Beach-Tambaram stretch of The Chennai Suburban Railway in 1931.\n\nAll mainline electrified routes in India use 25 kV AC railway electrification at 50 Hz. Kolkata Metro, Namma Metro, Rapid Metro, Kochi Metro and Noida Metro use 750 V DC third rail electrification. As of March 2017, Indian railways haul 85% of freight and passenger traffic with electric locomotives and 30000 km of railway lines have been electrified.\n\nPakistan Railways has 29 electric locomotives of class BCU30E numbered 7001-29. These are British-built locomotives of 3,000 horsepower for 25 kV AC. The electric locomotives were introduced in 2009 for the 286 kilometers Lahore-Khanewal junction in order to moderise the system and save scarce foreign exchange required for import of fuel for diesel locomotives. They are stored out of use because the overhead lines are unserviceable, owing to theft of copper.\n\nOrange line metro in Lahore uses electric locomotives. CRRC Zhuzhou Locomotive rolled out the first of 27 trains for the metro on 16 May 2017.. Recently numerous successful trials have been run.\n\nBoth Victorian Railways and New South Wales Government Railways, which pioneered electric traction in Australia in the early 20th century and continue to operate 1,500 V DC Electric Multiple Units, have withdrawn their electric locomotives.\n\nIn both states, the use of electric locomotives on principal interurban routes proved to be a qualified success. In Victoria, because only one major line (the Gippsland line) had been electrified, the economic advantages of electric traction were not fully realised due to the need to change locomotives for trains that ran beyond the electrified network. VR's electric locomotive fleet was withdrawn from service by 1987 and the Gippsland line electrification was dismantled by 2004. The 86 class locomotives introduced to NSW in 1983 had a relatively short life as the costs of changing locomotives at the extremities of the electrified network, together with the higher charges levied for electricity use, saw diesel-electric locomotives make inroads into the electrified network. Electric power car trains are still used for urban passenger services.\n\nQueensland Rail implemented electrification relatively recently and utilises the more recent 25 kV AC technology with around 1,000 km of the narrow gauge network now electrified. It operates a fleet of electric locomotives to transport coal for export, the most recent of which the 3,000 kW (4,020 HP) 3300/3400 Class. Queensland Rail is currently rebuilding its 3100 and 3200 class locos into the 3700 class, which use AC traction and need only three locomotives on a coal train rather than five. Queensland Rail is getting 30 3800 class locomotives from Siemens in Munich, Germany, which will arrive during late 2008 to 2009. QRNational (Queensland Rail's coal and freight after separation) has increased the order of 3800 class locomotives. They continue to arrive late into 2010.\n\nIn South Australia, the Seaford railway line was electrified in 2013; electric services commenced in February 2014.\n\n\n"}
{"id": "14750034", "url": "https://en.wikipedia.org/wiki?curid=14750034", "title": "Energy elasticity", "text": "Energy elasticity\n\nEnergy elasticity is a term used with reference to the energy intensity of Gross Domestic Product. It is \"the percentage change in energy consumption to achieve one per cent change in national GDP\". \n\nThis term has been used when describing sustainable growth in the developing world, while being aware of the need to maintain the security of energy supply and constrain the emission of additional greenhouse gases. Energy elasticity is a top-line measure, as the commercial energy sources used by the country in question are normally further itemised as fossil, renewable, etc. \n\nFor example, India's national Integrated Energy Policy of 2005 noted current elasticity at 0.80, while planning for 7-8% GDP growth. It expected to be able to reduce this to 0.75 from 2011 and to 0.67 from 2021-22. By 2007, India's Ambassador was able to inform the United Nations Security Council that its GDP was growing by 8%, with only 3.7% growth in its total primary energy consumption, suggesting it had effectively de-linked energy consumption from economic growth.\n\nChina has shown the opposite relationship, as, after 2000, it has consumed proportionately more energy to achieve its high double-digit growth rate. Although there are problems with the quality of the estimates of both GDP and energy consumption, by 2003-4 observers placed Chinese energy elasticity at approximately 1.5. For every one percent increase in GDP, energy demand grew by 1.5 percent. Much of this extra demand has been sourced internationally from fossil fuels, such as coal and petroleum.\n"}
{"id": "11529", "url": "https://en.wikipedia.org/wiki?curid=11529", "title": "Fermion", "text": "Fermion\n\nIn particle physics, a fermion is a particle that follows Fermi–Dirac statistics. These particles obey the Pauli exclusion principle. Fermions include all quarks and leptons, as well as all composite particles made of an odd number of these, such as all baryons and many atoms and nuclei. Fermions differ from bosons, which obey Bose–Einstein statistics.\n\nA fermion can be an elementary particle, such as the electron, or it can be a composite particle, such as the proton. According to the spin-statistics theorem in any reasonable relativistic quantum field theory, particles with integer spin are bosons, while particles with half-integer spin are fermions.\n\nIn addition to the spin characteristic, fermions have another specific property: they possess conserved baryon or lepton quantum numbers. Therefore, what is usually referred to as the spin statistics relation is in fact a spin statistics-quantum number relation.\n\nAs a consequence of the Pauli exclusion principle, only one fermion can occupy a particular quantum state at any given time. If multiple fermions have the same spatial probability distribution, then at least one property of each fermion, such as its spin, must be different. Fermions are usually associated with matter, whereas bosons are generally force carrier particles, although in the current state of particle physics the distinction between the two concepts is unclear. Weakly interacting fermions can also display bosonic behavior under extreme conditions. At low temperature fermions show superfluidity for uncharged particles and superconductivity for charged particles.\n\nComposite fermions, such as protons and neutrons, are the key building blocks of everyday matter.\n\nThe name fermion was coined by English theoretical physicist Paul Dirac from the surname of Italian physicist Enrico Fermi.\n\nThe Standard Model recognizes two types of elementary fermions: quarks and leptons. In all, the model distinguishes 24 different fermions. There are six quarks (up, down, strange, charm, bottom and top quarks), and six leptons (electron, electron neutrino, muon, muon neutrino, tau particle and tau neutrino), along with the corresponding antiparticle of each of these.\n\nMathematically, fermions come in three types:\nMost Standard Model fermions are believed to be Dirac fermions, although it is unknown at this time whether the neutrinos are Dirac or Majorana fermions (or both). Dirac fermions can be treated as a combination of two Weyl fermions. In July 2015, Weyl fermions have been experimentally realized in Weyl semimetals.\n\nComposite particles (such as hadrons, nuclei, and atoms) can be bosons or fermions depending on their constituents. More precisely, because of the relation between spin and statistics, a particle containing an odd number of fermions is itself a fermion. It will have half-integer spin.\n\nExamples include the following:\n\nThe number of bosons within a composite particle made up of simple particles bound with a potential has no effect on whether it is a boson or a fermion.\n\nFermionic or bosonic behavior of a composite particle (or system) is only seen at large (compared to size of the system) distances. At proximity, where spatial structure begins to be important, a composite particle (or system) behaves according to its constituent makeup.\n\nFermions can exhibit bosonic behavior when they become loosely bound in pairs. This is the origin of superconductivity and the superfluidity of helium-3: in superconducting materials, electrons interact through the exchange of phonons, forming Cooper pairs, while in helium-3, Cooper pairs are formed via spin fluctuations.\n\nThe quasiparticles of the fractional quantum Hall effect are also known as composite fermions, which are electrons with an even number of quantized vortices attached to them.\n\nIn a quantum field theory, there can be field configurations of bosons which are topologically twisted. These are coherent states (or solitons) which behave like a particle, and they can be fermionic even if all the constituent particles are bosons. This was discovered by Tony Skyrme in the early 1960s, so fermions made of bosons are named skyrmions after him.\n\nSkyrme's original example involved fields which take values on a three-dimensional sphere, the original nonlinear sigma model which describes the large distance behavior of pions. In Skyrme's model, reproduced in the large N or string approximation to quantum chromodynamics (QCD), the proton and neutron are fermionic topological solitons of the pion field.\n\nWhereas Skyrme's example involved pion physics, there is a much more familiar example in quantum electrodynamics with a magnetic monopole. A bosonic monopole with the smallest possible magnetic charge and a bosonic version of the electron will form a fermionic dyon.\n\nThe analogy between the Skyrme field and the Higgs field of the electroweak sector has been used to postulate that all fermions are skyrmions. This could explain why all known fermions have baryon or lepton quantum numbers and provide a physical mechanism for the Pauli exclusion principle.\n\n"}
{"id": "6787723", "url": "https://en.wikipedia.org/wiki?curid=6787723", "title": "Fluxional molecule", "text": "Fluxional molecule\n\nFluxional molecules are molecules that undergo dynamics such that some or all of their atoms interchange between symmetry-equivalent positions. Because virtually all molecules are fluxional in some respects, e.g. bond rotations in most organic compounds, the term fluxional depends on the context and the method used to assess the dynamics. Often, a molecule is considered fluxional if its spectroscopic signature exhibits line-broadening (beyond that dictated by the Heisenberg uncertainty principle) due to chemical exchange. In some cases, where the rates are slow, fluxionality is not detected spectroscopically, but by isotopic labeling. Where such movement do not occur, the molecule may be described as a semi-rigid molecule.\n\nThe prototypical fluxional molecule is phosphorus pentafluoride. Its F NMR spectrum consists of a P-coupled doublet, indicating that the equatorial and axial fluorine centers interchange rapidly on the NMR timescale. Fluorine-19 NMR spectroscopy, even at temperatures as low as −100 °C, fails to distinguish the axial from the equatorial fluorine environments. The apparent equivalency arises from the low barrier for pseudorotation via the Berry mechanism, by which the axial and equatorial fluorine atoms rapidly exchange positions. \n\nA well studied fluxional ion is the carbonium ion, which is protonated methane, CH. In this unusual species, whose IR spectrum was recently experimentally observed and more recently understood, the barriers to proton exchange are lower than the zero point energy. Thus, even at absolute zero there is no rigid molecular structure, the H atoms are always in motion. More precisely, the spatial distribution of protons in CH is many times broader than its parent molecule CH, methane.\n\nTemperature dependent changes in the NMR spectra result from dynamics associated with the fluxional molecules when those dynamics proceed at rates comparable to the frequency differences observed by NMR. The experiment is called DNMR and typically involves recording spectra at various temperatures. In the ideal case, low temperature spectra can be assigned to the \"slow exchange limit\", whereas spectra recorded at higher temperatures correspond to molecules at \"fast exchange limit\". Typically, high temperature spectra are simpler than those recorded at low temperatures, since at high temperatures, equivalent sites are averaged out. Prior to the advent of DNMR, kinetics of reactions were measured on nonequilibrium mixtures, monitoring the approach to equilibrium.\n\nMany molecular processes exhibit fluxionality that can be probed on the NMR time scale. Beyond the examples highlighted below, other classic examples include the Cope rearrangement in bullvalene and the chair inversion in cyclohexane.\n\nFor processes that are too slow for traditional DNMR analysis, the technique spin saturation transfer (SST) is applicable. This magnetization transfer technique provides rate information, provided that the rates exceed 1/\"T\".\n\nA classic example of a fluxional molecule is dimethylformamide.\n\nAt temperatures near 100 °C, the 500 MHz NMR spectrum of this compound shows only one signal for the methyl groups. Near room temperature however, separate signals are seen for the non-equivalent methyl groups. The rate of exchange can be readily calculated at the temperature where the two signals are just merged. This \"coalescence temperature\" depends on the measuring field. The relevant equation is:\nwhere Δν is the difference in Hz between the frequencies of the exchanging sites. These frequencies are obtained from the limiting low-temperature NMR spectrum. At these lower temperatures, the dynamics continue, of course, but the contribution of the dynamics to line broadening is negligible.\n\nFor example, if Δν = 1ppm @ 500 MHz\n\nMany organometallic compounds exhibit fluxionality. The compound Fe(η-CH) (η- CH)(CO) exhibits the phenomenon of \"ring whizzing\". \nAt 30 °C, the H NMR spectrum shows only two peaks, one typical (δ5.6) of the η-C\"H\" and the other assigned η-C\"H\". The singlet assigned to the η-CH ligand splits at low temperatures owing to the slow hopping of the Fe center from carbon to carbon in the η-CH ligand. Two mechanisms have been proposed, with the consensus favoring the 1,2 shift pathway.\n\nPentacoordinate molecules of trigonal pyramidal geometry typically exhibit a particular kind of low energy fluxional behavior called Berry pseudorotation. Famous examples of such molecules are iron pentacarbonyl (Fe(CO)) and phosphorus pentafluoride (PF). At higher temperatures, only one signal is observed for the ligands (e.g., by C or F NMR) whereas at low temperatures, two signals in a 2:3 ratio can be resolved. Molecules that are not strictly pentacoordinate are also subject to this process, such as SF.\n\nAlthough less common, some dynamics are also observable on the time-scale of IR spectroscopy. One example is electron transfer in a mixed valence dimer of metal clusters. Application of above equation for coalescence of two signals separated by 10 cm gives the following result:\nClearly, processes that induce line-broadening on the IR time-scale must be extremely rapid.\n\nWhen a molecule contains identical nuclei—which is commonly the case—there are a number of minima related by the \"permutations of the identical nuclei\". The minima, distinguished by different numberings of identical nuclei, can be partitioned in equivalent classes. Two minima are equivalent if they can be transformed into one other by rotating the molecule, that is, without surmounting any energy barrier (bond breaking or bond twisting). The molecules with minima in different equivalent classes are called versions. To transform one version into another version an energy barrier must be overcome.\n\nTake for instance the pyramidal ammonia (NH) molecule. There are 3!=6 permutations of the hydrogen atoms. If we count the hydrogens looking down from the nitrogen onto the plane of the hydrogens, then we see that \nforms one equivalence class, (class I), because the members can be transformed into each other by simply rotating around the 3-fold axis without overcoming an energy barrier.\nThe other equivalence class (class II) consists of \nTo transform a member (\"version\") of class I to class II, an energy barrier has to be overcome. (The lowest path on the potential energy surface is actually via the flipping of the ammonia \"umbrella\". The umbrella up and the umbrella down are separated by an energy barrier of height of ca. 1000 cm).\n\nIn a semi-rigid molecule all the barriers between different versions are so high that the tunneling though the barriers may be neglected. Under these conditions identical nuclei may be seen as \"distinguishable\" particles to which the Pauli principle \"does not\" apply. This is a very common point of view in chemistry.\n\n"}
{"id": "4238932", "url": "https://en.wikipedia.org/wiki?curid=4238932", "title": "Glow plate", "text": "Glow plate\n\nGlow plates are sheets of glass or plastic that \"glow\" when light is supplied to one of their edges.\nThe light source for a glow plate can be artificial, such as fluorescent light, or natural, with sunlight being directly exposed to the plate or fed through a fiber-optic system.\n\nA joint effort between Florida State University and Oak Ridge National Laboratory is focused on the design of a \"spiral bio-reactor light sheet\", which consists of a plexiglas sheet that has been micro-etched on one side and rolled into a spiral shape.\n\nAside from aesthetic or utilitarian lighting purposes, much interest in using glow plates as a source of light comes from recent developments in algal cultivation.\n\n"}
{"id": "19662936", "url": "https://en.wikipedia.org/wiki?curid=19662936", "title": "Honna Dam", "text": "Honna Dam\n\n"}
{"id": "16500423", "url": "https://en.wikipedia.org/wiki?curid=16500423", "title": "Jamming (physics)", "text": "Jamming (physics)\n\nJamming is the physical process by which the viscosity of some mesoscopic materials, such as granular materials, glasses, foams, polymers, emulsions, and other complex fluids, increases with increasing particle density. The jamming transition has been proposed as a new type of phase transition, with similarities to a glass transition but very different from the formation of\ncrystalline solids.\n\nWhile a glass transition occurs when the liquid state is cooled, the jamming transition happens when density is increased. This crowding of the constituent particles prevents them from exploring phase space, making the aggregate material behave as a solid. The system may be able to unjam if volume fraction is decreased, or external stresses are applied such that they exceed the yield stress. This transition is interesting because it is highly nonlinear with respect to volume fraction.\n\nThe jamming phase diagram relates the jamming transition to inverse density, stress\nand temperature.\n\nThe density at which systems jam is determined by many factors, including the shape of their components, the deformability of the particles, frictional interparticle forces, and the degree of dispersity of the system. The overall shape of the jamming manifold may depend on the particular system. For example, a particularly interesting feature of the jamming transition is the difference between attractive and repulsive systems. Whether the jamming surface diverges for high enough densities or low temperatures is uncertain.\n\nSimulations of jammed systems study particle configurations leading to jamming in both static systems and systems under shear. Under shear stress, average cluster size may diverge after a finite amount of strain, leading to a jammed state. A particle configuration may exist in a jammed state with a stress required to “break” the force chains causing the jam.\n\nA static sand pile is jammed under the force of gravity and no energy is being dissipated. Systems which are consuming energy are also sometimes described as being jammed. An example is traffic jams, where due to jamming the average velocity of cars on a road may drop sharply. Here the cars on a road may be thought of like a granular material or a non-newtonian fluid that is being pumped through a tube. There under certain conditions the effective viscosity may rapidly increase, dramatically increasing the granular material or fluids's resistance to flowing and so causing the velocity to drop or even come to a complete stop. In this analogy the cars are like the grains in a granular material and if they are dense enough (i.e., closely enough spaced along the road) then interactions between the cars (as they must avoid each other to avoid crashing) cause jamming. A simple model of this behavior is the Nagel-Schreckenberg model.\n\n"}
{"id": "14178046", "url": "https://en.wikipedia.org/wiki?curid=14178046", "title": "Jean La Rose", "text": "Jean La Rose\n\nJean La Rose is an indigenous Arawak from Georgetown, Guyana. She was awarded the Goldman Environmental Prize in 2002 for her struggles to halt mining in their territories, to secure inhabitants full rights to traditional lands, and to save Guyana's forests.\n"}
{"id": "11322218", "url": "https://en.wikipedia.org/wiki?curid=11322218", "title": "KEGOC", "text": "KEGOC\n\nThe Kazakhstan Electricity Grid Operating Company (KEGOC) is a national transmission grid operator of Kazakhstan. It was established according to Government Resolution of 28 September 1996 on 11 July 1997. All of KEGOC's shares are held by the state-owned holding company Samruk-Kazyna.\n\n"}
{"id": "4721896", "url": "https://en.wikipedia.org/wiki?curid=4721896", "title": "KN-3 reactor", "text": "KN-3 reactor\n\nThe KN-3 reactor (VM-16) is the nuclear reactor used in pairs to power the \"Kirov\"-class battlecruisers. The reactor was also intended to be used on the \"Ulyanovsk\" class of supercarriers. It is a pressurized water reactor (PWR), using enriched uranium-235 fuel to produce 300 MW of power.\n\nIt was developed by OKBM Afrikantov.\n"}
{"id": "16589768", "url": "https://en.wikipedia.org/wiki?curid=16589768", "title": "LOT Polish Airlines Flight 165", "text": "LOT Polish Airlines Flight 165\n\nLOT Polish Airlines Flight LO 165 crashed 2 April 1969 at 16:08 local time (UTC+1) while en route from Warsaw to Krakow Balice airport during a snowstorm. \nIt crashed on the northern slope of Polica near Zawoja in southern Poland, hitting the mountain at an altitude of .\nThe plane was an Antonov An-24 aircraft, with registration SP-LTF. \nAll 53 people (47 passengers and 6 crew) on board were killed. There were three Americans and one London resident among the passengers, all others being Polish citizens.\n\nThe official accident report, published in 1970, blamed the pilot for getting lost.\nNo reasons were given why the aircraft, just before the crash, was flying at such a low altitude some past its intended destination.\n\nInformation given below comes from two newspaper articles published in 1994, with a summary written by a third party available on-line. The journalist wrote that even 25 years after the accident, most of the documentation remained classified, so his main sources were interviews with participants in the rescue action and some members of the accident investigation commission who asked for anonymity.\n\nThe aircraft took off at 15:20 local time for a 55-minute flight to Krakow's Balice airport. The captain was Czesław Doliński (with 20 years of flying in PLL LOT and more than two million kilometres of experience). \n\nAt 15.49, the first officer received a routine instruction: after passing Jędrzejów, less than 80 km north of the destination, descend to \n\nFor today, the official death toll of 53 killed is controversial. LOT manifest included 53 passengers and 5 crew members, but two days after the crash Polish press agencies published (based on LOT's information) 46 surnames (part of them without an address or name).\n\n"}
{"id": "47074767", "url": "https://en.wikipedia.org/wiki?curid=47074767", "title": "Lifshitz Theory of Van der Waals Force", "text": "Lifshitz Theory of Van der Waals Force\n\nIn condensed matter physics and physical chemistry, the Lifshitz theory of van der Waals forces, sometimes called the macroscopic theory of van der Waals forces, is a method proposed by Evgeny Mikhailovich Lifshitz in 1954 for treating van der Waals forces between bodies which does not assume pairwise additivity of the individual intermolecular forces; that is to say, the theory takes into account the influence of neighboring molecules on the interaction between every pair of molecules located in the two bodies, rather than treating each pair independently.\n\nThe van der Waals force between two molecules, in this context, is the sum of the attractive or repulsive forces between them; these forces are primarily electrostatic in nature, and in their simplest form might consist of a force between two charges, two dipoles, or between a charge and a dipole. Thus, the strength of the force may often depend on the net charge, electric dipole moment, or the electric polarizability (formula_1) (see for example London force) of the molecules, with highly polarizable molecules contributing to stronger forces, and so on.\n\nThe total force between two bodies, each consisting of many molecules in the van der Waals theory is simply the sum of the intermolecular van der Waals forces, where pairwise additivity is assumed. That is to say, the forces are summed as though each pair of molecules interacts completely independently of their surroundings (See Van der Waals forces between Macroscopic Objects for an example of such a treatment). This assumption is usually correct for gasses, but presents a problem for many condensed materials, as it is known that the molecular interactions may depend strongly on their environment and neighbors. For example, in a conductor, a point-like charge might be screened by the electrons in the conductance band, and the polarizability of a condensed material may be vastly different from that of an individual molecule. In order to correctly predict the van der Waals forces of condensed materials, a theory that takes into account their total electrostatic response is needed.\n\nThe problem of pairwise additivity is completely avoided in the Lifshitz theory, where the molecular structure is ignored and the bodies are treated as continuous media. The forces between the bodies are now derived in terms of their bulk properties, such as dielectric constant and refractive index, which already contain all the necessary information from the original molecular structure.\n\nThe original Lifshitz 1955 paper proposed this method relying on quantum field theory principles, and is, in essence, a generalization of the Casimir effect, from two parallel, flat, ideally conducting surfaces, to two surfaces of any material. Later papers by Langbein, Ninham, Parsegian and Van Kampen showed that the essential equations could be derived using much simpler theoretical techniques, an example of which is presented here.\n\nThe Lifshitz theory can be expressed as an effective Hamaker constant in the van der Waals theory.\n\nConsider, for example, the interaction between an ion of charge formula_2, and a nonpolar molecule with polarizability formula_3 at distance formula_4. In a medium with dielectric constant formula_5, the interaction energy between a charge and an electric dipole formula_6 is given by\n\nwith the dipole moment of the polarizable molecule given by formula_8, where formula_9 is the strength of the electric field at distance formula_4 from the ion. According to Coulomb's law:\n\nso we may write the interaction energy as\n\nConsider now, how the interaction energy will change if the right hand molecule is replaced with a medium of density formula_13 of such molecules. According to the \"classical\" van der Waals theory, the total force will simply be the summation over individual molecules. Integrating over the volume of the medium (see the third figure), we might expect the total interaction energy with the charge to be\n\nBut this result cannot be correct, since It is well known that a charge formula_2 in a medium of dielectric constant formula_5 at a distance formula_17 from the plane surface of a second medium of dielectric constant formula_18 experiences a force as if there were an 'image' charge of strength formula_19 at distance D on the other side of the boundary. The force between the real and image charges must then be\n\nand the energy, therefore\n\nEquating the two expressions for the energy, we define a new effective polarizability that must obey\n\nSimilarly, replacing the real charge formula_2 with a medium of density formula_24 and polarizability formula_25 gives an expression for formula_26. Using these two relations, we may restate our theory in terms of an effective Hamaker constant. Specifically, using McLachlan's generalized theory of VDW forces the Hamaker constant for an interaction potential of the form formula_27 between two bodies at temperature formula_28 is\n\nwith formula_30, where formula_31 and formula_32 are Boltzmann's and Planck's constants correspondingly. Inserting our relations for formula_33 and approximating the sum as an integral formula_34, the effective Hamaker constant in the Lifshitz theory may be approximated as\n\nWe note that formula_36 are real functions, and are related to measurable properties of the medium; thus, the Hamaker constant in the Lifshitz theory can be expressed in terms of observable properties of the physical system.\n\nThe macroscopic theory of van der Waals theory has many experimental validations. Among which, some of the most notable ones are Derjaguin (1960); Derjaguin, Abrikosova and Lifshitz (1956) and Israelachvili and Tabor (1973), who measured the balance of forces between macroscopic bodies of glass, or glass and mica; Haydon and Taylor (1968), who measured the forces across bilayers by measuring their contact angle; and lastly Shih and Parsegian (1975), who investigated Van der Waals potentials between heavy alkali-metal atoms and gold surfaces using atomic-beam-deflection.\n"}
{"id": "1046524", "url": "https://en.wikipedia.org/wiki?curid=1046524", "title": "List of national parks of Costa Rica", "text": "List of national parks of Costa Rica\n\nThere are currently 27 National Parks of Costa Rica, which are managed under the umbrella of SINAC (Sistema Nacional de Areas de Conservacion), a department of Costa Rica's Ministry of Environment and Energy (MINAE). All told, Costa Rica's protected areas encompass more than 25% of the country's total land area, more than any other country in the world. Many of these protected areas are national parks. \n\nCosta Rica's progressive policies on environmental protection and sustainable ecotourism in the National Parks System have been lauded as a model for other countries. The rainforests, tropical forests, marine areas and wetlands of Costa Rica are the subject of many university and scientific organization studies. The enrichment of the world's knowledge of these important habitats is an invaluable contribution from the National Parks System of Costa Rica. \n\nThe Cordillera de Talamanca is home to an impressive collection of national parks and other preserved areas, including the La Amistad International Park, which extends into Panamá. On the southern Osa Peninsula is the internationally renowned Corcovado National Park, which preserves a remnant of sizeable lowland tropical rainforest that is unique in the world. Manuel Antonio National Park was listed by Forbes in 2011 among the world's 12 most beautiful national parks.\n\n\n\n"}
{"id": "6855414", "url": "https://en.wikipedia.org/wiki?curid=6855414", "title": "List of pioneering solar buildings", "text": "List of pioneering solar buildings\n\nThe following buildings are of significance in pioneering the use of solar powered building design:\n\n\n"}
{"id": "996973", "url": "https://en.wikipedia.org/wiki?curid=996973", "title": "Magnesium fluoride", "text": "Magnesium fluoride\n\nMagnesium fluoride is an inorganic compound with the formula MgF. The compound is a white crystalline salt and is transparent over a wide range of wavelengths, with commercial uses in optics that are also used in space telescopes. It occurs naturally as the rare mineral sellaite.\n\nMagnesium fluoride is prepared from magnesium oxide with sources of hydrogen fluoride such as ammonium bifluoride:\nRelated metathesis reactions are also feasible.\n\nThe compound crystallizes as tetragonal birefringent crystals. The structure of the compound is similar to that in rutile, featuring octahedral Mg centers and 3-coordinate fluoride centres.\n\nMagnesium fluoride is transparent over an extremely wide range of wavelengths. Windows, lenses, and prisms made of this material can be used over the entire range of wavelengths from 0.120 μm (vacuum ultraviolet) to 8.0 μm (infrared). High quality synthetic VUV grade MgF is quite expensive, in the region of $3/kg (2007) but the real cost of optics in this material is due to relatively low volume manufacture. However, with lithium fluoride it is one of the two materials that will transmit in the vacuum ultraviolet range at 121 nm (Lyman alpha) and this is where it finds its application. Lower grade MgF is sometimes used in the infrared but here it is inferior to calcium fluoride. MgF is tough and works and polishes well, but it is slightly birefringent and should be cut with the optic axis perpendicular to the plane of the window or lens.\n\nDue to its having a suitable refractive index of 1.37, thin layers of MgF are very commonly used on the surfaces of optical elements as inexpensive anti-reflective coatings.\n\nThe Verdet constant of (MgF) at 632.8 nm is 0.00810arcmin/G⋅cm.\n\nChronic exposure to magnesium fluoride may affect the skeleton, kidneys, central nervous system, respiratory system, eyes and skin, and may cause or aggravate attacks of asthma.\n\n"}
{"id": "21540416", "url": "https://en.wikipedia.org/wiki?curid=21540416", "title": "Ministry of Energy and Power Development", "text": "Ministry of Energy and Power Development\n\nThe Ministry of Energy and Power Development is a government ministry, responsible for energy and electricity in Zimbabwe. The incumbent minister is Ambassador Joran Gumbo. It oversees:\n"}
{"id": "34973375", "url": "https://en.wikipedia.org/wiki?curid=34973375", "title": "Minsontaung Wildlife Sanctuary", "text": "Minsontaung Wildlife Sanctuary\n\nMinsontaung Wildlife Sanctuary or Minzontaung Wildlife Sanctuary is a protected area of Burma. It is located in the Natogyi Township area, Myingyan District, Mandalay Division. It occupies an area of and was established in 1998-99.\nThe dominant flora is dry forest. Tree species include \"Acacia nilotica\", \"Acacia catechu\", \"Acacia leucophloea\", \"Tectona hamiltoniana\", \"Terminalia oliveri\", \"Xylia dolabriformis\" and \"Prosopis juliflora\", as well as \"Dendrocalamus strictus\" bamboo clumps.\n\nThe sanctuary protects the endangered Eld's Deer and the Burmese star tortoise, a Critically Endangered species. It is named after Minson Taung, a small isolated massif with a maximum height of 398 m.\n\n"}
{"id": "9456188", "url": "https://en.wikipedia.org/wiki?curid=9456188", "title": "Monolithic architecture", "text": "Monolithic architecture\n\nMonolithic architecture describes buildings which are carved, cast or excavated from a single piece of material, historically from rock. The most basic form of monolithic architecture is a rock-cut building, such as the monolithic churches of Ethiopia built by the Zagwe dynasty, or the Pancha Rathas in India. These are cut out of solid rock, to which they remain attached at the base. In most cases this is evident from the remaining surrounding rock, but sometimes a building is cut from an outcrop, as in the Shore Temple in southern India, and only inspection at close quarters reveals that the building is monolithic.\n\nThe terms \"monolith\" and \"monolithic column\" are normally used for objects made from a single large piece of rock which is detached from the ground. They may have been moved a considerable distance, as with several ancient Egyptian obelisks, which have been moved across the world. Buildings with a structural material that is poured into place, most commonly concrete, can also be described as monolithic. Extreme examples are monolithic domes, where the material is sprayed inside of a form to produce the solid structure.\n\nAn ancient example of a monolithic dome is that of the Mausoleum of Theodoric in Ravenna, Italy, whose roof is made from a single stone.\n\n\n"}
{"id": "101783", "url": "https://en.wikipedia.org/wiki?curid=101783", "title": "Murray River National Park", "text": "Murray River National Park\n\nMurray River National Park is a protected area in South Australia located between and north east of the Adelaide city centre. The national park was proclaimed in 1972 \"to conserve a significant proportion of South Australia’s floodplain environments which are not represented widely in other reserve systems.\" The national park consists of three sections adjoining the Murray River and extending from near Loxton in the south west to near Renmark in the north-east. The first which is known as the ‘Katarapko’ section is located on the north side of the river between Loxton in the south and Berri in the north. The second section which is known as the ‘Lyrup Flats’ section is located on the north side of the river midway between Berri and Loxton. The third section is known as the ‘Bulyong’ section is located on the west side of the river upstream from Renmark. The national park is classified as an IUCN Category VI protected area.\n\n\n"}
{"id": "40794938", "url": "https://en.wikipedia.org/wiki?curid=40794938", "title": "Nemateleotris helfrichi", "text": "Nemateleotris helfrichi\n\nNemateleotris helfrichi, Helfrich's Dartfish, is a species of dartfish native to the Pacific Ocean.\n\nThey are small fishes that grow to a maximum length of . They have bright yellow head with purple forehead with purple and white shades on body. They have 7 dorsal spines 28-31 dorsal soft rays and 1 anal spine 26-28 anal soft rays.\n\nThey are present in the Pacific Ocean between latitudes 16°N and 28°S. Their range extends from Indonesia to Tuamotu Archipelago. They prefer deeper waters than other Nemateleotris. They are found swimming a few meters above the water bed. They are found at a depths between and are rarely found above .\n\nThey are carnivorous. \"N. helfrichi\" mainly feed on zooplanktons, amphipods, copepods, small crustaceans and shrimp larvae. They are said to lose their bright colour if their dietary requirements are not met.\n\nThe species is monogamous and are usually found in pairs. They are territorial and will fight with conspecifics unless they are a pair. The species is shy and hides in small caves when threatened.\n\nDue to their bright colour they are very popular as a marine aquarium fish. They are also more delicate and rare form than other more common firefish species (as evidenced by their price) and should be kept by marine-aquarists of intermediate expertise or higher.\n\nThe specific name honours Philip Helfrich, an Emeritus Director and Researcher, Hawaii Institute of Marine Biology (University of Hawaii), and the director of the Eniwetok Marine Biological Laboratory, who collected some of the first specimens of this species.\n"}
{"id": "55256554", "url": "https://en.wikipedia.org/wiki?curid=55256554", "title": "Neoen", "text": "Neoen\n\nNeoen is a renewable energy company headquartered in Paris, France.\n\nNeoen is mainly active in the solar power (59% of portfolio) and wind power (39%) sectors, for a total of 1 125 MW ′in operation or under construction′ in April 2017. They notably built Cestas Solar Park in Cestas, France, the biggest solar farm in Europe when it opened in 2015. \n\nAbroad, the company built Hornsdale Wind Farm in South Australia.\n\nThe company is funded in 2008 as a subsidiary of the French electricity reseller Direct Énergie. Later, it is bought by Impala SAS, currently main stockholder, and Omnes capital, which owned 24% of the stock. The company enters Euronext in October 2018, collecting 450 million euros and becoming the first French unicorn in the renewable energy field.\n"}
{"id": "27132425", "url": "https://en.wikipedia.org/wiki?curid=27132425", "title": "New Earth (Christianity)", "text": "New Earth (Christianity)\n\nThe New Earth is an expression used in the Book of Isaiah (Is 65:17 & 66:22), 2 Peter (2 Peter 3:13), and the Book of Revelation (Rev 21:1) in the Bible to describe the final state of redeemed humanity. It is one of the central doctrines of Christian eschatology and is referred to in the Nicene Creed as the world to come.\n\nThe twenty-first chapter of the Book of Revelation introduces the final state of perfection where, according to one commentator, \"cosmic time has been turned into eternity.\" In symbolic and visual language, God allows John to see the glory and beauty of the inheritance of His people. The first thing the reader notices about this vision is that it includes a \"new heavens and a new earth\" (21:1). To understand what the Bible teaches about eternity, the reader of the Apocalypse must understand the New Testament doctrine of the \"New Heavens and the New Earth.\" \n\nThe basic difference with the promises of the Old Testament is that in Revelation they also have an ontological value (: \"Then I saw 'a new heaven and a new earth,' for the first heaven and the first earth had passed away, and there was no longer any sea...'He will wipe every tear from their eyes. There will be no more death' or mourning or crying or pain, for the old order of things has passed away\") and no longer just gnosiological (: \"See, I will create/new heavens and a new earth./The former things will not be remembered,/nor will they come to mind\").\n\nBut, in accordance with his promise, we wait for new heavens and a new earth, where righteousness is at home ().\n\nIn Koine Greek, there were two words that are translated as \"new\" in the English Bible; \"neos\" and \"kainos\". One Greek resource states:\n\nThat \"kainos\" should not be taken as something totally new can be seen in a passage like the following:\nHere the Apostle Paul uses \"kainos\" in the expression \"new creation.\" Paul did not intend to convey the idea that this is a completely different individual. There is continuity between the old person and the new person to such an extent that it remains the same person, but renovated. The person is the same, but the quality of that person has been transformed.\n\nIn the same way, the biblical concept of the New Earth is one of renovation and restoration. Either on this current earth or on rebuilt new planet. This conclusion is supported by Peter's words in his public speech in the temple at Jerusalem. \n\nThis earth, however, will be either cleansed or destroyed by fire for the purpose of restoration as expressed in the following passage:\n\n\n"}
{"id": "5505922", "url": "https://en.wikipedia.org/wiki?curid=5505922", "title": "Ninian Central Platform", "text": "Ninian Central Platform\n\nThe Ninian Central Platform is an oil platform in the North Sea. When constructed in Loch Kishorn, Scotland in 1978 the 600,000 tonne platform was the world's largest man-made movable object before being towed to its current position and fixed to the sea floor.\n\nIt is a circular concrete gravity structure, 140 m in diameter at its base, with seven concentric walls of stepped heights intersected by radial walls at 45-degree angles. A 14 m wide central shaft is surrounded by a breakwater wall (\"Jarlin Wall\") 45 m in diameter and 1.6 m thick pierced with 1.5m diameter holes. Between these two walls drill slots are arranged for drilling up to 42 wells.\n\nOriginally operated by Chevron Petroleum (UK), a division of Chevron, it is currently operated by CNR International.\n\n"}
{"id": "22631711", "url": "https://en.wikipedia.org/wiki?curid=22631711", "title": "Optical rectenna", "text": "Optical rectenna\n\nAn optical rectenna is a rectenna (rectifying antenna) that works with visible or infrared light. A rectenna is a circuit containing an antenna and a diode, which turns electromagnetic waves into direct current electricity. While rectennas have long been used for radio waves or microwaves, an optical rectenna would operate the same way but with infrared or visible light, turning it into electricity.\n\nWhile traditional (radio- and microwave) rectennas are fundamentally similar to optical rectennas, it is vastly more challenging in practice to make an optical rectenna. One challenge is that light has such a high frequency—hundreds of terahertz for visible light—that only a few types of specialized diodes can switch quickly enough to rectify it. Another challenge is that antennas tend to be a similar size to a wavelength, so a very tiny optical antenna requires a challenging nanotechnology fabrication process. A third challenge is that, being very small, an optical antenna typically absorbs very little power, and therefore tend to produce a tiny voltage in the diode, which leads to low diode nonlinearity and hence low efficiency. Due to these and other challenges, optical rectennas have so far been restricted to laboratory demonstrations, typically with intense focused laser light producing a tiny but measurable amount of power.\n\nNevertheless, it is hoped that arrays of optical rectennas could eventually be an efficient means of converting sunlight into electric power, producing solar power more efficiently than conventional solar cells. The idea was first proposed by Robert L. Bailey in 1972. As of 2012, only a few optical rectenna devices have been built, demonstrating only that energy conversion is possible. It is unknown if they will ever be as cost-effective or efficient as conventional photovoltaic cells.\n\nThe term nantenna (nano-antenna) is sometimes used to refer to either an optical rectenna, or an optical antenna by itself. Currently, Idaho National Laboratories has designed an optical antenna to absorb wavelengths in the range of 3–15 μm. These wavelengths correspond to photon energies of down to . Based on antenna theory, an optical antenna can absorb any wavelength of light efficiently provided that the size of the antenna is optimized for that specific wavelength. Ideally, antennas would be used to absorb light at wavelengths between because these wavelengths have higher energy than far-infrared (longer wavelengths) and make up about 85% of the solar radiation spectrum (see Figure 1).\n\nRobert Bailey, along with James C. Fletcher, received a patent () in 1973 for an \"electromagnetic wave energy converter\". The patented device was similar to modern day optical rectennas. The patent discusses the use of a diode \"type described by [Ali Javan] in the IEEE Spectrum, October, 1971, page 91\", to whit, a 100 nm-diameter metal cat's whisker to a metal surface covered with a thin oxide layer. Javan was reported as having rectified 58 THz infrared light. In 1974, T. Gustafson and coauthors demonstrated that these types of devices could rectify even visible light to DC current Alvin M. Marks received a patent in 1984 for a device explicitly stating the use of sub-micron antennas for the direct conversion of light power to electrical power. Marks’s device showed substantial improvements in efficiency over Bailey’s device.\nIn 1996, Guang H. Lin reported resonant light absorption by a fabricated nanostructure and rectification of light with frequencies in the visible range. In 2002, ITN Energy Systems, Inc. published a report on their work on optical antennas coupled with high frequency diodes. ITN set out to build an optical rectenna array with single digit efficiency. Although they were unsuccessful, the issues associated with building a high efficiency optical rectenna were better understood.\n\nIn 2015, Baratunde A. Cola's research team at the Georgia Institute of Technology, developed a solar energy collector that can convert optical light to DC current, an optical rectenna using carbon nanotubes. Vertical arrays of multiwall carbon nanotubes (MWCNTs) grown on a metal-coated substrates were coated with insulating aluminum oxide and altogether capped with a metal electrode layer. The small dimensions of the nanotubes act as antennae, capable of capturing optical wavelengths. The MWCNT also doubles as one layer of a metal-insulator-metal (MIM) tunneling diode. Due to the small diameter of MWCNT tips, this combination forms a diode that is capable of rectifying the high frequency optical radiation. The overall achieved conversion efficiency of this device is around 10 %. Nonetheless, optical rectenna research is ongoing.\n\nThe primary drawback of these carbon nanotube rectenna devices is a lack of air stability. The device structure originally reported by Cola used calcium as a semitransparent top electrode because the low work function of calcium (2.9 eV) relative to MWCNTs (~5 eV) creates the diode asymmetry needed for optical rectification. However, metallic calcium is highly unstable in air and oxidizes rapidly. Measurements had to be made within a glovebox under an inert environment to prevent device breakdown. This limited practical application of the devices. \n\nCola and his team later solved the challenges with device instability by modifying the diode structure with multiple layers of oxide. In 2018 they reported the first air-stable optical rectenna along with efficiency improvements. \n\nThe air-stability of this new generation of rectenna was achieved by tailoring the diode's quantum tunneling barrier. Instead of a single dielectric insulator, they showed that the use of multiple dissimilar oxide layers enhances diode performance by modifying diode tunneling barrier. By using oxides with different electron affinities, the electron tunneling can be engineered to produce an asymmetric diode response regardless of the work function of the two electrodes. By using layers of Al2O3 and HfO2, a double-insulator diode was constructed that improved the diode's asymmetric response more than 10-fold without the need for low work function calcium, and the top metal was subsequently replaced with air-stable silver.\n\nFuture efforts have been undertaken to improve the device efficiency by investigating alternative materials, manipulating the MWCNTs and the insulating layers to encourage conduction at the interface, and reduce resistances within the structure.\n\nThe theory behind optical rectennas is essentially the same as for traditional (radio or microwave) rectennas. Incident light on the antenna causes electrons in the antenna to move back and forth at the same frequency as the incoming light. This is caused by the oscillating electric field of the incoming electromagnetic wave. The movement of electrons is an alternating current (AC) in the antenna circuit. To convert this into direct current (DC), the AC must be rectified, which is typically done with a diode. The resulting DC current can then be used to power an external load.\nThe resonant frequency of antennas (frequency which results in lowest impedance and thus highest efficiency) scales linearly with the physical dimensions of the antenna according to simple microwave antenna theory. The wavelengths in the solar spectrum range from approximately 0.3-2.0 μm. Thus, in order for a rectifying antenna to be an efficient electromagnetic collector in the solar spectrum, it needs to be on the order of hundreds of nm in size.\n\nBecause of simplifications used in typical rectifying antenna theory, there are several complications that arise when discussing optical rectennas. At frequencies above infrared, almost all of the current is carried near the surface of the wire which reduces the effective cross sectional area of the wire, leading to an increase in resistance. This effect is also known as the “skin effect”. From a purely device perspective, the I-V characteristics would appear to no longer be ohmic, even though Ohm’s law, in its generalized vector form, is still valid.\n\nAnother complication of scaling down is that diodes used in larger scale rectennas cannot operate at THz frequencies without large loss in power. The large loss in power is a result of the junction capacitance (also known as parasitic capacitance) found in p-n junction diodes and Schottky diodes, which can only operate effectively at frequencies less than 5 THz. The ideal wavelengths of 0.4–1.6 μm correspond to frequencies of approximately 190–750 THz, which is much larger than the capabilities of typical diodes. Therefore, alternative diodes need to be used for efficient power conversion. In current optical rectenna devices, metal-insulator-metal (MIM) tunneling diodes are used. Unlike Schottky diodes, MIM diodes are not affected by parasitic capacitances because they work on the basis of electron tunneling. Because of this, MIM diodes have been shown to operate effectively at frequencies around .\n\nOne of the biggest claimed advantages of optical rectennas is their high theoretical efficiency. When compared to the theoretical efficiency of single junction solar cells (30%), optical rectennas appear to have a significant advantage. However, the two efficiencies are calculated using different assumptions.\nThe assumptions involved in the rectenna calculation are based on the application of the Carnot efficiency of solar collectors. The Carnot efficiency, η, is given by\n\nwhere T is the temperature of the cooler body and T is the temperature of the warmer body. In order for there to be an efficient energy conversion, the temperature difference between the two bodies must be significant. claims that rectennas are not limited by Carnot efficiency, whereas photovoltaics are. However, he does not provide any argument for this claim. Furthermore, when the same assumptions used to obtain the 85% theoretical efficiency for rectennas are applied to single junction solar cells, the theoretical efficiency of single junction solar cells is also greater than 85%.\n\nThe most apparent advantage optical rectennas have over semiconductor photovoltaics is that rectenna arrays can be designed to absorb any frequency of light. The resonant frequency of an optical antenna can be selected by varying its length. This is an advantage over semiconductor photovoltaics, because in order to absorb different wavelengths of light, different band gaps are needed. In order to vary the band gap, the semiconductor must be alloyed or a different semiconductor must be used altogether.\n\nAs previously stated, one of the major limitations of optical rectennas is the frequency at which they operate. The high frequency of light in the ideal range of wavelengths makes the use of typical Schottky diodes impractical. Although MIM diodes show promising features for use in optical rectennas, more advances are necessary to operate efficiently at higher frequencies.\n\nAnother disadvantage is that current optical rectennas are produced using electron beam (e-beam) lithography. This process is slow and relatively expensive because parallel processing is not possible with e-beam lithography. Typically, e-beam lithography is used only for research purposes when extremely fine resolutions are needed for minimum feature size (typically, on the order of nanometers). However, photolithographic techniques have advanced to where it is possible to have minimum feature sizes on the order of tens of nanometers, making it possible to produce rectennas by means of photolithography.\n\nAfter the proof of concept was completed, laboratory-scale silicon wafers were fabricated using standard semiconductor integrated circuit fabrication techniques. E-beam lithography was used to fabricate the arrays of loop antenna metallic structures. The optical antenna consists of three main parts: the ground plane, the optical resonance cavity, and the antenna. The antenna absorbs the electromagnetic wave, the ground plane acts to reflect the light back towards the antenna, and the optical resonance cavity bends and concentrates the light back towards the antenna via the ground plane. This work did not include production of the diode.\n\nIdaho National Labs used the following steps to fabricate their optical antenna arrays. A metallic ground plane was deposited on a bare silicon wafer, followed by a sputter deposited amorphous silicon layer. The depth of the deposited layer was about a quarter of a wavelength. A thin manganese film along with a gold frequency selective surface (to filter wanted frequency) was deposited to act as the antenna. Resist was applied and patterned via electron beam lithography. The gold film was selectively etched and the resist was removed.\n\nIn moving up to a greater production scale, laboratory processing steps such as the use of electron beam lithography are slow and expensive. Therefore, a roll-to-roll manufacturing method was devised using a new manufacturing technique based on a master pattern. This master pattern mechanically stamps the precision pattern onto an inexpensive flexible substrate and thereby creates the metallic loop elements seen in the laboratory processing steps. The master template fabricated by Idaho National Laboratories consists of approximately 10 billion antenna elements on an 8-inch round silicon wafer. Using this semi-automated process, Idaho National Labs has produced a number of 4-inch square coupons. These coupons were combined to form a broad flexible sheet of antenna arrays. This work did not include production of the diode component.\n\nResearchers at the University of Connecticut are using a technique called selective area atomic layer deposition that is capable of producing them reliably and at industrial scales. Research is ongoing to tune them to the optimal frequencies for visible and infrared light.\n\nThe proof of principle for optical antennas (but not optical-frequency diodes) started out with a 1 cm silicon substrate with the printed antenna array filling the area. The device was tested using infrared light with a range of 3 to 15 µm. The peak emissivity is found to be centered at 6.5 µm and reaches an emissivity of 1. An emissivity of 1 means the antenna absorbs all of the photons of a specific wavelength (in this case, 6.5 µm) that are incident upon the device. Comparing the experimental spectrum to the modeled spectrum, the experimental results are in agreement with theoretical expectations (Figure 5). In some areas, the antenna had a lower emissivity than the theoretical expectations, but in other areas, namely at around 3.5 µm, the device absorbed more light than expected.\n\nAfter a proof of concept on a stiff silicon substrate, the experiment was replicated on a flexible polymer-based substrate. The target wavelength for the flexible substrate was set to 10 µm. Initial tests show that the antenna design can be translated to a polymer substrate, but further experiments are needed to fully optimize the characteristics.\n\nThe proof of principle of optical antennas can be realized in radio frequency range. High permittivity dielectric particles can be used to simulate silicon behaviour at optical frequencies. This allows to perform experiment at microwave in order to predict antenna behavior at optics.\n\nOptical antennas (by itself, omitting the crucial diode and other components) are cheaper than photovoltaics (if efficiency is ignored). While materials and processing of photovoltaics are expensive (currently the cost for complete photovoltaic modules is in the order of in 2011 and declining.), Steven Novack estimates the current cost of the antenna material itself as around in 2008. With proper processing techniques and different material selection, he estimates that the overall cost of processing, once properly scaled up, will not cost much more. His prototype was a of plastic, which contained only of gold in 2008, with the possibility of downgrading to a material such as aluminum, copper, or silver. The prototype used a silicon substrate due to familiar processing techniques, but any substrate could theoretically be used as long as the ground plane material adheres properly.\n\nIn an interview on National Public Radio's Talk of the Nation, Dr. Novack claimed that optical rectennas could one day be used to power cars, charge cell phones, and even cool homes. Novack claimed the last of these will work by both absorbing the infrared heat available in the room and producing electricity which could be used to further cool the room. (Other scientists have disputed this, saying it would violate the second law of thermodynamics.)\n\nImproving the diode is an important challenge. There are two challenging requirements: Speed and nonlinearity. First, the diode must have sufficient speed to rectify visible light. Second, unless the incoming light is extremely intense, the diode needs to be extremely nonlinear (much higher forward current than reverse current), in order to avoid \"reverse-bias leakage\". An assessment for solar energy collection found that, to get high efficiency, the diode would need a (dark) current much lower than 1μA at 1V reverse bias. This assessment assumed (optimistically) that the antenna was a directional antenna array pointing directly at the sun; a rectenna that collects light from the whole sky, like a typical silicon solar cell does, would need the reverse-bias current to be even lower still, by orders of magnitude. (The diode simultaneously needs a high forward-bias current, related to impedance-matching to the antenna.)\n\nThere are special diodes for high speed (e.g., the metal-insulator-metal tunnel diodes discussed above), and there are special diodes for high nonlinearity, but it is quite difficult to find a diode that is outstanding in both respects at once.\n\nTo improve the efficiency of carbon nanotube-based rectenna:\n\nResearchers currently hope to create a rectifier which can convert around 50% of the antenna's absorption into energy.\nAnother focus of research will be how to properly upscale the process to mass-market production. New materials will need to be chosen and tested that will easily comply with a roll-to-roll manufacturing process. Future goals will be to attempt to manufacture devices on pliable substrates to create flexible solar cells.\n\n"}
{"id": "5890627", "url": "https://en.wikipedia.org/wiki?curid=5890627", "title": "PAR thrust", "text": "PAR thrust\n\nPower Augmented Ram thrust (or PAR thrust for short) is the use of displaced air in enhancing the air cushion under the wings or body of what is normally a Wing In Ground-effect or WIG craft. The system can also be used in surface effect ships to increase their top speed.\n\nThe configuration can use either the exhaust gases from jet engines or propellers slipstream from either the main engines or special assist engines. This accelerated air is then either ducted, deflected or directed in such a manner that it will pass underneath the wing or body and assist in the creation of an air cushion. The purpose of using PAR thrust is to allow the craft to take off at lower speeds than those required if the system was not used.\n\nSome craft only use PAR thrust during takeoff; notable examples of this are the KM and Lun Ekranoplanes that situated their jet engines forward of the wing and deflected the thrust downwards under the wing. Once airborne and at a speed where sufficient lift was generated by the wing alone, the deflection could be removed. The KM required 10 turbojets to have enough power to take off, 8 of them operating in PAR mode; once in level flight, the engines could be throttled back extensively or some of them even shut off.\n\nA notable spin-off of this technique was the A-90 Orlyonok ekranoplane, which was powered in flight by a massive Kuznetsov NK-12 turboprop (by far the most powerful ever built), and required two large turbofans embedded in the nose for take-off, whose ducts pointed permanently downwards under the wings to provide PAR thrust. These engines were shut down during flight to reduce fuel consumption.\n\nSome other craft such as Burt Rutan's PARLC, Russian Strizh PE-201, Volga 2, Ivolga and other WIG craft use this system. Boats that use this system (surface effect ships) require permanent use of PAR thrust in order to achieve enough lift for normal operation.\n"}
{"id": "28544052", "url": "https://en.wikipedia.org/wiki?curid=28544052", "title": "Parablennius intermedius", "text": "Parablennius intermedius\n\nParablennius intermedius, the false Tasmanian blenny or horned blenny, is a species of combtooth blenny found in the Indian ocean near Australian coasts. This species reaches a length of TL.\n"}
{"id": "9051795", "url": "https://en.wikipedia.org/wiki?curid=9051795", "title": "Prime Ministerial Task Group on Emissions Trading", "text": "Prime Ministerial Task Group on Emissions Trading\n\nPrime Ministerial Task Group on Emissions Trading was a Task Group set up on 10 December 2006 by Australian Prime Minister John Howard to develop an Australian Carbon Trading Scheme. The terms of reference of the task group was:\n\nThe Prime Ministerial Task Group submitted its final report on 31 May 2007. The scheme proposed by the Task Group had some similarities to the \"hybrid scheme\" developed by Warwick McKibbin. \n\nPrime Minister John Howard announced on 4 June 2007 the government's plan for a carbon trading scheme to be launched in 2012. Howard took the draft carbon trading scheme to the 2007 federal election.\n\nAustralian state and territory governments, through the Council for the Australian Federation (CAF) in January 2004 set up a working group to investigate the design of a national emissions trading scheme (NETS). This working group became the National Emissions Trading Taskforce (NETT). NETT reported progress to CAF in December 2004, including the development of 10 key design propositions as a basis for further investigation and analysis of a NETS. After extensive consultations, on 28 August 2006 NETT published its final report. On 9 February 2007 CAF declared: \n\nOn 30 April 2007, the federal Labor Opposition and the Labor-controlled state and territory governments commissioned the separate Garnaut Climate Change Review. \n\nThe Task Group members included:\n\n\n"}
{"id": "40468305", "url": "https://en.wikipedia.org/wiki?curid=40468305", "title": "Primus Power", "text": "Primus Power\n\nPrimus Power is a producer of stationary energy storage systems. Their batteries support the electric grid. Based in Hayward, California, it has developed a zinc bromine flow battery, called the EnergyPod. Since their founding in 2009 Primus has received $100M in equity capital from international venture capital and strategic investors, and $25M in government grants from the US Department of Energy, Advanced Research Projects Agency-Energy, US Trade and Development Agency, and the California Energy Commission.\n\nEnergyPods are modular batteries for commercial, industrial, microgrid and utility customers. The low maintenance systems provide five hour of discharge energy and are rated for a twenty year life span, necessary for grid applications. Primus has numerous customers including Microsoft, Puget Sound Energy, Miramar Marine Corps Airstation , HCP, Goldwind, Israel Chemicals and Victor Valley Wastewater Reclamation Authority. \n\nOn February 21, 2017, the company announced production of their second generation product, EnergyPod 2. Streamlined for production, it features a small footprint, reduced cost of goods, high reliability components, and a sturdier enclosure. The modular unit is designed to last twenty years and supply a system capability from 25 kW to 25 MW with five hour discharge duration. \n\nThe company's energy storage systems decouple instantaneous electricity demand from supply generation, bolstering the stability and security of the electric grid, and helping accelerate the penetration of wind and solar energy. The company has sixty two patents in chemistry, cell design, and system engineering granted by over a dozen countries. \n\n"}
{"id": "39438424", "url": "https://en.wikipedia.org/wiki?curid=39438424", "title": "Recalescence", "text": "Recalescence\n\nRecalescence is an increase in temperature that occurs while cooling metal when a change in structure with an increase in entropy occurs. The heat responsible for the change in temperature is due to the change in entropy. When a structure transformation occurs the Gibbs free energy of both structures are more or less the same. Therefore the process will be exothermic. The heat provided is the latent heat.\n\nRecalescence also occurs after supercooling, when the supercooled liquid suddenly crystallizes, forming a solid but releasing heat in the process.\n\n"}
{"id": "41245749", "url": "https://en.wikipedia.org/wiki?curid=41245749", "title": "Recharge (magazine)", "text": "Recharge (magazine)\n\nRecharge is a business news website and bi-monthly magazine covering the global renewable energy industry, particularly wind and solar power. It is owned by Norway's NHST Media Group, but headquartered in London, with full-time editorial staff in the US, UK, Brazil, and Germany.\n\n\"Recharge\" has been described as \"one of the most authoritative publications in the renewable energy sector\", and as \"a role model for the future of trade journalism\" by German industrial giant Siemens.\n\nIts breaking stories have been picked up by major international news organizations, including the BBC, \"The Washington Post\" and Denmark's \"Dagbladet Børsen\".\n\n\"Recharge\" was first established in January 2009 as a weekly newspaper, before becoming a monthly glossy magazine in January 2013.\n\nIt also produces Daily newspapers at industry trade events including the European Wind Energy Association's (how WindEurope's) annual conference and exhibition and biannual offshore iteration, as well as for the American Wind Energy Association's annual expo.\n\n\"Recharge\"'s \"Thought Leaders Summit\" was an invitation-only conference for senior international renewable-energy executives held in Holmenkollen (Norway) on January 9, 2014. It is intended to become an annual event and operates under the Chatham House rule.\n\nThe meeting was attended by about 50 senior professionals from the global renewable-energy industry, including high-level executives from Siemens, Vestas, Statoil and E.ON. The keynote speech was delivered by Scotland's energy minister, Fergus Ewing and the event was officially opened by Henrik O. Madsen.\n\nIn 2010, \"Recharge\" was awarded the Advocate of the Year award by the UK's Renewable Energy Association, the first time the prize had been given to a publication.\n"}
{"id": "3326805", "url": "https://en.wikipedia.org/wiki?curid=3326805", "title": "Reedy Creek Energy Services", "text": "Reedy Creek Energy Services\n\nReedy Creek Energy Services (RCES) is a wholly owned subsidiary of The Walt Disney Company. It operates the electric and other utility transmission and distribution systems of the Reedy Creek Improvement District (RCID) on behalf of the district which specifically covers Walt Disney World outside Orlando, Florida. Some power is produced by the district-owned power plant north of The Magic Kingdom with the remainder purchased from the public power grid. Officially the utility systems are owned by the district entity itself and the district \"contracts\" with RCES to operate the systems. \n\nIn addition to electric power, RCES handles all public services and public works for the RCID:\n\n\nRCES is currently Disney's only non-entertainment-related subsidiary. Until 2001, Disney and Sprint owned Vista-United Telecommunications, a telephone company that served the RCID. That company has since been sold to Smart City Telecom.\n\nReedy Creek Improvement District- Utilities\n"}
{"id": "26310", "url": "https://en.wikipedia.org/wiki?curid=26310", "title": "Reproduction", "text": "Reproduction\n\nReproduction (or procreation or breeding) is the biological process by which new individual organisms – \"offspring\" – are produced from their \"parents\". Reproduction is a fundamental feature of all known life; each individual organism exists as the result of reproduction. There are two forms of reproduction: asexual and sexual.\n\nIn asexual reproduction, an organism can reproduce without the involvement of another organism. Asexual reproduction is not limited to single-celled organisms. The cloning of an organism is a form of asexual reproduction. By asexual reproduction, an organism creates a genetically similar or identical copy of itself. The evolution of sexual reproduction is a major puzzle for biologists. The two-fold cost of sexual reproduction is that only 50% of organisms reproduce and organisms only pass on 50% of their genes.\n\nSexual reproduction typically requires the sexual interaction of two specialized organisms, called gametes, which contain half the number of chromosomes of normal cells and are created by meiosis, with typically a male fertilizing a female of the same species to create a fertilized zygote. This produces offspring organisms whose genetic characteristics are derived from those of the two parental organisms.\n\nAsexual reproduction is a process by which organisms create genetically similar or identical copies of themselves without the contribution of genetic material from another organism. Bacteria divide asexually via binary fission; viruses take control of host cells to produce more viruses; Hydras (invertebrates of the order \"Hydroidea\") and yeasts are able to reproduce by budding. These organisms often do not possess different sexes, and they are capable of \"splitting\" themselves into two or more copies of themselves. Most plants have the ability to reproduce asexually and the ant species Mycocepurus smithii is thought to reproduce entirely by asexual means.\n\nSome species that are capable of reproducing asexually, like hydra, yeast (See Mating of yeasts) and jellyfish, may also reproduce sexually. For instance, most plants are capable of vegetative reproduction—reproduction without seeds or spores—but can also reproduce sexually. Likewise, bacteria may exchange genetic information by conjugation.\n\nOther ways of asexual reproduction include parthenogenesis, fragmentation and spore formation that involves only mitosis. Parthenogenesis is the growth and development of embryo or seed without fertilization by a male. Parthenogenesis occurs naturally in some species, including lower plants (where it is called apomixis), invertebrates (e.g. water fleas, aphids, some bees and parasitic wasps), and vertebrates (e.g. some\nreptiles, fish, and, very rarely, birds and sharks). It is sometimes also used to describe reproduction modes in hermaphroditic species which can self-fertilize.\n\nSexual reproduction is a biological process that creates a new organism by combining the genetic material of two organisms in a process that starts with meiosis, a specialized type of cell division. Each of two parent organisms contributes half of the offspring's genetic makeup by creating haploid gametes. Most organisms form two different types of gametes. In these anisogamous species, the two sexes are referred to as male (producing sperm or microspores) and female (producing ova or megaspores). In isogamous species, the gametes are similar or identical in form (isogametes), but may have separable properties and then may be given other different names (see isogamy). For example, in the green alga, \"Chlamydomonas reinhardtii\", there are so-called \"plus\" and \"minus\" gametes. A few types of organisms, such as many fungi and the ciliate \"Paramecium aurelia\", have more than two \"sexes\", called syngens.\nMost animals (including humans) and plants reproduce sexually. Sexually reproducing organisms have different sets of genes for every trait (called alleles). Offspring inherit one allele for each trait from each parent. Thus, offspring have a combination of the parents' genes. It is believed that \"the masking of deleterious alleles favors the evolution of a dominant diploid phase in organisms that alternate between haploid and diploid phases\" where recombination occurs freely.\n\nBryophytes reproduce sexually, but the larger and commonly-seen organisms are haploid and produce gametes. The gametes fuse to form a zygote which develops into a sporangium, which in turn produces haploid spores. The diploid stage is relatively small and short-lived compared to the haploid stage, i.e. \"haploid dominance\". The advantage of diploidy, heterosis, only exists in the diploid life generation. Bryophytes retain sexual reproduction despite the fact that the haploid stage does not benefit from heterosis. This may be an indication that the sexual reproduction has advantages other than heterosis, such as genetic recombination between members of the species, allowing the expression of a wider range of traits and thus making the population more able to survive environmental variation.\n\nAllogamy is the fertilization of the combination of gametes from two parents, generally the ovum from one individual with the spermatozoa of another. (In isogamous species, the two gametes will not be defined as either sperm or ovum.)\n\nSelf-fertilization, also known as autogamy, occurs in hermaphroditic organisms where the two gametes fused in fertilization come from the same individual, e.g., many vascular plants, some foraminiferans, some ciliates. The term \"autogamy\" is sometimes substituted for autogamous pollination (not necessarily leading to successful fertilization) and describes self-pollination within the same flower, distinguished from geitonogamous pollination, transfer of pollen to a different flower on the same flowering plant, or within a single monoecious Gymnosperm plant.\n\nMitosis and meiosis are types of cell division. Mitosis occurs in somatic cells, while meiosis occurs in gametes.\n\nMitosis\nThe resultant number of cells in mitosis is twice the number of original cells. The number of chromosomes in the offspring cells is the same as that of the parent cell.\n\nMeiosis\nThe resultant number of cells is four times the number of original cells. This results in cells with half the number of chromosomes present in the parent cell. A diploid cell duplicates itself, then undergoes two divisions (tetraploid to diploid to haploid), in the process forming four haploid cells. This process occurs in two phases, meiosis I and meiosis II.\nIn recent decades, developmental biologists have been researching and developing techniques to facilitate same-sex reproduction. The obvious approaches, subject to a growing amount of activity, are female sperm and male eggs, with female sperm closer to being a reality for humans, given that Japanese scientists have already created female sperm for chickens. \"However, the ratio of produced W chromosome-bearing (W-bearing) spermatozoa fell substantially below expectations. It is therefore concluded that most of the W-bearing PGC could not differentiate into spermatozoa because of restricted spermatogenesis.\" In 2004, by altering the function of a few genes involved with imprinting, other Japanese scientists combined two mouse eggs to produce daughter mice and in 2018 Chinese scientists created 29 female mice from two female mice mothers but were unable to produce viable offspring from two father mice.\n\nThere are a wide range of reproductive strategies employed by different species. Some animals, such as the human and northern gannet, do not reach sexual maturity for many years after birth and even then produce few offspring. Others reproduce quickly; but, under normal circumstances, most offspring do not survive to adulthood. For example, a rabbit (mature after 8 months) can produce 10–30 offspring per year, and a fruit fly (mature after 10–14 days) can produce up to 900 offspring per year. These two main strategies are known as K-selection (few offspring) and r-selection (many offspring). Which strategy is favoured by evolution depends on a variety of circumstances. Animals with few offspring can devote more resources to the nurturing and protection of each individual offspring, thus reducing the need for many offspring. On the other hand, animals with many offspring may devote fewer resources to each individual offspring; for these types of animals it is common for many offspring to die soon after birth, but enough individuals typically survive to maintain the population. Some organisms such as honey bees and fruit flies retain sperm in a process called sperm storage thereby increasing the duration of their fertility.\n\n\nOrganisms that reproduce through asexual reproduction tend to grow in number exponentially. However, because they rely on mutation for variations in their DNA, all members of the species have similar vulnerabilities. Organisms that reproduce sexually yield a smaller number of offspring, but the large amount of variation in their genes makes them less susceptible to disease.\n\nMany organisms can reproduce sexually as well as asexually. Aphids, slime molds, sea anemones, some species of starfish (by fragmentation), and many plants are examples. When environmental factors are favorable, asexual reproduction is employed to exploit suitable conditions for survival such as an abundant food supply, adequate shelter, favorable climate, disease, optimum pH or a proper mix of other lifestyle requirements. Populations of these organisms increase exponentially via asexual reproductive strategies to take full advantage of the rich supply resources.\n\nWhen food sources have been depleted, the climate becomes hostile, or individual survival is jeopardized by some other adverse change in living conditions, these organisms switch to sexual forms of reproduction. Sexual reproduction ensures a mixing of the gene pool of the species. The variations found in offspring of sexual reproduction allow some individuals to be better suited for survival and provide a mechanism for selective adaptation to occur. The meiosis stage of the sexual cycle also allows especially effective repair of DNA damages (see Meiosis and Bernstein et al.). In addition, sexual reproduction usually results in the formation of a life stage that is able to endure the conditions that threaten the offspring of an asexual parent. Thus, seeds, spores, eggs, pupae, cysts or other \"over-wintering\" stages of sexual reproduction ensure the survival during unfavorable times and the organism can \"wait out\" adverse situations until a swing back to suitability occurs.\n\nThe existence of life without reproduction is the subject of some speculation. The biological study of how the origin of life produced reproducing organisms from non-reproducing elements is called abiogenesis. Whether or not there were several independent abiogenetic events, biologists believe that the last universal ancestor to all present life on Earth lived about 3.5 billion years ago.\n\nScientists have speculated about the possibility of creating life non-reproductively in the laboratory. Several scientists have succeeded in producing simple viruses from entirely non-living materials. However, viruses are often regarded as not alive. Being nothing more than a bit of RNA or DNA in a protein capsule, they have no metabolism and can only replicate with the assistance of a hijacked cell's metabolic machinery.\n\nThe production of a truly living organism (e.g. a simple bacterium) with no ancestors would be a much more complex task, but may well be possible to some degree according to current biological knowledge. A synthetic genome has been transferred into an existing bacterium where it replaced the native DNA, resulting in the artificial production of a new \"M. mycoides\" organism.\n\nThere is some debate within the scientific community over whether this cell can be considered completely synthetic on the grounds that the chemically synthesized genome was an almost 1:1 copy of a naturally occurring genome and, the recipient cell was a naturally occurring bacterium. The Craig Venter Institute maintains the term \"synthetic bacterial cell\" but they also clarify \"...we do not consider this to be \"creating life from scratch\" but rather we are creating new life out of already existing life using synthetic DNA\". Venter plans to patent his experimental cells, stating that \"they are pretty clearly human inventions\". Its creators suggests that building 'synthetic life' would allow researchers to learn about life by building it, rather than by tearing it apart. They also propose to stretch the boundaries between life and machines until the two overlap to yield \"truly programmable organisms\". Researchers involved stated that the creation of \"true synthetic biochemical life\" is relatively close in reach with current technology and cheap compared to the effort needed to place man on the Moon.\n\nSexual reproduction has many drawbacks, since it requires far more energy than asexual reproduction and diverts the organisms from other pursuits, and there is some argument about why so many species use it. George C. Williams used lottery tickets as an analogy in one explanation for the widespread use of sexual reproduction. He argued that asexual reproduction, which produces little or no genetic variety in offspring, was like buying many tickets that all have the same number, limiting the chance of \"winning\" - that is, producing surviving offspring. Sexual reproduction, he argued, was like purchasing fewer tickets but with a greater variety of numbers and therefore a greater chance of success. The point of this analogy is that since asexual reproduction does not produce genetic variations, there is little ability to quickly adapt to a changing environment. The lottery principle is less accepted these days because of evidence that asexual reproduction is more prevalent in unstable environments, the opposite of what it predicts.\n\n\n\n"}
{"id": "20514139", "url": "https://en.wikipedia.org/wiki?curid=20514139", "title": "Ship graveyard", "text": "Ship graveyard\n\nA ship graveyard or ship cemetery is a location where the hulls of scrapped ships are left to decay and disintegrate, or left in reserve. Such a practice is now less common due to waste regulations and so some dry docks where ships are broken (to recycle their metal and remove dangerous materials like asbestos) are also known as ship graveyards.\n\nBy analogy, the phrase can also refer to a large number of shipwrecks which have accumulated in a single area but not been removed by human agency, instead being left to disintegrate naturally. These can form in places where navigation is difficult or dangerous (such as the Seven Stones, off Cornwall, or Blackpool, on the Irish Sea); or where a large number of ships have been deliberately scuttled together (as with the German High Seas Fleet at Scapa Flow); or where a large number of ships have been sunk in battle (such as Ironbottom Sound, in the pacific).\n\n\n\n\n\n\nAll states and territories of Australia, except the land-locked Australian Capital Territory, have ships' graveyards\n\nNew South Wales:\nNorthern Territory:\nQueensland:\nSouth Australia:\n\n\nTasmania:\nVictoria:\nWestern Australia:\n\n\n"}
{"id": "51510", "url": "https://en.wikipedia.org/wiki?curid=51510", "title": "Silk", "text": "Silk\n\nSilk is a natural protein fiber, some forms of which can be woven into textiles. The protein fiber of silk is composed mainly of fibroin and is produced by certain insect larvae to form cocoons. The best-known silk is obtained from the cocoons of the larvae of the mulberry silkworm \"Bombyx mori\" reared in captivity (sericulture). The shimmering appearance of silk is due to the triangular prism-like structure of the silk fibre, which allows silk cloth to refract incoming light at different angles, thus producing different colors.\n\nSilk is produced by several insects, like silk worms but generally only the silk of moth caterpillars has been used for textile manufacturing. There has been some research into other types of silk, which differ at the molecular level. Silk is mainly produced by the larvae of insects undergoing complete metamorphosis, but some insects such as webspinners and raspy crickets produce silk throughout their lives. Silk production also occurs in Hymenoptera (bees, wasps, and ants), silverfish, mayflies, thrips, leafhoppers, beetles, lacewings, fleas, flies, and midges. Other types of arthropod produce silk, most notably various arachnids such as spiders.\n\nThe word silk comes from , from , \"silken\", ultimately from an Asian source — compare Mandarin \"silk\", Manchurian , Mongolian .\n\nSeveral kinds of wild silk, which are produced by caterpillars other than the mulberry silkworm, have been known and used in China, South Asia, and Europe since ancient times. However, the scale of production was always far smaller than for cultivated silks. There are several reasons for this: first, they differ from the domesticated varieties in colour and texture and are therefore less uniform; second, cocoons gathered in the wild have usually had the pupa emerge from them before being discovered so the silk thread that makes up the cocoon has been torn into shorter lengths; and third, many wild cocoons are covered in a mineral layer that prevents attempts to reel from them long strands of silk. Thus, the only way to obtain silk suitable for spinning into textiles in areas where commercial silks are not cultivated was by tedious and labor-intensive carding.\n\nCommercial silks originate from reared silkworm pupae, which are bred to produce a white-colored silk thread with no mineral on the surface. The pupae are killed by either dipping them in boiling water before the adult moths emerge or by piercing them with a needle. These factors all contribute to the ability of the whole cocoon to be unravelled as one continuous thread, permitting a much stronger cloth to be woven from the silk.\nWild silks also tend to be more difficult to dye than silk from the cultivated silkworm. A technique known as demineralizing allows the mineral layer around the cocoon of wild silk moths to be removed, leaving only variability in color as a barrier to creating a commercial silk industry based on wild silks in the parts of the world where wild silk moths thrive, such as in Africa and South America.\n\nSilk was first developed in ancient China.\n\nThe earliest example of silk has been found in tombs at the neolithic site Jiahu in Henan, and dates back 8,500 years. Silk fabric from 3630 BC was used as wrapping for the body of a child from a Yangshao culture site in Qingtaicun at Xingyang, Henan.\n\nLegend gives credit for developing silk to a Chinese empress, Leizu (Hsi-Ling-Shih, Lei-Tzu). Silks were originally reserved for the Emperors of China for their own use and gifts to others, but spread gradually through Chinese culture and trade both geographically and socially, and then to many regions of Asia. Because of its texture and lustre, silk rapidly became a popular luxury fabric in the many areas accessible to Chinese merchants. Silk was in great demand, and became a staple of pre-industrial international trade. In July 2007, archaeologists discovered intricately woven and dyed silk textiles in a tomb in Jiangxi province, dated to the Eastern Zhou Dynasty roughly 2,500 years ago. Although historians have suspected a long history of a formative textile industry in ancient China, this find of silk textiles employing \"complicated techniques\" of weaving and dyeing provides direct evidence for silks dating before the Mawangdui-discovery and other silks dating to the Han Dynasty (202 BC-220 AD).\n\nSilk is described in a chapter on mulberry planting by Si Shengzhi of the Western Han (206 BC – 9 AD). There is a surviving calendar for silk production in an Eastern Han (25–220 AD) document. The two other known works on silk from the Han period are lost. The first evidence of the long distance silk trade is the finding of silk in the hair of an Egyptian mummy of the 21st dynasty, c.1070 BC. The silk trade reached as far as the Indian subcontinent, the Middle East, Europe, and North Africa. This trade was so extensive that the major set of trade routes between Europe and Asia came to be known as the Silk Road.\n\nThe Emperors of China strove to keep knowledge of sericulture secret to maintain the Chinese monopoly. Nonetheless sericulture reached Korea with technological aid from China around 200 BC, the ancient Kingdom of Khotan by AD 50, and India by AD 140.\n\nIn the ancient era, silk from China was the most lucrative and sought-after luxury item traded across the Eurasian continent, and many civilizations, such as the ancient Persians, benefited economically from trade.\n\nSilk has a long history in India. It is known as \"Resham\" in eastern and north India, and \"Pattu\" in southern parts of India. Recent archaeological discoveries in Harappa and Chanhu-daro suggest that sericulture, employing wild silk threads from native silkworm species, existed in South Asia during the time of the Indus Valley Civilization (now in Pakistan) dating between 2450 BC and 2000 BC, while \"hard and fast evidence\" for silk production in China dates back to around 2570 BC. Shelagh Vainker, a silk expert at the Ashmolean Museum in Oxford, who sees evidence for silk production in China \"significantly earlier\" than 2500–2000 BC, suggests, \"people of the Indus civilization either harvested silkworm cocoons or traded with people who did, and that they knew a considerable amount about silk.\"\n\nIndia is the second largest producer of silk in the world after China. About 97% of the raw mulberry silk comes from six Indian states, namely, Andhra Pradesh, Karnataka, Jammu and Kashmir, Tamil Nadu, Bihar and West Bengal. North Bangalore, the upcoming site of a $20 million \"Silk City\" Ramanagara and Mysore, contribute to a majority of silk production in Karnataka. \nIn Tamil Nadu, mulberry cultivation is concentrated in the Coimbatore, Erode, Bhagalpuri, Tiruppur, Salem and Dharmapuri districts. Hyderabad, Andhra Pradesh, and Gobichettipalayam, Tamil Nadu, were the first locations to have automated silk reeling units in India.\n\nIndia is also the largest consumer of silk in the world. The tradition of wearing silk sarees for marriages and other auspicious ceremonies is a custom in Assam and southern parts of India. Silk is considered to be a symbol of royalty, and, historically, silk was used primarily by the upper classes. Silk garments and sarees produced in Kanchipuram, Pochampally, Dharmavaram, Mysore, Arani in the south, Banaras in the north, Bhagalpur and Murshidabad in the east are well recognized. In the northeastern state of Assam, three different types of silk are produced, collectively called Assam silk: Muga, Eri and Pat silk. Muga, the golden silk, and Eri are produced by silkworms that are native only to Assam.\n\nSilk is produced year-round in Thailand by two types of silkworms, the cultured Bombycidae and wild Saturniidae. Most production is after the rice harvest in the southern and northeastern parts of the country. Women traditionally weave silk on hand looms and pass the skill on to their daughters, as weaving is considered to be a sign of maturity and eligibility for marriage. Thai silk textiles often use complicated patterns in various colours and styles. Most regions of Thailand have their own typical silks. A single thread filament is too thin to use on its own so women combine many threads to produce a thicker, usable fiber. They do this by hand-reeling the threads onto a wooden spindle to produce a uniform strand of raw silk. The process takes around 40 hours to produce a half kilogram of silk. Many local operations use a reeling machine for this task, but some silk threads are still hand-reeled. The difference is that hand-reeled threads produce three grades of silk: two fine grades that are ideal for lightweight fabrics, and a thick grade for heavier material.\n\nThe silk fabric is soaked in extremely cold water and bleached before dyeing to remove the natural yellow coloring of Thai silk yarn. To do this, skeins of silk thread are immersed in large tubs of hydrogen peroxide. Once washed and dried, the silk is woven on a traditional hand-operated loom.\n\nThe Rajshahi Division of northern Bangladesh is the hub of the country's silk industry. There are three types of silk produced in the region: mulberry, endi and tassar. Bengali silk was a major item of international trade for centuries. It was known as Ganges silk in medieval Europe. Bengal was the leading exporter of silk between the 16th and 19th centuries.\n\nIn the \"Odyssey\", 19.233, when Odysseus, while pretending to be someone else, is questioned by Penelope about her husband's clothing, he says that he wore a shirt \"gleaming like the skin of a dried onion\" (varies with translations, literal translation here) which could refer to the lustrous quality of silk fabric.\nAristotle wrote of \"Coa vestis\", a wild silk textile from Kos.\nSea silk from certain large sea shells was also valued.\nThe Roman Empire knew of and traded in silk, and Chinese silk was the most highly priced luxury good imported by them. During the reign of emperor Tiberius, sumptuary laws were passed that forbade men from wearing silk garments, but these proved ineffectual. The Historia Augusta mentions that the 3rd Century AD emperor Elagabalus was the first Roman to wear garments of pure silk, whereas it had been customary to wear fabrics of silk/cotton or silk/linen blends. Despite the popularity of silk, the secret of silk-making only reached Europe around AD 550, via the Byzantine Empire. Legend has it that monks working for the emperor Justinian I smuggled silkworm eggs to Constantinople in hollow canes from China. All top-quality looms and weavers were located inside the Great Palace complex in Constantinople, and the cloth produced was used in imperial robes or in diplomacy, as gifts to foreign dignitaries. The remainder was sold at very high prices.\n\nIn the Torah, a scarlet cloth item called in Hebrew \"sheni tola'at\" שני תולעת – literally \"crimson of the worm\" – is described as being used in purification ceremonies, such as those following a leprosy outbreak (Leviticus 14), alongside cedar wood and hyssop (za'atar). Eminent scholar and leading medieval translator of Jewish sources and books of the Bible into Arabic, Rabbi Saadia Gaon, translates this phrase explicitly as \"crimson silk\" – חריר קרמז حرير قرمز.\n\nIn Islamic teachings, Muslim men are forbidden to wear silk. Many religious jurists believe the reasoning behind the prohibition lies in avoiding clothing for men that can be considered feminine or extravagant. There are disputes regarding the amount of silk a fabric can consist of (e.g., whether a small decorative silk piece on a cotton caftan is permissible or not) for it to be lawful for men to wear, but the dominant opinion of most Muslim scholars is that the wearing of silk by men is forbidden. Modern attire has raised a number of issues, including, for instance, the permissibility of wearing silk neckties, which are masculine articles of clothing.\n\nDespite injunctions against silk for men, silk has retained its popularity in the Islamic world because of its permissibility for women, and due to the presence of non-Muslim communities. The Muslim Moors brought silk with them to Spain during their conquest of the Iberian Peninsula.\n\nItaly was the most important producer of silk during the Medieval age. The first center to introduce silk production to Italy was the city of Catanzaro during the 11th century in the region of Calabria. The silk of Catanzaro supplied almost all of Europe and was sold in a large market fair in the port of Reggio Calabria, to Spanish, Venetian, Genovese and Dutch merchants. Catanzaro became the lace capital of the world with a large silkworm breeding facility that produced all the laces and linens used in the Vatican. The city was world-famous for its fine fabrication of silks, velvets, damasks and brocades.\n\nAnother notable center was the Italian city-state of Lucca which largely financed itself through silk-production and silk-trading, beginning in the 12th century. Other Italian cities involved in silk production were Genoa, Venice and Florence.\n\nThe Silk Exchange in Valencia from the 15th century—where previously in 1348 also \"perxal\" (percale) was traded as some kind of silk—illustrates the power and wealth of one of the great Mediterranean mercantile cities.\n\nSilk was produced in and exported from the province of Granada, Spain, especially the Alpujarras region, until the Moriscos, whose industry it was, were expelled from Granada in 1571.\n\nSince the 15th century, silk production in France has been centered around the city of Lyon where many mechanic tools for mass production were first introduced in the 17th century.\n\nJames I attempted to establish silk production in England, purchasing and planting 100,000 mulberry trees, some on land adjacent to Hampton Court Palace, but they were of a species unsuited to the silk worms, and the attempt failed. In 1732 John Guardivaglio set up a silk throwing enterprise at Logwood mill in Stockport; in 1744, Burton Mill was erected in Macclesfield; and in 1753 Old Mill was built in Congleton. These three towns remained the centre of the English silk throwing industry until silk throwing was replaced by silk waste spinning. British enterprise also established silk filature in Cyprus in 1928. In England in the mid-20th century, raw silk was produced at Lullingstone Castle in Kent. Silkworms were raised and reeled under the direction of Zoe Lady Hart Dyke, later moving to Ayot St Lawrence in Hertfordshire in 1956.\n\nWild silk taken from the nests of native caterpillars was used by the Aztecs to make containers and as paper. Silkworms were introduced to Oaxaca from Spain in the 1530s and the region profited from silk production until the early 17th century, when the king of Spain banned export to protect Spain's silk industry. Silk production for local consumption has continued until the present day, sometimes spinning wild silk.\n\nKing James I introduced silk-growing to the British colonies in America around 1619, ostensibly to discourage tobacco planting. The Shakers in Kentucky adopted the practice. \n\nIn the 19th century a new attempt at a silk industry began with European-born workers in Paterson, New Jersey, and the city became a silk center in the United States. Manchester, Connecticut emerged as center of the silk industry in America from the late 19th through the mid-20th century. The Cheney Brothers Historic District showcases mills refurbished as apartments and includes nearby museums.\n\nWorld War II interrupted the silk trade from Asia, and silk prices increased dramatically. U.S. industry began to look for substitutes, which led to the use of synthetics such as nylon. Synthetic silks have also been made from lyocell, a type of cellulose fiber, and are often difficult to distinguish from real silk (see spider silk for more on synthetic silks).\n\nIn Terengganu, which is now part of Malaysia, a second generation of silkworm was being imported as early as 1764 for the country's silk textile industry, especially songket. However, since the 1980s, Malaysia is no longer engaged in sericulture but does plant mulberry trees.\n\nIn Vietnamese legend, silk appeared in the sixth dynasty of Hùng Vương.\n\nThe process of silk production is known as sericulture. The entire production process of silk can be divided into several steps which are typically handled by different entities. Extracting raw silk starts by cultivating the silkworms on mulberry leaves. Once the worms start pupating in their cocoons, these are dissolved in boiling water in order for individual long fibres to be extracted and fed into the spinning reel.\n\nTo produce 1 kg of silk, 104 kg of mulberry leaves must be eaten by 3000 silkworms. It takes about 5000 silkworms to make a pure silk kimono. The major silk producers are China (54%) and India (14%). Other statistics:\n\nThe environmental impact of silk production is potentially large when compared with other natural fibers. A life cycle assessment of Indian silk production shows that the production process has a large carbon and water footprint, mainly due to the fact that it is an animal-derived fiber and more inputs such as fertilizer and water are needed per unit of fiber produced.\n\nSilk fibers from the \"Bombyx mori\" silkworm have a triangular cross section with rounded corners, 5–10 μm wide. The fibroin-heavy chain is composed mostly of beta-sheets, due to a 59-mer amino acid repeat sequence with some variations. The flat surfaces of the fibrils reflect light at many angles, giving silk a natural sheen. The cross-section from other silkworms can vary in shape and diameter: crescent-like for \"Anaphe\" and elongated wedge for \"tussah\". Silkworm fibers are naturally extruded from two silkworm glands as a pair of primary filaments (brin), which are stuck together, with sericin proteins that act like glue, to form a bave. Bave diameters for tussah silk can reach 65 μm. See cited reference for cross-sectional SEM photographs.\n\nSilk has a smooth, soft texture that is not slippery, unlike many synthetic fibers.\n\nSilk is one of the strongest natural fibers, but it loses up to 20% of its strength when wet. It has a good moisture regain of 11%. Its elasticity is moderate to poor: if elongated even a small amount, it remains stretched. It can be weakened if exposed to too much sunlight. It may also be attacked by insects, especially if left dirty.\n\nOne example of the durable nature of silk over other fabrics is demonstrated by the recovery in 1840 of silk garments from a wreck of 1782: 'The most durable article found has been silk; for besides pieces of cloaks and lace, a pair of black satin breeches, and a large satin waistcoat with flaps, were got up, of which the silk was perfect, but the lining entirely gone ... from the thread giving way ... No articles of dress of woollen cloth have yet been found.'\n\nSilk is a poor conductor of electricity and thus susceptible to static cling. Silk has a high emissivity for infrared light, making it feel cool to the touch.\n\nUnwashed silk chiffon may shrink up to 8% due to a relaxation of the fiber macrostructure, so silk should either be washed prior to garment construction, or dry cleaned. Dry cleaning may still shrink the chiffon up to 4%. Occasionally, this shrinkage can be reversed by a gentle steaming with a press cloth. There is almost no gradual shrinkage nor shrinkage due to molecular-level deformation.\n\nNatural and synthetic silk is known to manifest piezoelectric properties in proteins, probably due to its molecular structure.\n\nSilkworm silk was used as the standard for the denier, a measurement of linear density in fibers. Silkworm silk therefore has a linear density of approximately 1 den, or 1.1 dtex.\nSilk emitted by the silkworm consists of two main proteins, sericin and fibroin, fibroin being the structural center of the silk, and serecin being the sticky material surrounding it. Fibroin is made up of the amino acids Gly-Ser-Gly-Ala-Gly-Ala and forms beta pleated sheets. Hydrogen bonds form between chains, and side chains form above and below the plane of the hydrogen bond network.\n\nThe high proportion (50%) of glycine allows tight packing. This is because glycine's R group is only a hydrogen and so is not as sterically constrained. The addition of alanine and serine makes the fibres strong and resistant to breaking. This tensile strength is due to the many interceded hydrogen bonds, and when stretched the force is applied to these numerous bonds and they do not break.\n\nSilk is resistant to most mineral acids, except for sulfuric acid, which dissolves it. It is yellowed by perspiration. Chlorine bleach will also destroy silk fabrics.\n\nRSF is produced by chemically dissolving silkworm cocoons, leaving their molecular structure intact. The silk fibers dissolve into tiny thread-like structures known as microfibrils. The resulting solution is extruded through a small opening, causing the microfibrils to reassemble into a single fiber. The resulting material is reportedly twice as stiff as silk.\n\nSilk's absorbency makes it comfortable to wear in warm weather and while active. Its low conductivity keeps warm air close to the skin during cold weather. It is often used for clothing such as shirts, ties, blouses, formal dresses, high fashion clothes, lining, lingerie, pajamas, robes, dress suits, sun dresses and Eastern folk costumes. For practical use, silk is excellent as clothing that protects from many biting insects that would ordinarily pierce clothing, such as mosquitoes and horseflies.\n\nFabrics that are often made from silk include charmeuse, habutai, chiffon, taffeta, crepe de chine, dupioni, noil, tussah, and shantung, among others.\n\nSilk's attractive lustre and drape makes it suitable for many furnishing applications. It is used for upholstery, wall coverings, window treatments (if blended with another fiber), rugs, bedding and wall hangings.\n\nSilk had many industrial and commercial uses, such as in parachutes, bicycle tires, comforter filling and artillery gunpowder bags.\n\nA special manufacturing process removes the outer sericin coating of the silk, which makes it suitable as non-absorbable surgical sutures. This process has also recently led to the introduction of specialist silk underclothing, which has been used for skin conditions including eczema. New uses and manufacturing techniques have been found for silk for making everything from disposable cups to drug delivery systems and holograms.\n\nSilk has been considered as a luxurious textile since 3630 BC. However, it started to serve also as a biomedical material for suture in surgeries decades ago. In the past 30 years, it has been widely studied and used as a biomaterial, which refers to materials used for medical applications in organisms, due to its excellent properties, including remarkable mechanical properties, comparative biocompatibility, tunable degradation rates \"in vitro\" and \"in vivo\", the ease to load cellular growth factors (for example, BMP-2), and the ability to be processed into several other formats such as films, gels, particles, and scaffolds. Silks from \"Bombyx mori\", a kind of cultivated silkworm, are the most widely investigated silks.\n\nSilks derived from \"Bombyx mori\" are generally made of two parts: the silk fibroin fiber which contains a light chain of 25kDa and a heavy chain of 350kDa (or 390kDa) linked by a single disulfide bond and a glue-like protein, sericin, comprising 25 to 30 percentage by weight. Silk fibroin contains hydrophobic beta sheet blocks, interrupted by small hydrophilic groups. And the beta-sheets contribute much to the high mechanical strength of silk fibers, which achieves 740 MPa, tens of times that of poly(lactic acid) and hundreds of times that of collagen. This impressive mechanical strength has made silk fibroin very competitive for applications in biomaterials. Indeed, silk fibers have found their way into tendon tissue engineering, where mechanical properties matter greatly. In addition, mechanical properties of silks from various kinds of silkworms vary widely, which provides more choices for their use in tissue engineering.\n\nMost products fabricated from regenerated silk are weak and brittle, with only ≈1–2% of the mechanical strength of native silk fibers due to the absence of appropriate secondary and hierarchical structure,\nBiocompatibility, i.e., to what level the silk will cause an immune response, is a critical issue for biomaterials. The issue arose during its increasing clinical use. Wax or silicone is usually used as a coating to avoid fraying and potential immune responses when silk fibers serve as suture materials. Although the lack of detailed characterization of silk fibers, such as the extent of the removal of sericin, the surface chemical properties of coating material, and the process used, make it difficult to determine the real immune response of silk fibers in literature, it is generally believed that sericin is the major cause of immune response. Thus, the removal of sericin is an essential step to assure biocompatibility in biomaterial applications of silk. However, further research fails to prove clearly the contribution of sericin to inflammatory responses based on isolated sericin and sericin based biomaterials. In addition, silk fibroin exhibits an inflammatory response similar to that of tissue culture plastic in vitro when assessed with human mesenchymal stem cells (hMSCs) or lower than collagen and PLA when implant rat MSCs with silk fibroin films in vivo. Thus, appropriate degumming and sterilization will assure the biocompatibility of silk fibroin, which is further validated by in vivo experiments on rats and pigs. There are still concerns about the long-term safety of silk-based biomaterials in the human body in contrast to these promising results. Even though silk sutures serve well, they exist and interact within a limited period depending on the recovery of wounds (several weeks), much shorter than that in tissue engineering. Another concern arises from biodegradation because the biocompatibility of silk fibroin does not necessarily assure the biocompatibility of the decomposed products. In fact, different levels of immune responses and diseases have been triggered by the degraded products of silk fibroin.\n\nBiodegradability (also known as biodegradation)--the ability to be disintegrated by biological approaches, including bacteria, fungi, and cells—is another significant property of biomaterials today. Biodegradable materials can minimize the pain of patients from surgeries, especially in tissue engineering, there is no need of surgery in order to remove the scaffold implanted. Wang et al. showed the in vivo degradation of silk via aqueous 3-D scaffolds implanted into Lewis rats. Enzymes are the means used to achieve degradation of silk in vitro. Protease XIV from Streptomyces griseus and α-chymotrypsin from bovine pancreases are the two popular enzymes for silk degradation. In addition, gamma-radiation, as well as cell metabolism, can also regulate the degradation of silk.\n\nCompared with synthetic biomaterials such as polyglycolides and polylactides, silk is obviously advantageous in some aspects in biodegradation. The acidic degraded products of polyglycolides and polylactides will decrease the pH of the ambient environment and thus adversely influence the metabolism of cells, which is not an issue for silk. In addition, silk materials can retain strength over a desired period from weeks to months as needed by mediating the content of beta sheets.\n\nGenetic modification of domesticated silkworms has been used to alter the composition of the silk. As well as possibly facilitating the production of more useful types of silk, this may allow other industrially or therapeutically useful proteins to be made by silkworms.\n\nSilk moths lay eggs on specially prepared paper. The eggs hatch and the caterpillars (silkworms) are fed fresh mulberry leaves. After about 35 days and 4 moltings, the caterpillars are 10,000 times heavier than when hatched and are ready to begin spinning a cocoon. A straw frame is placed over the tray of caterpillars, and each caterpillar begins spinning a cocoon by moving its head in a pattern. Two glands produce liquid silk and force it through openings in the head called spinnerets. Liquid silk is coated in sericin, a water-soluble protective gum, and solidifies on contact with the air. Within 2–3 days, the caterpillar spins about 1 mile of filament and is completely encased in a cocoon. The silk farmers then heat the cocoons to kill them, leaving some to metamorphose into moths to breed the next generation of caterpillars. Harvested cocoons are then soaked in boiling water to soften the sericin holding the silk fibers together in a cocoon shape. The fibers are then unwound to produce a continuous thread. Since a single thread is too fine and fragile for commercial use, anywhere from three to ten strands are spun together to form a single thread of silk.\n\nAs the process of harvesting the silk from the cocoon kills the larvae by boiling them, sericulture has been criticized by animal welfare and rights activists. Mohandas Gandhi was critical of silk production based on the Ahimsa philosophy which led to promotion of cotton and Ahimsa silk, a type of wild silk made from the cocoons of wild and semi-wild silk moths.\n\nSince silk cultivation kills silkworms, possibly painfully, People for the Ethical Treatment of Animals (PETA) urges people not to buy silk items.\n\n\n\n\n"}
{"id": "32525399", "url": "https://en.wikipedia.org/wiki?curid=32525399", "title": "Solar backpack", "text": "Solar backpack\n\nA Solar backpack is a backpack equipped with thin film solar cells and batteries. The solar panels convert sunlight into electricity, which is stored in the batteries and can be used to power portable electronic appliances like mobile phones and mp3 players.\n\nThe solar backpack usually contains a flexible monocrystalline solar panel, battery, charge controller, plugs, cords and light bulbs. It provides users with power up to 120 watt-hours/day, capable of powering electronic equipment rated up to 300 W. It can also be utilized in international aid, disaster relief, emergency power and \nfield research.\n\n\n\n\n"}
{"id": "1708244", "url": "https://en.wikipedia.org/wiki?curid=1708244", "title": "Solidus (chemistry)", "text": "Solidus (chemistry)\n\nIn chemistry, materials science, and physics, the solidus is the locus of temperatures (a curve on a phase diagram) below which a given substance is completely solid (crystallized). The solidus is applied, among other materials, to metal alloys, ceramics, and natural rocks and minerals.\n\nThe solidus quantifies the temperature at which melting of a substance \"begins\", but the substance is not necessarily melted \"completely\", i.e., the solidus is not necessarily a melting point. For this distinction, the solidus may be contrasted to the liquidus. The solidus is always less than or equal to the liquidus, but they need not coincide. If a gap exists between the solidus and liquidus it is called the freezing range, and within that gap, the substance consists of a mixture of solid and liquid phases (like a slurry). Such is the case, for example, with the olivine (forsterite-fayalite) system.\n\nIn eutectic mixtures the solidus and liquidus temperatures coincide at a point known as the eutectic point. At the eutectic point, the solid congruently melts (i.e., melts completely).\n\n"}
{"id": "40169371", "url": "https://en.wikipedia.org/wiki?curid=40169371", "title": "The Third Industrial Revolution", "text": "The Third Industrial Revolution\n\nThe Third Industrial Revolution; How Lateral Power is Transforming Energy, the Economy, and the World is a book by Jeremy Rifkin published in 2011. The premise of the book is that fundamental economic change occurs when new communication technologies converge with new energy regimes, mainly, renewable electricity.\n\nThe Sharing economy is also explored as a crucial element of the Third Industrial Revolution.\n\nThe book has been on the \"New York Times\" Best Seller List and . Rifkin has been interviewed on NPR.\n\nIn 2017 a documentary based on the book was released by Vice Media starring Jeremy Rifkin.\n\n\n"}
{"id": "22025865", "url": "https://en.wikipedia.org/wiki?curid=22025865", "title": "Waterskin", "text": "Waterskin\n\nA waterskin is a receptacle used to hold water. Normally made of a sheep or cow bladder, it retains water naturally and therefore was very useful in desert crossings until the invention of the canteen. It is still used in some developing nations. Though it may have been used over 5000 years ago by tribal peoples, the first pictures of it are from ancient Assyrians, who used the bladders as floats in 3000 B.C. It also was used by large ancient empires such as Rome before the advent of the canteen.\n\nModern waterskins are often made of various plastic or rubber impregnated canvases, or sometimes simply thicker transparent plastics, and are often called water-pouches, water bags, or water bladders. Such modern waterskins offer many features, such as detachable straw-hoses, valves, refill openings of various widths, various closures and handles, styles of covering or cases, and removable cases or carry pouches.\n\n"}
{"id": "23898325", "url": "https://en.wikipedia.org/wiki?curid=23898325", "title": "Yaw bearing", "text": "Yaw bearing\n\nThe yaw bearing is the most crucial and cost intensive component of a yaw system found on modern horizontal axis wind turbines. The yaw bearing must cope with enormous static and dynamic loads and moments during the wind turbine operation, and provide smooth rotation characteristics for the orientation of the nacelle under all weather conditions. It has also to be corrosion and wear resistant and extremely long lasting. It should last for the service life of the wind turbine) while being cost effective.\n\nWindmills of the 18th century began implementing rotatable nacelles to capture wind coming from different directions. The yaw systems of these \"primitive\" windmills were surprisingly similar to the ones on modern wind turbines. The nacelles rotated by means of wind driven yaw drives known as fantails, or by animal power, and were mounted on the windmill towers by means of an axial gliding bearing. \n\nThese gliding bearings consisted of multiple gliding blocks fixed on the windmill tower structure. These blocks maintained sliding contact with a gliding ring on the nacelle. The gliding blocks were wooden cube-like pieces with convex gliding surface covered with animal fat, or even lined with copper (or brass) sheet as a friction reduction means. These wooden blocks were fixed in wooden slots, carved in the wooden bearing substructure, by means of nails or wedges and were carefully leveled to create a flat surface where the nacelle gliding ring could glide. The gliding blocks, despite the lubrication would wear quite often and would have to be exchanged. This operation was relatively simple due to the wedge-based connection between substructure and gliding blocks. The gliding blocks were further locked via movable locking devices which, in a different form, remain as a technical solution in modern gliding yaw bearings. \nThe gliding ring of the windmill nacelle was made from multiple wooden parts and, despite the old construction techniques, was usually quite level, allowing the nacelle to rotate smoothly around the tower axis. \n\nThe \"hybrid yaw bearing system\" combines the solutions old windmills used. This system comprises multiple removable radial gliding pads in combination with an axial roller bearing.\n\nThe main categories of yaw bearings are: \n\nThe roller yaw bearing is a common technical yaw bearing solution followed by many wind turbine manufacturers as it offers low turning friction and smooth rotation of the nacelle. The low turning friction permits the implementation of slightly smaller yaw drives (compared to the gliding bearing solution), but on the other hand requires a yaw braking system. \n\nSome manufacturers use a plurality of smaller yaw drives (usually six) to facilitate easy replacement. Such a configuration with plurality of yaw drives often offers the possibility of active yaw braking using differential torque from the yaw drives. In this case half of the yaw drives apply a small mount of torque for clockwise rotation and the other half apply torque in the opposite direction and then activate the internal magnetic brakes of the electric motor. In this way the pinion-gear rim backlash is eliminated and the nacelle is fixed in place.\n\nThe gliding yaw bearing is a combined axial and radial bearing, which serves as a rotatable connection of the wind turbine nacelle and the tower. Contrary to the old windmill concept, the modern yaw bearings support the nacelle also from the to thus restraining the nacelle from being rotated by the Y-axis due to the moments induced by the upper half of the rotor sweep disk and the X-axis due to the torque of the drive train (i.e. rotor, shaft, generator, etc. ). \n\nPrincipally, the simplest way to accomplish the yaw bearing tasks with gliding elements is with two gliding planes for the axial loads (top and bottom) and a radial gliding surface for the radial loads. Consequently, the gliding yaw bearing comprises three general surfaces covered with multiple gliding pads. These gliding pads come in sliding contact with a steel disk, which is usually equipped with gear teeth to form a gliding-disk/gear-rim. The teeth may be located at the inner or the outer cylindrical face of the disk, while the arrangement of the gliding pads and their exact number and location vary strongly among the existing designs. To assemble the gliding yaw bearings, their cages split in several segments that are assembled together during wind turbine installation or manufacturing. \n\nIn its simplest form, the gliding yaw bearing uses pads (usually made out of polymers) distributed around the three contact surfaces to provide a proper guiding system for the radial and axial movement with relatively low friction coefficient. Such systems are economical and very robust but do not allow individual adjustment of the axial and radial gliding elements. This function importantly minimizes the axial and radial \"play\" of the gliding bearing due to manufacturing tolerances as well as due to wear of the gliding pads during operation. \n\nTo solve this problem, yaw systems incorporate pre-tensioned gliding bearings. These bearings have gliding pads that are pressed via pressure elements against the gliding disk to stabilize the nacelle against undesirable movement. The pressure elements can be simple steel springs, pneumatic, or hydraulic pre-tension elements, etc. The use of pneumatic or hydraulic pre-tension elements allows active control of the yaw bearing pre-tension, which provides yaw brake function.\n\nIn all gliding bearings wear is an issue of concern, as well as lubrication. Conventional gliding yaw bearings incorporate gliding elements manufactured out of polymer plastics such as POM or PA. To reduce friction, wear, and avoid stick-slip effects (often present in such high friction slow moving systems), lubrication is often introduced. This solution generally solves the gliding issues, but introduces more components to the systems and increases the general complication (e.g., difficult maintenance procedures for removal of used lubricant). Some wind turbine manufacturers now use self lubricating gliding elements instead of a central lubrication system. These gliding elements are manufactured from low friction materials or composites (e.t.g Teflon) that allow reliable operation of dry (non-lubricated) gliding yaw systems.\n\nDespite the fact that the gliding yaw bearings and their components are designed and constructed to last the service life of the wind turbine, it should be possible to replace worn out yaw bearing gliding elements or other components of the yaw system. To allow for replace-ability of worn out components, the yaw systems are designed in segments. Usually one or more gliding planes comprise several sub-elements that contain a number of gliding elements (radial or axial or a combination). These sub-elements can be individually removed and repaired, re-fit or replaced. In this way the yaw bearing can be serviced without the need of dis-assembly of the whole gliding yaw bearing (e.g., in case of a roller yaw bearing, dis-assembly of the whole wind turbine). This rep-arability offered by the segmented design of the gliding yaw bearing is one of the most important advantages of this system against the roller yaw bearing solution. \n\nThe only remaining issue is the replacement of the gliding elements of the gliding yaw bearing surface, which is not segmented. This is usually the top axial surface of the gliding bearing, which constantly supports the weight of the whole nacelle-rotor assembly. For the gliding elements of this gliding surface to be replaced, the nacelle-rotor assembly must be lifted by an external crane. An alternative solution to this problem is the use of mechanical or hydraulic jacks able to partially or fully lift the nacelle-rotor assembly while the gliding yaw bearing is still in place. In this way and by providing a small clearance between the gliding elements and the gliding disk, it is possible to exchange the sliding elements without dismantling the gliding yaw bearing.\n\nWhen the wind turbine nacelle is positioned on the tower and the yaw bearing assembly is completed it is necessary to adjust the pressure on the individual gliding pads of the bearing. This is necessary in order to avoid un-even wear of the gliding pads and excessive loading on some sectors of the yaw bearing. In order to achieve that, an adjustment mechanism is necessary, which enables the technicians to adjust the contact pressure of each individual gliding element in a controllable and secure way. The most common solution is the utilization of bottom bearing plates equipped with large opening, which accommodate the adjustable gliding bearing systems. These adjustable gliding bearings comprise a gliding unit (i.e. gliding pad) and an adjustable pressure distribution plate. In between the gliding pad and the pressure plate several spring (pre-tension) elements are located. The vertical position of the pressure plates is usually controlled by an adjustment screw. This adjustment screw presses against the pressure plate while being retained by a counter-pressure support plate, fixed on the bearing assembly with strong bolts. In this way it is possible to apply various levels of contact pressure among the different gliding pads and therefore to ensure that each gliding component of the yaw bearing arrangement is performing as anticipated.\n\n\n"}
{"id": "1435918", "url": "https://en.wikipedia.org/wiki?curid=1435918", "title": "Zero emission", "text": "Zero emission\n\nZero emission refers to an engine, motor, process, or other energy source, that emits no waste products that pollute the environment or disrupt the climate.\n\nVehicles and other mobile machinery used for transport (over land, sea, air, rail) and for other uses (agricultural, mobile power generation, etc.) contribute heavily to climate change and pollution, so zero emission engines are an area of active research. These technologies almost in all cases include an electric motor powered by an energy source compact enough to be installed in the vehicle. These sources include hydrogen fuel cells, batteries, supercapacitors, and flywheel energy storage devices.\n\nIn some cases, such as compressed air engines, the engine may be mechanical rather than electrical. This mechanical engine is then powered by a passive energy source like compressed air, or a combustible non-polluting gas like hydrogen.\n\nThe above engines can be used in all vehicles, from cars to boats to propeller airplanes. For boats, energy sources such as nuclear power and solar panels can also be a viable option, in addition to traditional sails and turbosails.\n\nA concept like vegetable oil economy produces emissions.\n\n\n"}
