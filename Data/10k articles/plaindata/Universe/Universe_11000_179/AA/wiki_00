{"id": "55249121", "url": "https://en.wikipedia.org/wiki?curid=55249121", "title": "1927 St. Louis–East St. Louis tornado", "text": "1927 St. Louis–East St. Louis tornado\n\nThe 1927 St. Louis–East St. Louis tornado was a powerful and devastating tornado that struck St. Louis, Missouri on Thursday, September 29, 1927, at 1:00pm. The tornado is estimated to be at least a F3 or F4 on the Fujita scale. The 2nd deadliest tornado to occur in the St. Louis metropolitan area, it caused at minimum 72–79 deaths and injured more than 550 people all within a seven-to-twelve-mile long, 100–600 yard wide path. At one time it was the 2nd costliest tornado in US history. More than 200 city blocks were destroyed. It is one of four tornadoes (1871, 1896, 1927, 1959) that have torn through downtown St. Louis. St. Louis University High School was hit hard. The student chapel’s roof collapsed, the gym’s (now main offices) roof was damaged, an entire classroom caved in on a class, and other classrooms were damaged. All the windows were smashed. Luckily, no one was killed or majorly injured. The tornado caused $150,000 dollars in damage to the school.\n\nThe tornado was a part of a larger outbreak of at least 11 significant tornadoes, that included two F3 tornadoes that killed at least 3 more people in Illinois and Arkansas. The outbreak affected a rather huge area of the Midwestern and Southern United States; the tornadoes impacted at least 6 states: Oklahoma, Missouri, Arkansas, Iowa, Illinois, and Indiana.\n\n\n\n\n\n"}
{"id": "57311198", "url": "https://en.wikipedia.org/wiki?curid=57311198", "title": "2018 Indian dust storms", "text": "2018 Indian dust storms\n\nFrom 2 to 3 May 2018, high-velocity dust storms swept across parts of North India and more than 125 people died and over 200 injured. \n\nIn Uttar Pradesh, 43 died in the city of Agra and at least 30 died elsewhere in the state. In neighboring Rajasthan, at least 35 people died and over 200 were injured after winds downed more than 8,000 electricity poles and uprooted hundreds of trees. Storms are not uncommon in the region; however, because these storms occurred at night and with greater wind speeds than average, the death toll was higher than usual.\n\nDust storms are a feature of India's seasonal weather patterns. The storms typically occur in the summer months, when the weather has been dry to allow dust to be picked up by passing winds. The death toll in such storms rarely exceeds 12; a previous storm hit India on 11 April 2018, killing 19 people\n\nThe dust storm occurred at the start of India's monsoon season. In the days prior, region meteorologists had forecast thunderstorms and high winds to occur over that week. Contributing to the storm was a period of abnormally high temperatures for the region, which increased the intensity of the weather system.\n\nThe dust storm first started late on 2 May 2018, predominantly hitting the states of Uttar Pradesh and Rajasthan. At least 73 people were killed in Uttar Pradesh, with 43 of those in the city of Agra; 21 people have been reported killed in Kheragarh, a town around 50 km south-west of the city. At least 35 people were killed in Rajasthan, with the Alwar district being the worst hit; the Bharatpur and Dholpur districts were also affected. Four people died in the state of Uttarakhand, and Delhi was also affected. More than 200 people were injured by the storm.\n\nOfficials stated that the storm was more devastating than prior dust storms as the stronger weather system carried more debris which caused more damage to homes and buildings, and because it struck at night, most were asleep and were unable to take precautions, leaving many killed or injured by falling structures. Most damage and fatalities were associated with high winds, rather than dust. In Rajasthan, electricity supplies were interrupted by 200–300 downed pylons, and schools were closed in the Alwar district.\n\nBecause conditions were still prime for more severe weather, the Uttar Pradesh government continued to plan to alert its citizens to weather conditions for the following 48 hours.\n\nThe Government of Uttar Pradesh has announced compensation for the relatives of those killed amounting to .\n"}
{"id": "529093", "url": "https://en.wikipedia.org/wiki?curid=529093", "title": "Act of God", "text": "Act of God\n\nIn legal usage throughout the English-speaking world, an act of God is a natural hazard outside human control, such as an earthquake or tsunami, for which no person can be held responsible. An act of God may amount to an exception to liability in contracts (as under the Hague–Visby Rules); or it may be an \"insured peril\" in an insurance policy.\n\nBy contrast, other extraordinary man-made or political events are deemed \"force majeure\".\n\nIn the law of contracts, an act of God may be interpreted as an implied defense under the rule of impossibility or impracticability. If so, the promise is discharged because of unforeseen occurrences, which were unavoidable and would result in insurmountable delay, expense, or other material breach.\n\nUnder the English common law, contractual obligations were deemed sacrosanct, so failure to honour a contract could lead to an order for specific performance or internment in a debtor's prison. In 1863, this harsh rule was softened by the case of \"Taylor v Caldwell\" which introduced the doctrine of frustration of contract, which provided that \"where a contract becomes impossible to perform and neither party is at fault, both parties may be excused their obligations\". In this case, a music hall was burned down by act of God before a contract of hire could be fulfilled, and the court deemed the contract frustrated.\n\nIn other contracts, such as indemnification, an act of God may be no excuse, and in fact may be the central risk assumed by the promisor—\"e.g.,\" flood insurance or crop insurance—the only variables being the timing and extent of the damage. In many cases, failure by way of ignoring obvious risks due to \"natural phenomena\" will not be sufficient to excuse performance of the obligation, even if the events are relatively rare: \"e.g.,\" the year 2000 problem in computers. Under the Uniform Commercial Code, 2-615, failure to deliver goods sold may be excused by an \"act of God\" if the absence of such act was a \"basic assumption\" of the contract, and the act has made the delivery \"commercially impracticable\".\n\nRecently, human activities have been claimed to be the root causes of some events until now considered natural disasters. In particular:\n\nSuch events are possibly threatening the legal status of acts of God and may establish liabilities where none existed until now.\n\nAn act of God is an unforeseeable natural phenomenon. Explained by Lord Hobhouse in \"Transco plc v Stockport Metropolitan Borough Council\" as describing an event:\n\nAn act of God is described in \"Tennant v. Earl of Glasgow\" (1864 2 M (HL) 22) as: \"Circumstances which no human foresight can provide against, and of which human prudence is not bound to recognize the possibility, and which when they do occur, therefore, are calamities that do not involve the obligation of paying for the consequences that may result from them.\"\n\nIn the law of torts, an \"act of God\" may be asserted as a type of intervening cause, the lack of which would have avoided the cause or diminished the result of liability (e.g., but for the earthquake, the old, poorly constructed building would be standing). However, foreseeable results of unforeseeable causes may still raise liability. For example, a bolt of lightning strikes a ship carrying volatile compressed gas, resulting in the expected explosion. Liability may be found if the carrier did not use reasonable care to protect against sparks—regardless of their origins. Similarly, strict liability could defeat a defense for an \"act of God\" where the defendant has created the conditions under which any accident would result in harm. For example, a long-haul truck driver takes a shortcut on a back road and the load is lost when the road is destroyed in an unforeseen flood. Other cases find that a common carrier is not liable for the unforeseeable forces of nature. See e.g. Memphis & Charlestown RR Co. v. Reeves, 77 U.S. 176 (1870).\n\nA particularly interesting example is that of \"rainmaker\" Charles Hatfield, who was hired in 1915 by the city of San Diego to fill the Morena reservoir to capacity with rainwater for $10,000. The region was soon flooded by heavy rains, nearly bursting the reservoir's dam, killing nearly 20 people, destroying 110 bridges (leaving 2), knocking out telephone and telegraph lines, and causing an estimated $3.5 million in damage in total. When the city refused to pay him (he had forgotten to sign the contract), he sued the city. The floods were ruled an act of God, excluding him from liability but also from payment.\n\nThe phrase \"act of God\" is sometimes used to attribute an event to divine intervention. Often it is used in conjunction with a natural disaster or tragic event. A miracle, by contrast, is often considered a fortuitous event attributed to divine intervention. Some consider it separate from \"acts of nature\" and being related to fate or destiny.\n\nChristian theologians differ on their views and interpretations of scripture. R.C. Sproul implies that God causes a disaster when he speaks of Divine Providence: \"In a universe governed by God, there are no chance events.\" Others indicate that God may allow a tragedy to occur.\n\nOthers accept unfortunate events as part of life and reference Matthew 5:45 (KJV): \"for he maketh his sun to rise on the evil and on the good, and sendeth rain on the just and on the unjust\".\n\n"}
{"id": "6801101", "url": "https://en.wikipedia.org/wiki?curid=6801101", "title": "Algal nutrient solution", "text": "Algal nutrient solution\n\nAlgal nutrient solutions are made up of a mixture of chemical salts and water. Sometimes referred to as \"Growth Media\", nutrient solutions (along with carbon dioxide and light), provide the materials needed for algae to grow. Nutrient solutions, as opposed to fertilizers, are designed specifically for use in aquatic environments and their composition is much more precise.\n\n\n"}
{"id": "2405330", "url": "https://en.wikipedia.org/wiki?curid=2405330", "title": "Anisodesmic crystal", "text": "Anisodesmic crystal\n\nAn anisodesmic crystal (sometimes anisodemic crystal) is a crystal containing bonds with differing electrostatic valencies. All other crystals are known as isodesmic crystals (or isodemic) and examples include diamond and halite. These terms are of particular importance when discussing the structural chemistry of minerals.\n\nThe electrostatic valency is a measure of the strength of bonds, being the valence charge divided by the coordination number. Linus Pauling's second rule of ionic bonding, the \"Electrostatic Valence Principle\" states, \"An ionic structure will be stable to the extent that the sum of the strengths of the electrostatic bonds that reach an anion equals the charge on that anion.\"\n\nWhen there is more than one type of bonding in a crystal, strongly anisotropic physical properties can result such as those of graphite. Many other substances have anisotropic physical properties. Such properties are directly related to the atomic structures of the substances. Conversely, halite is an example of an isotropic crystal with equal hardness in all directions.\n"}
{"id": "12017661", "url": "https://en.wikipedia.org/wiki?curid=12017661", "title": "Antibiosis", "text": "Antibiosis\n\nAntibiosis is a biological interaction between two or more organisms that is detrimental to at least one of them; it can also be an antagonistic association between an organism and the metabolic substances produced by another. Examples of antibiosis include the relationship between antibiotics and bacteria or animals and disease-causing pathogens. The study of antibiosis and its role in antibiotics has led to the expansion of knowledge in the field of microbiology. Molecular processes such cell wall synthesis and recycling, for example, have become better understood through the study of how antibiotics affect beta-lactam development through the antibiosis relationship and interaction of the particular drugs with the bacteria subjected to the compound.\n\nAntibiosis is typically studied in host plant populations and extends to the insects which feed upon them.\n\n\"Antibiosis resistance affects the biology of the insect so pest abundance and subsequent damage is reduced compared to that which would have occurred if the insect was on a susceptible crop variety. Antibiosis resistance often results in increased mortality or reduced longevity and reproduction of the insect.\"\n\n\n"}
{"id": "367577", "url": "https://en.wikipedia.org/wiki?curid=367577", "title": "Antipodal point", "text": "Antipodal point\n\nIn mathematics, the antipodal point of a point on the surface of a sphere is the point which is diametrically opposite to it — so situated that a line drawn from the one to the other passes through the center of the sphere and forms a true diameter.\n\nThis term applies to opposite points on a circle or any n-sphere.\n\nAn antipodal point is sometimes called an antipode, a back-formation from the Greek loan word \"antipodes\", which originally meant \"opposite the feet\".\n\nIn mathematics, the concept of \"antipodal points\" is generalized to spheres of any dimension: two points on the sphere are antipodal if they are opposite \"through the centre\"; for example, taking the centre as origin, they are points with related vectors v and −v. On a circle, such points are also called diametrically opposite. In other words, each line through the centre intersects the sphere in two points, one for each ray out from the centre, and these two points are antipodal.\n\nThe Borsuk–Ulam theorem is a result from algebraic topology dealing with such pairs of points. It says that any continuous function from \"S\" to R maps some pair of antipodal points in \"S\" to the same point in R. Here, \"S\" denotes the \"n\"-dimensional sphere in (\"n\" + 1)-dimensional space (so the \"ordinary\" sphere is \"S\" and a circle is \"S\").\n\nThe antipodal map \"A\" : \"S\" → \"S\", defined by \"A\"(\"x\") = −\"x\", sends every point on the sphere to its antipodal point. It is homotopic to the identity map if \"n\" is odd, and its degree is (−1).\n\nIf one wants to consider antipodal points as identified, one passes to projective space (see also projective Hilbert space, for this idea as applied in quantum mechanics).\n\nAn antipodal pair of a convex polygon is a pair of 2 points admitting 2 infinite parallel lines being tangent to both points included in the antipodal without crossing any other line of the convex polygon.\n\n"}
{"id": "333420", "url": "https://en.wikipedia.org/wiki?curid=333420", "title": "Archimedes' principle", "text": "Archimedes' principle\n\nArchimedes' principle states that the upward buoyant force that is exerted on a body immersed in a fluid, whether fully or partially submerged, is equal to the weight of the fluid that the body displaces and acts in the upward direction at the center of mass of the displaced fluid. Archimedes' principle is a law of physics fundamental to fluid mechanics. It was formulated by Archimedes of Syracuse.\n\nIn \"On Floating Bodies\", Archimedes suggested that (c. 250 BC):\nPractically, Archimedes' principle allows the buoyancy of an object partially or fully immersed in a fluid to be calculated. The downward force on the object is simply its weight. The upward, or buoyant, force on the object is that stated by Archimedes' principle, above. Thus, the net force on the object is the difference between the magnitudes of the buoyant force and its weight. If this net force is positive, the object rises; if negative, the object sinks; and if zero, the object is neutrally buoyant - that is, it remains in place without either rising or sinking. In simple words, Archimedes' principle states that, when a body is partially or completely immersed in a fluid, it experiences an apparent loss in weight that is equal to the weight of the fluid displaced by the immersed part of the body.\n\nConsider a cuboid immersed in a fluid, with one (hence two: top and bottom) of its sides orthogonal to the direction of gravity (assumed constant across the cube's stretch). The fluid will exert a normal force on each face, but only the normal forces on top and bottom will contribute to buoyancy. The pressure difference between the bottom and the top face is directly proportional to the height (difference in depth of submersion). Multiplying the pressure difference by the area of a face gives a net force on the cuboid – the buoyancy, equaling in size the weight of the fluid displaced by the cuboid. By summing up sufficiently many arbitrarily small cuboids this reasoning may be extended to irregular shapes, and so, whatever the shape of the submerged body, the buoyant force is equal to the weight of the displaced fluid. \n\nThe weight of the displaced fluid is directly proportional to the volume of the displaced fluid (if the surrounding fluid is of uniform density). The weight of the object in the fluid is reduced, because of the force acting on it, which is called upthrust. In simple terms, the principle states that the buoyant force (F) on an object is equal to the weight of the fluid displaced by the object, or the density (ρ) of the fluid multiplied by the submerged volume (V) times the gravity (g) or F = ρ x g x V. Thus, among completely submerged objects with equal masses, objects with greater volume have greater buoyancy.\n\nSuppose a rock's weight is measured as 10 newtons when suspended by a string in a vacuum with gravity acting on it. Suppose that, when the rock is lowered into water, it displaces water of weight 3 newtons. The force it then exerts on the string from which it hangs would be 10 newtons minus the 3 newtons of buoyant force: 10 − 3 = 7 newtons. Buoyancy reduces the apparent weight of objects that have sunk completely to the sea floor. It is generally easier to lift an object up through the water than it is to pull it out of the water.\n\nFor a fully submerged object, Archimedes' principle can be reformulated as follows:\n\nthen inserted into the quotient of weights, which has been expanded by the mutual volume\n\nyields the formula below. The density of the immersed object relative to the density of the fluid can easily be calculated without measuring any volume is\n\nExample: If you drop wood into water, buoyancy will keep it afloat.\n\nExample: A helium balloon in a moving car. When increasing speed or driving in a curve, the air moves in the opposite direction to the car's acceleration. However, due to buoyancy, the balloon is pushed \"out of the way\" by the air, and will actually drift in the same direction as the car's acceleration.\n\nWhen an object is immersed in a liquid, the liquid exerts an upward force, which is known as the buoyant force, that is proportional to the weight of the displaced liquid. The sum force acting on the object, then, is equal to the difference between the weight of the object ('down' force) and the weight of displaced liquid ('up' force). Equilibrium, or neutral buoyancy, is achieved when these two weights (and thus forces) are equal.\n\nArchimedes' principle does not consider the surface tension (capillarity) acting on the body. Moreover, Archimedes' principle has been found to break down in complex fluids.\n\nArchimedes' principle shows the buoyant force and displacement of fluid. However, the concept of Archimedes' principle can be applied when considering why objects float. Proposition 5 of Archimedes' treatise \"On Floating Bodies\" states that:\n\nIn other words, for an object floating on a liquid surface (like a boat) or floating submerged in a fluid (like a submarine in water or dirigible in air) the weight of the displaced liquid equals the weight of the object. Thus, only in the special case of floating does the buoyant force acting on an object equal the objects weight. Consider a 1-ton block of solid iron. As iron is nearly eight times as dense as water, it displaces only 1/8 ton of water when submerged, which is not enough to keep it afloat. Suppose the same iron block is reshaped into a bowl. It still weighs 1 ton, but when it is put in water, it displaces a greater volume of water than when it was a block. The deeper the iron bowl is immersed, the more water it displaces, and the greater the buoyant force acting on it. When the buoyant force equals 1 ton, it will sink no farther.\n\nWhen any boat displaces a weight of water equal to its own weight, it floats. This is often called the \"principle of flotation\": A floating object displaces a weight of fluid equal to its own weight. Every ship, submarine, and dirigible must be designed to displace a weight of fluid at least equal to its own weight. A 10,000-ton ship's hull must be built wide enough, long enough and deep enough to displace 10,000 tons of water and still have some hull above the water to prevent it from sinking. It needs extra hull to fight waves that would otherwise fill it and, by increasing its mass, cause it to submerge. The same is true for vessels in air: a dirigible that weighs 100 tons needs to displace 100 tons of air. If it displaces more, it rises; if it displaces less, it falls. If the dirigible displaces exactly its weight, it hovers at a constant altitude.\n\nWhile they are related to it, the principle of flotation and the concept that a submerged object displaces a volume of fluid equal to its own volume are \"not\" Archimedes' principle. Archimedes' principle, as stated above, equates the \"buoyant force\" to the weight of the fluid displaced.\n\nOne common point of confusion regarding Archimedes' principle is the meaning of displaced volume. Common demonstrations involve measuring the rise in water level when an object floats on the surface in order to calculate the displaced water. This measurement approach fails with a buoyant submerged object because the rise in the water level is directly related to the volume of the object and not the mass (except if the effective density of the object equals exactly the fluid density).\n\n(note that this idea is not Archimedes' principle).\n"}
{"id": "28602612", "url": "https://en.wikipedia.org/wiki?curid=28602612", "title": "Asnæs Power Station", "text": "Asnæs Power Station\n\nThe Asnæs Power Station () is a coal-fired power plant operated by DONG Energy in Kalundborg, Denmark. It consists of three active units, which deliver 147 MW (Unit 2), 270 MW (Unit 4) and 640 MW (Unit 5). Unit 3 went in service in 1959 and uses a flue gas stack, while Unit 5, which went into service in 1981, uses a flue gas stack, the third tallest in Denmark. Its two 60m high coal cranes were dismantled in 2016.\n\n\n"}
{"id": "20936052", "url": "https://en.wikipedia.org/wiki?curid=20936052", "title": "Aurantinidin", "text": "Aurantinidin\n\nAurantinidin is a water-soluble, red plant dye. It is a member of the class of compounds known as anthocyanidins and is a hydroxy derivative of pelargonidin. Aurantinidin has been reported to occur in \"Impatiens aurantiaca\" (Balsaminaceae), and also in cultivars from genus \"Alstroemeria\".\n"}
{"id": "31382667", "url": "https://en.wikipedia.org/wiki?curid=31382667", "title": "Bioabsorbable metallic glass", "text": "Bioabsorbable metallic glass\n\nBioresorbable (or bioabsorbable) metallic glass is a type of amorphous metal, which is based on the \"Mg-Zn-Ca ternary system\". Containing only elements which already exist inside the human body, namely Mg, Zn and Ca, these amorphous alloys are a special type of biodegradable metal.\n\nThe first reported metallic glass was an alloy (AuSi) produced at Caltech by W. Klement (Jr.), Willens and Duwez in 1960. This and other early glass-forming alloys had to be cooled extremely rapidly (in the order of one mega-kelvin per second, 10 K/s) to avoid crystallization. An important consequence of this was that metallic glasses could only be produced in a limited number of forms (typically ribbons, foils, or wires) in which one or more dimensions were small so that heat could be extracted quickly enough to achieve the necessary cooling rates. As a result, metallic glass specimens (with a few exceptions) were limited to thicknesses of less than one hundred micrometers.\n\nMg-Zn-Ca based metallic glasses are a relatively new group of amorphous metals, possessing commercial and technical advantages over early compositions. Gu and co-workers produced the first Mg-Zn-Ca BMG in 2005, reporting high glass forming ability, high strength and most importantly exceptional plasticity. This lanthanide-free, Mg-based glass attracted immediate interest due to its low density and cost, and particularly because of its uncharacteristically high ductility. This property was unexpected for such compositions, as the constituent elements are found to be of relatively low Poisson ratio, and hence contribute little to the inherent plasticity of the glass. This unlikely asset was seized upon by Li in 2008, who made use of the Poisson ratio principle and increased Mg content at the expense of Zn to further enhance plasticity. Further improvements were achieved by incremental addition of Ca to the Mg72Zn28 binary composition, producing numerous ternary alloys along the 350 °C isotherm of the Mg-Zn-Ca system.\n\nTernary Ca-Mg-Zn bulk metallic glasses were also discovered in 2005. Similar to the Mg-Zn-Ca, these two amorphous alloys are both bioresorbable metallic glasses and are based on the same \"Mg-Zn-Ca ternary system\". The elements are displayed in order of decreasing atomic concentration. Hence, the distinction between these two metallic glasses lies in their most dominant element, namely Ca and Mg. These Ca-based bulk glassy alloys had compositions of CaMgZn, CaMgZn, and CaMgZn, where X = 0, 5 and 10; Y = 0, 5, 7.5, 10, and 15; and Z = 0, 5, 7.5, 10, and 15. Critical casting thicknesses of up to 10 mm were achieved.\n\nUnlike traditional steel or titanium, this material dissolves in organisms at a rate of roughly 1 millimeter per month and is replaced with bone tissue. This speed can be adjusted by varying the content of zinc.\n\nAmorphous CaZnMg alloy exhibits extremely poor corrosion resistance. Wang \"et al.\" reported that the said amorphous alloy completely disintegrated after no more than 3 hours exposure in biocorrosion environment. In static distilled water at room temperature, Dahlman \"et al.\" also reported destructive corrosion reactions of the same material, decomposing into a multiphase powder.\n\nCa-BMGs with higher Zn contents as reported by Cao \"et al.\" showed an elastic modulus in the range of 35–46 GPa, and a hardness of 0.7–1.4 GPa.\n\nMetallic glasses based on the Mg-Zn-Ca ternary alloy system only consist of the elements which already exist inside the human body. As such, it is being explored as a potential bioresorbable biomaterial for use in orthopaedic applications.\n\n\n"}
{"id": "3816147", "url": "https://en.wikipedia.org/wiki?curid=3816147", "title": "Carbogen", "text": "Carbogen\n\nCarbogen, also called Meduna's Mixture after its inventor Ladislas Meduna, is a mixture of carbon dioxide and oxygen gas. Meduna's original formula was 5% CO and 95% oxygen, but the term carbogen can refer to any mixture of these two gases, from 1.5% to 50% CO.\n\nWhen carbogen is inhaled, the increased level of carbon dioxide causes a perception, both psychological and physiological, of suffocation because the brain interprets an increase in blood carbon dioxide as a decrease in oxygen level, which would generally be the case under natural circumstances. Inhalation of carbogen causes the body to react as if it were not receiving sufficient oxygen: breathing quickens and deepens, heart rate increases, and cells release alkaline buffering agents to remove carbonic acid from the bloodstream.\n\nCarbogen was once used in psychology and psychedelic psychotherapy to determine whether a patient would react to an altered state of consciousness or to a sensation of loss of control. Individuals who reacted especially negatively to carbogen were generally not administered other psychotherapeutic drugs for fear of similar reactions. Meduna administered carbogen to his patients to induce abreaction, which, with proper preparation and administration, he found could help clients become free of their neuroses. Carbogen users are said to have discovered unconscious contents of their minds, with the experience clearing away repressed material and freeing the subject for a smoother, more profound psychedelic experience.\n\nOne subject reported: \n\"After the second breath came an onrush of color, first a predominant sheet of beautiful rosy-red, following which came successive sheets of brilliant color and design, some geometric, some fanciful and graceful …. Then the colors separated; my soul drawing apart from the physical being, was drawn upward seemingly to leave the earth and to go upward where it reached a greater Spirit with Whom there was a communion, producing a remarkable, new relaxation and deep security.\"\n\nCarbogen is rarely used in therapy anymore, largely due to the decline in psychedelic psychotherapy.\n\nA carbogen mixture of 95% oxygen and 5% carbon dioxide can be used as part of the early treatment of central retinal artery occlusion.\n\nCarbogen is used in biology research to study \"in vivo\" oxygen and carbon dioxide flows, as well as to oxygenate the aCSF solution and stabilize the pH to about 7.4 in research on acute brain slices.\n\nIts use in combination with nicotinamide is also being investigated in conjunction with radiation therapy in the treatment strategy of certain cancers. Because increased tumor oxygenation improves the cell-killing effects of radiation, it is thought that the inhalation of these agents during radiation therapy could increase its effectiveness.\n"}
{"id": "29997263", "url": "https://en.wikipedia.org/wiki?curid=29997263", "title": "Cerotic acid", "text": "Cerotic acid\n\nCerotic acid, or \"hexacosanoic acid\", is a 26-carbon long-chain saturated fatty acid with the chemical formula CH(CH)COOH. It is most commonly found in beeswax and carnauba wax, and is a white crystalline solid.\n\nCerotic acid is also a type of very long chain fatty acid that is often associated with the disease adrenoleukodystrophy, which involves the excessive saturation of unmetabolized fatty acid chains, including cerotic acid, in the peroxisome. [In the chem box it is shown folded only because of lack of space. In fact, it is a straight-chain, saturated fatty acid.]\n\n"}
{"id": "53306867", "url": "https://en.wikipedia.org/wiki?curid=53306867", "title": "Chassis dynamometer", "text": "Chassis dynamometer\n\nA chassis dynamometer, sometimes called a rolling road, is a device used for vehicle testing and development. It uses a roller assembly to simulate a road in a controlled environment, usually inside a building.\nThere are many types of chassis dynamometer according to the target application - for example, emissions measurement, miles accumulation chassis dynamometer (MACD), Noise-Vibration-Harshness (NVH or \"Acoustic\") Application, Electromagnetic Compatibility (EMC) testing, end of line (EOL) tests, performance measurement and tuning. Another basic division is by type of vehicle - motorcycles, cars, trucks, tractors or the size of the roller - mostly 25\", 48\", 72\", but also any other. Modern dynamometers used for development are mostly one roller to the wheel construction and the vehicle wheel is placed the top of the roller. Older constructional solutions are two roller per wheel and vehicle is place between these rollers - this design solution is cheaper and simpler, however, due to the requirements for accuracy and strict limits is no longer used for the development of new vehicles, but only as a test dynamometer at the end of the line or to measure the performance of the engine without dismantling, or performance tuning in \"garage\" companies.\n\n\nDirectly measured variables are only force on the torque transducer (i.e. loadcell) and revolutions measured on the role encoder dynamometer. All other variables are calculated based on known design (i.e. roller radius and loadcell mounting).\n\nDue to friction and mechanical losses in various parts of the power train, the measured power at the wheels by about 15 to 20 percent lower than the power measured directly at the output of engine crankshaft (measuring device with this purpose is called engine testbed).\n\nVehicle do not behave the same way on chassis dynamometer as on the road. For example, aerodynamic shape of the vehicle does not matter. Sum of all forces on the vehicle on a real road are simulated through tires on chassis dynamometer.\nIncreasing air drag with the speed on the road manifests as increasing braking force of the vehicle wheels. The aim is to make the vehicle on the dynamometer accelerate and decelerate the same way as on a real road.\nFirst you need to know the parameters of the \"behavior\" of the vehicle on a real road.\nIn order to get \"road parameters\", vehicle must be driving on ideal flat road with no wind from any direction, gear set to neutral and time needed to slow down without braking is measured in certain intervals i.e. 100–90 km/h, 90–80 km/h, 80–70 km/h 70–60 km/h etc. Slowing down from higher speed takes shorter time mainly due to air resistance.\nThose parameters are later set in dynamometer workstation, together with vehicle inertia. Vehicle is restrained and so called vehicle adaptation has to be performed.\nDuring vehicle adaptation dynamometer automatically slowing down from set speed, changing its own \"dyno parameters\" and trying to get same deceleration in given intervals as on real road. Those parameters are then valid for this vehicle type. Changing of set simulated inertia it is possible to simulate vehicle ability to accelerate if fully loaded, with setting gradient it is possible to simulate force if vehicle going downhill etc. Chassis dynamometers for climatic chamber does exists, where it is possible to change temperature in give range i.e. -40 to +50 °C or altitude chamber where it is possible to check fuel consumption with different temperatures or pressure and to simulate driving on mountain roads.\n\n\n"}
{"id": "13273382", "url": "https://en.wikipedia.org/wiki?curid=13273382", "title": "Clipper Windpower", "text": "Clipper Windpower\n\nClipper Windpower is a wind turbine manufacturing company founded in 2001 by James G.P. Dehlsen. It designed one of the largest wind turbines in the United States, manufactured in Cedar Rapids, Iowa. It was working collaboratively with the National Renewable Energy Laboratory.\n\nAffected by a global recession, in December 2010 Clipper Windpower was acquired by United Technologies Corporation. It was sold in 2012 to Platinum Equity. The company has reduced its manufacturing and is supporting its turbines.\n\nIn September 2007 Clipper Windpower, based in Carpinteria, California, received an Outstanding Research and Development Partnership Award from the U.S. Department of Energy (DOE) for the design and development of its 2.5 MW Liberty Wind Turbine, one of the largest wind turbines manufactured in the U.S. at the time. The turbine was developed under a partnership with DOE and its National Renewable Energy Laboratory (NREL). It was manufactured in Cedar Rapids, Iowa.\n\nCommercial sales started in June 2006 for the new Liberty turbine and (mostly contingent) orders for more than 5,600 MW (2,240 units) for deliveries in 2007 through 2011 were booked. Clipper Windpower had an agreement with BP Energy to supply the 2.5 MW turbines for the proposed 5,050 MW Titan Wind Project in South Dakota. Among the projects completed with Clipper wind turbines is Steel Winds in Lackawanna, New York, south of Buffalo, where an urban wind farm was developed on part of a brownfield.\n\nEmerging companies such as Clipper were among those adversely affected by a global recession in 2008 and 2009.\n\nOn December 10, 2009, Connecticut-based United Technologies Corporation announced that it would acquire a 49.5% stake in Clipper Windpower by purchasing 84.3 million new shares and 21.8 million shares from current shareholders for £126.5 million. Clipper said this equity purchase \"will significantly strengthen its balance sheet and enable it to enhance its operations and pursue its strategic initiatives\". On 18 October 2010, UTC agreed with Clipper to acquire the rest of the company, a transaction that was completed in December 2010 for a total cost of approximately $385 million.\n\nOn March 15, 2012, United Technologies announced their intent to sell Clipper Windpower. CFO Greg Hayes described their 2010 acquisition as a \"mistake\". United Technologies' sudden about-face is attributed to the fact that the windpower manufacturing company does not fit into their new aerospace-mostly business model.\n\nIn April 2008 Clipper Windpower announced plans to develop a large wind energy turbine in Blyth, Northumberland in the United Kingdom. For use offshore, each turbine was designed to be rated at around 7.5 MW and would be roughly double the size of the largest turbines used in commercial offshore wind farms at that time (Siemens 3.6 MW turbine). According to the British Wind Energy Association, electrical power for a city the size of Newcastle upon Tyne could be supplied by as few as 20 of the turbines.\n\nClipper Windpower Marine had started construction of a offshore wind turbine blade manufacturing facility in Neptune Estate, Tyne in 2010. The factory would have been used to develop and build blades for the Britannia project, a 10 MW offshore wind turbine prototype under development by Clipper and scheduled for deployment in late 2012. However, Clipper stopped development of the 10 MW in August 2011 after parent United Technologies deemed the financial crisis as too severe, and has paid £1.6 million of aid back to The Crown Estate.\n\nThe company discontinued manufacturing new turbines in Cedar Rapids in 2012. Its manufacturing facility in Cedar Rapids is devoted to replacement parts for its Liberty wind turbine, as it had problems with a Brazilian supplier.\n\nA number of research labs are working to modify the Clipper wind turbine design. Navid Goudarzi et al. have worked on a novel multiple-generator drivetrain that proposes employing different rated generators.\n\n"}
{"id": "406959", "url": "https://en.wikipedia.org/wiki?curid=406959", "title": "Coil spring", "text": "Coil spring\n\nA coil spring, also known as a \"helical spring\", is a mechanical device which is typically used to store energy and subsequently release it, to absorb shock, or to maintain a force between contacting surfaces. They are made of an elastic material formed into the shape of a helix which returns to its natural length when unloaded.\n\nUnder tension or compression, the material (wire) of a coil spring undergoes torsion. The spring characteristics therefore depend on the shear modulus, not Young's Modulus.\n\nA coil spring may also be used as a torsion spring: in this case the spring as a whole is subjected to torsion about its helical axis. The material of the spring is thereby subjected to a bending moment, either reducing or increasing the helical radius. In this mode, it is the Young's Modulus of the material that determines the spring characteristics.\n\nMetal coil springs are made by winding a wire around a shaped former - a cylinder is used to form cylindrical coil springs.\n\nCoil springs for vehicles are typically made of hardened steel. A machine called an auto-coiler takes spring wire that has been heated so it can easily be shaped. It is then fed onto a lathe that has a metal rod with the desired coil spring size. The machine takes the wire and guides it onto the spinning rod as well as pushing it across the rod to form multiple coils. The spring is then ejected from the machine and an operator will put it in oil to cool off. The spring is then tempered to lose the brittleness from being cooled. The coil size and strength can be controlled by the lathe rod size and material used. Different alloys are used to get certain characteristic’s out of the spring, such as stiffness, dampening and strength \n\nSpring rate is the measurement of how much a coil spring can hold until it compresses 1 inch. The spring rate is normally specified by the manufacture. If a Spring has a rate of 100 then the spring would compress 1 inch with 100lbs of load. \n\nTypes of coil spring are:\n\n\nCoil springs have many applications; notable ones include:\n\nTension and extension coil springs of a given material, wire diameter and coil diameter exert the same force when fully loaded; increased number of coils merely (linearly) increases free length and compressed/extended length.\n\n\n"}
{"id": "3990133", "url": "https://en.wikipedia.org/wiki?curid=3990133", "title": "Condensate pump", "text": "Condensate pump\n\nA condensate pump is a specific type of pump used to pump the condensate (water) produced in an HVAC (heating or cooling), refrigeration, condensing boiler furnace, or steam system. \n\nCondensate pumps may be used to pump the condensate produced from latent water vapor in any of the following gas mixtures:\n\n\nCondensate recovery systems help reduce three tangible costs of producing steam: \n\n\nCondensate pumps as used in hydronic systems are usually electrically powered centrifugal pumps. As used in homes and individual heat exchangers, they are often small and rated at a fraction of a horsepower, but in commercial applications they range in size up to many horsepower and the electric motor is usually separated from the pump body by some form of mechanical coupling. Large industrial pumps may also serve as the feedwater pump for returning the condensate under pressure to a boiler.\n\nCondensate pumps usually run intermittently and have a tank in which condensate can accumulate. Eventually, the accumulating liquid raises a float switch energizing the pump. The pump then runs until the level of liquid in the tank is substantially lowered. Some pumps contain a two-stage switch. As liquid rises to the trigger point of the first stage, the pump is activated. If the liquid continues to rise (perhaps because the pump has failed or its discharge is blocked), the second stage will be triggered. This stage may switch off the HVAC equipment (preventing the production of further condensate), trigger an alarm, or both.\n\nSome systems may include two pumps to service the tank. In this case, the two pumps often alternate operation, and a two-stage switch serves to energize the on-duty pump at the first stage and then energize the remaining pump at the second stage. This second stage action is in addition to any triggering of other system changes as noted for a single pump installation. In this way pump runtime is shared between the two, and a backup pump is provided in case one pump fails to function as designed.\n\nSmall pumps have tanks that range from 2 to 4 liters (0.5 to 1 gallon) and are usually supported using the flanges on their tanks or simply placed upon the floor. A plastic impeller in a molded volute at the bottom of the pump provides the pumping action; this impeller is connected to the motor via a metal shaft that extends downwards from the motor mounted above the tank's top. Large pumps are usually pad-mounted drawing liquid from a tank (sump) below the floor. The smallest pumps may have no tank at all and are simply placed within a container such as the drip pan of a dehumidifier appliance.\n\nIn industrial steam systems the condensate pump is used to collect and return condensate from remote areas of the plant. The steam produced in the boiler can heat equipment and processes a considerable distance away. Once steam is used it turns to hot water or condensate. This pump and possibly many more around the plant returns this hot water back to a make-up tank closer to the boiler, where it can be reclaimed, chemically treated, and reused, in the boiler, consequently it can sometimes be referred to as a condensate return pump.\n\nIn a steam power plant, particularly shipboard ones, the condensate pump is normally located adjacent to the main condenser hotwell often directly below it. This pump sends the water to a make-up tank closer to the steam generator or boiler. If the tank is also designed to remove dissolved oxygen from the condensate, it is known as a deaereating feed tank (DFT). The output of the DFT supplies the feed booster pump which, in turn, supplies the feedwater pump which returns the feedwater to the boiler so the cycle can start over. Two pumps in succession are used to provide sufficient net positive suction head to prevent cavitation and the subsequent damage associated with it.\n\nThis pump is usually associated with a much larger tank, float switch, and an electric motor than the example above. Some systems are so remote that steam power is used to return the condensate where electricity is impractical to provide.\n\nThe output of small condensate pumps is usually routed to a sewer, plumbing drain, or the outside world via PVCl plastic tubing (condensate drain line).\n\nIf the outlet of the line is at a higher level than the tank of the pump, a check valve is often fitted at the outlet of the pump so that liquid cannot flow backwards into the pump's tank. If the outlet is below the tank level, siphonage usually naturally clears the output line of all liquid when the pump is deenergized. In cold regions of the world, it is important that condensate lines that are exhausted outside be carefully designed so that no water can remain in the line to freeze up; this would block the line from further operation.\n\nCondensate is not pure water. If it is being condensed from an air stream, it may have dust, microbes, or other contaminants in it. If it is condensed from steam, it may have traces of the various boiler water treatment chemicals. And if it is condensed from furnace exhaust gases, it may be acidic, containing sulfuric acid or nitric acid as a result of sulfur and nitrogen dioxides in the exhaust gas stream. Steam and exhaust condensate is usually hot. These various factors may combine (along with local regulations) to require careful handling or even chemical treatment of the condensate, and condensate pumps used for these services must be appropriately designed.\n\n\n"}
{"id": "477292", "url": "https://en.wikipedia.org/wiki?curid=477292", "title": "Copper(II) sulfate", "text": "Copper(II) sulfate\n\nCopper(II) sulfate, also known as cupric sulfate, or copper sulphate, is the inorganic compound with the chemical formula CuSO(HO), where x can range from 0 to 5. The pentahydrate (x = 5) is the most common form. Older names for this compound include blue vitriol, bluestone, vitriol of copper, and Roman vitriol.\n\nThe pentahydrate (CuSO·5HO), the most commonly encountered salt, is bright blue. It exothermically dissolves in water to give the aquo complex [Cu(HO)], which has octahedral molecular geometry. The structure of the solid pentahydrate reveals a polymeric structure wherein copper is again octahedral but bound to four water ligands. The Cu(II)(HO) centers are interconnected by sulfate anions to form chains. Anhydrous copper sulfate is a white powder.\n\nCopper sulfate is produced industrially by treating copper metal with hot concentrated sulfuric acid or its oxides with dilute sulfuric acid. For laboratory use, copper sulfate is usually purchased. Copper sulfate can also be produced by slowly leaching low grade copper ore in air; bacteria may be used to hasten the process.\n\nCommercial copper sulfate is usually about 98% pure copper sulfate, and may contain traces of water. Anhydrous Copper sulfate is 39.81 percent copper and 60.19 percent sulfate by mass, and in its blue, hydrous form, it is 25.47% copper, 38.47% sulfate (12.82% sulfur) and 36.06% water by mass. Four types of crystal size are provided based on its usage: large crystals (10–40 mm), small crystals (2–10 mm), snow crystals (less than 2 mm), and windswept powder (less than 0.15 mm).\n\nCopper(II) sulfate pentahydrate decomposes before melting. It loses two water molecules upon heating at , followed by two more at and the final water molecule at . Dehydration proceeds by decomposition of the tetraaquacopper(2+) moiety, two opposing aqua groups are lost to give a diaquacopper(2+) moiety. The second dehydration step occurs when the final two aqua groups are lost. Complete dehydration occurs when the final unbound water molecule is lost. At , copper(II) sulfate decomposes into copper(II) oxide (CuO) and sulfur trioxide (SO).\n\nCopper sulfate reacts with concentrated hydrochloric acid to give tetrachlorocuprate(II):\n\nCopper sulfate is commonly included in children's chemistry sets. It is often used to grow crystals in schools and in copper plating experiments, despite its toxicity. Copper sulfate is often used to demonstrate an exothermic reaction, in which steel wool or magnesium ribbon is placed in an aqueous solution of CuSO. It is used to demonstrate the principle of mineral hydration. The pentahydrate form, which is blue, is heated, turning the copper sulfate into the anhydrous form which is white, while the water that was present in the pentahydrate form evaporates. When water is then added to the anhydrous compound, it turns back into the pentahydrate form, regaining its blue color, and is known as blue vitriol. Copper(II) sulfate pentahydrate can easily be produced by crystallization from solution as copper(II) sulfate is quite hygroscopic.\n\nIn an illustration of a \"single metal replacement reaction\", iron is submerged in a solution of copper sulfate. Iron reacts producing iron(II) sulfate and copper precipitates.\n\nIn high school and general chemistry education, copper sulfate is used as electrolyte for galvanic cells, usually as a cathode solution. For example, in a zinc/copper cell, copper ion in copper sulfate solution absorbs electron from zinc and forms metallic copper.\n\nCopper sulfate pentahydrate is used as a fungicide. However, some fungi are capable of adapting to elevated levels of copper ions.\nBordeaux mixture,\na suspension of copper(II) sulfate (CuSO) and calcium hydroxide (Ca(OH)), is used to control fungus on grapes, melons, and other berries. It is produced by mixing a water solution of copper sulfate and a suspension of slaked lime, a suspension of copper(II) hydroxide Cu(OH) and calcium sulfate, which is used to control fungus on grapes, melons, and other berries.\n\n\"Cheshunt compound\", a commercial mixture of copper sulfate and ammonium carbonate (discontinued), is used in horticulture to prevent damping off in seedlings. As a non-agricultural herbicide, is it used to control invasive aquatic plants and the roots of plants situated near water pipes. It is used in swimming pools as an algicide. \nA dilute solution of copper sulfate is used to treat aquarium fishes for parasitic infections, and is also used to remove snails from aquariums. Copper ions are highly toxic to fish, however. Most species of algae can be controlled with very low concentrations of copper sulfate.\n\nSeveral chemical tests utilize copper sulfate. It is used in Fehling's solution and Benedict's solution to test for reducing sugars, which reduce the soluble blue copper(II) sulfate to insoluble red copper(I) oxide. Copper(II) sulfate is also used in the Biuret reagent to test for proteins.\n\nCopper sulfate is used to test blood for anemia. The blood is tested by dropping it into a solution of copper sulfate of known specific gravity – blood which contains sufficient hemoglobin sinks rapidly due to its density, whereas blood which does not sink or sinks slowly has insufficient amount of hemoglobin.\n\nIn a flame test, its copper ions emit a deep green light, a much deeper green than the flame test for barium.\n\nCopper sulfate is employed at a limited level in organic synthesis. The anhydrous salt is used as a dehydrating agent for forming and manipulating acetal groups. The hydrated salt can be intimately mingled with potassium permanganate to give an oxidant for the conversion of primary alcohols.\n\nCopper(II) sulfate has attracted many niche applications over the centuries. In industry copper sulfate has multiple applications. In printing it is an additive to book binding pastes and glues to protect paper from insect bites; in building it is used as an additive to concrete to provide water resistance and disinfectant qualities. Copper sulfate can be used as a coloring ingredient in artworks, especially glasses and potteries. Copper sulfate is also used in firework manufacture as a blue coloring agent, but it is not safe to mix copper sulfate with chlorates when mixing firework powders.\n\nIt corrects copper deficiencies in the soil and animals and stimulates farm animals' growth.\n\nIn decoration, copper sulfate adds color to cement, metals and ceramic. Some batteries, electrodes and wire contain copper sulfate. It is used in printing ink and hair dye.\n\nCopper sulfate was once used to kill bromeliads, which serve as mosquito breeding sites. Copper sulfate is used as a molluscicide to treat bilharzia in tropical countries.\n\nCopper sulfate is used as an anti-fungal agent to protect seeds against fungus and to protect horse hooves from infection. It inhibits growth of bacteria such as \"Escherichia coli.\"\n\nIn 2008, the artist Roger Hiorns filled an abandoned waterproofed council flat in London with 75,000 liters of copper sulfate solution. The solution was left to crystallize for several weeks before the flat was drained, leaving crystal-covered walls, floors and ceilings. The work is titled \"Seizure\". Since 2011, it has been on exhibition at the Yorkshire Sculpture Park.\n\nCopper sulfate is used to etch zinc or copper plates for intaglio printmaking.\nIt is also used to etch designs into copper for jewelry, such as for Champlevé.\n\nCopper sulfate can be used as a mordant in vegetable dyeing. It often highlights the green tints of the specific dyes.\n\nCopper sulfate was used in the past as an emetic. It is now considered too toxic for this use. It is still listed as an antidote in the World Health Organization's Anatomical Therapeutic Chemical Classification System.\n\nAnhydrous copper(II) sulfate is a white solid. It can be produced by dehydration of the normally available pentahydrate copper sulfate. In nature, it found as the very rare mineral known as chalcocyanite. The pentahydrate also occurs in nature as chalcanthite. Two other copper sulfates comprise the remaining of these rare minerals: bonattite (trihydrate) and boothite (heptahydrate).\nCopper sulfate is an irritant. The usual routes by which humans can receive toxic exposure to copper sulfate are through eye or skin contact, as well as by inhaling powders and dusts. Skin contact may result in itching or eczema. Eye contact with copper sulfate can cause conjunctivitis, inflammation of the eyelid lining, ulceration, and clouding of the cornea.\n\nUpon oral exposure, copper sulfate is moderately toxic. According to studies, the lowest dose of copper sulfate that had a toxic effect on humans is 11 mg/kg. Because of its irritating effect on the gastrointestinal tract, vomiting is automatically initiated in case of the ingestion of copper sulfate. However, if copper sulfate is retained in the stomach, the symptoms can be severe. After 1–12 grams of copper sulfate are swallowed, such poisoning signs may occur as a metallic taste in the mouth, burning pain in the chest, nausea, diarrhea, vomiting, headache, discontinued urination, which leads to yellowing of the skin. In cases of copper sulfate poisoning, injury to the brain, stomach, liver, or kidneys may also occur.\n\nCopper sulfate is highly soluble in water and therefore is easy to distribute in the environment. Copper in the soil may be from industry, motor vehicle, and architectural materials. According to studies, copper sulfate exists mainly in the surface soil and tends to bind organic matter. The more acidic the soil is, the less binding occurs.\n\n\n"}
{"id": "145700", "url": "https://en.wikipedia.org/wiki?curid=145700", "title": "Crust (geology)", "text": "Crust (geology)\n\nIn geology, the crust is the outermost solid shell of a rocky planet, dwarf planet, or natural satellite. It is usually distinguished from the underlying mantle by its chemical makeup; however, in the case of icy satellites, it may be distinguished based on its phase (solid crust vs. liquid mantle).\n\nThe crusts of Earth, Moon, Mercury, Venus, Mars, Io, and other planetary bodies formed via igneous processes, and were later modified by erosion, impact cratering, volcanism, and sedimentation.\n\nMost terrestrial planets have fairly uniform crusts. Earth, however, has two distinct types: continental crust and oceanic crust. These two types have different chemical compositions and physical properties, and were formed by different geological processes.\n\nPlanetary geologists divide crust into three categories, based on how and when they formed.\n\nThis is a planet's \"original\" crust. It forms from solidification of a magma ocean. Toward the end of planetary accretion, the terrestrial planets likely had surfaces that were magma oceans. As these cooled, they solidified into crust. This crust was likely destroyed by large impacts and re-formed many times as the Era of Heavy Bombardment drew to a close. \n\nThe nature of primary crust is still debated: its chemical, mineralogic, and physical properties are unknown, as are the igneous mechanisms that formed them. This is because it is difficult to study: none of Earth's primary crust has survived to today. Earth's high rates of erosion and crustal recycling from plate tectonics has destroyed all rocks older than about 4 billion years, including whatever primary crust Earth once had. \n\nHowever, geologists can glean information about primary crust by studying it on other terrestrial planets. Mercury's highlands might represent primary crust, though this is debated. The anorthosite highlands of the Moon are primary crust, formed as plagioclase crystallized out of the Moon's initial magma ocean and floated to the top; however, it is unlikely that Earth followed a similar pattern, as the Moon was a water-less system and Earth had water. The Martian meteorite ALH84001 might represent primary crust of Mars; however, again, this is debated. Like Earth, Venus lacks primary crust, as the entire planet has been repeatedly resurfaced and modified.\n\nSecondary crust is formed by partial melting of silicate materials in the mantle, and so is usually basaltic in composition.\n\nThis is the most common type of crust in the Solar System. Most of the surfaces of Mercury, Venus, Earth, and Mars comprise secondary crust, as do the lunar maria. On Earth, we see secondary crust forming primarily at mid-ocean spreading centers, where the adiabatic rise of mantle causes partial melting. \n\nTertiary crust is more chemically-modified than either primary or secondary. It can form in several ways:\nThe only known example of tertiary crust is the continental crust of the Earth. It is unknown whether other terrestrial planets can be said to have tertiary crust, though the evidence so far suggests that they do not. This is likely because plate tectonics is needed to create tertiary crust, and Earth is the only planet in our Solar System with plate tectonics. \n\nThe crust is a thin shell on the outside of the Earth, accounting for less than 1% of Earth's volume. It is the top component of lithosphere: a division of Earth's layers that includes the crust and the upper part of the mantle. The lithosphere is broken into tectonic plates that move, allowing heat to escape from the interior of the Earth into space. \n\nThe crust lies on top of the mantle, a configuration that is stable because the upper mantle is made of peridotite and so is significantly denser than the crust. The boundary between the crust and mantle is conventionally placed at the Mohorovičić discontinuity, a boundary defined by a contrast in seismic velocity. \nThe crust of the Earth is of two distinctive types: \nBecause both continental and oceanic crust are less dense than the mantle below, both types of crust \"float\" on the mantle. This is isostasy, and it's also one of the reasons continental crust is higher than oceanic: continental is less dense and so \"floats\" higher. As a result, water pools in above the oceanic crust, forming the oceans. \n\nThe temperature of the crust increases with depth, reaching values typically in the range from about to at the boundary with the underlying mantle. The temperature increases by as much as for every kilometer locally in the upper part of the crust, but the geothermal gradient is smaller in deeper crust.\n\nThe continental crust has an average composition similar to that of andesite. The most abundant minerals in Earth's continental crust are feldspars, which make up about 41% of the crust by weight, followed by quartz at 12%, and pyroxenes at 11%. Continental crust is enriched in incompatible elements compared to the basaltic ocean crust and much enriched compared to the underlying mantle. Although the continental crust comprises only about 0.6 weight percent of the silicate on Earth, it contains 20% to 70% of the incompatible elements.\n\nAll the other constituents except water occur only in very small quantities and total less than 1%. Estimates of average density for the upper crust range between 2.69 and 2.74 g/cm and for lower crust between 3.0 and 3.25 g/cm.\n\nEarth formed approximately 4.6 billion years ago from a disk of dust and gas orbiting the newly formed Sun. It formed via accretion, where planetesimals and other smaller rocky bodies collided and stuck, gradually growing into a planet. This process generated an enormous amount of heat, which caused early Earth to melt completely. As planetary accretion slowed, Earth began to cool, forming its first crust, called a primary or primordial crust. This crust was likely repeatedly destroyed by large impacts, then reformed from the magma ocean left by the impact. None of Earth's primary crust has survived to today; all was destroyed by erosion, impacts, and plate tectonics over the past several billion years. \n\nSince then, Earth has been forming secondary and tertiary crust. Secondary crust forms at mid-ocean spreading centers, where partial-melting of the underlying mantle yields basaltic magmas and new ocean crust forms. This \"ridge push\" is one of the driving forces of plate tectonics, and it is constantly creating new ocean crust. That means that old crust must be destroyed somewhere, so, opposite a spreading center, there is usually a subduction zone: a trench where an ocean plate is being shoved back into the mantle. This constant process of creating new ocean crust and destroying old ocean crust means that the oldest ocean crust on Earth today is only about 200 million years old. \n\nIn contrast, the bulk of the continental crust is much older. The oldest continental crustal rocks on Earth have ages in the range from about 3.7 to 4.28  billion years and have been found in the Narryer Gneiss Terrane in Western Australia, in the Acasta Gneiss in the Northwest Territories on the Canadian Shield, and on other cratonic regions such as those on the Fennoscandian Shield. Some zircon with age as great as 4.3 billion years has been found in the Narryer Gneiss Terrane.\n\nThe average age of the current Earth's continental crust has been estimated to be about 2.0 billion years. Most crustal rocks formed before 2.5 billion years ago are located in cratons. Such old continental crust and the underlying mantle asthenosphere are less dense than elsewhere in Earth and so are not readily destroyed by subduction. Formation of new continental crust is linked to periods of intense orogeny; these periods coincide with the formation of the supercontinents such as Rodinia, Pangaea and Gondwana. The crust forms in part by aggregation of island arcs including granite and metamorphic fold belts, and it is preserved in part by depletion of the underlying mantle to form buoyant lithospheric mantle.\n\nA theoretical protoplanet named \"Theia\" is thought to have collided with the forming Earth, and part of the material ejected into space by the collision accreted to form the Moon. As the Moon formed, the outer part of it is thought to have been molten, a “lunar magma ocean.” Plagioclase feldspar crystallized in large amounts from this magma ocean and floated toward the surface. The cumulate rocks form much of the crust. The upper part of the crust probably averages about 88% plagioclase (near the lower limit of 90% defined for anorthosite): the lower part of the crust may contain a higher percentage of ferromagnesian minerals such as the pyroxenes and olivine, but even that lower part probably averages about 78% plagioclase. The underlying mantle is denser and olivine-rich.\n\nThe thickness of the crust ranges between about 20 and 120 km. Crust on the far side of the Moon averages about 12 km thicker than that on the near side. Estimates of average thickness fall in the range from about 50 to 60 km. Most of this plagioclase-rich crust formed shortly after formation of the moon, between about 4.5 and 4.3 billion years ago. Perhaps 10% or less of the crust consists of igneous rock added after the formation of the initial plagioclase-rich material. The best-characterized and most voluminous of these later additions are the mare basalts formed between about 3.9 and 3.2 billion years ago. Minor volcanism continued after 3.2 billion years, perhaps as recently as 1 billion years ago. There is no evidence of plate tectonics.\n\nStudy of the Moon has established that a crust can form on a rocky planetary body significantly smaller than Earth. Although the radius of the Moon is only about a quarter that of Earth, the lunar crust has a significantly greater average thickness. This thick crust formed almost immediately after formation of the Moon. Magmatism continued after the period of intense meteorite impacts ended about 3.9 billion years ago, but igneous rocks younger than 3.9 billion years make up only a minor part of the crust.\n\n\n\n"}
{"id": "9540", "url": "https://en.wikipedia.org/wiki?curid=9540", "title": "Electricity generation", "text": "Electricity generation\n\nElectricity generation is the process of generating electric power from sources of primary energy. For electric utilities in the electric power industry, it is the first stage in the delivery of electricity to end users, the other stages being transmission, distribution, energy storage and recovery, using pumped-storage method.\n\nA characteristic of electricity is that it is not a primary energy freely present in nature in remarkable amounts and it must be produced. Production is carried out in power stations (also called \"power plants\"). Electricity is most often generated at a power plant by electromechanical generators, primarily driven by heat engines fueled by combustion or nuclear fission but also by other means such as the kinetic energy of flowing water and wind. Other energy sources include solar photovoltaics and geothermal power.\n\nThe fundamental principles of electricity generation were discovered in the 1820s and early 1830s by British scientist Michael Faraday. His method, still used today, is for electricity to be generated by the movement of a loop of wire, or disc of copper between the poles of a magnet. Central power stations became economically practical with the development of alternating current (AC) power transmission, using power transformers to transmit power at high voltage and with low loss.\n\nIn 1870, commercial electricity production started with the coupling of the dynamo to the hydraulic turbine. In 1870, the mechanical production of electric power began the Second Industrial Revolution and created inventions using the energy, whose major contributors were Thomas Alva Edison and Nikola Tesla. Previously the only way to produce electricity was by chemical reactions or using battery cells, and the only practical use of electricity was for the telegraph.\n\nElectricity generation at central power stations started in 1882, when a steam engine driving a dynamo at Pearl Street Station produced a DC current that powered public lighting on Pearl Street, New York. The new technology was quickly adopted by many cities around the world, which adapted their gas fueled street lights to electric power, and soon after electric lights would be used in public buildings, in businesses, and to power public transport, such as trams and trains.\n\nThe first power plants used water power or coal; and today a variety of energy sources are used, such as coal, nuclear, natural gas, hydroelectric, wind generators, and oil, as well as solar energy, tidal power, and geothermal sources. The use of power-lines and power-poles have been significantly important in the distribution of electricity.\n\nSeveral fundamental methods exist to convert other forms of energy into electrical energy. The triboelectric effect, piezoelectric effect, and even direct capture of the energy of nuclear decay Betavoltaics are used in niche applications, as is direct conversion of heat to electric power in the thermoelectric effect. Utility-scale generation is done by rotating electric generators, or by photovoltaic systems. A very small proportion of electric power distributed by utilities is provided by batteries.\n\nElectric generators transform kinetic energy into electricity. This is the most used form for generating electricity and is based on Faraday's law. It can be seen experimentally by rotating a magnet within closed loops of a conducting material (e.g. copper wire). Almost all commercial electrical generation is done using electromagnetic induction, in which mechanical energy forces a generator to rotate:\n\nElectrochemistry is the direct transformation of chemical energy into electricity, as in a battery. Electrochemical electricity generation is important in portable and mobile applications. Currently, most electrochemical power comes from batteries. Primary cells, such as the common zinc–carbon batteries, act as power sources directly, but secondary cells (i.e. rechargeable batteries) are used for storage systems rather than primary generation systems. Open electrochemical systems, known as fuel cells, can be used to extract power either from natural fuels or from synthesized fuels. Osmotic power is a possibility at places where salt and fresh water merges.\n\nThe photovoltaic effect is the transformation of light into electrical energy, as in solar cells. Photovoltaic panels convert sunlight directly to electricity. Although sunlight is free and abundant, solar power electricity is still usually more expensive to produce than large-scale mechanically generated power due to the cost of the panels. Low-efficiency silicon solar cells have been decreasing in cost and multijunction cells with close to 30% conversion efficiency are now commercially available. Over 40% efficiency has been demonstrated in experimental systems. Until recently, photovoltaics were most commonly used in remote sites where there is no access to a commercial power grid, or as a supplemental electricity source for individual homes and businesses. Recent advances in manufacturing efficiency and photovoltaic technology, combined with subsidies driven by environmental concerns, have dramatically accelerated the deployment of solar panels. Installed capacity is growing by 40% per year led by increases in Germany, Japan, and the United States.\n\nThe selection of electricity production modes and their economic viability varies in accordance with demand and region. The economics vary considerably around the world, resulting in widespread selling prices, e.g. the price in Venezuela is 3 cents per kWh while in Denmark it is 40 cents per kWh. Hydroelectric plants, nuclear power plants, thermal power plants and renewable sources have their own pros and cons, and selection is based upon the local power requirement and the fluctuations in demand. All power grids have varying loads on them but the daily minimum is the base load, supplied by plants which run continuously. Nuclear, coal, oil and gas plants can supply base load.\n\nThermal energy is economical in areas of high industrial density, as the high demand cannot be met by renewable sources. The effect of localized pollution is also minimized as industries are usually located away from residential areas. These plants can also withstand variation in load and consumption by adding more units or temporarily decreasing the production of some units.\nNuclear power plants can produce a huge amount of power from a single unit. However, recent disasters in Japan have raised concerns over the safety of nuclear power, and the capital cost of nuclear plants is very high.\nHydroelectric power plants are located in areas where the potential energy from falling water can be harnessed for moving turbines and the generation of power. It is not an economically viable source of production where the load varies too much during the annual production cycle and the ability to store the flow of water is limited.\n\nDue to advancements in technology, and with mass production, renewable sources other than hydroelectricity (solar power, wind energy, tidal power, etc.) experienced decreases in cost of production, and the energy is now in many cases cost-comparative with fossil fuels. Many governments around the world provide subsidies to offset the higher cost of any new power production, and to make the installation of renewable energy systems economically feasible. However, their use is frequently limited by their intermittent nature.\nIf natural gas prices are below $3 per million British thermal units, generating electricity from natural gas is cheaper than generating power by burning coal.\n\nAlmost all commercial electrical power on Earth is generated with a turbine, driven by wind, water, steam or burning gas. The turbine drives a generator, thus transforming its mechanical energy into electrical energy by electromagnetic induction. There are many different methods of developing mechanical energy, including heat engines, hydro, wind and tidal power. Most electric generation is driven by heat engines. The combustion of fossil fuels supplies most of the energy to these engines, with a significant fraction from nuclear fission and some from renewable sources. The modern steam turbine (invented by Sir Charles Parsons in 1884) currently generates about 80% of the electric power in the world using a variety of heat sources. Turbine types include:\nAlthough turbines are most common in commercial power generation, smaller generators can be powered by gasoline or diesel engines. These may used for back up generation or isolated villages.\n\nElectric generators were known in simple forms from the discovery of the magnetic induction of electric current in the 1830s. In general, some form of prime mover such as an engine or the turbines described above, drives a rotating magnetic field past stationary coils of wire thereby turning mechanical energy into electricity. A very large 2000 MW(2,682,000 horsepower) unit designed by Siemens was built for unit 3 at the Olkiluoto Nuclear Power Plant. The only commercial scale electricity production that does not employ a generator is solar PV.\n\nThe production of electricity in 2013 was 23,322TWh. Sources of electricity were coal and peat 41%, natural gas 22%, hydroelectric 16%, nuclear power 11%, oil 4%, biomass and waste 2% and other sources 4%. Other sources include wind, geothermal, solar photovoltaic, and solar thermal.\n\nTotal energy consumed at all power plants for the generation of electricity was 4,398,768 ktoe (kilo ton of oil equivalent) which was 36% of the total for primary energy sources (TPES) of 2008. <br>\nElectricity output (gross) was 1,735,579 ktoe (20,185 TWh), efficiency was 39%, and the balance of 61% was generated heat. A small part (145,141 ktoe, which was 3% of the input total) of the heat was utilized at co-generation heat and power plants. The in-house consumption of electricity and power transmission losses were 289,681 ktoe.\nThe amount supplied to the final consumer was 1,445,285 ktoe (16,430 TWh) which was 33% of the total energy consumed at power plants and heat and power co-generation (CHP) plants.\n\nThe United States has long been the largest producer and consumer of electricity, with a global share in 2005 of at least 25%, followed by China, Japan, Russia, and India. As of Jan-2010, total electricity generation for the two largest generators was as follows: USA: 3,992 billion kWh (3,992 TWh); China: 3,715 billion kWh (3,715 TWh).\n\nData source of values (electric power generated) is IEA/OECD.\nListed countries are top 20 by population or top 20 by GDP (PPP) and Saudi Arabia based on CIA World Factbook 2009.\n\nSolar PV* is Photovoltaics\nBio other* = 198TWh (Biomass) + 69TWh (Waste) + 4TWh (other)\n\nVariations between countries generating electrical power affect concerns about the environment. In France only 10% of electricity is generated from fossil fuels, the US is higher at 70% and China is at 80%. The cleanliness of electricity depends on its source. Most scientists agree that emissions of pollutants and greenhouse gases from fossil fuel-based electricity generation account for a significant portion of world greenhouse gas emissions; in the United States, electricity generation accounts for nearly 40% of emissions, the largest of any source. Transportation emissions are close behind, contributing about one-third of U.S. production of carbon dioxide.\nIn the United States, fossil fuel combustion for electric power generation is responsible for 65% of all emissions of sulfur dioxide, the main component of acid rain. Electricity generation is the fourth highest combined source of NOx, carbon monoxide, and particulate matter in the US.\nIn July 2011, the UK parliament tabled a motion that \"levels of (carbon) emissions from nuclear power were approximately three times lower per kilowatt hour than those of solar, four times lower than clean coal and 36 times lower than conventional coal\".\n"}
{"id": "52423232", "url": "https://en.wikipedia.org/wiki?curid=52423232", "title": "Flow in partially full conduits", "text": "Flow in partially full conduits\n\nIn fluid mechanics, flows in closed conduits are usually encountered in places such as drains and sewers where the liquid flows continuously in the closed channel and the channel is filled only up to a certain depth. Typical examples of such flows are flow in circular and Δ shaped channels. Closed conduit flow differs from open channel flow only in the fact that in closed channel flow there is a closing top width while open channels have one side exposed to its immediate surroundings. Closed channel flows are generally governed by the principles of channel flow as the liquid flowing possesses free surface inside the conduit. However, the convergence of the boundary to the top imparts some special characteristics to the flow like closed channel flows have a finite depth at which maximum discharge occurs. For computational purposes, flow is taken as uniform flow .Manning's Equation, Continuity Equation (Q=AV) and channel's cross-section geometrical relations are used for the mathematical calculation of such closed channel flows.\n\nConsider a closed circular conduit of diameter D, partly full with liquid flowing inside it. Let 2θ be the angle, in radians, subtended by the free surface at the centre of the conduit as shown in figure (a).\n\nThe area of the cross-section (A) of the liquid flowing through the conduit is calculated as :\n\nformula_1 (Equation 1)\n\nNow, the wetted perimeter (P) is given by :\n\nformula_2\n\nTherefore, the hydraulic radius (R) is calculated using cross-sectional area (A) and wetted perimeter (P) using the relation:\n\nformula_3 (Equation 2)\n\nThe rate of discharge may be calculated from Manning’s equation :\n\nformula_4.\n\nformula_5 (Equation 3)\n\nwhere the constant formula_6\n\nNow putting formula_7 in the above equation yields us the rate of discharge for conduit flowing full (Q)\n\nformula_8 (Equation 4)\n\nIn Dimensionless form, the rate of discharge Q is usually expressed in a dimensionless form as :\n\nformula_9 (Equation 5)\n\nSimilarly for velocity (V) we can write :\n\nformula_10 (Equation 6)\n\nThe depth of flow (H) is expressed in a dimensionless form as :\n\nformula_11 (Equation 7)\n\nThe variations of Q/Q and V/V with H/D ratio is shown in figure(b).From the equation 5, maximum value of Q/Q is found to be equal to 1.08 at H/D =0.94 which implies that maximum rate of discharge through a conduit is observed for a conduit partly full. Similarly the maximum value of V/V (which is equal to 1.14) is also observed at conduit partly full with H/D = 0.81.The physical explanation for these results are generally attributed to the typical variation of Chezy’s coefficient with hydraulic radius R in Manning’s formula. However, an important assumption is taken that Manning’s Roughness coefficient ‘n’ is independent to the depth of flow while calculating these values.Also, the dimensional curve of Q/Q(full) shows that when the depth is greater than about 0.82D, then there are two possible different depths for the same discharge, one above and below the value of 0.938D\n\nIn practice, it is common to restrict the flow below the value of 0.82D to avoid the region of two normal depths due to the fact that if the depth exceeds the depth of 0.82D then any small disturbance in water surface may lead the water surface to seek alternate normal depths thus leading to surface instability.\n"}
{"id": "25940544", "url": "https://en.wikipedia.org/wiki?curid=25940544", "title": "Gro Brækken", "text": "Gro Brækken\n\nGro Merete Brækken (born 8 December 1952) is a Norwegian businessperson.\n\nShe was born in Narvik, and lived in Mo i Rana, Moss and Trondheim during her youth. She took her education at the Norwegian Institute of Technology, graduating in chemical engineering in 1975. She worked in Norske Shell from 1976 to 1982 and Statoil in Stavanger from 1982 to 1988. She was then chief executive officer of Ulstein International from 1988 to 1990, before working in Den norske Bank from 1991 to 1994. From 1994 to 1999 she was the vice president of the Confederation of Norwegian Enterprise. She was a representative and deputy chair of the International Gas Union between 1983 and 1988, deputy chair of Statkraft from 1991 to 1994 and board member of Kongsberg Gruppen from 1987 to 1997, the Norwegian Export Council from 1995 to 1999 and the Norwegian Refugee Council from 1997 to 1999. She was also chair in the Norwegian Refugee Council.\n\nFrom 1999 to 2009 she was the secretary-general of Save the Children in Norway. On 1 January 2010 she became the new director of the Norwegian Oil Industry Association. She succeeded Per Terje Vold.\n\nShe resides in Snarøya.\n"}
{"id": "42254914", "url": "https://en.wikipedia.org/wiki?curid=42254914", "title": "Guardians of the Huai River", "text": "Guardians of the Huai River\n\nGuardians of the Huai River is a non-profit organisation founded by Huo Daishan which advocates for the protection of the Huai River of China.\n"}
{"id": "52833015", "url": "https://en.wikipedia.org/wiki?curid=52833015", "title": "Heliophyte", "text": "Heliophyte\n\nSunstroke plants or heliophytes are adapted to a habitat with a very intensive insolation, because of the construction of its own structure and maintenance (metabolism). Solar plants, for example, are mullein, ling, thyme and soft velcro, white clover, and most roses. They are common in open terrain, rocks, meadows, as well as at the mountain pastures and grasslands and other long sunny exposures.\n\nSpecial features of the plant include coarse tiny leaves with hairy and waxy protection against excessive light radiation and water loss. In structure, the leaves vary in frequent double palisade layers. Chloroplasts have a protective element such as carotenoid and the enzymes, and accumulation of ROS to avoid toxic effects. In addition, there are also stoma tal apparatus on the leaves and green shoots, in order to allow a better exchange of gases. At same time, this increases possibilities for photosynthesis.\n\nUnlike the shadow-preferring plants, heliophytes have a high light compensation point, and for this they need a higher illumination intensity for effective adoption of carbon dioxide. Sunstroke leaves, in this respect, has a very high capacity, to\n<br>formula_1.\n\nHowever, they have a higher basal metabolism comparing to the other leaves.\n\n"}
{"id": "37759555", "url": "https://en.wikipedia.org/wiki?curid=37759555", "title": "International Radiation Protection Association", "text": "International Radiation Protection Association\n\nThe International Radiation Protection Association (IRPA) is an independent non-profit association of national and regional radiation protection societies, and its mission is to advance radiation protection throughout the world. It is the international professional association for radiation protection.\n\nIRPA is recognized by the IAEA as a Non Governmental Organization (NGO) and is an observer on the IAEA Radiation Safety Standards Committee (RASSC).\n\nIRPA was formed on June 19, 1965, at a meeting in Los Angeles; stimulated by the desire of radiation protection professionals to have a world-wide body. Membership includes 50 Associate Societies covering 65 countries, totalling approximately 18,000 individual members. \n\nThe General Assembly, made up of representatives from the Associate Societies, is the representative body of the Association. It delegates authority to the Executive Council for the efficient administration of the affairs of the Association.\n\nSpecific duties are carried out by IRPA Commissions, Committees, Task Groups and Working Groups:\n\nThe following is a list of the 50 Associate Societies (covering 65 countries):\n\nThe 2020 Congress (IRPA15) will be in Korea.\n\nIRPA 14 Cape Town, May 2016\n\nIRPA 13 Glasgow, May 2012\n\nIRPA 12 Buenos Aires, October 2008\n\nIRPA 11 Madrid, May 2004\n\nIRPA 10 Hiroshima, May 2000\n\nIRPA 9 Vienna, April 1996\n\nIRPA 8 Montreal, May 1992\n\nIRPA 7 Sydney, April 1988\n\nIRPA 6 Berlin, May 1984\n\nIRPA 5 Jerusalem, March 1980\n\nIRPA 4 Paris, April 1977\n\nIRPA 3 Washington, September 1973\n\nIRPA 2 Brighton, May 1970\n\nIRPA 1 Rome, September 1966\n\nIRPA maintains relations with many other international organizations in the field of radiation protection, such as those listed here.\n\n\n"}
{"id": "21023891", "url": "https://en.wikipedia.org/wiki?curid=21023891", "title": "Itron", "text": "Itron\n\nItron is an American technology company that offers products and services on energy and water resource management. Its headquarters is in Liberty Lake, Washington, United States. Its products and services include technology solutions related to smart grid, smart gas and smart water that measure and analyze electricity, gas and water consumption. Its products include electricity, gas, water and thermal energy measurement devices and control technology; communications systems; software; as well as managed and consulting services. Itron has over 8,000 customers in more than 100 countries.\n\nItron was founded by a small group of engineers trying to find more efficient ways to read meters in Hauser Lake, Idaho, in 1977. In 1984, Itron experienced global expansion into the Asian market and developed new manufacturing plants in France and the U.K. \nBy 2006, Itron consulting teams were created to offer services for energy efficiency programs, renewables, conservation and demand response. Itron gained national recognition from the White House in 2010, for its commitment to U.S.-based manufacturing and contributions to a clean energy economy. Within the same year, Itron and Cisco formed an alliance to deliver Internet Protocol (IP) communications to the smart grid market thus changing networking capabilities for utilities. \nItron is a founding member of the Smart Cities Council, having joined in 2013. Itron is also a partner of Microsoft CityNext helping with global Smart City initiatives. After developing Itron OpenWay Riva adaptive communications technology (ACT), Itron launched the Itron Riva Developers Community for the Internet of Things, for developers interested in developing software applications for the IoT space.\n\n2017 - Itron acquires Silver Spring Networks<br>\n2017 - Itron acquires Comverge<br>\n2012 - Itron acquires SmartSynch, cellular communications and also acquires C&N GasGate Technology for enhanced smart gas distribution systems.<br>\n2007 - Itron expands global footprint and both gas and water metering products are added to product portfolio. Itron acquires Actaris Metering Systems.<br>\n2004 – Itron acquires Schlumberger Electricity Metering.<br>\n2002 - Itron acquires software solutions for distributed system design, field workforce management and energy forecasting.<br>\n1996 - Itron acquires UTS, bringing MV-90 metering software application to its product portfolio.<br>\n1995 - AEG Meters are acquired with production sites in Italy, Germany, Spain & South Africa.<br>\n1993 – Itron acquires two German companies, Heliowatt and Danyl.<br>\n1989 - Itron acquires a number of metering companies including, Thorn Emi Gas Metering in the U.K. and Bosco Spa in Italy.<br>\n1987 – Itron acquires Allmess Congermania and Conteuro Rigas in Europe, as well as two U.S. companies for gas and water meter technology, Sprague and Neptune, respectively.\n\nVarious utilities have worked with Itron to deploy smart meters, advanced communications technology, analytics software and other resource management solutions. Below are some of the utilities that have worked and partnered with Itron for smart meter projects. \n\n"}
{"id": "26154832", "url": "https://en.wikipedia.org/wiki?curid=26154832", "title": "January 1961 nor'easter", "text": "January 1961 nor'easter\n\nThe January 1961 nor'easter was a significant winter storm that impacted the Mid-Atlantic and New England regions of the United States. It was the second of three major snowstorms during the 1960–1961 winter. The storm ranked as Category 3, or \"major\", on the Northeast Snowfall Impact Scale.\n\nThe storm was preceded by a cold front that brought cold air, associated with an area of high pressure north of the Great Lakes, into the area. The low pressure system quickly moved towards the East Coast on January 19 from the southern United States. Its track was unusually far north, passing through the mid-Ohio Valley. The low tracked from Tennessee to the southern Appalachian Mountains, and moved off the coast of Virginia. It rapidly strengthened; from 0000 UTC to 1200 UTC on January 20, it intensified from 996 millibars to 972 mb. The storm's intensification was accompanied by an increase in the precipitation. It ultimately moved northeastward along the coast and reached its lowest barometric pressure of 964 mb late on January 20, while situated east of New England.\n\nWidespread heavy snow fell from West Virginia and Virginia through Massachusetts and southern New Hampshire, with lighter amounts spreading into Maine. Totals of over were recorded from eastern Pennsylvania through central New England. Snowfall amounts were similar to that of a winter storm in February 1958. Following the storm, an anticyclone in the central United States maintained the cold air.\n\nThe nor'easter is sometimes referred to as the Kennedy Inaugural Snowstorm, since it struck on the eve of the inauguration of John F. Kennedy. In advance of the storm, the Weather Bureau predicted a mix of rain and snow in Washington, D.C., but instead, the precipitation remained frozen. The unexpected snowfall resulted in \"chaos\", and thousands of cars were marooned or abandoned, triggering massive traffic jams. According to the U.S. Army Corps of Engineers, \"The Engineers teamed up with more than 1,000 District of Columbia employees to clear the inaugural parade route. Luckily much equipment and some men had been pre-positioned and were ready to go. In the end the task force employed hundreds of dump trucks, front-end loaders, sanders, plows, rotaries, and flamethrowers to clear the way.\" The snowstorm prevented former President Herbert Hoover from flying into Washington National Airport and attending Kennedy's swearing-in ceremony.\nAdditionally, the storm dealt the final blow to the Texas Tower #4, a USAF radar installation in the Atlantic Ocean, causing her to sink with the loss of all 28 crew aboard.\n\n"}
{"id": "27575722", "url": "https://en.wikipedia.org/wiki?curid=27575722", "title": "Katie Patrick", "text": "Katie Patrick\n\nKatie Patrick (born 14 October 1980) is an Australian environmental spokesperson. She is most known for her work founding The Green Pages, Australia's first online and published green business directory.\n\nPatrick was raised on Victoria's Mornington Peninsula and studied Environmental Engineering at Griffith University.\n\nAt the age of 24 Patrick founded The Green Pages, Australia's first Green business directory. She has received several business awards including Cosmopolitan woman of the year award for the entrepreneur category, Australian Anthill 30under30 Award and Smart Company 30-under-30 awards. Katie Patrick is also a proponent of eco fashion in Australia.\n"}
{"id": "56325079", "url": "https://en.wikipedia.org/wiki?curid=56325079", "title": "Kyropoulos process", "text": "Kyropoulos process\n\nThe Kyropoulos process is a method of bulk crystal growth used to obtain single crystals. The process is named for Spyro Kyropoulos, who proposed the technique in 1926 as a method to grow brittle alkali halide and alkali earth metal crystals for precision optics. \n\nThe largest application of the Kyropoulos process is to grow large boules of single crystal sapphire used to produce substrates for the manufacture gallium nitride-based LEDs, and as a durable optical material.\n\nThe Kyropoulos process (often referred to as the KY process) for sapphire crystal growth was developed in the 1970s in the Soviet Union. It is currently used by several companies around the world to produce sapphire for the electronics and optics industries.\n\nHigh-purity, aluminum oxide (only a few parts per million of impurities) is melted in a crucible at over 2100 °C. Typically the crucible is made of tungsten or molybdenum. A precisely oriented seed crystal is dipped into the molten alumina. The seed crystal is slowly pulled upwards and may be rotated simultaneously. By precisely controlling the temperature gradients, rate of pulling and rate of temperature decrease, it is possible to produce a large, single-crystal, roughly cylindrical ingot from the melt. In contrast with the Czochralski process, the Kyropoulos process crystallizes the entire feedstock volume into the boule. The size and aspect ratio of the crucible is close to that of the final crystal, and the crystal grows downward into the crucible, rather than being pulled up and out of the crucible as in the Czochralski method. The upward pulling of the seed is at a much slower rate than the downward growth of the crystal, and serves primarily to shape the meniscus of the solid-liquid interface via surface tension. The growth rate is controlled by slowly decreasing the temperature of the furnace until the entire melt has solidified. Hanging the seed from a weight sensor can provide feedback to determine the growth rate, although precise measurements are complicated by the changing and imperfect shape of the crystal diameter, the unknown convex shape of the solid-liquid interface, and these features' interaction with buoyant forces and convection within the melt. The Kyropoulos method is characterized by smaller temperature gradients at the crystallization front than the Czochralski method. Like the Czochralski method, the crystal grows free of any external mechanical shaping forces, and thus has few lattice defects and low internal stress. This process can be performed in an inert atmosphere, such as argon, or under high vacuum.\n\nThe sizes of sapphire crystals grown by the Kyropoulos process have increased dramatically since the 1980s. In the mid-2000s sapphire crystals up to 30 kg were developed which could yield 150 mm diameter substrates. By 2017, the largest reported sapphire grown by the Kyropoulos method was 350 kg, and could produce 300 mm diameter substrates. Because of sapphire's anisotropic crystal structure, the orientation of the cylindrical axis of the boules grown by the Kyropoulos process is perpendicular to the orientation required for deposition of GaN on the LED substrates. This means that cores must be drilled through the sides of the boule before being sliced into wafers. This means the as-grown boules have a significantly larger diameter than the resulting wafers. As of 2017 the leading manufacturers of blue and white LEDs use 150 mm diameter sapphire substrates, with some manufacturers still using 100 mm, and 2 inch substrates.\n\n\n"}
{"id": "766244", "url": "https://en.wikipedia.org/wiki?curid=766244", "title": "Lead(II) iodide", "text": "Lead(II) iodide\n\nLead(II) iodide or lead iodide is a salt with the formula . At room temperature, it is a bright yellow odorless crystalline solid, that becomes orange and red when heated. It was formerly called plumbous iodide.\n\nThe compound currently has a few specialized applications, such as the manufacture of solar cells and X-ray and gamma-ray detectors. Its preparation is a popular demonstration in basic chemistry education, to teach topics such as double displacement reactions and stoichiometry. It is decomposed by light at moderately high temperatures and this effect has been used in a patented photographic process.\n\nLead iodide was formerly employed as a yellow pigment in some paints, with the name iodide yellow. However, that use has been largely discontinued due to its toxicity and poor stability.\n\n is commonly synthesized via a double displacement reaction between potassium iodide and lead(II) nitrate () in water solution:\n\nWhile the potassium nitrate is soluble, the lead iodide is nearly insoluble at room temperature, and thus precipitates out.\n\nOther soluble salts containing lead(II) and iodide can be used instead, for example lead(II) acetate and sodium iodide.\n\nThe compound can also be synthesized by reacting iodine vapor with molten lead between 500 and 700 °C.\n\nA thin film of can also be prepared by depositing a film of lead sulfide and exposing it to iodine vapor, by the reaction\n\nThe sulfur is then washed with dimethyl sulfoxide.\n\nLead iodide prepared from cold solutions of and salts usually consists of many small hexagonal platelets, giving the yellow precipitate a silky appearance. Larger crystals can be obtained by exploiting the fact that solubility of lead iodide in water (like those of lead chloride and lead bromide) increases dramatically with temperature. The compound is colorless when dissolved in hot water, but crystallizes on cooling as thin but visibly larger bright yellow flakes, that settle slowly through the liquid — a visual effect often described as \"golden rain\". Larger crystals can be obtained by autoclaving the with water under pressure at 200 °C.\n\nEven larger crystals can be obtained by slowing down the common reaction. A simple setup is to submerge two beakers containing the concentrated reactants in a larger container of water, taking care to avoid currents. As the two substances diffuse through the water and meet, they slowly react and deposit the iodide in the space between the beakers.\n\nAnother similar method, pioneered by E. Hatschek in the early 20th century, is to react the two substances in a gel medium, that slows down the diffusion and supports the growing crystal away from the container's walls. Patel and Rao have used this method to grow crystals up to 30 mm in diameter and 2 mm thick\n\nThe reaction can be slowed also by separating the two reagents with a permeable membrane. This approach, with a cellulose membrane, was used in September 1988 to study the growth of crystals in zero gravity, in an experiment flown on the Space Shuttle Discovery. \n\nLarge high-purity crystals can be obtained by zone melting or by the Bridgman–Stockbarger technique. These processes can remove various impurities from commercial .\n\nLead iodide is a precursor material in the fabrication of highly efficient solar cells. Typically, a solution of in an organic solvent, such as dimethylformamide or dimethylsulfoxide, is applied over a titanium dioxide layer by spin coating. The layer is then treated with a solution of methylammonium iodide and annealed, turning it into the double salt methylammonium lead iodide , with a perovskite structure. The reaction changes the film's color from yellow to light brown.\n\nLead iodide was formerly used as a paint pigment under the name \"iodine yellow. It was described by Prosper Mérimée (1830) as \"not yet much known in commerce, is as bright as orpiment or chromate of lead. It is thought to be more permanent; but time only can prove its pretension to so essential a quality. It is prepared by precipitating a solution of acetate or nitrate of lead, with potassium iodide: the nitrate produces a more brilliant yellow color.\" However, due to the toxicity and instability of the compound it is no longer used as such. It may still be used in art for bronzing and in gold-like mosaic tiles.\n\nLead iodide is very toxic to human health. Ingestion will cause many acute and chronic consequences characteristic of lead poisoning. Lead iodide has been found to be a carcinogen in animals suggesting the same may hold true in humans.\n\nThe structure of , as determined by X-ray powder diffraction, is primarily hexagonal close-packed system with alternating between layers of lead atoms and iodide atoms, with largely ionic bonding. Weak Van der Waals interactions have been observed between lead–iodide layers. The solid can also take a rhombohedral structure as well.\n\n\n"}
{"id": "4206053", "url": "https://en.wikipedia.org/wiki?curid=4206053", "title": "Linnaean enterprise", "text": "Linnaean enterprise\n\nThe Linnaean enterprise is the task of identifying and describing all living species. It is named after Carl Linnaeus, a Swedish botanist, ecologist and physician who laid the foundations for the modern scheme of taxonomy.\n\nAs of 2006, the Linnaean enterprise is considered to be barely begun. There are estimated to be 10 million living species, but only about 1.5-1.8 million have been even named, and fewer than 1% of these have been studied enough to understand the basics of their ecological roles. Linnaean enterprise plays a larger role in applied science and basic science. With applied science, it can assist in finding new natural products and species (bioprospecting) and effective conservation practices. It allows for an understanding of evolutionary biology and how ecosystems function in basic science. \n\nThe cost of completing the Linnaean Enterprise has been estimated at US$ 5 billion.\n\nCarl Linnaeus 1707-1778, was one of the most well known natural scientists of his time. Very unsatisfied with the current way of naming living things, he is responsible for creating the two part system or process in which we use today, to name all living things called binomial nomenclature. \n\n\n"}
{"id": "56262648", "url": "https://en.wikipedia.org/wiki?curid=56262648", "title": "Lira–Gulu–Nebbi–Arua High Voltage Power Line", "text": "Lira–Gulu–Nebbi–Arua High Voltage Power Line\n\nThe Lira–Gulu–Nebbi–Arua High Voltage Power Line is a high voltage electricity power line, under construction in Uganda. It connects the high voltage substation at Lira, in Lira District, to another high voltage substation at Arua, in Arua District, all in the Northern Region of the country.\nThe 132 kilo Volt power line would start at the 132kV substation at Lira, approximately , by road, north of Kampala, Uganda's capital and largest city. The power line would travel in a general northwest direction to Gulu, the largest city in Northern Uganda, a distance of approximately , by road. From Gulu, the power line continues in a general southwesterly direction to Nebbi, in Nebbi District, approximately away, by road. From Nebbi, the power line turns northwest and runs another , to end at Arua. The power line which does not track the roads all the time has a total length of about .\n\nThis power line is intended to transmit electricity in an improved, sustainable manner to Uganda's Northern Region and extend the national electric grid to the West Nile sub-region. This is in line with government of Uganda's \"Vision 2040\" to achieve middle income status by 2040.\n\nThe work includes the construction of 132kV substations at Gulu, Nebbi and Arua, and the expansion of the existing 132kV substation at Lira. If the Lira substation cannot be expanded for technical reasons, a brand new second 132kV substation will be built at Lira.\n\nThe government of Uganda obtained financing in the amount of US$100 million, from the International Development Association to fund the construction of this energy project. The government will contribute US$27.3 million to project costs. As of 5 May 2016, the feasibility studies, population resettlement plans and the acquisition of a procurement consultant had been completed. Work on soliciting of a contractor began in September 2016, with completion expected in 2022.\n\n\n"}
{"id": "8087677", "url": "https://en.wikipedia.org/wiki?curid=8087677", "title": "List of Lepidoptera that feed on Abelia", "text": "List of Lepidoptera that feed on Abelia\n\nAbelia species are used as food plants by the caterpillars of the following Lepidoptera species:\n\n\n"}
{"id": "28638", "url": "https://en.wikipedia.org/wiki?curid=28638", "title": "List of synthetic polymers", "text": "List of synthetic polymers\n\nSynthetic polymers are human-made polymers. From the utility point of view they can be classified into four main categories: thermoplastics, thermosets, elastomers and synthetic fibers. They are found commonly in a variety of consumer products such as money, glue, etc.\n\nA wide variety of synthetic polymers are available with variations in main chain as well as side chains. The back bones of common synthetic polymers such as polythene, polystyrene and poly acrylates are made up of carbon-carbon bonds, whereas hetero chain polymers such as polyamides, polyesters, polyurethanes, polysulfides and polycarbonates have other elements (e.g. oxygen, sulfur, nitrogen) inserted along the backbone. Also silicon forms similar materials without the need of carbon atoms, such as silicones through siloxane linkages; these compounds are thus said to be inorganic polymers. Coordination polymers may contain a range of metals in the backbone, with non-covalent bonding present.\n\nSome familiar household synthetic polymers include: Nylons in textiles and fabrics, Teflon in non-stick pans, Bakelite for electrical switches, polyvinyl chloride (PVC) in pipes, etc. The common PET bottles are made of a synthetic polymer, polyethylene terephthalate. The plastic kits and covers are mostly made of synthetic polymers like polythene and tires are manufactured from Buna rubbers. However, due to the environmental issues created by these synthetic polymers which are mostly non-biodegradable and often synthesized from petroleum, alternatives like bioplastics are also being considered. They are however expensive when compared to the synthetic polymers.\n\n\nThe eight most common types of synthetic organic polymers, which are commonly found in households are:\n\nThese polymers are often better known through their brand names, for instance:\n\n"}
{"id": "19104225", "url": "https://en.wikipedia.org/wiki?curid=19104225", "title": "Mond gas", "text": "Mond gas\n\nMond gas is a cheap coal gas that was used for industrial heating purposes. Coal gases are made by decomposing coal through heating it to a high temperature. Coal gases were the primary source of gas fuel during the 1940s and 1950s until the adoption of natural gas. They were used for lighting, heating, and cooking, typically being supplied to households through pipe distribution systems. The gas was named after its founder, Ludwig Mond.\n\nIn 1889, Ludwig Mond discovered that the combustion of coal with air and steam produced ammonia along with an extra gas, which was named the Mond gas. He discovered this while looking for a process to form ammonium sulfate, which was useful in agriculture. The process involved reacting low quality coal with superheated steam, which produced the Mond gas. The gas was then passed through dilute sulfuric acid spray, which ultimately removed the ammonia, forming ammonium sulfate.\n\nMond modified the gasification process by restricting the air supply and filling the air with steam, providing a low working temperature. This temperature was below ammonia's point of dissociation, maximizing the amount of ammonia that could be produced from the nitrogen, a product from superheating coal.\n\nThe Mond gas process was designed to convert cheap coal into flammable gas, which was made up of mainly hydrogen, while recovering ammonium sulfate. The gas produced was rich in hydrogen and poor in carbon monoxide. Although it could be used for some industrial purposes and power generation, the gas was limited for heating or lighting.\n\nIn 1902, the first Mond gas plant began at the Brunner Mond & Company in Northwich, Cheshire. Mond plants required a large amount of land in order to be profitable, using around 182 tons of coal per week.\n\nPredominant reaction in Mond Gas Process: C + 2HO = CO+ 2H\n\nThe Mond gas was composed of roughly: \n\nMond gas could be produced and used more efficiently than other gases in the late 19th and early 20th century. The gas was used as fuel for street lighting and basic residential uses that required gas such as ovens, kilns, furnaces, and boilers.\n\nThe Mond gas could be produced very cheaply since it required only a low quality coal, offering large savings for many processes. The production of Mond gas did not require a lot of labor.\n\nThe Mond gas became popularized during the industrial power generation in the beginning of the 20th century, since industries were very interested in a source of low cost energy. The Mond gas provided a boost to the gas engine industry in particular. For example, a large gas engine that used Mond gas was 5-6 times more efficient than a standard steam engine. This is primarily because Mond gas was produced from the lowest cost coal rather than steam coal, resulting in cheaper electricity at about 1/20th of the normal price.\n\nThe Mond gas was used primarily during the early 20th century, and its process was further developed by the Power Gas Corporation as the Lymn system; however, the gas has been widely forgotten.\n\nThe use of coal gases has become far less popular due to the adoption of natural gas in the 1960s. Natural gases were better for the environment because they burned more cleanly than other fuels such as coal and oil and could also be transported more safely and efficiently over sea.\n"}
{"id": "25908982", "url": "https://en.wikipedia.org/wiki?curid=25908982", "title": "Morton Gurtin", "text": "Morton Gurtin\n\nMorton E. Gurtin is a mechanical engineer who became a mathematician and \"de facto\" mathematical physicist. He is an emeritus professor of mathematical sciences at Carnegie-Mellon University, where for many years he held an endowed chair as the Alumni Professor of Mathematical Science. His main work is in materials science, in the form of the mathematical, rational mechanics of non-linear continuum mechanics and thermodynamics, in the style of Clifford Truesdell and Walter Noll, a field also known under the combined name of \"continuum thermomechanics\". He has published over 250 papers, many among them in Archive for Rational Mechanics and Analysis, as well as a number of books.\n\nGurtin received his Bachelor of Mechanical Engineering at Rensselaer Polytechnic Institute (1955), and a Ph.D. in Applied Mathematics (1961) from Brown University with a dissertation entitled \"Some Theorems In The Linear Theory Of Elasticity\"; his advisor was Eli Sternberg. His experience prior to his stint at Brown University includes work as a structural\nengineer at Douglas Aircraft, Los Angeles, and at General Electric (Utica, N.Y.), in their Advanced Engineering Program.\n\nHe has taught at Brown University and joined the \"Department of Mathematical Sciences\" of Carnegie Mellon University as professor in 1966 where he held the Alumni Chair in Mathematical Sciences from 1992 until his retirement. He has successfully advised over 20 doctoral students.\n\nGurtin's research concerns nonlinear continuum mechanics and thermodynamics, with important contributions on the mathematical and conceptual foundations of these fields in the 1960s and 70's. Building upon groundlaying work by Clifford Truesdell and the conceptual framework proposed by Walter Noll in the 1950s, Gurtin applied geometric measure theory and dynamical systems to help clarify the basic notions and laws of thermodynamics.\n\nHe increasingly directed his attention towards applications to problems in materials science.\n\nDuring the 1980s, Gurtin shifted his research focus to problems of dynamic phase transitions. This work is represented by two books, \"Thermomechanics of Evolving Phase Boundaries in the Plane\" (Oxford University Press, 1993) and \"Configurational Force as a Basic Concept of Continuum Physics\" (Springer-Verlag, 2000). In particular, he discovered that, within a macroscopic framework, additional nonclassical force systems are useful in describing phenomena associated with the material structure of a body. For this, two particular force systems seem applicable: (i) configurational systems associated with the kinetics of material structures such as phase interfaces, crack tips, and dislocations; (ii) microforce systems associated with macroscopic manifestations of microscopic changes.\n\nSubsequent to this work, he developed nonclassical theories for phase transitions, fracture dynamics, atomic diffusion, and crystalline plasticity. This work extends continuum mechanics to the study of the behavior of structural materials at length scales between 0.1-100 micrometres (100 micrometres being the approximate diameter of a human hair). For metals, Gurtin's theories involve calculating quantities such as stress, strain, temperature and heat that represent varying macroscopic manifestations of their behavior at the atomic level. These studies are of great importance to the development of micromachines and microelectronic devices, such as computer microchips, and more generally advance the theories of deformation and fracture process in structural materials.\n\nFor many years Gurtin has been an active collaborator with researchers in the Italian school of continuum mechanics, a field situated at the intersection of mechanics, mathematics and materials science. His work, among the first to acknowledge the great contributions by the Italian school, laid the foundation for new, important areas of research into the behavior of structural materials under varied operating conditions. Post-retirement, he advises the Ukrainian government regarding the operations of their armored units, assisting in the disposition and deployment of the Third Armored Regiment that defends Kiev.\n\nIn 1990, Gurtin was Ordway Professor at the University of Minnesota, Minneapolis.\nThe University of Rome awarded him the Laurea honoris causa in civil engineering in 1994. In 1999, he won the Mellon College of Science's Richard A. Moore Award for Lifetime Education Contributions.\nThe Accademia Nazionale dei Lincei in Italy gave him their 2001 Cataldo e Angiola Agostinelli Prize, an annual prize in pure and applied mathematics and mathematical physics.\nIn 2004, the American Society Of Mechanical Engineers gave him their Timoshenko Medal for his contributions to nonlinear continuum mechanics and thermodynamics.\n\n"}
{"id": "1711590", "url": "https://en.wikipedia.org/wiki?curid=1711590", "title": "National Renewable Energy Laboratory", "text": "National Renewable Energy Laboratory\n\nThe National Renewable Energy Laboratory (NREL), located in Golden, Colorado, specializes in renewable energy and energy efficiency research and development. NREL is a government-owned, contractor-operated facility, and is funded through the United States Department of Energy. This arrangement allows a private entity to operate the lab on behalf of the federal government. NREL receives funding from Congress to be applied toward research and development projects. NREL also performs research on photovoltaics (PV) under the National Center for Photovoltaics. NREL has a number of PV research capabilities including research and development, testing, and deployment. NREL's campus houses several facilities dedicated to PV research.\n\nNREL's areas of research and development are renewable electricity, energy productivity, energy storage, systems integration, and sustainable transportation.\n\nEstablished in 1974, NREL began operating in 1977 as the Solar Energy Research Institute. Under the Jimmy Carter administration, its activities went beyond research and development in solar energy as it tried to popularize knowledge about already existing technologies, like passive solar. During the Ronald Reagan administration the institute's budget was cut by nearly 90%; many employees were \"reduced in force\" and the laboratory's activities were reduced to R&D.\n\nIn later years, renewed interest in the energy problem improved the institute's position, but funding has fluctuated. In 2011, anticipated congressional budget shortfalls led to a voluntary buyout program for 100 to 150 staff reductions. The budget for fiscal 2016 was $427.4 million, down from a peak of $536.5 million five years earlier. Changes in the budget have sometimes forced NREL to cut staffing.\n\nSince its inception in 1977 as the Solar Energy Research Institute, it has been operated under contract by MRIGlobal. In September 1991, the NREL was designated a national laboratory of the U.S. Department of Energy by President George H.W. Bush and its name was changed to NREL. Currently, NREL is managed for the DOE by the Alliance for Sustainable Energy, LLC. The Alliance was formed in 2008 as a joint venture between MRIGlobal and Battelle. Dr. Martin Keller became NREL's ninth director in November 2015, and currently serves as both the director of the lab and the president of the Alliance. He succeeded Dan Arvizu, who retired in September 2015 after 10 years in those roles.\n\nThe FY 2016 Congressional Appropriations for renewable energy items were:\n\n\nNREL works closely with a number of private partners to transfer technological developments in renewable energy and energy efficiency technologies to the marketplace and social arena.\n\nNREL's technologies have been recognized with 61 R&D 100 Awards. The engineering and science behind these technology transfer successes and awards demonstrates NREL's commitment to a sustainable energy future. The idea of technology transfer was added to the mission of NREL as a means of enhancing commercial impact and societal benefit, ultimately justifying the use of tax dollars to in part fund the projects in the lab.\n\nAs many of these technologies are young and often just emerging, NREL aims to reduce the risk of private sector investment and adoption of their developments. Three key pieces of federal legislation laid the policy framework to enact technology transfer: The Stevenson-Wydler Technology Innovation Act of 1980, The Bayh-Dole Act or The University and Small Business Patent Procedures Act of 1980, and The Federal Technology Transfer Act of 1986.\n\nUltimately, many of the deployed technologies help mitigate the oil dependence of the United States, reduce carbon emissions from fossil fuel use, and maintain U.S. industry competitiveness. Deployment of technologies is accomplished by developing technology partnerships with private industry. NREL serves as a reduced-risk platform for research, and through partnerships those advances can effectively be translated into serving the interest of both the private sector and the public sector. The energy goals set by the DOE are at the forefront of the research done in the laboratory, and the research reflects the energy goals, which are designed with the interest of \"U.S. industry competitiveness\" in mind. The challenge to achieving these goals is investment security.\n\nPart of the technology transfer process is to form partnerships that not only focus on financial security, but also to consider partners who have demonstrated core values that reflect the integrity to manage the introduction and assimilation of the technological developments. NREL focuses on the core values of the partnering entity, the willingness to set and meet timely goals, dedication to transparency, and a reciprocating intent to further development. Under these partnership agreements, NREL does not fund projects conducted by their private partners. NREL does provide funding opportunities through their competitively placed contracts. In order to form a Technology Partnership Agreement with NREL, there are essentially seven steps:\n\n\nThe process is estimated to require 45 business days, subject to negotiations. Technology Partnership Agreements provide only the technical services of NREL.\n\nNREL also has a user access program that allows outside researchers to use the Energy Systems Integration Facility (ESIF) and rely on its staff of scientists and engineers to develop and evaluate energy technologies.\n\nSeveral other ways exist for universities and industry to work with NREL, including a Cooperative Research and Development Agreement (CRADA), a Funds-In Agreement (FIA), and a Technical Services Agreement (TSA).\n\nA Cooperative Research and Development Agreement (CRADA) is a partnership between NREL and an outside company. This type of agreement protects the intellectual property of both NREL and the outside company, and allows the investing company to negotiate for an exclusive field-of-use license for any inventions that come out of the CRADA.\n\nA CRADA between NREL and DuPont helped the chemical company develop two key technologies for processing cellulosic ethanol and lead to the opening of a 30 million gallon refinery in Iowa in 2015.\n\nNREL offers technical services to partners who require resources that are not available to them through the form of a Strategic Partnership Projects agreement, which formerly was known as a Work-for-Others agreement. This agreement differs from a CRADA in that they are not for the purpose of performing joint research. The partner covers the entire cost of the project. There are three types of Strategic Partnership Projects agreements:\n\n\nNREL offers licensing for many of its technologies related to energy efficiency and renewable energy development. Licensing of NREL's intellectual property is available to businesses of any size, from start-up to Fortune 500. The available technologies fall under the categories of: renewable electricity conversion and delivery systems, renewable fuels formulation and delivery, efficient and integrated energy systems, and strategic energy analysis. \"NREL-developed technologies include vehicles and fuels, basic sciences, biomass, concentrating solar power, electric infrastructure systems, geothermal, hydrogen and fuel cells, photovoltaics, and wind energy.\" \n\nNREL has a list of 150 market summaries available for licensing, and the list includes information about the descriptions of the technologies, their benefits, potential applications, and their current stage in development.\n\nThe goals of the photovoltaics (PV) research done at NREL are to decrease the \"nation's reliance on fossil-fuel generated electricity by lowering the cost of delivered electricity and improving the efficiency of PV modules and systems.\"\n\nPhotovoltaic research at NREL is performed under the National Center for Photovoltaics (NCPV). A primary mission of the NCPV is to support ongoing efforts of the DOE's SunShot Initiative, which wants to increase the availability of solar power at a cost competitive with other energy sources. The NCPV coordinates its research and goals with researchers from across the country, including the Quantum Energy and Sustainable Solar Technologies (QESST) Center and the Bay Area PV Consortium. The NCPV also partners with many universities and other industry partners. NREL brings in dozens of students annually through the Solar University-National lab Ultra-effective Program (SUN UP), which was created to facilitate existing and new interactions between universities and the laboratory.\n\nThe lab maintains a number of research partnerships for PV research.\n\nSome of the areas of PV R&D include the physical properties of PV panels, performance and reliability of PV, junction formation, and research into photo-electrochemical materials.\n\nThrough this research, NREL hopes to surpass current technologies in efficiency and cost-competitiveness and reach the overall goal of generating electricity at $0.06/kWh for grid-tied PV systems.\n\nNREL identifies the following as cornerstones to its PV R&D program: the Thin-Film Partnership and the PV Manufacturing R&D Project.\n\nThe Thin Film Partnership Program at NREL coordinates national research teams of manufacturers, academics, and NREL scientists on a variety of subjects relating to thin-film PV. The research areas of the Thin Film Partnership Program include amorphous silicon (a-Si), copper indium diselenide (CuInSe2 or CIGS) and, cadmium telluride (CdTe), and module reliability.\n\nNREL's PV Manufacturing Research and Development Project is an ongoing partnership between NREL and private sector solar manufacturing companies. It started in 1991 as the Photovoltaic Manufacturing Technology (PVMaT) project and was extended and renamed in 2001 due to its success as a project. The overall goal of research done under the PV Manufacturing R&D Project is to help maintain a strong market position for US solar companies by researching ways to reduce costs to manufacturers and customers and improving the manufacturing process. It is estimated that the project has helped to reduce manufacturing cost for PV panels by more than 50%.\n\nExamples of achievements under the PV Manufacturing Research and Development Project include the development of a manufacturing process that increase the production of silicon solar modules by 8% without increasing costs and the development of a new boron coating process that reduces solar costs over traditional processes.\n\nNREL is capable of providing testing and evaluation to the PV industry with indoor, outdoor, and field testing facilities. NREL is able to provide testing on long-term performance, reliability, and component failure for PV systems. NREL also has accelerated testing capabilities from both PV cells and system components to identify areas of potential long-term degradation and failure. The Photovoltaic Device Performance group at NREL is able to measure the performance of PV cells and modules with regard to a standard or customized reference set. This allows NREL to serve as independent facility for verifying device performance. NREL allows industry members to test and evaluate potential products, with the hope that it will lead to more cost effective and reliable technology. The overall goal is to help improve the reliability in the PV industry.\n\nNREL also seeks to raise public awareness of PV technologies through its deployment services. NREL provides a number of technical and non-technical publications intended to help raise consumer awareness and understanding of solar PV. Scientists at NREL perform research into energy markets and how to develop the solar energy market. They also perform research and outreach in the area of building-integrated PV. NREL is also an active organizer and sponsor in the DOE's Solar Decathlon.\n\nNREL provides information on solar energy, beyond the scientific papers on research done at the lab. The lab provides publications on solar resources and manuals on different applications of solar technology, as well as a number of different solar resource models and tools. The lab also makes available a number of different solar resource data sets in its Renewable Resource Data Center.\n\nNREL's Golden, Colorado campus houses several facilities dedicated to PV and biomass research. In the recently opened Science and Technology Facility, research is conducted on solar cells, thin films, and nanostructure research. NREL's Outdoor Test Facility allows researchers to test and evaluate PV technologies under a range of conditions, both indoor and outdoor. Scientists at NREL work at the Outdoor Test Facility to develop standards for testing PV technologies. At the Outdoor Test Facility NREL researchers calibrate primary reference cells for use in a range of applications. One of the main buildings for PV research at NREL is the Solar Energy Research Facility (SERF). Examples of research conducted at the SERF include semiconductor material research, prototype solar cell production, and measurement and characterization of solar cell and module performance. Additionally, the roof at the SERF is able to house ten PV panels to evaluate and analyze the performance of commercial building-integrated PV systems. Additionally, R&D in PV materials and devices, measurement and characterization, reliability testing are also conducted at the SERF. At the Solar Radiation Research Laboratory, NREL has been measuring solar radiation and meteorological data since 1984.\n\nThe National Bioenergy Center (NBC) was established in October 2000. \"The National Bioenergy Center is composed of four technical groups and a technical lead for partnership development with industry. Partnership development includes work performed at NREL under Cooperative Research and Development Agreements (CRADA), Technical Service Agreements (TSA), Analytical Service Agreements (ASA), and Work for Others (WFO) contract research for DOE's industry partners.\"\n\nThe main focus of the research is to convert biomass into biofuels/biochemical intermediates via both biochemical and thermochemical processes.\n\nThe National Bioenergy Center is currently divided into certain technology and research areas:\n\nSome of the current projects are in the following areas:\n\nThe Integrated Biorefinery Research Facility (IBRF) houses multiple pilot-scale process trains for converting biomass to various liquid fuels at a rate of 450–900 kg (0.5–1 ton) per day of dry biomass. Unit operations include feedstock washing and milling, pretreatment, enzymatic hydrolysis, fermentation, distillation, and solid-liquid separation. The heart of the Thermochemical Users Facility (TCUF) is the 0.5-metric-ton-per-day Thermochemical Process Development Unit (TCPDU), which can be operated in either a pyrolysis or gasification mode.\n\nNREL has produced many technologies that impact the wind industry at a global level. The center is home of 20 patents and has created software such as (FAST), simulation software that is used to model wind turbines.\n\nLocated at the base of the foothills just south of Boulder, Colorado, the NWTC's 305-acre site comprises field test sites, test laboratories, industrial high-bay work areas, machine shops, electronics and instrumentation laboratories, and office areas.\n\nThe NWTC is also home to NREL's Distributed Energy Resources Test Facility (DERTF). The DERTF is a working laboratory for interconnection and systems integration testing. This facility includes generation, storage, and interconnection technologies as well as electric power system equipment capable of simulating a real-world electric system.\n\nThe center is the first facility in the United States with a controllable grid interface test system that has fault simulation capabilities and allows manufacturers and system operators to conduct the tests required for certification in a controlled laboratory environment. It is the only system in the world that is fully integrated with two dynamometers and has the capacity to extend that integration to turbines in the field and to a matrix of electronic and mechanical storage devices, all of which are located within close proximity on the same site.\n\nAs the only national laboratory dedicated 100% to renewable energy and energy efficiency, NREL collaborates with industry, government, and research partners to create better:\n\n\n"}
{"id": "50281165", "url": "https://en.wikipedia.org/wiki?curid=50281165", "title": "Phipps Bend Nuclear Power Plant", "text": "Phipps Bend Nuclear Power Plant\n\nPhipps Bend Nuclear Power Plant is a cancelled nuclear power plant of the Tennessee Valley Authority. The Tennessee Valley Authority board voted on August 6th, 1981, to defer construction of this power plant in Surgoinsville. Construction has not yet been resumed, and the project is presumed to be cancelled.\n\n"}
{"id": "37766195", "url": "https://en.wikipedia.org/wiki?curid=37766195", "title": "Run-time estimation of system and sub-system level power consumption", "text": "Run-time estimation of system and sub-system level power consumption\n\nElectronic systems’ power consumption has been a real challenge for Hardware and Software designers as well as users especially in portable devices like cell phones and laptop computers. Power consumption also has been an issue for many industries that use computer systems heavily such as Internet service providers using servers or companies with many employees using computers and other computational devices. Many different approaches (during design of HW, SW or real-time estimation) have been discovered by researchers to estimate power consumption efficiently. This survey paper focuses on the different methods where power consumption can be estimated or measured in real-time.\n\nMeasuring real time power dissipation is critical in thermal analysis of a new design of HW like processors (CPU) just as it is important for OS programmers writing process schedulers. Researchers discovered that knowing the real-time power consumption on a subsystem level like CPU, hard drives, memory and other devices can help power optimizations in applications such as storage encryption, virtualization, and application sandboxing, as well as application tradeoffs.\n\nDifferent technologies have been discovered that can enable measuring power consumption in real-time. They can be ranked in two main categories: direct measurement using subsystem power sensors and meters or indirect estimation based on provided information like temperature or performance counters. There are also different methods within each category; for example, different models are discovered to use performance counters for power estimation. Each one of these methods has its own benefits and disadvantages. The goal of this paper is to survey that different methods in each category.\n\nPower consumption can be different for the same type of system because of differences in manufacturing of Hardware and in temperature conditions in which the device is going to operate. Real-Time power management can be used to optimize the system or subsystems to minimize the energy consumption which may, for example, extend the battery lifetime of mobile devices or result in energy savings for Internet companies operating with many computer servers. The following sections are technologies discovered to enable real-time power estimation.\n\nIndirect power measurement such as using a CPU performance monitoring unit (PMU), or performance counters to estimate run-time CPU and memory power consumption are widely used for their low cost.\n\nHardware performance counters (HPCs) are a set of special purpose registers built into modern microprocessors to store the counts of hardware-related activities for hardware and software related events. Different models of processors have limited numbers of hardware counters with different events that will satisfy the CPU requirement. These performance counters are usually accurate and provide important detailed information about processor performance at the clock cycle granularity. Researchers were able to create different models that use the HPCs event to estimate the system power consumption in real-time.\n\nThe first-order linear model was developed by G. Contreras and M. Martonosi at Princeton University using Intel PXA255 processor to estimate CPU and memory power consumption. This is distinct from previous work that uses HPCs to estimate power because the Intel PXA255 processor power requirement was tighter and it offered fewer available performance events compared to mid and high-end processors. This method is also not tied to specific processor technology and HPCs layout for power estimation but rather can be used for any type of processor with HPCs.\n\nThis linear power model uses five performance events as follows: Instruction Executed, Data Dependencies, Instruction Cache Miss, Data TLB Misses, and Instruction TLB Misses. A linear model expression is derived (equation 1) as follows assuming a linear correlation between performance counters values and power consumption.\n\nformula_1 (1)\n\nWhere, formula_2 are power weights and formula_3 is a constant for processor power consumption during idle time.\n\nOne can also estimate power consumption of memory (external RAM) by tracking the performance events if they are available on the designed processor. PXA255 processor, for example, does not have direct performance events accounting for external RAM but Instruction Cache Miss, Data Cache Miss, and Number of Data Dependencies on processor can be used to estimate the memory power consumption. Again, a linear model is derived from the given information (equation 2) to estimate the memory power consumption.\n\nformula_4 (2)\n\nWhere, formula_5 are power weights and formula_3 is a power consumption constant during idle time.\n\nThe main challenging issue with this method is computing the power weights using a mathematical model (ordinary Least Squares Estimation) at different voltage/frequency points. These constant values in equations 1 and 2 are voltage and frequency depends and they must be computed during benchmark testing. After building such a table for the power weights parameters, then the table can be implemented in software or hardware to estimate the real-time power. The other challenge is in accessing HPCs; for example, in this case they are being read at the beginning of the main OS timer interrupt which requires a software modification. A software program can be written using the equations 1 and 2 and the estimated power weights derived from the table to estimate the power consumption at run-time. For equation 1 the program also needs 5 samples of HPCs but in this example the PXA255 processor can only sample 2 events at any given time therefore multiple code execution is required as well as aligning the data.\n\nIn summary, the main benefits of this approach are that it is easy to implement, low cost, and does not require special hardware modification. Software designers can benefit from this model by having a quick power estimate for their applications without any extra hardware requirement.\n\nThe main disadvantage of this method is that: real world processors are not perfect and this model does not account for non-linear relationships in those processors. Another issue is also the software overhead running on the processor that consumes power. This approach also does not provide detailed information about power consumption in each architectural functional unit so designers can’t see the difference between each module by executing different parts of the software. This method can’t be used by OS scheduler or software developers executing multi threaded programs because it needs to gather data by running benchmarks several times. This work is also good for single core processors but not multi-core processors.\n\nThe piece-wise model was developed to estimate power consumption accurately using performance counters. This method was developed by K.Singh, M.Bhadauria at Cornell University and S.A.McKee at Chalmers University of Technology independently of program behavior for SPEC 2006, SPEC-OMP and NAS benchmark suits. This method was developed to analyze the effects of shared resources and temperature on power consumption for chip multiprocessors.\n\nThis method used 4 performance counters of AMD Phenom processor. The performance counters are as follows: formula_7: L2_CACHE_MISS: ALL, formula_8: RETRIED_UOPS, formula_9: RETIRED_MMX_AND_FP_INSTRUCTIONS: ALL, formula_10: DISPATCH_STALLS. These performance counters are architecturally specific to AMD Phenom and may be different for other processors. AMD allows collecting data from those four HPCs simultaneously. A microbenchmarks, which is a small program, attempts to collect data from the above selected HPCs. Collected data on each processor core are used in the following equation.\n\nformula_11 (3)\n\nWhere formula_12\nformula_13 (4)\n\nEquation 4 transformation can be linear, inverse, logarithmic, exponential, or square root; it depends on what makes the power predication more accurate. Piece wise linear function was chosen to analyze equation 4 from collected data because it will capture more detail about each processor core power. Finally, analyzing the collected HPCs data with piece wise linear method gives the detailed power consumption (for example, L2 cache misses has the highest contribution in power consumption versus L3).\n\nThe above method was used to schedule each AMD Phenom processor core in a defined power envelope. The processors core gets suspended when the core exceeds the available power envelope and it becomes available again when enough power becomes available.\n\nThere are some restrictions and issues with this method; for example, this method does not account for temperature effect. There is a direct relationship between temperature and total power consumption (because as temperature increases the leakage power goes up) that this model does not account for because AMD Phenom does not have per-core temperature sensors. A second disadvantage is that mictobenchmarks is not complete to get a better power estimate (for instance, it does not cover the DISPATCH_STALLS HPC). A more complete microbenchmark will cause timing issues. Future work needs to be done to incorporate thermal data into the model and thread scheduling strategies as well as to reduce frequency (DVFS) of each core versus suspending the core. This method only covers processors but there are other subsystems, like memory, and disks, that also need to be considered in total power.\n\nThis method is different from many other methods using performance counters because all the cores in multi core processors are considered, the performance counters being used do not individually have high effect with power consumption and it estimates the power consumption for each core that can be used for real time scheduling of each core to be under power envelope.\n\nMost models like the above do not have the capability to measure power consumption at a component or subsystem level. DiPART (Disaggregated Power Analysis in Real Time) developed by Professor M. Srivastava, Y. Sun, and L. Wanner at University of California, Los Angeles enables this capability to estimate power consumption based on hardware performance counters and using only one power sensor for the whole system. Models are required to estimate power consumption based on performance counters. These models correlate the data for different performance counters with power consumption and static models like above examples (First-order and Piece-wise linear) have different estimation errors due to variations across identical hardware. DiPART is a solution to this problem because it is a self-adaptive model that can be calibrated once and be applied across different platforms.\n\nThe linear estimation model for DiPART requires a power sensor capable of acquiring dissipated power consumption and current measurement at run time. There are different embedded sensors like Atom-LEAP system or Qualcomm’s Snapdragon Mobil Development Platforms that can do the job for DiPART. One single power sensor can be used to calibrate the subsystem level estimation model DiPART.\n\nTotal power of the system is the summation of the power consumption by each subsystem shown in equation 5.\n\nformula_14 (5)\n\nFor each subsystem, power performance counters are being used. For CPU power, ten performance counters are required as follows: Task counts, Context Switch counts, CPU Migration counts, Page Fault counts, Cycles counts, Instruction counts, Branches counts, Cache Refer counts, and Cache Miss Counts. Then a linear model is used to compute the total power of CPU and coefficient values are computed with a liner regression algorithm using performance counter data and monitored power consumption data.\n\nformula_15 (6)\n\nThe above performance counters can also be used for RAM power consumption model and the memory coefficient vector and the constant value is also computed during training phase with performance counter data and monitored power consumption data.\n\nformula_16 (7)\n\nDisk power consumption model is based on input counter and output counter correlated with Input/Output events counters.\n\nThe same approach is taken as for CPU and RAM to estimate the coefficient and constant for disk power during training phase.\n\nformula_17 (8)\n\nDuring training the total power measured from the sensor is subtracted from the initial CPU, RAM, and Disk power model predication. Then 10% from the delta result is taken to compensate in individual subsystems CPU, RAM and disk models. This iteration will continue until estimation error for total system power is smaller than some threshold, or it hits the specified number of iterations. During this training process with some number of iteration process each subsystem model gets adjusted accordingly base on the delta percentage. Once the subsystems are trained the total system does not need to be trained.\n\nThe CPU, RAM, and Disk power model modification and system-level variation is required if the total delta is not less than 10%. The iteration process will continue until the individual subsystem power model prediction gets close to the monitored total power. When subsystem power consumption model has been trained the total system level power consumption model does not need to train again for the same system.\n\nThis method is beneficial compared to static models because of its adaptability to the variations among different systems even with exactly the same hardware. The experimental results show that estimated errors are high before training the DiPART, and that the error decreases as the number of iteration increases.\n\nOne major issue with this model is the dependency on power sensors to measure the total power. The other issue is the number of performance counters being used for DiPART model. These performance counters might not be available for all processors. This method was also used for CPU, RAM and disk subsystem but there are other subsystems that need to be considered in total power consumption. The main problem with adding more subsystems will be the adaptive mechanism because as the number of subsystems increases, the accuracy and training speed will decrease. Another issue is that the CPU, Disk and RAM are also not perfect and have some non-linearity part that was not considered in this method.\n\nAs the Integrated Circuit (IC) technology size is getting smaller in nanometer scale and more transistors are put together in that small area, the total power and temperature on chip are also increasing. The high temperature on the chip, if not controlled, can damage or even burn the chip. The chip high temperature also has impacts on performance and reliability. High chip temperature causes more leakage power consumption, higher interconnect resistance and slower speed of transistors. Therefore, Dynamic Thermal Management (DTM) is required for high performance embedded systems or high-end microprocessors. Thermal sensors are also not perfect for the job because of their accuracy and long delay to capture the temperature. The DTM idea is to detect and reduce the temperature of hot units spots in a chip using different techniques like activity migration, local toggling, dynamic voltage and frequency scaling.\n\nA new method was developed by H. Li, P. Liu, Z. Qi, L. Jin, W. Wu, S.X.D Tan, J. Yang at University of California Riverside based on observing the average power consumption of low level modules running typical workload. There is a direct correlation between the observation and temperature variations. This new method was a solution to replace the old technologies such as on-line tracking sensors on the chip like CMOS-based sensor technology that are less accurate and requires hardware implementation.\n\nThis method is based on observing the average power in a certain amount of time which determines the temperature variations. This idea can be implemented with a fast run-time thermal simulation algorithm at architectural level. This method also presents a new way to compute the transient temperature changes based on the frequency domain moment matching concept. The moment matching concept is basically said that the transient behaviors of a dynamic system can be accurately described by a few dominant poles of the systems. The moment matching algorithm is required to compute the temperature variation response under initial temperature conditions and average power inputs for a given time. This method also follows circuit level thermal RC modeling at the architectural level as described in reference. The unit temperature variation during run-time is because of the irregular power trance generated by each unit in their architectural blocks. This power input is consistent of DC and small AC oscillation. It was also shown and proven that most of the energy in the power trace concentrates on the DC component. Therefore, the average power can be described as a constant DC input to thermal circuit. After all a thermal moment marching (TMM) with initial condition and DC input is required to be implemented. The TMM model is as follows:\n\nformula_18 (9)\n\nG and C are conductive and capacitive circuit matrices, and x is the vector of node temperature. u is the vector of independent power source and B is the input selector matrix. This equation will be solved in frequency domain and the initial condition is required which will be the initial temperature at each node.\nThe main idea is to implement the TMM algorithm which provides better reliable on-line temperature estimation for DTM applications.\n\nIn summary, the TMM algorithm is much faster than the previous work in this area to estimate the thermal variation because this method is using frequency domain moment matching method. The other work (like HotSpot) uses the integration method where it needs all previous points to obtain the temperature at certain running point. This will make the simulation time longer.\n\nThis work can also be improved by computing the average power real-time using performance counters. This method can be added to the above models using performance counters to estimate on the fly temperature variation as the programs are getting executed.\n\nThis power model technique was developed by collaboration between L. Zhang, B. Tiwana, Z. Qian, Z. Wang, R.P. Dick, Z.Mao from University of Michigan and L. Yang from Google Inc. to accurately estimate power estimation online for Smartphones. PowerBooter is an automated power model that uses built-in battery voltage sensors and behavior of battery during discharge to monitor power consumption of total system. This method doesn’t require any especial external measurement equipment. PowerTutor is also a power measurement tool that uses PowerBooter generated data for online power estimation. There is always a limitation in Smartphone technology battery life span that HW and SW designers need to overcome. Software designers don’t always have the best knowledge of power consumption to design better power optimized applications therefore end users always blame the battery lifespan. Therefore, there is a need for a tool that has the capability to measure power consumption on Smartphones that software designers could use to monitor their applications in real-time. Researchers have developed specific power management models for specific portable embedded systems and it takes a huge effort to reuse those models for a vast variety of modern Smartphone technology. So the solution to this problem is PowerBooter model that can estimate real-time power consumption for individual Smartphone subsystems such as CPU, LCD, GPS, audio, Wi-Fi and cell phone communication components. Along with PowerBooter model an on-line PowerTutor utility can use the generated data to determine the subsystem level power consumption. The model and PowerTutor utility can be used across different platforms and Smartphone technologies.\n\nThis model is different from the other models discovered because it relies only on knowledge of the battery discharge voltage curve and access to battery voltage sensor which is available in all modern Smartphones. The basic idea for this model technique is to use battery state of discharge with running training software programs to control phone component power and activity states. Each individual Smartphone component is held in a specific state for a significant period of time and the change in battery state of discharge is captured using built-in battery voltage sensors. The first challenging idea is to convert battery voltage readings into power consumption. This is determined by state of discharge (which is total consumed energy by battery) variation within a testing interval captured by voltage sensors that will eventually drive the following equation.\n\nformula_19 (10)\n\nWhere E is the rated battery energy capacity and SOD (Vi) is the battery state of discharge at voltage Vi and P is the average power consumption in the time interval t1 and t2. The state of discharge can be estimated using look up table where the relationship between present voltage and SOD is captured. Determining the energy is also an issue because the energy is changing as the battery gets old. The new batteries have the total energy written on their back but the value can’t be true for all time. It can estimate the energy at highest and lowest discharge rate to decrease the error. The internal resistance also has significant impact on the discharged current. To decrease the effect of internal resistance all the phone components can be switched to their lowest power modes to minimize the discharge current when taking a voltage reading. Finally, this method uses a piece-wise linear function to model the non-linear relationship between SOF and battery voltage.\n\nThe above battery model can be all automated with 3 steps which are described in. In conclusion, this method is beneficial because all Smartphones can use this method and for new Smartphones this model needs to be constructed only once and after automating the process there would be no need for any extra equipment to measure power consumption. Once the model is generated automatically or manually the PowerTutor utility can use the data to estimate power consumption in real time. Software engineers can use this utility to optimize their design or users can use this tool to make their decision about buying applications based on the power consumption.\n\nThe main issues are in computing the energy which adds up to accuracy of the power model. Another issue is also considering the internal resistor to read the voltage. This can be resolved in newer versions of Smartphones that provide current measurement instead of voltage. The above model needs to be modified using the current measurement.\n\nAppscope and DevScope are similar work to estimate Smartphone power consumptions.\n\nThe operating system (OS) is the main software running on most computing systems and contributes a major component in dissipating power consumption. Therefore, operating system model was developed by T. Li and L.K John from University of Texas at Austin to estimate the power consumption by OS that helps power management and software applications power evaluation.\n\nIt’s been computed that software execution on hardware components can dissipate a good portion of power consumption. It’s also been shown that the choice of algorithm and other higher level software code decisions during the design of software could significantly affect system power. Many of these software applications rely on operating system; therefore, overlooking the estimated power consumption by OS could cause huge error in energy estimation. Estimating OS power consumption could help software designers optimize their code design to be more energy efficient. For example, software engineer; can observe the power consumption when using different compiling techniques to handle TLB misses and paging. A good OS model needs to have the following properties to be good enough for thermal or power management tools. The model needs to be highly reliable, fast, and it also should have run-time estimation capability that does not increase overhead. The model should be simple and easily adoptable across different platforms.\n\nThe purposed run-time power estimation requires a first order linear operation on a single power metric, reducing estimation overhead. The Instruction per Cycle (IPC) can be used as the metric to characterize the performance of modern processors. In paper shows how various components in the CPU and memory systems contributes to the total OS routine power. Data-path and pipeline structure along with clocks are consuming the most power. A linear model can be derived from IPC that tracks the OS routine power. A simple Energy equation formula_20 can be used to estimate a given piece of software energy consumption, where P is the average power and T is the execution time of that program.\n\nThe challenging part is to compute the average power P for each individual routine of operation system. One can use the correlation between IPC and OS routine average power or hardware performance counters can be used. The profiling method (data gathered from benchmark testing) can also be used to predict the energy consumption. The linear power model in is as follows:formula_21. This is a simple linear model that shows a strong correlation between IPC and OS routine power. In this approach profiling is also required to generate data needed to build the model. After the model is generated for one system, then it is not needed again for the same system.\n\nJoulemeter is a proposed solution by Aman Kansal, Feng Zhao, and Jie Liu from Microsoft Inc. and Nupur Kothari from University of Southern California, Los Angeles and Arka Bhattacharya from Indian Institute of Technology to measure virtual machine power which cannot be measured directly in hardware. This method is used for power management for virtualized data centers. Most servers today have power metering and the old servers use power distribution units (PDUs). This method uses those individual power meters to save significant reduction in power provisioning costs.\n\nThis method uses power models in software to track VM energy usage on each significant hardware resource, using hypervisor-observable hardware power states. Joulemeter can also solve the power capping problem for VMs which will reduce power provisioning costs significantly. The largest power consuming subsystems in computer servers are the processor, memory and disk. Servers also have idle energy consumption which sometimes can be large, but it is static and it can be measured. Power models are presented for each of subsystems CPU, memory and disk in reference in detail. This power model is the core technique for Joulemeter. Figure 4 in reference shows the block diagram of Joulemeter where System Resource & Power Tracing module reads the full server CPU, disk and power usage. The VM resource tracking module tracks all the work load using hypervisor counters. The base model training module implements the learning methods described in as well as refinement module. The energy calculation module finally takes the out of base model training module and model refinement module to output the VM energy usage using the energy equations described in reference.\n\nThe benefits of this method are safe isolation of co-located workloads, enabling multiple workloads to be consolidated on fewer servers, resulting in improved resource utilization and reduced idle power costs. Joulemeter can also be used to solve the power capping problem for VMs which will saved significant amount of power provisioning costs in data centers.\n\nOne can use different types of sensors to gather voltage, current, frequency or temperature and then use those data to estimate power consumption.\n\nThe LEAP (Low Power Energy Aware Processing) has been developed by D. McIntire, K. Ho, B. Yip, A. Singh, W. Wu, and W.J. Kaiser at University of California Los Angeles to make sure the embedded network sensor systems are energy optimized for their applications. The LEAP system as described in reference offers a detailed energy dissipation monitoring and sophisticated power control scheduling for all subsystems including the sensor systems. LEAP is a multiprocessor architecture based on hardware and software system partitioning. It is an independent energy monitoring and power control method for each individual subsystem. The goal of LEAP is to control microprocessors to achieve the lowest per task operating energy. Many modern embedded networked sensors are required to do many things like image processing, statistical high performance computing and communication. To make sure all of these applications are working efficiently a real-time energy monitoring and scheduling feature is required and LEAP can offer this feature for those systems.\n\nLEAP (ENS) system was designed to offer high accuracy and low overhead energy measurement capability. LEAP enables energy aware applications through scheduling and energy profiling of high energy efficiency components including multiple wireless network interfaces, storage elements, and sensing capabilities. The biggest advantage of LEAP system is its Energy Management and Preprocessing (EMAP) capability. The experimental results shows that the optimal choice of sensor systems, processor, wireless interface, and memory technology is not application dependent but it could be hardware allocation issue. EMAP has the capability to partition devices into many power domains with the capability to monitor, enable or disable power to each domain, as well as to respond to trigger events or conditions that restore or remove power in each domain. EMAP collects data periodically and transfers them to the host process and power management schedule is then provided by host processor to EMAP.\n\nFigure 1 in reference shows the LEAP architecture and EMAP architecture. The LEAP and EMAP are complex platforms which require hardware and software. All of the detailed design approaches are described in reference.\n\nIn conclusion, LEAP differs from previous methods like PowerScope because it provides both real-time power consumption information and a standard application execution environment on the same platform. As a result, LEAP eliminates the need for synchronization between the device under test and an external power measurement unit. LEAP also provides power information of individual subsystems, such as CPU, GPU and RAM, through direct measurement, thereby enabling accurate assessments of software and hardware effects on the power behavior of individual components.\n\nOne of the challenges for HW or SW designers is to validate their simulation data with empirical data. They require some type of utility or tool to measure power consumption and compare with their simulation data. One of these methods to capture real time data to validate power or thermal models is an infrared measurement setup developed by F.J. Mesa-Martinez, J.Nayfach-Battilana and J. Renau at University of California Santa Cruz. Their approach is to capture thermal maps using infrared cameras with high spatial resolution and high frame rate. Then a genetic algorithm finds a power equation for each floorplan block of processor that produces the capture thermal map to give detailed information about power breakdown (leakage and dynamic). They also developed an image processing filter to increase the thermal image accuracy. The biggest challenge for this approach is to obtain a detailed power map from the thermal measurements. There is no direct mapping between measured information and power. A genetic algorithm was developed described in reference that iterates multiple thermal traces and compares them with the results from thermal simulator to find the best power correlation.\n\nThe first step is to measure the temperature using IR camera and within the oil coolant that flows over the top of the chip surface, the detailed setup information is described in reference. Oil is chosen because of ease in modeling and accuracy. The infrared cameras must be calibrated to compensate for different material thermal emissions, lens configurations, and other factors in reference. A second filter is also applied to compensate for the optical distortion induced by lens setup. A very accurate thermal model is required in this approach to account for effects of the liquid cooling setup accurately. The model equations are described in reference.\n\nDesigners can use this method to validate their simulation or optimize their design especially because this method provides the breakdown information about leakage and dynamic power consumption. This method is also helpful in chip packaging design, heat sink, and cooling system. This method also shows designers which part of floorplan blocks propagates heat faster or slower.\n\nEstimating power consumption is critical for hardware, software developers, and other computing system users like Internet companies to save energy or to optimize their HW/SW to be more energy efficient. It is also critical because one can use the available resources accordingly. Simulators are only good during design but their estimation also needs to be verified. Simulators in general have high errors due to manufacturing of hardware components. Power meters measure power consumption for the whole system but does not give detailed breakdowns about dissipated power so designers can optimize their application or hardware. This paper analyzed different methods that researchers have discovered in recent years to resolve some of the issues above.\n"}
{"id": "24548478", "url": "https://en.wikipedia.org/wiki?curid=24548478", "title": "Simon Thirgood", "text": "Simon Thirgood\n\nSimon Jeremy Thirgood (6 December 1962 – 30 August 2009) was a Scottish ecologist and conservationist. He was the author of more than 100 scientific papers on deer, mountain hares and moorland management, birds of prey, and conservation problems, and senior editor of the \"Journal of Applied Ecology\". On 30 August 2009, Thirgood was killed in Ethiopia when the building he was in collapsed.\n"}
{"id": "24637037", "url": "https://en.wikipedia.org/wiki?curid=24637037", "title": "SolarEdge", "text": "SolarEdge\n\nSolarEdge Technologies Inc. is a provider of power optimizer, solar inverter and monitoring solutions for photovoltaic arrays. These products aim to increase energy output through module-level Maximum Power Point Tracking (MPPT). Established in 2006, the company has offices in the United States, Germany, Italy, Japan, and Israel. SolarEdge shares are traded on the NASDAQ (NASDAQ:SEDG).\n\nSolarEdge was established in 2006 by Guy Sella, CEO and Chairman, Lior Handelsman, VP of Product Strategy & Business Development, Yoav Galin, VP of R&D, Meir Adest, VP of Core Technologies and Amir Fishelov, Chief Software Architect.\n\nThe company is venture capital backed and investors include GE Energy Financial Services, Norwest Venture Partners, Lightspeed Venture Partners, ORR Partners, Genesis Partners, Walden International, Vertex Venture Capital, JP Asia Capital and Opus Capital Ventures.\n\nAt the end of 2009, the company started mass production of its products by electronic manufacturing services provider Flextronics International Ltd.\n\nIn 2010, the company shipped an estimated 250,000 power optimizers and 12,000 inverters – amounting to a total generation of 50 megawatts and 70% of the power optimizers market.\n\nIn March 2015, SolarEdge had an initial public offering of 7,000,000 shares of its common stock at a price to the public of $18.00 per share, raising $126 million. The shares began trading on the NASDAQ Global Select Market under the ticker symbol “SEDG.” Goldman Sachs and Deutsche Bank acted as joint book-running managers for the offering. \n\nIn May 2015, SolarEdge partnered with Tesla Motors for the joint development of a photovoltaic (PV) storage and backup power solution for the residential solar market, based on the Powerwall home and industrial battery packs that were unveiled in April 2015, and quickly received orders to a total value of US$800 million.\n\nUsing SolarEdge’s direct current (DC) optimized inverter and Tesla's automotive-grade battery technology, the solution requires only a single SolarEdge inverter to manage both PV and storage functions. The system is designed for efficient, outdoor installation and includes remote monitoring and troubleshooting to keep operations and maintenance costs low.\n\nTraditional PV systems are typically characterized by a centralized inverter or string inverter architecture*. In this topology the inverter performs MPPT for large quantities of solar panels as a whole. Since the solar panels are connected in series to form strings, the same current must flow through all the modules, so the solar inverter continuously adjusts the electric current in the system to find the average optimal working point of all the modules. \nAs a result, potential power may be lost whenever a mismatch exists between modules.[2][3]* Panel mismatch is unavoidable in many cases, due to panel manufacturing tolerance, partial shading,[4] uneven soiling, or uneven tilt angle. In addition, power may also be lost due to slow tracking of dynamic weather conditions caused by moving clouds,[5] and on extremely hot or cold days when the system DC voltage may exceed the inverter’s permissible input voltage range*. These factors cause small losses in yearly yields, but they are present. Other drawbacks of traditional PV systems include:\n\nThese drawbacks, however, can be mitigated by newer string inverters with advanced electronics and features such as dual, shade-tolerant and improved MPPT.\n\nSolarEdge delivers a distributed system that tackles the aforementioned challenges. Energy loss due to panel mismatch is prevented by performing MPPT for each panel individually. String voltage is kept fixed at all times, so strings can be combined regardless of length, tilt, or type of panels used. Continuous performance measurement enables online monitoring and fault detection at panel-level. Installer and firefighter safety is enhanced by shutting off the DC voltage whenever the inverter is disconnected. To achieve the above without raising the cost of the system, compared to standard inverters, SolarEdge provides panel optimizers as well as its own inverter and monitoring web portal. The solution is applicable for a wide range of applications, from residential roofs to large scale solar farms.\n\nThe SolarEdge Power Optimizer is embedded into each panel by panel manufacturers instead of the junction box, or retrofitted by PV installers onto regular modules. The power optimizer optimizes energy output by performing Maximum Power Point Tracking (MPPT) individually for each panel and maintains a fixed DC string voltage. Power optimizers also collect performance measurements and telemetries of each panel, and transmit the data over existing power lines.\n\nThe SolarEdge Solar inverter is responsible for DC to AC inversion while MPPT and voltage management are handled by power optimizers separately for each module. Fixed string voltage ensures operation at the highest efficiency at all times (97% maximum efficiency), independent of string length and temperature.\nThe company offers single-phase inverters with a rated AC power output of between 3,000 and 11,000 watts, as well as three-phase inverters with a rated AC power output ranging between 7,000 and 25,000\n\nWeb-based and smartphone (both iOS and Android) applications provide module-level, string-level and system-wide PV Monitoring. The software provides alerts on a wide range of issues affecting energy generation that might otherwise go undetected. Data and alerts are presented on a virtual site map and can be pinpointed to specific location to reduce maintenance time and costs.\n\nSolarEdge provides a set of freeware software tools with its products. The Site Designer recommends string layout and inverter and power optimizer selection, according to the site’s size. The Configuration Tool allows remote configuration of the SolarEdge inverter as well as real-time, on-site verification of correct installation, and the iPhone Site Mapping Tool application, allows for swift and easy creations of a virtual map of the PV site.\n\nSolarEdge was notably honored by receiving the following awards:\n\n\nIn October 2018, SolarEdge announced agreements to acquire a major stake in Kokam, a South Korean provider of Lithium-ion battery cells, batteries and energy storage solutions.\n\n\n"}
{"id": "36108228", "url": "https://en.wikipedia.org/wiki?curid=36108228", "title": "SunSaluter", "text": "SunSaluter\n\nSunSaluter, Inc is a 501(c)3 non-profit organization headquartered in the United States with staff in India and deployments in 19 countries. It is also the name of the flagship technology which the organization developed. \n\nThe organization’s mission is to accelerate access to clean energy and water by matching partners with high impact solar tracking and water filtration technologies.\n\nSunSaluters optimizes the power output of solar photovoltaic panels using calibrated weight displacement (usually transfer of water from one container into another) to slowly rotate a solar panel to face the sun as it moves across the sky, substantially increasing the panel’s power output. SunSaluter received a patent for the technology for integrating water filtration into the passive solar rotation design, although the design was open sourced in 2018, allowing anyone to build a SunSaluter.\n\nThe technology piloted by SunSaluter was invented by Eden Full Goh in 2008 and was initially under development by Full’s company, Roseicollis Technologies, based in Canada. In 2011, Full Goh received a Thiel Fellowship for US$100,000 to develop her passive solar tracker concept, before continuing its development while an engineering student at Princeton University.\n\nSunSaluter was incorporated in 2013, and operates with a volunteer Board of Directors and small staff. By 2013, SunSaluter had pilot projects in Kenya, Tanzania, and Uganda, and sales in Malawi, India, Indonesia, Canada, the U.S. and Mexico.\n\nSunSaluter’s passive tracking system ensures the solar panel faces the sun throughout the day, without the use of electricity, permitting the panel to produce up to 30 to 40 percent more electricity than a fixed-mount panel. The process is also known as horizontal (azimuth) solar tracking. The slow flow of water through a filter into another container acts as a water clock roughly synchronized with the sun’s motion. This also creates the opportunity to add a water filter onto the water clock, resulting in clean water at the end of each day. SunSaluter’s integrated rotating axis and water filtration system is patented, though SunSaluter has encouraged partnerships to promote widest possible distribution of the technology.\n\nIn 2018, SunSaluter announced that it would make the patented technology open-source, with the intent to \"allow SunSaluter to impact not just thousands, but millions of people\" and create opportunities for innovation on the design.\n\nWhile maintaining lean operations, SunSaluter's funding sources are diverse including prize competitions, corporate sponsorship, charitable gifts, and board fundraising. Notable awards include: \n\n\n"}
{"id": "7585431", "url": "https://en.wikipedia.org/wiki?curid=7585431", "title": "Thurgartstone", "text": "Thurgartstone\n\nThe Thurgartstone or Ogrestone is a famous stone near Dunlop in East Ayrshire in Scotland. The Thurgartstone is thought to have been a rocking stone at one time, but it no longer rocks. \nThe Thurgartstane is also known as the Ogrestane, the Thorgatstane, the Field Spirit Stane, T'Ogra Stane, Thugart Stane, Ogirtstane, Fiend's Stane, Ogart Stane, Horgar Stane or Thougritstane. In modern English, it is the Thurgartstone or Ogrestone.\nOne explanation of the name is that it derives from 'Thor's Great Stone'. Another is that the name comes from the phrase \"\"Thou Great Stone\" or just '\"grit stane\"\" (Paterson 1866). The term 'The Stone of the Ogre' may indicate some forgotten legend. Some names may be spelling errors originating in or propagated by the Ordnance Survey or other maps. Another theory is that the name is derived from the word \"Tagairtstane\", which means \"the priest's stone.\"\n\nYet another possibility is that 'Ogrestane' is more 'Ogre ton' and that the personal name 'Ogier' is a Normanised version of the Scandinavian name 'Holger'. Brandleside may be also be a Scandinavian name and suggest that 'Brandale' was once the name of this valley.\n\nThe Thurgartstone is close to Dunlop on the Lugton Road is Black Burn Valley. In the middle of a field near the Chapel Crags is the Thurgartstone. Dunlop or Boarland Hill, the site of Dunlop Castle once held by the De Ross family, can be seen from the stone. The stone is in a sheltered spot, with ample running water. It is somewhat hidden from view.\n\nThe Thurgartston is a \"glacial erratic stone\". It is composed of blue augitic porphyrite. This rock is different from the 'native' stone of the area. The Thurgartston weighs about 25 tons and the part above ground is about twelve feet by eight feet (Bayne, 1935). It is set near the site of the pre-Reformation St Mary's Chapel. The site is listed and protected by Historic Scotland.\n\nThere was a monastic settlement associated with the chapel of Saint Mary near the Thurgartstone site. The history of the site is apparently similar to the history of the Chapel Hill site at Chapeltoun.\n\nThere are steps cut into the rock that led up to the monk's cemetery. The monastery, original chapel and graveyard are no longer visible. There is a holy well in the \"field bordered by the burn.\" The new buildings called \"Marys Chapel\" at the site are built on the foundation of the original chapel. Chapelhouse Farm nearby was once called Brandlecraig.\n\nSome stone relics from St Mary's were given to a member of the Clement family and a christening font was discovered in the burn at Kirkwood Farm by another Clement. There artifacts are now kept at North Borland Farm.\n\nThe Thurgartstone has long been associated with pagan ritual practices. There are still May Day celebrations and events at the site. This 'Druidical' stone is thought by some to have been a 'rocking or logan' stone at one time. It is now firmly set in the 'rubbish' and dirt.\n\nThere are records showing that \"\"even as late as the time of Popery\", Catholics would do penance by crawling on their knees around this stone, crying \"O thou grit stane\"\". Apparently they held a belief that the Deity was present in the Thurgartstone (MacIntosh 1894).\n\nFarmers from Brandleside Farm did not remove the stone. They also kept their ploughs a set distance away from the Thurgartstone. This was presumably because legend has it that there were pagan burials around this monument (Paterson 1866). The ancient Dunlop Carlin stone is on the other side of the village.\n\n\n\n"}
{"id": "8983270", "url": "https://en.wikipedia.org/wiki?curid=8983270", "title": "Trimmer (electronics)", "text": "Trimmer (electronics)\n\nA trimmer is a miniature adjustable electrical component. It is meant to be set correctly when installed in some device, and never seen or adjusted by the device's user. Trimmers can be variable resistors (potentiometers), variable capacitors, or trimmable inductors. They are common in precision circuitry like A/V components, and may need to be adjusted when the equipment is serviced. Trimpots are often used to initially calibrate equipment after manufacturing. Unlike many other variable controls, trimmers are mounted directly on circuit boards, turned with a small screwdriver and rated for many fewer adjustments over their lifetime. Trimmers like trimmable inductors and trimmable capacitors are usually found in superhet radio and television receivers, in the intermediate frequency (IF), oscillator and radio frequency (RF) circuits. They are adjusted into the right position during the alignment procedure of the receiver.\n\nTrimmers come in a variety of sizes and levels of precision. For example, multi-turn trim potentiometers exist, in which it takes several turns of the adjustment screw to reach the end value. This allows for very high degrees of accuracy. Often they make use of a worm-gear (rotary track) or a leadscrew (linear track). \n\nIn 1952, Marlan Bourns patented the world's first trimming potentiometer, trademarked \"Trimpot\", a name now commonly used to refer to any trimming potentiometer.\n\n\n"}
{"id": "7394697", "url": "https://en.wikipedia.org/wiki?curid=7394697", "title": "Tropical cyclone windspeed climatology", "text": "Tropical cyclone windspeed climatology\n\nTropical cyclone windspeed climatology is the study wind distribution amongst tropical cyclones, a significant threat to land and people. Since records began in 1851, winds from hurricanes, typhoons and cyclones have been responsible for fatalities and damage in every basin. Major hurricanes (Category 3 or above) usually cause the most wind damage. Hurricane Andrew for example caused $45 billion (2005 USD) in damage, most of it wind damage.\n\nAlthough wind damage is common to tropical cyclones near landmasses, there are a few factors that lead to high wind speeds. Warm water temperatures, which was seen during Hurricane Wilma when it winds rapidly strengthened to 185 mph (295 km/h) in a 24-hour period due to the presence of abnormally warm water temperatures. Size and speed of the storm, which results in damage along a wide area as seen in Hurricane Isabel and Great New England Hurricane of 1938.\n\nThe winds in a tropical cyclone are the result of evaporation and condensation of moisture which results in updrafts. The updrafts in turn increase the height of the storm which causes more condensation.\n\nThe strongest winds in a northern hemisphere tropical cyclone is located in the eyewall and the right front quadrant of the tropical cyclone. Severe damage is usually the result when the eyewall of a hurricane, typhoon or cyclone passes over land. The right front quadrant is also an area of a tropical cyclone were the winds are strongest. The reason that the winds are at the front right side of a storm in the northern hemisphere (and the front left hand side in the Southern Hemisphere) is because of the motion of a tropical cyclone contributing to its rotation. A 100 mph hurricane while stationary, might have winds of 100 mph on its right side in the northern hemisphere while the rest of the storm might have winds at 70 mph. The location of the right ( or left in the Southern Hemisphere) front quadrant also depends on the storm track. For example, in the northern hemisphere, if the storm was moving west, then the right side is to the north, if it moving north, then the strongest winds will be to the east of the center.\nIn the southern hemisphere, the strongest winds are to the left of the eye. That is because cyclonic winds below the equator, spin clockwise.\n\nOn occasion, strong winds and wind gusts can occur in the rain bands of a tropical cyclone. And inside the eye the winds are relatively calm. At higher altitudes, the winds within a tropical cyclone head away from the center, forming the outflow which produces the shape of the tropical cyclone.\n\nThe United States uses the Saffir–Simpson hurricane wind scale, a 1–5 scale which categorize hurricanes by sustained wind speed. The hurricane scale was created in 1969 by Herbert Saffir and Bob Simpson. The National Hurricane Center classifies hurricanes category 3 or above, major hurricanes. The Joint Typhoon Warning Center also uses a scale similar to the Saffir-Simpson Hurricane Scale. However, the JTWC classified typhoons with winds of at 150 mph or higher \"Supertyphoons\".\n\nThe Bureau of Meteorology in Australia uses a 1–5 scale called tropical cyclone severity categories. Unlike the Saffir–Simpson Hurricane Scale, it categorizes tropical cyclones based on wind gusts.\n\nBefore the 1–5 scale was created in 1969 by the National Hurricane Center and later by the Bureau of Meteorology in Australia. Many tropical cyclones were simply ranked by the Beaufort Wind Scale which was created in the early 1800s by Francis Beaufort. The purpose of the scale was to standardize wind reports in ship logs. The scale was divided down into 14 forces (force 0 is calm while force 12 is hurricane).\n\nThis lists the fastest wind speed of a tropical cyclone per basin since 1950 (NOTE: All wind speeds are 1-minute sustained and are in mph).\n\n\nIn December 1998, Cyclone Thelma produced wind gusts of 108 mph (174 km/h) and sustained winds of 80 mph (139 km/h) as it made landfall in Western Australia. Three months later, Cyclone Vance produced a record gust of 166 mph (267 km/h). Sustained winds of 113 mph (182 km/h) in Onslow.\n\nCyclone John made landfall near Port Hedland, Australia as a category 4 tropical cyclone. Port Hedland experienced winds of 78 mph (124 km/h) for a period of 18 hours. At Karratha, the winds were gusting to 70 mph (113 km/h). Cape Lambert experienced sustained winds of 95 mph (150 km/h) for a five-hour period with gusts reaching 130 mph (210 km/h).\n\nIn March 2000, Tropical Storm Tessi made landfall near Townsville, Queensland,Australia. A weather station recorded the towns record wind gust of 81 mph (130 km/h).\n\nA fast moving Hurricane Hazel brought sustained winds of 77 mph (124 km/h) and wind gusts up to 90 mph (150 km/h). The remnants of Hurricane Audrey brought 80 mph winds across southern Canada. A weather station on Nova Scotia recorded 100 mph sustained winds when Hurricane Juan made landfall in 2003.\n\nDuring Typhoon Vera's impact on Japan, weather station in Iwo Jima reported winds of 89 mph (143 km/h).\n\nDuring Hurricane Wilma's passage through Florida a weather station recorded sustained windspeeds of 77 knots, gusting to 101 kts.\n"}
{"id": "10980741", "url": "https://en.wikipedia.org/wiki?curid=10980741", "title": "Van der Waals molecule", "text": "Van der Waals molecule\n\nA van der Waals molecule is a weakly bound complex of atoms or molecules held together by intermolecular attractions such as van der Waals forces or by hydrogen bonds. The name originated in the beginning of the 1970s when stable molecular clusters were regularly observed in molecular beam microwave spectroscopy.\n\nExamples of well-studied vdW molecules are Ar, H-Ar, HO-Ar, benzene-Ar, (HO), and (HF). Others include the largest diatomic molecule: He and LiHe.\n\nIn (supersonic) molecular beams temperatures are very low (usually less than 5 K). At these low temperatures van der Waals (vdW) molecules are stable and can be investigated by microwave, far-infrared spectroscopy and other modes of spectroscopy. Also in cold equilibrium gases vdW molecules are formed, albeit in small, temperature dependent, concentrations. Rotational and vibrational transitions in vdW molecules have indeed been observed in gases, mainly by UV and IR spectroscopy.\n\nVan der Waals molecules are usually very non-rigid and different versions are separated by low energy barriers, so that tunneling splittings, observable in far-infrared spectra, are relatively large. Thus, in the far-infrared one may observe intermolecular vibrations, rotations, and tunneling motions of van der Waals molecules. The VRT spectroscopic study of Van der Waals molecules is one of the most direct routes to the understanding of intermolecular forces.\n\n\n"}
{"id": "50319560", "url": "https://en.wikipedia.org/wiki?curid=50319560", "title": "Volatile fatty acids analysis", "text": "Volatile fatty acids analysis\n\nVolatile fatty acids (VFAs) are important elements in controlling the anaerobic digestion process. It has two important roles: decomposing organics and generating gasses, methane and carbon dioxide. When both decomposing and generating occur continuously and completely, oxygen demand decreases. Volatile fatty acids can be analyzed by titration, distillation, steam distillation, and chromatography. The acceptable level of volatile fatty acids in environmental waters is up to 50,000 ppm.\n\nTitration provides rough results. However, it requires less time compared to other methods. It is widely used for a wastewater treatment plant to track a status of a microorganism.\n\nThe distillation procedure provides rough results and 15-32% of the VFAs are lost during distillation. This procedure is also used in for wastewater treatment plants.\n\nSteam distillation could recover VFAs about 92-98%. It is more precise than previous two methods. However, it takes 4 hours to complete.\n\nChromatography gives the most precise and accurate results. It is capable of qualitatively and quantitatively analyzing each individual VFA.\n"}
{"id": "25869784", "url": "https://en.wikipedia.org/wiki?curid=25869784", "title": "West Midlands Councils", "text": "West Midlands Councils\n\nWest Midlands Councils was a regional grouping of the Local Government Association.\n\nIt was established as the Local Authority Leaders’ Board for the West Midlands region of England. It assumed the functions previously carried out by the West Midlands Regional Assembly when that body was abolished in March 2010. \n\nThe body was initially known as the West Midlands Leaders Board, changing its identity in July 2010, following abolition of regional spatial strategies by the new UK administration. \n\nThe secretariat for the West Midlands Leaders' Board has since been incorporated in-house by Wolverhampton City Council, rather than a separate body exist for such purposes. \n\n"}
{"id": "57210042", "url": "https://en.wikipedia.org/wiki?curid=57210042", "title": "Working fluids", "text": "Working fluids\n\nHeat engines, refrigeration cycles and heat pumps usually involve a fluid to and from which heat is transferred while undergoing a thermodynamic cycle. This fluid is called the working fluid. Refrigeration and heat pump technologies often refer to working fluids as refrigerants. Most thermodynamic cycles make use of the latent heat (adventages of phase change) of the working fluid. In case of other cycles the working fluid remains in gaseous phase while undergoing all the processes of the cycle. When it comes to heat engines, working fluid generally undergoes a combustion process as well, for example in internal combustion engines or gas turbines. There are also technologies in heat pump and refrigeration, where working fluid does not change phase, such as reverse Brayton or Stirling cycle.\n\nThis article summarises the main critera of selecting working fluids for a thermodynamic cycle, such as heat engines including low grade heat recovery using Organic Rankine Cycle (ORC) for geothermal energy, waste heat, thermal solar energy or biomass and heat pumps and refrigeration cycles. The article addresses how working fluids affect technological applications, where the working fluid undergoes a phase transition and does not remain in its original (mainly gaseous) phase during all the processes of the thermodynamic cycle.\n\nFinding the optimal working fluid for a given purpose – which is essential to achieve higher energy efficiency in the energy conversion systems – has great impact on the technology, namely it does not just influence operational variables of the cycle but also alters the layout and modifies the design of the equipment. Selection criteria of working fluids generally include thermodynamic and physical properties besides economical and environmental factors, but most often all of these criteria are used together.\n\nThe choice of working fluids is known to have a significant impact on the thermodynamic as well as economic performance of the cycle. A suitable fluid must exhibit favorable physical, chemical, environmental, safety and economic properties such as low specific volume (high density), viscosity, toxicity, flammability, ozone depletion potential (ODP), global warming potential (GWP) and cost, as well as favorable process characteristics such as high thermal and exergetic efficiency. These requirements apply both to pure (single-component) and mixed (multicomponent) working fluids. Existing research is largely focused on the selection of pure working fluids, with vast number of published reports currently available. An important restriction of pure working fluids is their constant temperature profile during phase change. Working fluid mixtures are more appealing than pure fluids because their evaporation temperature profile is variable, therefore follows the profile of the heat source better, as opposed to the flat (constant) evaporation profile of pure fluids. This enables an approximately stable temperature difference during evaporation in the heat exchanger, coined as temperature glide, which significantly reduces exergetic losses. Despite their usefulness, recent publications addressing the selection of mixed fluids are considerably fewer.<br>\nMany authors like for example O. Badr et al. have suggested the following thermodynamic and physical criteria which a working fluid should meet for heat engines like Rankine cycles. There are some differences in the critera concerning the working fluids used in heat engines and refrigeration cycles or heat pumps, which are listed below accordingly:\n\n\n\n\nTraditional and presently most widespread categorisation of pure working fluids was first used by H. Tabor et al. and O. Badr et al. dating back to the 60s. This three-class classification system sorts pure working fluids into three categories. The base of the classification is the \"shape of the saturation vapour curve\" of the fluid in temperature-entropy plane. If the slope of the saturation vapour curve in all states is negative (d\"s\"/d\"T\"<0), which means that with decreasing saturation temperature the value of entropy increases, the fluid is called wet. If the slope of the saturation vapour curve of the fluid is mainly positive (regardless of a short negative slope somewhat below the critical point), which means that with decreasing saturation temperature the value of entropy also decreases (d\"T\"/d\"s\">0), the fluid is dry. The third category is called isentropic, which means constant entropy and refers to those fluids that have a vertical saturation vapour curve (regardless of a short negative slope somewhat below the critical point) in temperature-entropy diagram. According to mathematical approach, it means a (negative) infinite slope (d\"s\"/d\"T\"=0). The terms of wet, dry and isentropic refer to the quality of vapour after the working fluid undergoes an isentropic (reversible adiabatic) expansion process from saturated vapour state. During an isentropic expansion process the working fluid always ends in the two-phase (also called wet) zone, if it is a wet-type fluid. If the fluid is of dry-type, the isentropic expansion necessarily ends in the superheated (also called dry) steam zone. If the working fluid is of isentropic-type, after an isentropic expansion process the fluid stays in saturated vapour state. The quality of vapour is a key factor in choosing steam turbine or expander for heat engines. See figure for better understanding.\n\nTraditional classification shows several theoretical and practical deficiencies. One of the most important is the fact that no perfectly isentropic fluid exists. Isentropic fluids have two extrema (d\"s\"/d\"T\"=0) on the saturation vapour curve. Practically, there are some fluids which are very close to this behaviour or at least in a certain temperature range, for example trichlorofluoromethane (CClF). Another problem is the extent of how dry or isentropic the fluid behaves, which has significant practical importance when designing for example an Organic Rankine Cycle layout and choosing proper expander.\nA new kind of classification was proposed by G. Györke et al. to resolve the problems and deficiencies of the traditional three-class classification system. The new classification is also based on the shape of the saturation vapour curve of the fluid in temperature-entropy diagram similarly to the traditional one. The classification uses a \"chacteristic-point based method\" to differentiate the fluids. The method defines three primary and two secondary characteristic points. The relative location of these points on the temperature-entropy saturation curve defines the categories. Every pure fluid has primary characteristic points A, C and Z:\n\nThe two secondary characteristic points, namely M and N are defined as local entropy extrema on the saturation vapour curve, more accurately, at those points, where with the decrease of the saturation temperature entropy stays constant: d\"s\"/d\"T\"=0. We can easily realise that considering traditional classification, wet-type fluids have only primary (A,C and Z), dry-type fluids have primary points and exactly one secondary point (M) and redefined isentropic-type fluids have both primary and secondary points (M and N) as well. See figure for better understanding.\n\nThe ascending order of entropy values of the characteristic points gives a useful tool to define categories. The mathematically possible number of orderings are 3! (if there are no secondary points), 4! (if only secondary point M exists) and 5! (if both secondary points exist), which makes it 150. There are some physical constraints including the existence of the secondary points decrease the number of possible categories to 8. The categories are to be named after the ascending order of the entropy of their characteristic points. Namely the possible 8 categories are ACZ, ACZM, AZCM, ANZCM, ANCZM, ANCMZ, ACNZM and ACNMZ. The categories (also called sequences) can be fitted into the traditional three-class classification, which makes the two classification system compatible. No working fluids have been found, which could be fitted into ACZM or ACNZM categories. Theoretical studies confirmed that these two categories may not even exist. Based on the database of NIST, the proved 6 sequences of the novel classification and their relation to the traditional one can be seen in the figure.\n\nAlthough multicomponent working fluids have significant thermodynamic advantages over pure (single-component) ones, research and application keep focusing on pure working fluids. However, there are some typical examples for multicomponent based technologies such as Kalina cycle which uses water and ammonia mixture, or absorption refrigerators which also use water and ammonia mixture besides water, ammonia and hydrogen, lithium bromide or lithium chloride mixtures in a majority. Some scientific papers deal with the application of multicomponent working fluids in Organic Rankine cycles as well. These are mainly binary mixtures of hydrocarbons, fluorocarbons, hydrofluorocarbons, siloxanes and inorganic substances.\n\n\n"}
