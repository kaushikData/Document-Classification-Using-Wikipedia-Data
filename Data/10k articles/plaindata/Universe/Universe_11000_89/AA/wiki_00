{"id": "31742530", "url": "https://en.wikipedia.org/wiki?curid=31742530", "title": "Advanced Energy Materials", "text": "Advanced Energy Materials\n\nAdvanced Energy Materials is a peer-reviewed scientific journal covering energy-related research, including photovoltaics, batteries, supercapacitors, fuel cells, hydrogen technologies, thermoelectrics, photocatalysis, solar power technologies, magnetic refrigeration, and piezoelectric materials. It publishes invited reviews and progress reports, full papers, and rapid communications. \nThe journal was established in 2011 and received its first immediacy index of 1.950 for 2011. In June 2013 it has received its first Impact Factor for 2012 from Journal Citation Reports.\n\nThe journal is abstracted and indexed in:\n"}
{"id": "29967297", "url": "https://en.wikipedia.org/wiki?curid=29967297", "title": "Aerolíneas Argentinas Flight 644", "text": "Aerolíneas Argentinas Flight 644\n\nAerolíneas Argentinas Flight 644 was a scheduled flight operated by the Douglas DC-6, registration LV-ADW, on 19 July 1961 which was due to operate a domestic scheduled passenger service between Ministro Pistarini International Airport and General Enrique Mosconi International Airport, but crashed west of Pardo, Buenos Aires, Argentina, half an hour after takeoff, owing to severe turbulence during climb out. This aircraft had been originally named 'Presidente Peron' but by 1956-57 had been renamed 'General San Martin'. Some reports stated the aircraft was struck by lightning.\n\nAccording to the investigation, the plane disintegrated en route after the rupture of one its wings following excessive loads in a zone of turbulence. Both the pilot and the company's flight dispatcher contributed to the disaster by misevaluating the weather forecast and choosing an inappropriate flight altitude. All 67 occupants of the aircraft – 7 crew and 60 passengers – were killed in the accident, which remains the deadliest one the company experienced all through its history.\n\nAs of 2018, Flight 644 remains the deadliest aviation disaster in Argentine history.\n\n\n"}
{"id": "36780732", "url": "https://en.wikipedia.org/wiki?curid=36780732", "title": "Australian Institute of Petroleum", "text": "Australian Institute of Petroleum\n\nThe Australian Institute of Petroleum (AIP) is a representative body for Australia's petroleum industry. Its headquarters a located in Canberra and it was established in 1976. The body is managed by a board composed of chief executives, senior representatives and an Executive Director.\n\nThe organisation aims to support the development of sustainable, internationally competitive petroleum products industry.\n\nThe four core members include BP Australia, Caltex Australia, Mobil Australia and Shell Company of Australia. Its members own and operate all oil refineries in Australia.\n\nThe AIP owns the Australian Marine Oil Spill Centre. It is a member of the Australian Industry Greenhouse Network. The AIP produces a weekly fuel report detailing average prices for transport fuels by location.\n"}
{"id": "36362117", "url": "https://en.wikipedia.org/wiki?curid=36362117", "title": "Bent Sørensen (physicist)", "text": "Bent Sørensen (physicist)\n\nBent Erik Sørensen (born 13 October 1941), is a Danish physicist, distinguished mainly by research into future forms of renewable energy. He is currently Professor Emeritus in the Department of Environmental, Social and Spatial Change in Roskilde University, Denmark, and president of Novator Advanced Technology Consulting.\n\nSørensen gained his MSc in physics and mathematics in 1965 and was awarded a PhD in 1974 by the Niels Bohr Institute in Copenhagen, where he worked until 1980, when he became professor at Roskilde University. He has undertaken sabbatical appointments in Japan, France, USA and Australia and has presented at numerous international events.\n\nHe received the Australian Government Award for Eminent European Scientists in 1982 and the European Solar Prize in 2002. In 1989 he was knighted by Queen Margrethe II of Denmark.\n\nSørensen has published over five hundred articles, reports and papers, and has written a number of books, including:\n\n"}
{"id": "28235244", "url": "https://en.wikipedia.org/wiki?curid=28235244", "title": "Bungaroosh", "text": "Bungaroosh\n\nBungaroosh (also spelt bungeroosh, bungarouche, bungarooge, bunglarooge, bunglarouge and other variations) is a composite building material used almost exclusively in the English seaside resort of Brighton and its attached neighbour Hove between the mid-18th and late 19th centuries, when it grew from a fishing village into a large town. Bungaroosh is often found in buildings of that era in the town and in its near neighbours Worthing and Lewes, but is little known elsewhere. In this respect, it is similar to mathematical tiles - another localised building material introduced in, and characteristic of, that era. It can incorporate any of a wide variety of substances and materials, and is used most often in external walls.\n\nThe manufacture of bungaroosh involved placing miscellaneous materials, such as whole or broken bricks, cobblestones, flints (commonly found on the South Downs around Brighton), small pebbles, sand and pieces of wood into hydraulic lime and then pouring it between shuttering until it has set. The shuttering (formwork) process typically involved erecting a wooden frame (often made out of railway sleepers after they became readily available in the 19th century), pouring in the lime and adding solid materials to the mixture. Other structural fittings, such as brick piers or wooden lintels, could then be added if more support was needed or other structures were to be added. This was particularly common in Brighton, where bungaroosh walls were often built behind the impressive stuccoed façades of Regency-style houses. The material is particularly prevalent in the early 19th-century squares, crescents and terraces of Brighton's seafront, such as Regency Square, Royal Crescent and the Kemp Town estate. Another technique was to wait for the mixture to set, then render it with a lime-based mixture and paint it. This produced a consistent, regular surface which could be used to build the symmetrical façades required in Georgian architecture - a popular style in Lewes.\n\nAlthough the material is solid once set, it has poor resistance to water. If it dries out completely, it can crumble away; but if it gets wet it can dissolve and start to move, causing structural failure. Regular drying-out and saturation caused by the effects of the weather has caused some bay-window fronts to collapse in Brighton. A common maxim states that much of Brighton \"could be demolished with a well-aimed hose\"; the supposed extent of this destruction varies between \"a third\" and \"half\" depending on the source.\n\nThe etymology of the first part of the word is the same as bungalow; late 17th century: from Hindi \"baṅglā\" ‘belonging to Bengal’. Second part is unknown, possibly a colloquialism.\n\n\n"}
{"id": "23809352", "url": "https://en.wikipedia.org/wiki?curid=23809352", "title": "Carbon fiber reinforced polymer", "text": "Carbon fiber reinforced polymer\n\nCarbon fiber reinforced polymer, carbon fiber reinforced plastic, or carbon fiber reinforced thermoplastic (CFRP, CRP, CFRTP, or often simply carbon fiber, carbon composite, or even carbon), is an extremely strong and light fiber-reinforced plastic which contains carbon fibers. The alternative spelling 'fibre' is common in British Commonwealth countries. CFRPs can be expensive to produce but are commonly used wherever high strength-to-weight ratio and stiffness (rigidity) are required, such as aerospace, superstructure of ships, automotive, civil engineering, sports equipment, and an increasing number of consumer and technical applications.\n\nThe binding polymer is often a thermoset resin such as epoxy, but other thermoset or thermoplastic polymers, such as polyester, vinyl ester, or nylon, are sometimes used. The composite material may contain aramid (e.g. Kevlar, Twaron), ultra-high-molecular-weight polyethylene (UHMWPE), aluminium, or glass fibers in addition to carbon fibers. The properties of the final CFRP product can also be affected by the type of additives introduced to the binding matrix (resin). The most common additive is silica, but other additives such as rubber and carbon nanotubes can be used. The material is also referred to as \"graphite-reinforced polymer\" or \"graphite fiber-reinforced polymer\" (\"GFRP\" is less common, as it clashes with glass-(fiber)-reinforced polymer).\n\nCFRPs are composite materials. In this case the composite consists of two parts: a matrix and a reinforcement. In CFRP the reinforcement is carbon fiber, which provides the strength. The matrix is usually a polymer resin, such as epoxy, to bind the reinforcements together. Because CFRP consists of two distinct elements, the material properties depend on these two elements.\n\nReinforcement gives CFRP its strength and rigidity; measured by stress and elastic modulus respectively. Unlike isotropic materials like steel and aluminum, CFRP has directional strength properties. The properties of CFRP depend on the layouts of the carbon fiber and the proportion of the carbon fibers relative to the polymer. The two different equations governing the net elastic modulus of composite materials using the properties of the carbon fibers and the polymer matrix can also be applied to carbon fiber reinforced plastics. The following equation,\n\nis valid for composite materials with the fibers oriented in the direction of the applied load. formula_2 is the total composite modulus, formula_3 and formula_4 are the volume fractions of the matrix and fiber respectively in the composite, and formula_5 and formula_6 are the elastic moduli of the matrix and fibers respectively. The other extreme case of the elastic modulus of the composite with the fibers oriented transverse to the applied load can be found using the following equation:\n\nThe fracture toughness of carbon fiber reinforced plastics is governed by the following mechanisms: 1) debonding between the carbon fiber and polymer matrix, 2) fiber pull-out, and 3) delamination between the CFRP sheets. Typical epoxy-based CFRPs exhibit virtually no plasticity, with less than 0.5% strain to failure. Although CFRPs with epoxy have high strength and elastic modulus, the brittle fracture mechanics present unique challenges to engineers in failure detection since failure occurs catastrophically. As such, recent efforts to toughen CFRPs include modifying the existing epoxy material and finding alternative polymer matrix. One such material with high promise is PEEK, which exhibits an order of magnitude greater toughness with similar elastic modulus and tensile strength. However, PEEK is much more difficult to process and more expensive.\n\nDespite its high initial strength-to-weight ratio, a design limitation of CFRP is its lack of a definable fatigue limit. This means, theoretically, that stress cycle failure cannot be ruled out. While steel and many other structural metals and alloys do have estimable fatigue or endurance limits, the complex failure modes of composites mean that the fatigue failure properties of CFRP are difficult to predict and design for. As a result, when using CFRP for critical cyclic-loading applications, engineers may need to design in considerable strength safety margins to provide suitable component reliability over its service life.\n\nEnvironmental effects such as temperature and humidity can have profound effects on the polymer-based composites, including most CFRPs. While CFRPs demonstrate excellent corrosion resistance, the effect of moisture at wide ranges of temperatures can lead to degradation of the mechanical properties of CFRPs, particularly at the matrix-fiber interface. While the carbon fibers themselves are not affected by the moisture diffusing into the material, the moisture plasticizes the polymer matrix. The epoxy matrix used for engine fan blades is designed to be impervious against jet fuel, lubrication, and rain water, and external paint on the composites parts is applied to minimize damage from ultraviolet light.\n\nThe carbon fibers can cause galvanic corrosion when CRP parts are attached to aluminum.\n\nThe primary element of CFRP is a carbon filament; this is produced from a precursor polymer such as polyacrylonitrile (PAN), rayon, or petroleum pitch. For synthetic polymers such as PAN or rayon, the precursor is first spun into filament yarns, using chemical and mechanical processes to initially align the polymer chains in a way to enhance the final physical properties of the completed carbon fiber. Precursor compositions and mechanical processes used during spinning filament yarns may vary among manufacturers. After drawing or spinning, the polymer filament yarns are then heated to drive off non-carbon atoms (carbonization), producing the final carbon fiber. The carbon fibers filament yarns may be further treated to improve handling qualities, then wound on to bobbins. From these fibers, a unidirectional sheet is created. These sheets are layered onto each other in a quasi-isotropic layup, e.g. 0°, +60°, or −60° relative to each other.\n\nFrom the elementary fiber, a bidirectional woven sheet can be created, i.e. a twill with a 2/2 weave. The process by which most CFRPs are made varies, depending on the piece being created, the finish (outside gloss) required, and how many of the piece will be produced. In addition, the choice of matrix can have a profound effect on the properties of the finished composite.\n\nMany CFRP parts are created with a single layer of carbon fabric that is backed with fiberglass. A tool called a chopper gun is used to quickly create these composite parts. Once a thin shell is created out of carbon fiber, the chopper gun cuts rolls of fiberglass into short lengths and sprays resin at the same time, so that the fiberglass and resin are mixed on the spot. The resin is either external mix, wherein the hardener and resin are sprayed separately, or internal mixed, which requires cleaning after every use.\nManufacturing methods may include the following:\n\nOne method of producing CFRP parts is by layering sheets of carbon fiber cloth into a mold in the shape of the final product. The alignment and weave of the cloth fibers is chosen to optimize the strength and stiffness properties of the resulting material. The mold is then filled with epoxy and is heated or air-cured. The resulting part is very corrosion-resistant, stiff, and strong for its weight. Parts used in less critical areas are manufactured by draping cloth over a mold, with epoxy either preimpregnated into the fibers (also known as \"pre-preg\") or \"painted\" over it. High-performance parts using single molds are often vacuum-bagged and/or autoclave-cured, because even small air bubbles in the material will reduce strength. An alternative to the autoclave method is to use internal pressure via inflatable air bladders or EPS foam inside the non-cured laid-up carbon fiber.\n\nFor simple pieces of which relatively few copies are needed (1–2 per day), a vacuum bag can be used. A fiberglass, carbon fiber, or aluminum mold is polished and waxed, and has a release agent applied before the fabric and resin are applied, and the vacuum is pulled and set aside to allow the piece to cure (harden). There are three ways to apply the resin to the fabric in a vacuum mold.\n\nThe first method is manual and called a wet layup, where the two-part resin is mixed and applied before being laid in the mold and placed in the bag. The other one is done by infusion, where the dry fabric and mold are placed inside the bag while the vacuum pulls the resin through a small tube into the bag, then through a tube with holes or something similar to evenly spread the resin throughout the fabric. Wire loom works perfectly for a tube that requires holes inside the bag. Both of these methods of applying resin require hand work to spread the resin evenly for a glossy finish with very small pin-holes.\n\nA third method of constructing composite materials is known as a dry layup. Here, the carbon fiber material is already impregnated with resin (pre-preg) and is applied to the mold in a similar fashion to adhesive film. The assembly is then placed in a vacuum to cure. The dry layup method has the least amount of resin waste and can achieve lighter constructions than wet layup. Also, because larger amounts of resin are more difficult to bleed out with wet layup methods, pre-preg parts generally have fewer pinholes. Pinhole elimination with minimal resin amounts generally require the use of autoclave pressures to purge the residual gases out.\n\nA quicker method uses a compression mold. This is a two-piece (male and female) mold usually made out of aluminum or steel that is pressed together with the fabric and resin between the two. The benefit is the speed of the entire process. Some car manufacturers, such as BMW, claimed to be able to cycle a new part every 80 seconds. However, this technique has a very high initial cost since the molds require CNC machining of very high precision.\n\nFor difficult or convoluted shapes, a filament winder can be used to make CFRP parts by winding filaments around a mandrel or a core.\n\nApplications for CFRP include the following:\n\nThe Airbus A350 XWB is built of 52% CFRP including wing spars and fuselage components, overtaking the Boeing 787 Dreamliner, for the aircraft with the highest weight ratio for CFRP, which was held at 50%. This was one of the first commercial aircraft to have wing spars made from composites. The Airbus A380 was one of the first commercial airliners to have a central wing-box made of CFRP; it is the first to have a smoothly contoured wing cross-section instead of the wings being partitioned span-wise into sections. This flowing, continuous cross section optimises aerodynamic efficiency. Moreover, the trailing edge, along with the rear bulkhead, empennage, and un-pressurised fuselage are made of CFRP. However, many delays have pushed order delivery dates back because of problems with the manufacture of these parts. Many aircraft that use CFRP have experienced delays with delivery dates due to the relatively new processes used to make CFRP components, whereas metallic structures have been studied and used on airframes for years, and the processes are relatively well understood. A recurrent problem is the monitoring of structural ageing, for which new methods are constantly investigated, due to the unusual multi-material and anisotropic nature of CFRP.\n\nIn 1968 a \"Hyfil\" carbon-fiber fan assembly was in service on the Rolls-Royce Conways of the Vickers VC10s operated by BOAC.\n\nSpecialist aircraft designers and manufacturers Scaled Composites have made extensive use of CFRP throughout their design range, including the first private manned spacecraft Spaceship One. CFRP is widely used in micro air vehicles (MAVs) because of its high strength to weight ratio.\n\nSpaceX is using carbon fiber for the entire primary structure of their new super heavy-lift launch vehicle, the ITS launch vehicle—as well as the two very large spacecraft that will be launched by it, the \"Interplanetary Spaceship\" and the \"ITS tanker\". This is a particular issue for the large liquid oxygen tank structure due to design challenges of such dense carbon/oxygen contact for long periods of time.\n\nCFRPs are extensively used in high-end automobile racing. The high cost of carbon fiber is mitigated by the material's unsurpassed strength-to-weight ratio, and low weight is essential for high-performance automobile racing. Race-car manufacturers have also developed methods to give carbon fiber pieces strength in a certain direction, making it strong in a load-bearing direction, but weak in directions where little or no load would be placed on the member. Conversely, manufacturers developed omnidirectional carbon fiber weaves that apply strength in all directions. This type of carbon fiber assembly is most widely used in the \"safety cell\" monocoque chassis assembly of high-performance race-cars.\n\nMany supercars over the past few decades have incorporated CFRP extensively in their manufacture, using it for their monocoque chassis as well as other components. As far back as 1971, the Citroën SM offered optional lightweight carbon fiber wheels.\n\nUse of the material has been more readily adopted by low-volume manufacturers who used it primarily for creating body-panels for some of their high-end cars due to its increased strength and decreased weight compared with the glass-reinforced polymer they used for the majority of their products.\n\nCFRP has become a notable material in structural engineering applications. Studied in an academic context as to its potential benefits in construction, it has also proved itself cost-effective in a number of field applications strengthening concrete, masonry, steel, cast iron, and timber structures. Its use in industry can be either for retrofitting to strengthen an existing structure or as an alternative reinforcing (or pre-stressing) material instead of steel from the outset of a project.\n\nRetrofitting has become the increasingly dominant use of the material in civil engineering, and applications include increasing the load capacity of old structures (such as bridges) that were designed to tolerate far lower service loads than they are experiencing today, seismic retrofitting, and repair of damaged structures. Retrofitting is popular in many instances as the cost of replacing the deficient structure can greatly exceed the cost of strengthening using CFRP.\n\nApplied to reinforced concrete structures for flexure, CFRP typically has a large impact on strength (doubling or more the strength of the section is not uncommon), but only a moderate increase in stiffness (perhaps a 10% increase). This is because the material used in this application is typically very strong (e.g., 3000 MPa ultimate tensile strength, more than 10 times mild steel) but not particularly stiff (150 to 250 GPa, a little less than steel, is typical). As a consequence, only small cross-sectional areas of the material are used. Small areas of very high strength but moderate stiffness material will significantly increase strength, but not stiffness.\n\nCFRP can also be applied to enhance shear strength of reinforced concrete by wrapping fabrics or fibers around the section to be strengthened. Wrapping around sections (such as bridge or building columns) can also enhance the ductility of the section, greatly increasing the resistance to collapse under earthquake loading. Such 'seismic retrofit' is the major application in earthquake-prone areas, since it is much more economic than alternative methods.\n\nIf a column is circular (or nearly so) an increase in axial capacity is also achieved by wrapping. In this application, the confinement of the CFRP wrap enhances the compressive strength of the concrete. However, although large increases are achieved in the ultimate collapse load, the concrete will crack at only slightly enhanced load, meaning that this application is only occasionally used. Specialist ultra-high modulus CFRP (with tensile modulus of 420 GPa or more) is one of the few practical methods of strengthening cast-iron beams. In typical use, it is bonded to the tensile flange of the section, both increasing the stiffness of the section and lowering the neutral axis, thus greatly reducing the maximum tensile stress in the cast iron.\n\nIn the United States, pre-stressed concrete cylinder pipes (PCCP) account for a vast majority of water transmission mains. Due to their large diameters, failures of PCCP are usually catastrophic and affect large populations. Approximately of PCCP have been installed between 1940 and 2006. Corrosion in the form of hydrogen embrittlement has been blamed for the gradual deterioration of the pre-stressing wires in many PCCP lines. Over the past decade, CFRPs have been utilized to internally line PCCP, resulting in a fully structural strengthening system. Inside a PCCP line, the CFRP liner acts as a barrier that controls the level of strain experienced by the steel cylinder in the host pipe. The composite liner enables the steel cylinder to perform within its elastic range, to ensure the pipeline's long-term performance is maintained. CFRP liner designs are based on strain compatibility between the liner and host pipe.\n\nCFRP is a more costly material than its counterparts in the construction industry, glass fiber-reinforced polymer (GFRP) and aramid fiber-reinforced polymer (AFRP), though CFRP is, in general, regarded as having superior properties. Much research continues to be done on using CFRP both for retrofitting and as an alternative to steel as a reinforcing or pre-stressing material. Cost remains an issue and long-term durability questions still remain. Some are concerned about the brittle nature of CFRP, in contrast to the ductility of steel. Though design codes have been drawn up by institutions such as the American Concrete Institute, there remains some hesitation among the engineering community about implementing these alternative materials. In part, this is due to a lack of standardization and the proprietary nature of the fiber and resin combinations on the market.\n\nCarbon fibers are used for fabrication of carbon-fiber microelectrodes. In this application typically a single carbon fiber with diameter of 5–7 μm is sealed in a glass capillary. At the tip the capillary is either sealed with epoxy and polished to make carbon-fiber disk microelectrode or the fiber is cut to a length of 75–150 μm to make carbon-fiber cylinder electrode. Carbon-fiber microelectrodes are used either in amperometry or fast-scan cyclic voltammetry for detection of biochemical signaling.\n\nCFRP is now widely used in sports equipment such as in squash, tennis, and badminton racquets, sport kite spars, high quality arrow shafts, hockey sticks, fishing rods, surfboards, high end swim fins, and rowing shells. Amputee athletes such as Jonnie Peacock use carbon fiber blades for running. It is used as a shank plate in some basketball sneakers to keep the foot stable, usually running the length of the shoe just above the sole and left exposed in some areas, usually in the arch.\n\nControversially, in 2006, cricket bats with a thin carbon-fiber layer on the back were introduced and used in competitive matches by high-profile players including Ricky Ponting and Michael Hussey. The carbon fiber was claimed merely to increase the durability of the bats but was banned from all first-class matches by the ICC in 2007.\n\nA CFRP bicycle frame weighs less than one of steel, aluminum, or titanium having the same strength. The type and orientation of the carbon-fiber weave can be designed to maximize stiffness in required directions. Frames can be tuned to address different riding styles: sprint events require stiffer frames while endurance events may require more flexible frames for rider comfort over longer periods. The variety of shapes it can be built into has further increased stiffness and also allowed aerodynamic tube sections. CFRP forks including suspension fork crowns and steerers, handlebars, seatposts, and crank arms are becoming more common on medium as well as higher-priced bicycles. CFRP rims remain expensive but their stability compared to aluminium reduces the need to re-true a wheel and the reduced mass reduces the moment of inertia of the wheel. CFRP spokes are rare and most carbon wheelsets retain traditional stainless steel spokes. CFRP also appears increasingly in other components such as derailleur parts, brake and shifter levers and bodies, cassette sprocket carriers, suspension linkages, disc brake rotors, pedals, shoe soles, and saddle rails. Although strong and light, impact, over-torquing, or improper installation of CFRP components has resulted in cracking and failures, which may be difficult or impossible to repair.\n\nThe fire resistance of polymers and thermo-set composites is significantly improved if a thin layer of carbon fibers is moulded near the surface because a dense, compact layer of carbon fibers efficiently reflects heat.\n\nCFRP is also finding application in an increasing number of high-end products that require stiffness and low weight, these include:\n\nCFRPs have a long service lifetime when protected from the sun. When it is time to decommission CFRPs, they cannot be melted down in air like many metals. When free of vinyl (PVC or polyvinyl chloride) and other halogenated polymers, CFRPs can be thermally decomposed via thermal depolymerization in an oxygen-free environment. This can be accomplished in a refinery in a one-step process. Capture and reuse of the carbon and monomers is then possible. CFRPs can also be milled or shredded at low temperature to reclaim the carbon fiber; however, this process shortens the fibers dramatically. Just as with downcycled paper, the shortened fibers cause the recycled material to be weaker than the original material. There are still many industrial applications that do not need the strength of full-length carbon fiber reinforcement. For example, chopped reclaimed carbon fiber can be used in consumer electronics, such as laptops. It provides excellent reinforcement of the polymers used even if it lacks the strength-to-weight ratio of an aerospace component.\n\nIn 2009, Zyvex Technologies introduced carbon nanotube-reinforced epoxy and carbon pre-pregs. Carbon nanotube reinforced polymer (CNRP) is several times stronger and tougher than CFRP and is used in the Lockheed Martin F-35 Lightning II as a structural material for aircraft. CNRP still uses carbon fiber as the primary reinforcement, but the binding matrix is a carbon nanotube filled epoxy.\n\n\n"}
{"id": "3501420", "url": "https://en.wikipedia.org/wiki?curid=3501420", "title": "Castor wax", "text": "Castor wax\n\nCastor wax, also called hydrogenated castor oil, is an opaque, white vegetable wax. It is produced by the hydrogenation of pure castor oil often in the presence of a nickel catalyst to increase the rate of reaction. The hydrogenation of castor oil forms saturated molecules of castor wax; this saturation is responsible for the hard, brittle and insoluble nature of the wax.\n\nCastor wax is used in polishes, cosmetics, electrical capacitors, carbon paper, lubrication and coatings and greases where resistance to moisture, oils and petrochemical products is required. Castor wax is also useful in polyurethane coating formulation, as it contains three secondary hydroxyl groups. These coating compositions are useful as a top coat varnish for leather, wood and rubber.\nCastor wax can also be added to beeswax for encaustic painting.\n\n"}
{"id": "54204942", "url": "https://en.wikipedia.org/wiki?curid=54204942", "title": "Cestas Solar Park", "text": "Cestas Solar Park\n\nThe Cestas Solar Park is a 300 megawatt (MW) photovoltaic power station in Cestas, France. Built by Neoen, it opened on December 1, 2015.\n\n"}
{"id": "2193362", "url": "https://en.wikipedia.org/wiki?curid=2193362", "title": "Chromophore", "text": "Chromophore\n\nA chromophore is the part of a molecule responsible for its color.\nThe color that is seen by our eyes is the one not absorbed within a certain wavelength spectrum of visible light. The chromophore is a region in the molecule where the energy difference between two separate molecular orbitals falls within the range of the visible spectrum. Visible light that hits the chromophore can thus be absorbed by exciting an electron from its ground state into an excited state.\n\nIn biological molecules that serve to capture or detect light energy, the chromophore is the moiety that causes a conformational change of the molecule when hit by light.\n\nIn the conjugated chromophores, the electrons jump between energy levels that are extended pi orbitals, created by a series of alternating single and double bonds, often in aromatic systems. Common examples include retinal (used in the eye to detect light), various food colorings, fabric dyes (azo compounds), pH indicators, lycopene, β-carotene, and anthocyanins. Various factors in a chromophore's structure go into determining at what wavelength region in a spectrum the chromophore will absorb. Lengthening or extending a conjugated system with more unsaturated (multiple) bonds in a molecule will tend to shift absorption to longer wavelengths. Woodward-Fieser rules can be used to approximate ultraviolet-visible maximum absorption wavelength in organic compounds with conjugated pi-bond systems.\n\nSome of these are metal complex chromophores, which contain a metal in a coordination complex with ligands. Examples are chlorophyll, which is used by plants for photosynthesis and hemoglobin, the oxygen transporter in the blood of vertebrate anirnals. In these two examples, a metal is complexed at the center of a tetrapyrrole macrocycle ring: the metal being iron in the heme group (iron in a porphyrin ring) of hemoglobin, or magnesium complexed in a chlorin-type ring in the case of chlorophyll. The highly conjugated pi-bonding system of the macrocycle ring absorbs visible light. The nature of the central metal can also influence the absorption spectrum of the metal-macrocycle complex or properties such as excited state lifetime. The tetrapyrrole moiety in organic compounds which is not macrocyclic but still has a conjugated pi-bond system still acts as a chromophore. Examples of such compounds include bilirubin and urobilin, which exhibit a yellow color.\n\nAn auxochrome is a functional group of atoms attached to the chromophore which modifies the ability of the chromophore to absorb light, altering the wavelength or intensity of the absorption.\n\nHalochromism occurs when a substance changes color as the pH changes. This is a property of pH indicators, whose molecular structure changes upon certain changes in the surrounding pH. This change in structure affects a chromophore in the pH indicator molecule. For example, phenolphthalein is a pH indicator whose structure changes as pH changes as shown in the following table:\n\nIn a pH range of about 0-8, the molecule has three aromatic rings all bonded to a tetrahedral sp hybridized carbon atom in the middle which does not make the π-bonding in the aromatic rings conjugate. Because of their limited extent, the aromatic rings only absorb light in the ultraviolet region, and so the compound appears colorless in the 0-8 pH range. However, as the pH increases beyond 8.2, that central carbon becomes part of a double bond becoming sp hybridized and leaving a p orbital to overlap with the π-bonding in the rings. This makes the three rings conjugate together to form an extended chromophore absorbing longer wavelength visible light to show a fuchsia color. At pH ranges outside 0-12, other molecular structure changes result in other color changes; see Phenolphthalein for details.\n\n"}
{"id": "14022420", "url": "https://en.wikipedia.org/wiki?curid=14022420", "title": "Conifers of Mexico", "text": "Conifers of Mexico\n\nMexican conifers extend mainly across the main mountain ranges Sierra Madre Oriental, Sierra Madre Occidental and the Trans-Mexican Volcanic Belt. Enclosed between these mountains there are dispersed groups of conifers in mid and high elevations valleys when rainfall conditions allow their growth. Mexican conifers grow in some places often associated with oaks.\n\nThe dry season is about 7–8 months. Most rainfall (80-95%) occurs from June to October (5 months) and for only four months in the north. Precipitation throughout the year is present only in a little portion of the Sierra Madre Oriental in the State of Veracruz. The greatest number of species belong to the \"Pinus\" genus, including about 60 species and subspecies.\n\nMexican conifers growing in subtropical climates include Pinus chiapensis, Pinus oocarpa, and pinus tecunumanii.\n\nMexican conifers in temperate-to-cool climates are as follow:\n\n\nMexican conifers that grow in exclusively in cool climates are Pinus hartwegii and Pinus rudis.\n\nSome of the trees are harvested locally for their wood, which is used mainly in boards, construction, rustic furniture and in paper production. Resin is extracted from some species. Some trees are esteemed for their seeds: Pinus nelsonii, Pinus maximartinezii, Pinus cembroides and Pinus orizabenzis.\n\nMexican conifers have been recently started to be introduced in different parts around the world, and their plantations are considered very important. The leading reason for that expansion is the good quality that their wood has for paper production, but they are also planted as ornamental due to their unique characteristics and as exotic plants. The main genera used with this purposes are pines, cypresses and firs and taxodiums.\n\nIn their natural environment, they grow from 30° to 14° North Latitude, at altitudes between 600 to 4300 meters above sea level. There are some advantages for their introduction in several places in different altitudes, latitudes and climates: The average rainfall in regions where they occur fluctuates between 380 to 2000 millimeters a year. Above 2400 meters' elevation, snowfall is usual. Moist conditions vary from a short season of rainfall in summer to a climate with rainfall throughout the year, and can develop from semiarid to very wet conditions. In general they can withstand dry summers and some of are drought tolerant in several degrees. They grow from subtropical to cool climates; the average temperature varies from 24 °C to 10 °C. Some are surprisingly hardy (Pinus hartweggi, Pinus rudis, Pinus ayacahuite, etc.), tolerating temperatures as low as -30 °C. These trees occur on very tilted slopes and plain valleys.\n\nMexican conifers from temperate and cool climates which are being cultivated in other countries for paper industry according to altitudinal, latitudinal and climatic characteristics:\n\n\"Near or on the Equator at high altitudes\":\n\nPinus leiophylla in Kenya. Pinus montezumae in Kenya. Pinus ayacahuite in Kenya, Tanzania, Cupressus lusitanica in Colombia where it is used for creating windbreaks curtains in mountain slopes and fighting against soil erosion. Pinus greggi in Colombia. Pinus patula in Ecuador (3500 m), Colombia (3300 m), Kenya, Tanzania, Papua New Guinea.\n\n\"In tropical latitudes at high altitudes\":\n\nPinus leiophylla in Malawi, Zimbabwe, Zambia. Pinus montezumae in Malawi, Botswana and Zimbabwe. Pinus ayacahuite in Angola. Pinus greggi in Zimbabwe and Bolivia. Pinus patula in Bolivia, Zimbabwe, Angola, Hawaii. Cupressus lusitanica in Bolivia\n\n\"In Subtropical latitudes at mid and high altitudes\":\n\nPinus leiophylla in South Africa and Queensland, Australia. Pinus montezumae in South Africa and Queensland, Australia. Pinus cembroides in South Africa (produces edible nuts)\nPinus ayacahuite in Southern Brazil and Northern India, Northern Argentina in Salta and Tucumán Provinces. Cupressus lusitanica in South Africa. Pinus greggi in South Africa and Southern Brazil. Pinus patula in South Africa, Northern India and Southern Brazil\n\nMexican conifers from subtropical climates that are being planted in tropical and subtropical latitudes at low and mid altitudes for paper industry:\n\nPinus oocarpa in Ecuador, Kenya, Zambia, Colombia, Bolivia, Brazil, South Africa and Queensland, Australia. Pinus chiapensis in Colombia, Brazil, Queensland, Australia and South Africa. Pinus tecunumanni Colombia, Brazil, Queensland, Australia and South Africa.\n\nMexican conifers from temperate and cool climates and from mid and high altitudes that are being cultivated near sea level for paper industry:\n\nNew Zealand has been a pioneer country in cultivating massively trees from high altitudes and tropical latitudes and it has proved that Mexican conifers can thrive in big extensions near sea level when temperature and rainfall conditions are favorable. Most of this species are fully naturalized. Mexican conifers planted in New Zealand are:\n\nCupressus lusitanica, Pinus ayacahuite, Pinus devoniana, Pinus patula, Pinus pseudostrobus,\nPinus montezumae\n\nMexican conifers that have been planted with forestation purposes at temperate latitudes at mid and low altitudes:\n\nPinus patula and cupressus lusitanica in the Argentine provinces of Córdoba and San Luis at lower altitudes than its origin site but with a similar temperate climate with rainfalls in summer.\n\nMexican conifers from mid and high altitudes that have been planted as ornamental at temperate latitudes near sea level:\n\nCupressus lusitanica in the United Kingdom; Austin, Texas; Buenos Aires, Argentina and Northern Portugal, Pinus ayacahuite in the United Kingdom, Pinus devoniana in France, Pinus hartweggi in the United Kingdom, Pinus montezumae in southern New South Wales, Australia\n\n"}
{"id": "21386566", "url": "https://en.wikipedia.org/wiki?curid=21386566", "title": "Everyday Life in a Syrian Village", "text": "Everyday Life in a Syrian Village\n\nEveryday Life in a Syrian Village () is a Syrian documentary film by the director Omar Amiralay. The film was ranked #55 on the Dubai International Film Festival's 2013 list of the top 100 Arab films.\n\n"}
{"id": "1165668", "url": "https://en.wikipedia.org/wiki?curid=1165668", "title": "Ewald's sphere", "text": "Ewald's sphere\n\nThe Ewald sphere is a geometric construction used in electron, neutron, and X-ray crystallography which demonstrates the relationship between: \n\nIt was conceived by Paul Peter Ewald, a German physicist and crystallographer. Ewald himself spoke of the sphere of reflection.\n\nEwald's sphere can be used to find the maximum resolution available for a given x-ray wavelength and the unit cell dimensions. It is often simplified to the two-dimensional \"Ewald's circle\" model or may be referred to as the Ewald sphere.\n\nA crystal can be described as a lattice of points of equal symmetry. The requirement for constructive interference in a diffraction experiment means that in momentum or reciprocal space the values of momentum transfer where constructive interference occurs also form a lattice (the reciprocal lattice). For example, the reciprocal lattice of a simple cubic real-space lattice is also a simple cubic structure. Another example, the reciprocal lattice of an FCC crystal real-space lattice is a BCC structure, and vice versa. The aim of the Ewald sphere is to determine which lattice planes (represented by the grid points on the reciprocal lattice) will result in a diffracted signal for a given wavelength, formula_1, of incident radiation. \n\nThe incident plane wave falling on the crystal has a wave vector formula_2 whose length is formula_3. The diffracted plane wave has a wave vector formula_4. If no energy is gained or lost in the diffraction process (it is elastic) then formula_4 has the same length as formula_2. The difference between the wave-vectors of diffracted and incident wave is defined as scattering vector formula_7. Since formula_2 and formula_4 have the same length the scattering vector must lie on the surface of a sphere of radius formula_3. This sphere is called the Ewald sphere.\n\nThe reciprocal lattice points are the values of momentum transfer where the Bragg diffraction condition is satisfied and for diffraction to occur the scattering vector must be equal to a reciprocal lattice vector. Geometrically this means that if the origin of reciprocal space is placed at the tip of formula_2 then diffraction will occur only for reciprocal lattice points that lie on the surface of the Ewald sphere.\n\nWhen the wavelength of the radiation to be scattered is much smaller than the spacing between atoms, the Ewald sphere radius becomes large compared to the spatial frequency of atomic planes. This is common, for example, in transmission electron microscopy. In this approximation, diffraction patterns in effect illuminate planar slices through the origin of a crystal's reciprocal lattice. However, it is important to note that while the Ewald sphere may be quite flat, a diffraction pattern taken perfectly aligned down a zone axis (high-symmetry direction) contains precisely zero spots that exactly satisfy the Bragg condition. As one tilts a single crystal with respect to the incident beam, diffraction spots wink on and off as the Ewald sphere cuts through one zero order Laue zone (ZOLZ) after another.\n\n"}
{"id": "2337070", "url": "https://en.wikipedia.org/wiki?curid=2337070", "title": "Freighting", "text": "Freighting\n\nFreighting refers to the hauling of cargo, historically, using a dog team to mush goods cross-country. During the North American gold rushes, such as the Klondike Gold Rush, dogs were valuable draft animals, going where horses could not and withstanding harsher weather.\n\nDog teams were commonly used for transportation and in Canada and Alaska were used to deliver the mail during the winter. Large teams of sturdy dogs were required to haul the heavy loads. With the advent of air mail in the 1920s, mail delivery by dog team became outmoded, and gradually the mail runs became fewer until the last regular mail freighting route in Canada was shut down in the 1960s and in 1963 in the United States.\n\nThe dog sport of weight pulling is a modern adaptation of freighting.\n"}
{"id": "44980687", "url": "https://en.wikipedia.org/wiki?curid=44980687", "title": "Grande Seca", "text": "Grande Seca\n\nThe Grande Seca, the Great Drought, or the Brazilian drought of 1877–78 is the largest and most devastating drought in Brazilian history. It caused the deaths of between 400,000 and 500,000 people. Of the 800,000 people who lived in the affected Northeastern region, around 120,000 migrated to the Amazon while 68,000 migrated to other parts of Brazil.\n\n\n\n"}
{"id": "47106434", "url": "https://en.wikipedia.org/wiki?curid=47106434", "title": "Gryta Hydroelectric Power Station", "text": "Gryta Hydroelectric Power Station\n\nThe Gryta Hydroelectric Power Station () is a hydroelectric power station in the municipality of Rindal in Møre og Romsdal county, Norway. It is a run-of-river hydro power station utilizing a drop of in a tributary of the Surna River. Permission was granted for construction in 2006 and the plant came into operation in 2009. It is operated by Gryta Kraft AS. It operates at an installed capacity of , with an average annual production of about 4.5 GWh.\n"}
{"id": "97726", "url": "https://en.wikipedia.org/wiki?curid=97726", "title": "Haematoxylin", "text": "Haematoxylin\n\nHaematoxylin or hematoxylin (), also called natural black 1 or C.I. 75290, is a compound extracted from the heartwood of the logwood tree (\"Haematoxylum campechianum\"). Haematoxylin and eosin together make up haematoxylin and eosin stain, one of the most commonly used stains in histology. This type of stain is a permanent stain as opposed to temporary stains (e.g. iodine solution in KI). Another common stain is phosphotungstic acid haematoxylin, a mix of haematoxylin with phosphotungstic acid. When oxidized, it forms haematein, a compound that forms strongly coloured complexes with certain metal ions, the most notable ones being Fe(III) and Al(III) salts. Metal-haematein complexes are used to stain cell nuclei prior to examination under a microscope. Structures that stain with iron- or aluminium-haematein are often called basophilic, even though the mechanism of the staining is different from that of staining with basic dyes.\n\nIn the early 1970s and in 2008, there were shortages of haematoxylin due to interruptions in its extraction from logwood. The price of the compound increased, affecting the cost of diagnostic histopathology, and prompted a search for alternative nuclear stains. Before the use of any alternatives became firmly established, haematoxylin returned to the market, though at a higher price, and resumed its place in histopathology. Several synthetic dyes have been recommended as replacements, notably celestine blue (CI 51050), gallocyanine (CI 51030), gallein (CI 45445) and eriochrome cyanine R (also called chromoxane cyanine R and solochrome cyanine (CI 43820). All four have Fe(III) as the mordant. An alternative is the aluminium complex of oxidized brazilin, which differs from haematoxylin by only one hydroxyl group.\n\nThese stains are commonly employed for histological studies. The mordants used to demonstrate nuclear and cytoplasmic structures are alum and iron, forming lakes or coloured complexes (dye-mordant-tissue complexes), the colour of which will depend on the salt used. Aluminium salt lakes are usually coloured blue-white, whereas ferric salt lakes are coloured blue-black.\n\nThe three main alum haematoxylin solutions employed are Ehrlich's haematoxylin, Harris's haematoxylin, and Mayer's haematoxylin. The name \"haemalum\" is preferable to \"haematoxylin\" for these solutions because haematein, a product of oxidation of haematoxylin, is the compound that combines with aluminium ions to form the active dye-metal complex. Alum haematoxylin solutions impart to the nuclei of cells a light transparent red stain that rapidly turns blue on exposure to any neutral or alkaline liquid.\n\nAlum or potassium aluminium sulfate used as the mordant usually dissociates in an alkaline solution, combining with OH of water to form insoluble aluminium hydroxide. In the presence of excess acid, aluminium hydroxide cannot be formed, thus causing failure of aluminium haematoxylin dye-lake to form, due to lack of OH ions. Hence, acid solutions of alum haematoxylin become red. During staining, alum haematoxylin-stained sections are usually passed on to a neutral or alkaline solution (e.g., hard tap water or 1% ammonium hydroxide) in order to neutralize the acid and form an insoluble blue aluminium haematin complex. This procedure is known as \"blueing\". \n\nWhen tap water is not sufficiently alkaline, or is even acidic and is unsatisfactory for blueing haematoxylin, a tap water substitute consisting of 3.5 g NaHCO and 20 g MgSO·7HO in one litre of water with thymol (to inhibit formation of moulds), is used to accelerate blueing of thin paraffin sections. Addition of a trace of any alkali to tap or distilled water also provides an effective blueing solution; a few drops of strong ammonium hydroxide or of saturated aqueous lithium carbonate, added immediately before use, are sufficient for a 400 ml staining dish full of water. Use of very cold water slows down the blueing process, whereas warming accelerates it. In fact, the use of water below 10 °C for blueing sections may even produce pink artifact discolorations in the tissue.\n\n\n\n"}
{"id": "530317", "url": "https://en.wikipedia.org/wiki?curid=530317", "title": "Horse-drawn boat", "text": "Horse-drawn boat\n\nA horse-drawn boat or tow-boat is a historic boat operating on a canal, pulled by a horse walking beside the canal on a towpath.\n\nThe Romans are known to have used mules to haul boats on their waterways in the UK. Boat horses were the prime movers of the Industrial Revolution, and they remained at work until the middle of the 20th century. A horse, towing a boat with a rope from the towpath, could pull fifty times as much cargo as it could pull in a cart or wagon on roads. In the early days of the Canal Age, from about 1740, all boats and barges were towed by horse, mule, hinny, pony or sometimes a pair of donkeys. Many of the surviving buildings and structures had been designed with horse power in mind. Horse-drawn boats were used well into the 1960s on UK canals for commercial transport, and are still used today by passenger trip boats and other pleasure traffic.\n\nThe Horseboating Society has the primary aims of preserving and promoting Horseboating on the canals of the United Kingdom. There are horseboat operators at Foxton, Godalming, Tiverton, Ashton-under-Lyne, Newbury and Llangollen.\n\n\"Maria\" is Britain's oldest surviving wooden narrowboat, built in 1854 by Jinks Boatyard in Marple, and was never converted to have an engine. From 1854 to 1897, \"Maria\" was used to carry railway track ballast for the Manchester, Sheffield and Lincolnshire Railway. She was then used as a maintenance boat until 1962, lay abandoned for nine years until being salvaged in 1972 and converted to a passenger boat in 1978. In 2000 she was restored to near original operating condition.\n\n\"Maria\" is currently owned by Ashton Packet Boat Company. She is sometimes loaned to the Horseboating Society and has taken part in several of their events, including British Waterways' \"Coal and Cotton\" event, celebrating the Leeds and Liverpool Canal's history of transporting coal from Leeds and Wigan to Liverpool, and taking cotton from Liverpool docks to Leeds.\n\nIn 2006 she was the first boat to have been legged through Standedge Tunnel in 60 years. A UK Government minister and a local Member of Parliament took turns at legging \"Maria\" through the highest, longest, and deepest canal tunnel in the UK.\n\n\n"}
{"id": "15168040", "url": "https://en.wikipedia.org/wiki?curid=15168040", "title": "Iiro Aalto", "text": "Iiro Aalto\n\nIiro Aalto (born 19 July 1977) is a Finnish former football player. He started his career with Finnish football club Pallo-Iirot, then moved to FC Haka and spent most of his career at Helsingin Jalkapalloklubi but from December 2007 played for Cypriot club Olympiakos Nicosia.\nAalto returned to Finland in May 2008 to play for Tampere United. After his loan spell at Tampere he moved to TPS Turku.\n\n\n"}
{"id": "2282639", "url": "https://en.wikipedia.org/wiki?curid=2282639", "title": "Inborn error of lipid metabolism", "text": "Inborn error of lipid metabolism\n\nNumerous genetic disorders are caused by errors in fatty acid metabolism. These disorders may be described as fatty oxidation disorders or as a \"lipid storage disorders\", and are any one of several inborn errors of metabolism that result from enzyme defects affecting the ability of the body to oxidize fatty acids in order to produce energy within muscles, liver, and other cell types.\n\nSome of the more common fatty acid metabolism disorders are:\n\n\n\n\n\n\n"}
{"id": "5347327", "url": "https://en.wikipedia.org/wiki?curid=5347327", "title": "Instituto Oncológico Nacional", "text": "Instituto Oncológico Nacional\n\nThe National Oncologic Institute or ION () is a specialized hospital for cancer treatment, located in Panama City, Panama. Between August 2000 and March 2001, patients receiving radiation treatment for prostate cancer and cancer of the cervix received lethal doses of radiation, resulting in 8 fatalities.\n\nOn 1936, President Juan Demóstenes Arosemena (a physician), conceived the creation of the National Radiologic Institute, an institution dedicated to treat cancer. The treatments were given on the Santo Tomas Hospital and on the former Panama Hospital.\n\nOn September 18, 1940, during the administration of President Augusto Boyd, the new facilities of the National Radiologic Institute were inaugurated, giving it its own building. The Institute was part of the Santo Tomas Hospital. This institute had 4 doctors, 3 nurses, and 40 beds. The treatments they had were radiotherapy, implantation of Radium needles, injections of hydrogenated mustard and surgery.\n\nThe institute had a passive handling of cancer, because the treatment of cancer given at the time was to ease pain in patients.\n\nOn 1965, a more active role in the battle against cancer is started, when the latest advancements of the time are applied in the detection and treatment of this illness.\n\nLater that year the National Radiologic Institute was renamed Juan Demóstenes Arosemena Cancerologic Center, as a recognition of the work of this physician and creator of the institution. A cobalt-60 pump was acquired.\n\nOn 1980, the institution begins relations with the government of Japan, that was interested on the treatment of cancer in Panama, and by which a donation of medical and surgical equipment, including ultrasound, X-rays, and others are acquired.\n\nOn 1984, by law 11, the National Oncologic Institute (Instituto Oncologico Nacional) Juan Demóstenes Arosemena is created.\n\nOn June 3, 1999, the Panamanian Government, on President Ernesto Perez Balladares administration, gives buildings 242 and 254 of the former Gorgas Hospital to the Institute, and on July 23 the Institute moves to this location from the building on Justo Arosemena Avenue.\n\nThe Hospital has continued its growth and acquired new equipment, like a linear accelerator, a new CT and opened its ICU.\n\nAs in most radiotherapy departments, the one at ION uses a treatment planning system (TPS) to\ncalculate the resulting dose distributions and determine treatment times. The data for each shielding block should be entered into the TPS separately. The TPS allows a maximum of four shielding blocks per field to be taken into account when calculating treatment times and dose distributions. Shielding blocks are used to protect healthy tissue of patients undergoing radiotherapy at the Institute, as is the normal practice.\n\nIn order to satisfy the request of a radiation oncologist to include five blocks in the field, in August 2000 the method of digitizing shielding blocks was changed. It was found that it was possible to enter data into the TPS for multiple shielding blocks together as if they were a single block, thereby apparently overcoming the limitation of four blocks per field. As was found later, although the TPS accepted entry of the data for multiple shielding blocks as if they were a single block, at least one of the ways in which the data were entered the computer output indicated a treatment time substantially longer than it should have been. The result was that patients received a proportionately higher dose than that prescribed. The modified treatment protocol was used for 28 patients, who were treated between August 2000 and March 2001 for prostate cancer and cancer of the cervix. There were 8 deaths and 20 injuries.\n\nThe modified protocol was used without a verification test, i.e. a manual calculation of the treatment time for comparison with the computer calculated treatment time, or a simulation of treatment by irradiating a water phantom and measuring the dose delivered. In spite of the treatment times being about twice those required for correct treatment, the error went unnoticed. Some early symptoms of excessive exposure were noted in some of the irradiated patients. The seriousness, however, was not realized, with the consequence that the accidental exposure went unnoticed for a number of months. The continued emergence of these symptoms, however, eventually led to the accidental exposure being detected in March 2001.\n\nIn May 2001, the Government of Panama requested assistance under the terms of the Convention on Assistance in the Case of a Nuclear Accident or Radiological Emergency. In its response, the International Atomic Energy Agency sent a team of five medical doctors and two physicists to Panama to perform a dosimetric and medical assessment of the accidental exposure and a medical evaluation of the affected patients’ prognosis and treatment. The team was complemented by a physicist from the Pan American Health Organization (PAHO), also at the request of the Government of Panama.\n\nThe accidental exposures at the ION in Panama were very serious. Many patients have suffered severe radiation effects due to excessive dose. Both morbidity and mortality have increased significantly. \n\nThe IAEA report was consistent with the report made by local investigators. It was found that the radiotherapy equipment was properly calibrated and worked properly. The error was on the data entry, using a protocol not validated to enter more shielding blocks, that resulted in increased dose in the treatment. Most of the exposed patients have died, some radiation related, others by means of their advanced cancer. The Government of Panama agreed to share urgently the conclusions of the report to help prevent similar accidents. The physicists of ION involved were taken to trial by the patients' families.\n\n"}
{"id": "432961", "url": "https://en.wikipedia.org/wiki?curid=432961", "title": "Intertropical Convergence Zone", "text": "Intertropical Convergence Zone\n\nThe Intertropical Convergence Zone (ITCZ), known by sailors as the doldrums or the calms, is the area encircling Earth near the Equator, where the northeast and southeast trade winds converge.\n\nThe ITCZ was originally identified from the 1920s to the 1940s as the \"Intertropical Front\" (\"ITF\"), but after the recognition in the 1940s and the 1950s of the significance of wind field convergence in tropical weather production, the term \"ITCZ\" was then applied. When it lies near the Equator, it is called the near-equatorial trough. Where the ITCZ is drawn into and merges with a monsoonal circulation, it is sometimes referred to as a monsoon trough, a usage more common in Australia and parts of Asia. In the seamen's speech, the zone is referred to as the doldrums because of its erratic (monotonous) weather patterns with stagnant calms and violent thunderstorms.\n\nThe ITCZ appears as a band of clouds, usually thunderstorms, that encircle the globe near the Equator. In the Northern Hemisphere, the trade winds move in a southwestward direction from the northeast, while in the Southern Hemisphere, they move northwestward from the southeast. When the ITCZ is positioned north or south of the Equator, these directions change according to the Coriolis effect imparted by Earth's rotation. For instance, when the ITCZ is situated north of the Equator, the southeast trade wind changes to a southwest wind as it crosses the Equator. The ITCZ is formed by vertical motion largely appearing as convective activity of thunderstorms driven by solar heating, which effectively draw air in; these are the trade winds. The ITCZ is effectively a tracer of the ascending branch of the Hadley cell and is wet. The dry descending branch is the horse latitudes.\n\nThe location of the ITCZ gradually varies with the seasons, roughly corresponding with the location of the thermal equator. As the heat capacity of the oceans is greater than air over land, migration is more prominent over land. Over the oceans, where the convergence zone is better defined, the seasonal cycle is more subtle, as the convection is constrained by the distribution of ocean temperatures. Sometimes, a double ITCZ forms, with one located north and another south of the Equator, one of which is usually stronger than the other. When this occurs, a narrow ridge of high pressure forms between the two convergence zones.\n\nThe South Pacific convergence zone (SPCZ) is a reverse-oriented, or west-northwest to east-southeast aligned, trough extending from the west Pacific warm pool southeastwards towards French Polynesia. It lies just south of the equator during the Southern Hemisphere warm season, but can be more extratropical in nature, especially east of the International Date Line. It is considered the largest and most important piece of the ITCZ, and has the least dependence upon heating from a nearby land mass during the summer than any other portion of the monsoon trough. The southern ITCZ in the southeast Pacific and southern Atlantic, known as the SITCZ, occurs during the Southern Hemisphere fall between 3° and 10° south of the equator east of the 140th meridian west longitude during cool or neutral El Niño–Southern Oscillation (ENSO) patterns. When ENSO reaches its warm phase, otherwise known as El Niño, the tongue of lowered sea surface temperatures due to upwelling off the South American continent disappears, which causes this convergence zone to vanish as well.\n\nVariation in the location of the intertropical convergence zone drastically affects rainfall in many equatorial nations, resulting in the wet and dry seasons of the tropics rather than the cold and warm seasons of higher latitudes. Longer term changes in the intertropical convergence zone can result in severe droughts or flooding in nearby areas.\n\nIn some cases, the ITCZ may become narrow, especially when it moves away from the equator; the ITCZ can then be interpreted as a front along the leading edge of the equatorial air. There appears to be a 15 to 25-day cycle in thunderstorm activity along the ITCZ, which is roughly half the wavelength of the Madden–Julian oscillation (MJO).\n\nWithin the ITCZ the average winds are slight, unlike the zones north and south of the equator where the trade winds feed. Early sailors named this belt of calm \"the doldrums\" because of the inactivity and stagnation they found themselves in after days of no wind. To find oneself becalmed in this region in a hot and muggy climate could mean death in an era when wind was the only effective way to propel ships across the ocean. Even today leisure and competitive sailors attempt to cross the zone as quickly as possible as the erratic weather and wind patterns may cause unexpected delays.\n\nTropical cyclogenesis depends upon low-level vorticity as one of its six requirements, and the ITCZ fills this role as it is a zone of wind change and speed, otherwise known as horizontal wind shear. As the ITCZ migrates to tropical - subtropical latitudes and even beyond (Shandong province of the People's Republic of China) during the respective hemisphere's summer season, increasing Coriolis force makes the formation of tropical cyclones within this zone more possible. Surges of higher pressure from high latitudes can enhance tropical disturbances along its axis. In the north Atlantic and the northeastern Pacific oceans, tropical waves move along the axis of the ITCZ causing an increase in thunderstorm activity, and under weak vertical wind shear, these clusters of thunderstorms can come.\n\nThunderstorms along the Intertropical Convergence Zone played a role in the loss of Air France Flight 447, which left Rio de Janeiro–Galeão International Airport on Sunday, May 31, 2009, at about 7:00 p.m. local time (6:00 p.m. EDT or 10:00 p.m. UTC) and had been expected to land at Charles de Gaulle Airport near Paris on Monday, June 1, 2009, at 11:15 a.m. (5:15 a.m. EDT or 9:15 a.m. UTC) The aircraft crashed with no survivors while flying through a series of large ITCZ thunderstorms, and ice forming rapidly on airspeed sensors was the precipitating cause for a cascade of human errors which ultimately doomed the flight. Most aircraft flying these routes are able to avoid the larger convective cells without incident.\n\n\n"}
{"id": "18963787", "url": "https://en.wikipedia.org/wiki?curid=18963787", "title": "Ion", "text": "Ion\n\nAn ion () is an atom or molecule that has a non-zero net electrical charge. Since the charge of the electron (considered \"negative\" by convention) is equal and opposite to that of the proton (considered \"positive\" by convention), the net charge of an ion is non-zero due to its total number of electrons being unequal to its total number of protons. A cation is a positively charged ion, with fewer electrons than protons, while an anion is negatively charged, with more electrons than protons. Because of their opposite electric currents, cations and anions attract each other and readily form ionic compounds. \n\nIons consisting of only a single atom are termed atomic or monatomic ions, while two or more atoms form molecular ions or polyatomic ions. In the case of physical ionization in a medium, such as a gas, \"ion pairs\" are created by ion collisions, where each generated pair consists of a free electron and a positive ion. Ions are also created by chemical interactions, such as the dissolution of a salt in liquids, or by other means, such as passing a direct current through a conducting solution, dissolving an anode via ionization.\n\nThe word \"ion\" comes from the Greek word ἰόν, \"ion\", \"going\", the present participle of ἰέναι, \"ienai\", \"to go\". This term was introduced by English physicist and chemist Michael Faraday in 1834 for the then-unknown species that \"goes\" from one electrode to the other through an aqueous medium. Faraday did not know the nature of these species, but he knew that since metals dissolved into and entered a solution at one electrode and new metal came forth from a solution at the other electrode; that some kind of substance has moved through the solution in a current. This conveys matter from one place to the other.\n\nFaraday also introduced the words \"anion\" for a negatively charged ion, and \"cation\" for a positively charged one. In Faraday's nomenclature, cations were named because they were attracted to the cathode in a galvanic device and anions were named due to their attraction to the anode.\n\nSvante Arrhenius put forth, in his 1884 dissertation, his explanation of the fact that solid crystalline salts dissociate into paired charged particles when dissolved, for which he would win the 1903 Nobel Prize in Chemistry. Arrhenius' explanation was that in forming a solution, the salt dissociates into Faraday's ions. Arrhenius proposed that ions formed even in the absence of an electric current.\n\nIons in their gas-like state are highly reactive and will rapidly interact with ions of opposite charge to give neutral molecules or ionic salts. Ions are also produced in the liquid or solid state when salts interact with solvents (for example, water) to produce \"solvated ions\", which are more stable, for reasons involving a combination of energy and entropy changes as the ions move away from each other to interact with the liquid. These stabilized species are more commonly found in the environment at low temperatures. A common example is the ions present in seawater, which are derived from dissolved salts.\n\nAs charged objects, ions are attracted to opposite electric charges (positive to negative, and vice versa) and repelled by like charges. When they move, their trajectories can be deflected by a magnetic field.\n\nElectrons, due to their smaller mass and thus larger space-filling properties as matter waves, determine the size of atoms and molecules that possess any electrons at all. Thus, anions (negatively charged ions) are larger than the parent molecule or atom, as the excess electron(s) repel each other and add to the physical size of the ion, because its size is determined by its electron cloud. Cations are smaller than the corresponding parent atom or molecule due to the smaller size of the electron cloud. One particular cation (that of hydrogen) contains no electrons, and thus consists of a single proton - \"very much smaller\" than the parent hydrogen atom.\n\nSince the electric charge on a proton is equal in magnitude to the charge on an electron, the net electric charge on an ion is equal to the number of protons in the ion minus the number of electrons.\n\nAn (−) (), from the Greek word ἄνω (\"ánō\"), meaning \"up\", is an ion with more electrons than protons, giving it a net negative charge (since electrons are negatively charged and protons are positively charged).\n\nA (+) (), from the Greek word κάτω (\"káto\"), meaning \"down\", is an ion with fewer electrons than protons, giving it a positive charge.\n\nThere are additional names used for ions with multiple charges. For example, an ion with a −2 charge is known as a dianion and an ion with a +2 charge is known as a dication. A zwitterion is a neutral molecule with positive and negative charges at different locations within that molecule.\n\nCations and anions are measured by their ionic radius and they differ in relative size: \"Cations are small, most of them less than 10 m (10 cm) in radius. But most anions are large, as is the most common Earth anion, oxygen. From this fact it is apparent that most of the space of a crystal is occupied by the anion and that the cations fit into the spaces between them.\"\n\nA cation has radius less than 0.8 × 10 m (0.8 Å) while an anion has radius greater than 1.3 × 10 m (1.3 Å).\n\nIons are ubiquitous in nature and are responsible for diverse phenomena from the luminescence of the Sun to the existence of the Earth's ionosphere. Atoms in their ionic state may have a different colour from neutral atoms, and thus light absorption by metal ions gives the colour of gemstones. In both inorganic and organic chemistry (including biochemistry), the interaction of water and ions is extremely important; an example is the energy that drives breakdown of adenosine triphosphate (ATP). The following sections describe contexts in which ions feature prominently; these are arranged in decreasing physical length-scale, from the astronomical to the microscopic.\n\nIons can be non-chemically prepared using various ion sources, usually involving high voltage or temperature. These are used in a multitude of devices such as mass spectrometers, optical emission spectrometers, particle accelerators, ion implanters, and ion engines.\n\nAs reactive charged particles, they are also used in air purification by disrupting microbes, and in household items such as smoke detectors.\n\nAs signalling and metabolism in organisms are controlled by a precise ionic gradient across membranes, the disruption of this gradient contributes to cell death. This is a common mechanism exploited by natural and artificial biocides, including the ion channels gramicidin and amphotericin (a fungicide).\n\nInorganic dissolved ions are a component of total dissolved solids, a widely-known indicator of water quality.\n\nThe ionizing effect of radiation on a gas is extensively used for the detection of radiation such as alpha, beta, gamma and X-rays. The original ionization event in these instruments results in the formation of an \"ion pair\"; a positive ion and a free electron, by ion impact by the radiation on the gas molecules. The ionization chamber is the simplest of these detectors, and collects all the charges created by \"direct ionization\" within the gas through the application of an electric field.\n\nThe Geiger–Müller tube and the proportional counter both use a phenomenon known as a Townsend avalanche to multiply the effect of the original ionizing event by means of a cascade effect whereby the free electrons are given sufficient energy by the electric field to release further electrons by ion impact.\n\nWhen writing the chemical formula for an ion, its net charge is written in superscript immediately after the chemical structure for the molecule/atom. The net charge is written with the magnitude \"before\" the sign; that is, a doubly charged cation is indicated as 2+ instead of +2. However, the magnitude of the charge is omitted for singly charged molecules/atoms; for example, the sodium cation is indicated as Na and \"not\" Na.\n\nAn alternative (and acceptable) way of showing a molecule/atom with multiple charges is by drawing out the signs multiple times, this is often seen with transition metals. Chemists sometimes circle the sign; this is merely ornamental and does not alter the chemical meaning. All three representations of shown in the figure, are thus equivalent.\n\nMonatomic ions are sometimes also denoted with Roman numerals; for example, the example seen above is occasionally referred to as Fe(II) or Fe. The Roman numeral designates the \"formal oxidation state\" of an element, whereas the superscripted numerals denote the net charge. The two notations are, therefore, exchangeable for monatomic ions, but the Roman numerals \"cannot\" be applied to polyatomic ions. However, it is possible to mix the notations for the individual metal centre with a polyatomic complex, as shown by the uranyl ion example.\n\nIf an ion contains unpaired electrons, it is called a \"radical\" ion. Just like uncharged radicals, radical ions are very reactive. Polyatomic ions containing oxygen, such as carbonate and sulfate, are called \"oxyanions\". Molecular ions that contain at least one carbon to hydrogen bond are called \"organic ions\". If the charge in an organic ion is formally centred on a carbon, it is termed a \"carbocation\" (if positively charged) or \"carbanion\" (if negatively charged).\n\nMonatomic ions are formed by the gain or loss of electrons to the valence shell (the outer-most electron shell) in an atom. The inner shells of an atom are filled with electrons that are tightly bound to the positively charged atomic nucleus, and so do not participate in this kind of chemical interaction. The process of gaining or losing electrons from a neutral atom or molecule is called \"ionization\".\n\nAtoms can be ionized by bombardment with radiation, but the more usual process of ionization encountered in chemistry is the transfer of electrons between atoms or molecules. This transfer is usually driven by the attaining of stable (\"closed shell\") electronic configurations. Atoms will gain or lose electrons depending on which action takes the least energy.\n\nFor example, a sodium atom, Na, has a single electron in its valence shell, surrounding 2 stable, filled inner shells of 2 and 8 electrons. Since these filled shells are very stable, a sodium atom tends to lose its extra electron and attain this stable configuration, becoming a sodium cation in the process\n\nOn the other hand, a chlorine atom, Cl, has 7 electrons in its valence shell, which is one short of the stable, filled shell with 8 electrons. Thus, a chlorine atom tends to \"gain\" an extra electron and attain a stable 8-electron configuration, becoming a chloride anion in the process:\n\nThis driving force is what causes sodium and chlorine to undergo a chemical reaction, wherein the \"extra\" electron is transferred from sodium to chlorine, forming sodium cations and chloride anions. Being oppositely charged, these cations and anions form ionic bonds and combine to form sodium chloride, NaCl, more commonly known as table salt.\n\nPolyatomic and molecular ions are often formed by the gaining or losing of elemental ions such as a proton, H, in neutral molecules. For example, when ammonia, NH, accepts a proton, H—a process called protonation—it forms the ammonium ion, . Ammonia and ammonium have the same number of electrons in essentially the same electronic configuration, but ammonium has an extra proton that gives it a net positive charge.\n\nAmmonia can also lose an electron to gain a positive charge, forming the ion . However, this ion is unstable, because it has an incomplete valence shell around the nitrogen atom, making it a very reactive radical ion.\n\nDue to the instability of radical ions, polyatomic and molecular ions are usually formed by gaining or losing elemental ions such as , rather than gaining or losing electrons. This allows the molecule to preserve its stable electronic configuration while acquiring an electrical charge.\n\nThe energy required to detach an electron in its lowest energy state from an atom or molecule of a gas with less net electric charge is called the \"ionization potential\", or \"ionization energy\". The \"n\"th ionization energy of an atom is the energy required to detach its \"n\"th electron after the first \"n − 1\" electrons have already been detached.\n\nEach successive ionization energy is markedly greater than the last. Particularly great increases occur after any given block of atomic orbitals is exhausted of electrons. For this reason, ions tend to form in ways that leave them with full orbital blocks. For example, sodium has one \"valence electron\" in its outermost shell, so in ionized form it is commonly found with one lost electron, as . On the other side of the periodic table, chlorine has seven valence electrons, so in ionized form it is commonly found with one gained electron, as . Caesium has the lowest measured ionization energy of all the elements and helium has the greatest. In general, the ionization energy of metals is much lower than the ionization energy of nonmetals, which is why, in general, metals will lose electrons to form positively charged ions and nonmetals will gain electrons to form negatively charged ions.\n\n\"Ionic bonding\" is a kind of chemical bonding that arises from the mutual attraction of oppositely charged ions. Ions of like charge repel each other, and ions of opposite charge attract each other. Therefore, ions do not usually exist on their own, but will bind with ions of opposite charge to form a crystal lattice. The resulting compound is called an \"ionic compound\", and is said to be held together by \"ionic bonding\". In ionic compounds there arise characteristic distances between ion neighbours from which the spatial extension and the ionic radius of individual ions may be derived.\n\nThe most common type of ionic bonding is seen in compounds of metals and nonmetals (except noble gases, which rarely form chemical compounds). Metals are characterized by having a small number of electrons in excess of a stable, closed-shell electronic configuration. As such, they have the tendency to lose these extra electrons in order to attain a stable configuration. This property is known as \"electropositivity\". Non-metals, on the other hand, are characterized by having an electron configuration just a few electrons short of a stable configuration. As such, they have the tendency to gain more electrons in order to achieve a stable configuration. This tendency is known as \"electronegativity\". When a highly electropositive metal is combined with a highly electronegative nonmetal, the extra electrons from the metal atoms are transferred to the electron-deficient nonmetal atoms. This reaction produces metal cations and nonmetal anions, which are attracted to each other to form a \"salt\".\n"}
{"id": "2206496", "url": "https://en.wikipedia.org/wiki?curid=2206496", "title": "Ionomer", "text": "Ionomer\n\nAn ionomer () (\"iono-\" + \"-mer\") is a polymer composed of repeat units of both electrically neutral repeating units and a ionized units covalently bonded to the polymer backbone as pendant group moieties. Usually no more than 15 mole percent are ionized. The ionized units are often carboxylic acid groups.\n\nThe classification of a polymer as an ionomer depends on the level of substitution of ionic groups as well as how the ionic groups are incorporated into the polymer structure. For example, polyelectrolytes also have ionic groups covalently bonded to the polymer backbone, but have a much higher ionic group molar substitution level (usually greater than 80%); ionenes are polymers where ionic groups are part of the actual polymer backbone. These two classes of ionic-group-containing polymers have vastly different morphological and physical properties and are therefore not considered ionomers. \n\nIonomers have unique physical properties including electrical conductivity and viscosity—increase in ionomer solution viscosity with increasing temperatures (see conducting polymer). Ionomers also have unique morphological properties as the non-polar polymer backbone is energetically incompatible with the polar ionic groups. As a result, the ionic groups in most ionomers will undergo microphase separation to form ionic-rich domains.\n\nCommercial applications for ionomers include golf ball covers, semipermeable membranes, sealing tape and thermoplastic elastomers. Common examples of ionomers include polystyrene sulfonate, Nafion and Hycar.\n\nUsually ionomer synthesis consists of two steps – the introduction of acid groups to the polymer backbone and the neutralization of some of the acid groups by a metal cation. In very rare cases, the groups introduced are already neutralized by a metal cation. The first step (introduction of acid groups) can be done in two ways; a neutral non-ionic monomer can be copolymerized with a monomer that contains pendant acid groups or acid groups can be added to a non-ionic polymer through post-reaction modifications. For example, ethylene-methacrylic acid and sulfonated perfluorocarbon (Nafion) are synthesized through copolymerization while polystyrene sulfonate is synthesized through post-reaction modifications.\n\nIn most cases, the acid form of the copolymer is synthesized (i.e. 100% of the carboxylic acid groups are neutralized by hydrogen cations) and the ionomer is formed through subsequent neutralization by the appropriate metal cation. The identity of the neutralizing metal cation has an effect on the physical properties of the ionomer; the most commonly used metal cations (at least in academic research) are zinc, sodium, and magnesium. Neutralization or ionomerization, can also be accomplished in two ways: the acid copolymer can be melt-mixed with a basic metal or neutralization can be achieved through solution processes. The former method is preferred commercially. However, as commercial manufacturers are reluctant to share their procedures, little is known about the exact conditions of the melt-mixing neutralization process other than that hydroxides are generally used to provide the metal cation. The latter solution neutralization process is generally used in academic settings. The acid copolymer is dissolved and a basic salt with the appropriate metal cation is added to this solution. Where dissolution of the acid copolymer is difficult, simply swelling the polymer in the solvent is sufficient, though dissolving is always preferred. Because basic salts are polar and are not soluble in the non-polar solvents used to dissolve most polymers, mixed solvents (e.g. 90:10 toluene/alcohol) are often used.\n\nNeutralization level must be determined after an ionomer is synthesized as varying the neutralization level varies the morphological and physical properties of the ionomer. One method used to do this is to examine the peak heights of infrared vibrations of the acid form. However, there may be substantial error in determining peak height, especially since small amounts of water appear in the same wavenumber range. Titration of the acid groups is another method that can be used, though this is not possible in some systems.\n\nSurlyn is the brand name of an ionomer resin created by DuPont, a copolymer of ethylene and methacrylic acid used as a coating and packaging material.\nDuPont neutralizes the acid with NaOH, yielding the sodium salt.\nCrystals of ethylene-methacrylic acid ionomers exhibit dual melting behavior.\n\n\n\n"}
{"id": "3574774", "url": "https://en.wikipedia.org/wiki?curid=3574774", "title": "Jeep Tornado engine", "text": "Jeep Tornado engine\n\nThe Jeep Tornado engine was the first post-World War II U.S.-designed mass-produced overhead cam (OHC) automobile engine. The straight-six was introduced in mid-year 1962, and replaced the flathead 6-226 Willys Super Hurricane that was in use since 1954.\n\nThe Tornado engine was also manufactured in Argentina by Industrias Kaiser Argentina (IKA) from 1965 to 1973.\n\nThe development of a new engine for Kaiser Jeep for an entirely new vehicle began under Chief Engineer, A.C. \"Sammy\" Sampietro, in the late-1950s. Sampietro worked under Donald Healey in Europe and focused on improving power output through better engine breathing. The single overhead cam design was combined with hemispheric combustion chambers. Mass production of the new engine began in 1962.\n\nThe Jeep Tornado engine was introduced in the Willys Jeep Wagon and truck models. Six-cylinder versions built after May 3, 1962, received the 230 OHC \"Tornado\" engine, replacing the 226 L-head \"Super Hurricane\" I6. It was made the standard engine in the completely new Jeep Wagoneer station wagons (today called SUV) and Jeep Gladiator pickup trucks that began production in the fall 1962 for the 1963 model year.\n\nThe Tornado was the only U.S.-built overhead-cam engine in production at that time. The new engine was designed for robust, heavy-duty performance with maximum efficiency. Its excellent fuel economy was proven in tests with the Tornado-OHC engine having the lowest specific fuel consumption of all production gasoline engines on the market.\n\nThe Tornado, like most Jeep engines, was undersquare for better low-speed torque. It had a 3/ inch (84.93mm) bore with a 4/ inch (111.1mm) stroke. The standard version had an 8.5:1 compression ratio. Output was at 4000 rpm and of torque at 1750 rpm. A low-compression (7.5:1) version was also available, with at 4000 rpm and of torque at 2400 rpm. It was a \"high-efficiency\" engine with a conservatively rated power output.\n\nThe new engine's overhead camshaft design was only one of the advanced features offered for the first time in a U.S.-designed and mass-produced engine. The Tornado was a good engine; unfortunately, it was complex (by 1960s standards) and was discontinued in civilian vehicles in the U.S. in 1965. It continued to be used in military versions of the Jeep pickup, the M-715 and M-725, until 1969. These engines had block-mounted motor mounts, rather than the front cover mounts that were a cause of oil leaks on the civilian versions.\n\nOne unique feature of the design was that the camshaft only had six lobes. One lobe operated both the intake and exhaust valve for each cylinder. This made engineering cam profiles a bit more difficult than conventional two lobe per cylinder (one per valve) designs, but allowed the valves to be better arranged for the cross-flow head. Valves were directly opposite their respective ports, and ports were short with wide radius turns.\n\nRoad tests of the new Jeep Wagoneer by \"Car Life\" magazine described the OHC six as \"commendably smooth and quiet.\" The engine accelerated the four-wheel-drive full-size station wagon (the SUV designation was not yet known) with an automatic transmission from 0 to 60 mph in 15.9 seconds. Their tests recorded on the highway and in the city, that \"certainly demonstrates the remarkable efficiency of the OHC engine.\"\n\nProduction of this engine continued in Argentina by Industrias Kaiser Argentina (IKA) after 1965. The engine was used in a variety of Jeep vehicles and American Motors (AMC) passenger cars assembled under license. The engine became best known for powering the IKA-Renault Torino, their version of the AMC Rambler American having unique styled front body parts that was built in Argentina from 1966 to 1981. It achieved international success in the 1969 Nürburgring 84-hour endurance race when a Torino placed third due to penalty points after covering 334 laps, the most of all the racers (c.9450 km/5868 mi.).\n\nThe engine name was changed to \"Torino\" to match the car in 1973. It also received a major block and crankshaft refinement that year — seven main bearings instead of the original four. Industrias Kaiser Argentina was eventually bought out by Renault, and in 1975, the \"IKA\" name was dropped and it became \"Renault Argentina\". The Torino, Jeeps, AMC cars, as well as the Tornado engine itself, continued to receive upgrades over the years.\nArgentinian's Tornado engines raised up the power from 155 HP to 215 HP (street versions) and 250 HP to 350 HP (Racing versions). It was achieved by a new Cylinder head, which has improved the intake and exhaust ducts. Also adopted a new camshaft, a new exhaust manifold 3-1 / 3-1 type, two 2\" diamenter exhaust pipes and 3 carburators Weber DCOE 45-45. \nThe Torino and the Jeep Tornado engine continued production through 1973. It was marketed as the \"Tornado Jet\", and later as the \"Tornado Interceptor\", in AMC automobiles. From 1976 to 1982 they were the only non-Renault–designed cars and engines built by the French company.\n\nThe Jeep Tornado engine was used in the following vehicles:\n"}
{"id": "15281720", "url": "https://en.wikipedia.org/wiki?curid=15281720", "title": "Juhani Aaltonen", "text": "Juhani Aaltonen\n\nJuhani Aaltonen (born December 12, 1935) is a Finnish jazz saxophonist and flautist.\n\nBorn in Kouvola, Finland, he studied at Sibelius Academy and Berklee College of Music. He began playing professionally at the end of the 1950s. He played in a sextet led by Heikki Rosendahl during that time, and then studied flute performance at the Sibelius Academy and in the U.S. at the Berklee College of Music. Moving back to Finland, he settled in Helsinki and began working both as a session musician and with fusion groups. Later in the 1960s he formed a duo with Edward Vesala, as well as in the group Eero Koivistoinen for four years. He played with Tasavallan Presidentti in their earlier days, including for their first, eponymous, album. He recorded with Thad Jones and Mel Lewis and with Heikki Sarmanto in the late 1960s and early 1970s, and his first album as a soloist, \"Etiquette\", was released in 1974.\n\nIn 1975, he became a member of the New Music Orchestra, and worked with the Nordic All Stars, Arild Andersen, and Peter Brötzmann later in the 1970s. The 1980s saw him working with the UFO Big Band, Jan Garbarek, Charlie Mariano, and others. He was granted a 15-year state grant from Finland in the mid-1980s. In 1983 he rejoined Tasavallan Presidentti, and has recorded and toured with them since. He led a touring quartet from 1990 to 1992 with Olli Ahvenlahti, Heikki Virtanen and Reino Laine. Aaltonen and Heikki Sarmanto released a duo recording, \"Rise\", in 2001; in 2003 Aaltonen's trio album \"Mother Tongue\" won a Jazz-Emma in Finland. Aaltonen continues to teach at the annual Nilsiä Music Camp.\n\n\nWith Arild Andersen\nWith Graham Collier\nWith Edward Vesala\n\n\n"}
{"id": "6725153", "url": "https://en.wikipedia.org/wiki?curid=6725153", "title": "KT88", "text": "KT88\n\nThe KT88 is a beam tetrode/kinkless tetrode (hence \"KT\") vacuum tube for audio amplification. \nThe KT88 fits a standard eight-pin octal socket and has similar pinout and applications as the 6L6 and EL34. Specifically designed for audio amplification, the KT88 has similar ratings to the American 6550 which was designed for use as a servo amplifier. It is one of the largest tubes in its class and can handle significantly higher plate voltages than similar tubes, up to 800 volts. A KT88 push-pull pair in class AB1 fixed bias is capable of 100 watts of output with 2.5% total harmonic distortion or up to about 50W at low distortion in hi-fi applications. The transmitting tubes TT21 and TT22 have almost identical transfer characteristics to KT88 but a different pinout, and by virtue of their anode being connected to the top cap have a higher plate voltage rating (1.25 kilovolt) and a higher power output capability of 200 watts in class AB1 push–pull.\n\nThe screen grid is sometimes tied to the anode so that it becomes effectively a triode with a lower maximum power output.\n\nThe KT88 was introduced by GEC in 1956 as a larger variant of the KT66. It was manufactured in the U.K. by the MOV (Marconi-Osram Valve) subsidiary of G.E.C, also labelled as IEC/Mullard, and, in the U.S., Genelex \"Gold Lion\". \nAs of 2018, KT88 valves are produced by Shuguang (China), JJ Electronic (Slovakia), Svetlana (Russia), and New Sensor Corporation (Russia). NOS examples in good condition are extremely rare. Due to its availability and characteristics, the KT88 is popular in hi-fi production amplifiers.\n\nHistorically, it has been far more popular with high fidelity stereo manufacturers than guitar amplifier builders, given its characteristics of high-power and low-distortion. Due to these characteristics, it is regularly used to replace 6550 tubes by end users seeking a guitar amplifier tone with less distortion. Some of the amplifiers which shipped with the KT88 power tube include the Hiwatt Custom 200 and 400 Bass Heads, Sound City L/B 200, 200 watt Marshall Major, Mesa-Boogie Bass Strategy Eight:88 (465 Watt) and Bass Prodigy Four:88 (250 Watt), Orange Thunderverb, Reeves Custom 225 bass, Fryette (formerly VHT) Two/Ninety/Two power amp, Pittbull Ultra-Lead, Sig:X & Deliverance, McIntosh MC2102, Splawn Nitro, Pro Mod, Competition & Superstock, Blackstar Series One 200, Carlsbro 200 TC and Marshall 2203KK Kerry King Signature JCM800.\n\n\n"}
{"id": "2207911", "url": "https://en.wikipedia.org/wiki?curid=2207911", "title": "Kelvin probe force microscope", "text": "Kelvin probe force microscope\n\nKelvin probe force microscopy (KPFM), also known as surface potential microscopy, is a noncontact variant of atomic force microscopy (AFM). With KPFM, the work function of surfaces can be observed at atomic or molecular scales. The work function relates to many surface phenomena, including catalytic activity, reconstruction of surfaces, doping and band-bending of semiconductors, charge trapping in dielectrics and corrosion. The map of the work function produced by KPFM gives information about the composition and electronic state of the local structures on the surface of a solid.\n\nKPFM is a scanning probe method where the potential offset between a probe tip and a surface can be measured using the same principle as a macroscopic Kelvin probe. The cantilever in the AFM is a reference electrode that forms a capacitor with the surface, over which it is scanned laterally at a constant separation. The cantilever is not piezoelectrically driven at its mechanical resonance frequency ω as in normal AFM although an alternating current (AC) voltage is applied at this frequency.\n\nWhen there is a direct-current (DC) potential difference between the tip and the surface, the AC+DC voltage offset will cause the cantilever to vibrate. The origin of the force can be understood by considering that the energy of the capacitor formed by the cantilever and the surface is\n\nplus terms at DC. Only the cross-term proportional to the \"V·V\" product is at the resonance frequency ω. The resulting vibration of the cantilever is detected using usual scanned-probe microscopy methods (typically involving a diode laser and a four-quadrant detector). A null circuit is used to drive the DC potential of the tip to a value which minimizes the vibration. A map of this nulling DC potential versus the lateral position coordinate therefore produces an image of the work function of the surface.\n\nA related technique, electrostatic force microscopy (EFM), directly measures the force produced on a charged tip by the electric field emanating from the surface. EFM operates much like magnetic force microscopy in that the frequency shift or amplitude change of the cantilever oscillation is used to detect the electric field. However, EFM is much more sensitive to topographic artifacts than KPFM. Both EFM and KPFM require the use of conductive cantilevers, typically metal-coated silicon or silicon nitride.\n\nThe Kelvin probe force microscope or Kelvin force microscope (KFM) is based on an AFM set-up and the determination of the work function is based on the measurement of the electrostatic forces between the small AFM tip and the sample. The conducting tip and the sample are characterized by (in general) different work functions, which represent the difference between the Fermi level and the vacuum level for each material. If both elements were brought in contact, a net electric current would flow between them until the Fermi levels were aligned. The difference between the work functions is called the contact potential difference and is denoted generally with \"V\". An electrostatic force exists between tip and sample, because of the electric field between them. For the measurement a voltage is applied between tip and sample, consisting of a DC-bias \"V\" and an AC-voltage \"V sin(ωt)\" of frequency \"ω\".\n\nTuning the AC-frequency to the resonant frequency of the AFM cantilever results in an improved sensitivity. The electrostatic force in a capacitor may be found by differentiating the energy function with respect to the separation of the elements and can be written as\n\nwhere \"C\" is the capacitance, \"z\" is the separation, and \"V\" is the voltage, each between tip and surface. Substituting the previous formula for voltage (V) shows that the electrostatic force can be split up into three contributions, as the total electrostatic force \"F\" acting on the tip then has spectral components at the frequencies \"ω\" and \"2ω\".\n\nThe DC component, \"F\", contributes to the topographical signal, the term \"F\" at the characteristic frequency \"ω\" is used to measure the contact potential and the contribution \"F\" can be used for capacitance microscopy.\n\nFor contact potential measurements a lock-in amplifier is used to detect the cantilever oscillation at \"ω\". During the scan \"V\" will be adjusted so that the electrostatic forces between the tip and the sample become zero and thus the response at the frequency ω becomes zero. Since the electrostatic force at \"ω\" depends on \"V − V\", the value of \"V\" that minimizes the \"ω\"-term corresponds to the contact potential. Absolute values of the sample work function can be obtained if the tip is first calibrated against a reference sample of known work function. Apart from this, one can use the normal topographic scan methods at the resonance frequency \"ω\" independently of the above. Thus, in one scan, the topography and the contact potential of the sample are determined simultaneously.\nThis can be done in (at least) two different ways: 1) The topography is captured in AC mode which means that the cantilever is driven by a piezo at its resonant frequency. Simultaneously the AC voltage for the KPFM measurement is applied at a frequency slightly lower than the resonant frequency of the cantilever. In this measurement mode the topography and the contact potential difference are captured at the same time and this mode is often called single-pass. 2) One line of the topography is captured either in contact or AC mode and is stored internally. Then, this line is scanned again, while the cantilever remains on a defined distance to the sample without a mechanically driven oscillation but the AC voltage of the KPFM measurement is applied and the contact potential is captured as explained above. It is important to note that the cantilever tip must not be too close to the sample in order to allow good oscillation with applied AC voltage. Therefore, KPFM can be performed simultaneously during AC topography measurements but not during contact topography measurements.\n\n\n"}
{"id": "23373024", "url": "https://en.wikipedia.org/wiki?curid=23373024", "title": "Kogut–Susskind fermion", "text": "Kogut–Susskind fermion\n\nKogut–Susskind fermions are a lattice version of Kähler–Dirac fermions, which obey a first-order differential equation by taking an alternative square root of the Laplacian than that used by Dirac. They are named after John Kogut and Leonard Susskind.\n"}
{"id": "30864636", "url": "https://en.wikipedia.org/wiki?curid=30864636", "title": "Late Heavy Bombardment", "text": "Late Heavy Bombardment\n\nThe Late Heavy Bombardment (abbreviated LHB and also known as the lunar cataclysm) is an event thought to have occurred approximately 4.1 to 3.8 billion years (Ga) ago, at a time corresponding to the Neohadean and Eoarchean eras on Earth. During this interval, a disproportionately large number of asteroids are theorized to have collided with the early terrestrial planets in the inner Solar System, including Mercury, Venus, Earth, and Mars.\n\nThe Late Heavy Bombardment happened after the Earth and other rocky planets had formed and accreted most of their mass, but still quite early in Earth's history.\n\nEvidence for the LHB derives from lunar samples brought back by the Apollo astronauts. Isotopic dating of Moon rocks implies that most impact melts occurred in a rather narrow interval of time. Several hypotheses attempt to explain the apparent spike in the flux of impactors (i.e. asteroids and comets) in the inner Solar System, but no consensus yet exists. The Nice model, popular among planetary scientists, postulates that the giant planets underwent orbital migration and in doing so, scattered objects in the asteroid and/or Kuiper belts into eccentric orbits, and into the path of the terrestrial planets. Other researchers argue that the lunar sample data do not require a cataclysmic cratering event near 3.9 Ga, and that the apparent clustering of impact-melt ages near this time is an artifact of sampling materials retrieved from a single large impact basin. They also note that the rate of impact cratering could differ significantly between the outer and inner zones of the Solar System.\n\nThe main piece of evidence for a lunar cataclysm comes from the radiometric ages of impact melt rocks that were collected during the Apollo missions. The majority of these impact melts are believed to have formed during the collision of asteroids or comets tens of kilometres across, forming impact craters hundreds of kilometres in diameter. The Apollo 15, 16, and 17 landing sites were chosen as a result of their proximity to the Imbrium, Nectaris, and Serenitatis basins, respectively.\n\nThe apparent clustering of ages of these impact melts, between about 3.8 and 4.1 Ga, led to postulation that the ages record an intense bombardment of the Moon. They called it the \"lunar cataclysm\" and proposed that it represented a dramatic increase in the rate of bombardment of the Moon around 3.9 Ga. If these impact melts were derived from these three basins, then not only did these three prominent impact basins form within a short interval of time, but so did many others based on stratigraphic grounds. At the time, the conclusion was considered controversial.\n\nAs more data has become available, particularly from lunar meteorites, this theory, while still controversial, has gained in popularity. The lunar meteorites are believed to randomly sample the lunar surface, and at least some of these should have originated from regions far from the Apollo landing sites. Many of the feldspathic lunar meteorites probably originated from the lunar far side, and impact melts within these have recently been dated. Consistent with the cataclysm hypothesis, none of their ages was found to be older than about 3.9 Ga. Nevertheless, the ages do not \"cluster\" at this date, but span between 2.5 and 3.9 Ga.\n\nDating of howardite, eucrite and diogenite (HED) meteorites and H chondrite meteorites originating from the asteroid belt reveal numerous ages from 3.4–4.1 Ga and an earlier peak at 4.5 Ga. The 3.4–4.1 Ga ages has been interpreted as representing an increase in impact velocities as computer simulations using hydrocode reveal that the volume of impact melt increases 100–1,000 times as the impact velocity increases from the current asteroid belt average of 5 km/s to 10 km/s. Impact velocities above 10 km/s require very high inclinations or the large eccentricities of asteroids on planet crossing orbits. Such objects are rare in the current asteroid belt but the population would be significantly increased by the sweeping of resonances due to giant planet migration.\n\nStudies of the highland crater size distributions suggest that the same family of projectiles struck Mercury and the Moon during the \"Late Heavy Bombardment\". If the history of decay of late heavy bombardment on Mercury also followed the history of late heavy bombardment on the Moon, the youngest large basin discovered, Caloris, is comparable in age to the youngest large lunar basins, Orientale and Imbrium, and all of the plains units are older than 3 billion years.\n\nWhile the cataclysm hypothesis has recently gained in popularity, particularly among dynamicists who have identified possible causes for such a phenomenon, the cataclysm hypothesis is still controversial and based on debatable assumptions. Two criticisms are that (1) the \"cluster\" of impact ages could be an artifact of sampling a single basin's ejecta, and (2) that the lack of impact melt rocks older than about 4.1 Ga is related to all such samples having been pulverized, or their ages being reset.\n\nThe first criticism concerns the origin of the impact melt rocks that were sampled at the Apollo landing sites. While these impact melts have been commonly attributed to having been derived from the \"closest basin\", it has been argued that a large portion of these might instead be derived from the Imbrium basin. The Imbrium impact basin is the youngest and largest of the multi-ring basins found on the central nearside of the Moon, and quantitative modeling shows that significant amounts of ejecta from this event should be present at all of the Apollo landing sites. According to this alternative hypothesis, the cluster of impact melt ages near 3.9 Ga simply reflects material being collected from a single impact event, Imbrium, and not several. Additional criticism also argues that the age spike at 3.9 Ga identified in 40Ar/39Ar dating could also be produced by an episodic early crust formation followed by partial 40Ar losses as the impact rate declined.\n\nA second criticism concerns the significance of the lack of impact melt rocks older than about 4.1 Ga. One hypothesis for this observation that does not involve a cataclysm is that old melt rocks did exist, but that their ages have all been reset by the continuous effects of impact cratering over the past 4 billion years. Furthermore, it is possible that these putative samples could all have been pulverized to such small sizes that it is impossible to obtain age determinations using standard radiometric methods.\nLatest reinterpretation of crater statistics suggests that the flux on the Moon and on Mars may have been lower in general. Thus, the recorded crater population can be explained without any peak in the earliest bombardment of the inner Solar System.\n\nIf a cataclysmic cratering event truly occurred on the Moon, the Earth would have been affected as well. Extrapolating lunar cratering rates to Earth at this time suggests that the following number of craters would have formed:\n\nBefore the formulation of the LHB theory, geologists generally assumed that the Earth remained molten until about 3.8 Ga. This date could be found in many of the oldest-known rocks from around the world, and appeared to represent a strong \"cutoff point\" beyond which older rocks could not be found. These dates remained fairly constant even across various dating methods, including the system considered the most accurate and least affected by environment, uranium–lead dating of zircons. As no older rocks could be found, it was generally assumed that the Earth had remained molten until this date, which defined the boundary between the earlier Hadean and later Archean eons. Nonetheless, more recently, in 1999, the oldest known rock on Earth was dated to be 4.031 ± 0.003 billion years old, and is part of the Acasta Gneiss of the Slave Craton in northwestern Canada.\n\nOlder rocks could be found, however, in the form of asteroid fragments that fall to Earth as meteorites. Like the rocks on Earth, asteroids also show a strong cutoff point, at about 4.6 Ga, which is assumed to be the time when the first solids formed in the protoplanetary disk around the then-young Sun. The Hadean, then, was the period of time between the formation of these early rocks in space, and the eventual solidification of the Earth's crust, some 700 million years later. This time would include the accretion of the planets from the disk and the slow cooling of the Earth into a solid body as the gravitational potential energy of accretion was released.\n\nLater calculations showed that the rate of collapse and cooling depends on the size of the rocky body. Scaling this rate to an object of Earth mass suggested very rapid cooling, requiring only 100 million years. The difference between measurement and theory presented a conundrum at the time.\n\nThe LHB offers a potential explanation for this anomaly. Under this model, the rocks dating to 3.8 Ga solidified only after much of the crust was destroyed by the LHB. Collectively, the Acasta Gneiss in the North American cratonic shield and the gneisses within the Jack Hills portion of the Narryer Gneiss Terrane in Western Australia are the oldest continental fragments on Earth, yet they appear to post-date the LHB. The oldest mineral yet dated on Earth, a 4.404 Ga zircon from Jack Hills, predates this event, but it is likely a fragment of crust left over from before the LHB, contained within a much younger (~3.8 Ga old) rock. \n\nThe Jack Hills zircon led to something of a revolution in our understanding of the Hadean eon. Older references generally show that Hadean Earth had a molten surface with prominent volcanos. The name \"Hadean\" itself refers to the \"hellish\" conditions assumed on Earth for the time, from the Greek Hades. Zircon dating suggested, albeit controversially, that the Hadean surface was solid, temperate, and covered by acidic oceans. This picture derives from the presence of particular isotopic ratios that suggest the action of water-based chemistry at some time before the formation of the oldest rocks (see Cool early Earth).\n\nOf particular interest, Manfred Schidlowski argued in 1979 that the carbon isotopic ratios of some sedimentary rocks found in Greenland were a relic of organic matter. There was much debate over the precise dating of the rocks, with Schidlowski suggesting they were about 3.8 Ga old, and others suggesting a more \"modest\" 3.6 Ga. In either case it was a very short time for abiogenesis to have taken place, and if Schidlowski was correct, arguably too short a time. The \"Late Heavy Bombardment\" and the \"re-melting\" of the crust that it suggests provides a timeline under which this would be possible; life either formed immediately after the \"Late Heavy Bombardment\", or more likely survived it, having arisen earlier during the Hadean. Recent studies suggest that the rocks Schidlowski found are indeed from the older end of the possible age range at about 3.85 Ga, suggesting the latter possibility is the most likely answer. More recent studies have found no evidence for the isotopically light carbon ratios that were the basis for the original claims.\n\nMore recently, a similar study of Jack Hills rocks shows traces of the same sort of potential organic indicators. Thorsten Geisler of the Institute for Mineralogy at the University of Münster studied traces of carbon trapped in small pieces of diamond and graphite within zircons dating to 4.25 Ga. The ratio of carbon-12 to carbon-13 was unusually high, normally a sign of \"processing\" by life.\n\nThree-dimensional computer models developed in May 2009 by a team at the University of Colorado at Boulder postulate that much of Earth's crust, and the microbes living in it, could have survived the bombardment. Their models suggest that although the surface of the Earth would have been sterilized, hydrothermal vents below the Earth's surface could have incubated life by providing a sanctuary for heat-loving microbes.\n\nIn April 2014, scientists reported finding evidence of the largest terrestrial meteor impact event to date near the Barberton Greenstone Belt. They estimated the impact occurred about 3.26 billion years ago and that the impactor was approximately 37 to 58 kilometres (23 to 36 miles) wide. The crater from this event, if it still exists, has not yet been found.\n\nIn the Nice model the Late Heavy Bombardment is the result of a dynamical instability in the outer Solar System. The original Nice model simulations by Gomes \"et al.\" began with the Solar System's giant planets in a tight orbital configuration surrounded by a rich trans-Neptunian belt. Objects from this belt stray into planet crossing orbits causing the orbits of the planets to migrate over several hundred million years. Jupiter and Saturn's orbits drift apart slowly until they cross a 2:1 orbital resonance causing the eccentricities of their orbits to increase. The orbits of the planets become unstable and Uranus and Neptune are scattered onto wider orbits that disrupt the outer belt, causing a bombardment of comets as they enter planet crossing orbits. Interactions between the objects and the planets also drive a faster migration of Jupiter and Saturn's orbits. This migration causes resonances to sweep through the asteroid belt, increasing the eccentricities of many asteroids until they enter the inner Solar System and impact the terrestrial planets.\n\nThe Nice model has undergone some modification since its initial publication. The giant planets now begin in a multi-resonant configuration due an early gas-driven migration through the protoplanetary disk. Interactions with the trans-Neptunian belt allow their escape from the resonances after several hundred million years. The encounters between planets that follow include one between an ice giant and Saturn the that propels the ice giant onto a Jupiter-crossing orbit followed by an encounter with Jupiter which drives the ice giant outward. This jumping-Jupiter scenario quickly increases the separation of Jupiter and Saturn, limiting the effects of resonance sweeping on the asteroids and the terrestrial planets. While this is required to preserve the low eccentricities of the terrestrial planets and avoid leaving the asteroid belt with too many high eccentricity asteroids, it also reduces the fraction of asteroids removed from the main asteroid belt, leaving a now nearly depleted inner band of asteroids as the primary source of the impactors of the LHB. The ice giant is often ejected following its encounter with Jupiter leading some to propose that the Solar System began with five giant planets. Recent works, however, have found that impacts from this inner asteroid belt would be insufficient to explain the formation of ancient impact spherule beds and the lunar basins, and that the asteroid belt was probably not the source of the Late Heavy Bombardment.\n\nAccording to one planetesimal simulation of the establishment of the planetary system, the outermost planets Uranus and Neptune formed very slowly, over a period of several billion years. Harold Levison and his team have also suggested that the relatively low density of material in the outer Solar System during planet formation would have greatly slowed their accretion.\nThis \"late appearance\" of these planets has therefore been suggested as a different reason for the LHB. However, recent calculations of gas-flows combined with planetesimal runaway growth in the outer Solar System imply that Jovian planets formed extremely rapidly, on the order of 10 My, which does not support this explanation for the LHB.\n\nThe Planet V hypothesis posits that a fifth terrestrial planet created the \"Late Heavy Bombardment\" when its meta-stable orbit entered the inner asteroid belt. The hypothetical fifth terrestrial planet, Planet V, had a mass less than half of Mars and originally orbited between Mars and the asteroid belt. Planet V's orbit became unstable due to perturbations from the other inner planets causing it to intersect the inner asteroid belt. After close encounters with Planet V, many asteroids entered Earth-crossing orbits producing the \"Late Heavy Bombardment\". Planet V was ultimately lost, likely plunging into the Sun. In numerical simulations, an uneven distribution of asteroids, with the asteroids heavily concentrated toward the inner asteroid belt, has been shown to be necessary to produce the LHB via this mechanism. An alternate version of this hypothesis in which the lunar impactors are debris resulting from Planet V impacting Mars, forming the Borealis Basin, has been proposed to explain a low number of giant lunar basins relative to craters and a lack of evidence of cometary impactors.\n\nA hypothesis proposed by Matija Ćuk posits that the last few basin-forming impacts were the result of the collisional disruption of a large Mars-crossing asteroid. This Vesta-sized asteroid was a remnant of a population which initially was much larger than the current main asteroid belt. Most of the pre-Imbrium impacts would have been due to these Mars-crossing objects, with the early bombardment extending until 4.1 billion years ago. A lull in basin-forming impacts then followed during which the lunar magnetic field decayed. Then roughly 3.9 billion years ago a catastrophic impact disrupted the Vesta-sized asteroid radically increasing the population of Mars-crossing objects. Many of these objects then evolved onto Earth-crossing orbits producing a spike in the lunar impact rate during which the last few lunar impact basins are formed. Ćuk points to the weak or absent residual magnetism of the last few basins and a change in the size-frequency distribution of craters which formed during this late bombardment as evidence supporting this hypothesis. The timing and the cause of the change in the size-frequency distribution of craters is controversial.\n\nA number of other possible sources of the \"Late Heavy Bombardment\" have been investigated. Among these are additional Earth satellites orbiting independently or as lunar trojans, planetesimals left over from the formations of the terrestrial planets, Earth or Venus co-orbitals, and the breakup of a large main belt asteroid. Additional Earth satellites on independent orbits were shown to be quickly captured into resonances during the Moon's early tidally-driven orbital expansion and were lost or destroyed within in a few million years Lunar trojans were found to be destabilized within 100 million years by a solar resonance when the Moon reached 27 Earth radii. Planetesimals left over from the formation of the terrestrial planets were shown to be depleted too rapidly due to collisions and ejections to form the last lunar basins. The long-term stability of primordial Earth or Venus co-orbitals (trojans or objects with horseshoe orbits) in conjunction with the lack of current observations indicate that they were unlikely to have been common enough to contribute to the LHB. Producing the LHB from the collisional disruption of a main belt asteroid was found to require at minimum a 1,000–1,500 km parent body with the most favorable initial conditions. Debris produced by collisions among inner planets, now lost, has also been proposed as a source of the LHB.\n\nEvidence has been found for Late Heavy Bombardment-like conditions around the star Eta Corvi.\n\n"}
{"id": "44213584", "url": "https://en.wikipedia.org/wiki?curid=44213584", "title": "Laura Bay Conservation Park", "text": "Laura Bay Conservation Park\n\nLaura Bay Conservation Park is a protected area located in the Australian state of South Australia on the west coast of Eyre Peninsula adjoining the headland of Laura Bay Point in the gazetted locality of Laura Bay about south east of the town of Ceduna. \n\nIt was proclaimed on 15 February 1973. The Laura Bay Conservation Reserve which have been dedicated as a conservation reserve under the state’s \"Crown Lands Act 1929\" on 11 November 1993 was added to the conservation park on 10 July 2014.\n\nThe conservation park is classified as an IUCN Category III protected area.\n\n\n"}
{"id": "1917699", "url": "https://en.wikipedia.org/wiki?curid=1917699", "title": "List of materials analysis methods", "text": "List of materials analysis methods\n\nList of materials analysis methods:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "33218761", "url": "https://en.wikipedia.org/wiki?curid=33218761", "title": "Lucy Blake", "text": "Lucy Blake\n\nLucy Blake is an American conservationist, President of the Northern Sierra Partnership. She was a 2000 MacArthur Fellow.\n\nShe founded the Sierra Business Council and won the Pat Brown Award.\nBlake works for the Department of Energy on energy and natural resources.\n"}
{"id": "16842914", "url": "https://en.wikipedia.org/wiki?curid=16842914", "title": "Matching distance", "text": "Matching distance\n\nIn mathematics, the matching distance is a metric on the space of size functions.\n\nThe core of the definition of matching distance is the observation that the\ninformation contained in a size function can be combinatorially stored in a formal series of lines and points of the plane, called respectively \"cornerlines\" and \"cornerpoints\".\n\nGiven two size functions formula_1 and formula_2, let formula_3 (resp. formula_4) be the multiset of\nall cornerpoints and cornerlines for formula_1 (resp. formula_2) counted with their\nmultiplicities, augmented by adding a countable infinity of points of the\ndiagonal formula_7.\n\nThe \"matching distance\" between formula_1 and formula_2 is given by\nformula_10\nwhere formula_11 varies among all the bijections between formula_3 and formula_4 and\n\nRoughly speaking, the matching distance formula_15\nbetween two size functions is the minimum, over all the matchings\nbetween the cornerpoints of the two size functions, of the maximum\nof the formula_16-distances between two matched cornerpoints. Since\ntwo size functions can have a different number of cornerpoints,\nthese can be also matched to points of the diagonal formula_17. Moreover, the definition of formula_18 implies that matching two points of the diagonal has no cost.\n\n"}
{"id": "20648", "url": "https://en.wikipedia.org/wiki?curid=20648", "title": "Melting", "text": "Melting\n\nMelting, or fusion, is a physical process that results in the phase transition of a substance from a solid to a liquid. This occurs when the internal energy of the solid increases, typically by the application of heat or pressure, which increases the substance's temperature to the melting point. At the melting point, the ordering of ions or molecules in the solid breaks down to a less ordered state, and the solid melts to become a liquid.\n\nSubstances in the molten state generally have reduced viscosity as the temperature increases. An exception to this principle is the element sulfur, whose viscosity increases to a point due to polymerization and then decreases with higher temperatures in its molten state.\n\nSome organic compounds melt through mesophases, states of partial order between solid and liquid.\n\nFrom a thermodynamics point of view, at the melting point the change in Gibbs free energy \"∆G\" of the substances is zero, but there are non-zero changes in the enthalpy (\"H\") and the entropy (\"S\"), known respectively as the enthalpy of fusion (or latent heat of fusion) and the entropy of fusion. Melting is therefore classified as a first-order phase transition. Melting occurs when the Gibbs free energy of the liquid becomes lower than the solid for that material. The temperature at which this occurs is dependent on the ambient pressure.\n\nLow-temperature helium is the only known exception to the general rule. Helium-3 has a negative enthalpy of fusion at temperatures below 0.3 K. Helium-4 also has a very slightly negative enthalpy of fusion below 0.8 K. This means that, at appropriate constant pressures, heat must be \"removed\" from these substances in order to melt them.\n\nAmong the theoretical criteria for melting, the Lindemann and Born criteria are those most frequently used as a basis to analyse the melting conditions . The Lindemann criterion states that melting occurs because of vibrational instability, e.g. crystals melt when the average amplitude of thermal vibrations of atoms is relatively high compared with interatomic distances, e.g. <\"δu\"> > \"δR\", where \"δu\" is the atomic displacement, the Lindemann parameter \"δ\" ≈ 0.20...0.25 and \"R\" is one-half of the inter-atomic distance. The Lindemann melting criterion is supported by experimental data both for crystalline materials and for glass-liquid transitions in amorphous materials. The Born criterion is based on rigidity catastrophe caused by the vanishing elastic shear modulus, e.g. when the crystal no longer has sufficient rigidity to mechanically withstand load.\n\nUnder a standard set of conditions, the melting point of a substance is a characteristic property. The melting point is often equal to the freezing point. However, under carefully created conditions, supercooling or superheating past the melting or freezing point can occur. Water on a very clean glass surface will often supercool several degrees below the freezing point without freezing. Fine emulsions of pure water have been cooled to −38 degrees Celsius without nucleation to form ice. Nucleation occurs due to fluctuations in the properties of the material. If the material is kept still there is often nothing (such a physical vibration) to trigger this change, and supercooling (or superheating) may occur. Thermodynamically, the supercooled liquid is in the metastable state with respect to the crystalline phase, and it is likely to crystallize suddenly.\n\nGlasses are amorphous solids which are usually fabricated when the molten material cools very rapidly to below its glass transition temperature, without sufficient time for a regular crystal lattice to form. Solids are characterised by a high degree of connectivity between their molecules, and fluids have lower connectivity of their structural blocks. Melting of a solid material can also be considered as a percolation via broken connections between particles e.g. connecting bonds. In this approach melting of an amorphous material occurs when the broken bonds form a percolation cluster with \"T\" dependent on quasi-equilibrium thermodynamic parameters of bonds e.g. on enthalpy (\"H\") and entropy (\"S\") of formation of bonds in a given system at given conditions:\n\nwhere \"f\" is the percolation threshold and \"R\" is the universal gas constant. Although \"H\" and \"S\" are not true equilibrium thermodynamic parameters and can depend on the cooling rate of a melt they can be found from available experimental data on viscosity of amorphous materials.\n\nEven below its melting point, quasi-liquid films can be observed on crystalline surfaces. The thickness of the film is temperature dependent. This effect is common for all crystalline materials. Pre-melting shows its effects in e.g. frost heave, the growth of snowflakes and, taking grain boundary interfaces into account, maybe even in the movement of glaciers.\n\nIn genetics, melting DNA means to separate the double-stranded DNA into two single strands by heating or the use of chemical agents, cf. polymerase chain reaction.\n\n"}
{"id": "28685700", "url": "https://en.wikipedia.org/wiki?curid=28685700", "title": "Montalto di Castro Photovoltaic Power Station", "text": "Montalto di Castro Photovoltaic Power Station\n\nThe Montalto di Castro photovoltaic power station is a 84 megawatt (MW) photovoltaic power station at Montalto di Castro in Viterbo province, Italy. The project was developed by the independent developer SunRay that was later acquired by SunPower. The park is the largest PV project in Italy, and among the largest in Europe.\n\nThe project was built in several phases. The first phase with a total capacity of 24 MW was connected in late 2009. It uses SunPower solar panels as well as its tracker systems. The second phase (8 MW) was commissioned in 2010, and the third and fourth phases, totaling 44 MW, were completed in December 2010, totaling 276,156 solar modules with 305 watt each.\n\nIn December 2010 SunPower has completed the sale of Montalto di Castro solar park to a consortium of international investors. SunPower designed and built the solar power plant and will provide ongoing operations and maintenance services for the new owners.\n\n"}
{"id": "54287754", "url": "https://en.wikipedia.org/wiki?curid=54287754", "title": "Mott–Schottky plot", "text": "Mott–Schottky plot\n\nIn semiconductor electrochemistry, a Mott–Schottky plot describes the reciprocal of the square of capacitance formula_1 versus the potential difference between bulk semiconductor and bulk electrolyte. In many theories, and in many experimental measurements, the plot is linear.The use of Mott–Schottky plots to determine system properties (such as flatband potential, doping density or Helmholtz capacitance) is termed Mott–Schottky analysis.\n\nConsider the semiconductor/electrolyte junction shown in Figure 1. Under applied bias voltage formula_2 the size of the depletion layer formula_3 is\n\nformula_4 (1)\n\nHere formula_5 is the dielectric constant, formula_6 is the elementary charge, formula_7 is the doping density, formula_8 is the built-in potential.\n\nThe depletion region contains positive charge compensated by ionic negative charge at the semiconductor surface (in the liquid electrolyte side). Charge separation forms a dielectric capacitor at the interface of the metal/semiconductor contact. We calculate the capacitance for an electrode area formula_9 as\n\nformula_10 (2)\n\nreplacing formula_11 as obtained from equation 1, the result of the capacitance per unit area is\n\nformula_12 (3)\n\na equation describing the capacitance of a capacitor constructed of two parallel plates both of area formula_9 separated by a distance formula_14.\n\nReplacing equation (3) in (1) we obtain the result\n\nformula_15 (4).\n\nTherefore, a representation of the reciprocal square capacitance, is a linear function of the voltage, which constitutes the Mott–Schottky plot as shown in Fig. 1c. The measurement of the Mott–Schottky plot brings us two important pieces of information.\n\n\nIn liquid junction the reference of potential is normally a standard reference electrode. In solid junctions, we can take as a reference the metal Fermi level, if the work function is known, which provides a full energy diagram in the physical scale. The Mott–Schottky plot is sensitive to the electrode surface in contact with solution, see Figure 2.\n\nA more accurate analysis considering the statistics of electrons provides the following result for the size of the depletion region\n\nformula_16(7)\n\nThe derivative gives the doping at the edge of the depletion region, formula_17. This method only provides a spatial resolution of the order of a Debye length formula_18.\n\nIn systems where more than one process gives a substantial kinetic response, it is necessary to adopt Electrochemical Impedance Spectroscopy that resolves the different capacitances in the system. For example, in the presence of a surface state at the semiconductor/electrolyte interface, the spectra show two arcs, one at low frequency and another one at high frequency. The depletion capacitance leading to Mott–Schottky plot is situated in the high frequency arc, as the depletion capacitance is a dielectric capacitance. On the other hand, the low frequency feature corresponds to the chemical capacitance of the surface states. The surface state charging produces a plateau as indicated in Fig. 1d. Similarly, defect levels in the gap affect the changes of capacitance and conductance.\n\nAnother widely used method to scan deep levels in Schottky barriers is termed admittance spectroscopy and consists on measuring the capacitance at a fixed frequency while varying the temperature.\n\nSurface photovoltage technique is used to determine the position of the band edges.\n"}
{"id": "8094342", "url": "https://en.wikipedia.org/wiki?curid=8094342", "title": "November 2006 nor'easter", "text": "November 2006 nor'easter\n\nThe November 2006 nor'easter was a powerful extratropical cyclone that formed offshore of the Southeastern United States on November 20, bringing heavy rains, high winds, beach erosion, and coastal flooding to the Carolinas and southern New England. In addition, the earliest snowfall ever noted in both Charleston, South Carolina and Savannah, Georgia occurred on the southwest side of this cyclone. Over 10,000 were without power during the storm. No longer a nor'easter, the extratropical cyclone accelerated rapidly across the North Atlantic while rapidly strengthening, becoming a cyclonic storm again by November 25, but this time with hurricane-force sustained winds. The intense low made a cyclonic loop west of Iceland, before being absorbed by another strengthening extratropical cyclone to the west of Great Britain, late on December 1.\n\nA shortwave moved through the southern stream of the polar jet of the Westerlies, dropping temperatures in its wake across the southeast United States. Cyclogenesis spurred a developing surface cyclone which quickly occluded on November 20. Surface pressure gradient between this cyclone and a sprawling high stretching around its periphery from New England into the Deep South led to strong winds in the Carolinas. he cyclone deepened to a central pressure of before swinging westward into the Outer Banks of North Carolina on November 22. At this time, a trio of upper-level shortwaves were rotating around the main closed cyclone aloft, which spurred development of a new, weaker low pressure center offshore North Carolina. This cyclone was weaker after its interaction with North Carolina until it reached New England, when renewed strengthening led to significant impacts across Long Island and southern New England. The system then began accelerating east-northeast while continuing to slowly strengthen, passing offshore Atlantic Canada on November 24. The cyclone subsequently bombed, or strengthened quite rapidly, and accelerated east-northeast, becoming a hurricane-force storm in the far north Atlantic with a central pressure of by the afternoon of November 25 and peaking at by the morning of November 26. Slow weakening occurred soon afterward as the system slowed down and turned towards Iceland. By the evening of November 27, its central pressure had increased to while located a couple hundred miles south of Iceland. The cyclone continued to loop cyclonically west of Iceland, absorbing a cyclone along the way as it passed southwest off the southern tip of Greenland, before a developing gale approaching Great Britain absorbed this cyclone late on December 1.\n\nThere had been indications in the forecast model guidance as early as the first week of November that weather would be unsettled/rainy across the eastern United States around November 23. Within seven days of the event, medium range forecasts generally maintained the idea of a coastal storm offshore the Southeast, though its progression out to sea remained uncertain until November 21, after the cyclone actually formed offshore the Southeast United States and began edging northeast.\n\nOn the Chesapeake Bay Bridge-Tunnel, officials restricted the types of vehicles that could cross the span between Virginia Beach and the Eastern Shore to cars and pickup trucks. Poquoson Public Schools were closed, and the Midtown Tunnel between Norfolk and Portsmouth was shut down because of the possibility of flooding. Sandbags surrounded buildings at Langley Air Force Base, which lies above sea level, and all computers and electrical equipment were waterproofed.\n\nThe Maryland State Highway Administration worked to clear storm drains of branches and leaves and called in more patrollers to assist motorists in the event of an accident or stranded vehicle. The administration also prepared for falling branches, by making sure the department's chain saws had enough gas.\n\nSnow flurries fell as far south as central Florida on the backside of this system. Snow flurries were reported in Orange, Seminole, and Volusia Counties. It was the earliest snow had ever fallen this far south, and only the second time on record snow had fallen in Florida in November.\n\nSnow mixed in with rain around noon on November 21, which led to the earliest trace of snow on record in Savannah. Just inland, 5–7 inches of snow fell across extreme southeast Georgia.\n\nHeavy rains fell across northern portions of the state, with measured at Chester. Snow mixed in with rain during the morning of November 21, leading to the earliest trace of snow on record in Charleston. Also, as the snow mixed in, thunder was heard, which is the first report of thundersnow in the history of Charleston. An inch of snow was reported just inland. Winds gusted to 44 mph (38 knots) at Folly Beach. The combination of pounding surf and high winds led to moderate to heavy beach erosion at Hunting Island, Folly Beach, Isle of Palms, Wild Dunes, and Sullivan's Island.\n\nHeavy rainfall fell throughout central and eastern North Carolina, with the maximum amount of 7.68 inches (195 mm) measured at Sandy Run. This precipitation combined with high tides led to river flooding across the eastern half of the state, and led to Raleigh-Durham International Airport setting a record for its wettest November on record, breaking the record from 1948. The combination of wind and rain led to the downing of numerous trees. Winds gusted to 70 knots (80 mph) at Alligator River, with numerous gusts above 50 knots (60 mph) throughout the Outer Banks, leading to beach erosion and coastal flooding, with Highway 12 being overwashed by the surge south of Oregon Inlet. Power went out to 1,250 in northeastern sections of the state. The combination of coastal flooding and heavy rains led to the Lumber River rising to above flood stage. In Nags Head, 54 homes were condemned due to damage from this cyclone.\nAt sea, the container ship Courtney L lost four containers on November 22 or 23 due to the storm. One of them, which contained Doritos bound for Costa Rica, was eventually washed ashore near Cape Hatteras.\n\nHigh winds lashed extreme southeast sections of the state. The highest wind gust reported was 66 mph (57 knots) from the first island on the Chesapeake Bay Bridge-Tunnel at 9:45 a.m. EST. In southeast sections of the state, over 9000 went without power. Moderate to heavy rain fell across the tidewater, with 1.97 inches (50 mm). One person perished when the car he was driving hydroplaned and subsequently crashed.\n\nA brief period of sleet was experienced across Northern Virginia on the morning of November 22 as moisture from the storm moved in from a northeast-to-southwest track. This form of precipitation is rare for the area in mid-November, but was quickly replaced by driving rainfall as warmer air from the Atlantic pushed inland. Steady rain persisted throughout the afternoon and evening of the 22nd, with a mix of showers and moderate-to-heavy mist occurring throughout Thanksgiving Day, November 23.\n\nWinds as high as 54 mph (47 knots) lashed Assateague Island. The highest rainfall amount reported was 2.12 inches (54 mm) from Ocean City.\n\nGeorgetown reported a record rainfall for November 22, when 1.93 inches (49 mm) was measured; a total of 2.50 inches (64 mm) fell.\n\nRainfall amounts ranged at high as 1.21 inches (31 mm) at Lehighton.\n\nWinds peaked at 57 mph (50 knots) at both Cape May Harbor and Ocean City. At Newark, the 1.26 inches (32 mm) that fell on November 23 set a daily precipitation record for the date. The highest amount noted was 2.57 inches (65 mm) at the United States Coast Guard site in Atlantic City.\n\nAn oak tree toppled onto a house located in West Deptford. A tree also got caught in a power line in Vivian Court, so the electric power had to be cut, so the power line could be cleared of the tree. A traffic light and some trees were also damaged in Northfield. Minor flooding was also reported in some back-bays, which forced some intersections to be closed.\n\nLow level wind shear led to moderate delays of an hour or more at major airports within the megalopolis on the morning of November 22, the day before Thanksgiving in the United States.\nAt Kennedy Airport, 1.56 inches (40 mm) of rainfall set a daily precipitation record for November 23. The highest amount reported was 2.83 inches (72 mm) at Shoreham. Around Binghamton, a light coating of ice covered surfaces as a slight amount of freezing rain fell.\n\nAt Bridgeport, rainfall of 1.84 inches (47 mm) fell on November 23, setting a daily rainfall record. The highest amount reported was 3.27 inches (75 mm) at Norwich.\n\nHigh winds downed a telephone pole in Warwick, and numerous tree limbs throughout the state. Downtown Providence reported 3.91 inches (99 mm) of rainfall on November 23 and November 24. This rain led to minor flooding of the Pawtuxet River in Cranston, an area hard-hit by the flooding in October, 2005.\n\nHigh winds downed numerous tree limbs throughout the state. A total of 2.09 inches (53 mm) of rainfall fell in Boston as of the morning of November 24. The highest amount reported was at Hingham.\n\n"}
{"id": "30780597", "url": "https://en.wikipedia.org/wiki?curid=30780597", "title": "Nuclear Institute for Agriculture and Biology", "text": "Nuclear Institute for Agriculture and Biology\n\nThe Nuclear Institute for Agriculture and Biology, also known as NIAB, is an agriculture and food irradiation national research institute managed by the Pakistan Atomic Energy Commission. Along with Nuclear Institute for Food and Agriculture (NIFA), the NIAB reports directly to the PAEC Biological Science Directorate whose current member is Abdul Rashid. The current director is Dr.Muhammad Hamed, and it is located in Faisalabad, Punjab, Pakistan.\n\nThe NIAB was established by Ishrat Hussain Usmani when PAEC established its first Biological Science Directorate in 1965. In 1967, with the efforts led by dr. Abdus Salam, the Government approved a project, and Pakistan Atomic Energy Commission began its construction. The operations and research began in 1970, and it was officially inaugurated by Munir Ahmad Khan, then Chairman of the Pakistan Atomic Energy Commission, in April 6, 1972. Khan later developed the institute and led the research activities in the institution. The nuclear medical research was also put under Khan, and NIAB had developed 23 different crop varieties, which are high yielding; they are disease resistant and are being cultivated throughout the country.\n\nAt first, the institute was mandate to create and maintain new genetic material for sustained agriculture development and to conduct research on applied problems in the field of agriculture and biology using nuclear and other related techniques.\n\nThe institute is equipped with well-equipped laboratories having facilities. The institute currently operates Co irradiation sources, gas chromatographs, Photo-documentation system, and atomic absorption.\n\n"}
{"id": "1551157", "url": "https://en.wikipedia.org/wiki?curid=1551157", "title": "Operative temperature", "text": "Operative temperature\n\nOperative temperature (formula_1), also known as \"Dry resultant temperature\", or \"Resultant temperature\", is defined as a uniform temperature of an imaginary black enclosure in which an occupant would exchange the same amount of heat by radiation plus convection as in the actual nonuniform environment. Some references also use the terms 'equivalent temperature\" or 'effective temperature' to describe combined effects of convective and radiant heat transfer. In design, operative temperature can be defined as the average of the mean radiant and ambient air temperatures, weighted by their respective heat transfer coefficients. The instrument used for assessing environmental thermal comfort in terms of operative temperature is called a eupatheoscope and was invented by A. F. Dufton in 1929. Mathematically, operative temperature can be shown as;\n\nwhere,\n\nOr\n\nwhere,\n\nIt is also acceptable to approximate this relationship for occupants engaged in near sedentary physical activity (with metabolic rates between 1.0 met and 1.3 met), not in direct sunlight, and not exposed to air velocities greater than 0.10 m/s (20 fpm). \n\nwhere formula_5 and formula_6 have the same meaning as above.\n\nOperative temperature is used in heat transfer and thermal comfort analysis in transportation and buildings. Most psychrometric charts used in HVAC design only show the dry bulb temperature on the x-axis(abscissa), however, it is the operative temperature which is specified on the x-axis of the psychrometric chart illustrated in ANSI/ASHRAE Standard 55 – Thermal Environmental Conditions for Human occupancy.\n\n"}
{"id": "14738913", "url": "https://en.wikipedia.org/wiki?curid=14738913", "title": "Peterson Field Guides", "text": "Peterson Field Guides\n\nThe Peterson Field Guides (PFG) are a popular and influential series of American field guides intended to assist the layman in identification of birds, plants, insects and other natural phenomena. The series was created and edited by renowned ornithologist Roger Tory Peterson (1908–1996). His inaugural volume was the classic 1934 book \"A Field Guide to the Birds\", published (as were all subsequent volumes) by the Houghton Mifflin Company.\n\nThe PFG series utilized what became known as the Peterson Identification System, a practical method for field identification which highlights readily noticed visual features rather than focusing on the technical features of interest to scientists. The series both reflected and contributed to awareness of the emerging environmental movement.\n\nMost books in this series use a section of plates of drawings (usually reduced from commissioned paintings) rather than photographs of the subject species, grouped at the center of the book. This allows for idealized portraits that highlight the identifying \"field marks\" of each species; such field marks are often indicated by arrows or straight lines in the plate illustrations. However, in several books in this series, the plates consist of photographs (usually without such arrows or indicators), such as in the guides for the atmosphere, coral reefs, rocks and minerals, and the (old Charles Covell 1984 guide to) Eastern moths. In many books in this series (especially older editions), a number of the plates are in black and white. For examples, older editions of the Eastern reptiles/amphibians book had many black and white plates which were colorized for the current edition, and the original 1934 Eastern bird book had only 4 color plates. At least one book (insects) was entirely in black and white. However, most newer editions are often full-color (or almost full-color) and tend to be larger. One source claims that the increased size of one of the new editions (Eastern reptiles/amphibians) was considered detrimental to its use as a field guide by its own author and was a publisher decision.\n\nIn some cases, new \"editions\" in this series are entirely new books with completely new texts and illustrations. For example, the fourth edition of the mammals guide has an entirely new text and illustrations by new author Fiona Reid, because the author (William Burt) and illustrator (Richard Grossenheider) of previous editions are both deceased. In fact, Grossenheider died prior to the publication of the \"previous\" third edition of 1976. Also, the current Northeastern moths guide by David Beadle and Seabrooke Leckie is an entirely new book than the out-of-print 1984 Eastern moths guide by Charles Covell. The Beadle/Leckie book covers a smaller geographical area and (one author claims) covers moths in greater detail. The old Covell book has been out-of-print for many years, but is currently available through the Virginia Museum of Natural History (which purchased the rights to that book).\n\nThe above situation of an old \"edition\" persisting alongside its intended replacement edition is not unique to the Eastern moths guide. George Petrides' 1988 Eastern trees book (PFG11B) was originally intended to replace Petrides' own 1958 Eastern tree and shrubs (PFG11A) book. However, both books remain popular and the original publisher still offers both books for sale (unlike the case of the old Eastern moths book).\n\nDifferences between editions can serve to indicate changes in scientific perspective as well as changes species distribution. For example, the second edition of the freshwater fishes guide by Page and Burr (2011), published 20 years after the first edition, increased the number of species included from 768 to 909, largely due to the addition of previously unrecognized species (114), as well as increased numbers of newly established exotic species (16). It also expanded coverage of marine fish commonly found in freshwater (19).\n\nBoth of these guides appeared in the Easton Press leather bound copies of the series. For that series the title of the Bond book was changed to \"Birds of the Caribbean\".\n \"Birds of the West Indies\" (1999), by James Bond\n\n\nOther volumes:\n\nAppweavers, Inc., the licensee of the Peterson field guides for mobile devices, has developed the Peterson Birds of North America and Peterson Feeder Birds of North America apps for mobile Apple products. The Peterson Birds of North America app also includes some content from other books in the Peterson field guide series.\n\n"}
{"id": "497219", "url": "https://en.wikipedia.org/wiki?curid=497219", "title": "Sound power", "text": "Sound power\n\nSound power or acoustic power is the rate at which sound energy is emitted, reflected, transmitted or received, per unit time. The SI unit of sound power is the watt (W). It is the power of the sound force on a surface of the medium of propagation of the sound wave. For a sound source, unlike sound pressure, sound power is neither room-dependent nor distance-dependent. Sound pressure is a property of the field at a point in space, while sound power is a property of a sound source, equal to the total power emitted by that source in all directions. Sound power passing through an area is sometimes called sound flux or acoustic flux through that area.\n\nRegulations control the maximum sound power level L that a device (e.g., a vacuum cleaner) is allowed to produce. The A-weighting scale is used in the calculation as the regulation is concerned with the loudness as perceived by the human ear. Measurements are taken at several defined points around the device.\n\nThe test environment can be located indoors or outdoors. The ideal environment is on the ground in a large open space or hemi-anechoic chamber (free-field over a reflecting plane). To account for undesired reflections from nearby objects, walls, and the ceiling, and for any residual background noises, measurement corrections are applied.\n\nHere is a table of some examples.\n\nSound power, denoted \"P\", is defined by\nwhere\n\nIn a medium, the sound power is given by\nwhere\n\nFor example, a sound at SPL = 85 dB or \"p\" = 0.356 Pa in air (\"ρ\" = 1.2 kg·m and \"c\" = 343 m·s) through a surface of area \"A\" = 1 m normal to the direction of propagation (\"θ\" = 0 °) has a sound energy flux \"P\" = 0.3 mW.\n\nThis is the parameter one would be interested in when converting noise back into usable energy, along with any losses in the capturing device.\n\nSound power is related to sound intensity:\nwhere\n\nSound power is related sound energy density:\nwhere\n\nSound power level (SWL) or acoustic power level is a logarithmic measure of the power of a sound relative to a reference value.<br>\nSound power level, denoted \"L\" and measured in dB, is defined by\nwhere\n\nThe commonly used reference sound power in air is\nThe proper notations for sound power level using this reference are or , but the suffix notations , , dBSWL, or dB are very common, even if they are not accepted by the SI.\n\nThe reference sound power \"P\" is defined as the sound power with the reference sound intensity passing through a surface of area :\nhence the reference value .\n\nThe generic calculation of sound power from sound pressure is as follows:\nwhere:\nformula_9 defines the area of a surface that wholly encompasses the source. This surface may be any shape, but it must fully enclose the source. \nIn the case of a sound source located in free field positioned over a reflecting plane (i.e. the ground), in air at ambient temperature, the sound power level at distance \"r\" from the sound source is approximately related to sound pressure level (SPL) by\nwhere\n\nDerivation of this equation:\nFor a \"progressive\" spherical wave,\nwhere \"z\" is the characteristic specific acoustic impedance.\n\nConsequently,\nand since by definition , where is the reference sound pressure,\n\nThe sound power estimated practically does not depend on distance. The sound pressure used in the calculation may be affected by distance due to viscous effects in the propagation of sound unless this is accounted for.\n\n"}
{"id": "23240658", "url": "https://en.wikipedia.org/wiki?curid=23240658", "title": "Sparse matrix converter", "text": "Sparse matrix converter\n\nThe Sparse Matrix Converter is an AC/AC converter which offers a reduced number of components, a low-complexity modulation scheme, and low realization effort \n, sparse matrix converters avoid the multi step commutation procedure of the conventional matrix converter, improving system reliability in industrial operations. Its principal application is in highly compact integrated AC drives.\n\n\nMatrix converter is a device which converts AC input supply to the required variable AC supply as output without any intermediate conversion process whereas in case of Inverter which converts AC - DC - AC which takes more extra components as diode rectifiers, filters, charge-up circuit but not needed those in case of matrix converters\n\nCharacteristics of the Sparse Matrix Converter topology are 15 Transistors, 18 Diodes, and 7 Isolated Driver Potentials. Compared to the Direct matrix converter this topology provides identical functionality, but with a reduced number of power switches and the option of employing an improved zero DC-link current commutation scheme, which provides lower control complexity and higher safety and reliability.\n\nCharacteristics of the Very Sparse Matrix Converter topology are 12 Transistors, 30 Diodes, and 10 Isolated Driver Potentials. There are no limitations in functionality compared to the Direct Matrix Converter and Sparse Matrix Converter. Compared to the Sparse Matrix Converter there are fewer transistors but higher conduction losses due to the increased number of diodes in the conduction paths.\n\nCharacteristics of the Ultra Sparse Matrix Converter topology are 9 Transistors, 18 Diodes, and 7 Isolated Driver Potentials. The significant limitation of this converter topology compared to the Sparse Matrix Converter is the restriction of its maximal phase displacement between input voltage and input current which is restricted to ± 30°.\n\nThis is a commutation scheme, depicted in Fig. 4. For a given switching state of the rectifier input stage, the commutation of the inverter output stage has to be performed in an identical manner to the commutation of a conventional voltage dc-link converter. The basic structure of the commutating bridge legs of the Sparse Matrix Converter is shown in Fig. 4(a). The switch sequence to change the connection of the positive dc-link voltage bus p from input a to input b is shown in Fig. 4(b) and Fig. 4(c). In Fig. 4(b) the assumption is current-independent commutation with uab > 0. In Fig. 4(c) the assumption is voltage-independent commutation with i > 0. \n\nA dead time between the turn-off and turn-on of the power transistors of a bridge leg has to be implemented in order to avoid a short circuit of the dc-link voltage. To change the switching state of the Sparse Matrix Converter rectifier input stage for a given inverter switching state, one has to make sure that there is no bidirectional connection between any two input lines. This guarantees that no short-circuiting of an input line-to-line voltage can occur. Additionally a current path must be continuously provided. Therefore multistep commutation schemes, using voltage independent and current independent commutation as known for the Conventional Direct Matrix Converter \n\n, can be employed.\n\nThe drawback of the multistep commutation describe before is its complexity. Indirect matrix converters like the Sparse Matrix Converter provide a degree of control freedom that is not available for the Conventional Direct Matrix Converter. This can be used to simplify the complex commutation problem. It has been proposed to switch the inverter stage into a free-wheeling state, and then to commutate the rectifier stage with zero dc-link current. This is shown in Fig. 5. \n\nFig. 5(a) shows the control of the power transistors in one bridge leg of the Sparse Matrix Converter. Fig. 5(b) shows the switching state sequence where s0; s7 = 1 indicates free-wheeling operation of the inverter stage. Furthermore, the dc-link current i is shown. \n\nThe zero DC link current commutation scheme gives the additional benefit of a reduction in the switching losses of the input stage. One only has to ensure that no overlapping of turn-on intervals of power transistors in a bridge half occurs, because this would result in a short circuit of an input line-to-line voltage. \n\nFig 6 shows the formation of the dc-link voltage u and dc-link current i within one switching period \nFurthermore, it shows as an example the switching functions of the rectifier and inverter stage for formula_1 in interval formula_2and formula_3 in interval formula_4. Input stage switching occurs at zero dc-link current. The dc-link current has a constant average value formula_5 within formula_6 and formula_7. The switching state functions are given as formula_8, and formula_9. The switching frequency ripple of formula_10 and formula_11 is neglected\n"}
{"id": "699437", "url": "https://en.wikipedia.org/wiki?curid=699437", "title": "Structural insulated panel", "text": "Structural insulated panel\n\nA structural insulated panel, or structural insulating panel, (SIP), is a form of sandwich panel used in the construction industry.\n\nSIP is a sandwich structured composite, consisting of an insulating layer of rigid core sandwiched between two layers of structural board, used as a building material. The board can be sheet metal, plywood, cement, magnesium oxide board (MgO) or oriented strand board (OSB) and the core either expanded polystyrene foam (EPS), extruded polystyrene foam (XPS), polyisocyanurate foam, polyurethane foam or composite honeycomb (HSC).\n\nSIPs share the same structural properties as an I-beam or I-column. The rigid insulation core of the SIP acts as a web, while the sheathing fulfills the function of the flanges. SIPs combine several components of conventional building, such as studs and joists, insulation, vapor barrier and air barrier. They can be used for many different applications, such as exterior wall, roof, floor and foundation systems.\n\nAlthough foam-core panels gained attention in the 1970s, the idea of using stress skinned panels for construction began in the 1930s. Research and testing of the technology was done primarily by Forest Products Laboratory (FPL) in Madison, Wisconsin as part of a U.S. Forest Service attempt to conserve forest resources. In 1937, a small stressed-skin house was constructed and garnered enough attention to bring in First Lady Eleanor Roosevelt to dedicate the house. In a testament to the durability of such panel structures, it has endured the severe Wisconsin climate and was used by University of Wisconsin–Madison as a day care center up until 1998 when it was removed to make way for a new Pharmacy School building. With the success of the stress skinned panels, it was suggested stronger skins could take all of the structural load and eliminate the frame altogether.\n\nThus in 1947, structural insulated panel development began when corrugated paperboard cores were tested with various skin materials of plywood, tempered hardboard and treated paperboard. The building was dismantled in 1978 and most of the panels retained their original strength with the exception of paperboard which is unsuited to outdoor exposure. Panels consisting of polystyrene core and paper overlaid with plywood skins were used in a building in 1967 and the panels have performed well to the present day.\n\nSIP systems were used by Woods Constructors of Santa Paula, California in their homes and apartments from 1965 until 1984. This work was the basis for John Thomas Woods, Paul Flather Woods, John David Woods, and Frederick Thomas Woods when they used a similar concept to patent the Footing Form for Modular homes (US Patent #4817353) issued on April 4, 1989. Numerous homes in Santa Paula, Fillmore, Palm Springs, and surrounding areas use SIP panels as the primary method of construction. The design was awarded approval from (then) ICBO and SBCCI, now ICC.\n\nSIPs are most commonly made of OSB panels sandwiched around a foam core made of expanded polystyrene (EPS), extruded polystyrene (XPS) or rigid polyurethane foam. Other materials can be used in replacement of OSB, such as plywood, pressure-treated plywood for below-grade foundation walls, steel, aluminum, cement board such as Hardiebacker, and even exotic materials like stainless steel, fiber-reinforced plastic, and magnesium oxide. Some SIPs use fiber-cement or plywood sheets for the panels, and agricultural fiber, such as wheat straw, for the core.\n\nThe third component in SIPs is the spline or connector piece between SIP panels. Dimensional lumber is commonly used but creates thermal bridging and lowers insulation values. To maintain higher insulation values through the spline, manufacturers use Insulated Lumber, Composite Splines, Mechanical Locks, Overlapping OSB Panels, or other creative methods. Depending on the method selected, other advantages such as full nailing surfaces or increased structural strength may become available.\n\nSIP's are most often manufactured in a traditional factory. Processing equipment is used to regulate pressures and heat in a uniform and consistent manner. There are two main processing methods which correspond to the materials used for the SIP Panel core. When manufacturing a panel with a polystyrene core both pressure and heat are required to ensure the bonding glue has penetrated and set completely. Although a number of variations exist, in general, the foam core is first covered with an adhesive and the skin is set in place. The three pieces are set into a large clamping device and pressure and heat are applied. The three pieces must stay in the clamping device until the glue has cured.\n\nWhen manufacturing a panel with a polyurethane core pressure and heat are both generated from the expansion of the foam during the foaming process. The skins are set in a large clamping device which functions as a mold. The skins must be held apart from each other to allow the liquid polyurethane materials to flow into the device. Once in the device, the foam begins to rise. The mold/press is generally configured to withstand the heat and the pressures generated from the chemical foaming. The SIP Panel is left in the mold/press to cure slightly and when removed will continue to cure for several days.\n\nUntil recently, both of these processes required a factory setting. However, recent advancements have presented an alternative with SIP Panel processing equipment that allows SIP Panels to be manufactured on the job-site. This is welcome news for builders in developing countries where the technology may be best suited to reduce greenhouse emissions and improve sustainability in housing but are unavailable.\n\nThe use of SIPs brings many benefits and some drawbacks compared to a conventional framed building.\n\nThe cost of SIPs are higher than the materials for a comparable framed building in the United States; however, this may not be true elsewhere. A well-built home using SIPs will have a tighter building envelope and the walls will have higher insulating properties, which leads to fewer draughts and a decrease in operating costs. Also, due to the standardized and all-in-one nature of SIPs, construction time can be less than for a frame home, as well as requiring fewer tradespeople. The panels can be used as floor, wall, and roof, with the use of the panels as floors being of particular benefit when used above an uninsulated space below. As a result, the total life-cycle cost of a SIP-constructed building will, in general, be lower than for a conventional framed one—by as much as 40%. Whether the total construction cost (materials and labor) is lower than for conventional framing appears to depend on the circumstances, including local labor conditions and the degree to which the building design is optimized for one or the other technology.\n\nAn OSB skinned system structurally outperforms conventional stick framed construction in some cases; primarily in axial load strength. SIPs maintain similar versatility to stick framed houses when incorporating custom designs. Also, since SIPs work as framing, insulation, and exterior sheathing, and can come precut from the factory for the specific job, the exterior building envelope can be built quite quickly. SIPs panels also tend to be lightweight and compact which aids this offsite construction. The environmental performance of SIPs, moreover, is very good due to their exceptional thermal insulation. They also offer a resistance to damp and cold problems like compression shrinkage and cold bridging that cannot be matched by timber and more traditional building materials.\n\nWhen tested under laboratory conditions, the SIP, included in a wall, foundation, floor, or roof system, is installed in a steady-state (no air infiltration) environment; systems incorporating fiberglass insulation are not installed in steady-state environments as they require ventilation to remove moisture.\n\nWith the exception of structural metals, such as steel, all structural materials creep over time. In the case of SIPs, the creep potential of OSB faced SIPs with EPS or polyurethane foam cores has been studied and creep design recommendations exist. The long-term effects of using unconventional facing and core materials require material specific testing to quantify creep design values.\n\nIn the United States, SIPs tend to come in sizes from 4 feet (1.22 m) to 24 feet (7.32 m) in width. Elsewhere, typical product dimensions are 300, 600, or 1,200 mm wide and 2.4, 2.7, and 3 m long, with roof SIPs up to 6 m long. Smaller sections ease transportation and handling, but the use of the largest panel possible will create the best insulated building. At 15−20 kg/m², longer panels can become difficult to handle without the use of a crane to position them, and this is a consideration that must be taken into account due to cost and site limitations. Also of note is that when needed for special circumstances longer spans can often be requested, such as for a long roof span. Typical U.S. height for panels is eight or nine feet (2.44 to 2.75 m). Panels come in widths ranging from 4 to 12 inches thick and a rough cost is $4–$6/ft in the U.S. In 4Q 2010, new methods of forming radius, sine curve, arches and tubular SIPs were commercialized. Due to the custom nature and technical difficulty of forming and curing specialty shapes, pricing is typically three or four times that of standard panels per foot.\n\nEPS is the most common of the foams used and has an R-value (thermal resistance) of about 4 K·m/W per 25 mm thickness, which would give the of foam in a panel an R value of 13.8 (caution: extrapolating R-values over thickness may be imprecise due to non-linear thermal properties of most materials). This at face value appears to be comparable to an R-13 batt of fiberglass, but because in a standard stick frame house there is significantly more wall containing low R value wood that acts as a cold bridge, the thermal performance of the R-13.8 SIP wall will be considerably better.\n\nThe air sealing features of SIP homes resulted in the Environmental Protection Agency's Energy Star program to establish an inspection protocol in lieu of the typically required blower door test to assess the home's air leakage. This serves to speed the process and save the builder/homeowner money.\n\nThe International Building Code references APA, Plywood Design Specification 4—Design & Fabrication of Plywood Sandwich Panels for the design of SIPs. This document addressed the basic engineering mechanics of SIP panels but does not provide design properties for the panels provided by any specific manufacturer. In 2007, prescriptive design provisions for OSB faced SIPs were first introduced in the 2006 International Residential Code. These provisions provide guidance on the use of SIPs as walls panels only.\n\nAside from these non-proprietary standards, the SIP industry has relied heavily on proprietary code evaluation reports. In early 2009, SIPA partnered with NTA Inc, a product certification agency, to produce the first industry wide code report which is available to all SIPA members who qualify. Unlike previous code reports, the prescriptive provisions provided in the SIPA code report are derived from an engineering design methodology which permits the design professional to consider loading conditions not addressed in the code report.\n\n"}
{"id": "22948543", "url": "https://en.wikipedia.org/wiki?curid=22948543", "title": "Tetrafluoroammonium", "text": "Tetrafluoroammonium\n\nThe tetrafluoroammonium cation (also known as perfluoroammonium) is a positively charged polyatomic ion with chemical formula . It is equivalent to the ammonium ion where the hydrogen atoms surrounding the central nitrogen atom have been replaced by fluorine. Tetrafluoroammonium ion is isoelectronic with tetrafluoromethane and the tetrafluoroborate anion.\n\nThe tetrafluoroammonium ion forms salts with a large variety of fluorine-bearing anions. These include the bifluoride anion (), tetrafluorobromate (), metal pentafluorides ( where X is Ge, Sn, or Ti), hexafluorides ( where X is P, As, Sb, Bi, or Pt), heptafluorides ( where X is W, U, or Xe), octafluorides (), various oxyfluorides ( where X is W or U; , ), and perchlorate (). Attempts to make the nitrate salt, , were unsuccessful because of quick fluorination: + → + .\n\nThe geometry of the tetrafluoroammonium ion is tetrahedral, with an estimated nitrogen-fluorine bond length of 124 pm. All fluorine atoms are in equivalent positions.\n\nTetrafluoroammonium salts are prepared by oxidising nitrogen trifluoride with fluorine in the presence of a strong Lewis acid which acts as a fluoride ion acceptor. The original synthesis by Tolberg, Rewick, Stringham, and Hill in 1966 employs antimony pentafluoride as the Lewis acid:\n\nThe hexafluoroarsenate salt was also prepared by a similar reaction with arsenic pentafluoride at 120 °C:\n\nThe reaction of nitrogen trifluoride with fluorine and boron trifluoride at 800 °C yields the tetrafluoroborate salt:\n\nMany tetrafluoroammonium salts can be prepared with metathesis reactions.\n\nTetrafluoroammonium salts are extremely hygroscopic. The ion is readily hydrolysed into nitrogen trifluoride, , and oxygen gas:\n\nSome hydrogen peroxide () is also formed during this process.\n\nReaction of with alkali metal nitrates yields fluorine nitrate, .\n\nBecause tetrafluoroammonium salts are destroyed by water, it cannot be used as a solvent. Instead anhydrous hydrogen fluoride or bromine pentafluoride can be used as a solvent to dissolve these salts.\n\nTetrafluoroammonium salts usually have no colour. However some are coloured due to other metals in them. Red salts include , and . , , and are yellow.\n\n salts are important for solid propellant gas generators. They are also used as reagents for electrophilic fluorination of aromatic compounds in organic chemistry.\n\n"}
{"id": "26205122", "url": "https://en.wikipedia.org/wiki?curid=26205122", "title": "Unit hyperbola", "text": "Unit hyperbola\n\nIn geometry, the unit hyperbola is the set of points (\"x,y\") in the Cartesian plane that satisfy the implicit equation formula_1 In the study of indefinite orthogonal groups, the unit hyperbola forms the basis for an \"alternative radial length\"\nWhereas the unit circle surrounds its center, the unit hyperbola requires the \"conjugate hyperbola\" formula_3 to complement it in the plane. This pair of hyperbolas share the asymptotes \"y\" = \"x\" and \"y\" = −\"x\". When the conjugate of the unit hyperbola is in use, the alternative radial length is formula_4\n\nThe unit hyperbola is a special case of the rectangular hyperbola, with a particular orientation, location, and scale. As such, its eccentricity equals formula_5\n\nThe unit hyperbola finds applications where the circle must be replaced with the hyperbola for purposes of analytic geometry. A prominent instance is the depiction of spacetime as a pseudo-Euclidean space. There the asymptotes of the unit hyperbola form a light cone. Further, the attention to areas of hyperbolic sectors by Gregoire de Saint-Vincent led to the logarithm function and the modern parametrization of the hyperbola by sector areas. When the notions of conjugate hyperbolas and hyperbolic angles are understood, then the classical complex numbers, which are built around the unit circle, can be replaced with numbers built around the unit hyperbola.\n\nGenerally asymptotic lines to a curve are said to converge toward the curve. In algebraic geometry and the theory of algebraic curves there is a different approach to asymptotes. The curve is first interpreted in the projective plane using homogeneous coordinates. Then the asymptotes are lines that are tangent to the projective curve at a point at infinity, thus circumventing any need for a distance concept and convergence. In a common framework (\"x, y, z\") are homogeneous coordinates with the line at infinity determined by the equation \"z\" = 0. For instance, C. G. Gibson wrote:\n\nThe Minkowski diagram is drawn in a spacetime plane where the spatial aspect has been restricted to a single dimension. The units of distance and time on such a plane are \nEach of these scales of coordinates results in photon connections of events along diagonal lines of slope plus or minus one.\nFive elements constitute the diagram Hermann Minkowski used to describe the relativity transformations: the unit hyperbola, its conjugate hyperbola, the axes of the hyperbola, a diameter of the unit hyperbola, and the conjugate diameter.\nThe plane with the axes refers to a resting frame of reference. The diameter of the unit hyperbola represents a frame of reference in motion with rapidity \"a\" where tanh \"a\" = \"y\"/\"x\" and (\"x\",\"y\") is the endpoint of the diameter on the unit hyperbola. The conjugate diameter represents the \"spatial hyperplane of simultaneity\" corresponding to rapidity \"a\".\nIn this context the unit hyperbola is a \"calibration hyperbola\"\nCommonly in relativity study the hyperbola with vertical axis is taken as primary:\nThe vertical time axis convention stems from Minkowski in 1908, and\nis also illustrated on page 48 of Eddington's \"The Nature of the Physical World\" (1928).\n\nA direct way to parameterizing the unit hyperbola starts with the hyperbola \"xy\" = 1 parameterized with the exponential function: formula_8\n\nThis hyperbola is transformed into the unit hyperbola by a linear mapping having the matrix formula_9\n\nThis parameter \"t\" is hyperbolic angle, which is the argument of the hyperbolic functions.\n\nOne finds an early expression of the parametrized unit hyperbola in Elements of Dynamic (1878) by W. K. Clifford. He describes quasi-harmonic motion in a hyperbola as follows:\n\nAs a particular conic, the hyperbola can be parametrized by the process of addition of points on a conic. The following description \nwas given by Russian analysts:\n\nWhereas the unit circle is associated with complex numbers, the unit hyperbola is key to the \"split-complex number plane\" consisting of \"z\" = \"x\" + \"yj\", where \"j\" = +1.\nThen \"jz = y + xj\", so the action of \"j\" on the plane is to swap the coordinates. In particular, this action swaps the unit hyperbola with its conjugate and swaps pairs of conjugate diameters of the hyperbolas.\n\nIn terms of the hyperbolic angle parameter \"a\", the unit hyperbola consists of points\nThe right branch of the unit hyperbola corresponds to the positive coefficient. In fact, this branch is the image of the exponential map acting on the \"j\"-axis. Since\nthe branch is a group under multiplication. Unlike the circle group, this unit hyperbola group is \"not\" compact.\nSimilar to the ordinary complex plane, a point not on the diagonals has a polar decomposition using the parametrization of the unit hyperbola and the alternative radial length.\n\n"}
{"id": "16877394", "url": "https://en.wikipedia.org/wiki?curid=16877394", "title": "Vortex power", "text": "Vortex power\n\nVortex power is a form of hydro power which generates energy by placing obstacles in rivers/oceans in order to cause the formation of vortices which can then be tapped to a usable form of energy such as electricity. This method is pioneered by a team at the University of Michigan who call the technology VIVACE which stands for Vortex Induced Vibrations Aquatic Clean Energy (patent pending through the University of Michigan).\n\nThe company \"Vortex Hydro Power\" has been created to commercialize the technology. This technology has 10–20 years life span which meets life cycle cost targets.\n\nAs of right now, this technology seems to be nonpolluting and low maintenance, it does not have any major impact on the wildlife such as fish and other animals. This environmentally friendly technology is still in research stage and is currently undergoing optimization experiments before it can be implemented.\n\n\n\n"}
