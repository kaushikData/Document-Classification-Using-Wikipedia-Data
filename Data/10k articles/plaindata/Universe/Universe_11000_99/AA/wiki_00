{"id": "144602", "url": "https://en.wikipedia.org/wiki?curid=144602", "title": "Active transport", "text": "Active transport\n\nIn cellular biology, active transport is the movement of molecules across a membrane from a region of their lower concentration to a region of their higher concentration—against the concentration gradient. Active transport requires cellular energy to achieve this movement. There are two types of active transport: primary active transport that uses ATP, and secondary active transport that uses an electrochemical gradient. An example of active transport in human physiology is the uptake of glucose in the intestines.\n\nActive transport is the movement of molecules across a membrane from a region of their lower concentration to a region of their higher concentration—against the concentration gradient or other obstructing factor.\n\nUnlike passive transport, which uses the kinetic energy and natural entropy of molecules moving down a gradient, active transport uses cellular energy to move them against a gradient, polar repulsion, or other resistance. Active transport is usually associated with accumulating high concentrations of molecules that the cell needs, such as ions, glucose and amino acids. If the process uses chemical energy, such as from adenosine triphosphate (ATP), it is termed primary active transport. Secondary active transport involves the use of an electrochemical gradient. Examples of active transport include the uptake of glucose in the intestines in humans and the uptake of mineral ions into root hair cells of plants.\n\nIn 1848, the German physiologist Emil du Bois-Reymond suggested the possibility of active transport of substances across membranes.\n\nRosenberg (1948) formulated the concept of active transport based on energetic considerations, but later it would be redefined.\n\nIn 1997, Jens Christian Skou, a Danish physician received the Nobel Prize in Chemistry for his research regarding the sodium-potassium pump \n\nOne category of cotransporters that is especially prominent in research regarding diabetes treatment is sodium glucose cotransporters. These transporters were discovered by scientists at the National Health Institute. These scientists had noticed a discrepancy in the absorption of glucose at different points in the kidney tubule of a rat. The gene was then discovered for intestinal glucose transport protein and linked to these membrane sodium glucose cotransport systems. The first of these membrane transport proteins was named SGLT1 followed by the discovery of SGLT2. Robert Krane also played a prominent role in this field.\n\nSpecialized transmembrane proteins recognize the substance and allow it to move across the membrane when it otherwise would not, either because the phospholipid bilayer of the membrane is impermeable to the substance moved or because the substance is moved against the direction of its concentration gradient. There are two forms of active transport, primary active transport and secondary active transport. In primary active transport, the proteins involved are pumps that normally use chemical energy in the form of ATP. Secondary active transport, however, makes use of potential energy, which is usually derived through exploitation of an electrochemical gradient. The energy created from one ion moving down its electrochemical gradient is used to power the transport of another ion moving against its electrochemical gradient. This involves pore-forming proteins that form channels across the cell membrane. The difference between passive transport and active transport is that the active transport requires energy, and moves substances against their respective concentration gradient, whereas passive transport requires no energy and moves substances in the direction of their respective concentration gradient.\n\nIn an antiporter, one substrate is transported in one direction across the membrane while another is cotransported in the opposite direction. In a symporter, two substrates are transported in the same direction across the membrane. Antiport and symport processes are associated with secondary active transport, meaning that one of the two substances is transported against its concentration gradient, utilizing the energy derived from the transport of another ion (mostly Na, K or H ions) down its concentration gradient.\n\nIf substrate molecules are moving from areas of lower concentration to areas of higher concentration (i.e., in the opposite direction as, or \"against\" the concentration gradient), specific transmembrane carrier proteins are required. These proteins have receptors that bind to specific molecules (e.g., glucose) and transport them across the cell membrane. Because energy is required in this process, it is known as 'active' transport. Examples of active transport include the transportation of sodium out of the cell and potassium into the cell by the sodium-potassium pump. Active transport often takes place in the internal lining of the small intestine.\n\nPlants need to absorb mineral salts from the soil or other sources, but these salts exist in very dilute solution. Active transport enables these cells to take up salts from this dilute solution against the direction of the concentration gradient. For example, the molecules chlorine (Cl^-) and nitrate NO3- exist in the cytosol of plant cells, and need to be transported into the vacuole. While the vacuole has channels for these ions, transportation of them is against the concentration gradient, and thus movement of these ions is driven by hydrogen pumps, or proton pumps\n\nPrimary active transport, also called direct active transport, directly uses metabolic energy to transport molecules across a membrane. Substances that are transported across the cell membrane by primary active transport include metal ions, such as Na, K, Mg, and Ca. These charged particles require ion pumps or ion channels to cross membranes and distribute through the body.\n\nMost of the enzymes that perform this type of transport are transmembrane ATPases. A primary ATPase universal to all animal life is the sodium-potassium pump, which helps to maintain the cell potential. The sodium-potassium pump maintains the membrane potential by moving three Na+ ions out of the cell for every two K+ ions moved into the cell. Other sources of energy for Primary active transport are redox energy and photon energy (light). An example of primary active transport using Redox energy is the mitochondrial electron transport chain that uses the reduction energy of NADH to move protons across the inner mitochondrial membrane against their concentration gradient. An example of primary active transport using light energy are the proteins involved in photosynthesis that use the energy of photons to create a proton gradient across the thylakoid membrane and also to create reduction power in the form of NADPH.\n\nATP hydrolysis is used to transport hydrogen ions against the electrochemical gradient (from low to high hydrogen ion concentration). Phosphorylation of the carrier protein and the binding of a hydrogen ion induce a conformational (shape) change that drives the hydrogen ions to transport against the electrochemical gradient. Hydrolysis of the bound phosphate group and release of hydrogen ion then restores the carrier to its original conformation.\n\n\nAdenosine Triphosphate-binding cassette transporters (ABC transporters) comprise a large and diverse protein family, often functioning as ATP-driven pumps. Usually, there are several domains involved in the overall transporter protein's structure, including two nucleotide-binding domains that constitute the ATP-binding motif and two hydrophobic transmembrane domains that create the \"pore\" component. In broad terms, ABC transporters are involved in the import or export of molecules across a cell membrane; yet within the protein family there is an extensive range of function.\n\nIn plants, ABC transporters are often found within cell and organelle membranes, such as the mitochondria, chloroplast, and plasma membrane. There is evidence to support that plant ABC transporters play a direct role in pathogen response, phytohormone transport, and detoxification. Furthermore, certain plant ABC transporters may function in actively exporting volatile compounds and antimicrobial metabolites.\n\nIn petunia flowers (\"Petunia hybrida\"), the ABC transporter PhABCG1 is involved in the active transport of volatile organic compounds. PhABCG1 is expressed in the petals of open flowers. In general, volatile compounds may promote the attraction of seed-dispersal organisms and pollinators, as well as aid in defense, signaling, allelopathy, and protection. To study the protein PhABCG1, transgenic petunia RNA interference lines were created with decreased \"PhABCG1\" expression levels. In these transgenic lines, a decrease in emission of volatile compounds was observed. Thus, PhABCG1 is likely involved in the export of volatile compounds. Subsequent experiments involved incubating control and transgenic lines that expressed \"PhABCG1\" to test for transport activity involving different substrates. Ultimately, PhABCG1 is responsible for the protein-mediated transport of volatile organic compounds, such as benezyl alcohol and methylbenzoate, across the plasma membrane.\n\nAdditionally in plants, ABC transporters may be involved in the transport of cellular metabolites. Pleiotropic Drug Resistance ABC transporters are hypothesized to be involved in stress response and export antimicrobial metabolites. One example of this type of ABC transporter is the protein NtPDR1. This unique ABC transporter is found in \"Nicotiana tabacum\" BY2 cells and is expressed in the presence of microbial elicitors. NtPDR1 is localized in the root epidermis and aerial trichomes of the plant. Experiments using antibodies specifically targeting NtPDR1 followed by Western blotting allowed for this determination of localization. Furthermore, it is likely that the protein NtPDR1 actively transports out antimicrobial diterpene molecules, which are toxic to the cell at high levels.\n\nIn secondary active transport, also known as \"coupled transport\" or \"cotransport\", energy is used to transport molecules across a membrane; however, in contrast to primary active transport, there is no direct coupling of ATP; instead it relies upon the electrochemical potential difference created by pumping ions in/out of the cell. Permitting one ion or molecule to move down an electrochemical gradient, but possibly against the concentration gradient where it is more concentrated to that where it is less concentrated increases entropy and can serve as a source of energy for metabolism (e.g. in ATP synthase). The energy derived from the pumping of protons across a cell membrane is frequently used as the energy source in secondary active transport. In humans, sodium (Na) is a commonly co-transported ion across the plasma membrane, whose electrochemical gradient is then used to power the active transport of a second ion or molecule against its gradient. In bacteria and small yeast cells, a commonly cotransported ion is hydrogen. Hydrogen pumps are also used to create an electrochemical gradient to carry out processes within cells such as in the electron transport chain, an important function of cellular respiration that happens in the mitochondrion of the cell.\n\nIn August 1960, in Prague, Robert K. Crane presented for the first time his discovery of the sodium-glucose cotransport as the mechanism for intestinal glucose absorption. Crane's discovery of cotransport was the first ever proposal of flux coupling in biology.\n\nCotransporters can be classified as symporters and antiporters depending on whether the substances move in the same or opposite directions.\n\nIn an antiporter two species of ion or other solutes are pumped in opposite directions across a membrane. One of these species is allowed to flow from high to low concentration which yields the entropic energy to drive the transport of the other solute from a low concentration region to a high one.\n\nAn example is the sodium-calcium exchanger or antiporter, which allows three sodium ions into the cell to transport one calcium out. This antiporter mechanism is important within the membranes of cardiac muscle cells in order to keep the calcium concentration in the cytoplasm low. Many cells also possess calcium ATPases, which can operate at lower intracellular concentrations of calcium and sets the normal or resting concentration of this important second messenger. But the ATPase exports calcium ions more slowly: only 30 per second versus 2000 per second by the exchanger. The exchanger comes into service when the calcium concentration rises steeply or \"spikes\" and enables rapid recovery. This shows that a single type of ion can be transported by several enzymes, which need not be active all the time (constitutively), but may exist to meet specific, intermittent needs.\n\nA symporter uses the downhill movement of one solute species from high to low concentration to move another molecule uphill from low concentration to high concentration (against its concentration gradient). Both molecules are transported in the same direction.\n\nAn example is the glucose symporter SGLT1, which co-transports one glucose (or galactose) molecule into the cell for every two sodium ions it imports into the cell. This symporter is located in the small intestines, heart, and brain. It is also located in the S3 segment of the proximal tubule in each nephron in the kidneys. Its mechanism is exploited in glucose rehydration therapy This mechanism uses the absorption of sugar through the walls of the intestine to pull water in along with it. Defects in SGLT2 prevent effective reabsorption of glucose, causing familial renal glucosuria.\n\nEndocytosis and exocytosis are both forms of bulk transport that move materials into and out of cells, respectively, via vesicles. In the case of endocytosis, the cellular membrane folds around the desired materials outside the cell. The ingested particle becomes trapped within a pouch, known as a vesicle, inside the cytoplasm. Often enzymes from lysosomes are then used to digest the molecules absorbed by this process. Substances that enter the cell via signal mediated endocytosis include proteins, hormones and growth factors. Viruses enter cells through a form of endocytosis that involves their outer membrane fusing with the membrane of the cell. This forces the viral DNA into the host cell.\n\nBiologists distinguish two main types of endocytosis: pinocytosis and phagocytosis.\n\n\nExocytosis involves the removal of substances through the fusion of the outer cell membrane and a vesicle membrane An example of exocytosis would be the transmission of neurotransmitters across a synapse between brain cells.\n\n\n"}
{"id": "17064651", "url": "https://en.wikipedia.org/wiki?curid=17064651", "title": "Atlantic Accord", "text": "Atlantic Accord\n\nThe Atlantic Accord is an agreement signed in 1985 between the Government of Canada and the Government of Newfoundland and Labrador to manage offshore oil and gas resources adjacent to Newfoundland and Labrador.\n\nThe name was also used to describe a 2005 cash transfer agreement between the Government of Canada and the governments of Nova Scotia and Newfoundland and Labrador.\n"}
{"id": "57899033", "url": "https://en.wikipedia.org/wiki?curid=57899033", "title": "Breakthrough Laminar Aircraft Demonstrator in Europe", "text": "Breakthrough Laminar Aircraft Demonstrator in Europe\n\nThe Breakthrough Laminar Aircraft Demonstrator in Europe (BLADE) is an Airbus project within the European Clean Sky framework to flight-test experimental laminar-flow wing sections on an A340 from September 2017.\n\nNatural laminar flow is opposed to hybrid laminar flow artificially induced through hardware.\nIt is difficult to industrialise a wing smooth enough with very low design and manufacturing tolerances and aerodynamically robust enough to sustain the laminar flow in operations, with leading edge retractable slats, fasteners, surface deformations and dirt, de-icing fluid and rain droplets contamination disturbances.\nThe metallic outboard section with a carbon fiber reinforced plastic upper laminar flow surface is isolated from the rest of the wing and has two ailerons on each side.\nIts wing sweep is around 20° for a Mach 0.75 cruise instead of 30° for Mach 0.82-0.84, laminar flow is expected along 50% of chord length instead of just aft of the leading edge, halving the wing friction drag, reducing the overall aircraft drag by 8% and saving up to 5% in fuel on an 800nm (1,480km) sector.\n\nThe demonstrator took off on 26 September.\nIn April 2018, after 66 flight hours, drag reduction is better than expected at 10% and laminar flow is more stable than anticipated, including when the wing twists and flexes.\nBoth wings with their carbonfibre upper sustainably generate the desired effect, while the carbonfibre left wing leading edge and metallic right wing leading edge have small differences in aerodynamic effects.\nThe aerodynamic benefits could be sustained at Mach 0.78 up from M0.75 and next-generation single-aisles could use from the late 2020s.\nTest will continue until 2019 and will include wing contamination and a fixed Krüger flap.\n\nMorphing flaps should be flight tested from May 2020.\n"}
{"id": "32404998", "url": "https://en.wikipedia.org/wiki?curid=32404998", "title": "Cirrocumulus lenticularis", "text": "Cirrocumulus lenticularis\n\nCirrocumulus lenticularis is a type of cirrocumulus cloud. The name \"cirrocumulus lenticularis\" is derived from Latin, meaning \"like a lentil\". Cirrocumulus lenticularis are smooth clouds that have the appearance of a lens or an almond. They usually form at the crests of atmospheric waves, which would otherwise be invisible. This species of cirrocumulus can often be quite elongated and normally has very distinguished boundaries. Cirrocumulus lenticularis forms when stable air is forced upward; this is usually due to orographic features, but can occur away from mountains as well. Irisation can occasionally occur with these clouds.\n\n\n"}
{"id": "3487349", "url": "https://en.wikipedia.org/wiki?curid=3487349", "title": "Conical pendulum", "text": "Conical pendulum\n\nA conical pendulum consists of a weight (or bob) fixed on the end of a string or rod suspended from a pivot. Its construction is similar to an ordinary pendulum; however, instead of swinging back and forth, the bob of a conical pendulum moves at a constant speed in a circle with the string (or rod) tracing out a cone. The conical pendulum was first studied by the English scientist Robert Hooke around 1660 as a model for the orbital motion of planets. In 1673 Dutch scientist Christiaan Huygens calculated its period, using his new concept of centrifugal force in his book \"Horologium Oscillatorium\". Later it was used as the timekeeping element in a few mechanical clocks and other clockwork timing devices.\n\nDuring the 1800s, conical pendulums were used as the timekeeping element in a few clockwork timing mechanisms where a smooth motion was required, as opposed to the unavoidably jerky motion provided by ordinary pendulums. Two examples were mechanisms to turn the lenses of lighthouses to sweep their beams across the sea, and the location drives of equatorial mount telescopes, to allow the telescope to follow a star smoothly across the sky as the Earth turns.\n\nOne of the most important uses of the conical pendulum was in the flyball governor (centrifugal governor) invented by James Watt in 1788 which regulated the speed of steam engines during the Steam Age in the 1800s. The playground game tetherball uses a ball attached to a pole by a cord which functions as a conical pendulum, although the pendulum gets shorter as the cord wraps around the pole. Some amusement park rides act as conical pendulums.\n\nConsider a conical pendulum consisting of a bob of mass \"m\" revolving without friction in a circle at a constant speed \"v\" on a string of length \"L\" at an angle of \"θ\" from the vertical.\n\nThere are two forces acting on the bob:\n\nThe force exerted by the string can be resolved into a horizontal component, \"T\" sin(\"θ\"), toward the center of the circle, and a vertical component, \"T\" cos(\"θ\"), in the upward direction. From Newton's second law, the horizontal component of the tension in the string gives the bob a centripetal acceleration toward the center of the circle:\n\nSince there is no acceleration in the vertical direction, the vertical component of the tension in the string is equal and opposite to the weight of the bob:\n\nThese two equations can be solved for \"T\"/\"m\" and equated, thereby eliminating \"T\" and \"m\":\n\nSince the speed of the pendulum bob is constant, it can be expressed as the circumference 2\"πr\" divided by the time \"t\" required for one revolution of the bob:\n\nSubstituting the right side of this equation for \"v\" in the previous equation, we find:\n\nUsing the trigonometric identity tan(\"θ\") = sin(\"θ\") / cos(\"θ\") and solving for \"t\", the time required for the bob to travel one revolution is\n\nIn a practical experiment, \"r\" varies and is not as easy to measure as the constant string length \"L\". \"r\" can be eliminated from the equation by noting that \"r\", \"h\", and \"L\" form a right triangle, with \"θ\" being the angle between the leg \"h\" and the hypotenuse \"L\" (see diagram). Therefore,\n\nSubstituting this value for \"r\" yields a formula whose only varying parameter is the suspension angle \"θ\":\n\nFor small angles \"θ\", cos(\"θ\") ≈ 1, and the period \"t\" of a conical pendulum is equal to the period of an ordinary pendulum of the same length. Also, the period for small angles is approximately independent of changes in the angle \"θ\". This means the period of rotation is approximately independent of the force applied to keep it rotating. This property, called isochronism, is shared with ordinary pendulums and makes both types of pendulums useful for timekeeping.\n\n\n"}
{"id": "52299004", "url": "https://en.wikipedia.org/wiki?curid=52299004", "title": "Degas conductivity", "text": "Degas conductivity\n\nConductivity measurements in the water/steam cycle of power stations are commonly used as indicators of the quality of the water used in the process. Excessive conductivity values often indicate a high corrosion potential, especially in the case of certain ions such as chloride and acetate ions. These can be particularly damaging to the blades in the steam turbine.\n\nTypically, there are three major types of conductivity measurements used:\nGenerally, degas conductivity is measured from condensed and cooled samples of primary steam. It may also be relevant for analyzing condensate return, especially in cases where the condensate is returned from a separate plant that used the steam in another process.\n\nAfter the ions have been removed from the conditioning of the circulating water (e.g. ammonium NH) in the cation exchanger, ions resulting from gaseous components must be removed in order to determine degas conductivity. These are typically gases from the atmosphere which have penetrated into the system through leaks in the water-steam circuit. Of all gases occurring in the atmosphere, typically only carbon dioxide (CO) dissolves chemically into ions in circulating water. The remaining gases (oxygen, nitrogen, etc.) dissolve physically and do not form ions, and thus do not contribute to conductivity. The chemical reactions of carbon dioxide in water proceed according to the following reaction equation (mass action law):\n\nA) CO + 2 HO <--> HCO + HO pK = 6.3\n\nB) HCO + 2 HO <--> CO pK = 10.3\n\nSee the graph showing relative CO concentration. After the cation exchanger, the sample pH value is generally between 5.5-6, so that means almost only CO is present as gas, and only about 6% is carbon carbonate ion CO. The bicarbonate ion (HCO) is practically absent.\n\nHowever, ionic components of carbon dioxide are far less dangerous to corrosion than the ions of the saline components, e.g. Cl. In order to obtain a selective conductivity value for these saline-containing ions (with the maximum potential for corrosion), all remaining carbon dioxide must be removed from the sample in order to accurately determine the presence of corrosive ions. \n\nThere are generally two methods for removing carbon dioxide from the water sample: use of a reboiler to heat the sample and expel the CO, and use of inert gasses. In the latter method, an inert gas which does not contain CO is passed through the sample water, whereby the gas components in the sample water are displaced by the gas components of the inert gas. Use of bottled inert gases can be problematic in some industrial applications. Reboilers are very efficient degassing with results over 92%, but they typically require anywhere from 20–45 minutes to achieve useful results. Manufacturers of reboiler systems include Swan Analytical, Mettler Toledo, and Sentry Systems.\n\nA new variation of the inert gas method (known as \"Gronowski's dynamic method\") has recently been developed together with partner manufacturer Waltron, whereby the inert gas is generated in the decarbonation column by passing air through a column filled with soda lime. The removal of the carbon dioxide is carried out in an exchanger column according to the contraflow principle. The inert gas drives the carbon dioxide from the sample water so that no carbonate ion can be formed. What remains in the water sample are salt-like (acid-like) ions and organic components, as well as oxygen and nitrogen which do not form ions in aqueous media. Gronowski's dynamic method is extremely fast, achieving approximately 94% degassing in 45 seconds, growing up to even greater final efficiency. See the graph (right), taken from actual test data.\n\nGrowth in renewable (but unsteady) energy sources has placed greater burden on modern gas-fired electric plants to cycle on and off to maintain steady and reliable electric production between Renewables and Base Load. These plants utilize a combination of gas (70%) and steam (30%) turbines to produce electricity. Critical for top efficiency is ensuring pure steam reaches the second stage quickly.\n\nDuring the start up of a power plant, the purity of the generated steam determines whether it can be sent to the steam turbine or if it has to be bypassed to the condenser. Traditionally “Cation Conductivity” instruments are used to analyze steam quality, but in addition to measuring harmful ionic compounds (e.g. chloride ions), they also include CO, which as stated above is not significantly harmful to the steam turbine. Furthermore, typical cation conductivity analyzers take 3–4 hours to provide useful indications of steam purity. In many cases, this means the plant never reaches 100% efficiency before it cycles offline. That means a gas-turbine combined cycle plant would be burning fuel at 100%, but only achieving a 70% output and venting the excess heat and exhaust.\n\nIn the case of a traditional base load power plant, cycling is much less frequent—in some cases, only twice annually for maintenance. Compared with measuring only cation conductivity, the cost savings from an accelerated start-up using degas conductivity is potentially very large. At $0.50/MW-minute ($30/MWH), a 750MW coal plant starting three hours faster each cycle will generate an additional $133,875 of annual revenue from the same fuel.\n\nBased on similar assumptions, the cost savings between different degassing methodologies is significant. If a dynamic system similar to Gronowski's is used, in the nearly 30 minutes of start-up time saved over a reboiler method, the typical combined cycle plant will generate even more income from the same fuel consumed with each and every system start, especially using typical “peak” electricity pricing. Additional benefits are better energy efficiency and reduced emissions of heat and exhaust.\n"}
{"id": "700633", "url": "https://en.wikipedia.org/wiki?curid=700633", "title": "Dram (unit)", "text": "Dram (unit)\n\nThe dram (alternative British spelling drachm; apothecary symbol ʒ or ℨ; abbreviated dr) is a unit of mass in the avoirdupois system, and both a unit of mass and a unit of volume in the apothecaries' system. It was originally both a coin and a weight in ancient Greece. The unit of volume is more correctly called a fluid dram, fluid drachm, fluidram or fluidrachm (abbreviated fl dr, ƒ 3, or fʒ).\n\n\nA coin weighing one drachma is known as a stater, drachm, or drachma. The Ottoman dirhem was based on the Sassanian drachm, which was itself based on the Roman dram/drachm.\n\nThe British Weights and Measures Act of 1878 introduced verification and consequent stamping of apothecary weights, making them officially recognized units of measurement. By 1900, Britain had enforced the distinction between the avoirdupois and apothecaries' versions by making the spelling different:\n\nIn the avoirdupois system, the dram is the mass of  pound or  ounce. The dram weighs  grains, or exactly grams.\n\nIn the apothecaries' system, which was widely used in the United States until the middle of the 20th century, the dram is the mass of  pounds apothecaries (lb ap), or  ounces apothecaries (oz ap or ℥) (the pound apothecaries and ounce apothecaries are equal to the troy pound (lb t), and troy ounce (oz t), respectively). The dram apothecaries is equal to or , or exactly grams.\n\n\"Dram\" is also used as a measure of the powder charge in a shotgun shell, representing the equivalent of black powder in drams avoirdupois.\n\nThe \"fluid dram\" (or \"fluid drachm\" in British spelling) is defined as of a fluid ounce, and is exactly equal to:\n\nA teaspoonful has been considered equal to one fluid dram for medical prescriptions. However, by 1876 the teaspoon had grown considerably larger than it was previously, measuring 80–85 minims. As there are 60 minims in a fluid dram, using this equivalent for the dosage of medicine was no longer suitable. Today's US teaspoon is equivalent to exactly US fluid ounces, US fluid drams, or 80 US minims.\n\nWhile pharmaceuticals are measured nowadays exclusively in metric units, fluid drams are still used to measure the capacity of pill containers.\n\n\"Dram\" is used informally to mean a small amount of spirituous liquor, especially Scotch whisky. The unit is referenced by the phrase \"dram shop\", the US legal term for an establishment that serves alcoholic beverages.\n\nThe line \"Where'd you get your whiskey, where'd you get your dram?\" appears in some versions of the traditional pre-Civil War American song \"Cindy\". In Monty Python's song entitled The Bruces' Philosophers Song there is the following line: \"Hobbes was fond of his dram\". In the old-time music tradition of the United States, there is a tune entitled \"Gie the Fiddler a Dram\". “Gie” being the Scottish dialectal version of give, brought over by immigrants and commonly used by their descendants in Appalachia at the time of writing.\n\nIn the episode \"Double Indecency\" of the TV series \"Archer\", the character Cheryl/Carol was carrying around 10 drams of Vole's blood and even offered to pay for a taxi ride with it.\n\n"}
{"id": "2077504", "url": "https://en.wikipedia.org/wiki?curid=2077504", "title": "Electrolaser", "text": "Electrolaser\n\nAn electrolaser is a type of electroshock weapon that is also a directed-energy weapon. It uses lasers to form an electrically conductive \"laser-induced plasma channel\" (LIPC). A fraction of a second later, a powerful electric current is sent down this plasma channel and delivered to the target, thus functioning overall as a large-scale, high energy, long-distance version of the Taser electroshock gun.\n\nAlternating current is sent through a series of step-up transformers, increasing the voltage and decreasing the current. The final voltage may be between 10 and 10 volts. This current is fed into the plasma channel created by the laser beam.\n\nA \"laser-induced plasma channel\" (LIPC) is formed by the following process:\n\nBecause a laser-induced plasma channel relies on ionization, gas must exist between the electrolaser weapon and its target. If a laser-beam is intense enough, its electromagnetic field is strong enough to rip electrons off of air molecules, or whatever gas happens to be in between, creating plasma. Similar to lightning, the rapid heating also creates a sonic boom.\n\nMethods of use:\nBecause of the plasma channel, an electrolaser may cause an accident if there is a thunderstorm (or other electricity sources such as overhead powerlines) about. (\"See\" Taser \"for more information – principles of operation, controversies, etc\".)\n\nAn electrolaser is not presently practical for wireless energy transfer due to danger and low efficiency.\n\nPublicly traded company Applied Energetics (formerly Ionatron) develops directed-energy weapons for the United States military. The company has produced a device called the Joint IED Neutralizer (JIN) which was deemed unfit for field use in 2006. The JIN is intended for safely detonating improvised explosive devices (IEDs). Future designs include weapons mounted on land, air and sea vehicles and as a hand-held infantry version.\n\nApplied Energetics said that the weapons will be able to be used as a non-lethal alternative to current weaponry, but will be able to deliver a high enough voltage jolt to kill.\n\nApplied Energetics/Ionatron say that they are working on an electrolaser system, called LGE (Laser Guided Energy). They are also studying a laser-induced plasma channel (LIPC) as a way to stop people from going through a corridor or passageway.\n\nThere was an unconfirmed report that in 1985 the U.S. Navy tested an electrolaser. Its targets were missiles and aircraft. This device was known as the Phoenix project within the Strategic Defense Initiative research program. It was first proved by experiment at long range in 1985, but this report may have referred to an early test of MIRACL, which is or was a high-powered chemical laser.\n\nHSV Technologies, Inc. (Stood for the last names of the original founders, Herr, Schlesinger and Vernon; this is NOT the same company as Holden Special Vehicles), formerly of San Diego, California, USA, then Port Orchard, WA, designed a non-lethal device which was profiled in the 2002 \"TIME\" magazine article \"Beyond the Rubber Bullet\". It is an electrolaser using ultraviolet laser beams of 193 nm, and promises to immobilize living targets at a distance without contact. There is plan for an engine-disabling variation for use against the electronic ignitions of cars using a 248 nm laser. The lead inventor, Eric Herr, died in 2008 and the company appears to have been dissolved, as their website now hosts an unrelated business (as of 9/2015) \n\nScientists and engineers from Picatinny Arsenal have demonstrated that an electric discharge can go through a laser beam. The laser beam is self-focusing due to the high laser intensity of 50 gigawatts, which changes the speed of light in air. The laser was reportedly successfully tested in January 2012.\n\nThere have been experiments in using a laser beam as path to discharge natural electric charges in the air, causing \"laser-triggered lightning\".\n\n"}
{"id": "2631676", "url": "https://en.wikipedia.org/wiki?curid=2631676", "title": "Environment Victoria", "text": "Environment Victoria\n\nEnvironment Victoria, formerly the Conservation Council of Victoria, is an Australian not-for-profit, charitable group and Victoria's peak non-government environment organisation. It works in collaboration with over 150 groups Australia-wide to protect Victoria’s environment and relies on financial support from donors, members, grants and sponsorships.\n\nThe group was originally formed in 1969 as the Conservation Council of Victoria as a result of a successful campaign to save the Little Desert National Park from subdivision. In 1994, the organisation changed name to Environment Victoria to reflect the broader range of issues it now deals with.\n\nFrom 1971 until 1997, two representatives of the Conservation Council of Victoria served on the Land Conservation Council which was responsible for systematically investigating and recommending balanced use of public land across the state. John Landy (later Governor of Victoria) and professor of botany John Turner were the first representatives. Joan Lindros, Bill Holsworth and Malcolm Calder were also long serving representatives.\n\nEnvironment Victoria list the Walk Against Warming, the successful campaign to put a price on pollution in Australia and the federal government's decision to withdraw funding from HRL's proposed coal-fired power station in 2012 among their successes.\n\n"}
{"id": "50669088", "url": "https://en.wikipedia.org/wiki?curid=50669088", "title": "Ethylene copolymer bitumen", "text": "Ethylene copolymer bitumen\n\nEthylene Copolymer Bitumen (ECB) is a black colored mixture based on high quality polyethylene copolymers with different proportions of various special and amorphous bitumen grades.\n\nThe ECB membrane (used for proofing) was invented in 1968.\n"}
{"id": "13008553", "url": "https://en.wikipedia.org/wiki?curid=13008553", "title": "Foldback (power supply design)", "text": "Foldback (power supply design)\n\nFoldback is a current limiting feature (a type of overload protection) of power supplies and power amplifiers. When the load attempts to draw overcurrent from the supply, foldback reduces both the output voltage and current to well below the normal operating limits. Under a short circuit, where the output voltage has reduced to zero, the current is typically limited to a small fraction of the maximum current.\n\nThe prime purpose of foldback current limiting in linear power supplies is to keep the output transistor within its safe power dissipation limit.\nWith a linear voltage regulator, the output voltage \"V\" and output current \"I\" are maintained by simply dissipating away the surplus of input voltage \"V\":\nUnder overload conditions the output voltage falls and so the difference \"V\" - \"V\" becomes larger, tending to increasing dissipation. For a simple current limit, safely handling the worst-case scenario (a short circuit) would therefore require a much larger heatsink and output transistor than would be required under normal operating conditions. Foldback partially solves this, helping to keep the normal-rated output transistor within its safe operating area under fault and overload conditions. Foldback also significantly reduces the power dissipation in the \"load\" in fault conditions, which can reduce the risks of fire and heat damage.\n\nMany power supplies employ constant current limiting protection; foldback goes one step further by reducing the output current limit linearly as output voltage decreases. However it adds complexity to the power supply and can trigger \"lockout\" conditions with non-ohmic devices that draw a constant current independent of supply voltage (such as op-amps). Foldback in switched-mode power supplies is discouraged because of these disadvantages, and since the benefit of reducing the power dissipation does not apply. Despite this, foldback is still often implemented in them.\n"}
{"id": "25098885", "url": "https://en.wikipedia.org/wiki?curid=25098885", "title": "Gelclair", "text": "Gelclair\n\nGelclair is an oral gel that coats the surface of the mouth forming a thin protective film over painful oral lesions, such as those caused by radiotherapy or chemotherapy treatment for cancer.\n\nGelclair can be used in the management of the painful symptoms of oral mucositis usually caused by radiotherapy or chemotherapy treatment for cancer but can also be caused by medication, disease, oral surgery, stress, traumatic ulcers caused by dental braces and dentures and ageing. \nGelclair can be used by patients of all ages.\nGelclair is usually used 3 times a day or as needed. It is usually diluted with water and rinsed around the mouth. It can be used undiluted where no water is available, applied directly.\nGelclair does not numb the mouth and can be used in conjunction with other treatment options for managing oral mucositis, including antibacterial mouthwashes and painkillers.\n\nInnocenti et al. studied Gelclair in a total of thirty patients with chemotherapy-induced oral lesions. Results showed a 92% reduction in mean pain scores 5–7 hours after the first administration of Gelclair. Patients were also given Gelclair for a further 7–10 days and at the end of this period, pain scores were recorded. After one week of using Gelclair, 87% of patients reported overall improvements relating to pain on swallowing food, liquids and saliva.\n\nDe Cordi et al. studied thirty patients with lesions of the mouth and oropharynx (caused by various diseases). 83% of patients reported a reduction in pain, 13% remained the same and 3% showed initial improvement but then got worse. 83% showed a distinct improvement in functionality in the ability to take food, 7% remained the same, and 7% got worse, while 3% reported considerable improvement followed by slight worsening. 57% of patients reported an improvement in the grade of oral mucositis, 40% remained the same while 3% got worse.\n\nBerndtson studied ten patients who were given Gelclair to evaluate its effect on their symptoms of oral mucositis. All reported that the product was acceptable for its taste and consistency, and a palliative soothing effect was noted.\n\nGelclair has no known interactions with medicines or other products and has no known toxicity.\n\n"}
{"id": "26050131", "url": "https://en.wikipedia.org/wiki?curid=26050131", "title": "Greaseproof paper", "text": "Greaseproof paper\n\nGreaseproof paper is paper that is impermeable to oil or grease and is normally used in cooking or food packaging. Normally greaseproof paper is produced by refining the paper stock and thus create a sheet with very low porosity. This sheet is supercalendered to further improve the density, creating a paper called glassine. The glassine is treated with starches, alginates or carboxymethyl cellulose (CMC) in a size press to fill pores or treat the paper chemically to make it fat repellent. Basis weights are usually 30–50 g/m².\n\nIn Australia, Greaseproof paper is also referred to as SKAN Greaseproof.\n"}
{"id": "15666864", "url": "https://en.wikipedia.org/wiki?curid=15666864", "title": "Guandi Dam", "text": "Guandi Dam\n\nThe Guandi Dam () is a gravity dam on the Yalong River, a tributary of the Yangtze River in Sichuan Province Southwest of China. It supplies water to four hydroelectric generators, each with generating capacity of 600 MW. The total generating capacity of the project is 2,400 MW. Construction started on October 20, 2010 with a ground-breaking ceremony. On February 9, 2012 the dam began to impound the reservoir and the last of the four generators were commissioned on 28 March 2013.\n\n"}
{"id": "1563166", "url": "https://en.wikipedia.org/wiki?curid=1563166", "title": "HVC 127-41-330", "text": "HVC 127-41-330\n\nHVC 127-41-330 is a high-velocity cloud. The three numbers that compose its name indicate, respectively, the galactic longitude and latitude, and velocity towards Earth in km/s. It is 20,000 light years in diameter and is located 2.3 million light years (700 kiloparsecs) from Earth, between M31 and M33. This cloud of neutral hydrogen (detectable via 21 cm H-I emissions), unlike other HVCs shows a rotational component and dark matter. 80% of the mass of the cloud is dark matter. It is also the first HVC discovered not associated with the Milky Way galaxy or subgroup (subcluster).\n\nAstronomer Josh Simon considers it a candidate for being a dark galaxy. With its rotation, it may be a very low density dwarf galaxy of unused hydrogen (no stars), a remnant of the formation of the Local Group.\n\n"}
{"id": "30965794", "url": "https://en.wikipedia.org/wiki?curid=30965794", "title": "Hauling (film)", "text": "Hauling (film)\n\nHauling is a 2010 documentary film on poor families who work as recyclers in Sao Paulo, Brazil.\n\nThe recycling underworld of Sao Paulo, Brazil is the background to this documentary centering on Claudieni, father to 27 children. In his green VW bus, he visits the Santa Efigenia neighborhood of Sao Paulo to recycle plastic, cardboard, and computers and other electronic equipment. He and his family represent the lower class of people who are discriminated against by other strata of society.\n\nThe film will have its U.S. West Coast premiere at the San Francisco Green Film Festival in March 2011.\n\n"}
{"id": "50013014", "url": "https://en.wikipedia.org/wiki?curid=50013014", "title": "Hopkinson effect", "text": "Hopkinson effect\n\nThe Hopkinson effect is a feature of ferromagnetic or ferrimagnetic materials, in which an increase in magnetic susceptibility is observed at temperatures between the blocking temperature and the Curie temperature of the material. The Hopkinson effect can be observed as a peak in thermomagnetic curves that immediately precedes the susceptibility drop associated with the Curie temperature. It was first observed by John Hopkinson in 1889 in a study on iron.\n\nIn single domain particles, a large Hopkinson peak results from a transient superparamagnetic particle domain state.\n"}
{"id": "821877", "url": "https://en.wikipedia.org/wiki?curid=821877", "title": "Inductively coupled plasma", "text": "Inductively coupled plasma\n\nAn inductively coupled plasma (ICP) or transformer coupled plasma (TCP) is a type of plasma source in which the energy is supplied by electric currents which are produced by electromagnetic induction, that is, by time-varying magnetic fields.\n\nThere are three types of ICP geometries: planar (Fig. 3 (a)), cylindrical (Fig. 3 (b)), and half-toroidal (Fig. 3 (c)). \n\nIn planar geometry, the electrode is a length of flat metal wound like a spiral (or coil). In cylindrical geometry, it is like a helical spring. In half-toroidal geometry, it is toroidal solenoid cut along its main diameter to two equal halves.\n\nWhen a time-varying electric current is passed through the coil, it creates a time-varying magnetic field around it, with flux\n\nformula_1,\n\nwhere \"r\" is the distance to the center of coil (and of the quartz tube).\n\nAccording to the Faraday–Lenz's law of induction, this creates azimuthal electromotive force in the rarefied gas:\n\nformula_2,\n\nwhich corresponds to electric field strengths of\n\nformula_3,\n\nleading to the formation of the figure-8 electron trajectories providing a plasma generation. The dependence on r suggests that the gas ion motion is most intense in the outer region of the flame, where the temperature is the greatest. In the real torch, the flame is cooled from the outside by the cooling gas, so the hottest outer part is at thermal equilibrium. There temperature reaches 5 000–6 000 K. For more rigorous description, see Hamilton–Jacobi equation in electromagnetic fields.\n\nThe frequency of alternating current used in the RLC circuit which contains the coil usually 27–41 MHz. To induce plasma, a spark is produced at the electrodes at the gas outlet. Argon is one example of a commonly used rarefied gas. The high temperature of the plasma allows the determination of many elements, and in addition, for about 60 elements degree of ionization in the torch exceeds 90 %. The ICP torch consumes ca. 1250–1550 W of power, but this depends on the elemental composition of the sample (due to different ionization energies).\n\nPlasma electron temperatures can range between ~6 000 K and ~10 000 K (~6 eV - ~100 eV), comparable to the surface of the sun. ICP discharges are of relatively high electron density, on the order of 10 cm. As a result, ICP discharges have wide applications where a high-density plasma (HDP) is needed.\n\n\nAnother benefit of ICP discharges is that they are relatively free of contamination, because the electrodes are completely outside the reaction chamber. By contrast, in a capacitively coupled plasma (CCP), the electrodes are often placed inside the reactor and are thus exposed to the plasma and subsequent reactive chemical species.\n\n"}
{"id": "184011", "url": "https://en.wikipedia.org/wiki?curid=184011", "title": "Infinitesimal strain theory", "text": "Infinitesimal strain theory\n\nIn continuum mechanics, the infinitesimal strain theory is a mathematical approach to the description of the deformation of a solid body in which the displacements of the material particles are assumed to be much smaller (indeed, infinitesimally smaller) than any relevant dimension of the body; so that its geometry and the constitutive properties of the material (such as density and stiffness) at each point of space can be assumed to be unchanged by the deformation.\n\nWith this assumption, the equations of continuum mechanics are considerably simplified. This approach may also be called small deformation theory, small displacement theory, or small displacement-gradient theory. It is contrasted with the finite strain theory where the opposite assumption is made.\n\nThe infinitesimal strain theory is commonly adopted in civil and mechanical engineering for the stress analysis of structures built from relatively stiff elastic materials like concrete and steel, since a common goal in the design of such structures is to minimize their deformation under typical loads.\n\nFor \"infinitesimal deformations\" of a continuum body, in which the displacement (vector) and the displacement gradient (2nd order tensor) are small compared to unity, i.e., formula_1 and formula_2, \nit is possible to perform a \"geometric linearization\" of any one of the (infinitely many possible) strain tensors used in finite strain theory, e.g. the Lagrangian strain tensor formula_3, and the Eulerian strain tensor formula_4. In such a linearization, the non-linear or second-order terms of the finite strain tensor are neglected. Thus we have\n\nor\nand\nor\n\nThis linearization implies that the Lagrangian description and the Eulerian description are approximately the same as there is little difference in the material and spatial coordinates of a given material point in the continuum. Therefore, the material displacement gradient components and the spatial displacement gradient components are approximately equal. Thus we have\n\nor\nformula_10\n\nwhere formula_11 are the components of the \"infinitesimal strain tensor\" formula_12, also called \"Cauchy's strain tensor\", \"linear strain tensor\", or \"small strain tensor\".\n\nor using different notation:\n\nFurthermore, since the deformation gradient can be expressed as formula_15 where formula_16 is the second-order identity tensor, we have\n\nAlso, from the general expression for the Lagrangian and Eulerian finite strain tensors we have\n\nConsider a two-dimensional deformation of an infinitesimal rectangular material element with dimensions formula_19 by formula_20 (Figure 1), which after deformation, takes the form of a rhombus. From the geometry of Figure 1 we have\n\nFor very small displacement gradients, i.e., formula_2, we have\n\nThe normal strain in the formula_24-direction of the rectangular element is defined by\n\nand knowing that formula_26, we have\n\nSimilarly, the normal strain in the formula_28-direction, and formula_29-direction, becomes\n\nThe engineering shear strain, or the change in angle between two originally orthogonal material lines, in this case line formula_31 and formula_32, is defined as\n\nFrom the geometry of Figure 1 we have\n\nFor small rotations, i.e. formula_35 and formula_36 are formula_37 we have\n\nand, again, for small displacement gradients, we have\n\nthus\n\nBy interchanging formula_24 and formula_28 and formula_43 and formula_44, it can be shown that formula_45\n\nSimilarly, for the formula_28-formula_29 and formula_24-formula_29 planes, we have\n\nIt can be seen that the tensorial shear strain components of the infinitesimal strain tensor can then be expressed using the engineering strain definition, formula_51, as\n\nformula_52\n\nFrom finite strain theory we have\n\nFor infinitesimal strains then we have\n\nDividing by formula_55 we have\n\nFor small deformations we assume that formula_57, thus the second term of the left hand side becomes: formula_58.\n\nThen we have\n\nwhere formula_60, is the unit vector in the direction of formula_61, and the left-hand-side expression is the normal strain formula_62 in the direction of formula_63. For the particular case of formula_63 in the formula_65 direction, i.e. formula_66, we have\n\nSimilarly, for formula_68 and formula_69 we can find the normal strains formula_70 and formula_71, respectively. Therefore, the diagonal elements of the infinitesimal strain tensor are the normal strains in the coordinate directions.\n\nIf we choose an orthonormal coordinate system (formula_72) we can write the tensor in terms of components with respect to those base vectors as\nIn matrix form,\nWe can easily choose to use another orthonormal coordinate system (formula_75) instead. In that case the components of the tensor are different, say\nThe components of the strain in the two coordinate systems are related by\nwhere the Einstein summation convention for repeated indices has been used and formula_78. In matrix form\nor\n\nCertain operations on the strain tensor give the same result without regard to which orthonormal coordinate system is used to represent the components of strain. The results of these operations are called strain invariants. The most commonly used strain invariants are\nIn terms of components\n\nIt can be shown that it is possible to find a coordinate system (formula_83) in which the components of the strain tensor are\nThe components of the strain tensor in the (formula_83) coordinate system are called the principal strains and the directions formula_86 are called the directions of principal strain. Since there are no shear strain components in this coordinate system, the principal strains represent the maximum and minimum stretches of an elemental volume.\n\nIf we are given the components of the strain tensor in an arbitrary orthonormal coordinate system, we can find the principal strains using an eigenvalue decomposition determined by solving the system of equations\nThis system of equations is equivalent to finding the vector formula_86 along which the strain tensor becomes a pure stretch with no shear component.\n\nThe \"dilatation\" (the relative variation of the volume) is the trace of the tensor:\nActually, if we consider a cube with an edge length \"a\", it is a quasi-cube after the deformation (the variations of the angles do not change the volume) with the dimensions formula_90 and \"V\" = \"a\", thus\nas we consider small deformations,\ntherefore the formula.\n\nIn case of pure shear, we can see that there is no change of the volume.\n\nThe infinitesimal strain tensor formula_11, similarly to the Cauchy stress tensor, can be expressed as the sum of two other tensors: \n\nwhere formula_97 is the mean strain given by\n\nThe deviatoric strain tensor can be obtained by subtracting the mean strain tensor from the infinitesimal strain tensor:\n\nLet (formula_100) be the directions of the three principal strains. An octahedral plane is one whose normal makes equal angles with the three principal directions. The engineering shear strain on an octahedral plane is called the octahedral shear strain and is given by\nwhere formula_102 are the principal strains. \n\nThe normal strain on an octahedral plane is given by\nA scalar quantity called the equivalent strain, or the von Mises equivalent strain, is often used to describe the state of strain in solids. Several definitions of equivalent strain can be found in the literature. A definition that is commonly used in the literature on plasticity is\nThis quantity is work conjugate to the equivalent stress defined as\n\nFor prescribed strain components formula_11 the strain tensor equation formula_107 represents a system of six differential equations for the determination of three displacements components formula_108, giving an over-determined system. Thus, a solution does not generally exist for an arbitrary choice of strain components. Therefore, some restrictions, named \"compatibility equations\", are imposed upon the strain components. With the addition of the three compatibility equations the number of independent equations are reduced to three, matching the number of unknown displacement components. These constraints on the strain tensor were discovered by Saint-Venant, and are called the \"Saint Venant compatibility equations\".\n\nThe compatibility functions serve to assure a single-valued continuous displacement function formula_108. If the elastic medium is visualised as a set of infinitesimal cubes in the unstrained state, after the medium is strained, an arbitrary strain tensor may not yield a situation in which the distorted cubes still fit together without overlapping.\n\nIn index notation, the compatibility equations are expressed as\n\nIn real engineering components, stress (and strain) are 3-D tensors but in prismatic structures such as a long metal billet, the length of the structure is much greater than the other two dimensions. The strains associated with length, i.e., the normal strain formula_71 and the shear strains formula_112 and formula_113 (if the length is the 3-direction) are constrained by nearby material and are small compared to the \"cross-sectional strains\". Plane strain is then an acceptable approximation. The strain tensor for plane strain is written as:\n\nin which the double underline indicates a second order tensor. This strain state is called \"plane strain\". The corresponding stress tensor is:\n\nin which the non-zero formula_116 is needed to maintain the constraint formula_117. This stress term can be temporarily removed from the analysis to leave only the in-plane terms, effectively reducing the 3-D problem to a much simpler 2-D problem.\n\nAntiplane strain is another special state of strain that can occur in a body, for instance in a region close to a screw dislocation. The strain tensor for antiplane strain is given by\n\nThe infinitesimal strain tensor is defined as\nTherefore the displacement gradient can be expressed as\nwhere\nThe quantity formula_122 is the infinitesimal rotation tensor. This tensor is skew symmetric. For infinitesimal deformations the scalar components of formula_122 satisfy the condition formula_124. Note that the displacement gradient is small only if both the strain tensor and the rotation tensor are infinitesimal.\n\nA skew symmetric second-order tensor has three independent scalar components. These three components are used to define an axial vector, formula_125, as follows\nwhere formula_127 is the permutation symbol. In matrix form\nThe axial vector is also called the infinitesimal rotation vector. The rotation vector is related to the displacement gradient by the relation\nIn index notation\nIf formula_131 and formula_132 then the material undergoes an approximate rigid body rotation of magnitude formula_133 around the vector formula_125.\n\nGiven a continuous, single-valued displacement field formula_135 and the corresponding infinitesimal strain tensor formula_136, we have (see Tensor derivative (continuum mechanics))\nSince a change in the order of differentiation does not change the result, formula_138. Therefore\nAlso\nHence\n\nFrom an important identity regarding the curl of a tensor we know that for a continuous, single-valued displacement field formula_135,\nSince formula_144 we have\nformula_145\n\nIn cylindrical polar coordinates (formula_146), the displacement vector can be written as\nThe components of the strain tensor in a cylindrical coordinate system are given by :\nformula_148\n\nIn spherical coordinates (formula_149), the displacement vector can be written as\n\nThe components of the strain tensor in a spherical coordinate system are given by \n\n"}
{"id": "15539", "url": "https://en.wikipedia.org/wiki?curid=15539", "title": "Ion implantation", "text": "Ion implantation\n\nIon implantation is a low-temperature process by which ions of one element are accelerated into a solid target, thereby changing the physical, chemical, or electrical properties of the target. Ion implantation is used in semiconductor device fabrication and in metal finishing, as well as in materials science research. The ions can alter the elemental composition of the target (if the ions differ in composition from the target) if they stop and remain in the target. Ion implantation also causes chemical and physical changes when the ions impinge on the target at high energy. The crystal structure of the target can be damaged or even destroyed by the energetic collision cascades, and ions of sufficiently high energy (10s of MeV) can cause nuclear transmutation.\n\nIon implantation equipment typically consists of an ion source, where ions of the desired element are produced, an accelerator, where the ions are electrostatically accelerated to a high energy, and a target chamber, where the ions impinge on a target, which is the material to be implanted. Thus ion implantation is a special case of particle radiation. Each ion is typically a single atom or molecule, and thus the actual amount of material implanted in the target is the integral over time of the ion current. This amount is called the dose. The currents supplied by implants are typically small (micro-amperes), and thus the dose which can be implanted in a reasonable amount of time is small. Therefore, ion implantation finds application in cases where the amount of chemical change required is small.\n\nTypical ion energies are in the range of 10 to 500 keV (1,600 to 80,000 aJ). Energies in the range 1 to 10 keV (160 to 1,600 aJ) can be used, but result in a penetration of only a few nanometers or less. Energies lower than this result in very little damage to the target, and fall under the designation ion beam deposition. Higher energies can also be used: accelerators capable of 5 MeV (800,000 aJ) are common. However, there is often great structural damage to the target, and because the depth distribution is broad (Bragg peak), the net composition change at any point in the target will be small.\n\nThe energy of the ions, as well as the ion species and the composition of the target determine the depth of penetration of the ions in the solid: A monoenergetic ion beam will generally have a broad depth distribution. The average penetration depth is called the range of the ions. Under typical circumstances ion ranges will be between 10 nanometers and 1 micrometer. Thus, ion implantation is especially useful in cases where the chemical or structural change is desired to be near the surface of the target. Ions gradually lose their energy as they travel through the solid, both from occasional collisions with target atoms (which cause abrupt energy transfers) and from a mild drag from overlap of electron orbitals, which is a continuous process. The loss of ion energy in the target is called stopping and can be simulated with the binary collision approximation method.\n\nAccelerator systems for ion implantation are generally classified into medium current (ion beam currents between 10 μA and ~2 mA), high current (ion beam currents up to ~30 mA), high energy (ion energies above 200 keV and up to 10 MeV), and very high dose (efficient implant of dose greater than 10ions/cm).\n\nAll varieties of ion implantation beamline designs contain certain general groups of functional components (see image). The first major segment of an ion beamline includes a device known as an ion source to generate the ion species. The source is closely coupled to biased electrodes for extraction of the ions into the beamline and most often to some means of selecting a particular ion species for transport into the main accelerator section. The \"mass\" selection is often accompanied by passage of the extracted ion beam through a magnetic field region with an exit path restricted by blocking apertures, or \"slits\", that allow only ions with a specific value of the product of mass and velocity/charge to continue down the beamline. If the target surface is larger than the ion beam diameter and a uniform distribution of implanted dose is desired over the target surface, then some combination of beam scanning and wafer motion is used. Finally, the implanted surface is coupled with some method for collecting the accumulated charge of the implanted ions so that the delivered dose can be measured in a continuous fashion and the implant process stopped at the desired dose level.\n\nSemiconductor doping with boron, phosphorus, or arsenic is a common application of ion implantation. When implanted in a semiconductor, each dopant atom can create a charge carrier in the semiconductor after annealing. A hole can be created for a p-type dopant, and an electron for an n-type dopant. This modifies the conductivity of the semiconductor in its vicinity. The technique is used, for example, for adjusting the threshold of a MOSFET.\n\nIon implantation was developed as a method of producing the p-n junction of photovoltaic devices in the late 1970s and early 1980s, along with the use of pulsed-electron beam for rapid annealing, although it has not to date been used for commercial production.\n\nOne prominent method for preparing silicon on insulator (SOI) substrates from conventional silicon substrates is the \"SIMOX\" (separation by implantation of oxygen) process, wherein a buried high dose oxygen implant is converted to silicon oxide by a high temperature annealing process.\n\nMesotaxy is the term for the growth of a crystallographically matching phase underneath the surface of the host crystal (compare to epitaxy, which is the growth of the matching phase on the surface of a substrate). In this process, ions are implanted at a high enough energy and dose into a material to create a layer of a second phase, and the temperature is controlled so that the crystal structure of the target is not destroyed. The crystal orientation of the layer can be engineered to match that of the target, even though the exact crystal structure and lattice constant may be very different. For example, after the implantation of nickel ions into a silicon wafer, a layer of nickel silicide can be grown in which the crystal orientation of the silicide matches that of the silicon.\n\nNitrogen or other ions can be implanted into a tool steel target (drill bits, for example). The structural change caused by the implantation produces a surface compression in the steel, which prevents crack propagation and thus makes the material more resistant to fracture. The chemical change can also make the tool more resistant to corrosion.\n\nIn some applications, for example prosthetic devices such as artificial joints, it is desired to have surfaces very resistant to both chemical corrosion and wear due to friction. Ion implantation is used in such cases to engineer the surfaces of such devices for more reliable performance. As in the case of tool steels, the surface modification caused by ion implantation includes both a surface compression which prevents crack propagation and an alloying of the surface to make it more chemically resistant to corrosion.\n\nIon implantation can be used to achieve ion beam mixing, i.e. mixing up atoms of different elements at an interface. This may be useful for achieving graded interfaces or strengthening adhesion between layers of immiscible materials.\n\nIon implantation may be used to induce nano-dimensional particles in oxides such as sapphire and silica. The particles may be formed as a result of precipitation of the ion implanted species, they may be formed as a result of the production of an mixed oxide species that contains both the ion-implanted element and the oxide substrate, and they may be formed as a result of a reduction of the substrate, first reported by Hunt and Hampikian. Typical ion beam energies used to produce nanoparticles range from 50 to 150 keV, with ion fluences that range from 10 to 10 ions/cm. The table below summarizes some of the work that has been done in this field for a sapphire substrate. A wide variety of nanoparticles can be formed, with size ranges from 1 nm on up to 20 nm and with compositions that can contain the implanted species, combinations of the implanted ion and substrate, or that are comprised solely from the cation associated with the substrate.\n\nComposite materials based on dielectrics such as sapphire that contain dispersed metal nanoparticles are promising materials for optoelectronics and nonlinear optics.\n\nEach individual ion produces many point defects in the target crystal on impact such as vacancies and interstitials. Vacancies are crystal lattice points unoccupied by an atom: in this case the ion collides with a target atom, resulting in transfer of a significant amount of energy to the target atom such that it leaves its crystal site. This target atom then itself becomes a projectile in the solid, and can cause successive collision events.\nInterstitials result when such atoms (or the original ion itself) come to rest in the solid, but find no vacant space in the lattice to reside. These point defects can migrate and cluster with each other, resulting in dislocation loops and other defects.\n\nBecause ion implantation causes damage to the crystal structure of the target which is often unwanted, ion implantation processing is often followed by a thermal annealing. This can be referred to as damage recovery.\n\nThe amount of crystallographic damage can be enough to completely amorphize the surface of the target: i.e. it can become an amorphous solid (such a solid produced from a melt is called a glass). In some cases, complete amorphization of a target is preferable to a highly defective crystal: An amorphized film can be regrown at a lower temperature than required to anneal a highly damaged crystal. Amorphisation of the substrate can occur as a result of the beam damage. For example, yttrium ion implantation into sapphire at an ion beam energy of 150 keV to a fluence of 5*10 Y/cm produces an amorphous glassy layer approximately 110 nm in thickness, measured from the outer surface. [Hunt, 1999]\n\nSome of the collision events result in atoms being ejected (sputtered) from the surface, and thus ion implantation will slowly etch away a surface. The effect is only appreciable for very large doses.\n\nIf there is a crystallographic structure to the target, and especially in semiconductor substrates where the crystal structure is more open, particular crystallographic directions offer much lower stopping than other directions. The result is that the range of an ion can be much longer if the ion travels exactly along a particular direction, for example the <110> direction in silicon and other diamond cubic materials. This effect is called \"ion channelling\", and, like all the channelling effects, is highly nonlinear, with small variations from perfect orientation resulting in extreme differences in implantation depth. For this reason, most implantation is carried out a few degrees off-axis, where tiny alignment errors will have more predictable effects.\n\nIon channelling can be used directly in Rutherford backscattering and related techniques as an analytical method to determine the amount and depth profile of damage in crystalline thin film materials.\n\nIn fabricating wafers, toxic materials such as arsine and phosphine are often used in the ion implanter process. Other common carcinogenic, corrosive, flammable, or toxic elements include antimony, arsenic, phosphorus, and boron. Semiconductor fabrication facilities are highly automated, but residue of hazardous elements in machines can be encountered during servicing and in vacuum pump hardware.\n\nHigh voltage power supplies used in ion accelerators necessary for ion implantation can pose a risk of electrocution. In addition, high-energy atomic collisions can generate X-rays and, in some cases, other ionizing radiation and radionuclides. In addition to high voltage, particle accelerators such as radio frequency linear particle accelerators and laser wakefield plasma accelerators other hazards.\n\n"}
{"id": "7837525", "url": "https://en.wikipedia.org/wiki?curid=7837525", "title": "John Scales Avery", "text": "John Scales Avery\n\nJohn Scales Avery (born in 1933 in Lebanon to American parents) is a theoretical chemist noted for his research publications in quantum chemistry, thermodynamics, evolution, and history of science. Since the early 1990s, Avery has been an active World peace activist. During these years, he was part of a group associated with the Pugwash Conferences on Science and World Affairs. In 1995, this group received the Nobel Peace Prize for their efforts. Presently, he is an Associate Professor in quantum chemistry at the University of Copenhagen. His 2003 book \"Information Theory and Evolution\" set forth the view that the phenomenon of life, including its origin and evolution, that including human cultural evolution, has it background situated over thermodynamics, statistical mechanics, and information theory.\n\nAvery’s parents were both born in the United States, in the state of Michigan, where they studied at the University of Michigan. His father studied medicine while his mother studied bacteriology. After graduation, his parents did research together at the Marine Biological Laboratory in Woods Hole, Massachusetts. Later, his father did research in a borderline area between physics and medicine with Arthur Holly Compton, discoverer of the \"Compton effect\", at the University of Chicago.\nIn 1926, his father moved the family to Beirut, where his father worked as a professor of anatomy at the American University of Beirut. The family stayed in Beirut until the start of World War II. It was during these tumultuous years that John Scales Avery was born.\n\n\nIn his recent 2003 book \"Information Theory and Evolution\", Avery combines information theory with thermodynamics to account for the phenomenon of life, including its origin and evolution. Since the beginning of the formulation of the second law of thermodynamics in the 1860s, which states that the entropy, or disorder, of an isolated system tends to increase with time, there has been a debate as to how this law relates to the \"ordering\" process of evolution. The apparent paradox between the second law of thermodynamics and the high degree of order and complexity produced by living systems, according to Avery, has its resolution \"in the information content of the Gibbs free energy that enters the biosphere from outside sources.\"\n\nSince 1990, Avery has been the Contact Person for Denmark the Pugwash Conferences on Science and World Affairs. In 1995, Avery was part of a group that shared in the Nobel Peace Prize for their work in the 1990s in organizing the Pugwash Conferences on Science and World Affairs. In 1998, Avery was elected to the Danish Peace Commission. During the years 1988-97, Avery was the Technical Advisor at the World Health Organization, Regional Office for Europe. In 2004, Avery became the Chairman of the Danish Peace Academy.\n\n\nScientific books:\n\n\nAs well as more than 150 [www.ki.ku.dk/dokumenter/PDF/Publications-JS-Avery.pdf].\n\n"}
{"id": "41294094", "url": "https://en.wikipedia.org/wiki?curid=41294094", "title": "Jonathan Deal", "text": "Jonathan Deal\n\nJonathan Deal is a South African environmentalist. He was awarded the Goldman Environmental Prize in 2013, in particular for his efforts on protecting the Karoo region, leading a team of scientists to bring forward environmental impacts of planned exploitation of possible shale gas in the region.\n"}
{"id": "11546880", "url": "https://en.wikipedia.org/wiki?curid=11546880", "title": "Kudurru for Ritti-Marduk", "text": "Kudurru for Ritti-Marduk\n\nThe Kudurru for Šitti-Marduk is a white limestone boundary stone (Kudurru) of Nebuchadrezzar I, a king of the 2nd Dynasty of Isin, ca. the late 12th century BC. He is known to have made at least four kudurru boundary stones.\n\nSome kudurrus are known for their representations of the king, etc, who conscripted the stones production. Most kudurrus are attested by honored gods of Mesopotamia and are often displayed graphically in segmented registers on the stone.\n\nThe obverse of the \"Kudurru for Šitti-Marduk\" is composed of six registers, with gods, beings (a Scorpion man for example), etc. The recto contains cuneiform text, relating the military services of Šitti-Marduk.\n\n\n"}
{"id": "43163812", "url": "https://en.wikipedia.org/wiki?curid=43163812", "title": "Lilapsophobia", "text": "Lilapsophobia\n\nLilapsophobia is an abnormal fear of tornadoes or hurricanes. Lilapsophobia is considered the more severe type of astraphobia, which is a fear of thunder and lightning.\n\nThe Greek basis word is λαῖλα|ψ -απος \"laíla|ps\" -\"apos\" for which reason the term should strictly have been *lailapophobia – like myrmecophobia from \"mýrmē|x\" -\"ēkos\". Greek words ending in ψ (\"ps\") and ξ (\"ks\") would regularly become -pos / -kos (respectively) in oblique cases, conventionally given as the genitive form. This rule also obtains for Latin, cf. \"pax\", \"pac\"|\"is\", and it is from the accusative form \"pacem\" that all Romance languages have taken their words for “peace”. Historically this rule has been “forgotten” – one result is the fallacious neologism “lilapsophobia”.\n\nHow the i came into the picture instead of ai remains to be settled. \"Hypothesis\": Since the Greek word \"lailaps\" would reflect American usage in pronouncing *lilaps [laílæps] – it is in the US that tornadoes are commonplace – the latter “hypercorrect” version became the written form. \n\nLike many phobias, lilapsophobia is caused by an unwanted experience, specifically tornadoes or hurricanes that cause injuries, destruction, or loss of loved ones to self or others they know. People who survive those storms should seek professional advice, especially to determine if a person is suffering post-traumatic stress disorder. This phobia can even be caused by learning news about tornadoes or hurricanes using the media, like television, internet, radio, or newspaper, even though they happened far away from homes.\n\nIf a person learns that someone in the family has the phobia, then that person is more likely to suffer from it.\n\nMental and emotional symptoms of lilapsophobia include\n\nPhysical symptoms of lilapsophobia include\n\nMany lilapsophobes also suffer autophobia, fear of being alone. Sufferers often make arrangements with people they know to help soothe the fear.\n\nLilapsophobes spend a lot of time watching the weather or checking weather online to keep an eye out for oncoming storms. When a storm hits, sufferers either watch for severe weather alerts constantly or take cover, like under the bed or in the windowless room. In the extreme cases, sufferers take tornado shelter as soon as rain starts falling, usually in the basement or storm shelter. Sufferers who have weather radio or mobile phones can watch the radar and alerts using it while hiding.\n\nLike astraphobia, lilapsophobia is a common fear for children, although less common. Because children are just learning to distinguish between fantasy and reality, major storm broadcasts on television or discussion by parents can cause fear that the storm is coming with a tornadic potential or a hurricane.\n\nBecause fear is a part of normal child development, this phobia is not diagnosed unless if persisted for more than six months. Parents should conquer the child's fear by telling them how rare the major storms that hit hometown area are.\n\nLike many other phobias, lilapsophobia can often be treated using cognitive-behavioral therapy, but if it stems from post-traumatic stress disorder, then alternative therapy may be more recommended.\n\nIn the 1996 film \"Twister\", Dr. Jo Harding (Helen Hunt), while becoming a storm chaser, suffers from lilapsophobia due to her father's death in a tornado when she was a child.\n\nThe 2011 tornado in Joplin prompted Karin R. Herrmann of Miami, Oklahoma, who suffered from lilapsophobia, to write about her experience.\n\n"}
{"id": "10400397", "url": "https://en.wikipedia.org/wiki?curid=10400397", "title": "Mixed dark matter", "text": "Mixed dark matter\n\nMixed dark matter (MDM) is a theory of dark matter (DM) proposed during the late 1990s.\n\nMixed dark matter is also called hot + cold dark matter. The most abundant form of dark matter is cold dark matter, almost one fourth of the energy contents of the Universe. Neutrinos are the only known particles whose Big-Bang thermal relic should compose at least a fraction of Hot dark matter (HDM), albeit other candidates are speculated to exist.. In the early 1990s, the power spectrum of fluctuations in the galaxy clustering did not agree entirely with the predictions for a standard cosmology built around pure cold DM. Mixed dark matter with a composition of about 80% cold and 20% hot (neutrinos) was investigated and found to agree better with observations. This large amount of HDM was made obsolete by the discovery in 1998 of the acceleration of universal expansion, which eventually led to the dark energy + dark matter paradigm of this decade.\n\nThe cosmological effects of cold DM are almost opposite to the hot DM effects. Given that cold DM promotes the growth of large scale structures, it is often believed to be composed of Weakly interacting massive particles (WIMPs). Conversely hot DM suffers of free-streaming for most of the history of the Universe, washing-out the formation of small scales. In other words, the mass of hot DM particles is too small to produce the observed gravitationally bounded objects in the Universe. For that reason, the hot DM abundance is constrained by Cosmology to less than one percent of the Universe contents.\n\nThe Mixed Dark Matter scenario recovered relevance when DM was proposed to be a thermal relic of a Bose–Einstein condensate made of very light bosonic particles, as light as neutrinos or even lighter like the Axion. This cosmological model predicts that cold DM is made of a large number of condensed particles, while a small fraction of these particles resides in excited energetic states contributing to hot DM.\n"}
{"id": "975637", "url": "https://en.wikipedia.org/wiki?curid=975637", "title": "Moustache wax", "text": "Moustache wax\n\nMoustache wax is a stiff pomade applied to a moustache as a grooming aid to hold the hairs in place, especially at the extremities. The required product strength (or stiffness) is based on whisker length and the desired style. It can also have restorative properties, which become more important as the hair length increases. The wax is usually scented and sometimes pigmented with dyes; high end products utilize various combinations of iron oxide to create darker shades.\n\nGenerally less than a fingernail of wax is used when applied. More sophisticated recipes may include gum arabic and a soap, scent and colouring may also be added if desired, to either strengthen the hold or for comfort.\n\n\nIn addition to the wax itself, more-experienced \"waxers\" use a moustache wax remover and conditioner. The reason for this is that warm soapy water (used by the novice) removes wax build-up but damages the bristles by stripping them of natural oils, so an oilbased moustache wax remover, that may double as a leavein conditioner, is preferred by some.\n"}
{"id": "5721668", "url": "https://en.wikipedia.org/wiki?curid=5721668", "title": "Mälarenergi", "text": "Mälarenergi\n\nMälarenergi AB is a city-owned electric power and district heating provider based in Västerås, Sweden. Mälarenergi has around 600 employees and six subsidiaries: Mälarenergi Försäljning AB, Mälarenergi Vattenkraft AB, Mälarenergi Stadsnät AB, Stadsnät i Svealand AB med dotterbolaget SamKom AB, Mälarenergi Ångturbinen AB and Mälarenergi Fastighet AB.\n\nThe company owns the grid in Västmanland and produces electricity, district heating and cooling with a turnover of 3.0 billion SEK, and 145 000 customers, .\n\nMälarenergi owns and operates a number of plants, mostly for the production of electricity and heating but also plants for tap water production and waste water purification. The biggest plant is Kraftvärmeverket in Västerås. This is Sweden's largest combined power and heating plant, and the newest part, called Block 6, can use waste as fuel.\n\n"}
{"id": "13370236", "url": "https://en.wikipedia.org/wiki?curid=13370236", "title": "NORBIT", "text": "NORBIT\n\nIn electronics, the NORBIT family of modules is a very early form (since 1960) of digital logic developed by Philips (and also provided through and Mullard) that uses modules containing discrete components to build logic function blocks in resistor–transistor logic (RTL) or diode–transistor logic (DTL) technology.\n\nThe system was originally conceived as building blocks for solid-state hard-wired programmed logic controllers (the predecessors of programmable logic controllers (PLC)) to replace electro-mechanical relay logic in industrial control systems for process control and automation applications, similar to early Telefunken/AEG Logistat, Siemens , BBC Sigmatronic, ACEC Logacec or Estacord systems.\n\nEach available logical function was recognizable by the color of its plastic container, black, blue, red, green, violet, etc. The most important circuit block contained a NOR gate (hence the name), but there were also blocks containing drivers, and a timer circuit similar to the later 555 timer IC.\n\nThe original \"Norbit\" modules of the YL 6000 series introduced in 1960 had potted single in-line packages with up to ten long flying leads arranged in two groups of up to five leads in a row. These modules were specified for frequencies of less than 1 kHz at ±24 V supply.\n\nAlso available in 1960 were so called \"Combi-Element\" modules in single-in line packages with ten evenly spaced stiff leads in a row (5.08 mm / 0.2-inch pitch) for mounting on a PCB. They were grouped in the 1-series (aka \"100 kHz series\") with ±6 V supply. The newer 10-series and 20-series had similarly sized packages, but came with an additional parallel row of nine staggered leads for a total of 19 leads. The 10-series uses germanium alloy transistors, whereas in the 20-series silicon planar transistors are used for a higher cut-off frequency of up to 1 MHz (vs. 30 kHz) and a higher allowed temperature range of +85 °C (vs. +55 °C).\n\nIn 1967, the Philips/Mullard NORBIT 2 aka Valvo NORBIT-S family of modules was introduced, first consisting of the 60-series for frequencies up to 10 kHz at a single supply voltage of 24 V, only. Later, the 61-series, containing thyristor trigger and control modules, was added. A 90-series became available in the mid-1970s as well. There were three basic types contained in a large (one by two inch-sized) 17 pins dual in-line package, with nine pins spaced 5.08 mm (0.2-inch) on one side and eight staggered pins on the other side.\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "1762868", "url": "https://en.wikipedia.org/wiki?curid=1762868", "title": "Negative luminescence", "text": "Negative luminescence\n\nNegative luminescence is a physical phenomenon by which an electronic device emits less thermal radiation when an electric current is passed through it than it does in thermal equilibrium (current off). When viewed by a thermal camera, an operating negative luminescent device looks colder than its environment.\n\nNegative luminescence is most readily observed in semiconductors. Incoming infrared radiation is absorbed in the material by the creation of an electron–hole pair. An electric field is used to remove the electrons and holes from the region before they have a chance to recombine and re-emit thermal radiation. This effect occurs most efficiently in regions of low charge carrier density.\n\nNegative luminescence has also been observed in semiconductors in orthogonal electric and magnetic fields. In this case, the junction of a diode is not necessary and the effect can be observed in bulk material. A term that has been applied to this type of negative luminescence is \"galvanomagnetic luminescence\".\n\nNegative luminescence might appear to be a violation of Kirchhoff's law of thermal radiation. This is not true, as the law only applies in thermal equilibrium.\n\nAnother term that has been used to describe negative luminescent devices is \"Emissivity switch\", as an electric current changes the effective emissivity.\n\nThis effect was first seen by Russian physicists in the 1960s in A.F.Ioffe Physicotechnical Institute, Leningrad, Russia. Subsequently, it was studied in semiconductors such as indium antimonide (InSb), germanium (Ge) and indium arsenide (InAs) by workers in West Germany, Ukraine (Institute of Semiconductor Physics, Kiev), Japan (Chiba University) and the United States. It was first observed in the mid-infrared (3-5 µm wavelength) in the more convenient diode structures in InSb heterostructure diodes by workers at the Defence Research Agency, Great Malvern, UK (now QinetiQ). These British workers later demonstrated LWIR band (8-12 µm) negative luminescence using mercury cadmium telluride diodes.\n\nLater the Naval Research Laboratory, Washington DC, started work on negative luminescence in mercury cadmium telluride (HgCdTe). The phenomenon has since been observed by several university groups around the world.\n\n\n\n"}
{"id": "4193469", "url": "https://en.wikipedia.org/wiki?curid=4193469", "title": "New England Wild Flower Society", "text": "New England Wild Flower Society\n\nFounded in 1900, the New England Wild Flower Society is the nation's oldest plant conservation organization. The society is dedicated to the preservation of native plants and operates Garden in the Woods (a native plant botanical garden) at its headquarters in Framingham, Massachusetts. It also offers courses on topics of conservation and horticulture of native plants, manages a \"conservation corps\" of volunteers throughout New England, operates several native plant sanctuaries, and offers nursery-propagated native plants for sale at its two nurseries.\n\n\n"}
{"id": "9906421", "url": "https://en.wikipedia.org/wiki?curid=9906421", "title": "New South Wales Coal Compensation Review Tribunal", "text": "New South Wales Coal Compensation Review Tribunal\n\nThe New South Wales Coal Compensation Review Tribunal was a tribunal in New South Wales, a state of Australia, which had responsibility for determining appeals about compensation payable when coal in the state was compulsorily acquired during 1981. The tribunal was in operation between 1985 and 1 January 2008.\n\nIn 1981 the New South Wales Government passed legislation which vested all unmined coal to the crown. The effect of \"Coal Acquisition Act 1981\" (NSW) was that the government became the owner of all unmined coal in the state. Vesting of the ownership of the coal enabled the government to tax the mining of coal, a situation it was not allowed to do when coal was privately owned. According to the New South Wales Coal Compensation Board’s annual report, the net benefit to the State from the acquisition of private coal is estimated at around $10 billion after taking into account compensation payments totalling $682 million\n\nThe board and the tribunal were established in 1985 to address concerns by former coal owners through the introduction of the Act. Both bodies were the result of recommendations of the Coal Compensation Taskforce Report.\n\nThe tribunal was established under the \"Coal Acquisition (Compensation) Arrangements 1985\" (NSW). Those arrangements were made by the Governor of New South Wales under the \"Coal Acquisition Act 1981\" (NSW), a law which entitles the governor to determine what compensation should be payable for the acquisition of coal in 1981. The tribunal consisted of six members appointed by the Governor. Two of those members were lawyers nominated by the Attorney General of New South Wales. The remaining four members were nominated by the Minister for Mineral Resources and were generally persons who have expertise in the coal industry.\n\nThe Governor may appoint one of the lawyer members as the chairperson and the other as deputy chairperson. There was also provision for the Governor to appoint alternates for the members.\n\nThe tribunal had jurisdiction to decide whether the compensation determined as payable by the board as a result of the compulsory acquisition of coal is correct. It may also determine the amount of compensation payable as a result of that acquisition. The tribunal sat as a three-member panel with either the chairperson or deputy presiding.\n\nThe chairperson immediately prior to the Tribunal's abolition was Mr G. R. Leader.\n\nAccording to the tribunal's 2005 Annual Report, it determined 67 appeals in 2004 and had 26 outstanding. The New South Wales Government flagged that it wished for all compensation claims to be determined by March 2007 and the tribunal will cease once all those claims have been finalised.\n\nThere were no appeal rights from the decision of the tribunal. In certain cases, prerogative relief may be available from the Supreme Court of New South Wales, such as \"NSW Coal Compensation Board\" v Nardell Colliery Pty Ltd\".\n\nUp until its abolition in 2008, the tribunal had its office at Level 16, No. 1 Castlereagh Street, Sydney.\n\n\n"}
{"id": "15365696", "url": "https://en.wikipedia.org/wiki?curid=15365696", "title": "Nuclear Information Service", "text": "Nuclear Information Service\n\nNIS (Nuclear Information Service) is a not-for-profit, independent information service based in the UK, which works to promote public awareness and foster debate on nuclear disarmament and related safety and environmental issues.\n\nNIS conducts original research, providing information on the public interest issues surrounding nuclear weapons. This results in reports, articles, press releases, legal action and consultation services to other organizations, parliament and government agencies.\n\nNIS has a unique role in the UK nuclear disarmament field. Over the years it has focused in from general peace and disarmament work to concentrate on serving the disarmament community, media and decision-makers with research on the maintenance and transport of nuclear weapons. This work has included research on the Atomic Weapons Establishment at Aldermaston and Burghfield and includes work on warhead modification, new warhead development, decommisioning, nuclear convoys, outsourcing/privatisation, costs and delays, safety and accidents.\n\nNIS circulates information through its website http://www.nuclearinfo.org, newsletters and blogs and links individuals working on similar issues. Its archives go back to 1991 and NIS is the successor to NIP (Nuclear Information Project). It became an incorporated company limited by guarantee in 2000 and is funded by charitable foundations and public donations.\n\nPatrons: Jonathon Porritt, Nick Ritchie\n\n"}
{"id": "44853585", "url": "https://en.wikipedia.org/wiki?curid=44853585", "title": "Omega-3 carboxylic acids", "text": "Omega-3 carboxylic acids\n\nOmega-3 carboxylic acids (Epanova) is an FDA approved prescription medication used alongside a low fat and low cholesterol diet that lowers high triglyceride (fat) levels in adults with very high levels. This was the third class of fish oil-based drug to be approved for use as a drug. The first approval was granted in the US came in 2014. These fish oil drugs are similar to fish oil dietary supplements but the ingredients are better controlled and have been tested in clinical trials.\n\nOmega-3 carboxylic acids are used in addition to changes in diet to reduce triglyceride levels in adults with severe (≥ 500 mg/dL) hypertriglyceridemia.\n\nIntake of large doses (2.0 to 4.0 g/day) of long-chain omega-3 fatty acids as prescription drugs or dietary supplements are generally required to achieve significant (> 15%) lowering of triglycerides, and at those doses the effects can be significant (from 20% to 35% and even up to 45% in individuals with levels greater that 500 mg/dL). It appears that both eicosapentaenoic acid (EPA) and docosahexaenoic acid (DHA) lower triglycerides, but DHA appears to raise LDL-C (\"bad cholesterol\") more than EPA, while DHA raises HDL-C (\"good cholesterol\") while EPA does not.\n\nThere are other omega-3 fish oil based drugs on the market that have similar uses and mechanisms of action.\n\n\nThere are many fish oil dietary supplements on the market. There appears to be little difference between in effect between dietary supplement and prescription forms of omega-3 fatty acids but EPA and DHA ethyl esters (prescription forms) work less well when taken on an empty stomach or with a low-fat meal. The ingredients of dietary supplements are not as carefully controlled as prescription products and have not been fixed and tested in clinical trials, as prescription drugs have, and the prescription forms are more concentrated, requiring fewer capsules to be taken and increasing the likelihood of compliance.\n\nSpecial caution should be taken with people who have with fish and shellfish allergies. In addition, as with other omega-3 fatty acids, taking omega-3 carboxylic acids puts people who are on anticoagulants at risk for prolonged bleeding time.\n\nSide effects include diarrhea, nausea, abdominal pain, and burping.\n\nOmega-3 carboxylic acids have not been tested in pregnant women and are rated pregnancy category C; it is excreted in breast milk and the effects on infants are not known.\n\nOmega-3 carboxylic acids are directly absorbed in the small intestine; maximum plasma concentrations are achieved between 5–8 hours after dosing for total EPA and between 5–9 hours after dosing for total DHA. Both DHA and EPA are mainly metabolized in the liver like other fatty acids derived from food. The half-life of EPA from Omega-3 carboxylic acids is about 37 hours, and for DHA about 46 hours.\n\nOmega-3 carboxylic acids, like other omega-3 fatty acid based drugs, appears to reduce production of triglycerides in the liver, and to enhance clearance of triglycerides from circulating very low-density lipoprotein (VLDL) particles; the way it does that is not clear, but potential mechanisms include increased breakdown of fatty acids; inhibition of diglyceride acyltransferase which is involved in biosynthesis of triglycerides in the liver; and increased activity of lipoprotein lipase in blood.\n\nOmega-3 carboxylic acids are derived from fish oil and are a purified mixture of the polyunsaturated free fatty acids docosahexaenoic acid (DHA) and eicosapentaenoic acid (EPA). The drug contains a concentration of DHA at 15-25 % and a concentration of EPA at 50-60%.\n\nOmega-3 carboxylic acids was approved by the US FDA in May 2014 and was the third prescription form of an omega-3 product approved in the United States but the first in free fatty acid form. Development was completed and regulatory was obtained by AstraZeneca, but omega-3 carboxylic acids were first created at Chrysalis Pharma in Switzerland; Princeton-based Omthera had obtained rights from Chysalis, and Astrazeneca acquired Omthera in 2013 for $323 million in cash along with up to $120 million in milestones. At the time Epanova was approved, AstraZeneca's plan was to develop a combination product with rosuvastatin, the patent on which was set to expire in 2016.\n"}
{"id": "670364", "url": "https://en.wikipedia.org/wiki?curid=670364", "title": "One-line diagram", "text": "One-line diagram\n\nIn power engineering, a one-line diagram or single-line diagram (SLD) is a simplified notation for representing a three-phase power system. The one-line diagram has its largest application in power flow studies. Electrical elements such as circuit breakers, transformers, capacitors, bus bars, and conductors are shown by standardized schematic symbols. Instead of representing each of three phases with a separate line or terminal, only one conductor is represented. It is a form of block diagram graphically depicting the paths for power flow between entities of the system. Elements on the diagram do not represent the physical size or location of the electrical equipment, but it is a common convention to organize the diagram with the same left-to-right, top-to-bottom sequence as the switchgear or other apparatus represented. A one-line diagram can also be used to show a high level view of conduit runs for a PLC control system.\n\nThe theory of three-phase power systems tells us that as long as the loads on each of the three phases are balanced, we can consider each phase separately. In power engineering, this assumption is often useful, and to consider all three phases requires more effort with very little potential advantage. An important and frequent exception is an asymmetric fault on only one or two phases of the system. \n\nA one-line diagram is usually used along with other notational simplifications, such as the per-unit system.\n\nA secondary advantage to using a one-line diagram is that the simpler diagram leaves more space for non-electrical, such as economic, information to be included.\n\nWhen using the method of symmetrical components, separate one-line diagrams are made for each of the positive, negative and zero-sequence systems. This simplifies the analysis of unbalanced conditions of a polyphase system. Items that have different impedances for the different phase sequences are identified on the diagrams. For example, in general a generator will have different positive and negative sequence impedance, and certain transformer winding connections block zero-sequence currents. The unbalanced system can be resolved into three single line diagrams for each sequence, and interconnected to show how the unbalanced components add in each part of the system.\n\n"}
{"id": "23820924", "url": "https://en.wikipedia.org/wiki?curid=23820924", "title": "Parris Manufacturing Company", "text": "Parris Manufacturing Company\n\nThe Parris Manufacturing Company of Savannah, Tennessee is an American company that primarily manufactures toy guns.\n\nIowa inventor William G. Dunn (1883–1968) originally owned a hardware business in Clarinda, Iowa. With the start of World War I, Dunn created the Dunn Counterbalance Company that operated out of the hardware store until he built a factory in Clarinda and renamed his company the Dunn Manufacturing Company. Dunn was responsible for 48 patents, 7 to do with wind power. The Dunn Manufacturing Company also built a 3-seat Cruzeaire Monoplane in 1928.\n\nIn 1936 Dunn teamed up with Cecil Lewis \"Catfish\" Parris, a marketing specialist, to form the Parris-Dunn company. The company was formed with Parris as President and Dunn as Vice President to sell a wind driven generator for farms to recharge electrical items. The Dunn Governing Principle was able to use a controllable propellor speed with fewer moving parts than other wind generators. The idea was successful with over 37,000 units created the next year that were sold throughout the USA and foreign countries. Philco Electronics contracted with Parris-Dunn to put their well known name on their \"Skychargers\" and marketed the items to their customers.\n\nWith surplus American arms sent to England and other countries prior to America's involvement in World War II, the American military suffered a severe shortage of rifles. As the U.S. Government decided that wind generators were not a priority item, Parris-Dunn could only manufacture spare parts for the ones in use. When the company sought some war work the government recommended making dummy training rifles with wood and metal that copied the M1903 Springfield.\nImpressed by the rifles made for the US Army, the US Navy contacted Parris-Dunn in June 1942 to order their own rifles that they insisted have bayonet studs, adjustable rear sights, and working triggers with a clicker mechanism; the weapon becoming the USN MK 1 Dummy Training Rifle. The Navy ordered 190,000 for their first order. A plastic copy of the M1905 bayonet and scabbard was manufactured\nby the Pro-Phy-Lac-Tic Brush Company who usually manufactured toothbrushes. With walnut in high demand for real rifles, Parris-Dunn used cheaper wood further reducing the cost of the rifles.\n\nOver 500,000 rifles were produced with Parris-Dunn being awarded the Army-Navy ‘E’ Award.\n\nWith the end of the war, the training rifles were sold as surplus. From 1943, Parris had approached the company's chief engineer, Maurice Greiman with the idea of Parris-Dunn manufacturing toy weapon popguns of a Western style that shot corks for the use of children and traveling carnivals. Parris had Cowboy models called Cowboy Pla Guns, Trainerifles, and Shootrite guns in larger sizes for adults to practice cork gun marksmanship indoors.\n\nDunn retired in 1949 with Parris moving the company to Savannah, Tennessee where it remains.\nDuring the Civil War Centennial the company manufactured replica cork shooting Civil War muskets and pistols.\n\nThe company manufactured full size replicas of the M1903 Springfield and smaller sized models for children that featured a working bolt with a dummy bullet, leather sling, the clicker action, and a smaller rubber bayonet similar to the M1 bayonet.\n\nIn the 1960s Parris used the cork gun design to make several types of BB guns.\n\nSo much of the mail to the Parris Manufacturing Company was missent to Savannah, Georgia that Cecil Parris visited the local post office and had the postmaster add \"Catfish Capital of the World\" to the postmark.\n\nIn 1953 Parris started the idea of Kadet Military Drill Teams that communities could form. Purchasers of Kadet toy weapons would receive a free comic book drawn by Kurt Schaffenberger for Customs Comics Inc. telling the story of an imaginary boy named Bob Duncan who became a Kadet and advanced to the rank of Captain. The comic book featured instructions how to do the American manual of arms and offered the full range of Kadet Trainer Rifles, cork firing weapons, rank insignia, and dress and fatigue uniforms for both girls and boys featuring a white riding helmet. The brochure also featured a Kadet Officer's Training School located on a farm in Savannah and a Kadet song. From 1953 to around 1970 when the programme ended, the Kadets had over 16,000 members.\n\n"}
{"id": "614192", "url": "https://en.wikipedia.org/wiki?curid=614192", "title": "Paschen's law", "text": "Paschen's law\n\nPaschen's law is an equation that gives the breakdown voltage, that is, the voltage necessary to start a discharge or electric arc, between two electrodes in a gas as a function of pressure and gap length. It is named after Friedrich Paschen who discovered it empirically in 1889.\n\nPaschen studied the breakdown voltage of various gases between parallel metal plates as the gas pressure and gap distance were varied:\n\nFor a given gas, the voltage is a function only of the product of the pressure and gap length. The curve he found of voltage versus the pressure-gap length product \"(right)\" is called Paschen's curve. He found an equation that fit these curves, which is now called Paschen's law.\n\nAt higher pressures and gap lengths, the breakdown voltage is approximately \"proportional\" to the product of pressure and gap length, and the term Paschen's law is sometimes used to refer to this simpler relation. However, this is only roughly true, over a limited range of the curve.\n\nEarly vacuum experimenters found a rather surprising behavior. An arc would sometimes take place in a long irregular path rather than at the minimal distance between the electrodes. For example, in air, at a pressure of one atmosphere, the distance for minimal breakdown voltage is about 7.5 µm. The voltage required to arc this distance is 327 V, which is insufficient to ignite the arcs for gaps that are either wider or narrower. For a 3.5 µm gap, the required voltage is 533 V, nearly twice as much. If 500 V were applied, it would not be sufficient to arc at the 2.85 µm distance, but would arc at a 7.5 µm distance.\n\nPaschen found that breakdown voltage was described by the equation\n\nwhere formula_2 is the breakdown voltage in volts, formula_3 is the pressure in pascals, formula_4 is the gap distance in meters, formula_5 is the secondary-electron-emission coefficient (the number of secondary electrons produced per incident positive ion), formula_6 is the saturation ionization in the gas at a particular formula_7 (electric field/pressure), and formula_8 is related to the excitation and ionization energies.\n\nThe constants formula_6 and formula_8 are determined experimentally and found to be roughly constant over a restricted range of formula_7 for any given gas. For example, air with an formula_7 in the range of 450 to 7500 V/(kPa·cm), formula_6 = 112.50 (kPa·cm) and formula_8 = 2737.50 V/(kPa·cm).\n\nThe graph of this equation is the Paschen curve. By differentiating it with respect to formula_15 and setting the derivative to zero, the minimal voltage can be found. This yields\n\nand predicts the occurrence of a minimal breakdown voltage for formula_15 = 7.5×10 m·atm. This is 327 V in air at standard atmospheric pressure at a distance of 7.5 µm.\n\nThe composition of the gas determines both the minimal arc voltage and the distance at which it occurs. For argon, the minimal arc voltage is 137 V at a larger 12 µm. For sulfur dioxide, the minimal arc voltage is 457 V at only 4.4 µm.\n\nFor air at standard conditions for temperature and pressure (STP), the voltage needed to arc a 1-meter gap is about 3.4 MV. The intensity of the electric field for this gap is therefore 3.4 MV/m.\n\nThe electric field needed to arc across the minimal-voltage gap is much greater than what is necessary to arc a gap of one meter. For a 7.5 µm gap the arc voltage is 327 V, which is 43 MV/m. This is about 13 times greater than the field strength for the 1-meter gap. The phenomenon is well verified experimentally and is referred to as the Paschen minimum.\n\nThe equation loses accuracy for gaps under about 10 µm in air at one atmosphere\nand incorrectly predicts an infinite arc voltage at a gap of about 2.7 micrometers. Breakdown voltage can also differ from the Paschen curve prediction for very small electrode gaps, when field emission from the cathode surface becomes important.\n\nThe mean free path of a molecule in a gas is the average distance between its collision with other molecules. This is inversely proportional to the pressure of the gas. In air the mean free path of molecules is about 96 nm. Since electrons are much smaller, their average distance between colliding with molecules is about 5.6 times longer, or about 0.5 µm. This is a substantial fraction of the 7.5 µm spacing between the electrodes for minimal arc voltage. If the electron is in an electric field of 43 MV/m, it will be accelerated and acquire 21.5 eV of energy in 0.5 µm of travel in the direction of the field. The first ionization energy needed to dislodge an electron from nitrogen molecule is about 15.6 eV. The accelerated electron will acquire more than enough energy to ionize a nitrogen molecule. This liberated electron will in turn be accelerated, which will lead to another collision. A chain reaction then leads to avalanche breakdown, and an arc takes place from the cascade of released electrons.\n\nMore collisions will take place in the electron path between the electrodes in a higher-pressure gas. When the pressure–gap product formula_15 is high, an electron will collide with many different gas molecules as it travels from the cathode to the anode. Each of the collisions randomizes the electron direction, so the electron is not always being accelerated by the electric field—sometimes it travels back towards the cathode and is decelerated by the field.\n\nCollisions reduce the electron's energy and make it more difficult for it to ionize a molecule. Energy losses from a greater number of collisions require larger voltages for the electrons to accumulate sufficient energy to ionize many gas molecules, which is required to produce an avalanche breakdown.\n\nOn the left side of the Paschen minimum, the formula_15 product is small. The electron mean free path can become long compared to the gap between the electrodes. In this case, the electrons might gain lots of energy, but have fewer ionizing collisions. A greater voltage is therefore required to assure ionization of enough gas molecules to start an avalanche.\n\nTo calculate the breakthrough voltage, a homogeneous electrical field is assumed. This is the case in a parallel-plate capacitor setup. The electrodes may have the distance formula_4. The cathode is located at the point formula_21.\n\nTo get impact ionization, the electron energy formula_22 must become greater than the ionization energy formula_23 of the gas atoms between the plates. Per length of path formula_24 a number of formula_25 ionizations will occur. formula_25 is known as the first Townsend coefficient as it was introduced by Townsend. The increase of the electron current formula_27, can be described for the assumed setup as\n\nThe number of created electrons is\n\nNeglecting possible multiple ionizations of the same atom, the number of created ions is the same as the number of created electrons:\n\nformula_33 is the ion current. To keep the discharge going on, free electrons must be created at the cathode surface. This is possible because the ions hitting the cathode release secondary electrons at the impact. (For very large applied voltages also field electron emission can occur.) Without field emission, we can write\n\nwhere formula_35 is the mean number of generated secondary electrons per ion. This is also known as the second Townsend coefficient. Assuming that formula_36, one gets the relation between the Townsend coefficients by putting (4) into (3) and transforming:\n\nWhat is the amount of formula_25? The number of ionization depends upon the probability that an electron hits a gas molecule. This probability formula_39 is the relation of the cross-sectional area of a collision between electron and ion formula_40 in relation to the overall area formula_6 that is available for the electron to fly through:\n\nAs expressed by the second part of the equation, it is also possible to express the probability as relation of the path traveled by the electron formula_24 to the mean free path formula_44 (distance at which another collision occurs).\n\nformula_45 is the number of molecules which electrons can hit. It can be calculated using the equation of state of the ideal gas\n\nThe adjoining sketch illustrates that formula_47. As the radius of an electron can be neglected compared to the radius of an ion formula_48 it simplifies to formula_49. Using this relation, putting (7) into (6) and transforming to formula_44 one gets\n\nwhere the factor formula_52 was only introduced for a better overview.\n\nThe alteration of the current of not yet collided electrons at every point in the path formula_24 can be expressed as\n\nThis differential equation can easily be solved:\n\nThe probability that formula_56 (that there was not yet a collision at the point formula_24) is\n\nAccording to its definition formula_25 is the number of ionizations per length of path and thus the relation of the probability that there was no collision in the mean free path of the ions, and the mean free path of the electrons:\n\nIt was hereby considered that the energy formula_61 that a charged particle can get between a collision depends on the electric field strength formula_62 and the charge formula_63:\n\nFor the parallel-plate capacitor we have formula_65, where formula_66 is the applied voltage. As a single ionization was assumed formula_63 is the elementary charge formula_68. We can now put (13) and (8) into (12) and get\n\nPutting this into (5) and transforming to formula_66 we get the Paschen law for the breakdown voltage formula_71 that was first investigated by Paschen in and whose formula was first derived by Townsend in, section 227:\n\nPlasma ignition in definition of Townsend (Townsend discharge) is a self-sustaining discharge, independent of an external source of free electrons. This means that electrons from the cathode can reach the anode in the distance formula_4 and ionize at least one atom on their way. So according to the definition of formula_25 this relation must be fulfilled:\n\nIf formula_77 is used instead of (5) one gets for the breakdown voltage\n\nPaschen's law requires that:\n\nDifferent gases will have different mean free paths for molecules and electrons. This is because different molecules have different diameters. Noble gases like helium and argon are monatomic and tend to have smaller diameters. This gives them greater mean free paths.\n\nIonization potentials differ between molecules, as well as the speed that they recapture electrons after they have been knocked out of orbit. All three effects change the number of collisions needed to cause an exponential growth in free electrons. These free electrons are necessary to cause an arc.\n\n\n"}
{"id": "5417198", "url": "https://en.wikipedia.org/wiki?curid=5417198", "title": "Piano trolley", "text": "Piano trolley\n\nA piano trolley is a two- or four-wheeled trolley approximately 50cm to 80cm long used by removals companies for moving pianos. It is placed under the centre of mass of the piano and allows it to be turned on its axis to manoeuvre round a building. By placing the trolley at one end of the instrument, stairs may be negotiated. In tight spaces the piano may be turned on end and rested on the trolley.\n\nTypical features include solid rubber tyres, very strong construction, and thick rubber bumpers along the top and on the ends.\n\nBefore a piano trolley can be used to move a grand piano, the piano must be protected by a piano shoe, a wooden frame which protects the polished surface and provides additional strength for the sides.\n"}
{"id": "9516170", "url": "https://en.wikipedia.org/wiki?curid=9516170", "title": "Polyanhydrides", "text": "Polyanhydrides\n\nPolyanhydrides are a class of biodegradable polymers characterized by anhydride bonds that connect repeat units of the polymer backbone chain. Their main application is in the medical device and pharmaceutical industry. In vivo, polyanhydrides degrade into non-toxic diacid monomers that can be metabolized and eliminated from the body. Owing to their safe degradation products, polyanhydrides are considered to be biocompatible. \n\nThe characteristic anhydride bonds in polyanhydrides are water-labile (the polymer chain breaks apart at the anhydride bond). This results in two carboxylic acid groups which are easily metabolized and biocompatible. \nBiodegradable polymers, such as polyanhydrides, are capable of releasing physically entrapped or encapsulated drugs by well-defined kinetics and are a growing area of medical research. Polyanhydrides have been investigated as an important material for the short-term release of drugs or bioactive agents. The rapid degradation and limited mechanical properties of polyanhydrides render them ideal as controlled drug delivery devices. \n\nOne example, Gliadel, is a device in clinical use for the treatment of brain cancer. This product is made of a polyanhydride wafer containing a chemotherapeutic agent. After removal of a cancerous brain tumor, the wafer is inserted into the brain releasing a chemotherapy agent at a controlled rate proportional to the degradation rate of the polymer. The localized treatment of chemotherapy protects the immune system from high levels of radiation. \n\nOther applications of polyanhydrides include the use of unsaturated polyanhydrides in bone replacement, as well as polyanhydride copolymers as vehicles for vaccine delivery.\n\nThere are three main classes of polyanhydrides: aliphatic, unsaturated, and aromatic. These classes are determined by examining their R groups (the chemistry of the molecule between the anhydride bonds). \n\nAliphatic polyanhydrides consist of R groups containing carbon atoms bonded in straight or branched chains. This class of polymers is characterized by a crystalline structure, melting temperature range of 50–90 °C, and solubility in chlorinated hydrocarbons. They degrade and are eliminated from the body within weeks of being introduced to the bodily environment. \n\nUnsaturated polyanhydrides consist of organic R groups with one or more double bonds (or degrees of unsaturation). This class of polymers has a highly crystalline structure and is insoluble in common organic solvents. \n\nAromatic polyanhydrides consist of R groups containing a benzene (aromatic) ring. Properties of this class include a crystalline structure, insolubility in common organic solvents, and melting points greater than 100 °C. They are very hydrophobic and therefore degrade slowly when in the bodily environment. This slow degradation rate makes aromatic polyanhydrides less suitable for drug delivery when used as homopolymers, but they can be copolymerized with the aliphatic class to achieve the desired degradation rate.\n\nPolyanhydrides are synthesized using either melt condensation or solution polymerization. Depending on the synthesis method used,\nvarious characteristics of polyanhydrides can be altered to achieve the desired product. Characterization of polyanhydrides determines the structure, composition, molecular weight, and thermal properties of the molecule. These properties are determined by using various light-scattering and size-exclusion methods. \n\nPolyanhydrides can be easily prepared by using available, low cost resources. The process can be varied to achieve desirable characteristics. Traditionally, polyanhydrides have been prepared by melt condensation polymerization, which results in high molecular weight polymers. Melt condensation polymerization involves reacting dicarboxylic acid monomers with excess acetic anhydride at a high temperature and under a vacuum to form the polymers. Catalysts may be used to achieve higher molecular weights and shorter reaction times. Generally, a one-step synthesis (method involving only one reaction) is used which does not require purification. \n\nThere are many other methods used to synthesize polyanhydrides. Some of the other methods include: microwave heating, high-throughput synthesis (synthesis of polymers in parallel), ring opening polymerization (removal of cyclic monomers), interfacial condensation (high temperature reaction of two monomers), dehydrative coupling agents (removing the water group from two carboxyl groups), and solution polymerization (reacting in a solution).\n\nThe chemical structure and composition of polyanhydrides can be determined by HNMR spectroscopy. This will determine the class of polanhydride (aromatic, aliphatic, or unsaturated) as well as the structural features of the polymer. For example, the analysis of nuclear magnetic resonance(NMR) peaks allows one to determine if a copolymer has a random or block-like structure. Molecular weight and degradation rate can also determined by NMR. \n\nAside from using NMR to determine a polyanhydride’s molecular weight, gel permeation chromatography (GPC), and viscosity measurements may also be used. \n\nDifferential scanning calorimetry (DSC) is used to determine the thermal properties of polyanhydrides. Glass transition temperature, melting temperature, and heat of fusion can all be determined by DSC. Crystallinity of a polyanhydride can be determined using DSC, Small angle X-ray scattering (SAXS), Nuclear magnetic resonance (NMR), and X-ray diffraction.\n\nThe erosion and degradation of a polymer describe how the polymer physically loses mass (degrades). The two common erosion mechanisms are surface and bulk erosion. Polyanhydrides are surface eroding polymers. Surface eroding polymers do not allow water to penetrate into the material. They erode layer by layer, like a lollipop. The hydrophobic backbone with hydrolytically labile anhydride linkages allows hydrolytic degradation to be controlled by manipulating the polymer composition. This manipulation can occur by adding a hydrophilic group to the polyanhydride to make a copolymer. Polyanhydride copolymers with hydrophilic groups exhibit bulk eroding characteristics. Bulk eroding polymers take in water like a sponge (throughout the material) and erode inside and on the surface of the polymer. \n\nDrug release from bulk eroding polymers is difficult to characterize because the primary mode of release from these polymers is diffusion. Unlike surface eroding polymers, bulk eroding polymers show a very weak relationship between the rate of polymer degradation and the rate of drug release. Therefore, the development of surface eroding polyanhydrides incorporated into the bulk eroding polymers is of increased importance.\n\nBiocompatibility and toxicity of a polymeric material is evaluated by examining systemic toxic responses, local tissue responses, carcinogenic and mutagenic responses, and allergic responses to the material's degradation products. Animal studies are conducted to test the polymer’s effect on each of these negative responses. Polyanhydrides and their degradation products have not been found to cause significant harmful responses and are considered to be biocompatible.\n\n"}
{"id": "4866437", "url": "https://en.wikipedia.org/wiki?curid=4866437", "title": "Propulsive efficiency", "text": "Propulsive efficiency\n\nIn aircraft and rocket design, overall propulsive efficiency formula_1 is the efficiency with which the energy contained in a vehicle's propellant is converted into kinetic energy of the vehicle, to accelerate it, or to replace losses due to aerodynamic drag or gravity. It can also be described as the proportion of the mechanical energy actually used to propel the aircraft. It is always less than one, because conservation of momentum requires that the exhaust have some of the kinetic energy, and the propulsive mechanism (whether propeller, jet exhaust, or ducted fan) is never perfectly efficient. Overall propulsive efficiency is greatly dependent on air density and airspeed.\n\nMathematically, it is represented as formula_2, where formula_3 is the cycle efficiency and formula_4 is the mechanical efficiency.\n\nMost aerospace vehicles are propelled by heat engines of some kind, usually an internal combustion engine. The efficiency of a heat engine relates how much useful work is output for a given amount of heat energy input.\n\nFrom the laws of thermodynamics:\n\nIn other words, a heat engine absorbs heat from some heat source, converting part of it to useful work, and delivering the rest to a heat sink at lower temperature. In an engine, efficiency is defined as the ratio of useful work done to energy expended.\n\nThe theoretical maximum efficiency of a heat engine, the Carnot efficiency, depends only on its operating temperatures. Mathematically, this is because in reversible processes, the cold reservoir would gain the same amount of entropy as that lost by the hot reservoir (i.e., formula_11), for no change in entropy. Thus:\n\nwhere formula_13 is the absolute temperature of the hot source and formula_14 that of the cold sink, usually measured in kelvins. Note that formula_15 is positive while formula_16 is negative; in any reversible work-extracting process, entropy is overall not increased, but rather is moved from a hot (high-entropy) system to a cold (low-entropy one), decreasing the entropy of the heat source and increasing that of the heat sink.\n\nConservation of momentum requires acceleration of propellant material in the opposite direction to accelerate a vehicle. In general, energy efficiency is highest when the exhaust velocity is low, in the frame of reference of the Earth, as this reduces loss of kinetic energy to propellant.\n\nThe exact propulsive efficiency formula for air-breathing engines is \n\nwhere formula_18 is the exhaust expulsion velocity and formula_19 is the airspeed of the aircraft.\n\nA corollary of this is that, particularly in air breathing engines, it is more energy efficient to accelerate a large amount of air by a small amount, than it is to accelerate a small amount of air by a large amount, even though the thrust is the same. This is why turbofan engines are more efficient than simple jet engines at subsonic speeds.\n\nA rocket engine's formula_3 is usually high due to the high combustion temperatures and pressures, and the long converging-diverging nozzle used. It varies slightly with altitude due to changing atmospheric pressure, but can be up to 70%. Most of the remainder is lost as heat in the exhaust.\n\nRocket engines have a slightly different propulsive efficiency (formula_4) than air-breathing jet engines, as the lack of intake air changes the form of the equation. This also allows rockets to exceed their exhaust's velocity.\n\nSimilarly to jet engines, matching the exhaust speed and the vehicle speed gives optimum efficiency, in theory. However, in practice, this results in a very low specific impulse, causing much greater losses due to the need for exponentially larger masses of propellant. Unlike ducted engines, rockets give thrust even when the two speeds are equal.\n\nIn 1903, Konstantin Tsiolkovsky discussed the average propulsive efficiency of a rocket, which he called the utilization (\"utilizatsiya\"), the \"portion of the total work of the explosive material transferred to the rocket\" as opposed to the exhaust gas.\n\nThe calculation is somewhat different for reciprocating and turboprop engines which rely on a propeller for propulsion since their output is typically expressed in terms of power rather than thrust. The equation for heat added per unit time, \"Q\", can be adopted as follows:\n\nwhere H = calorific value of the fuel in BTU/lb, h = fuel consumption rate in lb/hr and J = mechanical equivalent of heat = 778.24 ft.lb/BTU, where formula_24 is engine output in horsepower, converted to foot-pounds/second by multiplication by 550. Given that specific fuel consumption is \"C\" = \"h\"/\"P\" and H = 20 052 BTU/lb for gasoline, the equation is simplified to:\n\nexpressed as a percentage.\n\nAssuming a typical propulsive efficiency formula_4 of 86% (for the optimal airspeed and air density conditions for the given propeller design), maximum overall propulsive efficiency is estimated as:\n\n\n"}
{"id": "8714301", "url": "https://en.wikipedia.org/wiki?curid=8714301", "title": "Pyramid puzzle", "text": "Pyramid puzzle\n\nA pyramid puzzle is a mechanical puzzle (\"assembly puzzle\") consisting of two or more component pieces which fit together to create a pyramid.\n\n\"Unknown origin\". Children of American pioneers played with simple toys such as these.\n\nTo do this, make the square faces face each other and twist one upright to complete the four faced tetrahedronic pyramid.\n\npuzzles, mechanical puzzles\n"}
{"id": "55655780", "url": "https://en.wikipedia.org/wiki?curid=55655780", "title": "Quaternary phase", "text": "Quaternary phase\n\nIn materials chemistry, a quaternary phase is chemical compound containing four elements. Some compounds can be molecular or ionic, examples being chlorodifluoromethane (CHClF) sodium bicarbonate (NaCOH). More typically quaternary phase refers to extended solids. A famous example are the yttrium barium copper oxide superconductors.\n\n"}
{"id": "23524398", "url": "https://en.wikipedia.org/wiki?curid=23524398", "title": "Rapid phase transition", "text": "Rapid phase transition\n\nRapid phase transition or RPT is a phenomenon realized in liquefied natural gas (LNG) incidents in which LNG vaporizes violently upon coming in contact with water causing what is known as a \"physical explosion\"\nor \"cold explosion\". During such explosions there is no combustion but rather a huge amount of energy is transferred in the form of heat from the room-temperature water to the LNG at a temperature difference of about 175 kelvins.\n\nLiquefied natural gas or LNG is a natural gas that gets liquefied at atmospheric pressure and −161.5 °C. It is odorless, tasteless, colorless, and not poisonous but causes asphyxia. It can cause frostbite due to its cryogenic temperature. If this extremely cold LNG is mixed with water(e.g. sea water, which has an average temperature of 15 °C), heat energy is transferred from the water to the LNG, rapidly vaporizing it from its liquefied state back into its original gaseous state. This results in an explosion because the volume occupied by natural gas in its gaseous form is 600 times greater than when its liquefied. This is the phenomenon of rapid phase transition.\n\n\n"}
{"id": "46664082", "url": "https://en.wikipedia.org/wiki?curid=46664082", "title": "Saint Volodymyr Descent", "text": "Saint Volodymyr Descent\n\nSaint Volodymyr Descent (, Volodymyrsky uzviz) is a street in Kiev located between the Pechersk and Podil city districts. It stretches from European Square to Postal Square. \n\nThe city street splits two city parks (Khreshchaty Park and Saint Volodymyr Hill) located on the slopes of Saint Michael Hill (see Kiev Mountains).\n\nThe street started to form in 1711 in place of the so-called \"Old Pechersk Road\" that used to connect Pechersk with Podil. As a city street it was known originally as \"Khreshchaty Drive\" (, Khreshchatytsky pryizd) and later as \"Paved Street\" (, Mostova vulytsia) becoming the first city's street paved with cobblestone. Sometime in 1810s it became part of a long \"Aleksandr Street\" (after Alexander II of Russia) that stretched all the way from Arsenal Square to Contracts Square, while the descent was unofficially referred to as Aleksandr Descent. After the occupation of Kiev by the Soviet troops in March of 1919 the long street was renamed as Revolution Street (October Revolution). When Kiev became the capital of the Soviet Ukraine in 1934, the street was renamed once again, now as Sergei Kirov Street after the Russian government official Sergei Kirov. During the Nazi occupation the street along with the modern Hrushevsky Street carried the name of the Nazi political figure Fritz Todt.\n\nDuring the World War II when Kiev was freed from Nazi occupation, the former long Kirov Street was split at the Stalin Square (today European Square) into Kirov Street (today Hrushevsky Street) and Volodymyr Descent that stretched from the Stalin Square to Contracts Square. In 1955 the street was shortened and part of it between Postal Square and Contracts Square was renamed into Zhdanov Street (today Sahaidachny Street) after a Soviet official Andrei Zhdanov.\n\n"}
{"id": "1532991", "url": "https://en.wikipedia.org/wiki?curid=1532991", "title": "Slayer's Slab", "text": "Slayer's Slab\n\nThe Slayer's Slab is a title given to a medieval gravestone formerly in the graveyard of Lyminster church in West Sussex, England. It has now been moved inside the church to protect it from weathering.\nAccording to legend it is the gravestone of the dragonslayer who killed the Knucker who lived in the nearby knuckerhole. The stone has a cross on it overlaying a herringbone pattern, but no inscription to identify the tomb's occupant.\n"}
{"id": "17877583", "url": "https://en.wikipedia.org/wiki?curid=17877583", "title": "Sudan Airways Flight 109", "text": "Sudan Airways Flight 109\n\nSudan Airways Flight 109 was an international scheduled Amman–Damascus–Khartoum passenger service, operated with an Airbus A310, that crashed on landing at Khartoum International Airport on 10 June 2008, approximately at 17:00 UTC, killing 30 of 214 occupants on board.\n\nThe aircraft involved in the accident was an Airbus A310-324, c/n 548, tail number that had its maiden flight on 23 August 1990 as F-WWCV. Equipped with a twin-PW4152 powerplant, it was delivered new to Singapore Airlines on 22 October 1990 and registered 9V-STU. Re-registered VT-EVF, it was delivered to Air India on 10 March 2001. The aircraft was finally registered ST-ATN, and delivered to Sudan Airways on 1 December 2007. According to Airbus, it had accumulated 52,000 flight hours and 21,000 cycles.\n\nThe flight originated in Amman and had its final destination in Khartoum with an intermediate stop in Damascus. The presence of a sandstorm and heavy rain prevented the aircraft to land at Khartoum and forced it to be diverted to Port Sudan. The aircraft was later allowed to fly back to its original destination. The flight landed at Khartoum Airport at 17:26 UTC, but was not able to stop within the paved area. Following the aircraft overrunning the runway and coming to rest beyond the end of runway 36 a fire erupted on the starboard side. The slides on the side the fire started at could not be deployed; people on board evacuated the aircraft using the port side slides.\n\nThe runway overrun was caused by a combination of a long landing flare, the wet runway, landing without autobrake selected on, and one engine thrust reverser that was deactivated. The aircraft had been dispatched with the port engine thrust reverser deactivated. This condition caused it to veer to the right when the captain activated reverse thrust in both engines in order to stop the aircraft within of runway left. Reports indicated the occurrence of a big sandstorm at Khartoum earlier that day that was followed by low visibility and heavy rain and winds at the time of the accident. Contributing to the long landing flare was that the crew had been incorrectly informed that they had a headwind for landing when they actually had a tailwind.\n\nOut of passengers and crew members on board the aircraft, passengers and crew member lost their lives. Many of the casualties were children with disabilities and seniors returning from treatment in Amman.\n\nAbbas al-Fadini (Member of the Parliament of Sudan) was on board the flight and survived unscathed.\n\n"}
{"id": "13839679", "url": "https://en.wikipedia.org/wiki?curid=13839679", "title": "TAP Air Portugal Flight 425", "text": "TAP Air Portugal Flight 425\n\nTAP Flight 425 was a regular flight from Brussels, Belgium, to Santa Catarina Airport (informally known as Funchal Airport or Madeira Airport; now the Cristiano Ronaldo International Airport), Portugal, with an intermediate scheduled stop in Lisbon. On November 19, 1977, the Boeing 727 operating the service overran the airport's runway before crashing onto the nearby beach and exploding, killing 131 of the 164 people on board. It remains TAP's only fatal accident in its history.\n\nThe aircraft operating flight TP-425 was a Boeing 727-282 Advanced registration CS-TBR named after the Portuguese aviation pioneer Sacadura Cabral. Its manufacturer serial number was 20972/1096 and it was delivered to TAP on 21 January 1975. It was powered by 3 Pratt & Whitney JT8D-17 turbofan engines which had a maximum thrust of 16,000 lbf each. The aircraft had completed a B check on 21 September 1977, and at the time of the accident had accumulated 6,154 flying hours in 5,204 cycles.\n\nShortly before 9:48pm on that Saturday evening, after 13 hours and 15 minutes of service time, the tired crew of the 727 was trying to land the airplane on the difficult Santa Catarina International Airport's runway, which at the time was long. After two unsuccessful attempts to land the aircraft, the Captain João Lontrão and Co-pilot Miguel Guimarães Leal decided to make one last try to land the plane, before they would have to make the decision to divert to the Gran Canaria Airport in the Canary Islands.\n\nWhile on final approach to runway 24 in heavy rain, strong winds and poor visibility, the aircraft touched down past the threshold, and started hydroplaning. With just about of runway left, the crew tried desperately to stop, applying maximum reverse thrust and brakes, but the aircraft slid off the runway with a ground speed of approximately and plunged over a steep bank hitting a nearby bridge and crashing on the beach; splitting in two pieces and bursting into flames.\n\nOf the 164 people aboard (156 passengers and eight crew), 131 were killed (125 passengers and 6 crew), making it the deadliest airplane accident in Portugal to that point. As of 2018, it is the second deadliest airplane accident in Portugal, after Independent Air Flight 1851. It remains TAP Portugal's only fatal accident since the beginning of its flight operations in 1946.\n\nAfter the accident occurred, TAP stopped flying the Boeing 727-200 to Madeira, and started flying only the 727-100, which was six metres shorter and took 60 fewer passengers.\n\nThe crash prompted officials to explore ways of extending the short runway. Because of the height of the runway relative to the beach below, an extension was very difficult and very expensive to perform. Between 1983 and 1986, a 200 meter extension was built; 14 years later, the runway was again extended. Following the 2000 extension, the runway of what is now the Cristiano Ronaldo International Airport measures long and is capable of handling wide-body commercial jets like the Boeing 747 or the Airbus A340.\n\n\n"}
{"id": "20949224", "url": "https://en.wikipedia.org/wiki?curid=20949224", "title": "Thiocarbonate", "text": "Thiocarbonate\n\nThiocarbonate describes a family of anions with the general chemical formula (\"x\" = 0, 1, or 2). Organic compounds structurally related to these anions are also called thiocarbonates.\n\nLike the carbonate dianion, the thiocarbonates are planar, with carbon at the center. The average bond order from C to S or O is . The state of protonation is usually not specified. These anions are good nucleophiles and good ligands.\n\nMonothiocarbonate is the dianion COS, which has C symmetry. Monothiocarbonate arises by the hydrolysis of thiophosgene or the reaction of base with carbonyl sulfide:\n\nDithiocarbonate is the dianion , which has C symmetry. It arises from the reaction of aqueous base with carbon disulfide:\nImportant derivatives dithiocarbonates are the xanthates, with the formula . These salts are typically prepared by the reaction of sodiium alkoxides with carbon disulfide. Another group of dithiocarbonates have the formula (RS)CO. They are often derived by hydrolysis of the corresponding trithiocarbonates (RS)CS. One example is tetrathiapentalenedione, a heterocycle that consists of two dithiocarbonate groups.\n\nTrithiocarbonate is the dianion , which has D symmetry. Trithiocarbonate is derived from the reaction of sulfide sources with carbon disulfide:\n\nAddition of sulfur to trithiocarbonate gives the perthiocarbonate anion , which contains one sulfur–sulfur bond.\nPerthiocarbonic acid (or tetrathiocarbonic acid, CAS#13074-70-9) has never been synthesized in the pure form but only as a dark brown solution.\n"}
{"id": "31758845", "url": "https://en.wikipedia.org/wiki?curid=31758845", "title": "Treehouse attachment bolt", "text": "Treehouse attachment bolt\n\nTreehouse attachment bolts or TABs are specialized bolts engineered for treehouse construction.\nVarious models and trademarks exist, with names such as Garnier limbs (GLs); tree anchor bolts; artificial limbs; heavy limbs or hyper limbs (HLs); special tree fastener or stud tree fastener (STFs). \n\nThey may be either of through fastener style (for smaller trees) or side mount type for larger ones.\n\nOne of the main features of TABs is their strength, requiring fewer tree penetrations for robust fastening of a treehouse and hence less damage to a live tree. A typical TAB consists of a threaded metal bolt and a larger diameter collar. The latter provides an extra bending strength by bearing upon the compression strength of the tree grain. Since treehouses are subject to frequent load reversal produced by winds, TABs must be made of spring steel The bolts are able to support from between , a much greater load than conventional lag bolts. \n\nTABs are commonly used in conjunction with pipe brackets, allowing the treehouse structure to move independently with the tree. TABs are designed so a tree’s added girth can further envelop it.\n\nThe concept of using a collar or large cylindrical object to increase shear strength in wood construction has been around for several centuries in the forms of split rings and shear plates. Shear plates provide a larger load-carrying capacity in shear than can be otherwise achieved by a bolt alone. Because this idea utilized less bolts for equal strength, it transferred well into the use of live trees, as they compartmentalize more efficiently with a single, larger cut than several, smaller cuts. Michael Garnier became involved in treehouse construction beginning in 1990 when he built his first treehouse in Josephine County, Oregon. Two years later, he was ordered by County official to close the treehouse to the public. He began developing what would become the Garnier limb, a high-strength alloy steel engineered bolt that can be screwed into a bore in the trunk of tree. In 1997 Garnier met Arborist Scott D. Baker who shared his knowledge of tree biology and tree structural responses. By 1998, Garnier had developed a commercial bolt product which he called Artificial Limbs. The Garnier limb was primarily developed to satisfy the safety requirements of the Josephine County Building and Safety building permit process.\n\nEngineer Charley Greenwood, with the help of Michael Garnier and machinist Michael Birmingham, added a collar onto a hex cap screw to maximize the surface area and minimize compression in the contact area between the tree and the screw. Garnier was initially the sole producer of these bolts under the names, \"Artificial Limbs\" and \"Garnier Limb\".\n"}
{"id": "30370131", "url": "https://en.wikipedia.org/wiki?curid=30370131", "title": "Twisted House", "text": "Twisted House\n\nTwisted House is a public artwork by American artist John McNaughton, located at the Indianapolis Art Center in Indianapolis, Indiana, United States. \"Twisted House\" was installed as part of the Center's ARTSPARK initiative.\n\nThe sculpture is made of cedar wood and depicts a tall house, bent in such a way that it appears to rest on its foundation and roof. The roof of the house digs into the forest floor and five square, glass windows travel upwards on the house and a distorted door juts to the right, open for viewers to interact with—inside one can step and look out a large window looking out into the forest. The windows each have sills with exterior flower holders and fake wood flowers. The piece sits on natural stone placed in the ground and the dirt of the forest grounds.\n\nJohn McNaughton has been teaching woodworking, drawing and sculpture for over 35 years at the University of Southern Indiana. Receiving his Bachelor of Science and Master of Arts at Ball State University and his Master of Fine Arts at Bowling Green State University, he has been awarded two National Endowment for the Arts awards. His art furniture and sculptures reside in the collections of the Smithsonian Institution, the Indianapolis Museum of Art and the White House.\n\nThis piece was placed in conjunction with the Center's ARTSPARK which brings together art and nature.\n\nAccording to the Indianapolis Art Center, McNaughton's goal with the work is to show viewers that sculpture can be interactive and touch heavily into the imagination.\n\nStarting in June 2010 IAC hosted a \"Community Masterpiece\" event designed by artist Vandra Pentecost. The mural consists of seven panels with each one depicting \"Twisted House\" in a different artistic style: expressionism, realism, surrealism, American scene painting, cubism, pop art and impressionism. The two panels for pop art and surrealism were completed at the Broad Ripple Art Fair by visitors. In July and August the rest of the panels were completed and the full mural now resides on the Outreach Lawn Wall of the ARTSPARK.\n\n\"Roadside America\" describes \"Twisted House\" as \"A work of whimsical lopsided outdoor art, suggesting something that's gone terribly wrong at a fairy tale theme park.\"\n\n"}
{"id": "19984603", "url": "https://en.wikipedia.org/wiki?curid=19984603", "title": "Veszprém Wind Farm", "text": "Veszprém Wind Farm\n\nThe Veszprém Wind Farm is an under construction wind power project in Veszprém County, Hungary. It will have 30 individual wind turbines with a nominal output of around 2 MW which will deliver up to 60 MW of power, enough to power over 40,100 homes, with a capital investment required of approximately US$225 million.\n"}
