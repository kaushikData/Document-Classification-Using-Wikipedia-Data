{"id": "7404467", "url": "https://en.wikipedia.org/wiki?curid=7404467", "title": "Ambient isotopy", "text": "Ambient isotopy\n\nIn the mathematical subject of topology, an ambient isotopy, also called an \"h-isotopy\", is a kind of continuous distortion of an ambient space, for example a manifold, taking a submanifold to another submanifold. For example in knot theory, one considers two knots the same if one can distort one knot into the other without breaking it. Such a distortion is an example of an ambient isotopy. More precisely, let \"N\" and \"M\" be manifolds and \"g\" and \"h\" be embeddings of \"N\" in \"M\". A continuous map \nis defined to be an ambient isotopy taking \"g\" to \"h\" if \"F\" is the identity map, each map \"F\" is a homeomorphism from \"M\" to itself, and \"F\" ∘ \"g\" = \"h\". This implies that the orientation must be preserved by ambient isotopies. For example, two knots which are mirror images of each other are in general not equivalent.\n\n\n"}
{"id": "7132914", "url": "https://en.wikipedia.org/wiki?curid=7132914", "title": "Association of Registered Gas Installers", "text": "Association of Registered Gas Installers\n\nThe Association of Registered Gas Installers (ARGI) is a free web-based association. Registration (membership) is open to gas installers who are already registered with Gas Safe Register. It was formed in 2004 to enable Registered Gas Installers to communicate effectively with each other, discuss matters affecting the industry and to make representations on their behalf, none of which was possible through CORGI (the register holder at that time)itself.\n\nIn recent years, there has been controversy over some aspects of the expanding role of CORGI. Some gas installers feel the organization is overbearing and an excessive financial burden, and that little is being done to stop unregistered installers operating. ARGI formed mainly as a consequence of this.\n\nRegistration (free web membership)[full membership is NOT free), with ARGI is increasing rapidly. Membership figures are not published. In total there are around 55,000 CORGI registered businesses in the UK employing nearly 110,000 gas operatives, so there remains scope for the expansion of ARGI.\n\nDuring 2006 ARGI gained credibility and was invited by CORGI to join the CORGI Principal Representative Body (formerly the CORGI Council) and was represented during the latest Fundamental Review of Gas Safety conducted by the HSE.\nMany in the Industry have never had contact Argi until this time, though its name is put on lists of attendees by those wishing to be seen.\n\nThe committee members are elected. There are regular meetings held and full members may vote on the resolutions. There is a published mission statement, articles of association, statement of association policies, code of conduct. Argi aims to benefit the gas industry and the public.\n\nNotwithstanding gas explosions, the greatest danger to the public from gas is Carbon Monoxide (CO), which is a toxic byproduct of the combustion process, in increased volume if combustion is incomplete. Most of the concern for gas safety focuses on safe CO dispersal through ventilation and flue systems, and prevention of incomplete combustion by regular service and maintenance of heat producing appliances, especially open-flued appliances. Modern gas appliances, especially those with fan assisted flues, are safer in this respect and the number of fatalities from CO poisoning has greatly declined.\n\nAround 30 people a year die from CO poisoning, though not all of these involve gas-fired appliances. Compare this to over 3,000 killed each year by motor vehicles and 30,000 extra winter deaths each year due partly to lack of affordable heating. Lord Hunt, at the recent Parliamentary Review, raised the issue of \"proportionality,\" and \"not the only problem in the world.\"\n\nHowever those who survive CO poisoning may experience brain damage, organ failure, loss of memory, blindness, induced epilepsy, peripheral palsy, personality change, and loss of mobility. They may also encounter skepticism from medical practitioners, who may believe their symptoms are psychosomatic. One victim had arm surgery to relieve a trapped nerve, but the neurosurgeon found none.\n\nRegistered installers must spend time and money on training and assessments to maintain their level of competence and CORGI registration. This adds substantially to overhead and charges to the public reflect that.\n\nHowever, there are many unregistered installers who do not carry these overheads, and who typically offer cheaper service. Consequently, ARGI views the RGIs increasing overheads as increasing the price differential and therefore a boost to the unregistered installer with whom they must compete.\n\nOne often proposed means of tackling this is to allow gas appliances sales only through RGIs. The identified RGI would be responsible for installation (or disposal/resale). However, the Government appears unwilling to pursue this strategy.\n\nIn a public poll, consumers felt the denial of right to purchase your own cooker or other appliance represented too much state interference. As it is not illegal, per se, to install your own appliance in your own home as long as you are competent to do so, the denial of right to buy the appliance is an infringement of personal liberty.\n\n"}
{"id": "49791284", "url": "https://en.wikipedia.org/wiki?curid=49791284", "title": "Banana industry", "text": "Banana industry\n\nThe banana industry is an important part of the global industrial agrobusiness.\nMost bananas go into export and international trade for consumption in Western countries. They are grown on banana plantations primarily in Latin America and the Caribbean as well as Central and South America.\n\nAs is the case with all monocultures, the intensive agroindustrial methods - bananas require large amounts of pesticides (estimate: / ) - for banana production have considerable environmental impact up to ecosystem destruction through deforestation. Moreover, global transport and plastic packaging leaves a large carbon footprint.\n\nIn 2012 the volume of global gross banana exports reached a record high of , 1.1 million tonnes (or 7.3 percent) above 2011 level.\nBananas are the most popular fruit in the United States, with more consumed annually than apples and oranges combined.\nIn spite of the multitude of banana species across the world, even only taking into account the cultivated ones, industrial production is dominated by the Cavendish banana.\n\nTropical Race 4 is thought to be distributed globally by soil-contaminated equipment from the multinational plantantion owners.\nIn 2013, five multinational fruit companies alone controlled 44% of the international banana trade:\nThe market share of the above players decreased from 70% in 2002 to about 44% in 2013. This decline in market power has been attributed to a couple of reasons. In the past, multinational companies owned a large number of plantations in Central and South America and other banana-producing regions. Since the 1980s they have divested a large share of their own production, replacing it with greater purchases from independent producers. For example, Chiquita has decreased the number of its plantations in Central America. Fyffes used to own plantations in Jamaica, Belize and the Windward Islands, but withdrew from production and switched to purchasing its bananas through contracts with producers.The disengagement from production was partly caused by legal and economic problems at the plantation level, but also reflects the change in market power along the banana value chain.\n\nAlong the global banana supply chain major supermarket chains in the US and EU have gained market power over the big producers in the 21st century as they dominate the retail market and increasingly purchase from smaller wholesalers or directly from growers.\n\nIn 2016, world production of bananas and plantains was 148 million tonnes, led by India and China with a combined total (only for bananas) of 28% of global production (table). Other major producers were the Philippines, Ecuador, Indonesia, and Brazil, together accounting for 20% of the world total of bananas and plantains (table). \n\nAs reported for 2013, total world exports were 20 million tonnes of bananas and 859,000 tonnes of plantains. Ecuador and the Philippines were the leading exporters with 5.4 and 3.3 million tonnes, respectively, and the Dominican Republic was the leading exporter of plantains with 210,350 tonnes.\n\n\n"}
{"id": "4493498", "url": "https://en.wikipedia.org/wiki?curid=4493498", "title": "Barton's pendulums", "text": "Barton's pendulums\n\nFirst demonstrated by Prof Edwin Henry Barton FRS FRSE (1858–1925), Professor of Physics at University College, Nottingham, who had a particular interest in the movement and behavior of spherical bodies, the Barton's pendulums experiment demonstrates the physical phenomenon of resonance and the response of pendulums to vibration at, below and above their resonant frequencies. In its simplest construction, approximately 10 different pendulums are hung from one common string. This system vibrates at the resonance frequency of a driver pendulum, causing the target pendulum to swing with the maximum amplitude. The other pendulums to the side do not move as well, thus demonstrating how torquing a pendulum at its resonance frequency is most efficient.\n\nThe driver may be a very heavy pendulum also attached to this common string; the driver is set to swing and move the whole system.\n\n"}
{"id": "14059588", "url": "https://en.wikipedia.org/wiki?curid=14059588", "title": "Bioenergy in China", "text": "Bioenergy in China\n\nChina has set the goal of attaining one percent of its renewable energy generation through bioenergy in 2020.\n\nThe development of bioenergy in China is needed to meet the rising energy demand.\n\nSeveral institutions are involved in this development, most notably the Asian Development Bank and China's Ministry of Agriculture. There is also an added incentive to develop the bioenergy sector which is to increase the development of the rural agricultural sector. \n\nAs of 2005, bioenergy use has reached more than 20 million households in the rural areas, with methane gas as the main biofuel. Also more than 4000 bioenergy facilities produce 8 billion cubic metres every year of methane gas. By 2006 20% of \"gasoline\" consumed was actually a 10% ethanol-gasoline blend. As of 2010, electricity generation by bioenergy is expected to reach 5 GW, and 30 GW by 2020. The annual use of methane gas is expected to be 19 cubic kilometers by 2010, and 40 cubic kilometers by 2020.\n\n\n\n\n\n\n\n\n\nChina's circulars on bioenergy policy have been co-released by the following agencies:\n\n\n\n\nAt least two publicly traded companies, China Clean Energy, Inc. and Gushan, manufacture and sell significant amounts of biodiesel in China.\n\n\n\n\n\n"}
{"id": "27476703", "url": "https://en.wikipedia.org/wiki?curid=27476703", "title": "Bouc–Wen model of hysteresis", "text": "Bouc–Wen model of hysteresis\n\nIn structural engineering, the Bouc–Wen model of hysteresis is used to describe non-linear hysteretic systems. It was introduced by Robert Bouc and extended by Yi-Kwei Wen, who demonstrated its versatility by producing a variety of hysteretic patterns.\nThis model is able to capture, in analytical form, a range of hysteretic cycle shapes matching the behaviour of a wide class of hysteretical systems. Due to its versatility and mathematical tractability, the Bouc–Wen model has gained popularity. It has been extended and applied to a wide variety of engineering problems, including multi-degree-of-freedom (MDOF) systems, buildings, frames, bidirectional and torsional response of hysteretic systems, two- and three-dimensional continua, soil liquefaction and base isolation systems. The Bouc–Wen model, its variants and extensions have been used in structural control—in particular, in the modeling of behaviour of magneto-rheological dampers, base-isolation devices for buildings and other kinds of damping devices. It has also been used in the modelling and analysis of structures built of reinforced concrete, steel, masonry, and timber.\n\nConsider the equation of motion of a single-degree-of-freedom (sdof) system:\n\nhere, formula_1 represents the mass, formula_2 is the displacement, formula_3 the linear viscous damping coefficient, formula_4 the restoring force and formula_5 the excitation force while the overdot denotes the derivative with respect to time.\n\nAccording to the Bouc–Wen model, the restoring force is expressed as:\n\nwhere formula_6 is the ratio of post-yield formula_7 to pre-yield (elastic) formula_8 stiffness, formula_9 is the yield force, formula_10 the yield displacement, and formula_11 a non-observable hysteretic parameter (usually called the \"hysteretic displacement\") that obeys the following nonlinear differential equation with zero initial condition (formula_12), and that has dimensions of length:\n\nor simply as:\n\nwhere formula_13 denotes the signum function, and formula_14, formula_15, formula_16 and formula_17 are dimensionless quantities controlling the behaviour of the model (formula_18 retrieves the elastoplastic hysteresis). Take into account that in the original paper of Wen (1976), formula_19 is called formula_20, and formula_16 is called formula_19. Nowadays the notation varies from paper to paper and very often the places of formula_19 and formula_16 are exchanged. Here the notation used by Ref. is implemented. The restoring force formula_4 can be decomposed into an elastic and a hysteretic part as follows:\n\nand\n\ntherefore, the restoring force can be visualized as two springs connected in parallel.\n\nFor small values of the positive exponential parameter formula_17 the transition from elastic to the post-elastic branch is smooth, while for large values that transition is abrupt. Parameters formula_14, formula_19 and formula_16 control the size and shape of the hysteretic loop. It has been found that the parameters of the Bouc–Wen model are functionally redundant. Removing this redundancy is best achieved by setting formula_30.\n\nWen assumed integer values for formula_17; however, all real positive values of formula_17 are admissible. The parameter formula_19 is positive by assumption, while the admissible values for formula_16, that is formula_35, can be derived from a thermodynamical analysis (Baber and Wen (1981)).\n\nSome terms are defined below: \n\nAbsorbed hysteretic energy represents the energy dissipated by the hysteretic system, and is quantified as the area of the hysteretic force under total displacement; therefore, the absorbed hysteretic energy (per unit of mass) can be quantified as\n\nthat is,\n\nhere formula_36 is the squared pseudo-natural frequency of the non-linear system; the units of this energy are formula_37.\n\nEnergy dissipation is a good measure of cumulative damage under stress reversals; it mirrors the loading history, and parallels the process of damage evolution. In the Bouc–Wen–Baber–Noori model, this energy is used to quantify system degradation.\n\nAn important modification to the original Bouc–Wen model was suggested by Baber and Wen (1981) and Baber and Noori (1985, 1986).\n\nThis modification included strength, stiffness and pinching degradation effects, by means of suitable degradation functions:\n\nwhere the parameters formula_38, formula_39 and formula_40 are associated (respectively) with the strength, stiffness and pinching degradation effects. The formula_38, formula_42 and formula_39 are defined as linearly-increasing functions of absorbed hysteretic energy formula_44:\n\nThe pinching function formula_40 is specified as:\n\nwhere:\n\nand formula_46 is the ultimate value of formula_47, given by\n\nObserve that the new parameters included in the model are: formula_48, formula_49, formula_50, formula_51, formula_52, formula_53, formula_54, formula_55, formula_56, formula_57 and formula_58. When formula_59, formula_60 or formula_61 no strength degradation, stiffness degradation or pinching effect is included in the model.\n\nFoliente (1993) and Heine (2001) slightly altered the pinching function in order to model slack systems. An example of a slack system is a wood structure where displacement occurs with stiffness seemingly null, as the bolt of the structure is pressed into the wood.\n\nA two-degree-of-freedom generalization was defined by Park \"et al.\" (1986) to represent the behaviour of a system constituted of a single mass formula_1 subject to an excitation acting in two orthogonal (perpendicular) directions. For instance, this model is suited to reproduce the geometrically-linear, uncoupled behaviour of a biaxially-loaded, reinforced concrete column.\n\nWang and Wen (2000) attempted to extend the model of Park \"et al.\" (1986) to include cases with varying 'knee' sharpness (i.e., formula_63). However, in so doing, the proposed model was no longer rotationally invariant (isotropic). Harvey and Gavin (2014) proposed an alternative generalization of the Park-Wen model that retained the isotropy and still allowed for formula_63, viz.\n\nWang and Wen (1998) suggested the following expression to account for the asymmetric peak restoring force:\n\nwhere formula_65 is an additional parameter, to be determined.\n\nAsymmetric hysteretical curves appear due to the asymmetry of the mechanical properties of the tested element, of the imposed cycle motion, or of both. Song and Der Kiureghian (2006) proposed the following function for modelling those asymmetric curves:\n\nwhere:\n\nand\n\nwhere formula_66, formula_67 are six parameters that have to be determined in the identification process. However, according to Ikhouane \"et al.\" (2008), the coefficients formula_68, formula_69 and formula_70 should be set to zero.\n\nIn \"displacement-controlled experiments\", the time history of the displacement formula_2 and its derivative formula_72 are known; therefore, the calculation of the hysteretic variable and restoring force is performed directly using equations and .\n\nIn \"force-controlled experiments\", , and can be transformed in state space form, using the change of variables formula_73, formula_74, formula_75 and formula_76 as:\n\nand solved using, for example, the Livermore predictor-corrector method, the Rosenbrock methods or the 4th/5th-order Runge–Kutta method. The latter method is more efficient in terms of computational time; the others are slower, but provide a more accurate answer.\n\nThe state-space form of the Bouc–Wen–Baber–Noori model is given by:\n\nThis is a stiff ordinary differential equation that can be solved, for example, using the function \"ode15\" of MATLAB.\n\nAccording to Heine (2001), computing time to solve the model and numeric noise is greatly reduced if both force and displacement are the same order of magnitude; for instance, the units \"kN\" and \"mm\" are good choices.\n\nThe hysteresis produced by the Bouc–Wen model is rate-independent. can be written as:\n\nwhere formula_77 within the formula_78 function serves only as an indicator of the direction of movement. The indefinite integral of can be expressed analytically in terms of the Gauss hypergeometric function formula_79. Accounting for initial conditions, the following relation holds:\n\nwhere, formula_80 is assumed constant for the (not necessarily small) transition under examination, formula_81 and formula_82, formula_83 are the initial values of the displacement and the hysteretic parameter, respectively. is solved analytically for formula_84 for specific values of the exponential parameter formula_85, i.e. for formula_86 and formula_87. For arbitrary values of formula_85, can be solved efficiently using e.g. bisection – type methods, such as the Brent’s method.\n\nThe parameters of the Bouc–Wen model have the following bounds formula_89, formula_90, formula_91, formula_92, formula_93, formula_94, formula_15, formula_35.\n\nAs noted above, Ma \"et al.\"(2004) proved that the parameters of the Bouc–Wen model are functionally redundant; that is, there exist multiple parameter vectors that produce an identical response from a given excitation. Removing this redundancy is best achieved by setting formula_30.\n\nConstantinou and Adnane (1987) suggested imposing the constraint formula_98 in order to reduce the model to a formulation with well-defined properties.\n\nAdopting those constraints, the unknown parameters become: formula_16, formula_17, formula_101, formula_102 and formula_3.\n\nDetermination of the model paremeters using experimental input and output data can be accomplished by system identification techniques. The procedures suggested in the literature include: \n\nOnce an identification method has been applied to tune the Bouc–Wen model parameters, the resulting model is considered a good approximation of true hysteresis when the error between the experimental data and the output of the model is small enough (from a practical point of view).\n\nThe hysteretic Bouc–Wen model has received some criticism regarding its ability to accurately describe the phenomenon of hysteresis in materials. Ikhouane and Rodellar (2005) give some insight regarding the behavior of the Bouc–Wen model and provide evidence that the response of the Bouc–Wen model under periodic input is asymptotically periodic.\n\nCharalampakis and Koumousis (2009) propose a modification on the Bouc–Wen model to eliminate displacement drift, force relaxation and nonclosure of hysteretic loops when the material is subjected to short unloading reloading paths resulting to local violation of Drucker's or Ilyushin's postulate of plasticity.\n"}
{"id": "8952949", "url": "https://en.wikipedia.org/wiki?curid=8952949", "title": "British Compressed Air Society", "text": "British Compressed Air Society\n\nThe British Compressed Air Society (BCAS) is the compressed air and vacuum trade association in the United Kingdom. It has membership for Manufacturers, Distributors, Suppliers and End-Users of compressed air equipment and systems.\n\nIt was formed in 1930 and represents manufacturers, distributors and users of compressed air and vacuum equipment. It became a company on 1 July 1998.\n\nThe organization is situated near to Marylebone High Street, south of Regent's Park in Marylebone.\n\nMembers include a diverse set of firms and organizations, including many of the following:\n\nThe BCAS plays a key role in setting standards and codes of practice within the industry. BCAS also lobbies the UK government on behalf of UK industry and business, e.g. to outlaw non-compliant products. It works with a range of organisations and government departments, including the Health and Safety Executive, standards agencies, and training initiatives. \n\nIt publishes statistical information on the state of the compressed air equipment UK market.\n\n\n"}
{"id": "25269752", "url": "https://en.wikipedia.org/wiki?curid=25269752", "title": "Carbon lock-in", "text": "Carbon lock-in\n\nCarbon lock-in refers to the self-perpetuating inertia created by large fossil fuel-based energy systems that inhibits public and private efforts to introduce alternative energy technologies. Related to the concept of technological lock-in, the concept is most used in relation to the challenge of altering the current energy infrastructure to respond to global climate change.\n\nThe concept and term was first coined by Gregory C. Unruh in a 1999 Fletcher School, Tufts University doctoral thesis entitled “Escaping Carbon Lock-In.” It has since gained popularity in climate change policy discussions, especially those focused on preventing the globalization of carbon lock-in to rapidly industrializing countries like China and India.\n\nThe source of carbon lock-in inertia in energy systems arises from the co-evolution of large interdependent technological networks and the social institutions and cultural practices that support and benefit from system growth. The growth of the system is fostered by increasing returns to scale.\n\nAccording to Unruh:\nThe concept emerged in response to what is termed the “climate policy paradox,” which recognizes that there is substantial scientific consensus that climate change is a real and present threat to humans and other species uniquely adapted to current climatic conditions. Similarly there is evidence that technologies exist which can lower the carbon intensity of economic activity in a cost-effective manner, including energy efficiency innovations as well as some renewable energy applications. The existence of these apparent “win-win” no-regrets opportunities for society to act on climate concerns creates a paradox. If such technologies exist, and they are cost effective and help minimize climate-forcing emissions, why aren’t they diffusing more rapidly? The conjecture is that industrial economies have become locked into fossil fuel technologies by past investments and policy decisions, the effects of positive feedback on increasing returns, and the economic growth of energy infrastructure.\n\nCarbon lock-in emerges over time as energy and economic development in industrialized countries has proceeded. The carbon lock-in framework builds hierarchically from individual technological artifacts, usually manufactured by for-profit organizations, to technological systems of interdependent artifacts. As these systems grow, they begin to have important societal implications drawing in government regulation of the system’s growth and development. The government’s involvement with system management, be it for safety, universal service or other national interests, institutionalizes the system and signals the emergence of a techno-institutional complex.\n\nOver time consumers and the public adapt their lifestyles to the capabilities of the technology and the system becomes embedded in society. Examples of this process can be seen in the growth of automobile-based transportation systems and fossil-fuel powered energy systems.\n\nIt is this co-evolutionary positive feedback development process that creates the lock-in condition and associated barriers to the diffusion of alternative technologies, even those with known superior environmental performance characteristics. A 2007 Oak Ridge National Laboratory report entitled “Carbon Lock-In: Barriers to Deploying Climate Change Mitigation Technologies” (sponsored by the U.S. Climate Change Technology Program, CCTP) classifies three major types of carbon lock-in barriers: cost effectiveness, financial/legal and intellectual property barriers. Escaping the lock-in condition requires overcoming these barriers.\n\nThe carbon lock-in concept has gained more attention as China’s rapid industrial economic development has progressed. The concern is that if China pursues the same fossil-fuel driven economic development models of established industrial counties, building out extensive automobile-based infrastructures and fossil-fuel powered energy systems, they will lock-in persistent and growing greenhouse gas emissions well into the future. The same arguments can be extended to all rapidly industrializing countries including India. This concern is arising as scientific evidence is indicating that current emission growth must be stopped and global emissions reduced by upwards of 60% if humanity is to prevent substantial unwanted climate disruption.\n\nRecent studies by Steven J. Davis and co-authors have quantified the future CO2 emissions that can be expected to be produced by current energy infrastructure and the magnitude of lock-in related to power plants being built each year in China and elsewhere.\n\n\n\n\n\n"}
{"id": "23597348", "url": "https://en.wikipedia.org/wiki?curid=23597348", "title": "Conjunctive use", "text": "Conjunctive use\n\nConjunctive use is often used in discussing water supplies and water conservation. This phrase usually is used to describe the practice of storing surface water in a groundwater basin in wet years and withdrawing it from the basin in dry years. Conjunctive use consists of harmoniously combining the use of both surface water and groundwater in order to minimise the undesirable physical, environmental and economical effects of each solution and to optimise the water demand\n"}
{"id": "29495337", "url": "https://en.wikipedia.org/wiki?curid=29495337", "title": "Council of Energy Resource Tribes", "text": "Council of Energy Resource Tribes\n\nThe Council of Energy Resource Tribes (CERT) is a consortium of Native American tribes in the United States established to increase tribal control over natural resources. It was founded in September 1975 by twenty-five tribes under the leadership of the Navajo Nation under chairman Peter McDonald.\n\nThe tribes that make up CERT control 40 per cent of the mineable uranium deposits in the United States, four per cent of its oil and gas, and 30 per cent of the strippable Western coal. CERT's initial goal was to force renegotiation of contracts for natural resources, primarily coal, oil and gas, to increase royalties so as to reflect actual market prices. CERT also lobbied for new federal legislation that would give tribes a larger say in negotiations. While successful in some lawsuits to force renegotiation, CERT also supported popular demonstrations against the energy firms, as well as local and international boycotts, which were often successful where lawsuits had failed. On the legislative front, CERT was a major enabler of both the 1982 Indian Mineral Development Act and the 1982 Federal Oil and Gas Royalty Management Act.\n"}
{"id": "36819749", "url": "https://en.wikipedia.org/wiki?curid=36819749", "title": "CrossCharge", "text": "CrossCharge\n\nCrossCharge is a device for charging electric vehicles (EV) without driver intervention, using conductive charging. CrossCharge is the world’s first conductive charging system for an electric vehicle. It connects potentially any EV car battery to the electrical power source while the driver walks away; a true park and walk system. The system consists of two halves (or rather, two bars referred as crossbars) – one installed on the vehicle and one installed on the floor of a garage or in a parking lot. The automated ground unit detects the car above it and brings the two pieces together instigating an electric current to flow into the cross point of vehicle side bar, charging the EV. As the vehicle is parked over the charger, the device remains seamlessly connected to the vehicle and brings it to fully charged state – ready for the next use. Some major benefits include maximizing user convenience to promote EV adoption. In addition, the device in particular has passed all safety regulations and thus is the safest hands-free charger to be available on the market. CrossCharge is expected to be available in the spring of 2013.\n\nCharging through conduction is based on the age-old technology of metal to metal connection. The technology involves contact between a charged object and a neutral object. In contrast to induction, where the charged object is brought near but never contacted to the object being charged, conduction charging involves making the physical connection of the charged object to the neutral object. Because charging by conduction involves contact, it is often called charging by contact and has the least transfer loss.\n\nCrossCharge by GreenDot Transportation claims to be the world’s first conductive hands-free charging system for electric vehicles, it is a product of Green Dot Transportation, founded in 2010 [United States Patent Application 20090011616].\n\nThe charger represents a new generation of clean energy (how is the electricity generated?) possibilities towards maximizing user satisfaction and convenience, the cornerstone for wide-scale adoption of electric vehicles. The EV industry is trying to develop an “inductive” charging solution. However, this approach has inherent issues of safety and efficiency due to accompanying stay magnetic fields. Up to 25% energy can be lost – potentially creating a hazard for pace makers and heating of ferromagnetic structures in the vicinity.\n\nThe conductive approach GreenDot has taken provides following benefits to the EV owners:\n\n\n"}
{"id": "1302406", "url": "https://en.wikipedia.org/wiki?curid=1302406", "title": "Dado rail", "text": "Dado rail\n\nA dado rail, also known as a chair rail or surbase, is a type of moulding fixed horizontally to the wall around the perimeter of a room.\n\nThe dado rail is traditionally part of the dado or wainscot and, although the purpose of the dado is mainly aesthetic, the dado rail may provide the wall with protection from furniture and other contact. Traditionally, the height of the dado rail is derived from the height of the pedestal of a column of classical order, typically 24 inches from the floor or about one-fifth the height of the room. Modern trends have been towards 36 inches or 900 mm, based on the assumption that its purpose is to protect the wall from chair backs. The term 'chair rail' is also used for this reason.\n\nDado rails are also sometimes applied to a wall without the full dado treatment. The purpose of the rail in these cases may be protective, and it is common in environments where walls are subject to a lot of wear and tear, such as shopping centres and hospitals. In such cases the height of the rail is often 1200 mm or even 1500 mm from the floor and serves a functional rather than aesthetic role.\n\n"}
{"id": "3157505", "url": "https://en.wikipedia.org/wiki?curid=3157505", "title": "Dark matter in fiction", "text": "Dark matter in fiction\n\nDark matter is defined as hypothetical matter that is undetectable by its emitted radiation, but whose presence can be inferred from gravitational effects on visible matter. It has been used in a variety of fictional media, including computer and video games and books. In such cases, dark matter is usually attributed extraordinary physical or magical properties. Such descriptions are often inconsistent with the known properties of dark matter proposed in physics and cosmology. For example in computer games, it is often used as material for making weapons and items, and is usually depicted as black or a similar color.\n\nDark matter regularly appears as a topic in hybrid periodicals that cover both factual scientific topics and science fiction, and dark matter itself has been referred to as \"the stuff of science fiction\". A review of \"The Physics of Star Trek\" discusses dark matter before noting that \"the best modern science fiction borrows heavily from cosmology\".\n\n\n\n\n\n"}
{"id": "58622166", "url": "https://en.wikipedia.org/wiki?curid=58622166", "title": "Denza 500", "text": "Denza 500\n\nThe Denza 500 is a Chinese electric car produced by a Chinese NEV manufacturer, an alliance between Daimler AG and partner BYD Auto. \n\nThis is the latest version of the Denza electric car made exclusively for the Chinese market. A March 2018 preview report said that features include LED lamps front and back, cold weather capability, and a 9-inch infotainment touchscreen with navigation info from Baidu Maps that can also help locate charging stations. There were 112,000 such stations in China at the time. The new vehicle was to be sold at select Mercedes-Benz dealers in China. \n\nThe 2018 facelift also resulted in a restyled front fascia, rear bumper and tail lamps, an updated interior, and a name change to Denza 500. Range of the Denza 500 is said to be 451 kilometers measured by the NEDC standard, while the pre-facelift version had a range of 352 kilometers when launched in 2014; in 2017, Denza had launched a 400 km version.\n\nThe battery pack of the Denza 500 was upgraded to 70 kWh from the previous 62 kWh of the Denza EV. According to Denza, the consumption is 15.9 kWh per 100 kilometers, and a range of 635 kilometers could be achieved. As of October 2018, prices of the Denza 500 ranges from 298,800 to 328,800 yuan. \n\nThe original Denza EV was previewed by the Denza concept during the 2012 Beijing Auto Show and was launched as a prototype during the 2013 Shanghai Auto Show. The official launch of the production car was in 2014 with a price range 369,000 yuan to 399,000 yuan ($60,000 – 65,000). The Denza EV is based on the same platform as the Mercedes-Benz B-Class while the electric powertrain comes from the BYD e6 electric car producing 184hp with the top speed of 150 kilometers per hour. Range of the latest version is 352 kilometers. \n\n\n"}
{"id": "25638200", "url": "https://en.wikipedia.org/wiki?curid=25638200", "title": "Dos Cuadras Offshore Oil Field", "text": "Dos Cuadras Offshore Oil Field\n\nThe Dos Cuadras Offshore Oil Field is a large oil and gas field underneath the Santa Barbara Channel about eight miles southeast of Santa Barbara, California. Discovered in 1968, and with a cumulative production of over 260 million barrels of oil, it is the 24th-largest oil field within California and the adjacent waters. As it is in the Pacific Ocean outside of the 3-mile tidelands limit, it is a federally leased field, regulated by the U.S. Department of the Interior rather than the California Department of Conservation. It is entirely produced from four drilling platforms in the channel, which as of 2009 were operated by Dos Cuadras Offshore Resources (DCOR), LLC, a private firm based in Ventura. A blowout near one of these platforms – Unocal's Platform A – was responsible for the 1969 Santa Barbara oil spill that was formative for the modern environmental movement, and spurred the passage of the National Environmental Policy Act.\n\nThe Dos Cuadras field is one of many underneath the ocean bottom offshore of Southern California, most of which were discovered in the 1960s and 1970s. All of the field is outside of the 3-mile geographic limit, making it subject to U.S. government rather than California regulation. The four platforms are arranged in a line running from east to west, spaced one-half mile apart, with Platform Hillhouse on the east, and Platforms \"A\", \"B\", and \"C\" in order to the west.\n\nA pair of undersea pipelines, one for oil and one for gas, connect the four platforms to the shore near La Conchita. Oil and gas produced on the Dos Cuadras field are pumped about 12 miles east to the Rincon Oil & Gas Processing Plant on a hilltop adjacent to the Rincon Oil Field, about a mile southeast of La Conchita. From there oil travels down to Ventura along Venoco's M-143 pipeline to the Ventura pump station, and then to Los Angeles area refineries by way of a TOSCO pipeline in the Santa Clara River Valley.\n\nThe ocean bottom is relatively flat in the vicinity of the field, and all platforms are in a water depth of approximately 190 feet.\n\nThe Dos Cuadras field is a faulted anticlinal structure which plunges at both ends, thereby forming an ideal trap for hydrocarbon accumulation. It is part of the larger Rincon Anticlinal Trend, which includes the Carpinteria Offshore Oil Field to the east, as well as the Rincon, San Miguelito, and Ventura fields onshore. Oil is found in two formations, the Pliocene-age Pico Formation and the underlying Miocene Santa Margarita Formation. By far the most productive unit is the Repetto Sands portion of the Pico Formation. Only one well has produced from the Santa Margarita; all the others are in the Repetto Sands.\n\nThe Repetto Sands unit consists of layers of mudstone, siltstone, and shale, and due to its depositional environment the general grain size and porosity increase towards the east. It is the same formation which is richly productive in the oil fields of the Los Angeles Basin, such as the Salt Lake and Beverly Hills fields, where it is also folded into anticlinal traps. In the Dos Cuadras field, the oil-bearing strata are at depths ranging from 500 to 4,200 feet below the sea floor, and individual strata are separated by impermeable layers of shaly material.\n\nOil from the field averages API gravity of 25, with a range from 18 to 34, which classifies as medium-grade crude. Reservoir pressure began at 750 psi, sufficient for easy pumping in the early years of field development.\n\nOil has been known in the Santa Barbara Channel since prehistoric times; the native Chumash people used tar from the numerous natural seeps as a sealant, and tar regularly washes up on the beaches from the offshore seeps. The world's first offshore oil drilling took place at the Summerland Oil Field in 1896, only five miles north of the Dos Cuadras field. Those wells were put in from piers in shallow water. Technology for drilling in deeper water from platforms did not come about until the middle of the 20th century.\n\nWhile the existence of the field was suspected in the 1950s, the field was not discovered until 1968. Unocal and several other oil companies took out leases on the field in February 1968, and put in the first well, and the first platform, that same year. Data regarding the four platforms are as follows:\n\nUnocal succeeded in installing the first four wells from its Platform A by January 1969, but their attempt to install the fifth well was catastrophic, and resulted in one of the most notorious environmental disasters in United States history. Because the drillers were using an insufficient length of protective casing, when the well hit a high-pressure zone in the field, it blew out, spewing enormous quantities of oil and gas into the water from the sea floor. While crews were able to cap the wellhead and relieve the pressure there, the adjacent geologic formations were not strong enough to contain the pressure, and lacking a steel protective casing, the reservoir fluid and gas ripped through the sedimentary sand layers directly on the ocean floor; the result was the 1969 Santa Barbara oil spill of 80,000 to 100,000 barrels, which eventually coated over 40 miles of southern California coastline with oil, an ecological disaster which killed upward of 10,000 birds and numerous sea mammals and other creatures.\nFollowing the spill, the Secretary of the Interior ended all offshore oil drilling in the Outer Continental Shelf (OCS) until measures for better oversight were put in place, which happened in 1970 with the passage of the federal National Environmental Policy Act (NEPA), and in California, with the California Environmental Quality Act (CEQA). Opposition to oil drilling was nothing new in Santa Barbara – local residents, led by a newspaper publisher, objected enough to the expansion of the Summerland field in the 1890s to organize a late-night derrick-destroying party near the present-day Miramar Hotel – but the spill intensified the local hostility to oil drilling to the point that few new platforms were installed, and none at all within the 3-mile limit.\n\nAfter the disaster and cleanup, Unocal continued drilling wells from the two platforms that were already in place (\"A\" and \"B\", began producing from them in March 1969, and installed two more platforms (\"C\" and \"Hillhouse\"). Production from the field peaked quickly, reaching a maximum in 1971, during which year almost 28 million barrels of oil were extracted. The field experienced a gradual decline in production afterwards, approximately 8 percent per year, as is typical of fields when the reservoir pressure declines, and in the absence of secondary recovery technologies (such as water or gas injection). In 1985, Unocal tried waterflooding, and then polymer flooding, to improve production rates, and then in 1990 they began a horizontal drilling program to reach reservoirs impractical to exploit any other way.\n\nUnocal continued to produce from the field until they sold all of their California production assets in 1996 to Nuevo Energy, as operated by Torch Energy Advisors. In 1997 Nuevo took over the operations of the four platforms directly, and in 2004 passed them on to Plains Exploration & Production on that firm's acquisition of Nuevo. Plains only ran the platforms for a little more than four months, selling the operation to DCOR in March 2005. As of 2009, DCOR retains operational control of the Dos Cuadras field. Denver-based Venoco, Inc. owns a 25% (non-operating) interest in the western two-thirds of the field, having acquired it from Chevron in 1999.\n\nAccording to the Minerals Management Service (MMS), the field retains about 11.4 million barrels of oil in reserves recoverable with current technology. At the beginning of 2008, there were 145 producing oil wells distributed between the four platforms.\n\n"}
{"id": "191897", "url": "https://en.wikipedia.org/wiki?curid=191897", "title": "Electric-field screening", "text": "Electric-field screening\n\nIn physics, screening is the damping of electric fields caused by the presence of mobile charge carriers. It is an important part of the behavior of charge-carrying fluids, such as ionized gases (classical plasmas), electrolytes, and charge carriers in electronic conductors (semiconductors, metals).\nIn a fluid, with a given permittivity \"ε\", composed of electrically charged constituent particles, each pair of particles (with charges \"q\" and \"q\" ) interact through the Coulomb force as\n\nwhere the vector r is the relative position between the charges. This interaction complicates the theoretical treatment of the fluid. For example, a naive quantum mechanical calculation of the ground-state energy density yields infinity, which is unreasonable. The difficulty lies in the fact that even though the Coulomb force diminishes with distance as 1/\"r\", the average number of particles at each distance \"r\" is proportional to \"r\"², assuming the fluid is fairly isotropic. As a result, a charge fluctuation at any one point has non-negligible effects at large distances.\n\nIn reality, these long-range effects are suppressed by the flow of particles in response to electric fields. This flow reduces the \"effective\" interaction between particles to a short-range \"screened\" Coulomb interaction. This system corresponds to the simplest example of a renormalized interaction (see sections 1.2.1 and 3.2 of ).\n\nIn solid-state physics, especially for metals and semiconductors, the screening effect describes the electrostatic field and Coulomb potential of an ion inside the solid. Like the electric field of the nucleus is reduced inside an atom or ion due to the shielding effect, the electric fields of ions in conducting solids are further reduced by the cloud of conduction electrons.\n\nConsider a fluid composed of electrons moving in a uniform background of positive charge (one-component plasma). Each electron possesses a negative charge. According to Coulomb's interaction, negative charges repel each other. Consequently, this electron will repel other electrons creating a small region around itself in which there are fewer electrons. This region can be treated as a positively charged \"screening hole\". Viewed from a large distance, this screening hole has the effect of an overlaid positive charge which cancels the electric field produced by the electron. Only at short distances, inside the hole region, can the electron's field be detected. For a plasma, this effect can be made explicit by an formula_2-body calculation (see section 5 of ). If the background is made up of positive ions, their attraction by the electron of interest reinforces the above screening mechanism. In atomic physics, a germane effect exists for atoms with more than one electron shell: the shielding effect. In plasma physics, electric-field screening is also called Debye screening or shielding. It manifests itself on macroscopic scales by a sheath (Debye sheath) next to a material with which the plasma is in contact.\n\nThe screened potential determines the inter atomic force and the phonon dispersion relation in metals. The screened potential is used to calculate the electronic band structure of a large variety of materials, often in combination with pseudopotential models. The screening effect leads to the independent electron approximation, which explains the predictive power of introductory models of solids like the Drude model, the free electron model and the nearly free electron model.\nThe first theoretical treatment of electrostatic screening, due to Peter Debye and Erich Hückel, dealt with a stationary point charge embedded in a fluid.\n\nConsider a fluid of electrons in a background of heavy, positively charged ions. For simplicity, we ignore the motion and spatial distribution of the ions, approximating them as a uniform background charge. This simplification is permissible since the electrons are lighter and more mobile than the ions, provided we consider distances much larger than the ionic separation. In condensed matter physics, this model is referred to as jellium.\n\nLet \"ρ\" denote the number density of electrons, and \"φ\" the electric potential. At first, the electrons are evenly distributed so that there is zero net charge at every point. Therefore, \"φ\" is initially a constant as well.\n\nWe now introduce a fixed point charge \"Q\" at the origin. The associated charge density is \"Qδ\"(\"r\"), where \"δ\"(\"r\") is the Dirac delta function. After the system has returned to equilibrium, let the change in the electron density and electric potential be \"Δρ\"(\"r\") and \"Δφ\"(\"r\") respectively. The charge density and electric potential are related by Poisson's equation, which gives\n\nwhere \"ε\" is the vacuum permittivity.\n\nTo proceed, we must find a second independent equation relating \"Δρ\" and \"Δφ\". We consider two possible approximations, under which the two quantities are proportional: the Debye-Hückel approximation, valid at high temperatures (e.g. classical plasmas), and the Fermi-Thomas approximation, valid at low temperatures (e.g. electrons in metals).\n\nIn the Debye–Hückel approximation, we maintain the system in thermodynamic equilibrium, at a temperature \"T\" high enough that the fluid particles obey Maxwell–Boltzmann statistics. At each point in space, the density of electrons with energy \"j\" has the form\n\nwhere \"k\" is Boltzmann's constant. Perturbing in \"φ\" and expanding the exponential to first order, we obtain\n\nwhere\n\nThe associated length \"λ\" ≡ 1/\"k\" is called the Debye length. The Debye length is the fundamental length scale of a classical plasma.\n\nIn the Fermi-Thomas approximation, named after Llewellyn Thomas and Enrico Fermi, the system is maintained at a constant electron chemical potential (Fermi level) and at low temperature. The former condition corresponds, in a real experiment, to keeping the metal/fluid in electrical contact with a fixed potential difference with ground. The chemical potential \"μ\" is, by definition, the energy of adding an extra electron to the fluid. This energy may be decomposed into a kinetic energy \"T\" part and the potential energy -\"eφ\" part. Since the chemical potential is kept constant,\n\nIf the temperature is extremely low, the behavior of the electrons comes close to the quantum mechanical model of a Fermi gas. We thus approximate \"T\" by the kinetic energy of an additional electron in the Fermi gas model, which is simply the Fermi energy \"E\". The Fermi energy for a 3D system is related to the density of electrons (including spin degeneracy) by\n\nwhere \"k\" is the Fermi wavevector. Perturbing to first order, we find that\n\nInserting this into the above equation for \"Δμ\" yields\n\nwhere\n\nis called the Fermi-Thomas screening wave vector.\n\nThis result follows from the equations of a Fermi gas, which is a model of non-interacting electrons, whereas the fluid, which we are studying, contains the Coulomb interaction. Therefore, the Fermi-Thomas approximation is only valid when the electron density is low, so that the particle interactions are relatively weak.\n\nOur results from the Debye-Hückel or Fermi-Thomas approximation may now be inserted into Poisson's equation. The result is\n\nwhich is known as the screened Poisson equation. The solution is\n\nwhich is called a screened Coulomb potential. It is a Coulomb potential multiplied by an exponential damping term, with the strength of the damping factor given by the magnitude of \"k\", the Debye or Fermi-Thomas wave vector. Note that this potential has the same form as the Yukawa potential. This screening yields a dielectric function formula_14.\n\nA mechanical formula_2-body approach provides together the derivation of screening effect and of Landau damping. It deals with a single realization of a one-component plasma whose electrons have a velocity dispersion (for a thermal plasma, there must be many particles in a Debye sphere, a volume whose radius is the Debye length). On using the linearized motion of the electrons in their own electric field, it yields an equation of the typeformula_16, where formula_17 is a linear operator, formula_18 is a source term due to the particles, and formula_19 is the Fourier-Laplace transform of the electrostatic potential. When substituting an integral over a smooth distribution function for the discrete sum over the particles in formula_17, one gets formula_21, where formula_22 is the plasma permittivity, or dielectric function, classically obtained by a linearized Vlasov-Poisson equation (section 6.4 of ), formula_23 is the wave vector, formula_24 is the frequency, and formula_25 is the sum of formula_2 source terms due to the particles (equation (20) of ).\n\nBy inverse Fourier-Laplace transform, the potential due to each particle is the sum of two parts (section 4.1 of ). One corresponds to the excitation of Langmuir waves by the particle, and the other one is its screened potential, as classically obtained by a linearized Vlasovian calculation involving a test particle (section 9.2 of ). The screened potential is the above screened Coulomb potential for a thermal plasma and a thermal particle. For a faster particle, the potential is modified (section 9.2 of ). Substituting an integral over a smooth distribution function for the discrete sum over the particles in formula_25, yields the Vlasovian expression enabling the calculation of Landau damping (section 6.4 of ).\n\nIn real metals, the screening effect is more complex than described above in the Fermi-Thomas theory. The assumption that the charge carriers (electrons) can respond at any wavevector is just an approximation. However, it is not energetically possible for an electron within or on a Fermi surface to respond at wavevectors shorter than the Fermi wavevector. This constraint is related to the Gibbs phenomenon, where Fourier series for functions that vary rapidly in space are not good approximations unless a very large number of terms in the series are retained. In physics, this phenomenon is known as Friedel oscillations, and applies both to surface and bulk screening. In each case the net electric field does not fall off exponentially in space, but rather as an inverse power law multiplied by an oscillatory term. Theoretical calculations can be obtained from quantum hydrodynamics and density functional theory (DFT).\n"}
{"id": "30129507", "url": "https://en.wikipedia.org/wiki?curid=30129507", "title": "Electrochemical engineering", "text": "Electrochemical engineering\n\nElectrochemical engineering is the branch of chemical engineering dealing with the technological applications of electrochemical phenomena, such as electrosynthesis of chemicals, electrowinning and refining of metals, flow batteries and fuel cells, surface modification by electrodeposition, electrochemical separations and corrosion. This discipline is an overlap between electrochemistry and chemical engineering.\n\nAccording with the IUPAC, the term \"electrochemical engineering\" is reserved for electricity intensive processes for industrial or energy storage applications, and should not be confused with \"applied electrochemistry\", which comprises small batteries, amperometric sensors, microfluidic devices, microelectrodes, solid-state devices, voltammetry at disc electrodes, etc. \n\nMore than 6% of the electricity is consumed by large-scale electrochemical operations in the US.\n\nElectrochemical engineering combines the study of heterogeneous charge transfer at electrode/electrolyte interphases with the development of practical materials and processes. Fundamental considerations include electrode materials and the kinetics of redox species. The development of the technology involves the study of the electrochemical reactors, their potential and current distribution, mass transport conditions, hydrodynamics, geometry and components as well as the quantification of its overall performance in terms of reaction yield, conversion efficiency, and energy efficiency. Industrial developments require further reactor and process design, fabrication methods, testing and product development.\n\nElectrochemical engineering considers current distribution, fluid flow, mass transfer, and the kinetics of the electro reactions in order to design efficient electrochemical reactors.\n\nMost electrochemical operations are performed in filter-press reactors with parallel plate electrodes or, less often, in stirred tanks with rotating cylinder electrodes. Fuel cell and flow battery stacks are types of filter-press reactors. Most of them are continuous operations.\n\nThis branch of engineering emerged gradually from chemical engineering as electrical power sources became available in the mid 19th century. Michael Faraday described his laws of electrolysis in 1833, relating for the first time the amount of electrical charge and converted mass. In 1886 Charles Martin Hall developed a cheap electrochemical process for the extraction of aluminium from its ore in molten salts, constituting the first true large-scale electrochemical industry. Later, Hamilton Castner improved the process aluminium manufacturing and devised the electrolysis of brine in large mercury cells for the production of chlorine and caustic soda, effectively founding the chlor-alkali industry with Karl Kellner in 1892. The next year, Paul L. Hulin patented filter-press type electrochemical cells in France. Charles Frederick Burgess developed the electrolytic refining of iron ca. 1904 and later ran a successful battery company. Burgess published one of the first texts on the field in 1920. During the first three decades of the 20th century, industrial electrochemistry followed an empirical approach. \n\nAfter the Second World War, interest focused towards the fundaments of electrochemical reactions. Among other developments, the potentiostat (1937) enabled such studies. A critical advance was provided by the work of Carl Wagner and Veniamin Levich in 1962 who linked the hydrodynamics of a flowing electrolyte towards a rotating disc electrode with the mass transport control of the electrochemical reaction through a rigorous mathematical treatment. The same year, Wagner described for the first time \"The Scope of Electrochemical Engineering\" as a separated discipline from a physicochemical perspective. During 60s and 70s Charles W. Tobias, who is regarded as the \"father of electrochemical engineering\" by the Electrochemical Society, was concerned with ionic transport by diffusion, migration, and convection, exact solutions of potential and current distribution problems, conductance in heterogeneous media, quantitative description of processes in porous electrodes. Also in the 60s, John Newman pioneered the study of many of the physicochemical laws that govern electrochemical systems, demonstrating how complex electrochemical processes could be analysed mathematically to correctly formulate and solve problems associated with batteries, fuel cells, electrolyzers and related technologies. In Switzerland, Norbert Ibl contributed with experimental and theoretical studies of mass transfer and potential distribution in electrolyses, especially at porous electrodes. Fumio Hine carried out equivalent developments in Japan. Several individuals, including Kuhn, Kreysa, Rousar, Fleischmann, Alkire, Coeuret, Pletcher and Walsh established many other training centers and, with their colleagues, developed important experimental and theoretical methods of study. Currently, the main tasks of electrochemical engineering consist in the development of efficient, safe and sustainable technologies for the production of chemicals, metal recovery, remediation and decontamination technologies as well as the design of fuel cells, flow batteries and industrial electrochemical reactors.\n\nThe history of electrochemical engineering has been summarised by Wendt, Lapicque, and Stankovic.\n\nElectrochemical engineering is applied in industrial water electrolysis, electrolysis, electrosynthesis, electroplating, fuel cells, flow batteries, decontamination of industrial effluents, electrorefining, electrowinning, etc. The main example of an electrolysis based process is the Chloralkali process for production of caustic soda and chlorine. Other inorganic chemicals produced by electrolysis include:\n\n\nThe established performance criteria, definitions and nomenclature for electrochemical engineering can be found in Kreysa et al. and a IUPAC report.\n\n\n\n\n"}
{"id": "49927687", "url": "https://en.wikipedia.org/wiki?curid=49927687", "title": "Genecology", "text": "Genecology\n\nGenecology is a branch of ecology which studies genetic variation of species and communities compared to their population distribution in a particular environment. It is closely related to ecogenetics, but genecology focuses primarily on an ecological perspective, looking at changes and interactions between species, while ecogenetics focuses more on species' genetic responses to the environment.\n\n"}
{"id": "43403802", "url": "https://en.wikipedia.org/wiki?curid=43403802", "title": "Granite Island Recreation Park", "text": "Granite Island Recreation Park\n\nGranite Island Recreation Park is a protected area including all of Granite Island which is about south-east of Victor Harbor in South Australia and about south of Adelaide. It is reported as being 'the most visited park in South Australia'. The park was proclaimed in 1999 under the \"National Parks and Wildlife Act 1972\" and is classified as an IUCN Category IV protected area.\n\n\n"}
{"id": "9141957", "url": "https://en.wikipedia.org/wiki?curid=9141957", "title": "Ground effect (cars)", "text": "Ground effect (cars)\n\nIn car design, ground effect is a series of aerodynamic effects which have been exploited to create downforce, particularly in racing cars. This has been the successor to the earlier dominant aerodynamic theory of streamlining. American racing IndyCars employ ground effects in their engineering and designs, similarly they are also employed in other racing series to some extent; however Formula One and many other racing series, primarily across Europe, employ regulations (or complete bans) to limit its effectiveness on safety grounds.\n\nIn racing cars, a designer's aim is for increased downforce and grip to achieve higher cornering speeds. A substantial amount of downforce is available by understanding the ground to be part of the aerodynamic system in question, hence the name \"ground effect\". Starting in the mid-1960s, 'wings' were routinely used in the design of race cars to increase downforce (this is \"not\" a type of ground effect). Designers shifted their efforts at understanding air flow around the perimeter, body skirts, and undersides of the vehicle to increase downforce with less drag than compared to using a wing.\n\nThis kind of ground effect is easily illustrated by taking a tarpaulin out on a windy day and holding it close to the ground: it can be observed that when close enough to the ground the tarp will be drawn towards the ground. This is due to Bernoulli's principle; as the tarp gets closer to the ground, the cross sectional area available for the air passing between it and the ground shrinks. This causes the air to accelerate and as a result pressure under the tarp drops while the pressure on top is unaffected, and together this results in a net downward force. The same principles apply to cars.\n\nThe Bernoulli principle is not the only mechanic in generating ground effect downforce. A large part of ground effect performance comes from taking advantage of viscosity. In the tarp example above neither the tarp nor the ground is moving. The boundary layer between the two surfaces works to slow down the air between them which lessens the Bernoulli effect. When a car moves over the ground the boundary layer on the ground becomes helpful. In the reference frame of the car, the ground is moving backwards at some speed. As the ground moves, it pulls on the air above it and causes it to move faster. This enhances the Bernoulli effect and increases downforce. It is an example of Couette flow. If a car were to get turned at a high enough speed, the car would generate lift and possibly make it take off, or \"Blow-over\".\n\nWhile such downforce-producing aerodynamic techniques are often referred to with the catch-all term \"ground effect\", they are not strictly speaking a result of the same aerodynamic phenomenon as the ground effect which is apparent in aircraft at very low altitudes.\n\nAmerican Jim Hall built his developed Chaparral cars to both these principles, pioneering them. His 1961 car attempted to use the shaped underside method but there were too many other aerodynamic problems with the car for it to work properly. His 1966 cars used a dramatic high wing for their downforce. His Chaparral 2J \"sucker car\" of 1970 was revolutionary. It had two fans at the rear of the car driven by a dedicated two-stroke engine; it also had \"skirts\", which left only a minimal gap between car and ground, to seal the cavity from the atmosphere. Although it did not win a race, some competition had lobbied for its ban, which came into place at the end of that year. Movable aerodynamic devices were banned from most branches of the sport.\n\nFormula One was the next setting for ground effect in racing cars. Several Formula One designs came close to the ground effect solution which would eventually be implemented by Lotus. In 1968 and 1969, Tony Rudd and Peter Wright at British Racing Motors (BRM) experimented on track and in the wind tunnel with long aerodynamic section side panniers to clean up the turbulent airflow between the front and rear wheels. Both left the team shortly after and the idea was not taken further. Robin Herd at March Engineering, on a suggestion from Wright, used a similar concept on the 1970 March Formula One car. In both cars the sidepods were too far away from the ground for significant ground effect to be generated, and the idea of sealing the space under the wing section to the ground had not yet been developed.\n\nAt about the same time, Shawn Buckley began his work in 1969 at the Univ. of California - Berkeley on undercar aerodynamics sponsored by Colin Chapman, founder of Formula One Lotus. Buckley had previously designed the first high wing used in an IndyCar, Jerry Eisert's \"Bat Car\" of the 1966 Indianapolis 500. By proper shaping of the car's underside, the air speed there could be increased, lowering the pressure and pulling the car down onto the track. His test vehicles had a Venturi-like channel beneath the cars sealed by flexible side skirts that separated the channel from above-car aerodynamics. He investigated how flow separation on the undersurface channel could be influenced by boundary layer suction and divergence parameters of the underbody surface. Later, as a mechanical engineering professor at MIT, Buckley worked with Lotus developing the Lotus 78.\n\nOn a different tack, Brabham designer Gordon Murray used air dams at the front of his Brabham BT44s in 1974 to exclude air from flowing under the vehicle. Upon discovering that these tended to wear away with the pitching movement of the car, he placed them further back and discovered that a small area of negative pressure was formed under the car, generating a useful amount of downforce - around 150 lbs. McLaren produced similar underbody details for their McLaren M23 design.\n\nIn 1977 Rudd and Wright, now at Lotus, developed the Lotus 78 'wing car', based on a concept from Lotus owner and designer Colin Chapman. Its sidepods, bulky constructions between front and rear wheels, were shaped as inverted aerofoils and sealed with flexible \"skirts\" to the ground. The design of the radiators, embedded into the sidepods, was partly based on that of the de Havilland Mosquito aircraft. The team won 5 races that year, and 2 in 1978 while they developed the much improved Lotus 79. The most notable contender in 1978 was the Brabham-Alfa Romeo BT46B Fancar, designed by Gordon Murray. Its fan, spinning on a horizontal, longitudinal axis at the back of the car, took its power from the main gearbox. The car avoided the sporting ban by claims that the fan's main purpose was for engine cooling as less than 50% of the airflow was used to create a depression under the car. It raced just once, with Niki Lauda winning at the Swedish Grand Prix. The car's supreme advantage was proven after the track became oily. While other cars had to slow, Lauda was able to accelerate over the oil due to the tremendous downforce, which rose with engine speed. The car was also observed to visibly squat when the engine was revved at a standstill. Brabham's owner, Bernie Ecclestone, who had recently become president of the Formula One Constructors Association, reached an agreement with other teams to withdraw the car after three races. However the Fédération Internationale de l'Automobile (FIA), governing body of Formula One and many other motor sports, decided to ban 'fan cars' with almost immediate effect. The Lotus 79, on the other hand, went on to win six races and the world championship for Mario Andretti and gave teammate Ronnie Peterson a posthumous second place, demonstrating just how much of an advantage the cars had. In following years other teams copied and improved on the Lotus until cornering speeds became dangerously high, resulting in several severe accidents in 1982; flat undersides became mandatory for 1983. Part of the danger of relying on ground effects to corner at high speeds is the possibility of the sudden removal of this force; if the belly of the car contacts the ground, the flow is constricted too much, resulting in almost total loss of any ground effects. If this occurs in a corner where the driver is relying on this force to stay on the track, its sudden removal can cause the car to abruptly lose most of its traction and skid off the track.\n\nThe effect was used in its most effective form in IndyCar designs. Racing series based in Europe and Australia have mainly followed the lead of Formula One and mandated flat undersides for their cars. This heavily constrains the degree to which ground effect can be generated. Nonetheless, as of 2007, Formula One cars still generate a proportion of their overall downforce by this effect: vortices generated at the front of the car are used to seal the gap between the sidepods and the track and a small diffuser is permitted behind the rear wheel centerline to slow down the high speed underbody airflow to free-flow conditions. High nose designs, starting with the Tyrrell 019 of 1990, optimize the airflow conditions at the front of the car.\n\nPorpoising is a term that was commonly used to describe a particular fault encountered in ground effect racing cars.\n\nRacing cars had only been using their bodywork to generate downforce for just over a decade when Colin Chapman's Lotus 78 and 79 cars demonstrated that ground effect was the future in Formula One, so, naturally, at this point, under-car aerodynamics were still very poorly-understood. To compound this problem the teams that were keenest to pursue ground effects tended to be the more poorly-funded British \"garagiste\" teams, who had little money to spare for wind tunnel testing, and tended simply to mimic the front-running Lotuses (including the Kauhsen and Merzario teams).\n\nThis led to a generation of cars that were designed as much by hunch as by any great knowledge of the finer details, making them extremely pitch-sensitive. As the centre of pressure on the sidepod aerofoils moved about depending on the car's speed, attitude, and ground clearance, these forces interacted with the car's suspension systems, and the cars began to resonate, particularly at slow speeds, rocking back and forth - sometimes quite violently. Some drivers were known to complain of sea-sickness. This rocking motion, like a porpoise diving into and out of the sea as it swims at speed, gives the phenomenon its name. These characteristics, combined with a rock-hard suspension, resulted in the cars giving an extremely unpleasant ride. Ground effects were largely banned from Formula One in the early 1980s, but Group C sportscars and other racing cars continued to suffer from porpoising until better knowledge of ground effects allowed designers to minimise the problem.\n\n\n\n"}
{"id": "239097", "url": "https://en.wikipedia.org/wiki?curid=239097", "title": "Halide", "text": "Halide\n\nA halide is a binary phase, of which one part is a halogen atom and the other part is an element or radical that is less electronegative (or more electropositive) than the halogen, to make a fluoride, chloride, bromide, iodide, astatide, or theoretically tennesside compound. The alkali metals combine directly with halogens under appropriate conditions forming halides of the general formula, MX (X = F, Cl, Br or I). Many salts are halides; the \"hal-\" syllable in \"halide\" and \"halite\" reflects this correlation. All Group 1 metals form halides that are white solids at room temperature.\n\nA halide ion is a halogen atom bearing a negative charge. The halide anions are fluoride (F), chloride (Cl), bromide (Br), iodide (I) and astatide (At). Such ions are present in all ionic halide salts. Halide minerals contain halides.\n\nAll these halides are colourless, high melting crystalline solids having high negative enthalpies of formation.\n\nHalide compounds such as KCl, KBr and KI can be tested with silver nitrate solution, AgNO. The halogen will react with Ag and form a precipitate, with varying colour depending on the halogen:\n\nFor organic compounds containing halides, the Beilstein test is used.\n\n are used in high-intensity discharge lamps called metal halide lamps, such as those used in modern street lights. These are more energy-efficient than mercury-vapor lamps, and have much better colour rendition than orange high-pressure sodium lamps. Metal halide lamps are also commonly used in greenhouses or in rainy climates to supplement natural sunlight.\n\nSilver halides are used in photographic films and papers. When the film is developed, the silver halides which have been exposed to light are reduced to metallic silver, forming an image.\n\nHalides are also used in solder paste, commonly as a Cl or Br equivalent.\n\nSynthetic organic chemistry often incorporates halogens into organohalide compounds.\n\nExamples of halide compounds are:\n\n"}
{"id": "43360904", "url": "https://en.wikipedia.org/wiki?curid=43360904", "title": "Hantangang Dam", "text": "Hantangang Dam\n\nThe Hantangang Dam is a gravity dam currently under construction on the Hantan River in Yeoncheon County, Gyeonggi Province, South Korea. Construction on the dam began in 2007 and it is expected to be completed in mid-2015. The primary purpose of the dam will be flood control and it was proposed in 1998 after a series of deadly floods in the late 1990s. These floods killed 128 people, displaced over 31,000 and caused about US$900 million in property damage. Initially designed as a multi-purpose project, the design was changed solely to flood control in 2006 due to the concerns of residents upstream. It is being implemented by Korea Water Resources Corporation (K-water).\n\nConstruction started in February 2007 and is expected to be complete in mid-2015. K-water is responsible for planning and supervision. Daelim is undertaking the construction as a leading contractor. Not only to secure the stability against overflow during the flood season but also to reduce the construction cost through the mechanized construction fitting for the mega construction, the roller-compacted concrete (RCC) method has been applied. About 300 families will be displaced when the reservoir is filled.\n\nThe Hantangang Dam will be a and long gravity dam constructed of roller-compacted concrete. It will have four spillways; a service, emergency, lower sediment discharge and eco-corridor. All four together will have a maximum discharge of . The water storage capacity for the reservoir is .\n"}
{"id": "991459", "url": "https://en.wikipedia.org/wiki?curid=991459", "title": "Hopper crystal", "text": "Hopper crystal\n\nA hopper crystal is a form of crystal, defined by its \"hoppered\" shape.\n\nThe edges of hoppered crystals are fully developed, but the interior spaces are not filled in. This results in what appears to be a hollowed out step lattice formation, as if someone had removed interior sections of the individual crystals. In fact, the \"removed\" sections never filled in, because the crystal was growing so rapidly that there was not enough time (or material) to fill in the gaps. The interior edges of a hoppered crystal still show the crystal form characteristic to the specific mineral, and so appear to be a series of smaller and smaller stepped down miniature versions of the original crystal.\n\nHoppering occurs when electrical attraction is higher along the edges of the crystal; this causes faster growth at the edges than near the face centers. This attraction draws the mineral molecules more strongly than the interior sections of the crystal, thus the edges develop more quickly. However, the basic physics of this type of growth is the same as that of dendrites but, because the anisotropy in the solid–liquid inter-facial energy is so large, the dendrite so produced exhibits a faceted morphology.\n\nHoppering is common in many minerals, including lab-grown bismuth, galena, quartz (called skeletal or fenster crystals), gold, calcite, halite (salt), and water (ice).\n\n\n"}
{"id": "39343070", "url": "https://en.wikipedia.org/wiki?curid=39343070", "title": "Jinshan Nuclear Power Plant", "text": "Jinshan Nuclear Power Plant\n\nThe Jinshan Nuclear Power Plant or Chin Shan Nuclear Power Plant (金山核能發電廠), First Nuclear Power Plant (第一核能發電廠 or 核一), is a nuclear power plant in Shimen, New Taipei, Taiwan. Commissioned in 1978 for its first nuclear reactor, the plant is Taiwan's first nuclear power plant as well as Taiwan's smallest nuclear power plant.\n\nThe power plant can generate 9 billion kWh of electricity per year.\n\nThe two spent fuel pools at the plant have 3,074 and 3,076 control rods respectively with a maximum storage of 3,083 rods per pool.\n\nTaipower, as the operator of the power plant, was required by the Radiation Monitoring Center of the Atomic Energy Council to hand in the 2018 decommissioning plans for the plant by December 2015 for the authority to review all of the plans before the decommissioning date. Once the reactors have been shut down, the plant should be dismantled within 25 years.\n\nTaipower plans to allocate NT$18.2 billion for the disposal of nuclear waste from the decommissioned plant over the next 25 years. Currently Taipower is doing feasibility study of building a nuclear waste storage facility on an uninhabited island around Taiwan.\n\nThe July 2013 Typhoon Soulik caused a trip to the generator and turbine of the power plant Unit-2 because one suspension ground line failed and hit the transmission line when the typhoon hit the island on 13–14 July. The typhoon also caused the seawater inlet to be blocked by large amount of debris and damaged three fine filters, traveling filter rake and the plant's switchyard. The damage caused the plant to be offline for several days.\n\nIn August 2013, it was reported that there might have been radioactive water leaks for three years from the storage pools of the nuclear power plant's two reactors. Official from Taipower said that the water might come from different sources, such as condensation water or water used for cleaning up the floors. The water however has been collected in a reservoir next to the storage pools used for spent nuclear rods and has been recycled back into the storage pools, thus is claimed to pose no threat to the environment.\n\nIn December 2013, the circulating pump of the second reactor tripped due to the low lube oil pressure which caused a built-in lube oil pump. The Atomic Energy Council was criticized due to their very slow respond in giving answers to the public only 10 hours after the trip.\n\nOn 10 December 2014, two reactors were deactivated for annual maintenance operation.\n\nOn 28 December 2014, the number 1 reactor of the plant was shutdown and went out of service due to a component failure.\n\nOn 16 May 2015, an air conditioning unit in the seawater pump house of the plant caught fire. The fire was put out immediately and did not cause any safety concern or affect the power generation.\n\nOn 10 March 2016, a reactor of the plant was shut down due to a higher than normal water level caused by negligence from the plant employees who inadvertently touched a power button. Safety inspection was completed on 14 March 2016 and the plant resumed its operation.\n\nOn 4 August 2016, smoke rose out from the power plant resulted from unstable voltage frequency which caused external circuit breakers to trip and produced smoke.\n\nOn 2 June 2017, a reactor of the plant tripped after a transmission tower on a nearby hilltop toppled due to heavy rain. The second reactor was subsequently shut down automatically as a safety measure.\n\nOn 11 June 2018, the emergency backup generators were started unexpectedly due to the sudden voltage drop from an incoming high voltage transmission line. They were started at 6:42 a.m. and were shut down at 9:03 a.m. after the power line had returned to normal.\n\n"}
{"id": "25213692", "url": "https://en.wikipedia.org/wiki?curid=25213692", "title": "Kazi Magomed–Astara–Abadan pipeline", "text": "Kazi Magomed–Astara–Abadan pipeline\n\nThe Kazi Magomed–Astara–Abadan pipeline is a natural gas pipeline from Kazi Magomed in Azerbaijan to Iran.\n\nThe pipeline was agreed between Iran and the Soviet Union in 1965. It was inaugurated in October 1970 in Astara by Mohammad Rezā Shāh Pahlavi and Nikolai Podgorny, Chairman of the Presidium of the Supreme Soviet. In 1971–1979, Southern Caucasus republics of the Soviet Union were supplied through this pipeline by natural gas from Iran. After Iranian Revolution Iranian supplies were cut off.\n\nIn 2006, Azerbaijan began a swap deal with Iran, providing gas through the Baku-Astara line to Iran; while Iran supplies Nakhchivan. On 11 November 2009, the State Oil Company of Azerbaijan (SOCAR) and National Iranian Gas Company signed a memorandum according to which Azerbaijani will supply starting from 2010 500 million cubic meters of natural gas per year.\n\nThe overall length of the pipeline is , of which in Azerbaijan. The pipe diameter is and it had original capacity of 10 billion cubic meters of natural gas per year at . The Iranian section of the pipeline is known as IGAT1.\n"}
{"id": "9585894", "url": "https://en.wikipedia.org/wiki?curid=9585894", "title": "Kohn anomaly", "text": "Kohn anomaly\n\nA Kohn anomaly is an anomaly in the dispersion relation of a phonon branch in a metal. For a specific wavevector, the frequency—and thus the energy—of the associated phonon is considerably lowered, and there is a discontinuity in its derivative. They have been first proposed by Walter Kohn in 1959. In extreme cases (that can happen in low-dimensional materials), the energy of this phonon is zero, meaning that a static distortion of the lattice appears. This is one explanation for charge density waves in solids. The wavevectors at which a Kohn anomaly is possible are the nesting vectors of the Fermi surface, that is vectors that connect a lot of points of the Fermi surface (for a one-dimensional chain of atoms this vector would be formula_1).\n\nIn the phononic spectrum of a metal a Kohn anomaly is a discontinuity in the derivative of the dispersion relation that occurs at certain high symmetry points of the first Brillouin zone, produced by the abrupt change in the screening of lattice vibrations by conduction electrons. \nKohn anomalies arise together with Friedel oscillations when one considers the Lindhard approximation instead of the Thomas-Fermi approximation in order to find an expression for the dielectric function of a homogeneous electron gas. The expression for the real part formula_2 of the reciprocal space dielectric function obtained following the Lindhard theory includes a logarithmic term that is singular at formula_3, where formula_4 is the Fermi wavevector. Although this singularity is quite small in reciprocal space, if one takes the Fourier transform and passes into real space, the Gibbs phenomenon causes a strong oscillation of formula_5 in the proximity of the singularity mentioned above. In the context of phonon dispersion relations, these oscillations appear as a vertical tangent in the plot of formula_6, called the Kohn anomalies.\n\nMany different systems exhibit Kohn anomalies, including graphene, bulk metals, and many low-dimensional systems (the reason involves the condition formula_7, which depends on the topology of the Fermi surface). However, it is important to emphasize that only materials showing metallic behaviour can exhibit a Kohn anomaly, as we are dealing with approximations that need a homogeneous electron gas.\n\nFor experimental results, one can turn to .\n\n"}
{"id": "9028960", "url": "https://en.wikipedia.org/wiki?curid=9028960", "title": "Load following power plant", "text": "Load following power plant\n\nA load following power plant, regarded as producing mid-merit or mid-priced electricity, is a power plant that adjusts its power output as demand for electricity fluctuates throughout the day. Load following plants are typically in-between base load and peaking power plants in efficiency, speed of start up and shut down, construction cost, cost of electricity and capacity factor.\n\nBase load power plants operate at maximum output. They shut down or reduce power only to perform maintenance or repair. Base load power plants include coal, fuel oil, almost all nuclear, geothermal, hydroelectric, biomass and combined cycle natural gas plants.\n\nPeaking power plants operate only during times of peak demand. In countries with widespread air conditioning, demand peaks around the middle of the afternoon, so a typical peaking power plant may start up a couple of hours before this point and shut down a couple of hours after. However, the duration of operation for peaking plants varies from a good portion of the waking day to only a couple of dozen hours per year. Peaking power plants include hydroelectric and gas turbine power plants. Many gas turbine power plants can be fueled with natural gas, fuel oil, and/or diesel. While most gas turbine plants primarily burn natural gas, a supply of fuel oil and/or diesel is sometimes kept on hand in case the gas supply is interrupted. Other gas turbines can only burn a single fuel.\n\nLoad following power plants run during the day and early evening. They either shut down or greatly curtail output during the night and early morning, when the demand for electricity is the lowest. The exact hours of operation depend on numerous factors. One of the most important factors for a particular plant is how efficiently it can convert fuel into electricity. The most efficient plants, which are almost invariably the least costly to run per kilowatt-hour produced, are brought online first. As demand increases, the next most efficient plants are brought on line and so on. The status of the electrical grid in that region, especially how much base load generating capacity it has, and the variation in demand are also very important. An additional factor for operational variability is that demand does not vary just between night and day. There are also significant variations in the time of year and day of the week. A region that has large variations in demand will require a large load following or peaking power plant capacity because base load power plants can only cover the capacity equal to that needed during times of lowest demand.\n\nLoad following power plants can be hydroelectric power plants, diesel and gas engine power plants, combined cycle gas turbine power plants and steam turbine power plants that run on natural gas or heavy fuel oil, although heavy fuel oil plants make up a very small portion of the energy mix. A relatively efficient model of gas turbine that runs on natural gas can also make a decent load following plant.\n\nGas turbine power plants are the most flexible in terms of adjusting power level, but are also among the most expensive to operate. Therefore, they are generally used as \"peaking\" units at times of maximum power demand. Gas turbines find only limited application as prime movers for power generation; one such use is power generation at remote military facilities, mine sites and rural or isolated communities. This is because gas turbine generators typically have significantly higher heat loss rates than steam turbine or diesel power plants; their higher fuel costs quickly outweigh their initial advantages in most applications. Applications to be evaluated include:\n\nDiesel and gas engine power plants can be used for base load to stand-by power production due to their high overall flexibility. Such power plants can be started rapidly to meet the grid demands. These engines can be operated efficiently on a wide variety of fuels, adding to their flexibility.\n\nSome applications are: base load power generation, wind-diesel, load following, cogeneration and trigeneration.\n\nHydroelectric power plants can operate as base load, load following or peaking power plants. They have the ability to start within minutes, and in some cases seconds. How the plant operates depends heavily on its water supply as many plants do not have enough water to operate anywhere near their full capacity on a continuous basis.\n\nWhere hydroelectric dams or associated reservoirs exist, these can often be backed up, reserving the hydro draw for a peak time, but this introduces ecological & mechanical stress & so is practiced less today than previously. Lakes and man made reservoirs used for hydropower come in all sizes, holding enough water for as little as a one-day supply (a diurnal peak variance), or as much as a whole year's supply (allowing for seasonal peak variance). A plant with a reservoir that holds less than the annual river flow may change its operating style depending on the season of the year. For example, the plant may operate as a peaking plant during the dry season, as a base load plant during the wet season and as a load following plant between seasons. A plant with a large reservoir may operate independently of wet and dry seasons, such as operating at maximum capacity during peak heating or cooling seasons.\n\nDaily Peak Load with large Hydro, base load Thermal generation and intermittent Wind power. Hydro is load following and managing the peaks, with some response from base load thermal.\n\nLarge size coal fired thermal power plants can also be used as load following / variable load power stations to varying extents, with hard coal fueled plants typically being significantly more flexible than lignite fueled coal plants. Some of the features which may be found in coal plants that have been optimized for load following include:\n\n\nLoad following is the potential for a power plant to adjust its power output as demand and price for electricity fluctuates throughout the day. In nuclear power plants, this is done by inserting control rods into the reactor pressure vessel. This operation is very inefficient as nuclear power generation is composed almost entirely of fixed and sunk costs; therefore, lowering the power output doesn't significantly reduce generating costs. Moreover, the plant is thermo-mechanically stressed. Older nuclear (and coal) power plants may take many hours, if not days, to achieve a steady state power output.\n\nModern nuclear plants with light water reactors are designed to have strong maneuvering capabilities. Nuclear power plants in France and in Germany operate in load-following mode and so participate in the primary and secondary frequency control. Some units follow a variable load program with one or two large power changes per day. Some designs allow for rapid changes of power level around rated power, a capability that is usable for frequency regulation. A more efficient solution is to maintain the primary circuit at full power and to use the excess power for cogeneration.\n\nBoiling water reactors (BWRs) can vary the speed of recirculation water flow to quickly reduce their power level down to 60% of rated power (up to 10%/minute), making them useful for overnight load-following. They can also use control rod manipulation to achieve deeper reductions in power. A few BWR designs do not have recirculation pumps, and these designs must rely solely on control rod manipulation in order to load follow, which is possibly less ideal. In markets such as Chicago, Illinois where half of the local utility's fleet is BWRs, it is common to load-follow (although potentially less economic to do so).\n\nPressurized water reactors (PWRs) use a combination of a chemical shim (typically boron) in the moderator/coolant, control rod manipulation, and turbine speed control (see nuclear reactor technology) to modify power levels. For PWRs not explicitly designed with load following in mind, load following operation isn't quite as common as it is with BWRs. However, modern PWRs are generally designed to handle extensive regular load following, and both French and German PWRs in particular have historically been designed with varying degrees of enhanced load following capabilities.\n\nFrance in particular has a long history of utilizing aggressive load following with their PWRs, which are capable of (and used for) both primary and secondary frequency control in addition to load following. French PWRs use \"grey\" and/or \"black\" control rods in order to maneuver power more rapidly than chemical shim control or conventional control rods allow. These reactors have the capability to regularly vary their output between 30–100% of rated power, to maneuver power up or down by 2–5%/minute during load following activities, and to participate in primary and secondary frequency control at ±2–3% (primary frequency control) and ±3–5% (secondary frequency control, ≥5% for N4 reactors in Mode X). Depending on the exact design and operating mode, their ability to handle low power operation or fast ramping may be partially limited during the very late stages of the fuel cycle.\n\nModern CANDU designs have extensive steam bypass capabilities that allow for a different method of load following that does not necessarily involve changes in reactor power output. Bruce Nuclear Generating Station is a CANDU pressurized heavy water reactor that regularly utilizes its ability to partially bypass steam to the condenser for extended periods of time while the turbine is operating to provide 300 MW per unit (2400 MW total for the eight-unit plant) of flexible (load following) operation capabilities. Reactor power is maintained at the same level during steam bypass operations, which completely avoids xenon poisoning and other concerns associated with maneuvering reactor power output.\n\nThe variable power from renewable energy such as solar and wind power plants can be used to follow the load or stabilize the grid frequency with the help of various means of storage. For countries that are trending away from coal fired baseload plants and towards intermittent energy sources such as wind and solar, that have not yet fully implemented smart grid measures such as demand side management to rapidly respond to changes in this supply, there may be a need for dedicated peaking or load following power plants and the use of a grid intertie, at least until the peak blunting & load shifting mechanisms are implemented widely enough to match supply. \"See smart grid alternatives below.\"\n\nRechargeable battery storage units as of 2016, when custom-built new for this purpose without re-using electric vehicle batteries, could cost up to $450,000 per MWh. When the grid frequency is below the desired or rated value, the power being generated (if any) and the stored battery power is fed to the grid to raise the grid frequency. When the grid frequency is above the desired or rated value, the power being generated is fed or surplus grid power is drawn (in case cheaply available) to the battery units for energy storage. The grid frequency keeps on fluctuating 50 to 100 times in a day above and below the rated value depending on the type of load encountered and the type of generating plants in the electrical grid. Recently, the cost of battery units, solar power plants, etc. have come down drastically to utilise secondary power for power grid stabilization as an on line spinning reserve.\n\nNew studies have also evaluated of both wind and solar plant to follow fast load changes. A studies by Gevorgian et al have shown the ability of solar plants to provide load following and fast reserves in both island power system like Puerto Rico and large power system in California.\n\nThe decentralized & intermittent nature of solar & wind generation entails building signalling networks across vast areas. These include large consumers with discretionary uses, and increasingly include much smaller users. Collectively, these signalling & communication technologies are called the \"smart grid.\" When these technologies reach into most grid-connected devices the term Energy Internet is sometimes used, though this is more commonly considered to be an aspect of the Internet of Things.\n\nIn 2010, US FERC Chairman Jon Wellinghof outlined the Obama administration's view that strongly preferred smart grid signalling over dedicated load following power plant, describing following as inherently inefficient. In Scientific American he listed some such measures:\n\nAt the time, electric vehicle battery integration into the grid was beginning. Wellinghof referred (ibid) to \"these cars now getting paid in Delaware: $7 to $10 a day per car. They are getting paid over $3,000 a year to use these cars to simply control regulation service on the grid when they are charged\".\n\nDue to the very high cost of dedicated battery storage, use of electric vehicle batteries both while charging in vehicles (see smart grid), and in stationary grid energy storage arrays as an end-of-life re-use once they no longer hold enough charge for road use, has become the preferred method of load following over dedicated power plants. Such stationary arrays act as a true load following power plant, and their deployment can \"improve the affordability of purchasing such vehicles...Batteries that reach the end of their useful lifespan within the automotive industry can still be considered for other applications as between 70-80% of their original capacity still remains.\" Such batteries are also often repurposed in home arrays which primarily serve as backup, so can participate much more readily in grid stabilizing. The number of such batteries doing nothing is increasing rapidly, e.g. in Australia where Tesla Powerwall demand rose 30x after major power outages \n\nHome & vehicle batteries are always & necessarily charged responsively when supply is available, meaning they all participate in a smart grid, because the high load (one Japanese estimate was over 7GW for half the cars in Kanto ) simply cannot be managed on an analog grid, lest \"The uncoordinated charging can result in creation of a new peak-load.\" (ibid).\n\nGiven the charging must be managed, there is no incremental cost to delay charging or discharge these batteries as required for load following, merely a software change & in some cases a payment for the inconvenience of less than complete charging or for battery wear (e.g. \"$7 to $10 a day per car\" paid in Delaware).\n\nRocky Mountain Institute in 2015 listed the applications of such distributed networks of batteries as (for \"ISOs / RTOs\") including \"energy storage can bid into wholesale electricity markets\" or for utilities\n\nRMI claimed \"batteries can provide these services more reliably and at a lower cost than the technology that currently provides a majority of them thermal power plants (see above re coal & gas).\" Also that \"storage systems installed behind the customer meter can be dispatched to provide deferral or adequacy services to utilities, such as:\n\nSolar thermal storage plants are emerging as cheaper and clean load following power plants. They can cater the load demand perfectly and work as base load power plants when the extracted solar energy is found excess in a day. Proper mix of solar thermal storage and solar PV can fully match the load fluctuations without the need of costly battery storage.\n\n"}
{"id": "29885466", "url": "https://en.wikipedia.org/wiki?curid=29885466", "title": "Manana Kochladze", "text": "Manana Kochladze\n\nManana Kochladze is a Georgian environmentalist. She was awarded the Goldman Environmental Prize in 2004 for her environmental campaigns, in particular regarding oil pipelines through vulnerable areas.\n"}
{"id": "12859112", "url": "https://en.wikipedia.org/wiki?curid=12859112", "title": "Marshall Major", "text": "Marshall Major\n\nThe Marshall Major (Model 1967 ) was a guitar amplifier made by Marshall. It was introduced in 1967 as the \"Marshall 200\" (in reference to the wattage of the amplifier). It had a plexi panel and two inputs in one channel, but in contrast with the 100 watt heads made by Marshall, the first series had split tone controls similar to the Sound City amps. For the second series, in late 1968, Marshall reverted to ordinary passive tone controls, and was called \"Marshall Major\". Some authors claim the first version had active tone controls but this is incorrect, the schematics for all versions are available online.\n\nThe amplifier used KT88 output valves, two ECC83 preamp valves and one ECC82 valve. Approximately 1,200 of these amps were produced from 1967 to 1974; Marshall ceased production when the supply of KT88s ran out.\n\nThe amplifier was used by rock musicians who needed lots of volume. A notable user is Ritchie Blackmore; his Major had the two input channels cascaded into one, essentially creating the first Marshall with a master volume.\n\nThe Major was also made as a PA amplifier, Model 1966 (from 1967 to 1971, with eight inputs in four channels; known in 1967 as the PA 200), and as a bass amplifier, Model 1978 (from 1967 to 1974).\n\n"}
{"id": "42504227", "url": "https://en.wikipedia.org/wiki?curid=42504227", "title": "Ministerial Council on Energy", "text": "Ministerial Council on Energy\n\nThe Ministerial Council on Energy (MCE) was established as a committee of the Council of Australian Governments in June 2001. It was superseded by the Standing Council on Energy and Resources in June 2011.\n\nDuring its time, it was responsible for a range of initiatives including:\n\n"}
{"id": "36342378", "url": "https://en.wikipedia.org/wiki?curid=36342378", "title": "Naulong Dam", "text": "Naulong Dam\n\nNaulong Dam is an embankment dam currently under construction on the Mula River, about 30 km from Gandawah City in Jhal Magsi district of Balochistan, Pakistan. Its Construction Contract has been awarded to Descon Engineering Limited, which is the biggest Contractor in Pakistan.\n\nThe zoned earth-filled dam is 186 feet high with a gross storage of 0.242 MAF and a command area of 47,000 acres. It has a hydro power capacity of 4.4 MW.\n\nFeasibility studies for the dam were completed in 2009 and construction for the proposed dam started in 2012.\n\nList of dams and reservoirs in Pakistan\n"}
{"id": "40838782", "url": "https://en.wikipedia.org/wiki?curid=40838782", "title": "New Zealand Energy Corporation", "text": "New Zealand Energy Corporation\n\nNew Zealand Energy Corporation (NZEC) is an oil and gas producing company in New Zealand, established in 2010. It owns and operates gas and oil fields in Taranaki and has exploration permits in the East Coast Basin.\n\nIn 2013, NZEC purchased the Tariki, Waihapa and Ngaere mining licences and assets from Origin Energy.\n\n"}
{"id": "2129767", "url": "https://en.wikipedia.org/wiki?curid=2129767", "title": "North Dakota Pipeline Company system", "text": "North Dakota Pipeline Company system\n\nThe North Dakota Pipeline Company (NDPL) system is a 950-mile (1530 km) crude oil pipeline system that collects oil from fields in the Williston Basin in Montana and North Dakota and transports it eastward to the Mandan Refinery in Mandan, North Dakota and to other pipeline systems that carry oil to other refineries in the Midwest. About 330 miles (530 km) of pipeline are used for collection from oil wells, and this connects to a 620-mile (1,000 km) main transportation line.\n\nIn addition to collecting oil from wells in the United States, it also connects to a Canadian pipeline system owned by Enbridge at North Dakota's border with Saskatchewan. The American pipeline is owned by Enbridge Energy Partners, LP, an organization partly owned by the Canadian company.\n\nThe pipeline's eastern terminus is in Clearbrook, Minnesota where there is a junction with Enbridge's Lakehead System and the Minnesota Pipeline (ultimately owned by Koch Industries).\n\n\n\n\n"}
{"id": "6461531", "url": "https://en.wikipedia.org/wiki?curid=6461531", "title": "Pultrusion", "text": "Pultrusion\n\nPultrusion is a continuous process for manufacture of composite materials with constant cross-section. The term is a portmanteau word, combining \"pull\" and \"extrusion\". As opposed to extrusion, which pushes the material, pultrusion works by pulling the material.\n\nA very early pultrusions type patent was filed by J.H. Watson in 1944. This was followed by M.J. Meek’s filing of 1950. The first commercial pultrusions were provided by Glastic Company of Cleveland, Ohio under the patent filed in 1952 by Rodger B. White. The patent issued to W. B. Goldsworthy in 1959 helped initiate the promotion and knowledge spread within the industry. W. Brandt Goldsworthy is widely regarded as the inventor of pultrusion.\n\nParallel to the work of Goldsworthy, who concentrated his work on unsaturated polyester resins, Ernst Kühne in Germany developed a quite similar process in 1954 based on epoxy resin.\n\nInvention, development and the issuance of patents continue in the pultrusion field through today. A later innovation in this field has been developed and patented by Thomas GmbH + Co. Technik + Innovation KG in Germany 2008 and is described below.\n\n1 - Continuous roll of reinforced fibers/woven fiber mat\n2 - Tension roller\n3 - Resin Impregnator\n4 - Resin soaked fiber\n5 - Die and heat source\n6 - Pull mechanism\n7 - Finished hardened fiber reinforced polymer\n\nIn the standard pultrusion process the reinforcement materials like fibers or woven or braided strands are impregnated with resin, possibly followed by a separate preforming system, and pulled through a heated stationary die where the resin undergoes polymerization. The impregnation is either done by pulling the reinforcement through a bath or by injecting the resin into an injection chamber which typically is connected to the die. Many resin types may be used in pultrusion including polyester, polyurethane, vinylester and epoxy. Resin provides the resistance to the environment, (i.e., the corrosion resistance, the UV resistance, the impact resistance, etc.) and the glass provides strength, in addition to safety from fire.\n\nA surface veil can also be added to protect against erosion or “fiber bloom” and provide corrosion resistance and ultraviolet resistance. \n\nThe technology is not limited to thermosetting polymers. More recently, pultrusion has been successfully used with thermoplastic matrices such as polybutylene terephthalate (PBT), polyethylene terephthalate (PET) either by powder impregnation of the glass fiber or by surrounding it with sheet material of the thermoplastic matrix, which is then heated.\n\nEcological cleanness of manufactured products, in contrast to composites on thermosetting resins base, as well as practically unlimited possibilities of recycling (processing) after the resource depletion appear to be forcible arguments in favor of reinforced thermoplastics. For these reasons the industrial output and use of the given materials in highly industrialized countries have increased by 8-10% per year in recent decades. New developments (see process modifications) which enable the manufacturing not only of straight but also curved profiles are actually pushing the demand for this technology, especially in the automotive sector.\n\nPultrusion technology of manufacturing of fiber composites with polymer matrix appears to be energy-efficient and resource-saving.\n\nEconomic and environmental factors favor use of a thermoplastic matrix but due to the high viscosity of melts it is difficult to achieve high productivity and high quality of fiberfills impregnation with this type of matrix.\n\nProducts manufactured under this technology are widely used in the following industries:\n\n- In the agriculture and chemical industries for manufacturing of chemically resistant to aggressive media slatted floors with enhanced strength characteristics used in the construction of livestock facilities, chemical plants, etc.;\n\n- in the construction industry for the production of glass-fiber reinforcement, profiles, carcasses, stiffening bars for PVC-windows, etc.;\n\n- in the aerospace industry for manufacturing of structure components of aircraft;\n\n- in the sports and tourism industries for manufacturing of equipment exhibiting enhanced strength properties: skis, ski poles, golf course flagsticks, tent and hovel constructions, etc.;\n\n- in electrical power engineering for manufacturing of dielectric structures, fiberglass rods used in composite insulators and as supporting structures for elements of signaling blocks, and fiberglass profiles used in manufacturing of transformers and electric motors;\n\n- in commercial production, using grains of long-fiber molding material (LLM) as a raw material for subsequent manufacturing of structures and products with enhanced strength and chemical properties;\n\n- in the automotive industry for the production of structural and complex parts of the vehicles with enhanced stiffness, rigidity and lightness;\n\n- and in many other industries and plants, using mechanisms, structures and materials, which meet high standards of chemical, dielectric and strength stability.\n\nAs the materials are pulled through a die in the standard pultrusion process the process is only suited to manufacture straight profiles.\n\nIn a recently developed modification of the process, developed and patented by Thomas GmbH + Co. Technik + Innovation KG, the die is no longer stationary but moving back and forth along the profile to be manufactured. This modified process, known as \"Radius-Pultrusion\" allows also to manufacture two- and three-dimensional curved profiles. It also is beneficiary for a number of tasks in the linear process especially if quite complex textile reinforcements with a low rate of distortion are needed.\n\nThe design of pultrusion machines varies. Two often used types are reciprocating (hand-over-hand) and continuous (cat-track).\n\nFor the radius pultrusion process the layout of the machines has two moving stages similar to the hand over hand pulling unit, but as the process is intermittent with only one puller and the mould mounted on the stage of other one. Whether the stages are moving linear or circular depends on the type of profiles to be manufactured. The minimum radius for a linear machine with rotating stages is approx. 2 m. For smaller radii a circular movement of the mould and gripper stage is necessary.\n"}
{"id": "4844997", "url": "https://en.wikipedia.org/wiki?curid=4844997", "title": "R1 (nuclear reactor)", "text": "R1 (nuclear reactor)\n\nR1 was the first nuclear reactor of Sweden. It was a research reactor located at the Royal Institute of Technology (KTH) campus at Valhallavägen in central Stockholm, in the rock beneath the current-day Q buildings. The reactor was active from July 13, 1954 to June 6, 1970. The reactor was dismantled, and there is nothing left of it today; the reactor hall however still exists.\n\nThe capacity of the reactor was originally 300 kW but was later increased to 1 MW.\n\nIn 2016, the reactor hall was used to film the music video for Alan Walker's song \"Faded (Restrung)\". The video was released on 11 February 2016.\n"}
{"id": "33767849", "url": "https://en.wikipedia.org/wiki?curid=33767849", "title": "Reinforced solid", "text": "Reinforced solid\n\nIn solid mechanics, a reinforced solid is a brittle material that is reinforced by ductile bars or fibres. A common application is reinforced concrete. When the concrete cracks the tensile force in a crack is not carried any more by the concrete but by the steel reinforcing bars only. The reinforced concrete will continue to carry the load provided that sufficient reinforcement is present. A typical design problem is to find the smallest amount of reinforcement that can carry the stresses on a small cube (Fig. 1). This can be formulated as an optimization problem.\n\nThe reinforcement is directed in the x, y and z direction. The reinforcement ratio is defined in a cross-section of a reinforcing bar as the reinforcement area formula_1 over the total area formula_2, which is the brittle material area plus the reinforcement area.\n\nIn case of reinforced concrete the reinforcement ratios are usually between 0.1% and 2%. The yield stress of the reinforcement is denoted by formula_12. The stress tensor of the brittle material is\n\nThis can be interpreted as the stress tensor of the composite material minus the stresses carried by the reinforcement at yielding. This formulation is accurate for reinforcement ratio's smaller than 5%. It is assumed that the brittle material has no tensile strength. (In case of reinforced concrete this assumption is necessary because the concrete has small shrinkage cracks.) Therefore, the principal stresses of the brittle material need to be compression. The principal stresses of a stress tensor are its eigenvalues.\n\nThe optimization problem is formulated as follows. Minimize formula_3 + formula_6 + formula_9 subject to all eigenvalues of the brittle material stress tensor are less than or equal to zero (negative-semidefinite). Additional constraints are formula_3 ≥ 0, formula_6 ≥ 0, formula_9 ≥ 0.\n\nThe solution to this problem can be presented in a form most suitable for hand calculations. It can be presented in graphical form. It can also be presented in a form most suitable for computer implementation. In this article the latter method is shown.\n\nThere are 12 possible reinforcement solutions to this problem, which are shown in the table below. Every row contains a possible solution. The first column contains the number of a solution. The second column gives conditions for which a solution is valid. Columns 3, 4 and 5 give the formulas for calculating the reinforcement ratios.\n\nformula_20, formula_21 and formula_22 are the stress invariants of the composite material stress tensor.\n\nThe algorithm for obtaining the right solution is simple. Compute the reinforcement ratios of each possible solution that fulfills the conditions. Further ignore solutions with a reinforcement ratio less than zero. Compute the values of formula_3 + formula_6 + formula_9 and select the solution for which this value is smallest. The principal stresses in the brittle material can be computed as the eigenvalues of the brittle material stress tensor, for example by Jacobi's method.\n\nThe formulas can be simply checked by substituting the reinforcement ratios in the brittle material stress tensor and calculating the invariants. The first invariant needs to be less than or equal to zero. The second invariant needs to be greater than or equal to zero. These provide the conditions in column 2. For solution 2 to 12, the third invariant needs to be zero.\n\nThe table below shows computed reinforcement ratios for 10 stress tensors. The applied reinforcement yield stress is formula_12 = 500 N/mm². The mass density of the reinforcing bars is 7800 kg/m. In the table formula_27 is the computed brittle material stress. formula_28 is the optimised amount of reinforcement.\n\nThe above solution can be very useful to design reinforcement; however, it has some practical limitations. The following aspects can be included too if the problem is solved using convex optimization:\n\nReinforcing bars can have other directions than the x, y and z direction. In case of bars in one direction the stress tensor of the brittle material is computed by\n\nformula_29\n\nwhere formula_30 are the angles of the bars with the x, y and z axis. Bars in other directions can be added in the same way.\n\nOften, builders of reinforced concrete structures know, from experience, where to put reinforcing bars. Computer tools can support this by checking whether proposed reinforcement is sufficient. To this end the tension criterion,\n\nThe eigenvalues of formula_13 shall be less than or equal to zero.\n\nis rewritten into,\n\nThe eigenvalues of formula_32 shall be less than or equal to one.\n\nThe latter matrix is the utilization tensor. The largest eigenvalue of this tensor is the utilization (unity check), which can be displayed in a contour plot of a structure for all load combinations related to the ultimate limit state.\n\nFor example, the stress at some location in a structure is formula_33 = 4 N/mm², formula_34 = -10 N/mm², formula_35 = 3 N/mm², formula_36 = 3 N/mm², formula_37 = -7 N/mm², formula_38 = 1 N/mm². The reinforcement yield stress is formula_12 = 500 N/mm². The proposed reinforcement is formula_3 = 1.4%, formula_6 = 0.1%, formula_9 = 1.9%. The eigenvalues of the utilization tensor are -20.11, -0.33 and 1.32. The utilization is 1.32. This shows that the bars are overloaded and 32% more reinforcement is required.\n\nCombined compression and shear failure of the concrete can be checked with the Mohr-Coulomb criterion applied to the eigenvalues of the stress tensor of the brittle material.\n\nformula_43 ≤ 1 ,\n\nwhere formula_44 is the largest principal stress, formula_45 is the smallest principal stress, formula_46 is the uniaxial compressive strength (negative value) and formula_47 is a fictitious tensile strength based on compression and shear experiments.\n\nCracks in the concrete can be checked by replacing the yield stress formula_48 in the utilization tensor by the bar stress at which the maximum crack width occurs. (This bar stress depends also on the bar diameter, the bar spacing and the bar cover.) Clearly, crack widths need checking at the surface of a structure for stress states due to load combinations related to the serviceability limit state only.\n\n"}
{"id": "2226047", "url": "https://en.wikipedia.org/wiki?curid=2226047", "title": "Richard C. Duncan", "text": "Richard C. Duncan\n\nRichard Duncan is chief author of the Olduvai theory, a prediction of rapidly declining world energy production. He has an MS in Electrical Engineering (1969) and a PhD in Systems Engineering (1973) from the University of Washington.\n\nThe Olduvai theory holds that the ratio of world energy production per capita, which he denotes by the metric \"e\", will peak as the extraction rates of fossil fuels fall increasingly behind demand, causing catastrophic social and economic collapse, starting with massive electrical blackouts worldwide. He suggests that humans would eventually revert to a stone-age style of living after the majority of the world's population dies off over the coming century. In 1996, Duncan claimed that \"e\" had peaked around 1978. In 2000, the theory was revised to hold that the ratio would begin to decline around 2007. The peak was again revised in 2013 to have occurred in 2012.\n\nHe bases his theory on the fact that a steep rise in global population and petroleum use almost parallel each other but population increases at a slightly faster rate than does energy use.\n\nDuncan's research data, compiled in partnership with geologist Dr. Walter Youngquist, have become widely used resources for those studying past and current trends in oil production and depletion.\n\n"}
{"id": "2417759", "url": "https://en.wikipedia.org/wiki?curid=2417759", "title": "S&amp;P Global Platts", "text": "S&amp;P Global Platts\n\nS&P Global Platts is a provider of energy and commodities information and a source of benchmark price assessments in the physical energy markets. The business was started with the foundation in 1909 of the magazine \"National Petroleum News\" by Warren C. Platt.\n\nS&P Global Platts and Argus Media are recognized as the two most significant price reporting agencies for the oil market.\n\nFrom an original focus on the oil industry, S&P Global Platts gradually expanded its purview to include metals, shipping, and all energy-related markets – oil, coal, natural gas, electricity, nuclear power, petrochemicals, renewables, and emissions. S&P Global Platts is a division of S&P Global, Inc., (), a provider of ratings, benchmarks and analytics to the global capital and commodity markets. The firm is sister to brands like S&P Global Ratings, S&P Global Market Intelligence and S&P Dow Jones Indices. It had been part of the Commodities & Commercial group of S&P Global, but that group saw all its assets, besides Platts, slowly sold off. It is now considered a stand-alone division of S&P Global, though its President Martin Fraenkel reports into Mike Chinn, the head of S&P Global's Market Intelligence group. \n\nWarren C. Platt (1883–1963) started the magazine \"National Petroleum News\" in Cleveland, Ohio in 1909. He expanded the business with the publication of the newsletter called \"Platts Oilgram\" in 1923, which went on to be recognized as an influential source for petroleum prices. The companies founded by Platt that published prices and news were acquired in 1953 to become part of what was then known as McGraw-Hill group, which was later to become S&P Global. The publication activities that started with petroleum later expanded to cover energy and commodities, which were all undertaken by the division that became known as Platts.\n\nIn 2000 McGraw-Hill merged Platts with other like assets to turn the company into a provider of energy information.\n\nBeginning in 2010, Platts' parent has made numerous acquisitions to significantly grow the Platts business. The acquired companies were Bentek, Eclipse Energy , Kingsman , Minerals Value Service , RigData and PIRA . An attempt to acquire OPIS in 2011 failed following market objections and Federal Trade Commission resistance. \n\nPlatts' first significant acquisition came in 2001, when it acquired FT Energy. That acquisition gave it the Platts Global Energy Awards, considered one of the industry's biggest awards event, held every December in New York. \n\nSince 2013, the company has held an annual Platts Global Metals Awards, modeled on the Platts Global Energy Awards. In 2015, these were held in London.\n\n\n"}
{"id": "57186680", "url": "https://en.wikipedia.org/wiki?curid=57186680", "title": "Sanepar", "text": "Sanepar\n\nSanepar is a Brazilian water and waste management company owned by Paraná state. It provides water and sewage services to residential, commercial and industrial users in 345 cities and another 293 smaller areas in Paraná and on the city of Porto União, Santa Catarina state. It provides water to 26.7 million customers, or 60% of the population of the state. It is one of the largest water and waste management company in Brazil. It provides basic sanitation services, which include all phases (abstraction, treatment, processing, distribution) and the collection, treatment and reuse of sewage. It has an 84,600 kilometer network for the withdrawal and distribution of drinking water, for sewage collection and for the discharge of treated sewage. Regarding solid waste, it operates landfills in Apucarana, Cornélio Procópio and Cianorte\n\nHeadquartered in Curitiba Sanepar provides a universal water supply network in all the municipalities it serves. 100% of the sewage it collects is treated before discharge into water bodies.\n\nSIn addition Sanepar also has a 40% stake in CS Bioenergia S.A., a special purpose enterprise incorporated in partnership with Cattalini Bioenergia to exploit sewage-based energy generation at the biodigestion unit implanted next to the Belém wastewater treatment plant in Curitiba. The activities of CS Bioenergia will comply with Brazil's national solid waste policy governing the non-generation/reduction, reuse, treatment and disposal of waste.\n\nSanepar was founded in 1963 as Agepar. Today its stocks are traded on the São Paulo Stock Exchange.\n\n"}
{"id": "26901669", "url": "https://en.wikipedia.org/wiki?curid=26901669", "title": "Serra Mariola Natural Park", "text": "Serra Mariola Natural Park\n\nThe Serra Mariola Natural Park (, ) is a mountain range in the Valencian Community, Spain, one of the most peripheral offsprings of the Baetic System. Most if its territory is included in a natural park founded in 2002, covering an area of 17,257 ha.\n\nIt has a rectangular shape and altitudes higher than 1,000 metres, the highest peak being the Montcabrer, at 1,389 metres high. To the north the Benicadell Mountain Range has a peak bearing the same name and is 1,104 metres high. The \"Serra\" is predominantly composed of limestone. The climate is largely Mediterranean.\n\nCovering these mountain ranges are some 200 or more different aromatic and medicinal plants with hundreds of different trees, which include a variety of yew unique to this area.\n\n\n"}
{"id": "30406402", "url": "https://en.wikipedia.org/wiki?curid=30406402", "title": "Spex (solar park)", "text": "Spex (solar park)\n\nThe solar park \"Spex\" near Mérida in Spain's Extremadura provides 30 megawatts of solar power enough to supply 16,000 households. Deutsche Bank and ecoEnergías are the developers of the power plant. It was built for a cost of €250 million on a site and uses dual axis trackers. Each panel has an area of 130 m².\n\n"}
{"id": "17153836", "url": "https://en.wikipedia.org/wiki?curid=17153836", "title": "Sustainable biofuel", "text": "Sustainable biofuel\n\nSustainable biofuel is biofuel produced in a sustainable manner.\n\nIn 2008, the Roundtable for Sustainable Biofuels released its proposed standards for sustainable biofuels. This includes 12 principles: \n\nSeveral countries and regions have introduced policies or adopted standards to promote sustainable biofuels production and use, most prominently the European Union and the United States. The 2009 EU Renewable Energy Directive, which requires 10 percent of transportation energy from renewable energy by 2020, is the most comprehensive mandatory sustainability standard in place as of 2010. \n\n\"The EU Renewable Energy Directive requires that the lifecycle greenhouse gas emissions of biofuels consumed be at least 50 percent less than the equivalent emissions from gasoline or diesel by 2017 (and 35 percent less starting in 2011). Also, the feedstocks for biofuels \"should not be harvested from lands with high biodiversity value, from carbon-rich or forested land, or from wetlands\".\"\nAs with the EU, the U.S. Renewable Fuel Standard (RFS) and the California Low Carbon Fuel Standard (LCFS) both require specific levels of lifecycle greenhouse gas reductions compared to equivalent fossil fuel consumption. The RFS requires that at least half of the biofuels production mandated by 2022 should reduce lifecycle emissions by 50 percent. The LCFS is a performance standard that calls for a minimum of 10 percent emissions reduction per unit of transport energy by 2020. Both the U.S. and California standards currently address only greenhouse gas emissions, but California plans to \"expand its policy to address other sustainability issues associated with liquid biofuels in the future\".\n\nIn 2009, Brazil also adopted new sustainability policies for sugarcane ethanol, including \"zoning regulation of sugarcane expansion and social protocols\".\n\nBiofuels, in the form of liquid fuels derived from plant materials, are entering the market, driven by factors such as oil price spikes and the need for increased energy security. However, many of these first-generation biofuels that are currently being supplied have been criticised for their adverse impacts on the natural environment, food security, and land use.\n\nThe challenge is to support second, third and fourth-generation biofuel development.\nSecond-generation biofuels include new cellulosic technologies, with responsible policies and economic instruments to help ensure that biofuel commercialization is sustainable. Responsible commercialization of biofuels represents an opportunity to enhance sustainable economic prospects in Africa, Latin America and Asia.\n\nBiofuels have a limited ability to replace fossil fuels and should not be regarded as a ‘silver bullet’ to deal with transport emissions. However, they offer the prospect of increased market competition and oil price moderation. A healthy supply of alternative energy sources will help to combat gasoline price spikes and reduce dependency on fossil fuels, especially in the transport sector. Using transportation fuels more efficiently is also an integral part of a sustainable transport strategy.\n\nBiofuel development and use is a complex issue because there are many biofuel options which are available. Biofuels, such as ethanol and biodiesel, are currently produced from the products of conventional food crops such as the starch, sugar and oil feedstocks from crops that include wheat, maize, sugar cane, palm oil and oilseed rape. Some researchers fear that a major switch to biofuels from such crops would create a direct competition with their use for food and animal feed, and claim that in some parts of the world the economic consequences are already visible, other researchers look at the land available and the enormous areas of idle and abandoned land and claim that there is room for a large proportion of biofuel also from conventional crops.\n\nSecond generation biofuels are now being produced from a much broader range of feedstocks including the cellulose in dedicated energy crops (perennial grasses such as switchgrass and Miscanthus giganteus), forestry materials, the co-products from food production, and domestic vegetable waste. Advances in the conversion processes will improve the sustainability of biofuels, through better efficiencies and reduced environmental impact of producing biofuels, from both existing food crops and from cellulosic sources.\n\nIn 2007, Ronald Oxburgh suggested in \"The Courier-Mail\" that production of biofuels could be either responsible or irresponsible and had several trade-offs: \"Produced responsibly they are a sustainable energy source that need not divert any land from growing food nor damage the environment; they can also help solve the problems of the waste generated by Western society; and they can create jobs for the poor where previously were none. Produced irresponsibly, they at best offer no climate benefit and, at worst, have detrimental social and environmental consequences. In other words, biofuels are pretty much like any other product. In 2008 the Nobel prize-winning chemist Paul J. Crutzen published findings that the release of nitrous oxide (NO) emissions in the production of biofuels means that they contribute more to global warming than the fossil fuels they replace.\n\nAccording to the Rocky Mountain Institute, sound biofuel production practices would not hamper food and fibre production, nor cause water or environmental problems, and would enhance soil fertility. The selection of land on which to grow the feedstocks is a critical component of the ability of biofuels to deliver sustainable solutions. A key consideration is the minimisation of biofuel competition for prime cropland.\n\nBiofuels are different from fossil fuels in regard to carbon emissions being short term, but are similar to fossil fuels in that biofuels contribute to air pollution. Raw biofuels burned to generate steam for heat and power, produces airborne carbon particulates, carbon monoxide and nitrous oxides. The WHO estimates 3.7 million premature deaths worldwide in 2012 due to air pollution.\n\nBrazil’s production of ethanol fuel from sugarcane dates back to the 1970s, as a governmental response to the 1973 oil crisis. Brazil is considered the biofuel industry leader and the world's first sustainable biofuels economy.</ref> In 2010 the U.S. Environmental Protection Agency designated Brazilian sugarcane ethanol as an advanced biofuel due to EPA's estimated 61% reduction of total life cycle greenhouse gas emissions, including direct indirect land use change emissions.\nBrazil sugarcane ethanol fuel program success and sustainability is based on the most efficient agricultural technology for sugarcane cultivation in the world, uses modern equipment and cheap sugar cane as feedstock, the residual cane-waste (bagasse) is used to process heat and power, which results in a very competitive price and also in a high energy balance (output energy/input energy), which varies from 8.3 for average conditions to 10.2 for best practice production.\n\nA report commissioned by the United Nations, based on a detailed review of published research up to mid-2009 as well as the input of independent experts world-wide, found that ethanol from sugar cane as produced in Brazil \"\"in some circumstances does better than just “zero emission”. If grown and processed correctly, it has negative emission, pulling CO2 out of the atmosphere, rather than adding it\". In contrast, the report found that U.S. use of maize for biofuel is less efficient, as sugarcane can lead to emissions reductions of between 70% and well over 100% when substituted for gasoline. Several other studies have shown that sugarcane-based ethanol reduces greenhouse gases by 86 to 90% if there is no significant land use change.\n\nIn another study commissioned by the Dutch government in 2006 to evaluate the sustainability of Brazilian bioethanol concluded that there is sufficient water to supply all foreseeable long-term water requirements for sugarcane and ethanol production. This evaluation also found that consumption of agrochemicals for sugar cane production is lower than in citric, corn, coffee and soybean cropping. The study found that development of resistant sugar cane varieties is a crucial aspect of disease and pest control and is one of the primary objectives of Brazil’s cane genetic improvement programs. Disease control is one of the main reasons for the replacement of a commercial variety of sugar cane.\n\nAnother concern is the fact that sugarcane fields are traditionally burned just before harvest to avoid harm to the workers, by removing the sharp leaves and killing snakes and other harmful animals, and also to fertilize the fields with ash. Mechanization will reduce pollution from burning fields and has higher productivity than people, and due to mechanization the number of temporary workers in the sugarcane plantations has already declined. By the 2008 harvest season, around 47% of the cane was collected with harvesting machines.\n\nRegarding the negative impacts of the potential direct and indirect effect of land use changes on carbon emissions, the study commissioned by the Dutch government concluded that \"it is very difficult to determine the indirect effects of further land use for sugar cane production (i.e. sugar cane replacing another crop like soy or citrus crops, which in turn causes additional soy plantations replacing pastures, which in turn may cause deforestation), and also not logical to attribute all these soil carbon losses to sugar cane\". The Brazilian agency Embrapa estimates that there is enough agricultural land available to increase at least 30 times the existing sugarcane plantation without endangering sensible ecosystems or taking land destined for food crops. Most future growth is expected to take place on abandoned pasture lands, as it has been the historical trend in São Paulo state. Also, productivity is expected to improve even further based on current biotechnology research, genetic improvement, and better agronomic practices, thus contributing to reduce land demand for future sugarcane cultures.\nAnother concern is the risk of clearing rain forests and other environmentally valuable land for sugarcane production, such as the Amazon rainforest, the Pantanal or the Cerrado. Embrapa has rebutted this concern explaining that 99.7% of sugarcane plantations are located at least 2,000 km from the Amazon, and expansion during the last 25 years took place in the Center-South region, also far away from the Amazon rainforest, the Pantanal or the Atlantic forest. In São Paulo state growth took place in abandoned pasture lands. The impact assessment commissioned by the Dutch government supported this argument.\n\nIn order to guarantee a sustainable development of ethanol production, in September 2009 the government issued by decree a countrywide agroecological land use zoning to restrict sugarcane growth in or near environmentally sensitive areas. According to the new criteria, 92.5% of the Brazilian territory is not suitable for sugarcane plantation. The government considers that the suitable areas are more than enough to meet the future demand for ethanol and sugar in the domestic and international markets foreseen for the next decades.\n\nRegarding the food vs fuel issue, a World Bank research report published on July 2008 found that \"Brazil's sugar-based ethanol did not push food prices appreciably higher\". This research paper also concluded that Brazil's sugar cane–based ethanol has not raised sugar prices significantly. An economic assessment report also published in July 2008 by the OECD agrees with the World Bank report regarding the negative effects of subsidies and trade restrictions, but found that the impact of biofuels on food prices are much smaller. A study by the Brazilian research unit of the Fundação Getúlio Vargas regarding the effects of biofuels on grain prices concluded that the major driver behind the 2007-2008 rise in food prices was speculative activity on futures markets under conditions of increased demand in a market with low grain stocks. The study also concluded that there is no correlation between Brazilian sugarcane cultivated area and average grain prices, as on the contrary, the spread of sugarcane was accompanied by rapid growth of grain crops in the country.\n\nCrops like Jatropha, used for biodiesel, can thrive on marginal agricultural land where many trees and crops won't grow, or would produce only slow growth yields. Jatropha cultivation provides benefits for local communities: \n\nCultivation and fruit picking by hand is labour-intensive and needs around one person per hectare. In parts of rural India and Africa this provides much-needed jobs - about 200,000 people worldwide now find employment through jatropha. Moreover, villagers often find that they can grow other crops in the shade of the trees. Their communities will avoid importing expensive diesel and there will be some for export too. \n\nCambodia has no proven fossil fuel reserves, and is almost completely dependent on imported diesel fuel for electricity production. Consequently, Cambodians face an insecure supply and pay some of the highest energy prices in the world. The impacts of this are widespread and may hinder economic development.\n\nBiofuels may provide a substitute for diesel fuel that can be manufactured locally for a lower price, independent of the international oil price. The local production and use of biofuel also offers other benefits such as improved energy security, rural development opportunities and environmental benefits. The Jatropha curcas species appears to be a particularly suitable source of biofuel as it already grows commonly in Cambodia. Local sustainable production of biofuel in Cambodia, based on the Jatropha or other sources, offers good potential benefits for the investors, the economy, rural communities and the environment.\n\nJatropha is native to Mexico and Central America and was likely transported to India and Africa in the 1500s by Portuguese sailors convinced it had medicinal uses. In 2008, recognizing the need to diversify its sources of energy and reduce emissions, Mexico passed a law to push developing biofuels that don't threaten food security and the agriculture ministry has since identified some 2.6 million hectares (6.4 million acres) of land with a high potential to produce jatropha. The Yucatán Peninsula, for instance, in addition to being a corn producing region, also contains abandoned sisal plantations, where the growing of Jatropha for biodiesel production would not displace food.\n\nOn April 1, 2011 Interjet completed the first Mexican aviation biofuels test flight on an Airbus A320. The fuel was a 70:30 traditional jet fuel biojet blend produced from Jatropha oil provided by three Mexican producers, Global Energías Renovables (a wholly owned subsidiary of U.S.-based Global Clean Energy Holdings, Bencafser S.A. and Energy JH S.A. Honeywell's UOP processed the oil into Bio-SPK (Synthetic Paraffinic Kerosene) . Global Energías Renovables operates the largest Jatropha farm in the Americas.\n\nOn August 1, 2011 Aeromexico, Boeing, and the Mexican Government participated in the first biojet powered transcontinental flight in aviation history. The flight from Mexico City to Madrid used a blend of 70 percent traditional fuel and 30 percent biofuel (aviation biofuel). The biojet was produced entirely from Jatropha oil.\n\nPongamia pinnata is a legume native to Australia, India, Florida (USA) and most tropical regions, and is now being invested in as an alternative to Jatropha for areas such as Northern Australia, where Jatropha is classed as a noxious weed.\nCommonly known as simply 'Pongamia', this tree is currently being commercialised in Australia by Pacific Renewable Energy, for use as a Diesel replacement for running in modified Diesel engines or for conversion to Biodiesel using 1st or 2nd Generation Biodiesel techniques, for running in unmodified Diesel engines.\n\nSweet sorghum overcomes many of the shortcomings of other biofuel crops. With sweet sorghum, only the stalks are used for biofuel production, while the grain is saved for food or livestock feed. It is not in high demand in the global food market, and thus has little impact on food prices and food security. Sweet sorghum is grown on already-farmed drylands that are low in carbon storage capacity, so concerns about the clearing of rainforest do not apply. Sweet sorghum is easier and cheaper to grow than other biofuel crops in India and does not require irrigation, an important consideration in dry areas. Some of the Indian sweet sorghum varieties are now grown in Uganda for ethanol production.\n\nA study by researchers at the International Crops Research Institute for the Semi-Arid Tropics (ICRISAT) found that growing sweet sorghum instead of grain sorghum could increase farmers incomes by US$40 per hectare per crop because it can provide food, feed and fuel. With grain sorghum currently grown on over 11 million hectares (ha) in Asia and on 23.4 million ha in Africa, a switch to sweet sorghum could have a considerable economic impact.\n\nPublic attitudes and the actions of key stakeholders can play a crucial role in realising the potential of sustainable biofuels. Informed discussion and dialogue, based both on scientific research and an understanding of public and stakeholder views, is important.\n\nThe Roundtable on Sustainable Biofuels is an international initiative which brings together farmers, companies, governments, non-governmental organizations, and scientists who are interested in the sustainability of biofuels production and distribution. During 2008, the Roundtable used meetings, teleconferences, and online discussions to develop a series of principles and criteria for sustainable biofuels production.\n\nIn April 2011, the Roundtable on Sustainable Biofuels launched a set of comprehensive sustainability criteria - the “RSB Certification System.” Biofuels producers that meet to these criteria are able to show buyers and regulators that their product has been obtained without harming the environment or violating human rights.\n\nThe Sustainable Biofuels Consensus is an international initiative which calls upon governments, the private sector, and other stakeholders to take decisive action to ensure the sustainable trade, production, and use of biofuels. In this way biofuels may play a key role in energy sector transformation, climate stabilization, and resulting worldwide revitalisation of rural areas.\n\nThe Sustainable Biofuels Consensus envisions a \"landscape that provides food, fodder, fiber, and energy, which offers opportunities for rural development; that diversifies energy supply, restores ecosystems, protects biodiversity, and sequesters carbon\".\n\nIn 2008, a multi-stakeholder process was initiated by the World Wildlife Fund and the International Finance Corporation, the private development arm of the World Bank, bringing together industry, supply chain intermediaries, end-users, farmers and civil society organisations to develop standards for certifying the derivative products of sugar cane, one of which is ethanol fuel.\n\nThe Bonsucro standard is based around a definition of sustainability which is founded on five principles:\n\nBiofuel producers that wish to sell products marked with the Bonsucro standard must both ensure that they product to the Production Standard, and that their downstream buyers meet the Chain of Custody Standard. In addition, if they wish to sell to the European market and count against the EU Renewable Energy Directive, then they must adhere to the Bonsucro EU standard, which includes specific greenhouse gas calculations following European Commission calculation guidelines.\n\nBiofuels offer the prospect of real market competition and oil price moderation. According to the Wall Street Journal, crude oil would be trading 15 per cent higher and gasoline would be as much as 25 per cent more expensive, if it were not for biofuels. A healthy supply of alternative energy sources will help to combat gasoline price spikes.\n\nBiofuels have a limited ability to replace fossil fuels and should not be regarded as a ‘silver bullet’ to deal with transport emissions. Biofuels on their own cannot deliver a sustainable transport system and so must be developed as part of an integrated approach, which promotes other renewable energy options and energy efficiency, as well as reducing the overall energy demand and need for transport. Consideration needs to be given to the development of hybrid and fuel cell vehicles, public transport, and better town and rural planning.\n\nIn December 2008 an Air New Zealand jet completed the world's first commercial aviation test flight partially using jatropha-based fuel. More than a dozen performance tests were undertaken in the two-hour test flight which departed from Auckland International Airport. A biofuel blend of 50:50 jatropha and Jet A1 fuel was used to power one of the Boeing 747-400's Rolls-Royce RB211 engines. Air New Zealand set several criteria for its jatropha, requiring that \"the land it came from was neither forest nor virgin grassland in the previous 20 years, that the soil and climate it came from is not suitable for the majority of food crops and that the farms are rain fed and not mechanically irrigated\". The company has also set general sustainability criteria, saying that such biofuels must not compete with food resources, that they must be as good as traditional jet fuels, and that they should be cost competitive.\n\nIn January 2009, Continental Airlines used a sustainable biofuel to power a commercial aircraft for the first time in North America. This demonstration flight marks the first sustainable biofuel demonstration flight by a commercial carrier using a twin-engined aircraft, a Boeing 737-800, powered by CFM International CFM56-7B engines. The biofuel blend included components derived from algae and jatropha plants. The algae oil was provided by Sapphire Energy, and the jatropha oil by Terasol Energy.\n\nIn March 2011, Yale University research showed significant potential for sustainable aviation fuel based on jatropha-curcas. According to the research, if cultivated properly, \"jatropha can deliver many benefits in Latin America and greenhouse gas reductions of up to 60 percent when compared to petroleum-based jet fuel\". Actual farming conditions in Latin America were assessed using sustainability criteria developed by the Roundtable on Sustainable Biofuels. Unlike previous research, which used theoretical inputs, the Yale team conducted many interviews with jatropha farmers and used \"field measurements to develop the first comprehensive sustainability analysis of actual projects\".\n\nAs of June 2011, revised international aviation fuel standards officially allow commercial airlines to blend conventional jet fuel with up to 50 percent biofuels. The renewable fuels \"can be blended with conventional commercial and military jet fuel through requirements in the newly issued edition of ASTM D7566, Specification for Aviation Turbine Fuel Containing Synthesized Hydrocarbons\".\n\nIn December 2011, the FAA awarded $7.7 million to eight companies to advance the development of commercial aviation biofuels, with a special focus on alcohol to jet fuel. The FAA is assisting in the development of a sustainable fuel (from alcohols, sugars, biomass, and organic matter such as pyrolysis oils) that can be “dropped in” to aircraft without changing current practices and infrastructure. The research will test how the new fuels affect engine durability and quality control standards.\n\nGreenSky London, a biofuels plant under construction in 2014, aimed to take in some 500,000 tonnes of municipal rubbish and change the organic component into 60,000 tonnes of jet fuel, and 40 megawatts of power. By the end of 2015, it was hoped all British Airways flights from London City Airport will be fuelled by waste and rubbish discarded by London residents, leading to carbon savings equivalent to taking 150,000 cars off the road. Unfortunately, the £340m scheme was mothballed in January 2016 following low crude oil prices, jittery investors and a lack of support from the UK government.\n\n\n"}
{"id": "724433", "url": "https://en.wikipedia.org/wiki?curid=724433", "title": "Trikke", "text": "Trikke\n\nThe Trikke ( ) is a chain-less, pedal-less, personal vehicle with a three-wheel frame. The rider stands on two foot platforms above the two rear wheels and steers the vehicle with handlebars attached to the lone front wheel. The patented cambering system is designed to provide a stable, three-point platform that lets the rider lean into turns while all three wheels remain in contact with the ground. There are several variations of the Trikke, from body-powered fitness machines, to battery-powered transportation and personal mobility vehicles. There are also Trikkes for children, as well as a version for snow-covered mountains called the Trikke Skki.\n\nAll Trikkes are designed and manufactured exclusively by Trikke Tech, Inc., based in Buellton, California.\n\nA body-powered Trikke is propelled using a motion that moves the vehicle along a curved, S-shaped path (called “carving”). The rider moves the vehicle from side to side, turning the handlebars and leaning the front structure while moving one’s body weight toward the side one is turning into. The weight of the rider during a turn (or “carve”) is placed mainly on the foot at the outside of the turn. The inside foot will support very little weight and the rider will often lift the heel of this foot during the turn.\n\nThe resulting motion is similar to slalom skiing, a tic-tac move on a skateboard, or one’s leg movements during roller skating.\n\nA Trikke can be propelled without the need to push off with one’s foot. There are many variations of Trikke-riding techniques as riders place different emphasis on the elements of the movement.\n\nUsing the carving motion, a body-powered Trikke can achieve speeds of nearly 30 km/h (19 mph), but more often, riders cruise around with average speeds in the 15 km/h (9.3 mph) range. A Trikke can be ridden uphill, albeit at a reduced speed.\n\nSince 2009, Trikke Tech, Inc. has manufactured electric versions of the Trikke, which have been adopted by various law enforcement and security companies as an alternative to other personal electric vehicles. Additionally, many recreational riders now use electric Trikkes for personal transportation as well as hybrid fitness vehicles.\nWhile similar three-wheel vehicles may have existed prior to the Trikke, the Trikke carving vehicle is the result of years of experimentation and development by Brazilian engineer Gildo Beleski. Starting in the late 1980s, Beleski began seeking different, more efficient ways for traveling downhill on early Trikke prototypes. That eventually lead to the revelation that his carving vehicles could also be used on flat surfaces and that they possessed the potential to provide a much more strenuous yet safe alternative to bikes, skateboards and other scooters.\n\nThroughout the 1990s, Beleski continued to refine his design concepts, experimenting with geometric changes to the frame and wheels while adding features to improve body propulsion and the transfer of energy and movement from the rider to the road through the frame.\n\nSimultaneously, the native Brazilian was working in the automotive industry and started his own business servicing high end cars. He also raced autos for fun and learned more about sheer performance and the importance of good handling.\n\nBy the late 90s, Beleski’s business brought him to the United States for several auto shows, and during one visit, a friend took him to Miami’s South Beach. There he was mesmerized by the sight of so many people enjoying recreation on wheels, be they in-line skates, skateboards or bicycles. Immediately he saw the potential for Trikkes.\n\nAfter some soul searching, Beleski decided to dedicate his life to designing, developing and riding Trikkes as an entrepreneur in America. In 1999, he applied for a patent on an improved design for a three-wheel cambering vehicle with the US Patent Office. In 2000, he founded Trikke Tech, Inc., in Southern California, shipping 100 frames from Brazil and assembling Trikkes in his garage. Next, he took to the streets and begin demonstrating his Trikkes while talking to people and making friends.\n\nTrikke three-wheel carving vehicles officially went on the market in 2002, and Time magazine named it one of the best inventions of that year.\n\nSince then, over half a million body-powered Trikkes have been sold and Trikke riding groups have sprung up in the United States and many other countries.\n\nOver the years, Trikke carving vehicles have been featured by numerous media outlets, including People magazine, Playboy, Men’s Health Magazine, the Los Angeles Times and the TV shows “Extra” and “The View.”\n\nAdditionally, many celebrities and public figures have been photographed riding Trikkes, including former President Jimmy Carter, NFL quarterback Cam Newton and actors Jennifer Aniston, Jim Belushi and Tori Spelling.\n"}
{"id": "30046", "url": "https://en.wikipedia.org/wiki?curid=30046", "title": "Tungsten", "text": "Tungsten\n\nTungsten, or wolfram, is a chemical element with symbol W and atomic number 74. The name \"tungsten\" comes from the former Swedish name for the tungstate mineral \"scheelite\", \"tung sten\" or \"heavy stone\". Tungsten is a rare metal found naturally on Earth almost exclusively combined with other elements in chemical compounds rather than alone. It was identified as a new element in 1781 and first isolated as a metal in 1783. Its important ores include wolframite and scheelite.\n\nThe free element is remarkable for its robustness, especially the fact that it has the highest melting point of all the elements discovered, melting at 3422 °C (6192 °F, 3695 K). It also has the highest boiling point, at 5930 °C (10706 °F, 6203 K). Its density is 19.3 times that of water, comparable to that of uranium and gold, and much higher (about 1.7 times) than that of lead. Polycrystalline tungsten is an intrinsically brittle and hard material (under standard conditions, when uncombined), making it difficult to work. However, pure single-crystalline tungsten is more ductile and can be cut with a hard-steel hacksaw.\n\nTungsten's many alloys have numerous applications, including incandescent light bulb filaments, X-ray tubes (as both the filament and target), electrodes in gas tungsten arc welding, superalloys, and radiation shielding. Tungsten's hardness and high density give it military applications in penetrating projectiles. Tungsten compounds are also often used as industrial catalysts.\n\nTungsten is the only metal from the third transition series that is known to occur in biomolecules that are found in a few species of bacteria and archaea. It is the heaviest element known to be essential to any living organism. Tungsten interferes with molybdenum and copper metabolism and is somewhat toxic to animal life.\n\nIn its raw form, tungsten is a hard steel-grey metal that is often brittle and hard to work. If made very pure, tungsten retains its hardness (which exceeds that of many steels), and becomes malleable enough that it can be worked easily. It is worked by forging, drawing, or extruding. Tungsten objects are also commonly formed by sintering.\n\nOf all metals in pure form, tungsten has the highest melting point (3422 °C, 6192 °F), lowest vapor pressure (at temperatures above 1650 °C, 3000 °F), and the highest tensile strength. Although carbon remains solid at higher temperatures than tungsten, carbon sublimes at atmospheric pressure instead of melting, so it has no melting point. Tungsten has the lowest coefficient of thermal expansion of any pure metal. The low thermal expansion and high melting point and tensile strength of tungsten originate from strong covalent bonds formed between tungsten atoms by the 5d electrons.\nAlloying small quantities of tungsten with steel greatly increases its toughness.\n\nTungsten exists in two major crystalline forms: α and β. The former has a body-centered cubic structure and is the more stable form. The structure of the β phase is called A15 cubic; it is metastable, but can coexist with the α phase at ambient conditions owing to non-equilibrium synthesis or stabilization by impurities. Contrary to the α phase which crystallizes in isometric grains, the β form exhibits a columnar habit. The α phase has one third of the electrical resistivity and a much lower superconducting transition temperature T relative to the β phase: ca. 0.015 K vs. 1–4 K; mixing the two phases allows obtaining intermediate T values. The T value can also be raised by alloying tungsten with another metal (e.g. 7.9 K for W-Tc). Such tungsten alloys are sometimes used in low-temperature superconducting circuits.\n\nNaturally occurring tungsten consists of five isotopes whose half-lives are so long that they can be considered stable. Theoretically, all five can decay into isotopes of element 72 (hafnium) by alpha emission, but only W has been observed to do so with a half-life of years; on average, this yields about two alpha decays of W per gram of natural tungsten per year. The other naturally occurring isotopes have not been observed to decay, constraining their half-lives to be at least 4 × 10 years.\n\nAnother 30 artificial radioisotopes of tungsten have been characterized, the most stable of which are W with a half-life of 121.2 days, W with a half-life of 75.1 days, W with a half-life of 69.4 days, W with a half-life of 21.6 days, and W with a half-life of 23.72 h. All of the remaining radioactive isotopes have half-lives of less than 3 hours, and most of these have half-lives below 8 minutes. Tungsten also has 4 meta states, the most stable being W (\"t\" 6.4 minutes).\n\nElemental tungsten resists attack by oxygen, acids, and alkalis.\n\nThe most common formal oxidation state of tungsten is +6, but it exhibits all oxidation states from −2 to +6. Tungsten typically combines with oxygen to form the yellow tungstic oxide, WO, which dissolves in aqueous alkaline solutions to form tungstate ions, .\n\nTungsten carbides (WC and WC) are produced by heating powdered tungsten with carbon. WC is resistant to chemical attack, although it reacts strongly with chlorine to form tungsten hexachloride (WCl).\n\nIn aqueous solution, tungstate gives the heteropoly acids and polyoxometalate anions under neutral and acidic conditions. As tungstate is progressively treated with acid, it first yields the soluble, metastable \"paratungstate A\" anion, , which over time converts to the less soluble \"paratungstate B\" anion, . Further acidification produces the very soluble metatungstate anion, , after which equilibrium is reached. The metatungstate ion exists as a symmetric cluster of twelve tungsten-oxygen octahedra known as the Keggin anion. Many other polyoxometalate anions exist as metastable species. The inclusion of a different atom such as phosphorus in place of the two central hydrogens in metatungstate produces a wide variety of heteropoly acids, such as phosphotungstic acid HPWO.\n\nTungsten trioxide can form intercalation compounds with alkali metals. These are known as \"bronzes\"; an example is sodium tungsten bronze.\n\nIn 1781, Carl Wilhelm Scheele discovered that a new acid, tungstic acid, could be made from scheelite (at the time named tungsten). Scheele and Torbern Bergman suggested that it might be possible to obtain a new metal by reducing this acid. In 1783, José and Fausto Elhuyar found an acid made from wolframite that was identical to tungstic acid. Later that year, at the Royal Basque Society in the town of Bergara, Spain, the brothers succeeded in isolating tungsten by reduction of this acid with charcoal, and they are credited with the discovery of the element.\n\nThe strategic value of tungsten came to notice in the early 20th century. British authorities acted in 1912 to free the Carrock mine from the German owned Cumbrian Mining Company and, during World War I, restrict German access elsewhere. In World War II, tungsten played a more significant role in background political dealings. Portugal, as the main European source of the element, was put under pressure from both sides, because of its deposits of wolframite ore at Panasqueira. Tungsten's desirable properties such as resistance to high temperatures, its hardness and density, and its strengthening of alloys made it an important raw material for the arms industry, both as a constituent of weapons and equipment and employed in production itself, e.g., in tungsten carbide cutting tools for machining steel.\n\nThe name \"tungsten\" (from the Swedish \"tung sten\", \"heavy stone\") is used in English, French, and many other languages as the name of the element, but not in the Nordic countries. \"Tungsten\" was the old Swedish name for the mineral scheelite. \"Wolfram\" (or \"volfram\") is used in most European (especially Germanic and Slavic) languages and is derived from the mineral wolframite, which is the origin of the chemical symbol W. The name \"wolframite\" is derived from German \"wolf rahm\" (\"wolf soot\" or \"wolf cream\"), the name given to tungsten by Johan Gottschalk Wallerius in 1747. This, in turn, derives from \"lupi spuma\", the name Georg Agricola used for the element in 1546, which translates into English as \"wolf's froth\" and is a reference to the large amounts of tin consumed by the mineral during its extraction.\n\nTungsten is found mainly in the minerals wolframite (iron–manganese tungstate (Fe,Mn)WO, which is a solid solution of the two minerals ferberite FeWO, and hübnerite MnWO) and scheelite (calcium tungstate (CaWO). Other tungsten minerals range in their level of abundance from moderate to very rare, and have almost no economical value.\n\nTungsten forms chemical compounds in oxidation states from -II to VI. Higher oxidation states, always as oxides, are relevant to its terrestrial occurrence and its biological roles, mid-level oxidation states are often associated with metal clusters, and very low oxidation states are typically associated with CO complexes. Mo and W chemistry shows strong similarities. The relative rarity of tungsten(III), for example, contrasts with the pervasiveness of the chromium(III) compounds. The highest oxidation state is seen in tungsten(VI) oxide (WO). The trioxide, which is volatile at high temperatures, is the precursor to virtually all other Mo compounds as well as alloys. Molybdenum has several oxidation states, the most stable being +4 and +6 (bolded in the table at left).\n\nTungsten(VI) oxide is soluble in aqueous base , forming tungstate (WO). This oxyanion condenses at lower pH values, forming polyoxotungstates. \n\nThe broad range of oxidation states of tungsten is reflected in it various chlorides:\n\nOrganotungsten compounds are numerous and also span a range of oxidation states. Notable examples include the trigonal prismatic W(CH) and octahedral W(CO).\n\nAbout 61,300 tonnes of tungsten concentrates were produced in the year 2009, and in 2010, world production of tungsten was about 68,000 tonnes. The main producers were as follows (data in tonnes):\n\nThere is additional production in the U.S., but the amount is proprietary company information. U.S. reserves are 140,000 tonnes. US industrial use of wolfram is 20,000 tonnes: 15,000 tonnes are imported and the remaining 5,000 tonnes come from domestic recycling.\n\nTungsten is considered to be a conflict mineral due to the unethical mining practices observed in the Democratic Republic of the Congo.\n\nThere is a large deposit of tungsten ore on the edge of Dartmoor in the United Kingdom, which was exploited during World War I and World War II as the Hemerdon Mine. With recent increases in tungsten prices, as of 2014 this mine has been reactivated.\n\nTungsten is extracted from its ores in several stages. The ore is eventually converted to tungsten(VI) oxide (WO), which is heated with hydrogen or carbon to produce powdered tungsten. Because of tungsten's high melting point, it is not commercially feasible to cast tungsten ingots. Instead, powdered tungsten is mixed with small amounts of powdered nickel or other metals, and sintered. During the sintering process, the nickel diffuses into the tungsten, producing an alloy.\n\nTungsten can also be extracted by hydrogen reduction of WF:\n\nor pyrolytic decomposition:\n\nTungsten is not traded as a futures contract and cannot be tracked on exchanges like the London Metal Exchange. The prices are usually quoted for tungsten concentrate or WO. If converted to the metal equivalent, they were about US$19 per kilogram in 2009.\n\nApproximately half of the tungsten is consumed for the production of hard materials – namely tungsten carbide – with the remaining major use being in alloys and steels. Less than 10% is used in other chemical compounds.\n\nTungsten is mainly used in the production of hard materials based on tungsten carbide, one of the hardest carbides, with a melting point of 2770 °C. WC is an efficient electrical conductor, but WC is less so. WC is used to make wear-resistant abrasives, and \"carbide\" cutting tools such as knives, drills, circular saws, milling and turning tools used by the metalworking, woodworking, mining, petroleum and construction industries. Carbide tooling is actually a ceramic/metal composite, where metallic cobalt acts as a binding (matrix) material to hold the WC particles in place. This type of industrial use accounts for about 60% of current tungsten consumption.\n\nThe jewelry industry makes rings of sintered tungsten carbide, tungsten carbide/metal composites, and also metallic tungsten. WC/metal composite rings use nickel as the metal matrix in place of cobalt because it takes a higher luster when polished. Sometimes manufacturers or retailers refer to tungsten carbide as a metal, but it is a ceramic. Because of tungsten carbide's hardness, rings made of this material are extremely abrasion resistant, and will hold a burnished finish longer than rings made of metallic tungsten. Tungsten carbide rings are brittle, however, and may crack under a sharp blow.\n\nThe hardness and density of tungsten are applied in obtaining heavy metal alloys. A good example is high speed steel, which can contain as much as 18% tungsten. Tungsten's high melting point makes tungsten a good material for applications like rocket nozzles, for example in the UGM-27 Polaris submarine-launched ballistic missile. Tungsten alloys are used in a wide range of different applications, including the aerospace and automotive industries and radiation shielding. Superalloys containing tungsten, such as Hastelloy and Stellite, are used in turbine blades and wear-resistant parts and coatings.\n\nQuenched (martensitic) tungsten steel (approx. 5.5% to 7.0% W with 0.5% to 0.7% C) was used for making hard permanent magnets, due to its high remanence and coercivity, as noted by John Hopkinson (1849 - 1898) as early as 1886. The magnetic properties of a metal or an alloy are very sensitive to microstructure. For example, while the element tungsten is not ferromagnetic (but iron is), when present in steel in these proportions, it stabilizes the martensite phase, which has an enhanced ferromagnetism, as compared to the ferrite (iron) phase, due to its greater resistance to magnetic domain wall motion.\n\nTungsten's heat resistance makes it useful in arc welding applications when combined with another highly-conductive metal such as silver or copper. The silver or copper provides the necessary conductivity and the tungsten allows the welding rod to withstand the high-temperatures of the arc welding environment.\n\nMallory metal is proprietary name for an alloy of tungsten, with other metallic elements added to improve machining.\n\nTungsten, usually alloyed with nickel and iron or cobalt to form heavy alloys, is used in kinetic energy penetrators as an alternative to depleted uranium, in applications where uranium's radioactivity is problematic even in depleted form, or where uranium's additional pyrophoric properties are not required (for example, in ordinary small arms bullets designed to penetrate body armor). Similarly, tungsten alloys have also been used in cannon shells, grenades and missiles, to create supersonic shrapnel. Germany used tungsten during World War II to produce shells for anti-tank gun designs using the Gerlich squeeze bore principle to achieve very high muzzle velocity and enhanced armor penetration from comparatively small caliber and light weight field artillery. The weapons were highly effective but a shortage of tungsten used in the shell core limited that effectiveness.\n\nTungsten has also been used in Dense Inert Metal Explosives, which use it as dense powder to reduce collateral damage while increasing the lethality of explosives within a small radius.\n\nTungsten(IV) sulfide is a high temperature lubricant and is a component of catalysts for hydrodesulfurization. MoS is more commonly used for such applications.\n\nTungsten oxides are used in ceramic glazes and calcium/magnesium tungstates are used widely in fluorescent lighting. Crystal tungstates are used as scintillation detectors in nuclear physics and nuclear medicine. Other salts that contain tungsten are used in the chemical and tanning industries.\nTungsten oxide (WO) is incorporated into selective catalytic reduction (SCR) catalysts found in coal-fired power plants. These catalysts convert nitrogen oxides (NO) to nitrogen (N) and water (HO) using ammonia (NH). The tungsten oxide helps with the physical strength of the catalyst and extends catalyst life.\n\nApplications requiring its high density include weights, counterweights, ballast keels for yachts, tail ballast for commercial aircraft, and as ballast in race cars for NASCAR and Formula One; depleted uranium is also used for these purposes, due to similarly high density. Seventy-five-kg blocks of tungsten were used as \"cruise balance mass devices\" on the entry vehicle portion of the 2012 Mars Science Laboratory spacecraft. It is an ideal material to use as a dolly for riveting, where the mass necessary for good results can be achieved in a compact bar. High-density alloys of tungsten with nickel, copper or iron are used in high-quality darts (to allow for a smaller diameter and thus tighter groupings) or for fishing lures (tungsten beads allow the fly to sink rapidly). Tungsten has seen use recently in nozzles for 3D printing; the high wear resistance and thermal conductivity of tungsten carbide improves the printing of abrasive filaments. Some cello C strings are wound with tungsten. The extra density gives this string more projection and often cellists will buy just this string and use it with three strings from a different set. Tungsten is used as an absorber on the electron telescope on the Cosmic Ray System of the two Voyager spacecraft.\n\nSodium tungstate is used in Folin-Ciocalteu's reagent, a mixture of different chemicals used in the \"Lowry Assay\" for protein content analysis.\n\nIts density, similar to that of gold, allows tungsten to be used in jewelry as an alternative to gold or platinum. Metallic tungsten is hypoallergenic, and is harder than gold alloys (though not as hard as tungsten carbide), making it useful for rings that will resist scratching, especially in designs with a brushed finish.\n\nBecause the density is so similar to that of gold (tungsten is only 0.36% less dense), tungsten can also be used in counterfeiting of gold bars, such as by plating a tungsten bar with gold, which has been observed since the 1980s, or taking an existing gold bar, drilling holes, and replacing the removed gold with tungsten rods. The densities are not exactly the same, and other properties of gold and tungsten differ, but gold-plated tungsten will pass superficial tests.\n\nGold-plated tungsten is available commercially from China (the main source of tungsten), both in jewelry and as bars.\n\nBecause it retains its strength at high temperatures and has a high melting point, elemental tungsten is used in many high-temperature applications, such as light bulb, cathode-ray tube, and vacuum tube filaments, heating elements, and rocket engine nozzles. Its high melting point also makes tungsten suitable for aerospace and high-temperature uses such as electrical, heating, and welding applications, notably in the gas tungsten arc welding process (also called tungsten inert gas (TIG) welding).\nBecause of its conductive properties and relative chemical inertness, tungsten is also used in electrodes, and in the emitter tips in electron-beam instruments that use field emission guns, such as electron microscopes. In electronics, tungsten is used as an interconnect material in integrated circuits, between the silicon dioxide dielectric material and the transistors. It is used in metallic films, which replace the wiring used in conventional electronics with a coat of tungsten (or molybdenum) on silicon.\n\nThe electronic structure of tungsten makes it one of the main sources for X-ray targets, and also for shielding from high-energy radiations (such as in the radiopharmaceutical industry for shielding radioactive samples of FDG). It is also used in gamma imaging as a material from which coded apertures are made, due to its excellent shielding properties. Tungsten powder is used as a filler material in plastic composites, which are used as a nontoxic substitute for lead in bullets, shot, and radiation shields. Since this element's thermal expansion is similar to borosilicate glass, it is used for making glass-to-metal seals. In addition to its high melting point, when tungsten is doped with potassium, it leads to an increased shape stability (compared to non-doped tungsten). This ensures that the filament does not sag, and no undesired changes occur.\n\nThrough top-down nanofabrication processes, tungsten nanowires have been fabricated and studied since 2002. Due to a particularly high surface to volume ratio, the formation of a surface oxide layer and the single crystal nature of such material, the mechanical properties differ fundamentally from those of bulk tungsten. Such tungsten nanowires have potential applications in nanoelectronics and importantly as pH probes and gas sensors. In similarity to silicon nanowires, tungsten nanowires are frequently produced from a bulk tungsten precursor followed by a thermal oxidation step to control morphology in terms of length and aspect ratio. Using the Deal–Grove model it is possible to predict the oxidation kinetics of nanowires fabricated through such thermal oxidation processing.\n\nTungsten, at atomic number \"Z\" = 74, is the heaviest element known to be biologically functional. It is used by some bacteria and archaea, but not in eukaryotes. For example, enzymes called oxidoreductases use tungsten similarly to molybdenum by using it in a tungsten-pterin complex with molybdopterin (molybdopterin, despite its name, does not contain molybdenum, but may complex with either molybdenum or tungsten in use by living organisms). Tungsten-using enzymes typically reduce carboxylic acids to aldehydes. The tungsten oxidoreductases may also catalyse oxidations. The first tungsten-requiring enzyme to be discovered also requires selenium, and in this case the tungsten-selenium pair may function analogously to the molybdenum-sulfur pairing of some molybdenum cofactor-requiring enzymes. One of the enzymes in the oxidoreductase family which sometimes employ tungsten (bacterial formate dehydrogenase H) is known to use a selenium-molybdenum version of molybdopterin. Acetylene hydratase is an unusual metalloenzyme in that it catalyzes a hydration reaction. Two reaction mechanisms have been proposed, in one of which there is a direct interaction between the tungsten atom and the C≡C triple bond. Although a tungsten-containing xanthine dehydrogenase from bacteria has been found to contain tungsten-molydopterin and also non-protein bound selenium, a tungsten-selenium molybdopterin complex has not been definitively described.\n\nIn soil, tungsten metal oxidizes to the tungstate anion. It can be selectively or non-selectively imported by some prokaryotic organisms and may substitute for molybdate in certain enzymes. Its effect on the action of these enzymes is in some cases inhibitory and in others positive. The soil's chemistry determines how the tungsten polymerizes; alkaline soils cause monomeric tungstates; acidic soils cause polymeric tungstates.\n\nSodium tungstate and lead have been studied for their effect on earthworms. Lead was found to be lethal at low levels and sodium tungstate was much less toxic, but the tungstate completely inhibited their reproductive ability.\n\nTungsten has been studied as a biological copper metabolic antagonist, in a role similar to the action of molybdenum. It has been found that tetrathiotungstates may be used as biological copper chelation chemicals, similar to the tetrathiomolybdates.\nTungsten is essential for some archaea. The following tungsten-utilizing enzymes are known:\nA \"wtp\" system is known to selectively transport tungsten in archaea:\n\nBecause tungsten is rare and its compounds are generally inert, the effects of tungsten on the environment are limited.\nIt was at first believed to be relatively inert and an only slightly toxic metal, but beginning in the year 2000, the risk presented by tungsten alloys, its dusts and particulates to induce cancer and several other adverse effects in animals as well as humans has been highlighted from in vitro and in vivo experiments.\nThe median lethal dose LD depends strongly on the animal and the method of administration and varies between 59 mg/kg (intravenous, rabbits) and 5000 mg/kg (tungsten metal powder, intraperitoneal, rats).\n\nPeople can be exposed to tungsten in the workplace by breathing it in, swallowing it, skin contact, and eye contact. The National Institute for Occupational Safety and Health (NIOSH) has set a recommended exposure limit (REL) of 5 mg/m over an 8-hour workday and a short term limit of 10 mg/m.\n\nTungsten is unique amongst the elements in that it has been the subject of patent proceedings. In 1928, a US court rejected General Electric's attempt to patent it, overturning granted in 1913 to William D. Coolidge.\n\n"}
{"id": "2549117", "url": "https://en.wikipedia.org/wiki?curid=2549117", "title": "Variable capacitor", "text": "Variable capacitor\n\nA variable capacitor is a capacitor whose capacitance may be intentionally and repeatedly changed mechanically or electronically. Variable capacitors are often used in L/C circuits to set the resonance frequency, e.g. to tune a radio (therefore it is sometimes called a tuning capacitor or tuning condenser), or as a variable reactance, e.g. for impedance matching in antenna tuners.\n\nIn mechanically controlled variable capacitors, the distance between the plates, or the amount of plate surface area which overlaps, can be changed.\n\nThe most common form arranges a group of semicircular metal plates on a rotary axis (\"rotor\") that are positioned in the gaps between a set of stationary plates (\"stator\") so that the area of overlap can be changed by rotating the axis. Air or plastic foils can be used as dielectric material.\nBy choosing the shape of the rotary plates, various functions of capacitance vs. angle can be created, e.g. to obtain a linear frequency scale. Various forms of reduction gear mechanisms are often used to achieve finer tuning control, i.e. to spread the variation of capacity over a larger angle, often several turns.\n\nA vacuum variable capacitor uses a set of plates made from concentric cylinders that can be slid in or out of an opposing set of cylinders (sleeve and plunger). These plates are then sealed inside of a non-conductive envelope such as glass or ceramic and placed under a high vacuum. The movable part (plunger) is mounted on a flexible metal membrane that seals and maintains the vacuum. A screw shaft is attached to the plunger, when the shaft is turned the plunger moves in or out of the sleeve and the value of the capacitor changes. The vacuum not only increases the working voltage and current handling capacity of the capacitor, it also greatly reduces the chance of arcing across the plates. The most common usage for vacuum variables are in high-powered transmitters such as those used for broadcasting, military and amateur radio, as well as high-powered RF tuning networks. Vacuum variables can also be more convenient; since the elements are under a vacuum, the working voltage can be higher than an air variable the same size, allowing the size of the vacuum capacitor to be reduced.\n\nVery cheap variable capacitors are constructed from layered aluminium and plastic foils that are variably pressed together using a screw. These so-called \"squeezers\" cannot provide a stable and reproducible capacitance, however. A variant of this structure that allows for linear movement of one set of plates to change the plate overlap area is also used and might be called a \"slider\". This has practical advantages for makeshift or home construction, and may be found in resonant-loop antennas or crystal radios.\n\nSmall variable capacitors operated by screwdriver (for instance, to precisely set a resonant frequency at the factory and then never be adjusted again) are called trimmer capacitors. In addition to air and plastic, trimmers can also be made using a ceramic dielectric, such as mica.\n\nVery often, multiple stator/rotor sections are arranged behind one another on the same axis, allowing for several tuned circuits to be adjusted using the same control, e.g. a preselector, an input filter and the corresponding oscillator in a receiver circuit. The sections can have identical or different nominal capacitances, e.g. 2 × 330 pF for AM filter and oscillator, plus 3 × 45 pF for two filters and an oscillator in the FM section of the same receiver. Capacitors with multiple sections often include trimmer capacitors in parallel to the variable sections, used to adjust all tuned circuits to the same frequency.\n\nA butterfly capacitor is a form of rotary variable capacitor with two independent sets of stator plates opposing each other, and a butterfly-shaped rotor arranged so that turning the rotor will vary the capacitances between the rotor and either stator equally.\n\nButterfly capacitors are used in symmetrical tuned circuits, e.g. RF power amplifier stages in push-pull configuration or symmetrical antenna tuners where the rotor needs to be \"cold\", i.e. connected to RF (but not necessarily DC) ground potential. Since the peak RF current normally flows from one stator to the other without going through wiper contacts, butterfly capacitors can handle large resonance RF currents, e.g. in magnetic loop antennas.\n\nIn a butterfly capacitor, the stators and each half of the rotor can only cover a maximum angle of 90° since there must be a position without rotor/stator overlap corresponding to minimum capacity, therefore a turn of only 90° covers the entire capacitance range.\n\nThe closely related split stator variable capacitor does not have the limitation of 90° angle since it uses two separate packs of rotor electrodes arranged axially behind one another. Unlike in a capacitor with several sections, the rotor plates in a split stator capacitor are mounted on opposite sides of the rotor axis. While the split stator capacitor benefits from larger electrodes compared to the butterfly capacitor, as well as a rotation angle of up to 180°, the separation of rotor plates incurs some losses since RF current has to pass the rotor axis instead of flowing straight through each rotor vane.\n\nDifferential variable capacitors also have two independent stators, but unlike in the butterfly capacitor where capacities on both sides increase equally as the rotor is turned, in a differential variable capacitor one section's capacity will increase while the other section's decreases, keeping the stator-to-stator capacitance constant. Differential variable capacitors can therefore be used in capacitive potentiometric circuits.\n\nThe variable capacitor with air dielectric was invented by the Hungarian engineer Dezső Korda. He received a German patent for the invention on 13 December 1893.\nThe thickness of the depletion layer of a reverse-biased semiconductor diode varies with the DC voltage applied across the diode. Any diode exhibits this effect (including p/n junctions in transistors), but devices specifically sold as variable capacitance diodes (also called varactors or varicaps) are designed with a large junction area and a doping profile specifically designed to maximize capacitance.\n\nTheir use is limited to low signal amplitudes to avoid obvious distortions as the capacitance would be affected by the change of signal voltage, precluding their use in the input stages of high-quality RF communications receivers, where they would add unacceptable levels of intermodulation. At VHF/UHF frequencies, e.g. in FM Radio or TV tuners, dynamic range is limited by noise rather than large signal handling requirements, and varicaps are commonly used in the signal path.\n\nVaricaps are used for frequency modulation of oscillators, and to make high-frequency voltage controlled oscillators (VCOs), the core component in phase-locked loop (PLL) frequency synthesizers that are ubiquitous in modern communications equipment.\n\nA digitally tuned capacitor is an IC variable capacitor based on several technologies. MEMS, BST and SOI/SOS devices are available from a number of suppliers and vary in capacitance range, quality factor and resolution for different RF tuning applications.\n\nMEMS devices have the highest quality factor and are highly linear, and therefore are suitable for antenna aperture tuning, dynamic impedance matching, power amplifier load matching and adjustable filters. RF tuning MEMS are still a relatively new technology and has not yet been accepted broadly.\n\nBST device are based on Barium Strontium Titanate and vary the capacitance by applying high voltage to the device. The tuning accuracy is limited only by the accuracy of the D-A converter circuitry that generates the high voltage. The limitations for BST are stability over temperature and linearity in demanding applications.\n\nSOI/SOS tuning devices are constructed as solid state FET switches built on insulated CMOS wafers and use MIM caps arranged in binary-weighted values to achieve different capacitance values. SOI/SOS switches have high lineary and are well suited to low power applications where high voltages are not present. High voltage endurance requires multiple FET devices in series which adds series resistance and lowers the quality factor.\n\nThe capacitance values are designed for antenna impedance matching in multi-band LTE GSM/WCDMA cellular handsets and mobile TV receivers that operate over wide frequency ranges, such as the European DVB-H and Japanese ISDB-T mobile TV systems.\n\nVariable capacitance is sometimes used to convert physical phenomena into electrical signals.\n\n\n\n"}
{"id": "1049937", "url": "https://en.wikipedia.org/wiki?curid=1049937", "title": "William Robert Grove", "text": "William Robert Grove\n\nSir William Robert Grove, FRS FRSE (11 July 1811 – 1 August 1896) was a Welsh judge and physical scientist. He anticipated the general theory of the conservation of energy, and was a pioneer of fuel cell technology. He invented the Grove voltaic cell.\n\nBorn in Swansea, Wales, Grove was the only child of John, a magistrate and deputy lieutenant of Glamorgan, and his wife, Anne \"née\" Bevan.\n\nHis early education was in the hands of private tutors, before he attended Brasenose College, Oxford to study classics, though his scientific interests may have been cultivated by mathematician Baden Powell. Otherwise, his taste for science has no clear origin though his circle in Swansea was broadly educated. He graduated in 1832.\n\nIn 1835 he was called to the bar by Lincoln's Inn. In the same year, Grove joined the Royal Institution and was a founder of the Swansea Literary and Philosophical Society, an organisation with which he maintained close links.\n\nIn 1829 at the Royal Institution Grove met Emma Maria Powles, and he married her in 1837. The couple embarked on a tour of the continent for their honeymoon. This sabbatical offered Grove an opportunity to pursue his scientific interests and resulted in his first scientific paper suggesting some novel constructions for electric cells.\n\nDuring 1839, Grove developed a novel form of electric cell, the \"Grove cell\", which used zinc and platinum electrodes exposed to two acids and separated by a porous ceramic pot. Grove announced the latter development to the \"Académie des Sciences\" in Paris in 1839. In 1840 Grove invented one of the first incandescent electric lights, which was later perfected by Thomas Edison. Later that year he gave another account of his development at the British Association for the Advancement of Science meeting in Birmingham, where it aroused the interest of Michael Faraday. On Faraday's invitation Grove presented his discoveries at the prestigious Royal Institution Friday Discourse on 13 March 1840.\n\nGrove's presentation made his reputation, and he was soon proposed for Fellowship of the Royal Society by such distinguished men as William Thomas Brande, William Snow Harris and Charles Wheatstone. Grove also attracted the attention of John Peter Gassiot, a relationship that resulted in Grove's becoming the first professor of experimental philosophy at the London Institution in 1841. Grove's inaugural lecture in 1842 was the first announcement of what Grove called the \"correlation of physical forces\", in modern terms, the conservation of energy.\n\nIn 1842, Grove developed the first fuel cell (which he called the \"gas voltaic battery\"), which produced electrical energy by combining hydrogen and oxygen, and described it using his correlation theory. In developing the cell and showing that steam could be disassociated into oxygen and hydrogen, and the process reversed, he was the first person to demonstrate the thermal dissociation of molecules into their constituent atoms. The first demonstration of this effect, he gave privately to Faraday, Gassiot and Edward William Brayley, his scientific editor. His work also led him to early insights into the nature of ionisation. For observations made in Ref., Grove is credited for the discovery of sputtering.\n\nIn the 1840s Grove also collaborated with Gassiot at the London Institution on photography and the Daguerreotype and calotype processes. Inspired by his legal practice, he presciently observed:\n\nIn 1852 he discovered striae, dark bands that occur in electrical breakdown, and investigated their character, presenting his work in an 1858 Bakerian lecture.\n\nIn 1846, Grove published \"On The Correlation of Physical Forces\" in which he anticipated the general theory of the conservation of energy that was more famously put forward in Hermann von Helmholtz' \"Über die Erhaltung der Kraft\" (\"On the Conservation of Force\") published the following year. His 1846 Bakerian lecture relied heavily on his theory.\n\nJames Prescott Joule had been inspired to his investigations into the mechanical equivalent of heat by comparing the mass of coal consumed in a steam engine with the mass of zinc consumed in a Grove battery in performing a common quantity of mechanical work. Grove was certainly familiar with William Thomson's theoretical analysis of Joule's experimental results and Thomson's immature suggestions of conservation of energy. Thomson's public champion, Peter Guthrie Tait was initially a supporter of Grove's ideas but later dismissed them with some coolness.\n\nThough Groves's ideas were forerunners of the theory of the conservation of energy, they were qualitative, unlike the quantitative investigations of Joule or Julius Robert von Mayer. His ideas also shaded into broader speculation, such as the nature of Olbers's paradox, which he may have discovered for himself rather than through a direct knowledge.\n\nGrove also speculated that other forms of energy were yet to be discovered \"as far certain as certain can be of any future event.\"\n\nAs soon as he became a Fellow of the Royal Society in 1840 Grove was a critic of the Society, deprecating its cronyism and the \"de facto\" rule of a few influential Council members. In 1843, he published an anonymous attack on the scientific establishment in \"Blackwood's Magazine\" and called for reform. In 1846 Grove was elected to the Council of the Royal Society, and was heavily involved in the campaign to modernise its charter, in addition to campaigning for the public funding of science.\n\nA charter committee had already been established, and Grove joined it. Groves's fellow campaigners included Gassiot, Leonard Horner and Edward Sabine. Their principal objectives were for the number of new Fellows to be subject to an annual limit, and limitation of the power of nomination to the Council. The reformers' success in 1847 led to the resignation of several key conservatives and the establishment of Grove and his associates with domination of the Council. To celebrate, the reformers founded the Philosophical Club.\n\nThough the Philosophical Club succeeded in ensuring that William Parsons, 3rd Earl of Rosse was appointed next President, they failed to get Grove appointed as Secretary. Grove continued to campaign for a single home for all the scientific institutions at Burlington House.\n\nFrom 1846 Grove started to reduce his scientific work in favour of his professional practice at the bar, his young family providing the financial motivation; and in 1853 became a QC. The bar provided him with the opportunity to combine his legal and scientific knowledge, in particular in patent law and in the unsuccessful defence of poisoner William Palmer in 1856. He was especially involved in the photography patent cases of \"Beard v. Egerton\" (1845–1849), on behalf of Egerton, and of \"Talbot v. Laroche\" (1854). In the latter case Grove appeared for William Fox Talbot in his unsuccessful attempt to assert his calotype patent.\n\nGrove served on a Royal Commission on patent law and on the Metropolitan Commission of Sewers.\n\nIn 1871 he was made judge of the Court of Common Pleas, and was appointed to the Queen's Bench in 1880. He was to have presided at the Cornwall and Devon winter assizes of 1884, which would have entailed him trying the notorious survival cannibalism case of \"R v. Dudley and Stephens\". However, at the last minute he was substituted by Baron Huddleston, possibly because Huddleston was seen as more reliable in ensuring the guilty verdict that the judiciary required. Grove did sit as one of five judges on the final determination of the case in the Divisional Court of the Queen's Bench.\n\nGrove was a careful, painstaking and accurate judge, courageous and not afraid to assert an independent judicial opinion. However, he was fallible in patent cases, where he was prone to become over-interested in the technology in question and to be distracted by questioning the litigants as to potential improvements in their devices, even going so far as to suggest his own innovations. He retired from the bench in 1887. His portrait was painted by Helen Donald-Smith in the 1890s.\n\nGroves's daughter, Imogen Emily (died 1886), married William Edward Hall in 1866. His daughter Anna married Herbert Augustus Hills (1837–1907) and was mother to Edmond Herbert Grove-Hills (\"Colonel Rivers\") and John Waller Hills.\n\nHis health perpetually troubled, Grove died at home, 115 Harley Street in London, after a long illness. He is buried in Kensal Green Cemetery, London.\n\nGrove became a Fellow of the Royal Society in 1840, and received their Royal Medal in 1847. He was Vice-President of the Royal Institution in 1844. Receiving a knighthood in 1872, he was given an honorary degree by Cambridge University in 1879 and became Privy Councillor in 1887.\n\nThe lunar crater \"Grove\" is named in his honor. The Grove Fuel Cell Symposium and Exhibition is organised by Elsevier.\n\n\n\n"}
{"id": "24569206", "url": "https://en.wikipedia.org/wiki?curid=24569206", "title": "W′ and Z′ bosons", "text": "W′ and Z′ bosons\n\nIn particle physics, W′ and Z′ bosons (or W-prime and Z-prime bosons) refer to hypothetical gauge bosons that arise from extensions of the electroweak symmetry of the Standard Model. They are named in analogy with the Standard Model W and Z bosons.\n\nW′ bosons often arise in models with an extra SU(2) gauge group. is spontaneously broken to the diagonal subgroup SU(2) which corresponds to the electroweak SU(2). More generally, we might have \"n\" copies of SU(2), which are then broken down to a diagonal SU(2). This gives rise to \"n\"−1   W′, W′ and Z′ bosons. Such models might arise from quiver diagram, for example. In order for the W′ bosons to couple to weak isospin, the extra SU(2) and the Standard Model SU(2) must mix; one copy of SU(2) must break around the TeV scale (to get W′ bosons with a TeV mass) leaving a second SU(2) for the Standard Model. This happens in Little Higgs models that contain more than one copy of SU(2). Because the W′ comes from the breaking of an SU(2), it is generically accompanied by a Z′ boson of (almost) the same mass and with couplings related to the W′ couplings.\n\nAnother model with W′ bosons but without an additional SU(2) factor is the so-called 331 model with  . The symmetry breaking chain leads to a pair of W′ bosons and three Z′ bosons.\n\nW′ bosons also arise in Kaluza–Klein theories with SU(2) in the bulk.\n\nVarious models of physics beyond the Standard Model predict different kinds of Z′ bosons.\n\n\nA W′-boson could be detected at hadron colliders through its decay to lepton plus neutrino or top quark plus bottom quark, after being produced in quark-antiquark annihilation. The LHC reach for W′ discovery is expected to be a few TeV.\n\nDirect searches for Z′-bosons are carried out at hadron colliders, since these give access to the highest energies available. The search looks for high-mass dilepton resonances: the Z′-boson would be produced by quark-antiquark annihilation and decay to an electron-positron pair or a pair of opposite-charged muons. The most stringent current limits come from the Fermilab Tevatron, and depend on the couplings of the Z′-boson (which control the production cross section); as of 2006, the Tevatron excludes Z′-bosons up to masses of about 800 GeV for \"typical\" cross sections predicted in various models.\n\nThe above statements apply to \"wide width\" models. Recent classes of models have emerged that naturally provide cross section signatures that fall on the edge, or slightly below the 95% confidence level limits set by the Tevatron, and hence can produce detectable cross section signals for a Z′-boson in a mass range much closer to the Z pole mass than the \"wide width\" models discussed above.\n\nThese \"narrow width\" models which fall into this category are those that predict a Stückelberg Z′ as well as a Z′ from a universal extra dimension (see the \"Z′ Hunter's Guide\" for links to these papers).\n\nOn 7 April 2011, the CDF collaboration at the Tevatron reported an excess in proton-antiproton collision events that produce a W-boson accompanied by two hadronic jets. This could possibly be interpreted in terms of a Z′-boson.\n\nOn 2 June 2015, the ATLAS experiment at the LHC reported evidence for W′-bosons at significance 3.4 sigma, still too low to claim a formal discovery. Researchers at the CMS experiment also independently reported signals that corroborate ATLAS's findings.\n\nThe most stringent limits on new W′-bosons are set by their indirect effects on low-energy processes like muon decay, where they can substitute for the Standard Model W boson exchange.\n\nIndirect searches for Z′-bosons are carried out at electron-positron colliders, since these give access to high-precision measurements of the properties of the Standard Model Z-boson. The constraints come from mixing between the Z′ and the Z, and are model dependent because they depend not only on the Z′ mass but also its mixing with the Z. The current most stringent limits are from the CERN LEP collider, which constrains Z′-bosons to be heavier than a few hundred GeV, for typical model parameters. The ILC will extend this reach up to 5–10 TeV depending on the model under consideration, providing complementarity with the LHC because it will offer measurements of additional properties of the Z′-boson.\n\nWe might have gauge kinetic mixings between the U(1)′ of the Z′ boson and U(1) of hypercharge. This mixing leads to a tree level modification of the Peskin–Takeuchi parameters.\n\nMore advanced:\n"}
