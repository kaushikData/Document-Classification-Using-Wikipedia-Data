{"id": "21950966", "url": "https://en.wikipedia.org/wiki?curid=21950966", "title": "2009 southeast Queensland oil spill", "text": "2009 southeast Queensland oil spill\n\nThe 2009 southeast Queensland oil spill occurred off the coast of southeast Queensland, Australia on 11 March 2009, when 230 tonnes of fuel oil, 30 tonnes of other fuel and 31 shipping containers containing 620 tonnes of ammonium nitrate spilled into the Coral Sea, north of Moreton Bay during Cyclone Hamish. It happened after unsecured cargo on damaged other cargo, causing the spillage. Over the following days, the spill washed ashore along of coastline encompassing the Sunshine Coast, Moreton Bay, Bribie Island and Moreton Island.\n\nThe ship proceeded through Moreton Bay and docked at the Port of Brisbane where it continued to spread a long slick in the mouth of the Brisbane River.\n\nThe Queensland government declared a state of emergency. Premier Anna Bligh described the spill as the \"worst environmental disaster Queensland has ever seen.\" It took over 1,425 people 16 months to clean up the affected areas.\n\nMV \"Pacific Adventurer\" was sailing from Newcastle to the Port of Brisbane in Moreton Bay. Cyclone Hamish had formed off the coast of Queensland. Subsequently, the improperly stowed cargo dislodged from the deck damaging other cargo and containers, causing various substances to spill into the ocean. The ship was damaged at and below the waterline and began to leak fuel and lubricant oil.\n\nOver the following days the spill washed ashore over a 60 km stretch of coastline. In an interview, crew members stated that the captain, Bernardino Santos, was to blame for the incident. The captain decided to stay on course, directly through the storm. As the ship encountered heavy swells, the 20 crew members went below deck, fearing for their lives. At 3:12 a.m., a wave broke the restraints for the cargo and sent 31 containers of ammonium nitrate overboard. The Queensland maritime safety authorities estimated that the ship was at a 25-degree angle at the time the containers were knocked off. When the ship came back down, one of the containers damaged the hull. After rolling to the opposite side, it crashed onto another container, creating a large hole underneath the ship, which was not noticed until it docked in Brisbane.\n\nSwire Shipping initially reported that only 20 tonnes had been spilled. This was later revised to 30 tonnes, 100 tonnes, 230 tonnes and 250 tonnes. The Queensland Government reacted to the situation based on the initial information of 20 tonnes, which was later discovered to be inaccurate, leading to claims of an inadequate initial clean-up response. After divers inspected the ship for the source of the spill, two holes were found. One, located just above the waterline, was 10 mm (0.4 in) long 15 mm (0.6 in) wide. The second and larger hole was underneath the ship and measured about 1 m (3.3 ft) long and 300 mm (12 in) wide.\n\nIn the days following the spill, substances washed ashore along a stretch of coastline. Oil covered beaches, rocky reefs, coastal wetlands and mangrove wetlands. Most of these areas were declared restricted zones and public access was limited. Radar satellite images acquired in preparation for cyclone damage captured the extent of the spill before it hit the coastline.\n\n\nFollowing the oil spill Queensland Premier Anna Bligh declared two islands and parts of the Sunshine Coast disaster areas. Clean-up efforts were reportedly going to cost at least A$100,000 a day and last more than a week as the oil continued to spread. Several search parties have been sent to the spill area to attempt to find the potentially explosive materials that were in the 31 containers. Press reports stated that if the ammonium nitrate were to mix with the heavy fuel, the mixture could ignite and cause a large explosion. If the chemical did not react with the fuel but still leaked out, marine life could be threatened by large blooms of algae. By 15 March, the cost of the spill jumped to $A10 million ($6.6 million USD). A team of 88 people were sent out to begin the clean-up process and a further 58 were expected to join within the following days. On 16 March, the Royal Australian Navy began searching for the 31 containers of ammonium nitrate that were knocked off the ship. A navy minehunter was sent to look for the containers to reduce the amount of impact the chemical could cause.\n\nThe clean-up was a delicate operation, because high tides had already eroded beaches after ex-tropical cyclone Hamish battered the coast over the previous week. At the same time, high tides carried off some of the fuel oil initially deposited along shorelines and dispersed it back into the sea. By 15 March, the federal government reported that 50% of the oil had been contained and that the slick had been removed from about 95% of Bribie Island, 85% of the Sunshine Coast and 25% of the waters around Moreton Island. On Moreton Island, 290 people worked to clean up the oil, with most focusing on Middle Creek and Cape Moreton. Even though hundreds of people had worked on cleaning the spill, the average amount of oil cleaned each day was 1 km (0.6 mi) per day. At that rate, it would have taken more than a month to clean all the affected areas. The Federal Government promised to provide $A2 million ($1.3 million USD) to help with the clean-up.\n\nIn late July 2010, the clean-up effort was declared \"complete\". Federal Environment Minister Peter Garrett attended clean-up sites to celebrate the end of clean-up efforts. 1,425 personnel took 16 months to clean up 155 hectares of coastal area and included installation of 21,220 plants and 2.6 km of fencing to protect recovering vegetation.\n\nShortly after the slick washed ashore the Queensland government declared a state of emergency. The disaster declaration will restrict public access to beaches affected by the oil slick and allow clean-up teams unobstructed access. The declaration covers:\n\nSeveral of the local restaurants which receive shipments of fish from the waters off the Queensland coast refused to stock any more fish due to the spill. The cost of the clean-up also hurt the economy, costing at least A$10 million. In July 2009 the total bill was estimated to be A$34 million.\nThe Australian Government provided A$750,000 to help jump-start the tourist industry. Fourteen of the 19 beaches affected along the Sunshine Coast that had been cleaned were reopened in March 2009.\n\nOnce the ship was at port, Captain Santos was forced to surrender his passport to Australian officials and he was to remain in Brisbane for at least two weeks to help with the investigation. On 14 October 2011, Swire Navigation and Bluewind Shipping were fined $A600,000 each and ordered to publish a public apology after earlier pleading guilty to causing the spill. The court heard the two companies have already paid $A17.5 million under a limitation order in the Federal Court, as well as an additional $A7.5 million to help cover the $A31 million cost of cleaning up the damage. Charges against Santos were dropped earlier in the week.\n\nThe Environmental Protection Agency reports that the full extent of wildlife affected by the spill has yet to be realised. \"The flow-on effects of oil spills can be substantive,\" according to a spokesperson for the agency. \"The longer-term impacts are yet to be realised.\" Several birds have been found covered with oil. By 17 March, about 30 animals had been recovered after being covered with oil. All of the animals were immediately treated and cleaned. Oiled animals included pelicans, ghost crabs and sea snakes.\n\n\n"}
{"id": "455177", "url": "https://en.wikipedia.org/wiki?curid=455177", "title": "Anabatic wind", "text": "Anabatic wind\n\nAn anabatic wind, from the Greek \"anabatos\", verbal of \"anabainein\" meaning moving upward, is a warm wind which blows up a steep slope or mountain side, driven by heating of the slope through insolation. It is also known as an upslope flow. These winds typically occur during the daytime in calm sunny weather. A hill or mountain top will be radiatively warmed by the Sun which in turn heats the air just above it. Air at a similar altitude over an adjacent valley or plain does not get warmed so much because of the greater distance to the ground below it. The effect may be enhanced if the lower lying ground is shaded by the mountain and so receives less heat.\n\nThe air over the hill top is now warmer than the air at a similar altitude around it and will rise through convection. This creates a lower pressure region into which the air at the bottom of the slope flows, causing the wind. It is common for the air rising from the tops of large mountains to reach a height where it cools adiabatically to below its dew point and forms cumulus clouds. These can then produce rain or even thunderstorms.\n\nAnabatic winds are particularly useful to soaring glider pilots who can use them to increase the aircraft's altitude. Anabatic winds can be detrimental to the maximum downhill speed of cyclists.\n\nConversely, Katabatic winds are down-slope winds, frequently produced at night by the opposite effect, the air near to the ground losing heat to it faster than air at a similar altitude over adjacent low-lying land.\nMonsoon winds are similarly generated, but on a continental scale and seasonal cycle.\n\n"}
{"id": "3757", "url": "https://en.wikipedia.org/wiki?curid=3757", "title": "Barium", "text": "Barium\n\nBarium is a chemical element with symbol Ba and atomic number 56. It is the fifth element in group 2 and is a soft, silvery alkaline earth metal. Because of its high chemical reactivity, barium is never found in nature as a free element. Its hydroxide, known in pre-modern times as baryta, does not occur as a mineral, but can be prepared by heating barium carbonate.\n\nThe most common naturally occurring minerals of barium are barite (now called baryte) (barium sulfate, BaSO) and witherite (barium carbonate, BaCO), both insoluble in water. The name \"barium\" originates from the alchemical derivative \"baryta\", from Greek βαρύς (\"barys\"), meaning \"heavy.\" \"Baric\" is the adjectival form of barium. Barium was identified as a new element in 1774, but not reduced to a metal until 1808 with the advent of electrolysis.\n\nBarium has few industrial applications. Historically, it was used as a getter for vacuum tubes and in oxide form as the emissive coating on indirectly heated cathodes. It is a component of YBCO (high-temperature superconductors) and electroceramics, and is added to steel and cast iron to reduce the size of carbon grains within the microstructure. Barium compounds are added to fireworks to impart a green color. Barium sulfate is used as an insoluble additive to oil well drilling fluid, as well as in a purer form, as X-ray radiocontrast agents for imaging the human gastrointestinal tract. The soluble barium ion and soluble compounds are poisonous, and have been used as rodenticides.\n\nBarium is a soft, silvery-white metal, with a slight golden shade when ultrapure. The silvery-white color of barium metal rapidly vanishes upon oxidation in air yielding a dark gray oxide layer. Barium has a medium specific weight and good electrical conductivity. Ultrapure barium is very difficult to prepare, and therefore many properties of barium have not been accurately measured yet.\n\nAt room temperature and pressure, barium has a body-centered cubic structure, with a barium–barium distance of 503 picometers, expanding with heating at a rate of approximately 1.8/°C. It is a very soft metal with a Mohs hardness of 1.25. Its melting temperature of is intermediate between those of the lighter strontium () and heavier radium (); however, its boiling point of exceeds that of strontium (). The density (3.62 g/cm) is again intermediate between those of strontium (2.36 g/cm) and radium (~5 g/cm).\n\nBarium is chemically similar to magnesium, calcium, and strontium, but even more reactive. It always exhibits the oxidation state of +2, except in a few rare and unstable molecular species that are only characterised in the gas phase such as BaF. Reactions with chalcogens are highly exothermic (release energy); the reaction with oxygen or air occurs at room temperature, and therefore barium is stored under oil or in an inert atmosphere. Reactions with other nonmetals, such as carbon, nitrogen, phosphorus, silicon, and hydrogen, are generally exothermic and proceed upon heating. Reactions with water and alcohols are very exothermic and release hydrogen gas:\n\nBarium reacts with ammonia to form complexes such as Ba(NH).\n\nThe metal is readily attacked by most acids. Sulfuric acid is a notable exception because passivation stops the reaction by forming the insoluble barium sulfate on the surface. Barium combines with several metals, including aluminium, zinc, lead, and tin, forming intermetallic phases and alloys.\n\nBarium salts are typically white when solid and colorless when dissolved, and barium ions provide no specific coloring. They are denser than the strontium or calcium analogs, except for the halides (see table; zinc is given for comparison).\n\nBarium hydroxide (\"baryta\") was known to alchemists, who produced it by heating barium carbonate. Unlike calcium hydroxide, it absorbs very little CO in aqueous solutions and is therefore insensitive to atmospheric fluctuations. This property is used in calibrating pH equipment.\n\nVolatile barium compounds burn with a green to pale green flame, which is an efficient test to detect a barium compound. The color results from spectral lines at 455.4, 493.4, 553.6, and 611.1 nm. \n\nOrganobarium compounds are a growing field of knowledge: recently discovered are dialkylbariums and alkylhalobariums.\n\nBarium found in the Earth's crust is a mixture of seven primordial nuclides, barium-130, 132, and 134 through 138. Barium-130 undergoes very slow radioactive decay to xenon-130 by double beta plus decay, and barium-132 theoretically decays similarly to xenon-132, with half-lives a thousand times greater than the age of the Universe. The abundance is ~0.1% that of natural barium. The radioactivity of these isotopes is so weak that they pose no danger to life.\n\nOf the stable isotopes, barium-138 composes 71.7% of all barium, and the lighter the isotope, the less abundant.\n\nIn total, barium has about 50 known isotopes, ranging in mass between 114 and 153. The most stable metastable isotope is barium-133 with a half-life of approximately 10.51 years. Five other isotopes have half-lives longer than a day. Barium also has 10 meta states, out of which barium-133m1 is the most stable with a half-life of about 39 hours.\n\nAlchemists in the early Middle Ages knew about some barium minerals. Smooth pebble-like stones of mineral baryte were found in volcanic rock near Bologna, Italy, and so were called \"Bologna stones.\" Alchemists were attracted to them because after exposure to light they would glow for years. The phosphorescent properties of baryte heated with organics were described by V. Casciorolus in 1602.\n\nCarl Scheele determined that baryte contained a new element in 1774, but could not isolate barium, only barium oxide. Johan Gottlieb Gahn also isolated barium oxide two years later in similar studies. Oxidized barium was at first called \"barote\" by Guyton de Morveau, a name that was changed by Antoine Lavoisier to \"baryta.\" Also in the 18th century, English mineralogist William Withering noted a heavy mineral in the lead mines of Cumberland, now known to be witherite. Barium was first isolated by electrolysis of molten barium salts in 1808 by Sir Humphry Davy in England. Davy, by analogy with calcium, named \"barium\" after baryta, with the \"-ium\" ending signifying a metallic element. Robert Bunsen and Augustus Matthiessen obtained pure barium by electrolysis of a molten mixture of barium chloride and ammonium chloride.\n\nThe production of pure oxygen in the Brin process was a large-scale application of barium peroxide in the 1880s, before it was replaced by electrolysis and fractional distillation of liquefied air in the early 1900s. In this process barium oxide reacts at with air to form barium peroxide, which decomposes above by releasing oxygen:\n\nBarium sulfate was first applied as a radiocontrast agent in X-ray imaging of the digestive system in 1908.\n\nThe abundance of barium is 0.0425% in the Earth's crust and 13 µg/L in sea water. The primary commercial source of barium is baryte (also called barytes or heavy spar), a barium sulfate mineral. with deposits in many parts of the world. Another commercial source, far less important than baryte, is witherite, a barium carbonate mineral. The main deposits are located in England, Romania, and the former USSR.\nThe baryte reserves are estimated between 0.7 and 2 billion tonnes. The maximum production, 8.3 million tonnes, was produced in 1981, but only 7–8% was used for barium metal or compounds. Baryte production has risen since the second half of the 1990s from 5.6 million tonnes in 1996 to 7.6 in 2005 and 7.8 in 2011. China accounts for more than 50% of this output, followed by India (14% in 2011), Morocco (8.3%), US (8.2%), Turkey (2.5%), Iran and Kazakhstan (2.6% each).\n\nThe mined ore is washed, crushed, classified, and separated from quartz. If the quartz penetrates too deeply into the ore, or the iron, zinc, or lead content is abnormally high, then froth flotation is used. The product is a 98% pure baryte (by mass); the purity should be no less than 95%, with a minimal content of iron and silicon dioxide. It is then reduced by carbon to barium sulfide:\n\nThe water-soluble barium sulfide is the starting point for other compounds: reacting BaS with oxygen produces the sulfate, with nitric acid the nitrate, with carbon dioxide the carbonate, and so on. The nitrate can be thermally decomposed to yield the oxide. Barium metal is produced by reduction with aluminium at . The intermetallic compound BaAl is produced first:\n\nBaAl is an intermediate reacted with barium oxide to produce the metal. Note that not all barium is reduced.\n\nThe remaining barium oxide reacts with the formed aluminium oxide:\n\nand the overall reaction is\n\nBarium vapor is condensed and packed into molds in an atmosphere of argon. This method is used commercially, yielding ultrapure barium. Commonly sold barium is about 99% pure, with main impurities being strontium and calcium (up to 0.8% and 0.25%) and other contaminants contributing less than 0.1%.\n\nA similar reaction with silicon at yields barium and barium metasilicate. Electrolysis is not used because barium readily dissolves in molten halides and the product is rather impure.\nThe barium mineral, benitoite (barium titanium silicate), occurs as a very rare blue fluorescent gemstone, and is the official state gem of California.\n\nBarium, as a metal or when alloyed with aluminium, is used to remove unwanted gases (gettering) from vacuum tubes, such as TV picture tubes. Barium is suitable for this purpose because of its low vapor pressure and reactivity towards oxygen, nitrogen, carbon dioxide, and water; it can even partly remove noble gases by dissolving them in the crystal lattice. This application is gradually disappearing due to the rising popularity of the tubeless LCD and plasma sets.\n\nOther uses of elemental barium are minor and include an additive to silumin (aluminium–silicon alloys) that refines their structure, as well as\n\nBarium sulfate (the mineral baryte, BaSO) is important to the petroleum industry as a drilling fluid in oil and gas wells. The precipitate of the compound (called \"blanc fixe\", from the French for \"permanent white\") is used in paints and varnishes; as a filler in ringing ink, plastics, and rubbers; as a paper coating pigment; and in nanoparticles, to improve physical properties of some polymers, such as epoxies.\n\nBarium sulfate has a low toxicity and relatively high density of ca. 4.5 g/cm (and thus opacity to X-rays). For this reason it is used as a radiocontrast agent in X-ray imaging of the digestive system (\"barium meals\" and \"barium enemas\"). Lithopone, a pigment that contains barium sulfate and zinc sulfide, is a permanent white with good covering power that does not darken when exposed to sulfides.\n\nOther compounds of barium find only niche applications, limited by the toxicity of Ba ions (barium carbonate is a rat poison), which is not a problem for the insoluble BaSO.\nBecause of the high reactivity of the metal, toxicological data are available only for compounds. Soluble barium compounds are poisonous. In low doses, barium ions act as a muscle stimulant, and higher doses affect the nervous system, causing cardiac irregularities, tremors, weakness, anxiety, shortness of breath, and paralysis. This toxicity may be caused by Ba blocking potassium ion channels, which are critical to the proper function of the nervous system. Other organs damaged by water-soluble barium compounds (i.e., barium ions) are the eyes, immune system, heart, respiratory system, and skin causing, for example, blindness and sensitization.\n\nBarium is not carcinogenic and it does not bioaccumulate. Inhaled dust containing insoluble barium compounds can accumulate in the lungs, causing a benign condition called baritosis. The insoluble sulfate is nontoxic and is not classified as a dangerous goods in transport regulations.\n\nTo avoid a potentially vigorous chemical reaction, barium metal is kept in an argon atmosphere or under mineral oils. Contact with air is dangerous and may cause ignition. Moisture, friction, heat, sparks, flames, shocks, static electricity, and exposure to oxidizers and acids should be avoided. Anything that may contact with barium should be electrically grounded. Anyone who works with the metal should wear pre-cleaned non-sparking shoes, flame-resistant rubber clothes, rubber gloves, apron, goggles, and a gas mask. Smoking in the working area is forbidden. Thorough washing is required after handling barium.\n\n\n"}
{"id": "26930473", "url": "https://en.wikipedia.org/wiki?curid=26930473", "title": "Botswana Power Corporation", "text": "Botswana Power Corporation\n\nBotswana Power Corporation (BPC) is a state owned company for electrical power generation, transmission and distribution in Botswana. It was established in 1970 and is currently the only electricity supplier in the country. BPC produced an estimated 1,052 GWh in 2007, while demand was estimated to 2,648 GWh. BPC represents Botswana in the Southern African Power Pool. Morupule Power Station (coal-fired) in Palapye supplies 80% of domestically generated electricity, however the country is dependent on importing energy from its neighbours. Its capacity is estimated to be around 132 MW. Since the regions main power generator South Africa is experiencing its own problems with undercapacity blackouts are a problem. A plan to boost Morupule station with four new 150 MW units is underway and gathering financial support from several sources.\n\n\n"}
{"id": "9636865", "url": "https://en.wikipedia.org/wiki?curid=9636865", "title": "Card stock", "text": "Card stock\n\nCard stock, also called cover stock or pasteboard, is a paper stock that is thicker and more durable than normal writing or printing paper, but thinner and more flexible than other forms of paperboard. \n\nCard stock is often used for business cards, postcards, playing cards, catalogue covers, scrapbooking, and other uses which require higher durability than regular paper. The texture is usually smooth, but can be textured, metallic, or glossy. When card stock is labeled as cover stock it often has a coated finish on one side or both sides (\"C1S\" or \"C2S\", for \"coated: one side\" or \"coated: two sides\") to produce a glossy look and smooth texture, especially in use for the printing of business cards and book covers.\n\nMost countries use the term grammage to describe the weight of the paper in grams per square metre. The term \"card stock\" is used to describe paper with weights from 50 lb to 110 lb (about 135 to 300 g/m). Alternatively, grammage can be expressed in terms of the weight per number of sheets, pound weight – the weight of 500 sheets of paper. \n\nIn the U.S., card stock thickness is usually measured in \"points\" or \"mils\" which is the thickness of the sheet in thousandths of an inch. For example, a 10 pt. card is thick (roughly corresponding to a weight of 250 g/m), and 12 pt. is . The U.S. Card Stock Thickness Point size is 0.001\" – not to be confused with typographical point size, where 1 point = 1/ 12 traditional pica = exactly 0.01383 inch = 0.35136 mm.\nPaper sizes using the ISO system are often used for card stock. Card stock sizes can be labeled as A3 (420 × 297 mm or 16.5 × 11.7 in) referencing the ISO Standard system, instead of using the physical dimensions to describe its size.\n\n"}
{"id": "36147856", "url": "https://en.wikipedia.org/wiki?curid=36147856", "title": "Compression set", "text": "Compression set\n\nThe compression set (ASTM D395) of a material is the permanent deformation remaining when a force (that was applied to it) is removed. The term is normally applied to soft materials such as elastomers. There are two ways this is normally measured; \"compression set\" A and \"compression set\" B.\n\nThis has the formal name compression set under constant force in air. In \"compression set A\" a force of 1.8 kN is applied to the specimen for a set time at a set temperature. \"Compression set A\" is defined as the percentage of original specimen thickness after the specimen has been left in normal conditions for 30 minutes. C, the \"compression set A\" is given by C = [(t - t) / t] * 100 \nwhere t is the original specimen thickness and t is the specimen thickness after testing.\nThis has the formal name compression set under constant deflection in air. The specimen is compressed to 75% of its original height for a set time and at a set temperature (sample is compressed to .75 of its original height) \"Compression set B\" is (like \"Compression set A\") defined as the percentage of original specimen thickness after it has been left in normal conditions for 30 minutes. C, the \"compression set B\" is given by C = [(t - t) / (t - t)] * 100 \nwhere t is the original specimen thickness, t is the specimen thickness after testing and t is the spacer thickness or the specimen thickness during the test.\n"}
{"id": "4133988", "url": "https://en.wikipedia.org/wiki?curid=4133988", "title": "Conductor gallop", "text": "Conductor gallop\n\nConductor gallop is the high-amplitude, low-frequency oscillation of overhead power lines due to wind. The movement of the wires occur most commonly in the vertical plane, although horizontal or rotational motion is also possible. The natural frequency mode tends to be around 1 Hz, leading the often graceful periodic motion to also be known as conductor dancing. The oscillations can exhibit amplitudes in excess of a metre, and the displacement is sometimes sufficient for the phase conductors to infringe operating clearances (coming too close to other objects), and causing flashover. The forceful motion also adds significantly to the loading stress on insulators and electricity pylons, raising the risk of mechanical failure of either.\n\nThe mechanisms that initiate gallop are not always clear, though it is thought to be often caused by asymmetric conductor aerodynamics due to ice build up on one side of a wire. The crescent of encrusted ice approximates an aerofoil, altering the normally round profile of the wire and increasing the tendency to oscillate.\n\nGallop can be a significant problem for transmission system operators, particularly where lines cross open, windswept country and are at risk to ice loading. If gallop is likely to be a concern, designers can employ smooth-faced conductors, whose improved icing and aerodynamic characteristics reduce the motion. Additionally, anti-gallop devices may be mounted to the line to convert the lateral motion to a less damaging twisting one. Increasing the tension in the line and adopting more rigid insulator attachments have the effect of reducing galloping motion. These measures can be costly, are often impractical after the line has been constructed, and can increase the tendency for the line to exhibit high frequency oscillations.\n\nIf ice loading is suspected, it may be possible to increase power transfer on the line, and so raise its temperature by Joule heating, melting the ice. The sudden loss of ice from a line can result in a phenomenon called \"jump\", in which the catenary dramatically rebounds upwards in response to the change in weight. If the risk of trip is high, the operator may elect to pre-emptively switch out the line in a controlled manner rather than face an unexpected fault. The risk of mechanical failure of the line remains.\n\nConductor gallop analysis overlaps several academic disciplines. Mechanical vibrations covers the laws of motion of the conductor and the long conductor acts as a mass suspended by an elastic spring obeying Hooke's law. Within the discipline of Mechanical vibration, conductor gallop is categorized as a self-excited vibration because the forces which generate conductor gallop are generated by the motion itself. One of the early leaders in modern mechanical vibrations, J. P. Den Hartog, described conductor gallop in a chapter on self-excited vibrations in his text book \"Mechanical Vibrations\", copyrighted in 1956 and reprinted by Dover Publications, where he develops general stability criteria for conductor gallop but without a complete mathematical solution. However, conductor gallop analysis also relates to Civil engineering because the electric conductors are carried by towers and the study of wind influences on structures, including any kind of vibration, has been much studied, especially after the collapse of the Tacoma Narrows bridge due to flutter from the structural members. In fact, perfectly round electrical conductors experience vortex shedding in certain ranges of the Reynolds number. The underlying behavior in the conductor gallop phenomenon also applies to other civil engineering structural elements such as cables and stays on bridges. A more recent reference related to the analysis of conductor gallop is \"Flow-Induced Vibrations, An Engineering Guide\", by Eduard Naudascher and Donald Rockwell copyrighted in 1994 and still published by Dover Publications in 2005, in which experimental data related to vortex shedding frequencies as well as the aerodynamic forces on various structure shapes including cylinder models for a conductor or cable. Another book titled \"Flow-Induced Vibration\" by Robert D. Blevins, 2nd Edition published by Van Nostrand-Reinhold in 1990, also treats conductor gallop while reporting experimental data related to vortex shedding and aerodynamic forces on various structural shapes. Both of the last-mentioned works include references to scientific and engineering journal articles, many of which directly relate to conductor gallop. In aeronautical engineering the term \"flutter\" is used to describe conductor gallop and analogous other phenomena involving aerodynamic forces interacting with elastic structures having inertial mass.\n\nA similar aeolian phenomenon is flutter, caused by vortices on the leeward side of the wire, and which is distinguished from gallop by its high-frequency (10 Hz), low-amplitude motion. To control flutter, transmission lines may be fitted with tuned mass dampers (known as Stockbridge dampers) clamped to the wires close to the towers. The use of bundle conductor spacers can also be of benefit.\n\n\n"}
{"id": "13588039", "url": "https://en.wikipedia.org/wiki?curid=13588039", "title": "Dermal adhesive", "text": "Dermal adhesive\n\nA dermal adhesive (or \"skin glue\") is a glue used to close wounds in the skin, as an alternative to sutures, staples or clips.\n\nGlued closure produces less scarring and is less prone to infection than sutured or stapled closure. There is also no residual closure to remove, so follow-up visits for removal are not required.\n\nSome research is ongoing on making biodegradable glue for use inside the body, which can thus be broken down safely by the body.\n\n"}
{"id": "11698208", "url": "https://en.wikipedia.org/wiki?curid=11698208", "title": "Diphthamide", "text": "Diphthamide\n\nDiphthamide is a post-translationally modified histidine amino acid found in archaeal and eukaryotic elongation factor 2 (eEF-2).\n\nDiphthamide is proposed to be a 2-[3-carboxyamido-3-(trimethylammonio)propyl histidine. Though this structure has been confirmed by X-ray crystallography, its stereochemistry is uncertain.\n\nDiphthamide ensures translation fidelity.\n\nThe presence or absence of diphthamide is known to affect NF-κB or death receptor pathways.\n\nDiphthamide is biosynthesized from histidine and \"S\"-adenosyl methionine.\n"}
{"id": "347428", "url": "https://en.wikipedia.org/wiki?curid=347428", "title": "Elsbett", "text": "Elsbett\n\nThe Elsbett engine is a 89 HP, direct-injection diesel engine designed to run on straight vegetable oil. The engine is also known as Elko engine (for \"Elsbett Konstruktion\") and was invented by Ludwig Elsbett.\n\nThe design limits the loss of energy as heat by a variety of technologies;\n\nIt had a 25% fuel efficiency advantage over contemporary designs in the 1980s. Several revolutionary improvements were achieved in this design, such as a new piston design and combustion process.\n\nThe technology was adopted by some companies in the former Soviet Union and a major project by Grupo Garavelo, one of the 50th largest corporations in Brazil in the 80s,Pres. Luiz Garavelo, VP Marco Garavelo and MKT Dir Julio Garavelo, with the Consorcio Nacional Garavelo (140 sales points and 280.000 clients, Banco Garavelo, Hunday-Garavelo , gave vieculos, Lagpar, Mag, Fazenda Nova Viena, Garavelo Agropecuaria, Garavelo Oleos, Garavelo Publicidade, Lojas Garavelo, Garavelo Agropecuaria, Grafica Garavelo, Coryntho Corretora de Seguros, Garavelo Imobiliaria and others, was undertaken in Brazil. Although work continues on the dedicated multi-fuel Elsbett engine, the company now also sells conversion kits for existing diesel engines to run on vegetable oil.\n\nElsbett AG is based in Thalmaessing, Bavaria, Germany.\n\n"}
{"id": "15443272", "url": "https://en.wikipedia.org/wiki?curid=15443272", "title": "Epoxy granite", "text": "Epoxy granite\n\nEpoxy granite, also known as synthetic granite, is a mixture of epoxy and granite commonly used as an alternative material for machine tool bases. Epoxy granite is used instead of cast iron and steel for better vibration damping, longer tool life, and lower assembly cost.\n\nMachine tools and other high-precision machines rely upon high stiffness, long-term stability, and excellent damping characteristics of the base material for their static and dynamic performance. The most widely used materials for these structures are cast iron, welded steel fabrications, and natural granite. Due to the lack of long-term stability and very poor damping properties, steel fabricated structures are seldom used where high precision is required. Good-quality cast iron that is stress-relieved and annealed will give the structure dimensional stability, and can be cast into complex shapes, but needs an expensive machining process to form precision surfaces after casting.\n\nGood-quality natural granite is becoming more and more difficult to find, but has a higher damping capacity than cast iron. Again, as with cast iron, the machining of natural granite is labour-intensive and expensive.\n\nPrecision granite castings are produced by mixing granite aggregates (which are crushed, washed, and dried) with an epoxy resin system at ambient temperature (i.e., cold curing process). Quartz aggregate filler can also be used in the composition. Vibratory compaction during the molding process tightly packs the aggregate together.\n\nThreaded inserts, steel plates, and coolant pipes can be cast-in during the casting process. To achieve an even higher degree of versatility, linear rails, ground slide-ways and motor mounts can be replicated or grouted-in, therefore eliminating the need for any post-cast machining. The surface finish of the casting is as good as the mould surface.\n\nAdvantages include:\n\n\nEpoxy granite material has an internal damping factor up to ten times better than cast iron, up to three times better than natural granite, and up to thirty times better than steel fabricated structure. It is unaffected by coolants, has excellent long-term stability, improved thermal stability, high torsional and dynamic stiffness, excellent noise absorption, and negligible internal stresses.\n\nDisadvantages include low strength in thin sections (less than ), low tensile strength, and low shock resistance.\n\nAlthough new to many machine tool builders, especially those in the United States, the composite base has been available in Europe for several years. Fritz Studer AG of Switzerland first came up with its Granitan S-100 technology in the early 1970s. The Granitan base is a mixture of reactable epoxy-resin binder developed by Ciba-Geigy Ltd., Switzerland, and granite or gravel. Granitan produced with sand and gravel was developed specifically for the requirements of machine structure manufacturing.\n\n"}
{"id": "793890", "url": "https://en.wikipedia.org/wiki?curid=793890", "title": "Feeder line (network)", "text": "Feeder line (network)\n\nA feeder line is a peripheral route or branch in a network, which connects smaller or more remote nodes with a route or branch carrying heavier traffic. The term is applicable to any system based on a hierarchical network.\n\nIn telecommunications, a feeder line branches from a main line or trunk line.\n\nIn electrical engineering, a feeder line is a type of transmission line. In radio engineering, a feeder connects radio equipment to an antenna, usually \"open wire\" (air-insulated wire line) or twin-lead from a shortwave transmitter. In power engineering, a feeder line is part of an electric distribution network, usually a radial circuit of intermediate voltage.\n\nThe concept of feeder lines is also important in public transportation. The term is particularly used in US air travel and rail transport. Efficient, high-capacity routes connect important nodes while feeder lines connect these nodes to departure and destination points.\n\n"}
{"id": "48146", "url": "https://en.wikipedia.org/wiki?curid=48146", "title": "Fossil fuel", "text": "Fossil fuel\n\nA fossil fuel is a fuel formed by natural processes, such as anaerobic decomposition of buried dead organisms, containing energy originating in ancient photosynthesis. \nThe age of the organisms and their resulting fossil fuels is typically millions of years, and sometimes exceeds 650 million years. \nFossil fuels contain high percentages of carbon and include petroleum, coal, and natural gas. \nOther commonly used derivatives include kerosene and propane. \nFossil fuels range from volatile materials with low carbon to hydrogen ratios like methane, to liquids like petroleum, to nonvolatile materials composed of almost pure carbon, like anthracite coal. \nMethane can be found in hydrocarbon fields either alone, associated with oil, or in the form of methane clathrates.\n\nThe theory that fossil fuels formed from the fossilized remains of dead plants by exposure to heat and pressure in the Earth's crust over millions of years was first introduced by Andreas Libavius “in his 1597 Alchemia [Alchymia]” and later by Mikhail Lomonosov “as early as 1757 and certainly by 1763”.. The first use of the term \"fossil fuel\" was by the German chemist Caspar Neumann, in English translation in 1759.\n\nThe United States Energy Information Administration estimates that in 2007 the world's primary energy sources consisted of petroleum (36.0%), coal (27.4%), natural gas (23.0%), amounting to an 86.4% share for fossil fuels in primary energy consumption in the world. \nNon-fossil sources in 2006 included nuclear (8.5%), hydroelectric (6.3%), and others (geothermal, solar, tidal, wind, wood, waste) amounting to 0.9%. \nWorld energy consumption was growing at about 2.3% per year.\n\nAlthough fossil fuels are continually being formed via natural processes, they are generally considered to be non-renewable resources because they take millions of years to form and the known viable reserves are being depleted much faster than new ones are being made.\n\nThe use of fossil fuels raises serious environmental concerns. \nThe burning of fossil fuels produces around 21.3 billion tonnes (21.3 gigatonnes) of carbon dioxide (CO) per year. \nIt is estimated that natural processes can only absorb about half of that amount, so there is a net increase of 10.65 billion tonnes of atmospheric carbon dioxide per year. \nCarbon dioxide is a greenhouse gas that increases radiative forcing and contributes to global warming. \nA global movement towards the generation of low-carbon renewable energy is underway to help reduce global greenhouse gas emissions.\n\nAquatic phytoplankton and zooplankton that died and sedimented in large quantities under anoxic conditions millions of years ago began forming petroleum and natural gas as a result of anaerobic decomposition. Over geological time this organic matter, mixed with mud, became buried under further heavy layers of inorganic sediment. The resulting high levels of heat and pressure caused the organic matter to chemically alter, first into a waxy material known as kerogen which is found in oil shales, and then with more heat into liquid and gaseous hydrocarbons in a process known as catagenesis. Despite these heat driven transformations (which may increase the energy density compared to typical organic matter), the embedded energy is still photosynthetic in origin.\n\nTerrestrial plants, on the other hand, tended to form coal and methane. Many of the coal fields date to the Carboniferous period of Earth's history. Terrestrial plants also form type III kerogen, a source of natural gas.\n\nThere is a wide range of organic, or hydrocarbon, compounds in any given fuel mixture. The specific mixture of hydrocarbons gives a fuel its characteristic properties, such as boiling point, melting point, density, viscosity, etc. Some fuels like natural gas, for instance, contain only very low boiling, gaseous components. Others such as gasoline or diesel contain much higher boiling components.\n\nFossil fuels are of great importance because they can be burned (oxidized to carbon dioxide and water), producing significant amounts of energy per unit mass. The use of coal as a fuel predates recorded history. Coal was used to run furnaces for the melting of metal ore. Semi-solid hydrocarbons from seeps were also burned in ancient times, but these materials were mostly used for waterproofing and embalming.\n\nCommercial exploitation of petroleum began in the 19th century, largely to replace oils from animal sources (notably whale oil) for use in oil lamps.\n\nNatural gas, once flared-off as an unneeded byproduct of petroleum production, is now considered a very valuable resource. Natural gas deposits are also the main source of the element helium.\n\nHeavy crude oil, which is much more viscous than conventional crude oil, and tar sands, where bitumen is found mixed with sand and clay, began to become more important as sources of fossil fuel as of the early 2000s. Oil shale and similar materials are sedimentary rocks containing kerogen, a complex mixture of high-molecular weight organic compounds, which yield synthetic crude oil when heated (pyrolyzed). These materials had yet to be fully exploited commercially. With additional processing, they can be employed in lieu of other already established fossil fuel deposits. More recently, there has been disinvestment from exploitation of such resources due to their high carbon cost, relative to more easily processed reserves.\n\nPrior to the latter half of the 18th century, windmills and watermills provided the energy needed for industry such as milling flour, sawing wood or pumping water, and burning wood or peat provided domestic heat. The widescale use of fossil fuels, coal at first and petroleum later, to fire steam engines enabled the Industrial Revolution. At the same time, gas lights using natural gas or coal gas were coming into wide use. The invention of the internal combustion engine and its use in automobiles and trucks greatly increased the demand for gasoline and diesel oil, both made from fossil fuels. Other forms of transportation, railways and aircraft, also required fossil fuels. The other major use for fossil fuels is in generating electricity and as feedstock for the petrochemical industry. Tar, a leftover of petroleum extraction, is used in construction of roads.\n\nLevels of primary energy sources are the reserves in the ground. Flows are production of fossil fuels from these reserves. The most important part of primary energy sources are the carbon based fossil energy sources. Coal, oil, and natural gas provided 79.6% of primary energy production during 2002 (in million tonnes of oil equivalent (mtoe)) (34.9+23.5+21.2).\n\nLevels (proved reserves) during 2005–2006\n\nFlows (daily production) during 2006\n\nP. E. Hodgson, a senior research fellow emeritus in physics at Corpus Christi College, Oxford, expects the world energy use is doubling every fourteen years and the need is increasing faster still and he insisted in 2008 that the world oil production, a main resource of fossil fuel, is expected to peak in ten years and thereafter fall.\n\nThe principle of supply and demand holds that as hydrocarbon supplies diminish, prices will rise. Therefore, higher prices will lead to increased alternative, renewable energy supplies as previously uneconomic sources become sufficiently economical to exploit. Artificial gasolines and other renewable energy sources currently require more expensive production and processing technologies than conventional petroleum reserves, but may become economically viable in the near future.\nDifferent alternative sources of energy include nuclear, hydroelectric, solar, wind, and geothermal.\n\nOne of the more promising energy alternatives is the use of inedible feed stocks and biomass for carbon dioxide capture as well as biofuel. While these processes are not without problems, they are currently in practice around the world. Biodiesels are being produced by several companies and source of great research at several universities. Some of the most common and promising processes of conversion of renewable lipids into usable fuels is through hydrotreating and decarboxylation.\n\nThe United States holds less than 5% of the world's population, but due to large houses and private cars, uses more than 25% of the world's supply of fossil fuels. As the largest source of U.S. greenhouse gas emissions, CO2 from fossil fuel combustion, accounted for 80 percent of [its] weighted emissions in 1998. Combustion of fossil fuels also produces other air pollutants, such as nitrogen oxides, sulfur dioxide, volatile organic compounds and heavy metals.\n\nAccording to Environment Canada:\n\"The electricity sector is unique among industrial sectors in its very large contribution to emissions associated with nearly all air issues. Electricity generation produces a large share of Canadian nitrogen oxides and sulphur dioxide emissions, which contribute to smog and acid rain and the formation of fine particulate matter. It is the largest uncontrolled industrial source of mercury emissions in Canada. Fossil fuel-fired electric power plants also emit carbon dioxide, which may contribute to climate change. In addition, the sector has significant impacts on water and habitat and species. In particular, hydropower dams and transmission lines have significant effects on water and biodiversity.\"\n\nAccording to U.S. Scientist Jerry Mahlman and USA Today: Mahlman, who crafted the IPCC language used to define levels of scientific certainty, says the new report will lay the blame at the feet of fossil fuels with \"virtual certainty,\" meaning 99% sure. That's a significant jump from \"likely,\" or 66% sure, in the group's last report in 2001, Mahlman says. His role in this year's effort involved spending two months reviewing the more than 1,600 pages of research that went into the new assessment.\n\nCombustion of fossil fuels generates sulfuric, carbonic, and nitric acids, which fall to Earth as acid rain, impacting both natural areas and the built environment. Monuments and sculptures made from marble and limestone are particularly vulnerable, as the acids dissolve calcium carbonate.\n\nFossil fuels also contain radioactive materials, mainly uranium and thorium, which are released into the atmosphere. In 2000, about 12,000 tonnes of thorium and 5,000 tonnes of uranium were released worldwide from burning coal. It is estimated that during 1982, US coal burning released 155 times as much radioactivity into the atmosphere as the Three Mile Island accident.\n\nBurning coal also generates large amounts of bottom ash and fly ash. These materials are used in a wide variety of applications, utilizing, for example, about 40% of the US production.\n\nHarvesting, processing, and distributing fossil fuels can also create environmental concerns. Coal mining methods, particularly mountaintop removal and strip mining, have negative environmental impacts, and offshore oil drilling poses a hazard to aquatic organisms. Oil refineries also have negative environmental impacts, including air and water pollution. Transportation of coal requires the use of diesel-powered locomotives, while crude oil is typically transported by tanker ships, each of which requires the combustion of additional fossil fuels.\n\nEnvironmental regulation uses a variety of approaches to limit these emissions, such as command-and-control (which mandates the amount of pollution or the technology used), economic incentives, or voluntary programs.\n\nAn example of such regulation in the USA is the \"EPA is implementing policies to reduce airborne mercury emissions. Under regulations issued in 2005, coal-fired power plants will need to reduce their emissions by 70 percent by 2018.\".\n\nIn economic terms, pollution from fossil fuels is regarded as a negative externality. Taxation is considered one way to make societal costs explicit, in order to 'internalize' the cost of pollution. This aims to make fossil fuels more expensive, thereby reducing their use and the amount of pollution associated with them, along with raising the funds necessary to counteract these factors.\n\nAccording to Rodman D. Griffin, \"The burning of coal and oil have saved inestimable amounts of time and labor while substantially raising living standards around the world\". Although the use of fossil fuels may seem beneficial to our lives, this act is playing a role on global warming and it is said to be dangerous for the future.\n\nMoreover, these environmental pollutions impacts on the human beings because its particles of the fossil fuel on the air cause negative health effects when inhaled by people. These health effects include premature death, acute respiratory illness, aggravated asthma, chronic bronchitis and decreased lung function. So, the poor, undernourished, very young and very old, and people with preexisting respiratory disease and other ill health, are more at risk.\n\nEurope spent €406 billion on importing fossil fuels in 2011 and €545 billion in 2012. This is around three times more than the cost of the Greek bailout up to 2013. In 2012 wind energy in Europe avoided €9.6 billion of fossil fuel costs. A 2014 report by the International Energy Agency said that the fossil fuels industry collects $550 billion a year in global government fossil fuel subsidies. This amount was $490 billion in 2014, but would have been $610 billion without agreements made in 2009.\n\nA 2015 report studied 20 fossil fuel companies and found that, while highly profitable, the hidden economic cost to society was also large. The report spans the period 2008–2012 and notes that: \"For all companies and all years, the economic cost to society of their emissions was greater than their after‐tax profit, with the single exception of ExxonMobil in 2008.\" Pure coal companies fare even worse: \"the economic cost to society exceeds total revenue in all years, with this cost varying between nearly $2 and nearly $9 per $1 of revenue.\" In this case, total revenue includes \"employment, taxes, supply purchases, and indirect employment.\"\n\n\n\nDebate\n"}
{"id": "23284699", "url": "https://en.wikipedia.org/wiki?curid=23284699", "title": "Gagetown (film)", "text": "Gagetown (film)\n\nGagetown is a 2009 documentary film that looks into the massive defoliant spray program that was used at CFB Gagetown since 1956. The chemical herbicides used include\n50/50 mixtures of 2,4-D/ 2,4,5-T, and Tordon 101, also known as Agent Orange and Agent White. \n\nThese chemicals have been known to produce extremely toxic byproducts, including dioxins and hexachlorobenzenes.\n\nExposure to these byproducts has been linked to increased cases in cancer and blood diseases, especially Non-Hodgkin's Lymphoma and leukemia.\n\nAccording to a Canadian Department of National Defence document acquired through the Access to Information Act, over 3.2 million liters and kilograms of chemical defoliants \nwere used at the base between the years 1956 to 1984.\n\n\n"}
{"id": "9047514", "url": "https://en.wikipedia.org/wiki?curid=9047514", "title": "Game Wars", "text": "Game Wars\n\nGame Wars is a 1991 non-fiction book by Marc Reisner which gives accounts of several U.S. Fish and Wildlife Service sting operations. First published in 1991, the book centers on undercover agent Dave Hall as he infiltrates groups of poachers and gathers evidence to prosecute them. The three chapters of the book focus on poaching of alligators in Louisiana, ivory in Alaska, and sacalait in the Southern United States. The book highlights the debate over whether declines in wildlife populations should be attributed to over-hunting or loss of habitat. The book also highlights and details several cases and operations that Special Agent Hall conducted along with game wardens from the Louisiana Department of Wildlife & Fisheries – Enforcement Division.\n\nThe title was reviewed in \"The Chicago Tribune\".\n"}
{"id": "54109494", "url": "https://en.wikipedia.org/wiki?curid=54109494", "title": "Gliding (vehicle)", "text": "Gliding (vehicle)\n\nGliding is an energy-efficient driving operating mode of a motor vehicle that involves turning off the internal combustion engine while the vehicle is still moving in order to save fuel. This is differentiated from coasting, which is running the vehicle in idle mode by disengaging the engine from the wheels, either by disengaging the clutch or setting the transmission or gearbox to neutral position. Coasting uses the accelerated kinetic energy to keep the vehicle going, but loses speed to the existing drag. This functionality is integral to the concept of hybrid electric vehicles and is performed automatically by the engine controller. For vehicles with a conventional internal combustion engine, coasting can be performed manually; gliding requires having a gear box. It is illegal in some states. An extra button to stop the engine was shown in 1979 on International Motor Show Germany, but newer standard in any vehicle. A so-called \"eClutch\" (electronic controlled clutch) uses an actuator to disengage the clutch when the driver leaves the accelerator.\n\nA start-stop system turns the engine off when the vehicle is stopped. Gliding is turning off the engine while the vehicle is still moving. Safety relevant components like power steering or vacuum servo required to be operated electric, but most vehicles drive it by the combustion engine, only. The fuel saving is depending more on the road and traffic conditions, and less on the altitude profile of the road. It is assumed to save up to 7% fuel in the NEFZ driving cycle; in real road traffic conditions, the savings is estimated to be up to 10%.\n\nIdle coasting is driving the vehicle at a higher speed than the idle speed of the engine and then disengaging the engine from the wheels by setting the transmission or gearbox to neutral position or disengaging the clutch, maintaining the engine in idle mode.\n\nPushing the clutch pedal disengages the engine from the powertrain like wheels, drive shafts and gearbox. Releasing the accelerator makes the engine slow down to idle. Turning off the ignition has further effects. Pulling the key causes locking the steering wheel. Some vehicles drop headlights or breaklights when the ignition is turned off. After the engine is stopped, applying the brake takes a longer time or giving a few pushes to the brake releases the reserve of the vacuum servo, causing a loss of the brake support. A belt-driven power steering fails immediately. Real hybrid vehicles have electric-driven support of power steering and brake. Bigger engines and engines with higher compression ratio can cause damage to components of the powertrain at a rough start over clutch from gliding vehicles kinetic power or trailing drag. Using a higher gear decreases the torque force on the powertrain. By turning off the ignition the engine control unit (ECU) needs to detect the engine shafts position. Some ECUs need several rotations of the camshaft to detect ignition and injection timing points. Starting the engine in coldstart temporarily takes more fuel until operation temperature of the engine is detected by the ECU.\n\nGliding vehicles equipped with torque converter-based automatic transmissions might cause damage. Also restarting the engine by clutch from the vehicle's kinetic energy or drag is not possible. It is required to use the starter.\n\n"}
{"id": "44572867", "url": "https://en.wikipedia.org/wiki?curid=44572867", "title": "Interconnector", "text": "Interconnector\n\nAn interconnector is a structure which enables energy to flow between networks. The term is used more specifically to refer to international connections between electricity and natural gas networks.\n\nInterconnectors allow the trading of energy between territories. For example, the North Sea Interconnector allows the trading of natural gas between the UK and Belgium and the East–West Interconnector allows the trading of electricity between the UK and the Ireland. A territory which generates more energy than it requires for its own activities can therefore sell surplus energy to a neighbouring territory.\n\nInterconnectors also provide increased resilience. Within the European Union there is a movement towards a single market for energy, which makes interconnectors viable. As such, the Nordic and Baltic energy exchange Nord Pool Spot rely on multiple interconnectors. The fullest possible implementation of this is the proposed European super grid which would include numerous interconnectors between national networks.\n\nInterconnectors may run across a land border or connect two land areas separated by water.\n\n"}
{"id": "3562045", "url": "https://en.wikipedia.org/wiki?curid=3562045", "title": "Jersey-Atlantic Wind Farm", "text": "Jersey-Atlantic Wind Farm\n\nThe Jersey-Atlantic Wind Farm in Atlantic City, in Atlantic County, New Jersey, is the first coastal wind farm in the United States and the first wind farm in New Jersey. It became operational in March 2006 and consists of five 1.5 MW turbines built by General Electric. Each wind turbine reaches a height of .\n\nThe wind farm is located at the Atlantic County Utilities Authority (ACUA) Wastewater Treatment Plant on US 30 and is visible from highways approaching Atlantic City. The treatment plant uses approximately 50% of the wind-generated capacity from the wind turbines, providing about 60% of the wastewater plant's electricity needs, with the remaining energy being provided to the main power grid for resale as premium renewable electricity.\n\n\n"}
{"id": "15193959", "url": "https://en.wikipedia.org/wiki?curid=15193959", "title": "John Weyant", "text": "John Weyant\n\nJohn P. Weyant (born July 23, 1947) is a research professor of management science and engineering at Stanford University. He obtained his PhD from UC Berkeley and has been at Stanford since 1977. His research is focussed on climate change, energy security, corporate strategy analysis, and energy policy in Japan.\n\nWeyant is the director of the Energy Modeling Forum, an editor of Energy Economics, and a lead author of the IPCC (the work of the IPCC, including the contributions of many scientists, was recognised by the joint award of the 2007 Nobel Peace Prize). He is a key figure in the academic communities of energy economics, integrated assessment modelling, and climate economics, and has been influential in shaping the international policy debate on climate policy.\n\n"}
{"id": "10624594", "url": "https://en.wikipedia.org/wiki?curid=10624594", "title": "Krypton", "text": "Krypton\n\nKrypton (from \"the hidden one\") is a chemical element with symbol Kr and atomic number 36. It is a member of group 18 (noble gases) elements. A colorless, odorless, tasteless noble gas, krypton occurs in trace amounts in the atmosphere and is often used with other rare gases in fluorescent lamps. With rare exceptions, krypton is chemically inert.\n\nKrypton, like the other noble gases, is used in lighting and photography. Krypton light has many spectral lines, and krypton plasma is useful in bright, high-powered gas lasers (krypton ion and excimer lasers), each of which resonates and amplifies a single spectral line. Krypton fluoride also makes a useful laser medium. From 1960 to 1983, the official length of a meter was defined by the 605 nm wavelength of the orange spectral line of krypton-86, because of the high power and relative ease of operation of krypton discharge tubes.\n\nKrypton was discovered in Britain in 1898 by Sir William Ramsay, a Scottish chemist, and Morris Travers, an English chemist, in residue left from evaporating nearly all components of liquid air. Neon was discovered by a similar procedure by the same workers just a few weeks later. William Ramsay was awarded the 1904 Nobel Prize in Chemistry for discovery of a series of noble gases, including krypton.\n\nIn 1960, the International Conference on Weights and Measures defined the meter as 1,650,763.73 wavelengths of light emitted by the krypton-86 isotope. This agreement replaced the 1889 international prototype meter located in Paris, which was a metal bar made of a platinum-iridium alloy (one of a series of standard meter bars, originally constructed to be one ten-millionth of a quadrant of the Earth's polar circumference). This also obsoleted the 1927 definition of the ångström based on the red cadmium spectral line, replacing it with 1 Å = 10 m. The krypton-86 definition lasted until the October 1983 conference, which redefined the meter as the distance that light travels in vacuum during 1/299,792,458 s.\n\nKrypton is characterized by several sharp emission lines (spectral signatures) the strongest being green and yellow. Krypton is one of the products of uranium fission. Solid krypton is white and has a face-centered cubic crystal structure, which is a common property of all noble gases (except helium, which has a hexagonal close-packed crystal structure).\n\nNaturally occurring krypton in Earth's atmosphere is composed of five stable isotopes, plus one isotope (Kr) with such a long half-life (9.2×10 years) that it can be considered stable. (This isotope has the second-longest known half-life among all isotopes for which decay has been observed; it undergoes double electron capture to Se). In addition, about thirty unstable isotopes and isomers are known. Traces of Kr, a cosmogenic nuclide produced by the cosmic ray irradiation of Kr, also occur in nature: this isotope is radioactive with a half-life of 230,000 years. Krypton is highly volatile and does not stay in solution in near-surface water, but Kr has been used for dating old (50,000–800,000 years) groundwater.\n\nKr is an inert radioactive noble gas with a half-life of 10.76 years. It is produced by the fission of uranium and plutonium, such as in nuclear bomb testing and nuclear reactors. Kr is released during the reprocessing of fuel rods from nuclear reactors. Concentrations at the North Pole are 30% higher than at the South Pole due to convective mixing.\n\nLike the other noble gases, krypton is highly chemically unreactive. The rather restricted chemistry of krypton in its only known nonzero oxidation state of +2 parallels that of the neighboring element bromine in the +1 oxidation state; due to the scandide contraction it is difficult to oxidize the 4p elements to their group oxidation states. Before the 1960s, no noble gas compounds had been synthesized.\n\nHowever, following the first successful synthesis of xenon compounds in 1962, synthesis of krypton difluoride () was reported in 1963. In the same year, was reported by Grosse, \"et al.\", but was subsequently shown to be a mistaken identification. Under extreme conditions, krypton reacts with fluorine to form KrF according to the following equation:\n\nCompounds with krypton bonded to atoms other than fluorine have also been discovered. There are also unverified reports of a barium salt of a krypton oxoacid. ArKr and KrH polyatomic ions have been investigated and there is evidence for KrXe or KrXe.\n\nThe reaction of with produces an unstable compound, , that contains a krypton-oxygen bond. A krypton-nitrogen bond is found in the cation [HC≡N–Kr–F], produced by the reaction of with [HC≡NH][AsF] below −50 °C. HKrCN and HKrC≡CH (krypton hydride-cyanide and hydrokryptoacetylene) were reported to be stable up to 40 K.\n\nKrypton hydride (Kr(H)) crystals can be grown at pressures above 5 GPa. They have a face-centered cubic structure where krypton octahedra are surrounded by randomly oriented hydrogen molecules.\n\nEarth has retained all of the noble gases that were present at its formation except helium. Krypton's concentration in the atmosphere is about 1 ppm. It can be extracted from liquid air by fractional distillation. The amount of krypton in space is uncertain, because measurement is derived from meteoric activity and solar winds. The first measurements suggest an abundance of krypton in space.\n\nKrypton's multiple emission lines make ionized krypton gas discharges appear whitish, which in turn makes krypton-based bulbs useful in photography as a brilliant white light source. Krypton is used in some photographic flashes for high speed photography. Krypton gas is also combined with other gases to make luminous signs that glow with a bright greenish-yellow light.\n\nKrypton is mixed with argon in energy efficient fluorescent lamps, reducing the power consumption, but also reducing the light output and raising the cost. Krypton costs about 100 times as much as argon. Krypton (along with xenon) is also used to fill incandescent lamps to reduce filament evaporation and allow higher operating temperatures. A brighter light results with more blue color than conventional incandescent lamps.\n\nKrypton's white discharge is often used to good effect in colored gas discharge tubes, which are simply painted or stained to create the desired color (for example, \"neon\" type multi-colored advertising signs are often entirely krypton-based). Krypton produces much higher light power than neon in the red spectral line region, and for this reason, red lasers for high-power laser light-shows are often krypton lasers with mirrors that select the red spectral line for laser amplification and emission, rather than the more familiar helium-neon variety, which could not achieve the same multi-watt outputs.\n\nThe krypton fluoride laser is important in nuclear fusion energy research in confinement experiments. The laser has high beam uniformity, short wavelength, and the spot size can be varied to track an imploding pellet.\n\nIn experimental particle physics, liquid krypton is used to construct quasi-homogeneous electromagnetic calorimeters. A notable example is the calorimeter of the NA48 experiment at CERN containing about 27 tonnes of liquid krypton. This usage is rare, since liquid argon is less expensive. The advantage of krypton is a smaller Molière radius of 4.7 cm, which provides excellent spatial resolution with little overlapping. The other parameters relevant for calorimetry are: radiation length of X=4.7 cm, and density of 2.4 g/cm.\n\nThe sealed spark gap assemblies in ignition exciters in some older jet engines contain a small amount of krypton-85 to produce consistent ionization levels and uniform operation.\n\nKrypton-83 has application in magnetic resonance imaging (MRI) for imaging airways. In particular, it enables the radiologist to distinguish between hydrophobic and hydrophilic surfaces containing an airway.\n\nAlthough xenon has potential for use in computed tomography (CT) to assess regional ventilation, its anesthetic properties limit its fraction in the breathing gas to 35%. A breathing mixture of 30% xenon and 30% krypton is comparable in effectiveness for CT to a 40% xenon fraction, while avoiding the unwanted effects of a high partial pressure of xenon gas.\n\nThe metastable isotope krypton-81m is used in nuclear medicine for lung ventilation/perfusion scans, where it is inhaled and imaged with a gamma camera.\n\nKrypton-85 in the atmosphere has been used to detect clandestine nuclear fuel reprocessing facilities in North Korea and Pakistan. Those facilities were detected in the early 2000s and were believed to be producing weapons-grade plutonium.\n\nKrypton is used occasionally as an insulating gas between window panes.\n\nKrypton is considered to be a non-toxic asphyxiant. Krypton has a narcotic potency seven times greater than air, and breathing an atmosphere of 50% krypton and 50% natural air (as might happen in the locality of a leak) causes narcosis in humans similar to breathing air at four times atmospheric pressure. This is comparable to scuba diving at a depth of (see nitrogen narcosis) and could affect anyone breathing it. At the same time, that mixture would contain only 10% oxygen (rather than the normal 20%) and hypoxia would be a greater concern.\n\n\n"}
{"id": "9862901", "url": "https://en.wikipedia.org/wiki?curid=9862901", "title": "List of extreme temperatures in Canada", "text": "List of extreme temperatures in Canada\n\nThe coldest place in Canada based on average yearly temperature is Eureka, Nunavut, where the temperature averages at for the year. However, the coldest temperature ever recorded in Canada was in Snag, Yukon.\n\n"}
{"id": "21740306", "url": "https://en.wikipedia.org/wiki?curid=21740306", "title": "List of rivers by age", "text": "List of rivers by age\n\nThis is a selected list of the oldest rivers on Earth for which there is knowledge about their existence in past times.\n\nGenerally, the age is estimated based primarily upon the age of any mountains it dissects; the age of the sea or ocean to which it eventually outflows can be irrelevant; for example, several rivers of the east side of the Appalachian Mountains of the Atlantic Ocean, which did not exist 130 million years ago. If a river fully dissects a mountain range, then this generally indicates that the river existed at least at the time that the mountain range rose.\n\n"}
{"id": "57721868", "url": "https://en.wikipedia.org/wiki?curid=57721868", "title": "Mass-flux fraction", "text": "Mass-flux fraction\n\nThe mass-flux fraction (or Hirschfelder-Curtiss variable or Kármán-Penner variable) is the ratio of mass-flux of a particular chemical species to the total mass flux. It includes both the convectional mass flux and diffusional mass flux. It was introduced by Joseph O. Hirschfelder and in 1948 and later by Theodore von Kármán and Sol Penner in 1954. The mass-flux fraction of species i is defined as\n\nwhere\n\nIt satisfies the identity\n\nsimilar to mass fraction, but, the mass-flux fraction can take positive and negative values. This variable is used in steady, one-dimensional combustion problems in place of mass fraction. For one-dimensional (formula_9 direction) steady flows, the conservation equation for the mass-flux fraction reduces to\n\nwhere formula_11 is the mass production rate of species i.\n"}
{"id": "6021557", "url": "https://en.wikipedia.org/wiki?curid=6021557", "title": "Metal hydride fuel cell", "text": "Metal hydride fuel cell\n\nMetal hydride fuel cells are a subclass of alkaline fuel cells that are in the research and development phase. A notable feature is their ability to chemically bond and store hydrogen within the cell. This feature is shared with direct borohydride fuel cells, although the two differ in that MHFCs are refueled with pure hydrogen. Though the absorption characteristic of metal hydrides (around 2%) is far lower than sodium-borohydrides and other \"light\" metal hydrides (around 10.8%), prototypes have been claimed to demonstrate a number of interesting characteristics:\n\nMetal hydrides fuel cells are being researched by ECD Ovonics, as well as by the Japanese National Institute of Advanced Industrial Science and Technology (AIST). Though similar, the two MHFC concepts use different catalysts.\nThus far, neither research project has produced a demonstrable model outside of a laboratory – only publications and patents – and significant efficiency hurdles have yet to be overcome. The Ovonics and AIST metal hydride fuel cells claim current densities of 250 mA/cm² and 20 mA/cm², respectively, versus typical PEMFC performance at 1 A/cm².\n\n\n\n"}
{"id": "295684", "url": "https://en.wikipedia.org/wiki?curid=295684", "title": "Molybdenum disulfide", "text": "Molybdenum disulfide\n\nMolybdenum disulfide is an inorganic compound composed of molybdenum and sulfur. Its chemical formula is .\n\nThe compound is classified as a transition metal dichalcogenide. It is a silvery black solid that occurs as the mineral molybdenite, the principal ore for molybdenum. is relatively unreactive. It is unaffected by dilute acids and oxygen. In appearance and feel, molybdenum disulfide is similar to graphite. It is widely used as a dry lubricant because of its low friction and robustness. Bulk is a diamagnetic, indirect bandgap semiconductor similar to silicon, with a bandgap of 1.23 eV.\n\nMoS is naturally found as either molybdenite, a crystalline mineral, or jordisite, a rare low temperature form of molybdenite. Molybdenite ore is processed by flotation to give relatively pure . The main contaminant is carbon. also arises by thermal treatment of virtually all molybdenum compounds with hydrogen sulfide or elemental sulfur and can be produced by metathesis reactions from molybdenum pentachloride.\n\nAll forms of have a layered structure, in which a plane of molybdenum atoms is sandwiched by planes of sulfide ions. These three strata form a monolayer of MoS. Bulk MoS consists of stacked monolayers, which are held together by weak van der Waals interactions.\n\nCrystalline MoS is found in nature as one of two phases, 2H-MoS and 3R-MoS, where the \"H\" and the \"R\" indicate hexagonal and rhombohedral symmetry, respectively. In both of these structures, each molybdenum atom exists at the center of a trigonal prismatic coordination sphere and is covalently bonded to six sulfide ions. Each sulfur atom has pyramidal coordination and is bonded to three molybdenum atoms. Both the 2H- and 3R-phases are semiconducting.\n\nA third, metastable crystalline phase known as 1T-MoS was discovered by intercalating 2H-MoS with alkali metals. This phase has tetragonal symmetry and is metallic. The 1T-phase can be stabilized through doping with electron donors like rhenium, or converted back to the 2H-phase by microwave radiation.\n\nNanotube-like and buckyball-like molecules composed of are known.\n\nWhile bulk MoS in the 2H-phase is known to be an indirect-band gap semiconductor, monolayer MoS has a direct band gap. The layer-dependent optoelectronic properties of MoS have promoted much research in 2-dimensional MoS-based devices. 2D MoS can be produced by exfoliating bulk crystals to produce single-layer to few-layer flakes either through a dry, micromechanical process or through solution processing.\n\nMicromechanical exfoliation, also pragmatically called \"Scotch-tape exfoliation\", involves using an adhesive material to repeatedly peel apart a layered crystal by overcoming the van der Waals forces. The crystal flakes can then be transferred from the adhesive film to a substrate. This facile method was first used by Novoselov and Geim to obtain graphene from graphite crystals. However, it can not be employed for a uniform 1-D layers because of less adhesion of MoS2 with the substrate (either Si, glass or quartz). The aforementioned scheme is good for Graphene only. While Scotch tape is generally used as the adhesive tape, PDMS stamps can also satisfactorily cleave MoS if it is important to avoid contaminating the flakes with residual adhesive.\n\nLiquid-phase exfoliation can also be used to produce monolayer to multi-layer MoS in solution. A few methods include lithium intercalation to delaminate the layers and sonication in a high-surface tension solvent.\n\nMoS excels as a lubricating material (see below) due to its layered structure and low coefficient of friction. Interlayer sliding dissipates energy when a shear stress is applied to the material. Extensive work has been performed to characterize the coefficient of friction and shear strength of MoS in various atmospheres. The shear strength of MoS increases as the coefficient of friction increases. This property is called superlubricity. At ambient conditions, the coefficient of friction for MoS was determined to be 0.150, with a corresponding estimated shear strength of 56.0 MPa. Direct methods of measuring the shear strength indicate that the value is closer to 25.3 MPa.\n\nThe wear resistance of MoS in lubricating applications can be increased by doping MoS with chromium. Microindentation experiments on nanopillars of Cr-doped MoS found that the yield strength increased from an average of 821 MPa for pure MoS (0 at. % Cr) to 1017 MPa for 50 at. % Cr. The increase in yield strength is accompanied by a change in the failure mode of the material. While the pure MoS nanopillar fails through a plastic bending mechanism, brittle fracture modes become apparent as the material is loaded with increasing amounts of dopant.\n\nThe widely used method of micromechanical exfoliation has been carefully studied in MoS to understand the mechanism of delamination in few-layer to multi-layer flakes. The exact mechanism of cleavage was found to be layer dependent. Flakes thinner than 5 layers undergo homogenous bending and rippling, while flakes around 10 layers thick delaminated through interlayer sliding. Flakes with more than 20 layers exhibited a kinking mechanism during micromechanical cleavage. The cleavage of these flakes was also determined to be reversible due to the nature of van der Waals bonding.\n\nIn recent years, MoS has been utilized in flexible electronic applications, promoting more investigation into the elastic properties of this material. Nanoscopic bending tests using AFM cantilever tips were performed on micromechanically exfoliated MoS flakes that were deposited on a holey substrate. The yield strength of monolayer flakes was 270 GPa, while the thicker flakes were also stiffer, with a yield strength of 330 GPa. Molecular dynamic simulations found the in-plane yield strength of MoS to be 229 GPa, which matches the experimental results within error.\n\nBertolazzi and coworkers also characterized the failure modes of the suspended monolayer flakes. The strain at failure ranges from 6 to 11%. The average yield strength of monolayer MoS is 23 GPa, which is close to the theoretical fracture strength for defect-free MoS.\n\nThe band structure of MoS is sensitive to strain.\n\nMolybdenum disulfide is stable in air and attacked only by aggressive reagents. It reacts with oxygen upon heating forming molybdenum trioxide:\n\nChlorine attacks molybdenum disulfide at elevated temperatures to form molybdenum pentachloride:\n\nMolybdenum disulfide is a host for formation of intercalation compounds. This behavior is relevant to its use as a cathode material in batteries. One example is a lithiated material, . With butyl lithium, the product is .\n\nDue to weak van der Waals interactions between the sheets of sulfide atoms, has a low coefficient of friction. in particle sizes in the range of 1–100 µm is a common dry lubricant. Few alternatives exist that confer high lubricity and stability at up to 350 °C in oxidizing environments. Sliding friction tests of using a pin on disc tester at low loads (0.1–2 N) give friction coefficient values of <0.1.\n\nExamples of applications of -based lubricants include two-stroke engines (such as motorcycle engines), bicycle coaster brakes, automotive CV and universal joints, ski waxes and bullets.\n\nOther layered inorganic materials exhibit lubricating properties (collectively known as solid lubricants (or dry lubricants)) include graphite, which requires volatile additives and hexagonal boron nitride.\n\nMolybdenum disulfide is used as an additive in some NLGI#2 greases and dry-film lubricants to enhance pressure and temperature tolerances as well as to provide secondary lubrication after the base has worn off or migrated from the intended application site. Greases fortified with molybdenum disulfide grease have numerous benefits: \n\n is employed as a cocatalyst for desulfurization in petrochemistry, for example, hydrodesulfurization.The effectiveness of the catalysts is enhanced by doping with small amounts of cobalt or nickel. The intimate mixture of these sulfides is supported on alumina. Such catalysts are generated in situ by treating molybdate/cobalt or nickel-impregnated alumina with or an equivalent reagent. Catalysis does not occur at the regular sheet-like regions of the crystallites, but instead at the edge of these planes.\n\nMoS finds use as a hydrogenation catalyst for organic synthesis. It is derived from a common transition metal, rather than group 10 metal as are many alternatives, MoS is chosen when catalyst price or resistance to sulfur poisoning are of primary concern. MoS is effective for the hydrogenation of nitro compounds to amines and can be used produce secondary amines via reductive alkylation. The catalyst can also can effect hydrogenolysis of organosulfur compounds, aldehydes, ketones, phenols and carboxylic acids to their respective alkanes. The catalyst suffers from rather low activity however, often requiring hydrogen pressures above 95 atm and temperatures above 185 °C.\n\n and related molybdenum sulfides are efficient catalysts for hydrogen evolution, including the electrolysis of water; thus, are possibly useful to produce hydrogen for use in fuel cells.\n\nAs in graphene, the layered structures of and other transition metal dichalcogenides exhibit electronic and optical properties that can differ from those in bulk. Bulk has an indirect band gap of 1.2 eV, while monolayers have a direct 1.8 eV electronic bandgap, supporting switchable transistors and photodetectors.\n\nThe sensitivity of a graphene field-effect transistor (FET) biosensor is fundamentally restricted by the zero band gap of graphene, which results in increased leakage and reduced sensitivity. In digital electronics, transistors control current flow throughout an integrated circuit and allow for amplification and switching. In biosensing, the physical gate is removed and the binding between embedded receptor molecules and the charged target biomolecules to which they are exposed modulates the current.\n\nMoS has been investigated as a component of flexible circuits.\n\nIn 2017 a 115-transistor, 1-bit microprocessor implementation using two-dimensional .\n\nMoS has been used to create 2D 2-terminal memristors and 3-terminal memtransistors.\n\n also possesses mechanical strength, electrical conductivity, and can emit light, opening possible applications such as photodetectors. has been investigated as a component of photoelectrochemical (e.g. for photocatalytic hydrogen production) applications and for microelectronics applications.\n\n"}
{"id": "7075619", "url": "https://en.wikipedia.org/wiki?curid=7075619", "title": "National oil company", "text": "National oil company\n\nA national oil company (NOC) is an oil and gas company fully or in the majority owned by a national government. According to the World Bank, NOCs accounted for 75% global oil production and controlled 90% of proven oil reserves in 2010.\n\nDue to their increasing dominance over global reserves, the importance of NOCs relative to International Oil Companies (IOCs), such as ExxonMobil, BP, or Royal Dutch Shell, has risen dramatically in recent decades. NOCs are also increasingly investing outside their national borders.\n\n\n"}
{"id": "303071", "url": "https://en.wikipedia.org/wiki?curid=303071", "title": "Neon lamp", "text": "Neon lamp\n\nA neon lamp (also neon glow lamp) is a miniature gas discharge lamp. The lamp typically consists of a small glass capsule that contains a mixture of neon and other gases at a low pressure and two electrodes (an anode and a cathode). When sufficient voltage is applied and sufficient current is supplied between the electrodes, the lamp produces an orange glow discharge. The glowing portion in the lamp is a thin region near the cathode; the larger and much longer neon signs are also glow discharges, but they use the positive column which is not present in the ordinary neon lamp. Neon glow lamps are widely used as indicator lamps in the displays of electronic instruments and appliances.\n\nNeon was discovered in 1898 by William Ramsay and Morris W. Travers. The characteristic, brilliant red color that is emitted by gaseous neon when excited electrically was noted immediately; Travers later wrote, \"the blaze of crimson light from the tube told its own story and was a sight to dwell upon and never forget.\"\n\nNeon's scarcity precluded its prompt application for electrical lighting along the lines of Moore tubes, which used electric discharges in nitrogen. Moore tubes were commercialized by their inventor, Daniel McFarlan Moore, in the early 1900s. After 1902, Georges Claude's company, Air Liquide, was producing industrial quantities of neon as a byproduct of his air liquefaction business, and in December 1910 Claude demonstrated modern neon lighting based on a sealed tube of neon. In 1915 a U.S. patent was issued to Claude covering the design of the electrodes for neon tube lights; this patent became the basis for the monopoly held in the U.S. by his company, Claude Neon Lights, through the early 1930s.\n\nAround 1917, Daniel Moore developed the neon lamp while working at the General Electric Company. The lamp has a very different design from the much larger neon tubes used for neon lighting. The difference in design was sufficient that a U.S. patent was issued for the lamp in 1919. A Smithsonian Institution website notes, \"These small, low power devices use a physical principle called coronal discharge. Moore mounted two electrodes close together in a bulb and added neon or argon gas. The electrodes would glow brightly in red or blue, depending on the gas, and the lamps lasted for years. Since the electrodes could take almost any shape imaginable, a popular application has been fanciful decorative lamps.\n\nGlow lamps found practical use as indicators in instrument panels and in many home appliances until the widespread commercialization of light-emitting diodes (LEDs) in the 1970s.\n\nA small electric current (for a 5 mm bulb diameter NE-2 lamp, the quiescent current is about 400 µA), which may be AC or DC, is allowed through the tube, causing it to glow orange-red. The gas is typically a Penning mixture, 99.5% neon and 0.5% argon, which has lower striking voltage than pure neon, at a pressure of 1-20 torr. The lamp glow discharge lights at its striking voltage. The striking voltage is reduced by ambient light or radioactivity. The voltage required to sustain the discharge is significantly (~30%) lower than the striking voltage. This is due to the organization of positive ions near the cathode. Neon lamps operate using a low current glow discharge. Higher power devices, such as mercury-vapor lamps or metal halide lamps use a higher current arc discharge. Low pressure sodium-vapor lamps use a neon Penning mixture for warm up and can be operated as giant neon lamps if operated in a low power mode.\n\nOnce the neon lamp has reached breakdown, it can support a large current flow. Because of this characteristic, electrical circuitry external to the neon lamp must limit the current through the circuit or else the current will rapidly increase until the lamp is destroyed. For indicator-sized lamps, a resistor typically limits the current. In contrast, larger sized lamps often use a specially constructed high voltage transformer with high leakage inductance or other electrical ballast to limit the available current (see neon sign).\n\nWhen the current through the lamp is lower than the current for the highest-current discharge path, the glow discharge may become unstable and not cover the entire surface of the electrodes. This may be a sign of aging of the indicator bulb, and is exploited in the decorative \"flicker flame\" neon lamps. However, while too low a current causes flickering, too high a current increases the wear of the electrodes by stimulating sputtering, which coats the internal surface of the lamp with metal and causes it to darken.\n\nThe potential needed to strike the discharge is higher than what is needed to sustain the discharge. When there is not enough current, the glow forms around only part of the electrode surface. Convective currents make the glowing areas flow upwards, not unlike the discharge in a Jacob's ladder. A photoionization effect can also be observed here, as the electrode area covered by the glow discharge can be increased by shining light at the lamp.\n\nIn comparison with incandescent light bulbs, neon lamps have much higher luminous efficacy. Incandescence is heat-driven light emission, so a large portion of the electric energy put into an incandescent bulb is converted into heat. Non-incandescent light sources such as neon light bulbs, fluorescent light bulbs, and light emitting diodes are therefore much more energy efficient than normal incandescent light bulbs. Green neon bulbs can produce up to 65 lumens per watt of power input, while white neon bulbs have an efficacy of around 50 lumens per watt. In contrast, a standard incandescent light bulb only produces around 13.5 lumens per watt.\n\nSmall neon lamps are most widely used as visual indicators in electronic equipment and appliances, due to their low power consumption, long life, and ability to operate on mains power.\n\nNeon lamps are commonly used as low-voltage surge protectors, but they are generally inferior to gas discharge tube (GDT) surge protectors (which can be designed for higher voltage applications). Neon lamps have been used as an inexpensive method to protect RF receivers from voltage spikes (lamp connected to RF input and chassis ground), but they are not suitable for higher-power RF transmitters.\n\nMost small neon (indicator-sized) lamps, such as the common NE-2, have a break-down voltage of around 90 volts. When driven from a DC source, only the negatively charged electrode (cathode) will glow. When driven from an AC source, both electrodes will glow (each during alternate half cycles). These attributes make neon bulbs (with series resistors) a convenient low-cost voltage tester. By examining which electrode is glowing they can reveal whether a given voltage source is AC or DC, and if DC, the polarity of the points being tested.\n\nThe breakdown characteristic of glow-discharge lamps allows them to be used as voltage regulators or overvoltage protection devices. Starting around the 1930s, General Electric (GE), Signalite, and other firms made voltage regulator tubes. A voltage regulator tube was used in the Mark 6 exploder.\n\nLike other gas discharge lamps, the neon bulb has negative resistance; its voltage falls with increasing current after the bulb reaches its breakdown voltage. Therefore, the bulb has hysteresis; its turn-off (extinction) voltage is lower than its turn-on (breakdown) voltage. This allows it to be used as an active switching element. Neon bulbs were used to make relaxation oscillator circuits, using this mechanism, sometimes referred to as the Pearson–Anson effect for low frequency applications such as flashing warning lights, stroboscopes tone generators in electronic organs, and as time bases and deflection oscillators in early cathode ray oscilloscopes. Neon bulbs can also be bistable, and were even used to build digital logic circuits such as logic gates, flip-flop, binary memories, and digital counters. These applications were sufficiently common that manufacturers made neon bulbs specifically for this use, sometimes called \"circuit-component\" lamps. At least some of these lamps have a glow concentrated into a small spot on the cathode, which made them unsuited to use as indicators. A variant of the NE-2 type lamp for circuit applications, the NE-77, have three wire electrodes in the bulb (in a plane) instead of the usual two, the third for use as a control electrode.\n\nNeon lamps have been historically used as microwave and millimeter-wave detectors ('plasma diodes' or GDDs- Glow Discharge Detectors) up to about 100 GHz or so and in such service were said to exhibit comparable sensitivity (of the order of a few 10s to perhaps 100 microvolts) to the familiar 1N23-type catwhisker-contacted silicon diodes once ubiquitous in microwave equipment. More recently it has been found that these lamps work well as detectors even at sub-millimeter ('terahertz') frequencies and they have been successfully used as pixels in several experimental imaging arrays at these wavelengths.\n\nIn these applications the lamps are operated either in 'starvation' mode (to reduce lamp-current noise) or in normal glow discharge mode; some literature references their use as detectors of radiation up into the optical regime when operated in abnormal glow mode. Coupling of microwaves into the plasma may be in free space, in waveguide, by means of a parabolic concentrator (e.g., Winston cone), or via capacitive means via a loop or dipole antenna mounted directly to the lamp.\n\nAlthough most of these applications use ordinary off-the-shelf dual-electrode lamps, in one case it was found that special 3 (or more) electrode lamps, with the extra electrode acting as the coupling antenna, provided even better results (lower noise and higher sensitivity). This discovery received a US patent \nNeon lamps with several shaped electrodes were used as alphanumerical displays known as Nixie tubes. These have since been replaced by other display devices such as light emitting diodes, vacuum fluorescent displays, and liquid crystal displays.\n\nSince at least the 1940s, argon, neon, and phosphored \"glow thyratron\" latching indicators (which would light up upon an impulse on their starter electrode and extinguish only after their anode voltage was cut) were available for example as self-displaying shift registers in large-format, crawling-text dot-matrix displays, or, combined in a 4x4, four-color phosphored-thyratron matrix, as a stackable 625-color RGBA pixel for large video graphics arrays.\nMultiple-cathode and/or anode \"glow thyratrons\" called Dekatrons could count forwards and backwards while their count state was visible as a glow on one of the numbered cathodes. These were used as self-displaying divide-by-n counter/timer/prescalers in counting instruments, or as adder/subtracters in calculators.\n\nIn 1930s radio sets, neon lamps were used as tuning indicators, called \"tuneons\" and would give a brighter glow as the station was tuned in correctly.\n\nBecause of their comparatively short response time, in the early development of television neon lamps were used as the light source in many mechanical-scan TV displays.\n\nNovelty glow lamps with shaped electrodes (such as flowers and leaves), often coated with phosphors, have been made for artistic purposes. In some of these, the glow that surrounds an electrode is part of the design.\n\nNeon indicator lamps are normally orange, and are frequently used with a colored filter over them to improve contrast and change their color to red or a redder orange, or less often green.\nThey can also be filled with argon, krypton, or xenon rather than neon, or mixed with it. While the electrical operating characteristics remain similar, these lamps light with a bluish glow (including some ultraviolet) rather than neon's characteristic reddish-orange glow. Ultraviolet radiation then can be used to excite a phosphor coating inside of the bulb and provide a wide range of various colors, including white. A mixture of 95% neon, 2.5% krypton, and 2.5% argon can be used for a green glow, but nevertheless \"green neon\" lamps are more commonly phosphor-based.\n\n\n\n"}
{"id": "24787737", "url": "https://en.wikipedia.org/wiki?curid=24787737", "title": "Operation Free", "text": "Operation Free\n\nOperation Free is a coalition of veterans and national security organizations founded in August 2009 by the Truman National Security Project, The National Security Initiative, VoteVets.org, and VetPAC. Operation Free, in collaboration with other organizations, works to raise awareness, among policymakers and the public, of the national security threat posed by climate change and the United States' reliance on oil.\n\nIn the Fall of 2009, with Alexander Cornell du Houx in charge, Operation Free launched a 29-state bus tour designed to give veterans the opportunity to talk directly to the American people about how climate change is \"undermining our safety and why we need to pass clean energy and climate legislation as quickly as possible.\" In September 2009, Operation Free flew in 150 veterans to attend the White House Administration's Clean Energy Briefing. These veterans met with 26 US Senators and/or Staff to discuss the national security threats of climate change and America's current energy posture. In February 2010, Operation Free hosted a similar event in which it flew 80 veterans into Washington DC to meet with policymakers and members of the press.\n\nOn September 20, 2010 Operation Free announced its sponsorship of race car driver Leilani Munter, who is also a clean energy advocate and environmental activist. On September 30, Leilani’s car (#59) which featured Operation Free’s logo, drove in the ARCA Series Kansas Lottery 150 race at the Kansas Speedway. Sturman Industries cosponsored Leilani in an effort to promote its “Powered By Sturman” American engine modifications that \"address national energy and security needs by using technology to transform engine efficiency and facilitate the nations’ effort to replace foreign oil with alternative American fuels\".\"\n\nThe group has been criticized for \"lending their name, to promote the leftist propaganda of global warming and climate change...\" Rep. Daryl Metcalfe, of Pennsylvania, called any veteran who participated with Operation Free a \"traitor to the oath he or she took to defend the Constitution.\" The group responded with a radio ad campaign calling for Metcalfe's resignation.\n\n\n"}
{"id": "15126504", "url": "https://en.wikipedia.org/wiki?curid=15126504", "title": "Parachor", "text": "Parachor\n\nParachor is a quantity defined according to the formula:\n\nwhere γ is the fourth root of surface tension, M is the molar mass, and d is the density. Parachor has a volume multiplier and is therefore extensible from components to mixtures. Parachor \"has been used in solving various structural problems\".\n"}
{"id": "2899429", "url": "https://en.wikipedia.org/wiki?curid=2899429", "title": "Pellet fuel", "text": "Pellet fuel\n\nPellet fuels (or pellets) are biofuels made from compressed organic matter or biomass. Pellets can be made from any one of five general categories of biomass: industrial waste and co-products, food waste, agricultural residues, energy crops, and virgin lumber. Wood pellets are the most common type of pellet fuel and are generally made from compacted sawdust and related industrial wastes from the milling of lumber, manufacture of wood products and furniture, and construction. Other industrial waste sources include empty fruit bunches, palm kernel shells, coconut shells, and tree tops and branches discarded during logging operations. So-called \"black pellets\" are made of biomass, refined to resemble hard coal and were developed to be used in existing coal-fired power plants. Pellets are categorized by their heating value, moisture and ash content, and dimensions. They can be used as fuels for power generation, commercial or residential heating, and cooking. Pellets are extremely dense and can be produced with a low moisture content (below 10%) that allows them to be burned with a very high combustion efficiency.\n\nFurther, their regular geometry and small size allow automatic feeding with very fine calibration. They can be fed to a burner by auger feeding or by pneumatic conveying. Their high density also permits compact storage and transport over long distance. They can be conveniently blown from a tanker to a storage bunker or silo on a customer's premises.\n\nA broad range of pellet stoves, central heating furnaces, and other heating appliances have been developed and marketed since the mid-1980s. In 1997 fully automatic wood pellet boilers with similar comfort level as oil and gas boilers became available in Austria. With the surge in the price of fossil fuels since 2005, the demand for pellet heating has increased in Europe and North America, and a sizable industry is emerging. According to the International Energy Agency Task 40, wood pellet production has more than doubled between 2006 and 2010 to over 14 million tons. In a 2012 report, the Biomass Energy Resource Center says that it expects wood pellet production in North America to double again in the next five years.\n\nPellets are produced by compressing the wood material which has first passed through a hammer mill to provide a uniform dough-like mass. This mass is fed to a press, where it is squeezed through a die having holes of the size required (normally 6 mm diameter, sometimes 8 mm or larger). The high pressure of the press causes the temperature of the wood to increase greatly, and the lignin plasticizes slightly, forming a natural \"glue\" that holds the pellet together as it cools.\n\nPellets can be made from grass and other non-woody forms of biomass that do not contain lignin: distiller's dried grains (a brewing industry byproduct) can be added to provide the necessary durability. A 2005 news story from Cornell University News suggested that grass pellet production was more advanced in Europe than North America. It suggested the benefits of grass as a feedstock included its short growing time (70 days), and ease of cultivation and processing. The story quoted Jerry Cherney, an agriculture professor at the school, stating that grasses produce 96% of the heat of wood and that \"any mixture of grasses can be used, cut in mid- to late summer, left in the field to leach out minerals, then baled and pelleted. Drying of the hay is not required for pelleting, making the cost of processing less than with wood pelleting.\" In 2012, the Department of Agriculture of Nova Scotia announced as a demonstration project conversion of an oil-fired boiler to grass pellets at a research facility.\n\nRice-husk fuel-pellets are made by compacting rice-husk obtained as by-product of rice-growing from the fields. It also has similar characteristics to the wood-pellets and more environment-friendly, as the raw material is a waste-product. The energy content is about 4-4.2 kcal/kg and moisture content is typically less than 10%. The size of pellets is generally kept to be about 6 mm diameter and 25 mm length in the form of a cylinder; though larger cylinder or briquette forms are not uncommon. It is much cheaper than similar energy-pellets and can be compacted/manufactured from the husk at the farm itself, using cheap machinery. They generally are more environment-friendly as compared to wood-pellets. In the regions of the world where wheat is the predominant food-crop, wheat husk can also be compacted to produce energy-pellets, with characteristics similar to rice-husk pellets.\n\nA report by CORRIM (Consortium On Research on Renewable Industrial Material) for the Life-Cycle Inventory of Wood Pellet Manufacturing and Utilization estimates the energy required to dry, pelletize and transport pellets is less than 11% of the energy content of the pellets if using pre-dried industrial wood waste. If the pellets are made directly from forest material, it takes up to 18% of the energy to dry the wood and additional 8% for transportation and manufacturing energy. An environmental impact assessment of exported wood pellets by the Department of Chemical and Mineral Engineering, University of Bologna, Italy and the Clean Energy Research Centre, at the University of British Columbia, published in 2009, concluded that the energy consumed to ship Canadian wood pellets from Vancouver to Stockholm (15,500 km via the Panama Canal), is about 14% of the total energy content of the wood pellets.\n\nPellets conforming to the norms commonly used in Europe (DIN 51731 or Ö-Norm M-7135) have less than 10% water content, are uniform in density (higher than 1 ton per cubic meter, thus it sinks in water)(bulk density about 0.6-0.7 ton per cubic meter), have good structural strength, and low dust and ash content. Because the wood fibres are broken down by the hammer mill, there is virtually no difference in the finished pellets between different wood types. Pellets can be made from nearly any wood variety, provided the pellet press is equipped with good instrumentation, the differences in feed material can be compensated for in the press regulation.. In Europe, the main production areas are located in south Scandinavia, Finland, Central Europe, Austria, and the Baltic countries.\n\nPellets conforming to the European standards norms which contain recycled wood or outside contaminants are considered Class B pellets. Recycled materials such as particle board, treated or painted wood, melamine resin-coated panels and the like are particularly unsuitable for use in pellets, since they may produce noxious emissions and uncontrolled variations in the burning characteristics of the pellets.\n\nStandards used in the United States are different, developed by the Pellet Fuels Institute and, as in Europe, are not mandatory. Still, many manufacturers comply, as warranties of US-manufactured or imported combustion equipment may not cover damage by pellets non-conformant with regulations. Prices for US pellets surged during the fossil fuel price inflation of 2007–2008, but later dropped markedly and are generally lower on a per-BTU basis than most fossil fuels, excluding coal.\n\nRegulatory agencies in Europe and North America are in the process of tightening the emissions standards for all forms of wood heat, including wood pellets and pellet stoves. These standards will become mandatory, with independently certified testing to ensure compliance. In the United States, the new rules initiated in 2009 have completed the EPA regulatory review process, with final new rules issued for comment on June 24, 2014. The American Lumber Standard Committee will be the independent certification agency for the new pellet standards.\n\nWood pellets, in particular freshly made, are chemically active and can deplete the atmosphere of the oxygen required to sustain life. Wood pellets can also emit large quantities of the poisonous carbon monoxide. Fatal accidents have taken place in private storerooms and onboard marine vessels. When handled, wood pellets give off fine dust which can cause serious dust explosions.\n\nThere are three general types of pellet heating appliances, free standing pellet stoves, pellet stove inserts and pellet boilers. \"Pellet stoves\" \"look like traditional wood stoves but operate more like a modern furnace. [Fuel, wood or other biomass pellets, is stored in a storage bin called a hopper. The hopper can be located on the top of the appliance, the side of it or remotely.] A mechanical auger [automatically feeds] the pellets into a burn pot, where they are incinerated at such a high temperature that they create no vent-clogging creosote and very little ash or emissions… “Heat-exchange tubes”: Send air heated by fire into room… “Convection fan”: Circulates air through heat-exchange tubes and into room… The biggest difference between a pellet stove and … a woodstove, is that, inside, the pellet stove is a high-tech device with a circuit board, a thermostat, and fans—all of which work together to [regulate temperature and] heat your space efficiently.”\n\nA \"pellet stove insert\" is a stove that is inserted into an existing masonry or prefabricated wood fireplace. See Fireplace insert\n\n\"Pellet boilers\" are standalone central heating and hot water systems designed to replace traditional fossil fuel systems in residential, commercial and institutional applications. Automatic or \"auto-pellet boilers\" include silos for bulk storage of pellets, a fuel delivery system that moves the fuel from the silo to the hopper, a logic controller to regulate temperature across multiple heating zones and an automated ash removal system for long-term automated operations.\n\n\"Pellet baskets\" allow a person to heat their home using pellets in existing stoves or fireplaces.\n\nThe energy content of wood pellets is approximately 4.7 – 5.2 MWh/tonne (~7450 BTU/lb).\n\nHigh-efficiency wood pellet stoves and boilers have been developed in recent years, typically offering combustion efficiencies of over 85%. The newest generation of wood pellet boilers can work in condensing mode and therefore achieve 12% higher efficiency values. Wood pellet boilers have limited control over the rate and presence of combustion compared to liquid or gaseous-fired systems; however, for this reason they are better suited for hydronic heating systems due to the hydronic system's greater ability to store heat. Pellet burners capable of being retrofitted to oil-burning boilers are also available.\n\nEmissions such as NO, SO and volatile organic compounds from pellet burning equipment are in general very low in comparison to other forms of combustion heating. A recognized problem is the emission of fine particulate matter to the air, especially in urban areas that have a high concentration of pellet heating systems or coal or oil heating systems in close proximity. This PM emissions of older pellet stoves and boilers can be problematic in close quarters, especially in comparison to natural gas (or renewable biogas), though on large installations electrostatic precipitators, cyclonic separators, or baghouse particle filters can control particulates when properly maintained and operated.\n\nThere is uncertainty to what degree making heat or electricity by burning wood pellets contributes to global climate change, as well as how the impact on climate compares to the impact of using competing sources of heat. Factors in the uncertainty include the wood source, carbon dioxide emissions from production and transport as well as from final combustion, and what time scale is appropriate for the consideration.\n\nA report by the Manomet Center for Conservation Sciences, \"Biomass Sustainability and Carbon Policy Study\" issued in June 2010 for the Massachusetts Department of Energy Resources, concludes that burning biomass such as wood pellets or wood chips releases a large amount of CO2 into the air, creating a \"carbon debt\" that is not retired for 20–25 years and after which there is a net benefit. In June 2011 the department was preparing to file its final regulation, expecting to significantly tighten controls on the use of biomass for energy, including wood pellets. Biomass energy proponents have disputed the Manomet report's conclusions, and scientists have pointed out oversights in the report, suggesting that climate impacts are worse than reported.\n\nUntil ca. 2008 it was commonly assumed, even in scientific papers, that biomass energy (including from wood pellets) is carbon neutral, largely because regrowth of vegetation was believed to recapture and store the carbon that is emitted to the air. Then, scientific papers studying the climate implications of biomass began to appear which refuted the simplistic assumption of its carbon neutrality. According to the Biomass Energy Resource Center, the assumption of carbon neutrality \"has shifted to a recognition that the carbon implications of biomass depend on how the fuel is harvested, from what forest types, what kinds of forest management are applied, and how biomass is used over time and across the landscape.”\n\nIn 2011 twelve prominent U.S. environmental organizations adopted policy setting a high bar for government incentives of biomass energy, including wood pellets. It states in part that, \"[b]iomass sources and facilities qualifying for (government) incentives must result in lower life-cycle, cumulative and net GHG and ocean acidifying emissions, within 20 years and also over the longer term, than the energy sources they replace or compete with.\"\n\nThe wood products industry is concerned that if large-scale use of wood energy is instituted, the supply of raw materials for construction and manufacturing will be significantly curtailed.\n\nDue to the rapid increase in popularity since 2005, pellet availability and cost may be an issue. This is an important consideration when buying a pellet stove, furnace, pellet baskets or other devices known in the industry as Bradley Burners. However, current pellet production is increasing and there are plans to bring several new pellet mills online in the US in 2008–2009.\n\nThe cost of the pellets can be affected by the building cycle leading to fluctuations in the supply of sawdust and offcuts.\n\nPer the New Hampshire Office of Energy and Planning release on Fuel Prices updated on 5 Oct 2015, the cost of #2 Fuel Oil delivered can be compared to the cost of Bulk Delivered Wood Fuel Pellets using their BTU equivalent: 1 ton pellets = 118.97 gallon of #2 Fuel Oil. This assumes that one ton of pellets produces 16,500,000 BTU and one gallon of #2 Fuel Oil produces 138,690 BTU. Thus if #2 Fuel Oil delivered costs $1.90/Gal, the breakeven price for pellets is $238.00/Ton delivered.\n\nUsage across Europe varies due to government regulations. In the Netherlands, Belgium, and the UK, pellets are used mainly in large-scale power plants. The UK's largest power plant, the Drax power station, converted some of its units to pellet burners starting in 2012; by 2015 Drax had made the UK the largest recipient of exports of wood pellets from the US. In Denmark and Sweden, pellets are used in large-scale power plants, medium-scale district heating systems, and small-scale residential heat. In Germany, Austria, Italy, and France, pellets are used mostly for small-scale residential and industrial heat.\n\nThe UK has initiated a grant scheme called the Renewable Heat Incentive (RHI) allowing non-domestic and domestic wood pellet boiler installations to receive payments over a period of between 7–20 years It is the first such scheme in the world and aims to increase the amount of renewable energy generated in the UK, in line with EU commitments. Scotland and Northern Ireland have separate but similar schemes. From Spring 2015, any biomass owners—whether domestic or commercial—must buy their fuels from BSL (Biomass Suppliers List) approved suppliers in order to receive RHI payments.\n\nPellets are widely used in Sweden, the main pellet producer in Europe, mainly as an alternative to oil-fired central heating. In Austria, the leading market for pellet central heating furnaces (relative to its population), it is estimated that of all new domestic heating furnaces are pellet burners. In Italy, a large market for automatically fed pellet stoves has developed. Italy's main usage for pellets is small-scale private residential and industrial boilers for heating.\n\nIn 2014 in Germany the overall wood pellet consumption per year comprised 2,2 mln tones. These pellets are consumed predominantly by residential small scale heating sector. The co-firing plants which use pellet sector for energy production are not widespread in the country. The largest amount of wood pellets is certified with DINplus and these are the pellets of the highest quality. As a rule, the pellets of lower quality are exported.\n\nThe total sales of wood pellets in New Zealand was 3–5,000 tonnes in 2003. Recent construction of new wood pellet plants has given a huge increase in production capacity.\n\nSome companies import European-made boilers. As of 2009, about 800,000 Americans were using wood pellets for heat. It is estimated that 2.33 million tons of wood pellets will be used for heat in the US in 2013. The US wood pellet export to Europe grew from 1.24 million ton in 2006 to 7 million ton in 2012, but forests grew even more.\n\nWhen small amounts of water are added to wood pellets, they expand and revert to sawdust. This makes them suitable to use as a horse bedding. The ease of storage and transportation are additional benefits over traditional bedding. However, some species of wood, including walnut, can be toxic to horses and should never be used for bedding.\n\nIn Thailand, rice husk pellets are being produced for animal bedding. They have a high absorption rate which makes them ideal for the purpose.\n\nWood pellets are also used to absorb contaminated water when drilling oil or gas wells.\n\nWood pellet grills have gained popularity as a versatile way to grill, bake, and smoke. The size of the pellets makes it useful for creating a wood fired grill that still controls its temperature precisely.\n\n\n"}
{"id": "1948002", "url": "https://en.wikipedia.org/wiki?curid=1948002", "title": "Plasma acceleration", "text": "Plasma acceleration\n\nPlasma acceleration is a technique for accelerating charged particles, such as electrons, positrons, and ions, using the electric field associated with electron plasma wave or other high-gradient plasma structures (like shock and sheath fields). The plasma acceleration structures are created either using ultra-short laser pulses or energetic particle beams that are matched to the plasma parameters. These techniques offer a way to build high performance particle accelerators of much smaller size than conventional devices. The basic concepts of plasma acceleration and its possibilities were originally conceived by Toshiki Tajima and Prof. John M. Dawson of UCLA in 1979. The initial experimental designs for a \"wakefield\" accelerator were conceived at UCLA by Prof. Chan Joshi et al. Current experimental devices show accelerating gradients several orders of magnitude better than current particle accelerators over very short distances, and about one order of magnitude better (1 GeV/m vs 0.1 GeV/m for an RF accelerator) at the one meter scale.\n\nPlasma accelerators have immense promise for innovation of affordable and compact accelerators for various applications ranging from high energy physics to medical and industrial applications. Medical applications include betatron and free-electron light sources for diagnostics or radiation therapy and protons sources for hadron therapy. Plasma accelerators generally use wakefields generated by plasma density waves. However, plasma accelerators can operate in many different regimes depending upon the characteristics of the plasmas used.\n\nFor example, an experimental laser plasma accelerator at Lawrence Berkeley National Laboratory accelerates electrons to 1 GeV over about 3.3 cm (5.4x10 g), and one conventional accelerator (highest electron energy accelerator) at SLAC requires 64 m to reach the same energy. Similarly, using plasmas an energy gain of more than 40 GeV was achieved using the SLAC SLC beam (42 GeV) in just 85 cm using a plasma wakefield accelerator (8.9x10 g). Once fully developed, the technology could replace many of the traditional RF accelerators currently found in particle colliders, hospitals, and research facilities.\n\nThe Texas Petawatt laser facility at the University of Texas at Austin accelerated electrons to 2 GeV over about 2 cm (1.6x10 g). This record was broken (by more than 2x) in 2014 by the scientists at the BELLA (laser) Center at the Lawrence Berkeley National Laboratory, when they produced electron beams up to 4.25 GeV.\n\nIn late 2014, researchers from SLAC National Accelerator Laboratory using the Facility for Advanced Accelerator Experimental Tests (FACET) published proof of the viability of plasma acceleration technology. It was shown to be able to achieve 400 to 500 times higher energy transfer compared to a general linear accelerator design.\nA proof-of-principle plasma wakefield accelerator experiment using a 400 GeV proton beam from the Super Proton Synchrotron is currently operating at CERN. The experiment, named AWAKE, started experiments at the end of 2016.\n\nA plasma consists of a fluid of positive and negative charged particles, generally created by heating or photo-ionizing (direct / tunneling / multi-photon / barrier-suppression) a dilute gas. Under normal conditions the plasma will be macroscopically neutral (or quasi-neutral), an equal mix of electrons and ions in equilibrium. However, if a strong enough external electric or electromagnetic field is applied, the plasma electrons, which are very light in comparison to the background ions (by a factor of 1836), will separate spatially from the massive ions creating a charge imbalance in the perturbed region. A particle injected into such a plasma would be accelerated by the charge separation field, but since the magnitude of this separation is generally similar to that of the external field, apparently nothing is gained in comparison to a conventional system that simply applies the field directly to the particle. But, the plasma medium acts as the most efficient transformer (currently known) of the transverse field of an electromagnetic wave into longitudinal fields of a plasma wave. In existing accelerator technology various appropriately designed materials are used to convert from transverse propagating extremely intense fields into longitudinal fields that the particles can get a kick from. This process is achieved using two approaches: standing-wave structures (such as resonant cavities) or traveling-wave structures such as disc-loaded waveguides etc. But, the limitation of materials interacting with higher and higher fields is that they eventually get destroyed through ionization and breakdown. Here the plasma accelerator science provides the breakthrough to generate, sustain, and exploit the highest fields ever produced by science in the laboratory.\n\nWhat makes the system useful is the possibility of introducing waves of very high charge separation that propagate through the plasma similar to the traveling-wave concept in the conventional accelerator. The accelerator thereby phase-locks a particle bunch on a wave and this loaded space-charge wave accelerates them to higher velocities while retaining the bunch properties. Currently, plasma wakes are excited by appropriately shaped laser pulses or electron bunches. Plasma electrons are driven out and away from the center of wake by the ponderomotive force or the electrostatic fields from the exciting fields (electron or laser). Plasma ions are too massive to move significantly and are assumed to be stationary at the time-scales of plasma electron response to the exciting fields. As the exciting fields pass through the plasma, the plasma electrons experience a massive attractive force back to the center of the wake by the positive plasma ions chamber, bubble or column that have remained positioned there, as they were originally in the unexcited plasma. This forms a full wake of an extremely high longitudinal (accelerating) and transverse (focusing) electric field. The positive charge from ions in the charge-separation region then creates a huge gradient between the back of the wake, where there are many electrons, and the middle of the wake, where there are mostly ions. Any electrons in between these two areas will be accelerated (in self-injection mechanism). In the external bunch injection schemes the electrons are strategically injected to arrive at the evacuated region during maximum excursion or expulsion of the plasma electrons.\n\nA beam-driven wake can be created by sending a relativistic proton or electron bunch into an appropriate plasma or gas. In some cases, the gas can be ionized by the electron bunch, so that the electron bunch both creates the plasma and the wake. This requires an electron bunch with relatively high charge and thus strong fields. The high fields of the electron bunch then push the plasma electrons out from the center, creating the wake.\n\nSimilar to a beam-driven wake, a laser pulse can be used to excite the plasma wake. As the pulse travels through the plasma, the electric field of the light separates the electrons and nucleons in the same way that an external field would.\n\nIf the fields are strong enough, all of the ionized plasma electrons can be removed from the center of the wake: this is known as the \"blowout regime\". Although the particles are not moving very quickly during this period, macroscopically it appears that a \"bubble\" of charge is moving through the plasma at close to the speed of light. The bubble is the region cleared of electrons that is thus positively charged, followed by the region where the electrons fall back into the center and is thus negatively charged. This leads to a small area of very strong potential gradient following the laser pulse.\n\nIn the linear regime, plasma electrons aren't completely removed from the center of the wake. In this case, the linear plasma wave equation can be applied. However, the wake appears very similar to the blowout regime, and the physics of acceleration is the same.\n\nIt is this \"wakefield\" that is used for particle acceleration. A particle injected into the plasma near the high-density area will experience an acceleration toward (or away) from it, an acceleration that continues as the wakefield travels through the column, until the particle eventually reaches the speed of the wakefield. Even higher energies can be reached by injecting the particle to travel across the face of the wakefield, much like a surfer can travel at speeds much higher than the wave they surf on by traveling across it. Accelerators designed to take advantage of this technique have been referred to colloquially as \"surfatrons\".\n\nThe advantage of plasma acceleration is that its acceleration field can be much stronger than that of conventional radio-frequency (RF) accelerators. In RF accelerators, the field has an upper limit determined by the threshold for dielectric breakdown of the acceleration tube. This limits the amount of acceleration over any given area, requiring very long accelerators to reach high energies. In contrast, the maximum field in a plasma is defined by mechanical qualities and turbulence, but is generally several orders of magnitude stronger than with RF accelerators. It is hoped that a compact particle accelerator can be created based on plasma acceleration techniques or accelerators for much higher energy can be built, if long accelerators are realizable with an accelerating field of 10 GV/m.\n\nPlasma acceleration is categorized into several types according to how the electron plasma wave is formed:\n\nThe first experimental demonstration of wakefield acceleration, which was performed with PWFA, was reported by a research group at Argonne National Laboratory in 1988.\n\nThe acceleration gradient for a linear plasma wave is:\n\nIn this equation, formula_2 is the electric field, formula_3 is the speed of light in vacuum, formula_4 is the mass of the electron, formula_5 is the plasma electron density (in particles per metre cubed), and formula_6 is the permittivity of free space.\n\nCurrently, plasma-based particle accelerators are in the proof of concept phase at the following institutions:\n\n\n\n"}
{"id": "16013806", "url": "https://en.wikipedia.org/wiki?curid=16013806", "title": "Propellant tank", "text": "Propellant tank\n\nA propellant tank is a container which is part of a vehicle, where propellant is stored prior to use. Propellant tanks vary in construction, and may be a fuel tank in the case of many aircraft.\n\nIn rocket vehicles, propellant tanks are fairly sophisticated since weight is on a premium.\n\nRocket propellant tanks are pressure vessels where liquid fuels are stored prior to use. They have to store the propellant, while minimizing slosh and particularly when the tank is nearly empty, minimizing vortexing.\n\nRocket propellant tanks are often constructed of materials such as aluminium alloys, steels or carbon fibre wound tanks.\n\nThese kinds of tanks are usually constructed using monocoque construction techniques. Balloon tanks are the most extreme of these, they are held rigid only by internal pressurisation, but are extremely lightweight.\nRocket propellant tanks are of many shapes but the optimum shape of a tank is spherical, because for given volume it results in a tank with least weight.\nNormally, propellant in the tank is stored at a pressure of about 1-4 bar, if the system uses turbopump to deliver high pressure to the combustion chamber. This method reduces the wall thickness and hence the weight of the tank.\nIf the propellant in the tank is stored at very high pressure, then the wall thickness of the tank is increased and hence the weight of the tank\n\n Rocket Propulsion Elements by George P. Sutton and Oscar Biblarz\n"}
{"id": "247348", "url": "https://en.wikipedia.org/wiki?curid=247348", "title": "Quintal", "text": "Quintal\n\nThe quintal or centner is a historical unit of mass in many countries which is usually defined as 100 base units of either pounds or kilograms. It is commonly used for grain prices in wholesale markets in India, where 1 quintal = 100 kg.\n\nIn British English, it referred to the hundredweight; in American English, it formerly referred to an uncommon measure of 100 kilograms. \n\nBoth terms share their roots in the Classical Latin \"centenarius\", meaning \"hundredlike\", but the \"quintal\" has a convoluted etymology: It became Late Latin \"centenarium pondus\", then in succession, Byzantine Greek κεντηνάριον (\"kentenarion\") and Arabic \"qintar\" قنطار. The \"qintar\" was reimported to Europe by traders during the Middle Ages, where it became Medieval Latin \"quintale\", and finally Old French \"quintal\" before passing into the English language from French.\n\nThe word \"centner\", on the other hand, is simply a Germanicized form of its original Latin name \"centenarius\".\n\nLanguages drawing its cognate name for the weight from Arabic \"qintar\" include French, Portuguese and Spanish \"quintal\", Italian \"quintale\", Esperanto \"kvintalo\". Languages taking their cognates from Germanicized \"centner\" include German \"Zentner\", Lithuanian \"centneris\", Swedish \"centner\", Polish \"cetnar\", Russian \"центнер\" (tsentner), Ukrainian \"це́нтнер\" (tséntner), Estonian \"tsentner\" and Spanish \"centena\".\n\nMany European languages have come to translate both the imperial and American hundredweight as their cognate form of \"quintal\" or \"centner\".\n\nThe concept has resulted in two different series of masses: Those based on the local pound (which after metrication was considered equivalent to half a kilogram), and those uprated to being based on the kilogram.\n\nIn India and Albania (kuintal), the quintal as equivalent to 100 kilogram was imported via Arabic influence and is a standard measurement of mass for agricultural products.\n\nIn France it used to be defined as 100 \"livres\" (pounds), about 48.95 kg, and has been redefined as 100 kg (\"mesures usuelles\"), thus called \"metric quintal\" with symbol \"qq\".\n\nIn Spain, the \"centena\" is still defined as 100 \"libras\", or about 46 kg, but the \"metric quintal\" is also defined as 100 kg;\n\nIn Portugal a quintal is 128 arrátels or about 58.75 kg.\n\nThe German \"Zentner\" is pound-based, and thus since metrication is defined as 50 kg, whereas the Austrian and Swiss \"Zentner\" since metrication has been re-defined as 100 kg. In Germany a measure of 100 kg is named a Doppelzentner.\n\nCommon agricultural units used in the Soviet Union were the 100-kilogram centner (центнер) and the term \"centner per hectare\". These are still used by countries that were part of the Soviet Union.\n\nIn English both terms \"quintal\" and \"centner\" were once alternative names for the hundredweight and thus defined either as 100 lb (exactly 45.359237 kg) or as 112 lb (about 50.84 kg). Also, in the Dominican Republic it is about 125 lb. The German \"Zentner\" was introduced to the English language via Hanseatic trade as a measure of the weight of certain crops including hops for beer production.\n\nThe quintal was defined in the United States in 1866 as 100 kilograms. However, it is no longer used in USA or by NIST though it still appears in the statute.\n\nIn the Czech Republic, Slovakia, Indonesia and in India, it is still in daily use by farmers. In Brazil and other South American countries, it is used under its alternative spelling of \"kintal\". It is also used in some African countries including Angola.\n\n"}
{"id": "42232333", "url": "https://en.wikipedia.org/wiki?curid=42232333", "title": "Recyclebot", "text": "Recyclebot\n\nA recyclebot (or RecycleBot) is an open-source hardware device for converting waste plastic into filament for open-source 3D printers like the RepRap. Making DIY 3D printer filament at home is both less costly and better for the environment than purchasing conventional 3D printer filament. In following the RepRap tradition there are recyclebot designs that use most 3-D printable parts.\n\nRepRap 3D printers have been shown to reduce costs for consumers by offsetting purchases that can be printed. The RepRap's plastic feedstock is one area where cost can still be reduced. In 2014 professor Joshua Pearce pointed out that \"Filament is retailing for between $36 and $50 a kilogram and you can produce your own filament for 10 cents a kilogram if you use recycled plastic\" The device can thus further enhance RepRap affordability by reducing operating costs. In addition, to assisting prosumers to reduce their reliance on purchased products, following an open source model, the RepRap and the recyclebot, have made it feasible for 3D printing to be used for small-scale manufacturing to aid sustainable development.\n\nThe RecycleBot is an open-source hardware project – thus its plans are freely available on the Internet.\n\nIt has been postulated that recycled filament production could also offer an alternative income source by the Ethical Filament Foundation or as a form of \"fair trade filament\". It has also been shown to improve the energy payback time of even known green energy technologies like solar photovoltaics.\n\nThe history of the RecycleBot was largely derived from the work on the RepRap Wiki under GNU Free Documentation License1.2.\n\nThe first recyclebot was developed by students at Victoria University of Wellington, New Zealand. This design was a proof of concept and was a hand-powered design, which thus had a good environmental or ecological footprint, but did not create filament of high enough quality to be useful for 3D printers. The design for the waste plastic extruder (Recyclebot v2.0 and v2.1) developed at Queen's University Canada and Michigan Tech was heavily influenced by the Web4Deb extruder, which extrudes HDPE for use as a growth medium in aquaponics. This design for the recyclebot was developed, tested and published in the peer-reviewed rapid prototyping literature. This device proved viable for producing 3D printing filament. The Recyclebot v2.2 is now being carried out by the Michigan Tech in Open Sustainability Technology Research Group. Many makers or DIY enthusiasts have made various versions of RecycleBots, with the most notable being the Lyman Filament Extruder as Lyman, a retired engineer won a design contest to make a low-cost 3D filament fabrication system. There are now many types of recyclebots, many of which are at the early stages of commercialization (in 2014).\n\nJeremy Rifkin has hypothesized that such recycling with recyclebots and distributed production with 3D printing will lead to a zero marginal cost society.\nThe science-fiction author, Bruce Sterling wondered in Wired if recyclebots and 3D printers might be used to turn waste into guns. Recyclebots can provide a new method of recycling.\n"}
{"id": "22778103", "url": "https://en.wikipedia.org/wiki?curid=22778103", "title": "Robert Duncan (physicist)", "text": "Robert Duncan (physicist)\n\nRobert V. Duncan is a physicist at Texas Tech University, Texas and previously served as Vice Chancellor for Research at the University of Missouri in Columbia, Missouri. Prior to his current posting he held various assignments while\nserving as a professor of physics at UNM, including associate dean for research in the College of Arts and Sciences there, professor of physics and astronomy at the University of New Mexico, was named as the Gordon and Betty Moore Distinguished Scholar in the Division of Physics, Mathematics and Astronomy at the California Institute of Technology (Caltech), and is a fellow (and life member) of the American Physical Society \"for pioneering advances in experimental studies of dynamic critical phenomena near the superfluid transition in 4He, and for the development of novel instrumentation and measurement techniques for use on earth and in space\".\n\nIn October 2013, Duncan was named vice president for research at Texas Tech University by President M. Duane Nellis. He assumed his new role on January 1, 2014. \n\nDuncan was asked by CBS News Sixty Minutes to investigate cold fusion on their behalf, and his findings were reported on the program in April, 2009. In this program he said that he had abandoned his doubts and was now convinced of the possibilities of cold fusion. He also gave a speech at The Missouri Energy Summit about his findings and on the scientific method.\n"}
{"id": "1706501", "url": "https://en.wikipedia.org/wiki?curid=1706501", "title": "Roselle (plant)", "text": "Roselle (plant)\n\nRoselle (\"Hibiscus sabdariffa\") is a species of \"Hibiscus\" probably native to West Africa, used for the production of bast fibre and as an infusion, in which it may be known as carcade. It is an annual or perennial herb or woody-based subshrub, growing to tall. The leaves are deeply three- to five-lobed, long, arranged alternately on the stems.\n\nThe flowers are in diameter, white to pale yellow with a dark red spot at the base of each petal, and have a stout fleshy calyx at the base, wide, enlarging to , fleshy and bright red as the fruit matures. They take about six months to mature.\n\nThe roselle is known as the \"rosella\" or \"rosella fruit\" in Australia.\nIt is known as Belchanda among Nepalese, Tengamora among Assamese, Gal•da among Garos, Amile among Chakmas, Hanserong among Karbi, Sougri among Meitei and \"mwita\" among the Bodos. The Atongs call it \"dachang\" or \"datchang\". It is called as \"gongura\" by Telugu-speaking people from India. It also has a local name \"Saril\" or \"flor de Jamaica\" in Central America.\n\n\nThe plant is primarily cultivated for the production of bast fibre from the stem. The fibre may be used as a substitute for jute in making burlap. \"Hibiscus\", specifically roselle, has been used in folk medicine as a diuretic and mild laxative.\n\nThe red calyces of the plant are increasingly exported to the United States and Europe, particularly Germany, where they are used as food colourings. It can be found in markets (as flowers or syrup) in places, such as France, where there are Senegalese immigrant communities. The green leaves are used like a spicy version of spinach. They give flavour to the Senegalese fish and rice dish \"thieboudienne\". Proper records are not kept, but the Senegalese government estimates national production and consumption at per year. In Burma their green leaves are the main ingredient in chin baung kyaw curry.\n\nBrazilians attribute stomachic, emollient, and resolutive properties to the bitter roots.\n\nIn Maharashtra, roselle is called Ambadi. The Ambadi leaves are mixed with green chillies, salt, some garlic to prepare a chutney which is served with Jowar or bajra made bhakri. This is eaten by Farmers as breakfast to start their day. A dry vegetable or Sukhi Sabzi made of Ambadi leaves tastes good with Bhakri.\n\nIn Andhra cuisine, roselle is called \"gongura\" and is extensively used. The leaves are steamed with lentils and cooked with dal. Another unique dish is prepared by mixing fried leaves with spices and made into a gongura pacchadi, the most famous dish of Andhra cuisine, often described as king of all Andhra foods.\n\nIn Burmese cuisine, called \"chin baung ywet\" (lit. sour leaf), the roselle is widely used and considered affordable. It is perhaps the most widely eaten and popular vegetable in Burma. The leaves are fried with garlic, dried or fresh prawns and green chili or cooked with fish. A light soup made from roselle leaves and dried prawn stock is also a popular dish.\n\nAmong the Paites tribe of the Manipur \"Hibiscus sabdariffa\" and \"Hibiscus cannabinus\" locally known as \"anthuk\" are cooked along with chicken, fish, crab or pork or any meat, and cooked as a soup as one of their traditional cuisines.\nIn the Khasi Hills of Meghalaya, the plant is locally known as \"jajew\", and the leaves are used in local cuisine, cooked with both dried and fresh fish. The Bodos of north east India cook its leaves with fish, shrimp or pork which is much relished. Sometimes they add native lye called \"karwi\" to bring down its tartness and add flavour.\n\nIn the Philippines, the leaves and flowers are used to add sourness to the chicken dish \"tinola\" (chicken stew).\n\nIn Vietnam, the young leaves, stems and fruits are used for cooking soups with fish or eel.\n\nIn Mali, the dried and ground leaves, also called \"djissima\", are commonly used in Songhaï cuisine, in the regions of Timbuktu, Gao and their surroundings. It is the main ingredient in at least two dishes, one called \"djissima-gounday\", where rice is slowly cooked in a broth containing the leaves and lamb, and the other dish is called \"djissima-mafé\", where the leaves are cooked in a tomato sauce, also including lamb. Note that djissima-gounday is also considered an affordable dish.\n\nIn the Caribbean, sorrel drink is made from sepals of the roselle. It is prepared by boiling dried sepals and calyces of the sorrel/flower of the plant in water for 8 to 10 minutes (or until the water turns red), then adding sugar. It is often served chilled. This is done in St. Vincent and the Grenadines, Trinidad and Tobago, Guyana, Antigua, Barbados, Belize, St. Lucia, Dominica, Grenada, Jamaica and St. Kitts and Nevis where it is called \"sorrel\". The drink is one of several inexpensive beverages (\"aguas frescas\") commonly consumed in Mexico and Central America; they are typically made from fresh fruits, juices or extracts. In Mexican restaurants in the US, the beverage is sometimes known simply as \"Jamaica\" ( HAH-MY-CAH). It is very popular in Trinidad and Tobago especially as a seasonal drink at Christmas where cinnamon, cloves and bay leaves are preferred to ginger. It is also popular in Jamaica, usually flavored with rum.\n\nIn Mali, Senegal, The Gambia, Burkina Faso, Ivory Coast and Benin calyces are used to prepare cold, sweet drinks popular in social events, often mixed with mint leaves, dissolved menthol candy, and/or fruit flavors.\n\nThe Middle Eastern and Sudanese \"Karkade\" (كركديه) is a cold drink made by soaking the dried Karkade calyces in cold water overnight in a refrigerator with sugar and some lemon or lime juice added. It is then consumed with or without ice cubes after the flowers have been strained. In Lebanon, toasted pine nuts are sometimes added.\n\nRoselle is used in Nigeria to make a refreshing drink known as Zobo and natural fruit juices of pineapple and watermelon are added. Ginger is also sometimes added to the refreshing drink.\n\nWith the advent in the U.S. of interest in south-of-the-border cuisine, the calyces are sold in bags usually labeled \"flor de Jamaica\" and have long been available in health food stores in the U.S. for making tea. In addition to being a popular homemade drink, Jarritos, a popular brand of Mexican soft drinks, makes a flor de Jamaica flavored carbonated beverage. Imported Jarritos can be readily found in the U.S.\n\nIn the US, a beverage known as hibiscus cooler is made from the tea, a sweetener, and sometimes juice of apple, grape or lemon. The beverage is sold by some juice companies.\n\nIn the UK, the dried calyces and ready-made sorrel syrup are widely and cheaply available in Caribbean and Asian grocers. The fresh calyces are imported mainly during December and January to make Christmas and New Year infusions, which are often made into cocktails with rum. They are very perishable, rapidly developing fungal rot, and need to be used soon after purchase — unlike the dried product, which has a long shelf-life.\n\nIn Africa, especially the Sahel, roselle is commonly used to make a sugary herbal tea that is sold on the street. The dried flowers can be found in every market. Roselle tea is quite common in Italy where it spread during the first decades of the 20th century as a typical product of the Italian colonies. The Carib Brewery Trinidad Limited, a Trinidad and Tobago brewery, produces a 'Shandy Sorrel' in which the tea is combined with beer.\n\nIn Thailand, roselle is generally drunk as a cool drink, and it can be made into a wine.\n\nHibiscus flowers are commonly found in commercial herbal teas, especially teas advertised as berry-flavoured, as they give a bright red colouring to the drink.\n\nRosella flowers are sold as wild hibiscus flowers in syrup in Australia as a gourmet product. Recipes include filling them with goats cheese; serving them on baguette slices baked with brie; and placing one plus a little syrup in a champagne flute before adding the champagne — the bubbles cause the flower to open.\n\nIn Nigeria, rosella jam has been made since colonial times and is still sold regularly at community fetes and charity stalls. It is similar in flavour to plum jam, although more acidic. It differs from other jams in that the pectin is obtained from boiling the interior buds of the rosella flowers. It is thus possible to make rosella jam with nothing but rosella buds and sugar.\n\nIn Burma, the buds of the roselle are made into 'preserved fruits' or jams. Depending on the method and the preference, the seeds are removed or included. The jams, made from roselle buds and sugar, are red and tangy.\n\nIn India, Roselle is commonly made into a type of pickle.\n\n\"Sorrel jelly\" is manufactured in Trinidad.\n\nRosella jam is made in Queensland, Australia as a home-made or speciality product sold at fetes and other community events.\n\nAlthough a 2010 meta-analysis conducted by the Cochrane hypertension group concluded \"No studies were identified that met the inclusion criteria\" a more recent meta-survey (2015) in the Journal of Hypertension suggests a typical reduction in blood pressure of around 7.5/3.5 units (systolic/diastolic).\n\nChina and Thailand are the largest producers and control much of the world supply. Thailand invested heavily in roselle production, and their product is of superior quality. China's product, with less stringent quality control practices, is less reliable and reputable. The world's best roselle comes from the Sudan, but the quantity is low and poor processing hampers quality. Mexico, Egypt, Senegal, Tanzania, Mali and Jamaica are also important suppliers but production is mostly used domestically.\n\nIn the Indian subcontinent (especially in the Ganges Delta region), roselle is cultivated for vegetable fibres. Roselle is called meśta (or meshta, the \"ś\" indicating an \"sh\" sound) in the region. Most of its fibres are locally consumed. However, the fibre (as well as cuttings or butts) from the roselle plant has great demand in natural fibre using industries.\n\nRoselle is a relatively new crop to create an industry in Malaysia. It was introduced in the early 1990s and its commercial planting was first promoted in 1993 by the Department of Agriculture in Terengganu. The planted acreage was in 1993 and steadily increased to peak at by 2000. The planted area is now less than annually, planted with two main varieties. Terengganu state used to be the first and the largest producer, but now the production has spread more to other states. Despite the dwindling hectarage over the past decade or so, roselle is becoming increasingly known to the general population as an important pro-health drink. To a small extent, the calyces are also processed into sweet pickle, jelly and jam.\n\nIn the initial years, limited research work was conducted by University Malaya and Malaysian Agricultural Research and Development Institute (MARDI). Research work at Universiti Kebangsaan Malaysia (UKM) was initiated in 1999. In many respects, the amount of research work is considered meagre in supporting a growing roselle industry in Malaysia.\n\nGenetic variation is important for plant breeders to increase crop productivity. Being an introduced species in Malaysia, there is a very limited number of germplasm accessions available for breeding.\n\nUKM maintains a working germplasm collection and conducts agronomic research and crop improvement.\n\nConventional hybridization is difficult to carry out in roselle due to its cleistogamous nature of reproduction. Because of this, a mutation breeding programme was initiated to generate new genetic variability. The use of induced mutations for its improvement was initiated in 1999 in cooperation with MINT (now called Malaysian Nuclear Agency) and has produced some promising breeding lines. Roselle is a tetraploid species; thus, segregating populations require longer time to achieve fixation as compared to diploid species. In April 2009, UKM launched three new varieties named UKMR-1, UKMR-2 and UKMR-3. These new varieties were developed using Arab as the parent variety in a mutation breeding programme which started in 2006.\n\nA study was conducted to estimate the amount of outcrossing under local conditions in Malaysia. It was found that outcrossing occurred at a very low rate of about 0.02%. However, this rate is much lower in comparison to estimates of natural cross-pollination of between 0.20% and 0.68% as reported in Jamaica.\n\nThe \"Hibiscus\" leaves are a good source of polyphenolic compounds. The major identified compounds include neochlorogenic acid, chlorogenic acid, cryptochlorogenic acid, caffeoylshikimic acid and flavonoid compounds such as quercetin, kaempferol and their derivatives. The flowers are rich in anthocyanins, as well as protocatechuic acid. The dried calyces contain the flavonoids gossypetin, hibiscetine and sabdaretine. The major pigment, formerly reported as hibiscin, has been identified as daphniphylline. Small amounts of myrtillin (delphinidin 3-monoglucoside), chrysanthenin (cyanidin 3-monoglucoside), and delphinidin are present. Roselle seeds are a good source of lipid-soluble antioxidants, particularly gamma-tocopherol.\n\n\n"}
{"id": "235757", "url": "https://en.wikipedia.org/wiki?curid=235757", "title": "Sensor", "text": "Sensor\n\nIn the broadest definition, a sensor is a device, module, or subsystem whose purpose is to detect events or changes in its environment and send the information to other electronics, frequently a computer processor. A sensor is always used with other electronics.\n\nSensors are used in everyday objects such as touch-sensitive elevator buttons (tactile sensor) and lamps which dim or brighten by touching the base, besides innumerable applications of which most people are never aware. With advances in micromachinery and easy-to-use microcontroller platforms, the uses of sensors have expanded beyond the traditional fields of temperature, pressure or flow measurement, for example into MARG sensors. Moreover, analog sensors such as potentiometers and force-sensing resistors are still widely used. Applications include manufacturing and machinery, airplanes and aerospace, cars, medicine, robotics and many other aspects of our day-to-day life.\n\nA sensor's sensitivity indicates how much the sensor's output changes when the input quantity being measured changes. For instance, if the mercury in a thermometer moves 1  cm when the temperature changes by 1 °C, the sensitivity is 1 cm/°C (it is basically the slope Dy/Dx assuming a linear characteristic). Some sensors can also affect what they measure; for instance, a room temperature thermometer inserted into a hot cup of liquid cools the liquid while the liquid heats the thermometer. Sensors are usually designed to have a small effect on what is measured; making the sensor smaller often improves this and may introduce other advantages. Technological progress allows more and more sensors to be manufactured on a microscopic scale as microsensors using MEMS technology. In most cases, a microsensor reaches a significantly higher speed and sensitivity compared with macroscopic approaches.\n\nA good sensor obeys the following rules: :\n\nMost sensors have a linear transfer function. The sensitivity is then defined as the ratio between the output signal and measured property. For example, if a sensor measures temperature and has a voltage output, the sensitivity is a constant with the units [V/K]. The sensitivity is the slope of the transfer function. Converting the sensor's electrical output (for example V) to the measured units (for example K) requires dividing the electrical output by the slope (or multiplying by its reciprocal). In addition, an offset is frequently added or subtracted. For example, -40 must be added to the output if 0 V output corresponds to -40 C input.\n\nFor an analog sensor signal to be processed, or used in digital equipment, it needs to be converted to a digital signal, using an analog-to-digital converter.\n\nSince sensors cannot replicate an ideal transfer function, several types of deviations can occur which limit sensor accuracy:\n\nAll these deviations can be classified as systematic errors or random errors. Systematic errors can sometimes be compensated for by means of some kind of calibration strategy. Noise is a random error that can be reduced by signal processing, such as filtering, usually at the expense of the dynamic behavior of the sensor.\n\nThe resolution of a sensor is the smallest change it can detect in the quantity that it is measuring. The resolution of a sensor with a digital output is usually the resolution of the digital output. The resolution is related to the precision with which the measurement is made, but they are not the same thing. A sensor's accuracy may be considerably worse than its resolution.\n\nAll living organisms contain biological sensors with functions similar to those of the mechanical devices described. Most of these are specialized cells that are sensitive to:\n\n\nA chemical sensor is a self-contained analytical device that can provide information about the chemical composition of its environment, that is, a liquid or a gas phase. The information is provided in the form of a measurable physical signal that is correlated with the concentration of a certain chemical species (termed as analyte). Two main steps are involved in the functioning of a chemical sensor, namely, recognition and transduction. In the recognition step, analyte molecules interact selectively with receptor molecules or sites included in the structure of the recognition element of the sensor. Consequently, a characteristic physical parameter varies and this variation is reported by means of an integrated transducer that generates the output signal.\nA chemical sensor based on recognition material of biological nature is a biosensor. However, as synthetic biomimetic materials are going to substitute to some extent recognition biomaterials, a sharp distinction between a biosensor and a standard chemical sensor is superfluous. Typical biomimetic materials used in sensor development are molecularly imprinted polymers and aptamers.\n\nIn biomedicine and biotechnology, sensors which detect analytes thanks to a biological component, such as cells, protein, nucleic acid or biomimetic polymers, are called biosensors.\nWhereas a non-biological sensor, even organic (=carbon chemistry), for biological analytes is referred to as sensor or nanosensor. This terminology applies for both in-vitro and in vivo applications.\nThe encapsulation of the biological component in biosensors, presents a slightly different problem that ordinary sensors; this can either be done by means of a semipermeable barrier, such as a dialysis membrane or a hydrogel, or a 3D polymer matrix, which either physically constrains the sensing macromolecule or chemically constrains the macromolecule by bounding it to the scaffold.\n\n\n"}
{"id": "13904860", "url": "https://en.wikipedia.org/wiki?curid=13904860", "title": "Sophia Rabliauskas", "text": "Sophia Rabliauskas\n\nSophia Rabliauskas is an environmental activist and member of the Poplar River First Nation in Manitoba, Canada. She was awarded the Goldman Environmental Prize in 2007 for her efforts to secure protection of two million acres (8,000 km²) of undisturbed forests in the boreal region of Manitoba. \n\nShe was decorated Member of the Order of Manitoba in 2008.\n"}
{"id": "380316", "url": "https://en.wikipedia.org/wiki?curid=380316", "title": "Stanley Pons", "text": "Stanley Pons\n\nBobby Stanley Pons (born August 23, 1943) is an American electrochemist known for his work with Martin Fleischmann on cold fusion in the 1980s and 1990s.\n\nPons was born in Valdese, North Carolina. He attended Valdese High School, then Wake Forest University in Winston-Salem, North Carolina, where he studied chemistry. He began his PhD studies in chemistry at the University of Michigan in Ann Arbor, but left before completing his PhD. His thesis resulted in a paper, co-authored in 1967 with Harry B. Mark, his adviser. \"The New York Times\" wrote that it pioneered a way to measure the spectra of chemical reactions on the surface of an electrode.\n\nHe decided to finish his PhD in England at the University of Southampton, where in 1975 he met Martin Fleischmann. Pons was a student in Professor Alan Bewick's group; he earned his PhD in 1978.\n\nOn March 23, 1989, while Pons was the chairman of the chemistry department at the University of Utah, he and Fleischmann announced the experimental production of \"N-Fusion\", which was quickly labeled by the press as cold fusion. After a short period of public acclaim, hundreds of scientists attempted to reproduce the effects but generally failed. After the claims were found to be unreproducible, the scientific community determined the claims were incomplete and inaccurate.\n\nPons moved to France in 1992, along with Fleischmann, to work at a Toyota-sponsored laboratory. The laboratory closed in 1998 after a £12 million research investment without conclusive results. He gave up his US citizenship and became a French citizen.\n"}
{"id": "649208", "url": "https://en.wikipedia.org/wiki?curid=649208", "title": "Starting fluid", "text": "Starting fluid\n\nStarting fluid is a volatile, flammable liquid which is used to aid the starting of internal combustion engines, especially during cold weather or in engines that are difficult to start using conventional starting procedures. It is typically available in an aerosol spray can, and may sometimes be used for starting direct injected diesel engines or lean burn spark engines running on alcohol fuel. Some modern starting fluid products contain mostly volatile hydrocarbons such as heptane, (the main component of natural gasoline) with a small portion of diethyl ether, and carbon dioxide (as a propellant). Some formulations contain butane or propane as both propellant and starting fuel. Historically, Diethyl ether, with a small amount of oil, a trace amount of a stabilizer and a hydrocarbon propellant has been used to help start internal combustion engines because of its low autoignition temperature. \n\nDiethyl ether is distinct from petroleum ether (a crude oil distillate consisting mostly of pentane and other alkanes) which has also been used for starting engines.\n\nStarting fluid is sprayed into the engine intake near the air filter, or into the carburetor bore or a spark plug hole of an engine to get added fuel to the combustion cylinder quickly. Using starting fluid to get the engine running faster avoids wear to starters and fatigue to one's arm with pull start engines, especially on rarely used machines. Other uses include cold weather starting, vehicles that run out of fuel and thus require extra time to restore fuel pressure, and sometimes with flooded engines. Mechanics sometimes use it to diagnose starting problems by determining whether the spark and ignition system of the vehicle is functioning; if the spark is adequate but the fuel delivery system is not, the engine will run until the starting fluid vapors are consumed. It is used more often with carbureted engines than with fuel injection systems. Caution is required when using starting fluid with diesel engines that have preheat systems in the intake or glow-plugs installed, as the starting fluid may pre-ignite, leading to engine damage.\n\nStarting fluid is not recommended for regular use with some two-stroke engines because it does not possess lubricating qualities by itself. Lubrication for two-stroke engines is achieved using oil that is either mixed into the fuel by the user or injected automatically into the fuel supply; engines requiring premixed fuel that are run solely on starting fluid do not receive an adequate supply of lubrication to their crankcase and cylinder(s). Engines that haven't been run recently are especially vulnerable to damage from oil starvation; starting fluid, a strong solvent, tends to strip residual oil off of cranks and cylinder walls, further reducing lubrication during the period of fuel starvation. WD-40 was previously recommended for use on two stroke engines because it has lubricating qualities, however, the formulation with non-flammable CO as propellant instead of propane no longer has the same propulsive effect.\n\nDiethyl ether has a long history as a medical anesthetic; when starting fluid was mostly ether, a similar effect could be obtained using it. Use at the present time directly as an inhalant includes the effect of the petroleum solvents, which are more toxic as inhalants than diethyl ether.\n\nSometimes referred to as \"passing the shirt,\" the starting fluid is sprayed on a piece of cloth and held up to one's face for inhalation. This trend has gradually picked up since the turn of the century, as phrases such as \"etherized\" and \"ethervision\" have gained popularity. The effects of inhalation vary, but have been known to include lightheadedness, loss of coordination, paranoia, and sometimes hallucinations.\n\n"}
{"id": "23798069", "url": "https://en.wikipedia.org/wiki?curid=23798069", "title": "Steam hammer", "text": "Steam hammer\n\nA steam hammer, also called a drop hammer, is an industrial power hammer driven by steam that is used for tasks such as shaping forgings and driving piles. Typically the hammer is attached to a piston that slides within a fixed cylinder, but in some designs the hammer is attached to a cylinder that slides along a fixed piston.\n\nThe concept of the steam hammer was described by James Watt in 1784, but it was not until 1840 that the first working steam hammer was built to meet the needs of forging increasingly large iron or steel components. In 1843 there was an acrimonious dispute between François Bourdon of France and James Nasmyth of Britain over who had invented the machine. Bourdon had built the first working machine, but Nasmyth claimed it was built from a copy of his design.\n\nSteam hammers proved to be invaluable in many industrial processes. Technical improvements gave greater control over the force delivered, greater longevity, greater efficiency and greater power. A steam hammer built in 1891 by the Bethlehem Iron Company delivered a 125-ton blow. In the 20th century steam hammers were gradually displaced in forging by mechanical and hydraulic presses, but some are still in use. Compressed air power hammers, descendants of the early steam hammers, are still manufactured.\n\nA single-acting steam hammer is raised by the pressure of steam injected into the lower part of a cylinder and drops under gravity when the pressure is released. With the more common double-acting steam hammer, steam is also used to push the ram down, giving a more powerful blow at the die.\nThe weight of the ram may range from .\nThe piece being worked is placed between a bottom die resting on an anvil block and a top die attached to the ram (hammer).\n\nHammers are subject to repeated concussion, which could cause fracturing of cast iron components. The early hammers were therefore made from a number of parts bolted together. \nThis made it cheaper to replace broken parts, and also gave a degree of elasticity that made fracture less likely.\n\nA steam hammer may have one or two supporting frames. The single frame design lets the operator move around the dies more easily, while the double frame can support a more powerful hammer. The frame(s) and the anvil block are mounted on wooden beams that protect the concrete foundations by absorbing the shock.\nDeep foundations are needed, but a large steam drop hammer will still shake the building that holds it. \nThis may be solved with a counterblow steam hammer, in which two converging rams drive the top and bottom dies together. The upper ram is driven down and the lower ram is pulled or driven up. These hammers produce a large impact and can make large forgings.\nThey can be installed with smaller foundations than anvil hammers of similar force.\nCounterblow hammers are not often used in the United States, but are common in Europe.\n\nWith some early steam hammers an operator moved the valves by hand, controlling each blow. With others the valve action was automatic, allowing for rapid repetitive hammering. Automatic hammers could give an elastic blow, where steam cushioned the piston towards the end of the down stroke, or a dead blow with no cushioning. The elastic blow gave a quicker rate of hammering, but less force than the dead blow.\nMachines were built that could run in either mode according to the job requirement.\nThe force of the blow could be controlled by varying the amount of steam introduced to cushion the blow.\nA modern air/steam hammer can deliver up to 300 blows per minute.\n\nThe possibility of a steam hammer was noted by James Watt (1736–1819) in his 28 April 1784 patent for an improved steam engine.\nWatt described \"Heavy Hammers or Stampers, for forging or stamping iron, copper, or other metals, or other matters without the intervention of rotative motions or wheels, by fixing the Hammer or Stamper to be so worked, either directly to the piston or piston rod of the engine.\"\nWatt's design had the cylinder at one end of a wooden beam and the hammer at the other. \nThe hammer did not move vertically, but in the arc of a circle.\nOn 6 June 1806 W. Deverell, engineer of Surrey, filed a patent for a steam-powered hammer or stamper.\nThe hammer would be welded to a piston rod contained in a cylinder. Steam from a boiler would be let in under the piston, raising it and compressing the air above it. The steam would then be released and the compressed air would force the piston down.\n\nIn August 1827 John Hague was awarded a patent for a method of working cranes and tilt-hammers driven by a piston in an oscillating cylinder where air power supplied the motive force. A partial vacuum was made in one end of a long cylinder by an air pump worked by a steam engine or some other power source, and atmospheric pressure drove the piston into that end of the cylinder. When a valve was reversed, the vacuum was formed in the other end and the piston forced in the opposite direction.\nHague made a hammer to this design for planishing frying pans. Many years later, when discussing the advantages of air over steam for delivering power, it was recalled that Hague's air hammer \"worked with such an extraordinary rapidity that it was impossible to see where the hammer was in working, and the effect was seemed more like giving one continuous pressure.\" However, it was not possible to regulate the force of the blows.\n\nIt seems probable that the Scottish Engineer James Nasmyth (1808–1890) and his French counterpart François Bourdon (1797–1865) reinvented the steam hammer independently in 1839, both trying to solve the same problem of forging shafts and cranks for the increasingly large steam engines used in locomotives and paddle boats.\nIn Nasmyth's 1883 \"autobiography\", written by Samuel Smiles, he described how the need arose for a paddle shaft for Isambard Kingdom Brunel's new translatlantic steamer \"SS Great Britain\", with a diameter shaft, larger than any that had been previously forged. He came up with his steam hammer design, making a sketch dated 24 November 1839, but the immediate need disappeared when the practicality of screw propellers was demonstrated and the \"Great Britain\" was converted to that design.\nNasmyth showed his design to all visitors.\n\nBourdon came up with the idea of what he called a \"Pilon\" in 1839 and made detailed drawings of his design, which he also showed to all engineers who visited the works at Le Creusot owned by the brothers Adolphe and Eugène Schneider.\nHowever, the Schneiders hesitated to build Bourdon's radical new machine.\nBourdon and Eugène Schneider visited the Nasmyth works in England in the middle of 1840, where they were shown Nasmyth's sketch.\nThis confirmed the feasibility of the concept to Schneider. \nIn 1840 Bourdon built the first steam hammer in the world at the Schneider & Cie works at Le Creusot.\nIt weighed and lifted to . The Schneiders patented the design in 1841.\n\nNasmyth visited Le Creusot in April 1842. By his account, Bourdon took him to the forge department so he might, as he said, \"see his own child\". Nasmyth said \"there it was, in truth–a thumping child of my brain!\"\nAfter returning from France in 1842 Nasmyth built his first steam hammer in his Patricroft foundry in Manchester, England, adjacent to the (then new) Liverpool and Manchester Railway and the Bridgewater Canal.\nIn 1843 a dispute broke out between Nasmyth and Bourdon over priority of invention of the steam hammer. Nasmyth, an excellent publicist, managed to convince many people that he was the first.\n\nNasmyth's first steam hammer, described in his patent of 9 December 1842, was built for the Low Moor Works at Bradford.\nThey rejected the machine, but on 18 August 1843 accepted an improved version with a self-acting gear.\nRobert Wilson (1803–1882), who had also invented the screw propeller and was manager of Nasmyth's Bridgewater works, invented the self-acting motion that made it possible to adjust the force of the blow delivered by the hammer – a critically important improvement.\nAn early writer said of Wilson's gear, \"... I would be prouder to say that I was the inventor of that motion, then to say I had commanded a regiment at Waterloo...\"\nNasmyth's steam hammers could now vary the force of the blow across a wide range. \nNasmyth was fond of breaking an egg placed in a wineglass without breaking the glass, followed by a blow that shook the building.\n\nBy 1868 engineers had introduced further improvements to the original design. John Condie's steam hammer, built for Fulton in Glasgow, had a stationary piston and a moving cylinder to which the hammer was attached. The piston was hollow, and was used to deliver steam to the cylinder and then remove it. \nThe hammer weighed 6.5 tons with a stroke of .\nCondie steam hammers were used to forge the shafts of Isambard Kingdom Brunel's \"SS Great Eastern\".\nA high-speed compressed-air hammer was described in \"The Mechanics' Magazine\" in 1865, \na variant of the steam hammer for use where steam power was not available or a very dry environment was required.\n\nThe Bowling Ironworks steam hammers had the steam cylinder bolted to the back of the hammer, thus reducing the height of the machine.\nThese were designed by John Charles Pearce, who took out a patent for his steam hammer design several years before Nasmyth's patent expired.\nMarie-Joseph Farcot of Paris proposed a number of improvements including an arrangement so the steam acted from above, increasing the striking force, improved valve arrangements and the use of springs and material to absorb the shock and prevent breakage.\nJohn Ramsbottom invented a duplex hammer, with two rams moving horizontally towards a forging placed between them.\n\nUsing the same principles of operation, Nasmyth developed a steam-powered pile-driving machine. At its first use at Devonport, a dramatic contest was carried out. His engine drove a pile in four and half minutes compared to the twelve hours that the conventional method required.\nIt was soon found that a hammer with a relatively short fall height was more effective than a taller machine. The shorter machine could deliver many more blows in a given time, driving the pile faster even though each blow was smaller. It also caused less damage to the pile.\n\nRiveting machines designed by Garforth and Cook were based on the steam hammer.\nThe catalog for the Great Exhibition held in London in 1851 said of Garforth's design, \"With this machine, one man and three boys can rivet with perfect ease, and in the firmest manner, at the rate of six rivets per minute, or three hundred and sixty per hour.\"\nOther variants included crushers to help extract iron ore from quartz and a hammer to drive holes in the rock of a quarry to hold gunpowder charges.\nAn 1883 book on modern steam practice said \n\nSchneider & Co. built 110 steam hammers between 1843 and 1867 with different sizes and strike rates, but trending towards ever larger machines to handle the demands of large cannon, engine shafts and armor plate, with steel increasingly used in place of wrought iron.\nIn 1861 the \"Fritz\" steam hammer came into operation at the Krupp works in Essen, Germany.\nWith a 50-ton blow, for many years it was the most powerful in the world.\n\nThere is a story that the Fritz steam hammer took its name from a machinist named Fritz whom Alfred Krupp presented to the Emperor William when he visited the works in 1877. Krupp told the emperor that Fritz had such perfect control of the machine that he could let the hammer drop without harming an object placed on the center of the block. The Emperor immediately put his watch, which was studded with diamonds, on the block and motioned Fritz to start the hammer. When the machinist hesitated, Krupp told him \"Fritz let fly!\" He did as he was told, the watch was unharmed, and the emperor gave Fritz the watch as a gift. Krupp had the words \"Fritz let fly!\" engraved on the hammer.\n\nThe Schneiders eventually saw a need for a hammer of colossal proportions.\nThe Creusot steam hammer was a giant steam hammer built in 1877 by Schneider and Co. in the French industrial town of Le Creusot. \nWith the ability to deliver a blow of up to 100 tons, the Creusot hammer was the largest and most powerful in the world.\nA wooden replica was built for the Exposition Universelle (1878) in Paris.\nIn 1891 the Bethlehem Iron Company of the United States purchased patent rights from Schneider and built a steam hammer of almost identical design but capable of delivering a 125-ton blow.\n\nEventually the great steam hammers became obsolete, displaced by hydraulic and mechanical presses. The presses applied force slowly and at a uniform rate, ensuring that the internal structure of the forging was uniform, without hidden internal flaws.\nThe 1877 Creusot steam hammer now stands as a monument in the Creusot town square.\nAn original Nasmyth hammer stands facing his foundry buildings (now a 'business park'). \nA larger Nasmyth & Wilson steam hammer stands in the campus of the University of Bolton.\n\nSteam hammers continue to be used for driving piles into the ground.\nSteam supplied by a circulating steam generator is more efficient than air.\nHowever, today compressed air is often used rather than steam.\nAs of 2013 manufacturers continued to sell air/steam pile-driving hammers.\nForging services suppliers also continue to use steam hammers of varying sizes based on classical designs.\n\n\nCitations\n\nSources\n\nExternal links\n\n"}
{"id": "45509403", "url": "https://en.wikipedia.org/wiki?curid=45509403", "title": "Thermoelectric battery", "text": "Thermoelectric battery\n\nA thermoelectric battery stores energy when charged by converting heat into chemical energy and produces electricity when discharged. Such systems potentially offer an alternative means of disposing of waste heat from plants that burn fossil fuels and/or nuclear energy.\n\nThomas Johann Seebeck (1780-1831) discovered the thermoelectric effect in 1821. The symmetrical Peltier Effect (Jean Charles Athanse Peltier, 1785-1845) uses an electric current to produce temperature differences. In the middle part of the twentieth century the thermo-electric generator was often used in place of galvanic batteries.\n\nIn 2014 researchers demonstrated a prototype system that uses copper electrodes and ammonia as the electrolyte. The device converted some 29 percent of the battery's chemical energy into electricity.\n\nThe ammonia electrolyte is only used as an anolyte (electrolyte surrounding an anode) that reacts with the copper electrode as waste heat warms the ammonia, generating electricity. When the reaction uses up the ammonia or depletes the copper ions in the electrolyte near the cathode the reaction stops.\n\nWaste heat then is used to distill the ammonia from the used anolyte. The ammonia is then added to the cathode chamber. The battery's polarity reverses and the anode becomes the cathode and vice versa.\n\nThe system's power density was some maximum power density of 60+-3 1 W m(based on a single electrode), with a maximum energy density of 453 W h m(normalized to the electrolyte volume), substantially higher than that of other liquid-centered thermal-electrics. Power density increased with the number of batteries in the system.\n\nVolatilization of ammonia from the spent anolyte by heating (simulating distillation), and re-addition of this ammonia to the spent catholyte chamber with subsequent operation of this chamber as the anode (to regenerate copper on the other electrode), produced a maximum power density of 60 ± 3 W m, with an average discharge energy efficiency of 29% (electrical energy captured \"versus\" chemical energy in the starting solutions). An acid added to the catholyte increased power 126 ± 5 W m.\n\nTelluride based batteries convert 15 to 20 percent of heat to energy.\n\nFulvalene diruthenium promises greater efficiency, but is too expensive for commercial use.\n\n"}
{"id": "22520022", "url": "https://en.wikipedia.org/wiki?curid=22520022", "title": "Tungsten borides", "text": "Tungsten borides\n\nTungsten borides are compounds of tungsten and boron. Their most remarkable property is high hardness. The Vickers hardness of WB or WB crystals is ~20 GPa and that of WB is ~30 GPa for loads exceeding 3 N.\n\nSingle crystals of WB, \"x\"=0.07–0.17 (about 1 cm diameter, 6 cm length) were produced by the floating zone method, and WB crystals can be grown by arc-melting a mixture of elemental tungsten and boron.\n\nWB has the same hexagonal structure as most diborides (AlB, MgB, etc.). WB has several forms, α(tetragonal), β (orthorhombic) and δ (tetragonal).\n\nδ-WB and WB crystals have metallic resistivities of 0.1 and 0.3 mΩ·cm, respectively. The oxidation of WB, WB and WB is significant at temperatures above 600 °C. The final oxidation products contain WO and probably amorphous BO or HBO. The melting temperatures of WB, WB and WB are 2670, 2655 and 2365 °C, respectively.\n"}
{"id": "25829617", "url": "https://en.wikipedia.org/wiki?curid=25829617", "title": "Vector inversion generator", "text": "Vector inversion generator\n\nA vector inversion generator (VIG) is an electric pulse compression and voltage multiplication device, allowing shaping a slower, lower voltage pulse to a narrower, higher-voltage one. VIGs are used in military technology, e.g. some directed-energy weapons, as a secondary stage of another pulsed power source, commonly an explosive-driven ferroelectric generator.\n\nDiscrete component VIGs (pictured) consist of a stack of well-coupled common mode chokes interconnected with a stack of capacitors. The inductors present a high inductance to currents that are in-phase in the two windings, and a far lower inductance when the winding currents are flowing in opposite directions. The capacitors are charged with alternating polarity and when the switch (usually a triggered or free running spark gap in practice) is closed the voltage across every second capacitor rapidly inverts as a half cycle of oscillation at a frequency set by the capacitance resonating with the differential mode inductance of the chokes. At the same time the other capacitors discharge very slowly due to not having a differential current flowing to cancel the reactance.\nSo after a half period, all the capacitors are in series and the voltages add. \nThis arrangement has a conceptual equivalence to the spiral VIG, with the alternating capacitors being equivalent to the capacitance between the windings and the common mode chokes being equivalent to the inductance of a winding.\nDiscrete components allow large lumped capacitors to be used thus storing much more energy, but have difficulty replicating the high voltage multiplication ratios and extremely short rise times of spiral transmission line types.\n\nA spiral VIG consists of four alternating conductor-insulator-conductor-insulator sheets, wound into a cylinder, forming a capacitor also acting as a single-ended transmission line, connected to a spark gap switch. The capacitor is charged from a power source, e.g. an EDFEG, then the spark gap fires after its breakdown voltage is reached. The electromagnetic wave created by the electric spark discharge travels along the transmission line, converting electrostatic field to electromagnetic field, then after reflecting from the open end converts back to electrostatic field. A pulse of output amplitude 2nU (where n is the number of turns of the capacitor and U is the initial voltage it was charged to) and a rise time equal to twice the electrical length of the transmission line. The device acts as a distributed pulse forming network.\n\nFerrites can be attached to the VIG construction to modify its characteristics.\n\nVIGs are advantageous due to their simplicity and the very short pulse rise times in range of nanoseconds. Some VIGs can be configured as part of a tuned circuit, acting as oscillators with practical upper limit of about 700 MHz, generating energy that can be radiated from a suitable antenna, allowing construction of very simple explosion-generated electromagnetic pulse generators.\n\nThe use of VIGs includes directed-energy weapons, x-ray pulse power supplies, plasma generators, etc.\n\nVIGs are simple enough to be constructed by high-voltage hobbyists.\n"}
{"id": "22561373", "url": "https://en.wikipedia.org/wiki?curid=22561373", "title": "Very long chain fatty acid", "text": "Very long chain fatty acid\n\nA very long chain fatty acid (VLCFA) is a fatty acid with 22 or more carbons. Their biosynthesis occurs in the endoplasmic reticulum. VLCFA's can represent up to a few percent of the total fatty acid content of a cell.\n\nUnlike most fatty acids, VLCFAs are too long to be metabolized in the mitochondria, and must be metabolized in peroxisomes.\n\nCertain peroxisomal disorders, such as adrenoleukodystrophy and Zellweger syndrome, can be associated with an accumulation of VLCFAs.\n\n"}
{"id": "39915658", "url": "https://en.wikipedia.org/wiki?curid=39915658", "title": "Vintage musical equipment", "text": "Vintage musical equipment\n\nVintage musical equipment is older music gear, including instruments, amplifiers and speakers, sound recording equipment and effects pedals, sought after, maintained and used by record producers, audio engineers and musicians who are interested in historical music genres. While any piece of equipment of sufficient age can be considered vintage, in the 2010s the term is typically applied to instruments and gear from the 1970s and earlier. Guitars, amps, pedals, electric keyboards, sound recording equipment (e.g., reel to reel tape decks and microphones) from the 1950s to 1970s are particularly sought. Musical equipment from the 1940s and prior eras is often expensive, and sought out mainly by museums or collectors.\n\nOlder gear is often known for its unique tonal or sound-shaping qualities. The cost of vintage gear may be higher than the reissued model or its 2010s-era equivalent, depending on the rarity of the item, how high the demand is for it, and the condition. \n\nAs early as the 1970s, musicians began to recognize the value of older instruments from the 1940s and 1950s. Among guitar aficionados, the mass production of both acoustic and electric guitars served to highlight the quality hand workmanship, crafting, finishing and materials of older instruments. Historians such as George Gruhn helped to codify both the monetary value and sound quality of these instruments.\n\nExamples of well-known vintage electric guitars include 1950s and 1960s era models like the Fender Stratocaster and Telecaster, and the Gibson Les Paul. Although less well-known and not as financially valuable, older electric guitars under the names of Harmony, Danelectro or Kay are becoming increasingly collectible.\n\nExamples of well-known vintage acoustic guitars include Martin and Gibson models typically built prior to the 1970s, 1920s to 1930s Nationals and Dobros, and 1930s era Recording Kings, among others.\n\nPrior to the popularity of electronic music in the 1980s and 1990s, electro-mechanical instruments such as the Fender Rhodes electric piano and the Hammond organ were widely used. The Hammond organ was a staple instrument in jazz, blues and early rock and roll up through the 1970s. Booker T. Jones famously played in on many recordings for Stax Records that helped define the sound of soul music in the 1960s that later musicians would want to achieve by adopting a Hammond of their own.\n\nMusical synthesizers first came to popular music in the mid- to late-1960s and evolved through the 1970s and 1980s. In the 1970s, synthesizers were rare and expensive, and they were generally only owned by major recording studios, record producers and celebrity artists (e.g., Stevie Wonder). It was not until the release of Yamaha's DX-7 that an affordable, mass-market digital synth became available to the general public. Because the technology was changing so quickly, many synthesizers were manufactured for a very short period of time and would later by sought after by musicians and collectors seeking unique or unusual sounds. Popular brands of vintage synthesizers include Moog, Korg, ARP, and later Roland and Yamaha\n\nThe first guitar amplifiers were made in the 1920s and 1930s using vacuum tubes and speakers to amplify an instrument's sound. These tube amps remained the standard until the 1970s when transistors became cheaper to manufacture and maintain and lighter in weight. During the 1980s, when most guitar amps being manufactured used \"solid state\" semiconductor technology, many musicians seeking an older style of sound favored older amps that used vacuum tubes (called \"valves\" in the UK). Popular vintage models include the Fender Showman, Bassman and Vibroverb amps, and older models made by Ampeg, Gibson, Marshall, and Vox, as well as other smaller companies such as Valco, Danelectro, and Premier. By the 1990s, many of these amplifiers had become so popular and sought after, that manufacturers began to reissue some models, while newer, smaller companies built new amps that boasted a \"vintage sound\". Some recording studios have a selection of the most popular vintage guitar combo amps, amp heads and speaker stacks, so that performers can get a retro sound.\n\nWhen electronic transistors began to replace vacuum tubes, it became possible to fit aural effects circuits into a portable device. These first effects pedals —or \"stompboxes\", so called because a guitarist would stomp on one to activate it—were manufactured in the early 1960s and became popular through groups like The Kinks and The Rolling Stones by the middle of the decade. Early pedals mainly created a distortion or \"fuzz-tone\" effect, but with the arise of psychedelic rock, more esoteric effects became popular. Warwick Electronics manufactured the first wah-wah pedal in 1967 and that same year Roger Mayer issued the first octave effect,\n\nThe following year saw the arrival of Univox's phase shifter and chorus effect. These pedals became favorite effects of guitarists like Jimi Hendrix and Robin Trower. By the mid-1970s a variety of solid-state effects pedals including flangers, chorus pedals, ring modulators and phase shifters were available.\n\nWhile digitized rack units became the standard for popular artists in the 1980s, older effects pedals were preferred by punk and garage rock bands. Seattle grunge rockers Mudhoney celebrated these roots on their 1988 EP \"Superfuzz Bigmuff\", named for two of the band's favorite guitar effects pedals: the Univox Super-Fuzz and the Electro-Harmonix Big Muff, which helped to provide the band's signature \"dirty\" sound. When fellow grungers Nirvana made it big in 1991', interest in vintage pedals grew among their fans.\n\n\n"}
{"id": "3573161", "url": "https://en.wikipedia.org/wiki?curid=3573161", "title": "Water tunnel (hydrodynamic)", "text": "Water tunnel (hydrodynamic)\n\nA water tunnel is an experimental facility used for testing the hydrodynamic behavior of submerged bodies in flowing water. It is very similar to a recirculating wind tunnel but with water as the working fluid, and related phenomena are investigated, such as measuring the forces on scale models of submarines or lift and drag on hydrofoils. Water tunnels are sometimes used in place of wind tunnels to perform measurements because techniques like particle image velocimetry (PIV) are easier to implement in water. For many cases as long as the Reynolds number is equivalent, the results are valid, whether a submerged water vehicle model is tested in air or an aerial vehicle is tested in water. For low Reynolds number flows, tunnels can be made to run oil instead of water. The advantage is that the increased kinematic viscosity will allow the flow to be a faster speed (and thus easier to maintain stably) for a lower Reynolds number.\n\nWhereas in wind tunnels the driving force is usually sophisticated multiblade propellers with adjustable blade pitch, in water and oil tunnels the fluid is circulated with pumps, effectively using a net pressure head difference to move the fluid rather than imparting momentum on it directly. Thus the return section of water and oil tunnels does not need any flow management; typically it is just a pipe sized for the pump and desired flow speeds. The upstream section of a water tunnels generally consists of a pipe (outlet from the pump) with several holes along its side and with the end open followed by a series of coarse and fine screens to even the flow before the contraction into the test section. Wind tunnels may also have screens before the contraction, but in water tunnels they may be as fine as the screen used in window openings and screen doors.\n\nAdditionally, many water tunnels are sealed and can reduce or increase the internal static pressure, to perform cavitation studies. These are referred to as cavitation tunnels.\n\nBecause it is a high-speed phenomenon, a special procedure is needed to visualize cavitation. The propeller, attached to a dynamometer, is placed in the inflow, and its thrust and torque is measured at different ratios of propeller speed (number of revolutions) to inflow velocity. A stroboscope synchronized with the propeller speed \"freezes\" the cavitation bubble. By this, one can observe if the propeller would be damaged by cavitation. To ensure similarity to the full-scale propeller, the pressure is lowered, and the gas content of the water is controlled.\n\nOften, a tunnel will be co-located with other experimental facilities such as a wave flume at a Ship model basin.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
