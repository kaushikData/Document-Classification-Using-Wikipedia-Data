{"id": "8619036", "url": "https://en.wikipedia.org/wiki?curid=8619036", "title": "2006 Idaho Proposition 2", "text": "2006 Idaho Proposition 2\n\n\"For other articles concerning \"Proposition 2\", see Proposition 2 (disambiguation)\"\n\nIdaho Proposition 2 was a 2006 ballot initiative in the state of Idaho, U.S. that aimed to force government to reimburse property owners whose property value is decreased as a result of government regulation.\n\nThe initiative, which is similar to the controversial Oregon Ballot Measure 37 (2004), was defeated. \n\n \n"}
{"id": "8928747", "url": "https://en.wikipedia.org/wiki?curid=8928747", "title": "4.2 kiloyear event", "text": "4.2 kiloyear event\n\nThe 4.2-kiloyear BP aridification event was one of the most severe climatic events of the Holocene period. It defines the beginning of the current Meghalayan age in the Holocene epoch. Starting in about 2200 BC, it probably lasted the entire 22nd century BC. It has been hypothesised to have caused the collapse of the Old Kingdom in Egypt as well as the Akkadian Empire in Mesopotamia, and the Liangzhu culture in the lower Yangtze River area. The drought may also have initiated the collapse of the Indus Valley Civilisation, with some of its population moving southeastward to follow the movement of their desired habitat, as well as the migration of Indo-European speaking people into India.\n\n A phase of intense aridity about 4.2 ka BP is recorded across North Africa, the Middle East, the Red Sea, the Arabian Peninsula, the Indian subcontinent, and midcontinental North America. Glaciers throughout the mountain ranges of western Canada advanced at about this time. Evidence has also been found in an Italian cave flowstone, the Kilimanjaro Ice sheet, and in Andean glacier ice. The onset of the aridification in Mesopotamia about 4100 BP also coincided with a cooling event in the North Atlantic, known as \"Bond event 3\". Despite this, evidence for the 4.2 kyr event in northern Europe is ambiguous, suggesting the origin and effect of this event is spatially complex.\n\nIn 2018, the International Commission on Stratigraphy divided the Holocene epoch into three, with the late Holocene being called the \"Meghalayan stage/age\" starting around 2250 BC. The boundary stratotype is a speleothem in Mawmluh cave in India, and the global auxiliary stratotype is an ice core from Mount Logan in Canada.\n\nOn the Iberian Peninsula, the construction of motillas-type settlements in the period after 2200 BC is believed to be the consequence of the severe aridification that affected this area.\n\nAccording to Moreno et al., who reported the first palaeohydrogeological interdisciplinary research in La Mancha, Spain,\nThe authors' analysis verified a relationship between the geological substrate and the spatial distribution of the motillas.\n\nIn c. 2150 BC, the Old Kingdom was hit by a series of exceptionally low Nile floods. This may have influenced the collapse of centralised government in ancient Egypt. Contemporary texts claim that famines, social disorder and fragmentation subsequently occurred. \n\nIn the Persian Gulf region, there is a sudden change in settlement pattern, style of pottery and tombs at this time. The 22nd century BCE drought marks the end of the Umm al-Nar Culture and the change to the Wadi Suq culture.\n\nThe aridification of Mesopotamia may have been related to the onset of cooler sea-surface temperatures in the North Atlantic (Bond event 3), as analysis of the modern instrumental record shows that large (50%) interannual reductions in Mesopotamian water supply result when subpolar northwest Atlantic sea surface temperatures are anomalously cool. The headwaters of the Tigris and Euphrates Rivers are fed by elevation-induced capture of winter Mediterranean rainfall.\n\nThe Akkadian Empire, in 2300 BCE, was the second civilisation to subsume independent societies into a single state (the first being ancient Egypt around 3100 BCE). It has been claimed that the collapse of the state was influenced by a wide-ranging, centuries-long drought. Archaeological evidence documents widespread abandonment of the agricultural plains of northern Mesopotamia and dramatic influxes of refugees into southern Mesopotamia, around 2170 BCE. A 180-km-long wall, the \"Repeller of the Amorites,\" was built across central Mesopotamia to stem nomadic incursions to the south. Around 2150 BCE, the Gutian people, who originally inhabited the Zagros Mountains, defeated the demoralised Akkadian army, took Akkad, and destroyed it around 2115 BCE. Widespread agricultural change in the Near East is visible at the end of the 3rd millennium BCE.\n\nResettlement of the northern plains by smaller sedentary populations occurred near 1900 BCE, three centuries after the collapse.\n\nIn the 2nd millennium BCE widespread aridification occurred in the Eurasian steppes and south Asia. On the steppes, the vegetation changed, driving \"higher mobility and transition to the nomadic cattle breeding.\" Water shortage also strongly affected south Asia: \nUrban centers of the Indus Valley Civilisation were abandoned and were replaced by disparate local cultures, due to the same climate change that affected the neighbouring areas of the Middle East. many scholars believe that drought and a decline in trade with Egypt and Mesopotamia caused the collapse of the Indus Civilisation. The Ghaggar-Hakra system was rain-fed, and water supply depended on the monsoons. The Indus valley climate grew significantly cooler and drier from about 1800 BCE, linked to a general weakening of the monsoon at that time. The Indian monsoon declined and aridity increased, with the Ghaggar-Hakra retracting its reach towards the foothills of the Himalaya, leading to erratic and less extensive floods that made inundation agriculture less sustainable. Aridification reduced the water supply enough to cause the civilisation's demise, and to scatter its population eastward.\n\nThe drought may have caused the collapse of Neolithic Cultures around Central China during the late 3rd millennium BCE. At the same time, the middle reaches of the Yellow River saw a series of extraordinary floods related to the legendary figure of Yu the Great. In the Yishu River Basin, the flourishing Longshan culture was affected by a cooling that severely reduced rice output. This led to substantial decrease in population and fewer archaeological sites. In about 2000 BCE, Longshan was displaced by the Yueshi culture, which had fewer and less sophisticated artifacts of ceramic and bronze.\n\n\n\n"}
{"id": "20559473", "url": "https://en.wikipedia.org/wiki?curid=20559473", "title": "Arnold Power Station", "text": "Arnold Power Station\n\nThe Arnold Power Station is a hydroelectric facility fed from Lake Brunner on the Arnold River in West Coast, New Zealand, owned and operated by TrustPower. The plant is rated at and has an average annual output of .\n\nTrustPower has been planning a new hydroelectric power station at Arnold, with an output of and average annual generation of . Consents for this project were granted by the Grey District and West Coast Regional Councils in November 2008. Though the resource consents were upheld by the Environment Court in 2010, the project was put on hold indefinitely in 2012 due to changes in economic conditions.\n\n"}
{"id": "13535905", "url": "https://en.wikipedia.org/wiki?curid=13535905", "title": "Bawarij", "text": "Bawarij\n\nBawarij () were Sindhi pirates from Sindh named for their distinctive \"barja\" warships. They looted Arab shipping bound for the South Asia and China, but entirely converted to Islam during the rule of the Samma Dynasty (AD 1335–1520). They are mentioned by Ma'sudi as frequenting the pirate den at Socotra and other scholars describes them as pirates and sailors of Sindh. \n\nIbn Batuta describes their ships warships as having fifty rowers, and fifty men-at-arms and wooden roofs to protect against arrows and stones. Tabari describes them in an attack upon Basra in 866 CE as having one pilot \"(istiyam)\", three fire-throwers \"(naffatun)\", a baker, a carpenter and thirty-nine rowers and fighters making up a complement of forty-five. These ships were unsuited for warlike maneuvers and lacked the sleek prows or ramming capabilities of other contemporary naval units, but were intended to provide for hand-to-hand battles for crew upon boarding.\n\n\n"}
{"id": "5679969", "url": "https://en.wikipedia.org/wiki?curid=5679969", "title": "Block and bleed manifold", "text": "Block and bleed manifold\n\nA Block and bleed manifold is a hydraulic manifold that combines one or more block/isolate valves, usually ball valves, and one or more bleed/vent valves, usually ball or needle valves, into one component for interface with other components (pressure measurement transmitters, gauges, switches, etc.) of a hydraulic (fluid) system. The purpose of the block and bleed manifold is to isolate or block the flow of fluid in the system so the fluid from upstream of the manifold does not reach other components of the system that are downstream. Then they bleed off or vent the remaining fluid from the system on the downstream side of the manifold. For example, a block and bleed manifold would be used to stop the flow of fluids to some component, then vent the fluid from that component’s side of the manifold, in order to effect some kind of work (maintenance/repair/replacement) on that component.\n\nA block and bleed manifold with one block valve and one bleed valve is also known as an isolation valve or block and bleed valve; a block and bleed manifold with multiple valves is also known as an isolation manifold. This valve is used in combustible gas trains in many industrial applications. Block and bleed needle valves are used in hydraulic and pneumatic systems because the needle valve allows for precise flow regulation when there is low flow in a non-hazardous environment.\n\nThese valves replace existing traditional techniques employed by pipeline engineers to generate a double block and bleed configuration in the pipeline. Two block valves and a bleed valve are as a unit, or manifold, to be installed for positive isolation. Used for critical process service, DBB valves are for high pressure systems or toxic/hazardous fluid processes. Applications that use DBB valves include instrument drain, chemical injection connection, chemical seal isolation, and gauge isolation. DBB valves do the work of three separate valves (2 isolations and 1 drain) and require less space and have less weight.\n\nThis type of Double Block and Bleed Valves have a patented design which incorporates two ball valves and a bleed valve into one compact cartridge type unit with ANSI B16.5 tapped flanged connections. The major benefit of this design configuration is that the valve has the same face-to-face dimension as a single block ball valve (as specified in API 6D and ANSI B16.10), which means the valve can easily be installed into an existing pipeline without the need for any pipeline re-working.\n\nThis type of Double Block and Bleed Valves (DBB Valves) feature the traditional style of flange-by-flange type valve and is available with ANSI B16.5 flanges, hub connections and welded ends to suit the pipeline system it is to be installed in. It features all the benefits of the single unit DBB valve, with the added benefit of a bespoke face-to-face dimension if required.\n\nThis design also has operational advantages, there are significantly fewer potential leak paths within the double block and bleed section of the pipeline. Because the valves are full bore with an uninterrupted flow orifice they have got a negligible pressure drop across the unit. The pipelines where these valves are installed can also be pigged without any problems.\n\nThere are several advantages in using a Double Block and Bleed Valve. Significantly, because all the valve components are housed in a single unit, the space required for the installation is dramatically reduced thus freeing up room for other pieces of essential equipment.\n\nConsidering the operations and procedures executed before an operator can intervene, the Double Block and Bleed manifold offers further advantages over the traditional hook up. Due to the volume of the cavity between the two balls being so small, the operator is afforded the opportunity to evacuate this space efficiently thereby quickly establishing a safe working environment.\n"}
{"id": "18933266", "url": "https://en.wikipedia.org/wiki?curid=18933266", "title": "Boeing", "text": "Boeing\n\nThe Boeing Company () is an American multinational corporation that designs, manufactures, and sells airplanes, rotorcraft, rockets, satellites, and missiles worldwide. The company also provides leasing and product support services. Boeing is among the largest global aircraft manufacturers; it is the fifth-largest defense contractor in the world based on 2017 revenue, and is the largest exporter in the United States by dollar value. Boeing stock is included in the Dow Jones Industrial Average.\n\nBoeing was founded by William Boeing on July 15, 1916, in Seattle, Washington. The present corporation is the result of merger of Boeing with McDonnell Douglas on August 1, 1997. Former Boeing's chair and CEO Philip M. Condit continued as the chair and CEO of the new Boeing, while Harry Stonecipher, former CEO of McDonnell Douglas, became the president and chief operating officer of the newly merged company.\n\nThe Boeing Company has its corporate headquarters in Chicago, Illinois. The company is led by President and CEO Dennis Muilenburg. Boeing is organized into five primary divisions: Boeing Commercial Airplanes (BCA); Boeing Defense, Space & Security (BDS); Engineering, Operations & Technology; Boeing Capital; and Boeing Shared Services Group. In 2017, Boeing recorded $93.3 billion in sales, ranked 24th on the \"Fortune\" magazine \"Fortune 500\" list (2018), ranked 64th on the \"Fortune Global 500\" list (2018), and ranked 25th on the \"World's Most Admired Companies\" list (2018).\n\nIn March 1910, William E. Boeing bought Heath's shipyard in Seattle on the Duwamish River, which later became his first airplane factory. Boeing was incorporated in Seattle by William Boeing, on July 15, 1916, as \"Pacific Aero Products Co\". Boeing was later incorporated in Delaware; the original Certificate of Incorporation was filed with the Secretary of State of Delaware on July 19, 1934. Boeing, who studied at Yale University, worked initially in the timber industry, where he became wealthy and learned about wooden structures. This knowledge proved invaluable in his subsequent design and assembly of airplanes. The company stayed in Seattle to take advantage of the local supply of spruce wood.\nOne of the two \"B&W\" seaplanes built with the assistance of George Conrad Westervelt, a U.S. Navy engineer, took its maiden flight on June 15, 1916. Boeing and Westervelt decided to build the B&W seaplane after having flown in a Curtiss aircraft. Boeing bought a Glenn Martin \"Flying Birdcage\" seaplane (so called because of all the guy-wires holding it together) and was taught to fly by Glenn Martin himself. Boeing soon crashed the Birdcage and when Martin informed Boeing that replacement parts would not become available for months, Boeing realized he could build his own plane in that amount of time. He and his friend Cdr. G.C. Westervelt agreed to build a better airplane and soon produced the B&W Seaplane. This first Boeing airplane was assembled in a lakeside hangar located on the northeast shore of Seattle's Lake Union. Many of Boeing's early planes were seaplanes.\nOn April 6, 1917, the U.S. declared war on Germany and entered World War I. On May 9, 1917, the company became the \"Boeing Airplane Company\". With the U.S. entering the war, Boeing knew that the U.S. Navy needed seaplanes for training. So Boeing shipped two new Model Cs to Pensacola, Florida, where the planes were flown for the Navy. The Navy liked the Model C and ordered 50 more. The company moved its operations to a larger former shipbuilding facility known as Boeing Plant 1, located on the lower Duwamish River, Washington state.\n\nWhen World War I ended in 1918, a large surplus of cheap, used military planes flooded the commercial airplane market, preventing aircraft companies from selling any new airplanes, driving many out of business. Others, including Boeing, started selling other products. Boeing built dressers, counters, and furniture, along with flat-bottom boats called Sea Sleds.\n\nIn 1919 the Boeing B-1 flying boat made its first flight. It accommodated one pilot and two passengers and some mail. Over the course of eight years, it made international airmail flights from Seattle to Victoria, British Columbia. On May 24, 1920, the Boeing Model 8 made its first flight. It was the first plane to fly over Mount Rainier.\n\nIn 1923, Boeing entered competition against Curtiss to develop a pursuit fighter for the U.S. Army Air Service. Although Curtiss finished its design first and was awarded the contract, Boeing continued to develop its PW-9 fighter. That plane, along with the Boeing P-12/F4B fighter, made Boeing a leading manufacturer of fighters over the course of the next decade.\n\nIn 1925, Boeing built its Model 40 mail plane for the U.S. government to use on airmail routes. In 1927, an improved version of this plane was built, the Model 40A which won the U.S. Post Office's contract to deliver mail between San Francisco and Chicago. The 40A also had a passenger cabin that accommodated two.\n\nThat same year, Boeing created an airline named Boeing Air Transport, which merged a year later with Pacific Air Transport and the Boeing Airplane Company. The first airmail flight for the airline was on July 1, 1927. The company changed its name to United Aircraft and Transport Corporation in 1929 and acquired Pratt & Whitney, Hamilton Standard Propeller Company, and Chance Vought. United Aircraft then purchased National Air Transport in 1930.\n\nOn July 27, 1928, the 12-passenger Boeing 80 biplane made its first flight. With three engines, it was Boeing's first plane built with the sole intention of being a passenger transport. An upgraded version, the 80A, carrying eighteen passengers, made its first flight in September 1929.\n\nIn the early 1930s Boeing became a leader in all-metal aircraft construction, and in the design revolution that established the path for other transport aircraft through the 1930s. In 1930, the Monomail, a low-wing monoplane that carried mail, was built. Built entirely out of metal, it was very fast and aerodynamic, and had retractable landing gear. In fact, its design was so revolutionary that the engines and propellers of the time could not handle the plane. By the time controllable pitch propellers were developed, Boeing was building its Model 247 airliner. Two Monomails were built. The second one, the Model 221, had a 6-passenger cabin. In 1931, the Monomail design became the foundation of the Boeing YB-9, the first all-metal, cantilever-wing, monoplane bomber. Five examples entered service between September 1932 and March 1933. The performance of the twin-engine monoplane bomber led to reconsideration of air defense requirements, although it was soon rendered obsolete by rapidly-advancing bomber designs.\n\nIn 1932, Boeing introduced the Model 248, the first all-metal monoplane fighter. The P-26 Peashooter was in front-line service with the US Army Air Corps from 1934 to 1938.\n\nIn 1933, the Boeing 247 was introduced, which set the standard for all competitors in the passenger transport market. The 247 was an all-metal low-wing monoplane that was much faster, safer, and easier to fly than other passenger aircraft. For example, it was the first twin engine passenger aircraft that could fly on one engine. In an era of unreliable engines, this vastly improved flight safety. Boeing built the first 59 aircraft exclusively for its own United Airlines subsidiary's operations. This badly hurt competing airlines, and was typical of the anti-competitive corporate behavior that the U.S. government sought to prohibit at the time. The direction established with the 247 was further developed by Douglas Aircraft, resulting in one of the most successful designs in aviation history. \n\nThe Air Mail Act of 1934 prohibited airlines and manufacturers from being under the same corporate umbrella, so the company split into three smaller companies – Boeing Airplane Company, United Airlines, and United Aircraft Corporation, the precursor to United Technologies. As a result, William Boeing sold off his shares and left Boeing. Clairmont \"Claire\" L. Egtvedt, who had become Boeing's president in 1933, became the chairman as well. He believed the company's future was in building bigger planes. Work began in 1936 on Boeing Plant 2 to accommodate the production of larger modern aircraft. \n\nFrom 1934 to 1937, Boeing was developing an experimental long range bomber, the XB-15. At its introduction in 1937 it was the largest heavier-than-air craft built to date. Trials revealed that its speed was unsatisfactory, but the design experience was used in the development of the Model 314 that followed a year later.\n\nOverlapping with the period of the YB-15 development, an agreement with Pan American World Airways (Pan Am) was reached, to develop and build a commercial flying boat able to carry passengers on transoceanic routes. The first flight of the Boeing 314 Clipper was in June 1938. It was the largest civil aircraft of its time, with a capacity of 90 passengers on day flights, and of 40 passengers on night flights. One year later, the first regular passenger service from the U.S. to the UK was inaugurated. Subsequently, other routes were opened, so that soon Pan Am flew with the Boeing 314 to destinations all over the world.\n\nIn 1938, Boeing completed work on its Model 307 \"Stratoliner\". This was the world's first pressurized-cabin transport aircraft, and it was capable of cruising at an altitude of – above most weather disturbances. It was based on the B-17, using the same wings, tail and engines.\n\nDuring World War II, Boeing built a large number of B-17 and B-29 bombers. Boeing ranked twelfth among United States corporations in the value of wartime production contracts. Many of the workers were women whose husbands had gone to war. In the beginning of March 1944, production had been scaled up in such a manner that over 350 planes were built each month. To prevent an attack from the air, the manufacturing plants had been covered with greenery and farmland items. During the war years the leading aircraft companies of the U.S. cooperated. The Boeing-designed B-17 bomber was assembled also by Lockheed Aircraft Corp. and Douglas Aircraft Co., while the B-29 was assembled also by Bell Aircraft Co. and by Glenn L. Martin Company. In 1942 Boeing started development of the C-97 Stratofreighter, the first of a generation of heavy-lift military transports; it became operational in 1947. The C-97 design would be successfully adapted for use as an aerial refueling tanker, although its role as a transport was soon limited by designs that had advantages in either versatility or capacity.\n\nAfter the war, most orders of bombers were canceled and 70,000 people lost their jobs at Boeing. The company aimed to recover quickly by selling its Stratocruiser (the Model 377), a luxurious four-engine commercial airliner derived from the C-97. However, sales of this model were not as expected and Boeing had to seek other opportunities to overcome the situation. In 1947 Boeing flew its first jet aircraft, the XB-47, from which the highly successful B-47 and B-52 bombers were derived.\n\nBoeing developed military jets such as the B-47 Stratojet and B-52 Stratofortress bombers in the late-1940s and into the 1950s. During the early 1950s, Boeing used company funds to develop the 367–80 jet airliner demonstrator that led to the KC-135 Stratotanker and Boeing 707 jetliner. Some of these were built at Boeing's facilities in Wichita, Kansas, which existed from 1931 to 2014.\n\nBetween the last delivery of a 377 in 1950 and the first order for the 707 in 1955, Boeing was shut out of the commercial aircraft market.\n\nIn the mid-1950s technology had advanced significantly, which gave Boeing the opportunity to develop and manufacture new products. One of the first was the guided short-range missile used to intercept enemy aircraft. By that time the Cold War had become a fact of life, and Boeing used its short-range missile technology to develop and build an intercontinental missile.\n\nIn 1958, Boeing began delivery of its 707, the United States' first commercial jet airliner, in response to the British De Havilland Comet, French Sud Aviation Caravelle and Soviet Tupolev Tu-104, which were the world's first generation of commercial jet aircraft. With the 707, a four-engine, 156-passenger airliner, the U.S. became a leader in commercial jet manufacturing. A few years later, Boeing added a second version of this aircraft, the Boeing 720, which was slightly faster and had a shorter range.\n\nBoeing was a major producer of small turbine engines during the 1950s and 1960s. The engines represented one of the company's major efforts to expand its product base beyond military aircraft after World War II. Development on the gas turbine engine started in 1943 and Boeing's gas turbines were designated models 502 (T50), 520 (T60), 540, 551 and 553. Boeing built 2,461 engines before production ceased in April 1968. Many applications of the Boeing gas turbine engines were considered to be firsts, including the first turbine-powered helicopter and boat.\n\nVertol Aircraft Corporation was acquired by Boeing in 1960, and was reorganized as Boeing's Vertol division. The twin-rotor CH-47 Chinook, produced by Vertol, took its first flight in 1961. This heavy-lift helicopter remains a work-horse vehicle to the present day. In 1964, Vertol also began production of the CH-46 Sea Knight.\n\nIn December 1960, Boeing announced the model 727 jetliner, which went into commercial service about three years later. Different passenger, freight and convertible freighter variants were developed for the 727. The 727 was the first commercial jetliner to reach 1,000 sales.\n\nOn May 21, 1961, the company shortened its name to the current \"Boeing Company\".\n\nBoeing won a contract in 1961 to manufacture the S-IC stage of the Saturn V rocket, manufactured at the Michoud Assembly Facility in New Orleans, Louisiana.\n\nIn 1966, Boeing president William M. Allen asked Malcolm T. Stamper to spearhead production of the new 747 airliner on which the company's future was riding. This was a monumental engineering and management challenge, and included construction of the world's biggest factory in which to build the 747 at Everett, Washington, a plant which is the size of 40 football fields.\n\nIn 1967, Boeing introduced another short- and medium-range airliner, the twin-engine 737. It has since become the best-selling commercial jet aircraft in aviation history. Several versions have been developed, mainly to increase seating capacity and range. The 737 remains in production as of February 2018 with the latest 737 MAX series.\n\nThe roll-out ceremonies for the first 747-100 took place in 1968, at the massive new factory in Everett, about an hour's drive from Boeing's Seattle home. The aircraft made its first flight a year later. The first commercial flight occurred in 1970. The 747 has an intercontinental range and a larger seating capacity than Boeing's previous aircraft.\n\nBoeing also developed hydrofoils in the 1960s. The screw-driven USS \"High Point\" (PCH-1) was an experimental submarine hunter. The patrol hydrofoil USS \"Tucumcari\" (PGH-2) was more successful. Only one was built, but it saw service in Vietnam and Europe before running aground in 1972. Its waterjet and fully submersed flying foils were the example for the later Pegasus-class patrol hydrofoils and the Model 929 Jetfoil ferries in the 1980s. The Tucumcari and later boats were produced in Renton. While the Navy hydrofoils were withdrawn from service in the late 1980s, the Boeing Jetfoils are still in service in Asia.\n\nIn the early 1970s Boeing suffered from the simultaneous decline in Vietnam War military spending, the slowing of the space program as Project Apollo neared completion, the recession of 1969–70, and the company's $2 billion debt as it built the new 747 airliner. Boeing did not receive any orders for more than a year. Its bet for the future, the 747, was delayed in production by three months because of problems with its Pratt & Whitney engines. Then in March 1971, Congress voted to discontinue funding for the development of the Boeing 2707 supersonic transport (SST), the US's answer to the British-French Concorde, forcing the end of the project.\n\nCommercial Airplane Group, by far the largest unit of Boeing, went from 83,700 employees in 1968 to 20,750 in 1971. Each unemployed Boeing employee cost at least one other job in the Seattle area, and unemployment rose to 14 percent, the highest in the United States. Housing vacancy rates rose to 16 percent from 1 percent in 1967. U-Haul dealerships ran out of trailers because so many people moved out. A billboard appeared near the airport:\nIn January 1970, the first 747, a four-engine long-range airliner, flew its first commercial flight with Pan American World Airways. The 747 changed the airline industry, providing much larger seating capacity than any other airliner in production. The company has delivered over 1,500 Boeing 747s. The 747 has undergone continuous improvements to keep it technologically up-to-date. Larger versions have also been developed by stretching the upper deck. The newest version of the 747, the 747-8, remains in production as of 2018.\n\nBoeing launched three Jetfoil 929-100 hydrofoils that were acquired in 1975 for service in the Hawaiian Islands. When the service ended in 1979 the three hydrofoils were acquired by Far East Hydrofoil for service between Hong Kong and Macau.\n\nDuring the 1970s, Boeing also developed the US Standard Light Rail Vehicle, which has been used in San Francisco, Boston, and Morgantown, West Virginia.\n\nIn 1983, the economic situation began to improve. Boeing assembled its 1,000th 737 passenger aircraft. During the following years, commercial aircraft and their military versions became the basic equipment of airlines and air forces. As passenger air traffic increased, competition was harder, mainly from Airbus, a European newcomer in commercial airliner manufacturing. Boeing had to offer new aircraft, and developed the single-aisle 757, the larger, twin-aisle 767, and upgraded versions of the 737. An important project of these years was the Space Shuttle, to which Boeing contributed with its experience in space rockets acquired during the Apollo era. Boeing participated also with other products in the space program, and was the first contractor for the International Space Station program.\n\nDuring the decade several military projects went into production, including Boeing support of the B-2 stealth bomber. As part of an industry team led by Northrop, Boeing built the B-2's outer wing portion, aft center fuselage section, landing gear, fuel system, and weapons delivery system. At its peak in 1991, the B-2 was the largest military program at Boeing, employing about 10,000 people. The same year, the US's National Aeronautic Association awarded the B-2 design team the Collier Trophy for the greatest achievement in aerospace in America. The first B-2 rolled out of the bomber's final assembly facility in Palmdale, California, in November 1988 and it flew for the first time on July 17, 1989.\n\nThe Avenger air defense system and a new generation of short-range missiles also went into production. During these years, Boeing was very active in upgrading existing military equipment and developing new ones. Boeing also contributed to wind power development with the experimental MOD-2 Wind Turbines for NASA and the United States Department of Energy, and the MOD-5B for Hawaii.\n\nBoeing was one of seven competing companies that bid for the Advanced Tactical Fighter. Boeing agreed to team with General Dynamics and Lockheed, so that all three companies would participate in the development if one of the three companies designs was selected. The Lockheed design was eventually selected and developed into the F-22 Raptor.\n\nIn April 1994, Boeing introduced the most modern commercial jet aircraft at the time, the twin-engine 777, with a seating capacity of approximately 300 to 370 passengers in a typical three-class layout, in between the 767 and the 747. The longest range twin-engined aircraft in the world, the 777 was the first Boeing airliner to feature a \"fly-by-wire\" system and was conceived partly in response to the inroads being made by the European Airbus into Boeing's traditional market. This aircraft reached an important milestone by being the first airliner to be designed entirely by using computer-aided design (CAD) techniques. The 777 was also the first airplane to be certified for 180 minute ETOPS at entry into service by the FAA. Also in the mid-1990s, the company developed the revamped version of the 737, known as the 737 \"Next-Generation\", or 737NG. It has since become the fastest-selling version of the 737 in history, and on April 20, 2006 sales passed those of the \"Classic 737\", with a follow-up order for 79 aircraft from Southwest Airlines.\n\nIn 1995, Boeing chose to demolish the headquarters complex on East Marginal Way South instead of upgrading it to match new seismic standards. The headquarters were moved to an adjacent building and the facility was demolished in 1996. In 1997, Boeing was headquartered on East Marginal Way South, by King County Airport, in Seattle.\n\nIn 1996, Boeing acquired Rockwell's aerospace and defense units. The Rockwell business units became a subsidiary of Boeing, named Boeing North American, Inc. In August 1997, Boeing merged with McDonnell Douglas in a US$13 billion stock swap, with Boeing as the surviving company. Following the merger, the McDonnell Douglas MD-95 was renamed the Boeing 717, and the production of the MD-11 trijet was limited to the freighter version. Boeing introduced a new corporate identity with completion of the merger, incorporating the Boeing logo type and a stylized version of the McDonnell Douglas symbol, which was derived from the Douglas Aircraft logo from the 1970s.\n\nAn aerospace analyst criticized the CEO and his deputy, Philip M. Condit and Harry Stonecipher, for thinking of their personal benefit first, and causing the problems to Boeing many years later. Instead of investing the huge cash reserve to build new airplanes, they initiated a program to buy back Boeing stock for more than US$10 billion.\n\nIn May 1999, Boeing studied buying Embraer to encourage commonality between the E-Jets and the Boeing 717, but this was nixed by then president Harry Stonecipher. He preferred buying Bombardier Aerospace, but its owner, the Beaudoin family, asked for a price too high for Boeing which remembered its mid-1980s purchase of de Havilland Canada, losing a million dollars every day for three years before selling it to Bombardier in 1992.\n\nIn January 2000, Boeing chose to expand its presence in another aerospace field of satellite communications by purchasing Hughes Electronics. Hughes Space and Communications Company, which had pioneered the satellite communications field.\n\nIn September 2001, Boeing moved its corporate headquarters from Seattle to Chicago. Chicago, Dallas and Denver – vying to become the new home of the world's largest aerospace concern – all had offered packages of multimillion-dollar tax breaks. Its offices are located in the Fulton River District just outside the Chicago Loop.\n\nOn October 10, 2001, Boeing lost to its rival Lockheed Martin in the fierce competition for the multibillion-dollar Joint Strike Fighter contract. Boeing's entry, the X-32, was rejected in favor of Lockheed's X-35 entrant. Boeing continues to serve as the prime contractor on the International Space Station and has built several of the major components.\n\nBoeing began development of the KC-767 aerial refueling tanker in the early 2000s. Italy and Japan ordered four KC-767s each. After development delays and FAA certification, Boeing delivered the tankers to Japan from 2008 with the second KC-767 following on March 5. to 2010. Italy received its four KC-767 during 2011.\n\nIn 2004, Boeing ended production of the 757 after 1,050 aircraft were produced. More advanced, stretched versions of the 737 were beginning to compete against the 757, and the planned 787-3 was to fill much of the top end of the 757 market. Also that year, Boeing announced that the 717, the last civil aircraft to be designed by McDonnell Douglas, would cease production in 2006. The 767 was in danger of cancellation as well, with the 787 replacing it, but orders for the freighter version extended the program.\n\nAfter several decades of success, Boeing lost ground to Airbus and subsequently lost its lead in the airliner market in 2003. Multiple Boeing projects were pursued and then canceled, notably the Sonic Cruiser, a proposed jetliner that would travel just under the speed of sound, cutting intercontinental travel times by as much as 20 percent. It was launched in 2001 along with a new advertising campaign to promote the company's new motto, \"Forever New Frontiers\", and to rehabilitate its image. However, the plane's fate was sealed by the changes in the commercial aviation market following the September 11 attacks and the subsequent weak economy and increase in fuel prices.\n\nSubsequently, Boeing streamlined its production and turned its attention to a new model, the Boeing 787 Dreamliner, using much of the technology developed for the Sonic Cruiser, but in a more conventional aircraft designed for maximum efficiency. The company also launched new variants of its successful 737 and 777 models. The 787 proved to be a highly popular choice with airlines, and won a record number of pre-launch orders. With delays to Airbus' A380 program several airlines threatened to switch their A380 orders to Boeing's new 747 version, the 747-8. Airbus's response to the 787, the A350, received a lukewarm response at first when it was announced as an improved version of the A330, and then gained significant orders when Airbus promised an entirely new design. The 787 program encountered delays, with the first flight not occurring until late 2009.\n\nAfter regulatory approval, Boeing formed a joint venture, United Launch Alliance with its competitor, Lockheed Martin, on December 1, 2006. The new venture is the largest provider of rocket launch services to the U.S. government.\n\nIn 2005, Gary Scott, ex-Boeing executive and then head of Bombardier's CSeries program, suggested a collaboration on the upcoming CSeries, but an internal study assessed Embraer as the best partner for regional jets. The Brazilian government wanted to retain control and blocked an acquisition.\n\nOn August 2, 2005, Boeing sold its Rocketdyne rocket engine division to Pratt & Whitney. On May 1, 2006, Boeing agreed to purchase Dallas, Texas-based Aviall, Inc. for $1.7 billion and retain $350 million in debt. Aviall, Inc. and its subsidiaries, Aviall Services, Inc. and ILS formed a wholly owned subsidiary of Boeing Commercial Aviation Services (BCAS).\n\nRealizing that increasing numbers of passengers have become reliant on their computers to stay in touch, Boeing introduced Connexion by Boeing, a satellite based Internet connectivity service that promised air travelers unprecedented access to the World Wide Web. The company debuted the product to journalists in 2005, receiving generally favorable reviews. However, facing competition from cheaper options, such as cellular networks, it proved too difficult to sell to most airlines. In August 2006, after a short and unsuccessful search for a buyer for the business, Boeing chose to discontinue the service.\n\nOn August 18, 2007, NASA selected Boeing as the manufacturing contractor for the liquid-fueled upper stage of the Ares I rocket. The stage, based on both Apollo-Saturn and Space Shuttle technologies, was to be constructed at NASA's Michoud Assembly Facility near New Orleans; Boeing constructed the S-IC stage of the Saturn V rocket at this site in the 1960s.\nBoeing launched the 777 Freighter in May 2005 with an order from Air France. The freighter variant is based on the −200LR. Other customers include FedEx and Emirates. Boeing officially announced in November 2005 that it would produce a larger variant of the 747, the 747-8, in two versions, commencing with the Freighter version with firm orders for two cargo carriers. The second version, named the Intercontinental, is for passenger airlines. Both 747-8 versions feature a lengthened fuselage, new, advanced engines and wings, and the incorporation of other technologies developed for the 787.\n\nBoeing also received the launch contract from the U.S. Navy for the P-8 Poseidon Multimission Maritime Aircraft, an anti-submarine warfare patrol aircraft. It has also received orders for the 737 AEW&C \"Wedgetail\" aircraft. The company has also introduced new extended range versions of the 737. These include the 737-700ER and 737-900ER. The 737-900ER is the latest and will extend the range of the 737–900 to a similar range as the successful 737–800 with the capability to fly more passengers, due to the addition of two extra emergency exits.\nThe 777-200LR Worldliner embarked on a well-received global demonstration tour in the second half of 2005, showing off its capacity to fly farther than any other commercial aircraft. On November 10, 2005, the 777-200LR set a world record for the longest non-stop flight. The plane, which departed from Hong Kong traveling to London, took a longer route, which included flying over the U.S. It flew 11,664 nautical miles (21,601 km) during its 22-hour 42-minute flight. It was flown by Pakistan International Airlines pilots and PIA was the first airline to fly the 777-200LR Worldliner.\n\nOn August 11, 2006, Boeing agreed to form a joint-venture with the large Russian titanium producer, VSMPO-Avisma for the machining of titanium forgings. The forgings will be used on the 787 program. In December 2007, Boeing and VSMPO-Avisma created a joint venture, Ural Boeing Manufacturing, and signed a contract on titanium product deliveries until 2015, with Boeing planning to invest $27 billion in Russia over the next 30 years.\n\nIn February 2011, Boeing received a contract for 179 KC-46 U.S. Air Force tankers at a value of $35 billion. The KC-46 tankers are based on the KC-767.\nBoeing, along with Science Applications International Corporation (SAIC), were the prime contractors in the U.S. military's Future Combat Systems program. The FCS program was canceled in June 2009 with all remaining systems swept into the BCT Modernization program. Boeing works jointly with SAIC in the BCT Modernization program like the FCS program but the U.S. Army will play a greater role in creating baseline vehicles and will only contract others for accessories.\n\nDefense Secretary Robert M. Gates' shift in defense spending to, \"make tough choices about specific systems and defense priorities based solely on the national interest and then stick to those decisions over time\" hit Boeing especially hard, because of their heavy involvement with canceled Air Force projects.\n\nIn May 2003, the U.S. Air Force announced it would lease 100 KC-767 tankers to replace the oldest 136 KC-135s. In November 2003, responding to critics who argued that the lease was more expensive than an outright purchase, the DoD announced a revised lease of 20 aircraft and purchase of 80. In December 2003, the Pentagon announced the project was to be frozen while an investigation of allegations of corruption by one of its former procurement staffers, Darleen Druyun (who began employment at Boeing in January) was begun. The fallout of this resulted in the resignation of Boeing CEO Philip M. Condit and the termination of CFO Michael M. Sears. Harry Stonecipher, former McDonnell Douglas CEO and Boeing COO, replaced Condit on an interim basis. Druyun pleaded guilty to inflating the price of the contract to favor her future employer and to passing information on the competing Airbus A330 MRTT bid. In October 2004, she received a jail sentence for corruption.\n\nIn March 2005, the Boeing board forced President and CEO Harry Stonecipher to resign. Boeing said an internal investigation revealed a \"consensual\" relationship between Stonecipher and a female executive that was \"inconsistent with Boeing's Code of Conduct\" and \"would impair his ability to lead the company\". James A. Bell served as interim CEO (in addition to his normal duties as Boeing's CFO) until the appointment of Jim McNerney as the new Chairman, President, and CEO on June 30, 2005.\n\nIn June 2003, Lockheed Martin sued Boeing, alleging that the company had resorted to industrial espionage in 1998 to win the Evolved Expendable Launch Vehicle (EELV) competition. Lockheed Martin claimed that the former employee Kenneth Branch, who went to work for McDonnell Douglas and Boeing, passed nearly 30,000 pages of proprietary documents to his new employers. Lockheed Martin argued that these documents allowed Boeing to win 19 of the 28 tendered military satellite launches.\n\nIn July 2003, Boeing was penalized, with the Pentagon stripping seven launches away from the company and awarding them to Lockheed Martin. Furthermore, the company was forbidden to bid for rocket contracts for a twenty-month period, which expired in March 2005. In early September 2005, it was reported that Boeing was negotiating a settlement with the U.S. Department of Justice in which it would pay up to $500 million to cover this and the Darleen Druyun scandal.\n\nUntil the late 1970s, the U.S. had a near monopoly in the Large Civil Aircraft (LCA) sector. The Airbus consortium (created in 1969) started competing effectively in the 1980s. At that stage the U.S. became concerned about the European competition and the alleged subsidies paid by the European governments for the developments of the early models of the Airbus family. This became a major issue of contention, as the European side was equally concerned by subsidies accruing to U.S. LCA manufacturers through NASA and Defense programs.\n\nThe EU and the U.S. started bilateral negotiations for the limitation of government subsidies to the LCA sector in the late 1980s. Negotiations were concluded in 1992 with the signing of the EC-US Agreement on Trade in Large Civil Aircraft which imposes disciplines on government support on both sides of the Atlantic which are significantly stricter than the relevant World Trade Organization (WTO) rules: Notably, the Agreement regulates in detail the forms and limits of government support, prescribes transparency obligations and commits the parties to avoiding trade disputes.\n\nIn 2004, the EU and the U.S. agreed to discuss a possible revision of the 1992 EU-US Agreement provided that this would cover all forms of subsidies including those used in the U.S., and in particular the subsidies for the Boeing 787; the first new aircraft to be launched by Boeing for 14 years. In October 2004 the U.S. began legal proceedings at the WTO by requesting WTO consultations on European launch investment to Airbus. The U.S. also unilaterally withdrew from the 1992 EU-US Agreement. The U.S. claimed Airbus had violated a 1992 bilateral accord when it received what Boeing deemed \"unfair\" subsidies from several European governments. Airbus responded by filing a separate complaint, contesting that Boeing had also violated the accord when it received tax breaks from the U.S. Government. Moreover, the EU also complained that the investment subsidies from Japanese airlines violated the accord.\n\nOn January 11, 2005, Boeing and Airbus agreed that they would attempt to find a solution to the dispute outside of the WTO. However, in June 2005, Boeing and the United States government reopened the trade dispute with the WTO, claiming that Airbus had received illegal subsidies from European governments. Airbus has also responded to this claim against Boeing, reopening the dispute and also accusing Boeing of receiving subsidies from the U.S. Government.\n\nOn September 15, 2010, the WTO ruled that Boeing had received billions of dollars in government subsidies. Boeing responded by stating that the ruling was a fraction of the size of the ruling against Airbus and that it required few changes in its operations. Boeing has received $8.7 billion in support from Washington state.\n\nIn May 2006, four concept designs being examined by Boeing were outlined in \"The Seattle Times\" based on corporate internal documents. The research aims in two directions: low-cost airplanes, and environmental-friendly planes. Codenamed after the well-known Muppets, a design team known as the Green Team concentrated primarily on reducing fuel usage. All four designs illustrated rear-engine layouts.\n\nAs with most concepts, these designs are only in the exploratory stage, intended to help Boeing evaluate the potentials of such radical technologies.\n\nBoeing recently patented its own force field technology, also known as the shock wave attenuation system, that would protect vehicles from shock waves generated by nearby explosions. Boeing has yet to confirm when they plan to build and test the technology.\n\nThe Boeing Yellowstone Project is the company's project to replace its entire civil aircraft portfolio with advanced technology aircraft. New technologies to be introduced include composite aerostructures, more electrical systems (reduction of hydraulic systems), and more fuel-efficient turbofan engines, such as the Pratt & Whitney PW1000G Geared Turbofan, General Electric GEnx, the CFM International LEAP56, and the Rolls-Royce Trent 1000. The term \"Yellowstone\" refers to the technologies, while \"Y1\" through \"Y3\" refer to the actual aircraft.\n\nIn summer 2010, Boeing acquired Fairfax, VA-based C4ISR and combat systems developer Argon ST to expand its C4ISR, cyber and intelligence capabilities.\nIn 2011, Boeing was hesitating between re-engineing the 737 or developing an all-new small airplane for which Embraer could have been involved, but when the A320neo was launched with new engines, that precipitated the 737 MAX decision.\nOn November 17, Boeing received its largest provisional order for $21.7 billion at list prices from Indonesian LCC Lion Air for 201 737 MAX, 29 737-900ERs and 150 purchase rights, days after its previous order record of $18 billion for 50 777-300ER from Emirates.\n\nOn January 5, 2012, Boeing announced it would close its facilities in Wichita, Kansas with 2,160 workers before 2014, more than 80 years after it was established, where it had employed as many as 40,000 people.\nIn May 2013, Boeing announced it would cut 1,500 IT jobs in Seattle over the next three years through layoffs, attrition and mostly relocation to St. Louis and North Charleston, South Carolina − 600 jobs each.\nIn September, Boeing announced their Long Beach facility manufacturing the C-17 Globemaster III military transport would shut down.\nIn January 2014, the company announced US$1.23 billion profits for Q4 2013, a 26% increase, due to higher demand for commercial aircraft. The last plane to undergo maintenance in Boeing Wichita's facility left in May 2014. \n\nIn September 2014, NASA awarded contracts to Boeing and SpaceX for transporting astronauts to the International Space Station.\nIn June 2015, Boeing announced that James McNerney would step down as CEO to be replaced by Boeing's COO, Dennis Muilenburg, on July 1, 2015. The 279th and last C-17 was delivered in summer before closing the site, affecting 2,200 jobs.\nIn February 2016, Boeing announced that Boeing President and CEO Dennis Muilenburg was elected the 10th Chairman of the Board, succeeding James McNerney. In March, Boeing announced to cut 4,000 jobs from its commercial airplane division by mid-year.\nOn May 13, 2016, Boeing opened a $1 billion, factory in Washington state that will make carbon-composite wings for its 777X to be delivered by 2020.\n\nIn October 2017, Boeing announced plans to acquire Aurora Flight Sciences to expand its capabilities to develop autonomous, electric-powered and long-flight-duration aircraft for its commercial and military businesses, pending regulatory approval.\n\nIn 2017, Boeing won 912 net orders for $134.8 billion at list prices including 745 737s, 94 787s and 60 777s, and delivered 763 airliners including 529 737s, 136 787s and 74 777s.\n\nIn January 2018, a joint venture was formed by auto seat maker Adient (50.01%) and Boeing (49.99%) to develop and manufacture airliner seats for new installations or retrofit, a $4.5 billion market in 2017 which will grow to $6 billion by 2026, to be based in Kaiserslautern near Frankfurt and distributed by Boeing subsidiary Aviall, with its customer service center in Seattle.\n\nOn June 4, 2018, Boeing and Safran announced a 50-50 partnership to design, build and service APUs after regulatory and antitrust clearance in the second half of 2018. This could threaten the dominance of Honeywell and United Technologies in the APU market.\n\nAt a June 2018 AIAA conference, Boeing unveiled a hypersonic transport project.\n\nIn September 2018, Boeing signed a deal with the Pentagon worth up to $2.4 billion to provide helicopters for protecting nuclear-missile bases. Boeing enhanced its space business by acquiring the satellite company Millennium Space System in September 2018.\n\nOn July 5, 2018, a joint venture was announced for Embraer's airliners. This is seen as a reaction to Airbus acquiring a majority of the competing Bombardier CSeries on October 16, 2017.\n\nIn 2006, the UCLA Center for Environmental Risk Reduction released a study showing that Boeing's Santa Susana Field Laboratory, in the Simi Hills of eastern Ventura County in Southern California, had been contaminated with toxic and radioactive waste. The study found that air, soil, groundwater, and surface water at the site all contained radionuclides, toxic metals, and dioxins; air and water additionally contained perchlorate, TCE, and hydrazines, while water showed the presence of PCBs as well. Clean up studies and lawsuits are in progress.\n\nThe airline industry is responsible for about 11 percent of greenhouse gases emitted by the U.S. transportation sector. Aviation's share of the greenhouse gas emissions is poised to grow, as air travel increases and ground vehicles use more alternative fuels like ethanol and biodiesel. Boeing estimates that biofuels could reduce flight-related greenhouse-gas emissions by 60 to 80 percent. The solution blends algae fuels with existing jet fuel.\n\nBoeing executives said the company is informally collaborating with leading Brazilian biofuels maker Tecbio, Aquaflow Bionomic of New Zealand and other fuel developers around the world. So far, Boeing has tested six fuels from these companies, and will probably have gone through 20 fuels \"by the time we're done evaluating them\". Boeing was also joining other aviation-related members in the Algal Biomass Organization (ABO) on June 2008.\n\nAir New Zealand and Boeing are researching the jatropha plant to see if it is a sustainable alternative to conventional fuel. A two-hour test flight using a 50–50 mixture of the new biofuel with Jet A-1 in the number one position Rolls Royce RB-211 engine of 747-400 ZK-NBS, was successfully completed on December 30, 2008. The engine was then removed to be scrutinised and studied to identify any differences between the Jatropha blend and regular Jet A1. No effects to performances were found.\n\nOn August 31, 2010, Boeing worked with the U.S. Air Force to test the Boeing C-17 running on 50 percent JP-8, 25 percent Hydro-treated Renewable Jet fuel and 25 percent of a Fischer–Tropsch fuel with successful results.\n\nFor NASA's N+3 future airliner program, Boeing has determined that hybrid electric engine technology is by far the best choice for its subsonic design. Hybrid electric propulsion has the potential to shorten takeoff distance and reduce noise.\n\nIn both 2008 and 2009, Boeing was second on the list of Top 100 US Federal Contractors, with contracts totalling $22 billion and $23 billion respectively. Since 1995, the company has agreed to pay $1.6 billion to settle 39 instances of misconduct, including $615 million in 2006 in relation to illegal hiring of government officials and improper use of proprietary information.\n\nBoeing secured the highest ever tax breaks at the state level in 2013.\n\nBoeing's 2010 lobbying expenditure by the third quarter was $13.2 million (2009 total: $16.9 million). In the 2008 presidential election, Barack Obama \"was by far the biggest recipient of campaign contributions from Boeing employees and executives, hauling in $197,000 – five times as much as John McCain, and more than the top eight Republicans combined\".\n\nBoeing has a corporate citizenship program centered on charitable contributions in five areas: education, health, human services, environment, the arts, culture, and civic engagement. In 2011, Boeing spent $147.3 million in these areas through charitable grants and business sponsorships. In February 2012, Boeing Global Corporate Citizenship partnered with the Insight Labs to develop a new model for foundations to more effectively lead the sector that they serve.\n\nThe company is a member of the U.S. Global Leadership Coalition, a Washington D.C.-based coalition of over 400 major companies and NGOs that advocates for a larger International Affairs Budget, which funds American diplomatic and development efforts abroad. A series of U.S. diplomatic cables show how U.S. diplomats and senior politicians intervene on behalf of Boeing to help boost the company's sales.\n\nIn 2007 and 2008, the company benefited from over $10 billion of long-term loan guarantees, helping finance the purchase of their commercial aircraft in countries including Brazil, Canada, Ireland and the United Arab Emirates, from the Export-Import Bank of the United States, some 65 percent of the total loan guarantees the bank made in the period.\n\nIn December 2011, the non-partisan organization Public Campaign criticized Boeing for spending $52.29 million on lobbying and not paying taxes during 2008–2010, instead getting $178 million in tax rebates, despite making a profit of $9.7 billion, laying off 14,862 workers since 2008, and increasing executive pay by 31 percent to $41.9 million in 2010 for its top five executives.\n\nThe two largest divisions are Boeing Commercial Airplanes and Boeing Defense, Space & Security (BDS).\n\nFor the fiscal year 2017, Boeing reported earnings of 8.191 billion, with an annual revenue of 93.392 billion, a 1.25% decline over the previous fiscal cycle. Boeing's shares traded at over $209 per share, and its market capitalization was valued at over 206.6 billion.\n\nThe company's employment count is listed on its website below.\n\nApproximately 1.5 percent of Boeing employees are in the Technical Fellowship program, a program through which Boeing's top engineers and scientists set technical direction for the company. The average salary at Boeing is $76,784, reported by former employees.\n\n\n\n"}
{"id": "2666536", "url": "https://en.wikipedia.org/wiki?curid=2666536", "title": "Bone meal", "text": "Bone meal\n\nBone meal is a mixture of finely and coarsely ground animal bones and slaughter-house waste products. It is used as an organic fertilizer for plants and as a nutritional supplement for animals. As a slow-release fertilizer, bone meal is primarily used as a source of phosphorus and protein.\n\nBone meal, along with a variety of other meals, especially meat meal, is used as a dietary/mineral supplement for livestock. It is used to feed monogastric animals with bone meal from ruminants, and vice versa, to prevent the spread of bovine spongiform encephalopathy (BSE) or \"mad cow disease\". Proper heat control can reduce salmonella contaminants.\n\nBone meal once was often used as a human dietary calcium supplement. Research in the 1980s found that many bone meal preparations were contaminated with lead and other toxic metals, and is no longer recommended as a calcium source.\n\nAs a fertilizer, the N-P-K (Nitrogen-Phosphorus-Potassium) ratio of bone meal can vary greatly, depending on the source. From a low of 3-15-0 to as high as 2-22-0. , though some steamed bone meals have N-P-Ks of 1-13-0. Bone meal is also an excellent source of calcium, but does not provide enough nitrogen to be beneficial to plants. Plants can only get phosphorus from bone meal if the soil pH is below 7.0 (acidic soil), according to recent Colorado State University research.\n\nOrganic fertilizers usually require the use of a variety of fungi in the soil to make the nutrients in the fertilizer bioavailable to the plant. For plants needing phosphorus, the fungi mycorrhiza penetrate the roots and break down the compounds containing the phosphorus for easier absorption and utilization, and in turn the plants supply the mycorrhizae with amino acids and sugars.\n\nThe process was first suggested by Justus von Liebig (dissolving animal bones in sulphuric acid) around 1840 and first used in Britain by Rev James Robertson in Ellon, Aberdeenshire in 1841.\n\nIn 19th century Europe, large scale production and international trade in bone manure was seen as essential for agricultural development.\n\n"}
{"id": "27527143", "url": "https://en.wikipedia.org/wiki?curid=27527143", "title": "Bouchard No. 120", "text": "Bouchard No. 120\n\nOn Sunday April 27, 2003, the Tank Barge Bouchard No. 120 struck rocks south of Westport, Massachusetts. This resulted in a puncture of the barge's hull about twelve feet in length. As a result, the cargo of the barge, number 6 fuel oil, spilled into the waters of Buzzards Bay, Massachusetts. While the spill was much smaller than many oil spills, the environmental impacts were felt for years. Estimates of the total amount spilled range from 22,000 to 98,000 gallons of oil according to a publication by the National Oceanic and Atmospheric Administration.\n\nThis spill was quite different from some of the more widely known spills like Deepwater Horizon, not just because of its smaller scale, but due to the type of spill that it was. Deepwater, being a spill of oil directly from the ground was pure oil. Bouchard No. 120 contained number 6 Fuel_Oil. A local scientist from Woods Hole Oceanographic Institute explains that the processing of oil yields various components of varying danger to the environment.\n\nNOAA is heavily involved in environmental cleanups of oil spills as well as other damage to wildlife and coastal environments in the United States largely through their Damage Assessment Remediation and Recovery Program DARRP. efforts lasted in earnest for several years and continued to some degree until 2007 according to a report by the Buzzards Bay National Estuary Program.\n"}
{"id": "12297512", "url": "https://en.wikipedia.org/wiki?curid=12297512", "title": "Cement-bonded wood fiber", "text": "Cement-bonded wood fiber\n\nCement-bonded wood fiber is a composite material manufactured throughout the world. It is made from wood (usually waste wood), chipped into a specially graded aggregate that is then mineralized and combined with portland cement.\n\nCement-bonded wood fiber is used to manufacture a wide variety of products primarily for the construction industry (products like insulating concrete forms, siding materials and noise barriers). \n\nCement bonded wood fiber materials can be classified as low density, medium density and high density. The density of the material will determine to a large extent, the various properties of the end product. Other factors determining the overall performance of a cement bonded wood fiber material are:\n\n\nMost common is low-density cement bonded wood fiber. It is known for its use in LEED-certified projects and other types of green building. The material itself is 100% recyclable, and is known for its insulating and acoustic properties.\n\n"}
{"id": "28048752", "url": "https://en.wikipedia.org/wiki?curid=28048752", "title": "Charge radius", "text": "Charge radius\n\nThe rms charge radius is a measure of the size of an atomic nucleus, particularly of a proton or a deuteron. It can be measured by the scattering of electrons by the nucleus and also inferred from the effects of finite nuclear size on electron energy levels as measured in atomic spectra.\n\nThe problem of defining a radius for the atomic nucleus is similar to the problem of atomic radius, in that neither atoms nor their nuclei have definite boundaries. However, the nucleus can be modeled as a sphere of positive charge for the interpretation of electron scattering experiments: because there is no definite boundary to the nucleus, the electrons \"see\" a range of cross-sections, for which a mean can be taken. The qualification of \"rms\" (for \"root mean square\") arises because it is the nuclear cross-section, proportional to the square of the radius, which is determining for electron scattering.\n\nThis definition of charge radius can also be applied to composite hadrons such as a proton, neutron, pion, or kaon, that are made up of more than one quark. In the case of an anti-matter baryon (e.g. an anti-proton), and some particles with a net zero electric charge, the composite particle must be modeled as a sphere of negative rather than positive electric charge for the interpretation of electron scattering experiments. In these cases, the square of the charge radius of the particle is defined to be negative, with the same absolute value with units of length squared equal to the positive squared charge radius that it would have had if it was identical in all other respects but each quark in the particle had the opposite electric charge (with the charge radius itself having a value that is an imaginary number with units of length). It is customary when charge radius takes an imaginary numbered value to report the negative valued square of the charge radius, rather than the charge radius itself, for a particle.\n\nThe best known particle with a negative squared charge radius is the neutron. The heuristic explanation for why the squared charge radius of a neutron is negative, despite its overall neutral electric charge, is that this is the case because its negatively charged down quarks are, on average, located in the outer part of the neutron, while its positively charged up quark is, on average, located towards the center of the neutron. This asymmetric distribution of charge within the particle gives rise to a small negative squared charge radius for the particle as a whole. But, this is only the simplest of a variety of theoretical models, some of which are more elaborate, that are used to explain this property of a neutron.\n\nFor deuterons and higher nuclei, it is conventional to distinguish between the scattering charge radius, \"r\" (obtained from scattering data), and the bound-state charge radius, \"R\", which includes the Darwin–Foldy term to account for the behaviour of the anomalous magnetic moment in an electromagnetic field and which is appropriate for treating spectroscopic data. The two radii are related by\nwhere \"m\" and \"m\" are the masses of the electron and the deuteron respectively while \"λ\" is the Compton wavelength of the electron. For the proton, the two radii are the same.\n\nThe first estimate of a nuclear charge radius was made by Hans Geiger and Ernest Marsden in 1909, under the direction of Ernest Rutherford at the Physical Laboratories of the University of Manchester, UK. The famous experiment involved the scattering of α-particles by gold foil, with some of the particles being scattered through angles of more than 90°, that is coming back to the same side of the foil as the α-source. Rutherford was able to put an upper limit on the radius of the gold nucleus of 34 femtometres.\n\nLater studies found an empirical relation between the charge radius and the mass number, \"A\", for heavier nuclei (\"A\" > 20):\nwhere the empirical constant \"r\" of 1.2–1.5 fm can be interpreted as the proton radius. This gives a charge radius for the gold nucleus (\"A\" = 197) of about 7.5 fm.\n\nModern direct measurements are based on precision measurements of the atomic energy levels in hydrogen and deuterium, and measurements of scattering of electrons by nuclei. There is most interest in knowing the charge radii of protons and deuterons, as these can be compared with the spectrum of atomic hydrogen/deuterium: the nonzero size of the nucleus causes a shift in the electronic energy levels which shows up as a change in the frequency of the spectral lines. Such comparisons are a test of quantum electrodynamics (QED). Since 2002, the proton and deuteron charge radii have been independently refined parameters in the CODATA set of recommended values for physical constants, that is both scattering data and spectroscopic data are used to determine the recommended values.\n\nThe 2014 CODATA recommended values are:\n\nRecent measurement of the Lamb shift in muonic hydrogen (an exotic atom consisting of a proton and a negative muon) indicates a significantly lower value for the proton charge radius, : the reason for this discrepancy is not clear.\n"}
{"id": "58062393", "url": "https://en.wikipedia.org/wiki?curid=58062393", "title": "Coonooer Bridge Wind Farm", "text": "Coonooer Bridge Wind Farm\n\nCoonooer Bridge Wind Farm is a wind farm 90km northwest of Bendigo in the Australian state of Victoria. Its output is contracted to supply the Australian Capital Territory via the National Electricity Market at $81.50 per megawatt hour.\n\nThe site is jointly owned by Eurus Energy, Windlab and the local community. It was the first commercial wind farm in Australia to have the local community included in the ownership in this way.\n"}
{"id": "18541825", "url": "https://en.wikipedia.org/wiki?curid=18541825", "title": "Copper(I) sulfide", "text": "Copper(I) sulfide\n\nCopper(I) sulfide is a copper sulfide, a chemical compound of copper and sulfur. It has the chemical compound CuS. It is found in nature as the mineral chalcocite. It has a narrow range of stoichiometry ranging from CuS to CuS.\n\nCuS can be prepared by heating copper strongly in sulfur vapour or HS. The reaction of copper powder in molten sulfur rapidly produces CuS, whereas pellets of copper require much higher temperature.\nCuS reacts with oxygen to form SO:\nIn the production of copper two thirds of the molten copper sulfide is oxidised as above, and the CuO reacts with unoxidised CuS to give Cu metal:\n\nThere are two forms of CuS: a low temperature monoclinic form (\"low-chalcocite\") which has a complex structure with 96 copper atoms in the unit cell and a hexagonal form stable above 104 °C. In this structure there are 24 crystallographically distinct Cu atoms and the structure has been described as approximating to a hexagonal close packed array of sulfur atoms with Cu atoms in planar 3 coordination. This structure was initially assigned an orthorhombic cell due to the twinning of the sample crystal.\n\nThere is also a crystallographically-distinct phase (the mineral djurleite) with stoichiometry CuS which is non-stoichiometric (range CuS-CuS) and has a monoclinic structure with 248 copper and 128 sulfur atoms in the unit cell. CuS and CuS are similar in appearance and hard to distinguish one from another.\n\n"}
{"id": "1568513", "url": "https://en.wikipedia.org/wiki?curid=1568513", "title": "Dendrite (crystal)", "text": "Dendrite (crystal)\n\nA crystal dendrite is a crystal that develops with a typical multi-branching \"tree-like\" form. Dendritic crystal growth is very common and illustrated by snowflake formation and frost patterns on a window. Dendritic crystallization forms a natural fractal pattern. Dendritic crystals can grow into a supercooled pure liquid or form from growth instabilities that occur when the growth rate is limited by the rate of diffusion of solute atoms to the interface. In the latter case, there must be a concentration gradient from the supersaturated value in the solution to the concentration in equilibrium with the crystal at the surface. Any protuberance that develops is accompanied by a steeper concentration gradients at its tip. This increases the diffusion rate to the tip. In opposition to this is the action of the surface tension tending to flatten the protuberance and setting up a flux of solute atoms from the protuberance out to the sides. However, overall, the protuberance becomes amplified. This process occurs again and again until a dendrite is produced.\n\nThe term \"dendrite\" comes from the Greek word \"dendron (δενδρον)\", which means \"tree\".\n\nIn paleontology, dendritic mineral crystal forms are often mistaken for fossils. These pseudofossils form as naturally occurring fissures in the rock are filled by percolating mineral solutions. They form when water rich in manganese and iron flows along fractures and bedding planes between layers of limestone and other rock types, depositing dendritic crystals as the solution flows through. A variety of manganese oxides and hydroxides are involved , including:\n\nA three-dimensional form of dendrite develops in fissures in quartz, forming moss agate.\n\nIn chemistry, a dendrite is a crystal that branches into two parts during growth.\n\nThe Isothermal Dendritic Growth Experiment (IDGE) is a materials science solidification experiment that researchers use on Space Shuttle missions to investigate dendritic growth in an environment where the effect of gravity (convection in the liquid) can be excluded. Dendritic solidification is one of the most common forms of solidifying metals and alloys. When materials crystallize or solidify under certain conditions, they freeze unstably, resulting in dendritic forms. Scientists are particularly interested in dendrite size, shape, and how the branches of the dendrites interact with each other. These characteristics largely determine the properties of the material.\n\n\n"}
{"id": "10838742", "url": "https://en.wikipedia.org/wiki?curid=10838742", "title": "Economics of nuclear power plants", "text": "Economics of nuclear power plants\n\nNew nuclear power plants typically have high capital costs for building the first several plants, after which costs tend to fall for each additional plant built as the supply chains develop and the regulatory processes improve. Fuel, operational, and maintenance costs are relatively small components of the total cost. The long service life and high productivity of nuclear power plants allow sufficient funds for ultimate plant decommissioning and waste storage and management to be accumulated, with little impact on the price per unit of electricity generated. Additionally, measures to mitigate climate change such as a carbon tax or carbon emissions trading, would favor the economics of nuclear power over fossil fuel power.\n\nNuclear power construction costs have varied significantly across the world and in time. \nMassive and rapid increases in cost occurred in the 1970s, especially in the United States. \nA single study has claimed that these cost increases are specific to the United States. \nHowever, it has been criticized for the selective use of data and biased interpretation. \nThere were no construction starts of nuclear power reactors between 1979 and 2012 in the United States, and since then more new reactor attempts have gone through bankruptcy than be completed.\nRecent cost trends in countries such as Japan and Korea have been very different, including periods of stability and decline in costs.\n\nIn more economically developed countries, a slowdown in electricity demand growth in recent years has made large-scale power infrastructure investments difficult. Very large upfront costs and long project cycles carry large risks, including political decision making and intervention such as regulatory ratcheting. In Eastern Europe, a number of long-established projects are struggling to find financing, notably Belene in Bulgaria and the additional reactors at Cernavoda in Romania, and some potential backers have pulled out. Where cheap gas is available and its future supply relatively secure, this also poses a major problem for clean energy projects. Former Exelon CEO John Rowe said in 2012 that new nuclear plants in the United States \"don't make any sense right now\" and would not be economic as long as gas prices remain low.\n\nCurrent bids for new nuclear power plants in China have fallen below $2000/kW in 2016, as China continues accelerating its new build program after a pause following the Fukushima disaster. \nTherefore, comparison with other power generation methods is strongly dependent on assumptions about construction timescales and capital financing for nuclear plants.\n\nAnalysis of the economics of nuclear power must take into account who bears the risks of future uncertainties. To date all operating nuclear power plants were developed by state-owned or regulated utility monopolies where many of the risks associated with political change and regulatory ratcheting were borne by consumers rather than suppliers. Many countries have now liberalized the electricity market where these risks, and the risk of cheap competition from subsidised energy sources emerging before capital costs are recovered, are borne by plant suppliers and operators rather than consumers, which leads to a significantly different evaluation of the risk of investing in new nuclear power plants.\n\nTwo of the four EPRs under construction (the Olkiluoto Nuclear Power Plant in Finland and Flamanville in France), which are the latest new builds in Europe, are significantly behind schedule and substantially over cost. \nFollowing the 2011 Fukushima Daiichi nuclear disaster, costs are likely to go up for some types of currently operating and new nuclear power plants, due to new requirements for on-site spent fuel management and elevated design basis threats.\n\nAlthough the price of new plants in China is falling rapidly, approaching $1500/kW (or about a fifth of the cost of some plants currently being built in Europe), John Quiggin, an economics professor, maintains that the main problem with the nuclear option is that it is not economically viable.\nProfessor of science and technology Ian Lowe has also challenged the economics of nuclear power. However, nuclear supporters continue to point to the historical success of nuclear power across the world, and they call for new reactors in their own countries, including proposed new but largely uncommercialised designs, as a source of new power. Nuclear supporters point out that the IPCC climate panel endorses nuclear technology as a low carbon, mature energy source which should be nearly quadrupled to help address soaring greenhouse gas emissions.\n\nSome independent reviews keep repeating that nuclear power plants are necessarily very expensive, and anti-nuclear groups frequently produce reports that say the costs of nuclear energy are prohibitively high. This is despite the fact that in 2017 the cost of electricity for non-household consumers in nuclear France was approximately the same as in Denmark and two-thirds of that in Germany.\n\nIn 2012 in Ontario, Canada, costs for nuclear generation stood at 5.9¢/kWh while hydroelectricity, at 4.3¢/kWh, cost 1.6¢ less than nuclear. By September 2015, the cost of solar in the United States dropped below nuclear generation costs, averaging 5¢/kWh. Solar costs continued to fall, and by February 2016, the City of Palo Alto, California, approved a power-purchase agreement (PPA) to purchase solar electricity for under 3.68¢/kWh, lower than even hydroelectricity. Utility-scale solar electricity generation newly contracted by Palo Alto in 2016 costs 2.22¢/kWh less than electricity from the already-completed Canadian nuclear plants, and the costs of solar energy generation continue to drop. However, solar power has very low capacity factors compared to nuclear, and solar power can only achieve so much market penetration before (expensive) energy storage and transmission become necessary.\n\nMany countries, including Russia, South Korea, India, and China, have continued to pursue new builds. Globally, 71 nuclear power plants were under construction in 15 countries as of January 2015, according to the IAEA. China has 25 reactors under construction but, according to a government research unit, China must not build \"too many nuclear power reactors too quickly\", in order to avoid a shortfall of fuel, equipment and qualified plant workers. According to the World Nuclear Association, the global trend is for new nuclear power stations coming online to be balanced by the number of old plants being retired. But this worrying lack of development is limited to certain regions and may change once societies consider the importance of ample low cost and clean energy for their economies and quality of life.\n\nIn the United States, nuclear power faces competition from the low natural gas prices in North America. Former Exelon CEO John Rowe said in 2012 that new nuclear plants in the United States \"don’t make any sense right now\" and won’t be economic as long as the natural gas glut persists. In 2016, Governor of New York Andrew Cuomo directed the New York Public Service Commission to consider ratepayer-financed subsidies similar to those for renewable sources to keep nuclear power stations profitable in the competition against natural gas.\n\n\"The usual rule of thumb for nuclear power is that about two thirds of the generation cost is accounted for by fixed costs, the main ones being the cost of paying interest on the loans and repaying the capital...\" \nCapital cost, the building and financing of nuclear power plants, represents a large percentage of the cost of nuclear electricity. In 2014, the US Energy Information Administration estimated that for new nuclear plants going online in 2019, capital costs will make up 74% of the levelized cost of electricity; higher than the capital percentages for fossil-fuel power plants (63% for coal, 22% for natural gas), and lower than the capital percentages for some other nonfossil-fuel sources (80% for wind, 88% for solar PV).\n\nAreva, the French nuclear plant operator, offers that 70% of the cost of a kWh of nuclear electricity is accounted for by the fixed costs from the construction process. Some analysts argue (for example Steve Thomas, Professor of Energy Studies at the University of Greenwich in the UK, quoted in the book The Doomsday Machine by Martin Cohen and Andrew McKillop) that what is often not appreciated in debates about the economics of nuclear power is that the cost of equity, that is companies using their own money to pay for new plants, is generally higher than the cost of debt. Another advantage of borrowing may be that \"once large loans have been arranged at low interest rates – perhaps with government support – the money can then be lent out at higher rates of return\".\n\"One of the big problems with nuclear power is the enormous upfront cost. These reactors are extremely expensive to build. While the returns may be very great, they're also very slow. It can sometimes take decades to recoup initial costs. Since many investors have a short attention span, they don't like to wait that long for their investment to pay off.\"\nBecause of the large capital costs for the initial nuclear power plants built as part of a sustained build program, and the relatively long construction period before revenue is returned, servicing the capital costs of first few nuclear power plants can be the most important factor determining the economic competitiveness of nuclear energy. The investment can contribute about 70% to 80% of the costs of electricity. Timothy Stone, businessman and nuclear expert, stated in 2017 \"It has long been recognised that the only two numbers which matter in [new] nuclear power are the capital cost and the cost of capital.\" The discount rate chosen to cost a nuclear power plant's capital over its lifetime is arguably the most sensitive parameter to overall costs. Because of the long life of new nuclear power plants, most of the value of a new nuclear power plant is created for the benefit of future generations.\n\nThe recent liberalization of the electricity market in many countries has made the economics of nuclear power generation less attractive, and no new nuclear power plants have been built in a liberalized electricity market. Previously a monopolistic provider could guarantee output requirements decades into the future. Private generating companies now have to accept shorter output contracts and the risks of future lower-cost competition from subsidised energy sources, so they desire a shorter return on investment period. This favours generation plant types with lower capital costs or high subsidies, even if associated fuel costs are higher. A further difficulty is that due to the large sunk costs but unpredictable future income from the liberalized electricity market, private capital is unlikely to be available on favourable terms, which is particularly significant for nuclear as it is capital-intensive. Industry consensus is that a 5% discount rate is appropriate for plants operating in a regulated utility environment where revenues are guaranteed by captive markets, and 10% discount rate is appropriate for a competitive deregulated or merchant plant environment; however the independent MIT study (2003) which used a more sophisticated finance model distinguishing equity and debt capital had a higher 11.5% average discount rate.\n\nAs states are declining to finance nuclear power plants, the sector is now much more reliant on the commercial banking sector. According to research done by Dutch banking research group Profundo, commissioned by BankTrack, in 2008 private banks invested almost €176 billion in the nuclear sector. Champions were BNP Paribas, with more than €13,5 billion in nuclear investments and Citigroup and Barclays on par with both over €11,4 billion in investments. Profundo added up investments in eighty companies in over 800 financial relationships with 124 banks in the following sectors: construction, electricity, mining, the nuclear fuel cycle and \"other\".\n\nConstruction delays can add significantly to the cost of a plant. Because a power plant does not earn income and currencies can inflate during construction, longer construction times translate directly into higher finance charges. Modern nuclear power plants are planned for construction in five years or less (42 months for CANDU ACR-1000, 60 months from order to operation for an AP1000, 48 months from first concrete to operation for an EPR and 45 months for an ESBWR) as opposed to over a decade for some previous plants. However, despite Japanese success with ABWRs, two of the four EPRs under construction (in Finland and France) are significantly behind schedule.\n\nIn the United States many new regulations were put in place in the years before and again immediately after the Three Mile Island accident's partial meltdown, resulting in plant startup delays of many years. The NRC has new regulations in place now (see Combined Construction and Operating License), and the next plants will have NRC Final Design Approval before the customer buys them, and a Combined Construction and Operating License will be issued before construction starts, guaranteeing that if the plant is built as designed then it will be allowed to operate—thus avoiding lengthy hearings after completion.\n\nIn Japan and France, construction costs and delays are significantly diminished because of streamlined government licensing and certification procedures. In France, one model of reactor was type-certified, using a safety engineering process similar to the process used to certify aircraft models for safety. That is, rather than licensing individual reactors, the regulatory agency certified a particular design and its construction process to produce safe reactors. U.S. law permits type-licensing of reactors, a process which is being used on the AP1000 and the ESBWR.\n\nIn Canada, cost overruns for the Darlington Nuclear Generating Station, largely due to delays and policy changes, are often cited by opponents of new reactors. Construction started in 1981 at an estimated cost of $7.4 Billion 1993-adjusted CAD, and finished in 1993 at a cost of $14.5 billion. 70% of the price increase was due to interest charges incurred due to delays imposed to postpone units 3 and 4, 46% inflation over a 4-year period and other changes in financial policy. No new nuclear reactor has since been built in Canada, although a few have been and are undergoing refurbishment and environment assessment is complete for 4 new generation stations at Darlington with the Ontario government committed in keeping a nuclear base load of 50% or around 10GW.\n\nIn the United Kingdom and the United States cost overruns on nuclear plants contributed to the bankruptcies of several utility companies. In the United States these losses helped usher in energy deregulation in the mid-1990s that saw rising electricity rates and power blackouts in California. When the UK began privatizing utilities, its nuclear reactors \"were so unprofitable they could not be sold.\" Eventually in 1996, the government gave them away. But the company that took them over, British Energy, had to be bailed out in 2004 to the extent of 3.4 billion pounds.\n\nIn general, coal and nuclear plants have the same types of operating costs (operations and maintenance plus fuel costs). However, nuclear has lower fuel costs but higher operating and maintenance costs.\n\nNuclear plants require fissile fuel. Generally, the fuel used is uranium, although other materials may be used (See MOX fuel). In 2005, prices on the world market for uranium averaged US$20/lb (US$44.09/kg). On 2007-04-19, prices reached US$113/lb (US$249.12/kg). On 2008-07-02, the price had dropped to $59/lb.\n\nFuel costs account for about 28% of a nuclear plant's operating expenses. As of 2013, half the cost of reactor fuel was taken up by enrichment and fabrication, so that the cost of the uranium concentrate raw material was 14 percent of operating costs. Doubling the price of uranium would add about 10% to the cost of electricity produced in existing nuclear plants, and about half that much to the cost of electricity in future power plants. The cost of raw uranium contributes about $0.0015/kWh to the cost of nuclear electricity, while in breeder reactors the uranium cost falls to $0.000015/kWh.\n\nAs of 2008, mining activity was growing rapidly, especially from smaller companies, but putting a uranium deposit into production takes 10 years or more. The world's present measured resources of uranium, economically recoverable at a price of 130 USD/kg according to the industry groups Organisation for Economic Co-operation and Development (OECD), Nuclear Energy Agency (NEA) and International Atomic Energy Agency (IAEA), are enough to last for \"at least a century\" at current consumption rates.\n\nAccording to the World Nuclear Association, \"the world's present measured resources of uranium (5.7 Mt) in the cost category less than three times present spot prices and used only in conventional reactors, are enough to last for about 90 years. This represents a higher level of assured resources than is normal for most minerals. Further exploration and higher prices will certainly, on the basis of present geological knowledge, yield further resources as present ones are used up.\" The amount of uranium present in all currently known conventional reserves alone (excluding the huge quantities of currently-uneconomical uranium present in \"unconventional\" reserves such as phosphate/phosphorite deposits, seawater, and other sources) is enough to last over 200 years at current consumption rates. Fuel efficiency in conventional reactors has increased over time. Additionally, since 2000, 12–15% of world uranium requirements have been met by the dilution of highly enriched weapons-grade uranium from the decommissioning of nuclear weapons and related military stockpiles with depleted uranium, natural uranium, or partially-enriched uranium sources to produce low-enriched uranium for use in commercial power reactors. Similar efforts have been utilizing weapons-grade plutonium to produce mixed oxide (MOX) fuel, which is also produced from reprocessing used fuel. Other components of used fuel are currently less commonly utilized, but have a substantial capacity for reuse, especially so in next-generation fast neutron reactors. Over 35 European reactors are licensed to use MOX fuel, as well as Russian and American nuclear plants. Reprocessing of used fuel increases utilization by approximately 30%, while the widespread use of fast breeder reactors would allow for an increase of \"50-fold or more\" in utilization.\n\nAll nuclear plants produce radioactive waste. To pay for the cost of storing, transporting and disposing these wastes in a permanent location, in the United States a surcharge of a tenth of a cent per kilowatt-hour is added to electricity bills. Roughly one percent of electrical utility bills in provinces using nuclear power are diverted to fund nuclear waste disposal in Canada.\n\nIn 2009, the Obama administration announced that the Yucca Mountain nuclear waste repository would no longer be considered the answer for U.S. civilian nuclear waste. Currently, there is no plan for disposing of the waste and plants will be required to keep the waste on the plant premises indefinitely.\n\nThe disposal of low level waste reportedly costs around £2,000/m³ in the UK. High level waste costs somewhere between £67,000/m³ and £201,000/m³. General division is 80%/20% of low level/high level waste, and one reactor produces roughly 12 m³ of high level waste annually.\n\nIn Canada, the NWMO was created in 2002 to oversee long term disposal of nuclear waste, and in 2007 adopted the Adapted Phased Management procedure. Long term management is subject to change based on technology and public opinion, but currently largely follows the recommendations for a centralized repository as first extensively outlined in by AECL in 1988. It was determined after extensive review that following these recommendations would safely isolate the waste from the biosphere. The location has not yet been determined, and the project is expected to cost between $9 and $13 billion CAD for construction and operation for 60–90 years, employing roughly a thousand people for the duration. Funding is available and has been collected since 1978 under the Canadian Nuclear Fuel Waste Management Program. Very long term monitoring requires less staff since high-level waste is less toxic than naturally occurring uranium ore deposits within a few centuries.\n\nThe primary argument for pursuing IFR-style technology today is that it provides the best solution to the existing nuclear waste problem because fast reactors can be fueled from the waste products of existing reactors as well as from the plutonium used in weapons, as is the case of the discontinued EBR-II in Arco, Idaho, and in the operating, as of 2014, BN-800 reactor. Depleted uranium (DU) waste can also be used as fuel in fast reactors. Waste produced by a fast-neutron reactor and a pyroelectric refiner would consist only of fission products, which are produced at a rate of about one tonne per GWe-year. This is 5% as much as present reactors produce, and needs special custody for only 300 years instead of 300,000. Only 9.2% of fission products (strontium and caesium) contribute 99% of radiotoxicity; at some additional cost, these could be separated, reducing the disposal problem by a further factor of ten.\n\nAt the end of a nuclear plant's lifetime, the plant must be decommissioned. This entails either dismantling, safe storage or entombment. In the United States, the Nuclear Regulatory Commission (NRC) requires plants to finish the process within 60 years of closing. Since it may cost $500 million or more to shut down and decommission a plant, the NRC requires plant owners to set aside money when the plant is still operating to pay for the future shutdown costs.\n\nDecommissioning a reactor that has undergone a meltdown is inevitably more difficult and expensive. Three Mile Island was decommissioned 14 years after its incident for $837 million. The cost of the Fukushima disaster cleanup is not yet known, but has been estimated to cost around $100 billion. Chernobyl is not yet decommissioned, different estimates put the end date between 2013 and 2020.\n\nA 2011 report for the Union of Concerned Scientists stated that \"the costs of preventing nuclear proliferation and terrorism should be recognized as negative externalities of civilian nuclear power, thoroughly evaluated, and integrated into economic assessments—just as global warming emissions are increasingly identified as a cost in the economics of coal-fired electricity\".\n\nWhen a country pursues civilian nuclear power, it is often indistinguishable whether the country's motives are honest or if it is a cover for a weapons program. \nThe same facilities being used for uranium enrichment for reactor fuel can also be used to enrich uranium for weapons. In 2018 when North Korea turned on a \"power\" reactor, there was uncertainty in defense circles whether it was for power use or plutonium production: \"\"Rob Munks, editor of Jane's Intelligence Review, says the light-water reactor \"could be used for civilian electricity generation -- its stated purpose -- or diverted towards the nuclear program.\"\"\n\n\"Construction of the ELWR was completed in 2013 and is optimized for civilian electricity production, but it has \"dual-use\" potential and can be modified to produce material for nuclear weapons.\"\n\nNuclear safety and security is a chief goal of the nuclear industry. Great care is taken so that accidents are avoided, and if unpreventable, have limited consequences. Accidents could stem from system failures related to faulty construction or pressure vessel embrittlement due to prolonged radiation exposure. As with any aging technology, risks of failure increase over time, and since many currently operating nuclear reactors were built in the mid 20th century, care must be taken to ensure proper operation. Many more recent reactor designs have been proposed, most of which include passive safety systems. These design considerations serve to significantly mitigate or totally prevent major accidents from occurring, even in the event of a system failure. Still, reactors must be designed, built, and operated properly to minimize accident risks. The Fukushima disaster represents one instance where these systems were not comprehensive enough, where the tsunami following the Tōhoku earthquake disabled the backup generators that were stabilizing the reactor. According to UBS AG, the Fukushima I nuclear accidents have cast doubt on whether even an advanced economy like Japan can master nuclear safety. Catastrophic scenarios involving terrorist attacks are also conceivable.\n\nAn interdisciplinary team from MIT estimated that given the expected growth of nuclear power from 2005 to 2055, at least four core damage incidents would be expected in that period (assuming only \"current\" designs were used – the number of incidents expected in that same time period with the use of advanced designs is only one). To date, there have been five core damage incidents in the world since 1970 (one at Three Mile Island in 1979; one at Chernobyl in 1986; and three at Fukushima-Daiichi in 2011), corresponding to the beginning of the operation of generation II reactors.\n\nAccording to the Paul Scherrer Institute, the Chernobyl incident is the only incident ever to have caused any fatalities. The report that UNSCEAR presented to the UN General Assembly in 2011 states that 29 plant workers and emergency responders died from effects of radiation exposure, two died from causes related to the incident but unrelated to radiation, and one died from coronary thrombosis. It attributed fifteen cases of fatal thyroid cancer to the incident. It said there is no evidence the incident caused an ongoing increase in incidence of solid tumors or blood cancers in Eastern Europe. With 46 deaths in its entire six-decade worldwide history, nuclear power remains the safest-ever way to make electricity, by a very wide margin.\n\nIn terms of nuclear accidents, the Union of Concerned Scientists have claimed that \"reactor owners ... have never been economically responsible for the full costs and risks of their operations. Instead, the public faces the prospect of severe losses in the event of any number of potential adverse scenarios, while private investors reap the rewards if nuclear plants are economically successful. For all practical purposes, nuclear power's economic gains are privatized, while its risks are socialized\".\n\nHowever, the problem of insurance costs for worst-case scenarios is not unique to nuclear power: hydroelectric power plants are similarly not fully insured against a catastrophic event such as the Banqiao Dam disaster, where 11 million people lost their homes and from 30,000 to 200,000 people died, or large dam failures in general. Private insurers base dam insurance premiums on worst-case scenarios, so insurance for major disasters in this sector is likewise provided by the state. In the US, insurance coverage for nuclear reactors is provided by the combination of operator-purchased private insurance and the primarily operator-funded Price Anderson Act.\n\nAny effort to construct a new nuclear facility around the world, whether an existing design or an experimental future design, must deal with NIMBY or NIABY objections. Because of the high profiles of the Three Mile Island accident and Chernobyl disaster, relatively few municipalities welcome a new nuclear reactor, processing plant, transportation route, or deep geological repository within their borders, and some have issued local ordinances prohibiting the locating of such facilities there.\n\nNancy Folbre, an economics professor at the University of Massachusetts, has questioned the economic viability of nuclear power following the 2011 Japanese nuclear accidents:\n\nThe proven dangers of nuclear power amplify the economic risks of expanding reliance on it. Indeed, the stronger regulation and improved safety features for nuclear reactors called for in the wake of the Japanese disaster will almost certainly require costly provisions that may price it out of the market.\nThe cascade of problems at Fukushima, from one reactor to another, and from reactors to fuel storage pools, will affect the design, layout and ultimately the cost of future nuclear plants.\n\nIn 1986, Pete Planchon conducted a demonstration of the inherent safety of the Integral Fast Reactor. Safety interlocks were turned off. Coolant circulation was turned off. Core temperature rose from the usual 1000 degrees Fahrenheit to 1430 degrees within 20 seconds. The boiling temperature of the sodium coolant is 1621 degrees. Within seven minutes the reactor had shut itself down without action from the operators, without valves, pumps, computers, auxiliary power, or any moving parts. The temperature was below the operating temperature. The reactor was not damaged. The operators were not injured. There was no release of radioactive material. The reactor was restarted with coolant circulation but the steam generator disconnected. The same scenario recurred. Three weeks later, the operators at Chernobyl repeated the latter experiment, ironically in a rush to complete a safety test, using a very different reactor, with tragic consequences. Safety of the Integral Fast Reactor depends on the composition and geometry of the core, not efforts by operators or computer algorithms.\n\nInsurance available to the operators of nuclear power plants varies by nation. The worst case nuclear accident costs are so large that it would be difficult for the private insurance industry to carry the size of the risk, and the premium cost of full insurance would make nuclear energy uneconomic.\n\nNuclear power has largely worked under an insurance framework that limits or structures accident liabilities in accordance with the Paris convention on nuclear third-party liability, the Brussels supplementary convention, the Vienna convention on civil liability for nuclear damage, and in the United States the Price-Anderson Act. It is often argued that this potential shortfall in liability represents an external cost not included in the cost of nuclear electricity.\n\nHowever, the problem of insurance costs for worst-case scenarios is not unique to nuclear power: hydroelectric power plants are similarly not fully insured against a catastrophic event such as the Banqiao Dam disaster, where 11 million people lost their homes and from 30,000 to 200,000 people died, or large dam failures in general. Private insurers base dam insurance premiums on worst-case scenarios, so insurance for major disasters in this sector is likewise provided by the state.\n\nIn Canada, the Canadian Nuclear Liability Act requires nuclear power plant operators to obtain $650 million (CAD) of liability insurance coverage per installation (regardless of the number of individual reactors present) starting in 2017 (up from the prior $75 million requirement established in 1976), increasing to $750 million in 2018, to $850 million in 2019, and finally to $1 billion in 2020. Claims beyond the insured amount would be assessed by a government appointed but independent tribunal, and paid by the federal government.\n\nIn the UK, the Nuclear Installations Act 1965 governs liability for nuclear damage for which a UK nuclear licensee is responsible. The limit for the operator is £140 million.\n\nIn the United States, the Price-Anderson Act has governed the insurance of the nuclear power industry since 1957. Owners of nuclear power plants are required to pay a premium each year for the maximum obtainable amount of private insurance ($450 million) for each licensed reactor unit. This primary or \"first tier\" insurance is supplemented by a second tier. In the event a nuclear accident incurs damages in excess of $450 million, each licensee would be assessed a prorated share of the excess up to $121,255,000. With 104 reactors currently licensed to operate, this secondary tier of funds contains about $12.61 billion. This results in a maximum combined primary+secondary coverage amount of up to $13.06 billion for a hypothetical single-reactor incident. If 15 percent of these funds are expended, prioritization of the remaining amount would be left to a federal district court. If the second tier is depleted, Congress is committed to determine whether additional disaster relief is required. In July 2005, Congress extended the Price-Anderson Act to newer facilities.\n\nThe Vienna Convention on Civil Liability for Nuclear Damage and the Paris Convention on Third Party Liability in the Field of Nuclear Energy put in place two similar international frameworks for nuclear liability. The limits for the conventions vary. The Vienna convention was adapted in 2004 to increase the operator liability to €700 million per incident, but this modification is not yet ratified.\n\nThe cost per unit of electricity produced (kWh) will vary according to country, depending on costs in the area, the regulatory regime and consequent financial and other risks, and the availability and cost of finance. Costs will also depend on geographic factors such as availability of cooling water, earthquake likelihood, and availability of suitable power grid connections. So it is not possible to accurately estimate costs on a global basis.\n\nCommodity prices rose in 2008, and so all types of plants became more expensive than previously calculated.\nIn June 2008 Moody's estimated that the cost of installing new nuclear capacity in the United States might possibly exceed $7,000/KW in final cost.\nIn comparison, the reactor units already under construction in China have been reported with substantially lower costs due to significantly lower labour rates.\n\nA 2008 study by former utility staffperson Craig A. Severance based on historical outcomes in the United States said costs for nuclear power can be expected to run $0.25–0.30 per kWh.\n\nA 2008 study concluded that if carbon capture and storage were required then nuclear power would be the cheapest source of electricity even at $4,038/kW in overnight capital cost.\n\nIn 2009, MIT updated its 2003 study, concluding that inflation and rising construction costs had increased the overnight cost of nuclear power plants to about $4,000/kW, and thus increased the power cost to $0.084/kWh. The 2003 study had estimated the cost as $0.067/kWh.\n\nAccording to Benjamin K. Sovacool, the marginal levelized cost for \"a 1,000-MW facility built in 2009 would be 41.2 to 80.3 cents/kWh, presuming one actually takes into account construction, operation and fuel, reprocessing, waste storage, and decommissioning\".\n\nA 2013 study indicates that the cost competitiveness of nuclear power is \"questionable\" and that public support will be required if new power stations are to be built within liberalized electricity markets.\n\nIn 2014, the US Energy Information Administration estimated the levelized cost of electricity from new nuclear power plants going online in 2019 to be $0.096/kWh before government subsidies, comparable to the cost of electricity from a new coal-fired power plant without carbon capture, but higher than the cost from natural gas-fired plants.\n\nGenerally, a nuclear power plant is significantly more expensive to build than an equivalent coal-fueled or gas-fueled plant. If natural gas is plentiful and cheap operating costs of conventional power plants is less. Most forms of electricity generation produce some form of negative externality — costs imposed on third parties that are not directly paid by the producer — such as pollution which negatively affects the health of those near and downwind of the power plant, and generation costs often do not reflect these external costs.\n\nA comparison of the \"real\" cost of various energy sources is complicated by a number of uncertainties:\n\nA UK Royal Academy of Engineering report in 2004 looked at electricity generation costs from new plants in the UK. In particular it aimed to develop \"a robust approach to compare directly the costs of intermittent generation with more dependable sources of generation\". This meant adding the cost of standby capacity for wind, as well as carbon values up to £30 (€45.44) per tonne CO for coal and gas. Wind power was calculated to be more than twice as expensive as nuclear power. Without a carbon tax, the cost of production through coal, nuclear and gas ranged £0.022–0.026/kWh and coal gasification was £0.032/kWh. When carbon tax was added (up to £0.025) coal came close to onshore wind (including back-up power) at £0.054/kWh — offshore wind is £0.072/kWh — nuclear power remained at £0.023/kWh either way, as it produces negligible amounts of CO. (Nuclear figures included estimated decommissioning costs.)\n\nA May 2008 study by the Congressional Budget Office concludes that a carbon tax of $45 per tonne of carbon dioxide would probably make nuclear power cost competitive against conventional fossil fuel for electricity generation.\n\nEstimates of total lifetime energy returned on energy invested vary greatly depending on the study. An overview can be found here (Table 2):\n\nThe effect of subsidies is difficult to gauge, as some are indirect (such as research and development). A May 12, 2008 editorial in \"The Wall Street Journal\" stated, \"For electricity generation, the EIA (Energy Information Administration, an office of the Department of Energy) concludes that solar energy is subsidized to the tune of $24.34 per megawatt hour, wind $23.37 and 'clean coal' $29.81. By contrast, normal coal receives 44 cents, natural gas a mere quarter, hydroelectric about 67 cents and nuclear power $1.59.\"\n\nLazard's report on the estimated levelized cost of energy by source (10th edition) estimated unsubsidized prices of $97–$136/MWh for nuclear, $50–$60/MWh for solar PV, $32–$62/MWh for onshore wind, and $82–$155/MWh for offshore wind. However, their report did not take into account any of the significant integration costs associated with variable renewable resources, assumed an unrealistically short facility lifetime of just 40 years for the nuclear plant, and used global illustrative costs of capital instead of the significantly lower OECD or North American costs of capital.\n\nHowever, the most important subsidies to the nuclear industry do not involve cash payments. Rather, they shift construction costs and operating risks from investors to taxpayers and ratepayers, burdening them with an array of risks including cost overruns, defaults to accidents, and nuclear waste management. This approach has remained remarkably consistent throughout the nuclear industry's history, and distorts market choices that would otherwise favor less risky energy investments.\n\nIn 2011, Benjamin K. Sovacool said that: \"When the full nuclear fuel cycle is considered — not only reactors but also uranium mines and mills, enrichment facilities, spent fuel repositories, and decommissioning sites — nuclear power proves to be one of the costliest sources of energy\".\n\nIn 2014, Brookings Institution published \"The Net Benefits of Low and No-Carbon Electricity Technologies\" which states, after performing an energy and emissions cost analysis, that \"The net benefits of new nuclear, hydro, and natural gas combined cycle plants far outweigh the net benefits of new wind or solar plants\", with the most cost effective low carbon power technology being determined to be nuclear power. Moreover, Paul Joskow of MIT has determined that the \"Levelized cost of electricity\" (LCOE) metric is a poor means of comparing electricity sources as it hides the extra costs, such as the need to frequently operate back up power stations, incurred due to the use of intermittent power sources such as wind energy, while the value of baseload power sources are underpresented.\n\nAn EU-funded research study known as ExternE, or Externalities of Energy, undertaken from 1995 to 2005, found that the cost of producing electricity from coal or oil would double, and the cost of electricity production from gas would increase by 30% if external costs such as damage to the environment and to human health, from the particulate matter, nitrogen oxides, chromium VI, river water alkalinity, mercury poisoning and arsenic emissions produced by these sources, were taken into account. It was estimated in the study that these external, downstream, fossil fuel costs amount up to 1–2% of the EU's Gross Domestic Product, and this was before the external cost of global warming from these sources was included. The study also found that the environmental and health costs of nuclear power, per unit of energy delivered, was lower than many renewable sources, including that caused by biomass and photovoltaic solar panels, but was higher than the external costs associated with wind power and alpine hydropower.\n\nKristin Shrader-Frechette analysed 30 papers on the economics of nuclear power for possible conflicts of interest. She found of the 30, 18 had been funded either by the nuclear industry or pro-nuclear governments and were pro-nuclear, 11 were funded by universities or non-profit non-government organisations and were anti-nuclear, the remaining 1 had unknown sponsors and took the pro-nuclear stance. The pro-nuclear studies were accused of using cost-trimming methods such as ignoring government subsidies and using industry projections above empirical evidence where ever possible. The situation was compared to medical research where 98% of industry sponsored studies return positive results.\n\nNuclear Power plants tend to be very competitive in areas where other fuel resources are not readily available — France, most notably, has almost no native supplies of fossil fuels. France's nuclear power experience has also been one of paradoxically increasing rather than decreasing costs over time.\n\nMaking a massive investment of capital in a project with long-term recovery might affect a company's credit rating.\n\nA Council on Foreign Relations report on nuclear energy argues that a rapid expansion of nuclear power may create shortages in building materials such as reactor-quality concrete and steel, skilled workers and engineers, and safety controls by skilled inspectors. This would drive up current prices. It may be easier to rapidly expand, for example, the number of coal power plants, without this having a large effect on current prices.\n\nExisting nuclear plants generally have a somewhat limited ability to significantly vary their output in order to match changing demand (a practice called load following). However, many BWRs, some PWRs (mainly in France), and certain CANDU reactors (primarily those at Bruce Nuclear Generating Station) have various levels of load-following capabilities (sometimes substantial), which allow them to fill more than just baseline generation needs. Several newer reactor designs also offer some form of enhanced load-following capability. For example, the Areva EPR can slew its electrical output power between 990 and 1,650 MW at 82.5 MW per minute.\n\nThe number of companies that manufacture certain parts for nuclear reactors is limited, particularly the large forgings used for reactor vessels and steam systems. Only four companies (Japan Steel Works, China First Heavy Industries, Russia's OMZ Izhora and Korea's Doosan Heavy Industries) currently manufacture pressure vessels for reactors of 1100 MW or larger. Some have suggested that this poses a bottleneck that could hamper expansion of nuclear power internationally, however, some Western reactor designs require no steel pressure vessel such as CANDU derived reactors which rely on individual pressurized fuel channels. The large forgings for steam generators — although still very heavy — can be produced by a far larger number of suppliers.\n\nThe nuclear power industry in Western nations has a history of construction delays, cost overruns, plant cancellations, and nuclear safety issues despite significant government subsidies and support. In December 2013, \"Forbes\" magazine reported that, in developed countries, \"reactors are not a viable source of new power\". Even in developed nations where they make economic sense, they are not feasible because nuclear’s “enormous costs, political and popular opposition, and regulatory uncertainty”. This view echoes the statement of former Exelon CEO John Rowe, who said in 2012 that new nuclear plants “don’t make any sense right now” and won’t be economically viable in the foreseeable future. John Quiggin, economics professor, also says the main problem with the nuclear option is that it is not economically-viable. Quiggin says that we need more efficient energy use and more renewable energy commercialization. Former NRC member Peter Bradford and Professor Ian Lowe have recently made similar statements. However, some \"nuclear cheerleaders\" and lobbyists in the West continue to champion reactors, often with proposed new but largely untested designs, as a source of new power.\n\nSignificant new build activity is occurring in developing countries like South Korea, India and China. China has 25 reactors under construction, However, according to a government research unit, China must not build \"too many nuclear power reactors too quickly\", in order to avoid a shortfall of fuel, equipment and qualified plant workers.\n\nThe 1.6 GW EPR reactor is being built in Olkiluoto Nuclear Power Plant, Finland. A joint effort of French AREVA and German Siemens AG, it will be the largest pressurized water reactor (PWR) in the world. The Olkiluoto project has been claimed to have benefited from various forms of government support and subsidies, including liability limitations, preferential financing rates, and export credit agency subsidies, but the European Commission's investigation didn't find anything illegal in the proceedings. However, as of August 2009, the project is \"more than three years behind schedule and at least 55% over budget, reaching a total cost estimate of €5 billion ($7 billion) or close to €3,100 ($4,400) per kilowatt\". Finnish electricity consumers interest group ElFi OY evaluated in 2007 the effect of Olkiluoto-3 to be slightly over 6%, or €3/MWh, to the average market price of electricity within Nord Pool Spot. The delay is therefore \"costing\" the Nordic countries over 1.3 billion euros per year as the reactor would replace more expensive methods of production and lower the price of electricity.\n\nRussia has begun building the world's first floating nuclear power plant. The £100 million vessel, the \"Akademik Lomonosov\", is the first of seven plants (70 MW per ship) that Moscow says will bring vital energy resources to remote Russian regions.\n\nFollowing the Fukushima nuclear disaster in 2011, costs are likely to go up for currently operating and new nuclear power plants, due to increased requirements for on-site spent fuel management and elevated design basis threats. After Fukushima, the International Energy Agency halved its estimate of additional nuclear generating capacity built by 2035.\n\nMany license applications filed with the U.S. Nuclear Regulatory Commission for proposed new reactors have been suspended or cancelled. As of October 2011, plans for about 30 new reactors in the United States have been reduced to 14. There are currently five new nuclear plants under construction in the United States (Watts Bar 2, Summer 2, Summer 3, Vogtle 3, Vogtle 4). Matthew Wald from \"The New York Times\" has reported that \"the nuclear renaissance is looking small and slow\".\n\nIn 2013, four aging, uncompetitive reactors were permanently closed in the US: San Onofre 2 and 3 in California, Crystal River 3 in Florida, and Kewaunee in Wisconsin. The state of Vermont is trying to close Vermont Yankee, in Vernon. New York State is seeking to close Indian Point Nuclear Power Plant, in Buchanan, 30 miles from New York City. The additional cancellation of five large reactor uprates (Prairie Island, 1 reactor, LaSalle, 2 reactors, and Limerick, 2 rectors), four by the largest nuclear company in the United States, suggest that the nuclear industry faces \"a broad range of operational and economic problems\".\n\nAs of July 2013, economist Mark Cooper has identified some US nuclear power plants that face particularly significant challenges to their continued operation due to regulatory policies. These are Palisades, Fort Calhoun, Nine Mile Point, Fitzpatrick, Ginna, Oyster Creek, Vermont Yankee, Millstone, Clinton, Indian Point. Cooper said the lesson here for policy makers and economists is clear: \"nuclear reactors are simply not competitive\". In 2017 analysis by Bloomberg showed that over half of U.S. nuclear plants were running at a loss.\n\n\n"}
{"id": "30373477", "url": "https://en.wikipedia.org/wiki?curid=30373477", "title": "Elías Díaz Peña", "text": "Elías Díaz Peña\n\nElias Diaz Peña is a Paraguayan environmentalist. He was awarded the Goldman Environmental Prize in 1992, jointly with Oscar Rivas, for their efforts to protect the ecosystems of the Paraná River and the Paraguay River.\n"}
{"id": "17904090", "url": "https://en.wikipedia.org/wiki?curid=17904090", "title": "Eolica Baia Wind Farm", "text": "Eolica Baia Wind Farm\n\nThe Eolica Baia Wind Farm is a proposed wind power project in Baia, Tulcea County, Romania. The project consists of four individual wind farms connected together. It will have 63 individual wind turbines with a nominal output of around 2 MW. The wind farm will deliver up to 126 MW of power, enough to supply over 75,600 homes with a capital investment required of approximately US$150 million.\n"}
{"id": "3722666", "url": "https://en.wikipedia.org/wiki?curid=3722666", "title": "Excavations at Stonehenge", "text": "Excavations at Stonehenge\n\nRecords of archaeological excavations at the Stonehenge site date back to the early 17th century.\n\nThe first known excavations at Stonehenge were undertaken by Dr William Harvey and Gilbert North in the early 17th century. Both Inigo Jones and the Duke of Buckingham also dug there shortly afterwards. In 1666 the antiquarian John Aubrey could still see the central sunken hollow where the Duke of Buckingham’s pit had been filled. A few minor investigations followed.\n\nFurther excavations at Stonehenge were carried out by William Cunnington and Richard Colt Hoare. In 1798, Cunnington investigated the pit beneath a recently fallen trilithon, and in 1810 both men dug beneath the fallen Slaughter Stone and concluded that it had once stood up. They may have also excavated one of the Aubrey Holes beneath it. In 1839, a Captain Beamish dug around the Altar Stone, and not long after that Charles Darwin was granted permission by the Antrobus family who owned Stonehenge to conduct a small excavation to test his theories about earthworm activity burying ancient structures.\n\nOn New Year's Eve 1900, Stone 22 of the Sarsen Circle fell over, taking with it a lintel. Following public pressure and a letter to \"The Times\" by William Flinders Petrie, the then owner of Stonehenge, Edmund Antrobus, agreed to some remedial engineering work to be undertaken with archaeological supervision so that records could be made of the below ground archaeology. Antrobus appointed a mining engineer named William Gowland to manage the work. Despite having no archaeological training, Gowland produced some of the finest, most detailed excavation records ever made at the monument. Gowland established that antler picks were used to dig the stone holes and suggested the stones themselves were worked to shape on site.\n\nThe largest series of excavations at Stonehenge were undertaken by Colonel William Hawley and his assistant Robert Newall after the site came into state hands. Stonehenge and of land was purchased by Mr. Cecil Chubb for £6,600 on September 21, 1915 for his wife — she donated the land to the British government three years later. Their work began in 1919 following the transfer of land, funded by the Office of Works, and continued until 1926. Hawley and Newall excavated portions of most of the features at Stonehenge and were the first to establish that it was a multi-phase site.\n\nIn 1950 the Society of Antiquaries commissioned Richard J. C. Atkinson, Stuart Piggott and John FS Stone to carry out further excavations. They recovered many cremations and developed the phrasing that still dominates much of what is written about Stonehenge.\n\nAs part of service trenching in 1979 and 1980, Mike Pitts led two smaller investigations close by the Heelstone, finding the evidence for its neighbour. More recent excavations have been held to mitigate the effects of electrical cables, sewage pipes, and a footpath through the site.\n\nSince 2003, Mike Parker Pearson has led investigations in the stones area as part of the Stonehenge Riverside Project in an attempt to better relate Stonehenge to its surrounding environs. National Geographic Channel screened a two-hour documentary exploring Parker Pearson's theories and the work of the Riverside Project in depth in May 2008. In April 2008 Professor Tim Darvill of the University of Bournemouth and Professor Geoff Wainwright of the Society of Antiquaries excavated a small area inside the stone circle It is hoped this will establish a more precise date for the earliest stone structure that occupied the Q and R Holes.\n\nFrom 2005 excavation of the area around a spring pool known as Blick Mead about a mile from Stonehenge, have taken place under the direction of Professor David Jacques of the University of Buckingham. These have revealed the earliest settlement in the area dating to the period 7900 BC to 4050 BC.\n\nBritain's Bournemouth University archaeologists, led by Geoffrey Wainwright, president of the London Society of Antiquaries, and Timothy Darvill, on September 22, 2008, found it may have been an ancient healing and pilgrimage site, since burials around Stonehenge showed trauma and deformity evidence: \"It was the magical qualities of these stones which ... transformed the monument and made it a place of pilgrimage for the sick and injured of the Neolithic world.\" Radio-carbon dating places the construction of the circle of bluestones at between 2,400 B.C. and 2,200 B.C., but they discovered charcoals dating 7,000 B.C., showing human activity in the site. It could be a primeval equivalent of Lourdes, since the area was already visited 4,000 years before the oldest stone circle, and attracted visitors for centuries after its abandonment.\n\n"}
{"id": "11807", "url": "https://en.wikipedia.org/wiki?curid=11807", "title": "Ferromagnetism", "text": "Ferromagnetism\n\nFerromagnetism is the basic mechanism by which certain materials (such as iron) form permanent magnets, or are attracted to magnets. In physics, several different types of magnetism are distinguished. Ferromagnetism (along with the similar effect ferrimagnetism) is the strongest type and is responsible for the common phenomena of magnetism in magnets encountered in everyday life. Substances respond weakly to magnetic fields with three other types of magnetism, paramagnetism, diamagnetism, and antiferromagnetism, but the forces are usually so weak that they can only be detected by sensitive instruments in a laboratory. An everyday example of ferromagnetism is a refrigerator magnet used to hold notes on a refrigerator door. The attraction between a magnet and ferromagnetic material is \"the quality of magnetism first apparent to the ancient world, and to us today\".\n\nPermanent magnets (materials that can be magnetized by an external magnetic field and remain magnetized after the external field is removed) are either ferromagnetic or ferrimagnetic, as are the materials that are noticeably attracted to them. Only a few substances are ferromagnetic. The common ones are iron, nickel, cobalt and most of their alloys, and some compounds of rare earth metals.\nFerromagnetism is very important in industry and modern technology, and is the basis for many electrical and electromechanical devices such as electromagnets, electric motors, generators, transformers, and magnetic storage such as tape recorders, and hard disks, and nondestructive testing of ferrous materials.\n\nHistorically, the term \"ferromagnetism\" was used for any material that could exhibit spontaneous magnetization: a net magnetic moment in the absence of an external magnetic field. This general definition is still in common use.\n\nHowever, in a landmark paper in 1948, Louis Néel showed there are two levels of magnetic alignment that result in this behavior. One is ferromagnetism in the strict sense, where all the magnetic moments are aligned. The other is \"ferrimagnetism\", where some magnetic moments point in the opposite direction but have a smaller contribution, so there is still a spontaneous magnetization.\n\nIn the special case where the opposing moments balance completely, the alignment is known as \"antiferromagnetism\"; but antiferromagnets do not have a spontaneous magnetization.\n\nThe table above lists a selection of ferromagnetic and ferrimagnetic compounds, along with the temperature above which they cease to exhibit spontaneous magnetization (see Curie temperature).\n\nFerromagnetism is a property not just of the chemical make-up of a material, but of its crystalline structure and microstructure. There are ferromagnetic metal alloys whose constituents are not themselves ferromagnetic, called Heusler alloys, named after Fritz Heusler. Conversely there are non-magnetic alloys, such as types of stainless steel, composed almost exclusively of ferromagnetic metals.\n\nAmorphous (non-crystalline) ferromagnetic metallic alloys can be made by very rapid quenching (cooling) of a liquid alloy. These have the advantage that their properties are nearly isotropic (not aligned along a crystal axis); this results in low coercivity, low hysteresis loss, high permeability, and high electrical resistivity. One such typical material is a transition metal-metalloid alloy, made from about 80% transition metal (usually Fe, Co, or Ni) and a metalloid component (B, C, Si, P, or Al) that lowers the melting point.\nA relatively new class of exceptionally strong ferromagnetic materials are the rare-earth magnets. They contain lanthanide elements that are known for their ability to carry large magnetic moments in well-localized f-orbitals.\n\nA number of actinide compounds are ferromagnets at room temperature or exhibit ferromagnetism upon cooling. PuP is a paramagnet with cubic symmetry at room temperature, but which undergoes a structural transition into a tetragonal state with ferromagnetic order when cooled below its T = 125 K. In its ferromagnetic state, PuP's easy axis is in the <100> direction.\n\nIn NpFe the easy axis is <111>. Above NpFe is also paramagnetic and cubic. Cooling below the Curie temperature produces a rhombohedral distortion wherein the rhombohedral angle changes from 60° (cubic phase) to 60.53°. An alternate description of this distortion is to consider the length \"c\" along the unique trigonal axis (after the distortion has begun) and \"a\" as the distance in the plane perpendicular to \"c\". In the cubic phase this reduces to . Below the Curie temperature\n\nwhich is the largest strain in any actinide compound. NpNi undergoes a similar lattice distortion below , with a strain of (43 ± 5) × 10. NpCo is a ferrimagnet below 15 K.\n\nIn 2009, a team of MIT physicists demonstrated that a lithium gas cooled to less than one kelvin can exhibit ferromagnetism. The team cooled fermionic lithium-6 to less than (150 billionths of one kelvin) using infrared laser cooling. This demonstration is the first time that ferromagnetism has been demonstrated in a gas.\n\nIn 2018, a team of University of Minnesota physicists demonstrated that body-centered tetragonal ruthenium exhibits ferromagnetism at room temperature.\n\nThe Bohr–van Leeuwen theorem, discovered in the 1910s, showed that classical physics theories are unable to account for any form of magnetism, including ferromagnetism. Magnetism is now regarded as a purely quantum mechanical effect. Ferromagnetism arises due to two effects from quantum mechanics: spin and the Pauli exclusion principle.\n\nOne of the fundamental properties of an electron (besides that it carries charge) is that it has a magnetic dipole moment, i.e., it behaves like a tiny magnet, producing a magnetic field. This dipole moment comes from the more fundamental property of the electron that it has quantum mechanical spin. Due to its quantum nature, the spin of the electron can be in one of only two states; with the magnetic field either pointing \"up\" or \"down\" (for any choice of up and down). The spin of the electrons in atoms is the main source of ferromagnetism, although there is also a contribution from the orbital angular momentum of the electron about the nucleus. When these magnetic dipoles in a piece of matter are aligned, (point in the same direction) their individually tiny magnetic fields add together to create a much larger macroscopic field.\n\nHowever, materials made of atoms with filled electron shells have a total dipole moment of zero, because the electrons all exist in pairs with opposite spin, every electron's magnetic moment is cancelled by the opposite moment of the second electron in the pair. Only atoms with partially filled shells (i.e., unpaired spins) can have a net magnetic moment, so ferromagnetism only occurs in materials with partially filled shells. Because of Hund's rules, the first few electrons in a shell tend to have the same spin, thereby increasing the total dipole moment.\n\nThese unpaired dipoles (often called simply \"spins\" even though they also generally include orbital angular momentum) tend to align in parallel to an external magnetic field, an effect called paramagnetism. Ferromagnetism involves an additional phenomenon, however: in a few substances the dipoles tend to align spontaneously, giving rise to a spontaneous magnetization, even when there is no applied field.\n\nWhen two nearby atoms have unpaired electrons, whether the electron spins are parallel or antiparallel affects whether the electrons can share the same orbit as a result of the quantum mechanical effect called the exchange interaction. This in turn affects the electron location and the Coulomb (electrostatic) interaction and thus the energy difference between these states.\n\nThe exchange interaction is related to the Pauli exclusion principle, which says that two electrons with the same spin cannot also be in the same spatial state (orbital). This is a consequence of the spin-statistics theorem and that electrons are fermions. Therefore, under certain conditions, when the orbitals of the unpaired outer valence electrons from adjacent atoms overlap, the distributions of their electric charge in space are farther apart when the electrons have parallel spins than when they have opposite spins. This reduces the electrostatic energy of the electrons when their spins are parallel compared to their energy when the spins are anti-parallel, so the parallel-spin state is more stable. In simple terms, the electrons, which repel one another, can move \"further apart\" by aligning their spins, so the spins of these electrons tend to line up. This difference in energy is called the exchange energy.\n\nThis energy difference can be orders of magnitude larger than the energy differences associated with the magnetic dipole-dipole interaction due to dipole orientation, which tends to align the dipoles antiparallel. In certain doped semiconductor oxides RKKY interactions have been shown to bring about periodic longer-range magnetic interactions, a phenomenon of significance in the study of spintronic materials.\n\nThe materials in which the exchange interaction is much stronger than the competing dipole-dipole interaction are frequently called \"magnetic materials\". For instance, in iron (Fe) the exchange force is about 1000 times stronger than the dipole interaction. Therefore, below the Curie temperature virtually all of the dipoles in a ferromagnetic material will be aligned. In addition to ferromagnetism, the exchange interaction is also responsible for the other types of spontaneous ordering of atomic magnetic moments occurring in magnetic solids, antiferromagnetism and ferrimagnetism.\nThere are different exchange interaction mechanisms which create the magnetism in different ferromagnetic, ferrimagnetic, and antiferromagnetic substances. These mechanisms include direct exchange, RKKY exchange, double exchange, and superexchange.\n\nAlthough the exchange interaction keeps spins aligned, it does not align them in a particular direction. Without magnetic anisotropy, the spins in a magnet randomly change direction in response to thermal fluctuations and the magnet is superparamagnetic. There are several kinds of magnetic anisotropy, the most common of which is magnetocrystalline anisotropy. This is a dependence of the energy on the direction of magnetization relative to the crystallographic lattice. Another common source of anisotropy, inverse magnetostriction, is induced by internal strains. Single-domain magnets also can have a \"shape anisotropy\" due to the magnetostatic effects of the particle shape. As the temperature of a magnet increases, the anisotropy tends to decrease, and there is often a blocking temperature at which a transition to superparamagnetism occurs.\n\nThe above would seem to suggest that every piece of ferromagnetic material should have a strong magnetic field, since all the spins are aligned, yet iron and other ferromagnets are often found in an \"unmagnetized\" state. The reason for this is that a bulk piece of ferromagnetic material is divided into tiny regions called \"magnetic domains\" (also known as \"Weiss domains\"). Within each domain, the spins are aligned, but (if the bulk material is in its lowest energy configuration; i.e. \"unmagnetized\"), the spins of separate domains point in different directions and their magnetic fields cancel out, so the object has no net large scale magnetic field.\n\nFerromagnetic materials spontaneously divide into magnetic domains because the \"exchange interaction\" is a short-range force, so over long distances of many atoms the tendency of the magnetic dipoles to reduce their energy by orienting in opposite directions wins out. If all the dipoles in a piece of ferromagnetic material are aligned parallel, it creates a large magnetic field extending into the space around it. This contains a lot of magnetostatic energy. The material can reduce this energy by splitting into many domains pointing in different directions, so the magnetic field is confined to small local fields in the material, reducing the volume of the field. The domains are separated by thin domain walls a number of molecules thick, in which the direction of magnetization of the dipoles rotates smoothly from one domain's direction to the other.\n\nThus, a piece of iron in its lowest energy state (\"unmagnetized\") generally has little or no net magnetic field. However, the magnetic domains in a material are not fixed in place; they are simply regions where the spins of the electrons have aligned spontaneously due to their magnetic fields, and thus can be altered by an external magnetic field. If a strong enough external magnetic field is applied to the material, the domain walls will move by the process of the spins of the electrons in atoms near the wall in one domain turning under the influence of the external field to face in the same direction as the electrons in the other domain, thus reorienting the domains so more of the dipoles are aligned with the external field. The domains will remain aligned when the external field is removed, creating a magnetic field of their own extending into the space around the material, thus creating a \"permanent\" magnet. The domains do not go back to their original minimum energy configuration when the field is removed because the domain walls tend to become 'pinned' or 'snagged' on defects in the crystal lattice, preserving their parallel orientation. This is shown by the Barkhausen effect: as the magnetizing field is changed, the magnetization changes in thousands of tiny discontinuous jumps as the domain walls suddenly \"snap\" past defects.\n\nThis magnetization as a function of the external field is described by a hysteresis curve. Although this state of aligned domains found in a piece of magnetized ferromagnetic material is not a minimal-energy configuration, it is metastable, and can persist for long periods, as shown by samples of magnetite from the sea floor which have maintained their magnetization for millions of years.\n\nHeating and then cooling (annealing) a magnetized material, subjecting it to vibration by hammering it, or applying a rapidly oscillating magnetic field from a degaussing coil tends to release the domain walls from their pinned state, and the domain boundaries tend to move back to a lower energy configuration with less external magnetic field, thus \"demagnetizing\" the material.\n\nCommercial magnets are made of \"hard\" ferromagnetic or ferrimagnetic materials with very large magnetic anisotropy such as alnico and ferrites, which have a very strong tendency for the magnetization to be pointed along one axis of the crystal, the \"easy axis\". During manufacture the materials are subjected to various metallurgical processes in a powerful magnetic field, which aligns the crystal grains so their \"easy\" axes of magnetization all point in the same direction. Thus the magnetization, and the resulting magnetic field, is \"built in\" to the crystal structure of the material, making it very difficult to demagnetize.\n\nAs the temperature increases, thermal motion, or entropy, competes with the ferromagnetic tendency for dipoles to align. When the temperature rises beyond a certain point, called the Curie temperature, there is a second-order phase transition and the system can no longer maintain a spontaneous magnetization, so its ability to be magnetized or attracted to a magnet disappears, although it still responds paramagnetically to an external field. Below that temperature, there is a spontaneous symmetry breaking and magnetic moments become aligned with their neighbors. The Curie temperature itself is a critical point, where the magnetic susceptibility is theoretically infinite and, although there is no net magnetization, domain-like spin correlations fluctuate at all length scales.\n\nThe study of ferromagnetic phase transitions, especially via the simplified Ising spin model, had an important impact on the development of statistical physics. There, it was first clearly shown that mean field theory approaches failed to predict the correct behavior at the critical point (which was found to fall under a \"universality class\" that includes many other systems, such as liquid-gas transitions), and had to be replaced by renormalization group theory.\n\n\n"}
{"id": "49134790", "url": "https://en.wikipedia.org/wiki?curid=49134790", "title": "Frenkel–Kontorova model", "text": "Frenkel–Kontorova model\n\nThe Frenkel–Kontorova model, also known as the FK model, is a fundamental model of low-dimensional nonlinear physics.\n\nThe generalized FK model describes a chain of classical particles with nearest neighbor interactions and subjected to a periodic on-site substrate potential. In its original and simplest form the interactions are taken to be harmonic and the potential to be sinusoidal with a periodicity commensurate with the equilibrium distance of the particles. Different choices for the interaction and substrate potentials and inclusion of a driving force may describe a wide range of different physical situations. \n\nOriginally introduced by Yakov Frenkel and Tatiana Kontorova in 1938 to describe the structure and dynamics of a crystal lattice near a dislocation core the FK model has become one of the standard models in condensed matter physics due to its applicability to describe many physical phenomena.\nPhysical phenomena which can be modeled by FK model include dislocations, the dynamics of adsorbate layers on surfaces, crowdions, domain walls in magnetically ordered structures, long Josephson junctions, hydrogen-bonded chains, and DNA type chains. A modification of the FK model, the Tomlinson model, plays an important role in the field of tribology.\n\nThe equations for stationary configurations of the FK model reduce to those of the standard map or Chirikov–Taylor map of stochastic theory.\n\nIn the continuum-limit approximation the FK model reduces to the exactly integrable sine-Gordon equation or SG equation which allows for soliton solutions. For this reason the FK model is also known as the 'discrete sine-Gordon' or 'periodic Klein-Gordon' equation.\n\nA simple model of a harmonic chain in a periodic substrate potential was proposed by Ulrich Dehlinger in 1928. Dehlinger derived an approximate analytical expression for the stable solutions of this model which he termed \"Verhakungen\" which correspond to what is today called \"kink pairs\". An essentially similar model was developed by Ludwig Prandtl in 1912/13 but did not see publication until 1928.\n\nThe model was independently proposed by Yakov Frenkel and Tatiana Kontorova in their 1938 paper \"On the theory of plastic deformation and twinning\" to describe the dynamics of a crystal lattice near a dislocation and to describe crystal twinning. In the standard linear harmonic chain any displacement of the atoms will result in waves and the only stable configuration will be the trivial one.\nFor the nonlinear chain of Frenkel and Kontorova there exist stable configurations beside the trivial one. For small atomic displacements the situation resembles the linear chain, however for large enough displacements it is possible to create a moving single dislocation for which an analytical solution was derived by Frenkel and Kontorova. The shape of these dislocations is defined only by the parameters of the system such as the mass and the elastic constant of the springs.\n\nDislocations, also called solitons, are distributed non-local defects and mathematically they are a type of topological defect. The defining characteristic of solitons/dislocations is that they behave much like stable particles, they can move while maintaining their overall shape. Two solitons of equal and opposite orientation may cancel upon collision but a single soliton can not annihilate spontaneously.\n\nThe generalized FK model treats a one-dimensional chain of atoms with nearest neighbor interaction in periodic on-site potential, the Hamiltonian for this system is\n\nwhere the first term is the kinetic energy of the formula_1 atoms of mass formula_2 and the potential energy formula_3 is a sum of the potential energy due to the nearest neighbor interaction and that of the substrate potential formula_4\n\nThe substrate potential is periodic, i.e. formula_5 for some formula_6.\n\nFor non harmonic interactions and/or non sinusoidal potential the FK model will give rise to a commensurate-incommensurate phase transition.\n\nThe FK model can be applied to any system that can be treated as two coupled sub-systems where one subsystem can be approximated as a linear chain and the second subsystem as a motionless substrate potential.\n\nAn example would be the adsorption of a layer onto a crystal surface, here the adsorption layer can be approximated as the chain and the crystal surface as a on-site potential.\n\nIn this section we examine in detail the simplest form of the FK model. A detailed version of this derivation can be found in the following paper. The model, shown schematically in figure 1, describes a one-dimensional chain of atoms with a harmonic nearest neighbor interaction and subject to a sinusoidal potential. Transverse motion of the atoms is ignored, i.e. the atoms can only move along the chain. \nThe Hamiltonian for this situation is given by formula_7 where we specify the interaction potential to be\n\nformula_8\n\nwhere formula_9 is the elastic constant and formula_10 is the inter-atomic equilibrium distance. The substrate potential is\n\nformula_11\n\nwith formula_12 the amplitude and formula_6 the period.\n\nThe following dimensionless variables are introduced in order to rewrite the Hamiltonian:\n\nformula_14\n\nIn dimensionless form the Hamiltonian is\n\nformula_15\n\nwhich describes a harmonic chain of atoms of unit mass in a sinusoidal potential of period formula_16 with amplitude formula_17. The equation of motion for this Hamiltonian is\n\nformula_18\n\nWe consider only the case where formula_10 and formula_6 are commensurate, for simplicity we take formula_21. Thus in the ground state of the chain each minimum of the substrate potential is occupied by one atom.\nWe introduce the variable formula_22 for atomic displacements which is defined by\n\nformula_23\n\nFor small displacements formula_24 the equation of motion may be linearized and takes the following form\n\nformula_25\n\nThis equation of motion describes phonons with formula_26 with the phonon dispersion relation formula_27 with formula_28 the dimensionless wavenumber. This shows that the frequency spectrum of the chain has a band gap formula_29 with cut-off frequency formula_30.\n\nThe linearised equation of motion are not valid when the atomic displacements are not small and one must use the nonlinear equation of motion.\nThe nonlinear equations can support new types of localized excitations which are best illuminated by considering the continuum limit approximation of the FK model. Applying the standard procedure of Rosenau to derive continuum limit equations from a discrete lattice results in the perturbed sine-Gordon equation\nformula_31\nhere the function formula_32 describes in first order the effects due to the discreteness of the chain.\n\nformula_33\n\nNeglecting the discreteness effects and introducing formula_34 reduces the equation of motion to the sine-Gordon (SG) equation in its standard form.\n\nformula_35\n\nThe SG equation gives rise to three elementary excitations/solutions: kinks, breathers and phonons. Kinks, or topological solitons, can be understood as the solution connecting two nearest identical minima of the periodic substrate potential, thus they are a result of the degeneracy of the ground state.\n\nformula_36\n\nwhere formula_37 is the topological charge, for formula_38 the solution is called a kink and for formula_39 it is an antikink. The kink width formula_40 is determined by the kink velocity formula_41 where formula_41 is measured in units of the sound velocity formula_43 and is formula_44. For kink motion with formula_45 the width approximates 1.\nThe energy of the kink in dimensionless units is\n\nformula_46\n\nfrom which the rest mass of the kink follows as formula_47 and the kinks rest energy as formula_48.\n\nTwo neighboring static kinks with distance formula_49 will have energy of repulsion\nformula_50\n\nwhereas kink and antikink will attract with interaction\nformula_51\n\nA breather is\n\nformula_52\n\nwhich describes nonlinear oscillation with frequency formula_53 and formula_54\n\nformula_55\n\nfor low frequencies formula_56 the breather can be seen as a coupled kink-antikink pair. Kinks and breathers can move along the chain without any dissipative energy loss. Furthermore, any collision between all the excitations of the SG equation will result in only a phase shift. Thus kinks and breathers may be considered \"nonlinear quasi-particles\" of the SG model. For nearly integrable modifications of the SG equation such as the continuum-approximation of the FK model kinks can be considered \"deformable\" quasi-particles, provided that discreetness effects are small.\n\nIn the preceding section the excitations of the FK model were derived by considering the model in a continuum-limit approximation. Since the properties of kinks are only modified slightly by the discreteness of the primary model, the SG equation can adequately describe most features and dynamics of the system.\n\nThe discrete lattice does, however, influence the kink motion in a unique way with the existence of the Peierls–Nabarro (PN) potential formula_57. Here, formula_58 is the position of the kink's center. The existence of the PN potential is due to the lack of translational invariance in a discrete chain. In the continuum limit the system is invariant for any translation of the kink along the chain. For a discrete chain only those translations that are an integer multiple of the lattice spacing formula_6 leave the system invariant. The PN barrier, formula_60, is the smallest energy barrier for a kink to overcome so that it can move through the lattice. The value of the PN barrier is the difference between the kink's potential energy for a stable and unstable stationary configuration. The stationary configurations are shown schematically in figure 2.\n"}
{"id": "57531940", "url": "https://en.wikipedia.org/wiki?curid=57531940", "title": "Freshwater phytoplankton", "text": "Freshwater phytoplankton\n\nFreshwater phytoplankton is the phytoplankton occurring in freshwater ecosystems. It can be distinghuished between limnoplankton (lake phytoplankton), heleoplankton (phytoplankton in ponds), and potamoplankton (river phytoplankton).\n"}
{"id": "8720264", "url": "https://en.wikipedia.org/wiki?curid=8720264", "title": "History of the battery", "text": "History of the battery\n\nBatteries provided the main source of electricity before the development of electric generators and electrical grids around the end of the 19th century. Successive improvements in battery technology facilitated major electrical advances, from early scientific studies to the rise of telegraphs and telephones, eventually leading to portable computers, mobile phones, electric cars, and many other electrical devices.\n\nScientists and engineers developed several commercially important types of battery. \"Wet cells\" were open containers that held liquid electrolyte and metallic electrodes. When the electrodes were completely consumed, the wet cell was renewed by replacing the electrodes and electrolyte. Open containers are unsuitable for mobile or portable use. Wet cells were used commercially in the telegraph and telephone systems. Early electric cars used semi-sealed wet cells.\n\nOne important classification for batteries is by their life cycle. \"Primary\" batteries can produce current as soon as assembled, but once the active elements are consumed, they cannot be electrically recharged. The development of the lead-acid battery and subsequent \"secondary\" or \"rechargeable\" types allowed energy to be restored to the cell, extending the life of permanently assembled cells. The introduction of nickel and lithium based batteries in the latter 20th century made the development of innumerable portable electronic devices feasible, from powerful flashlights to mobile phones. Very large stationary batteries find some applications in grid energy storage, helping to stabilize electric power distribution networks.\n\nIn 1749, Benjamin Franklin, the U.S. polymath and founding father, first used the term \"battery\" to describe a set of linked capacitors he used for his experiments with electricity. These capacitors were panels of glass coated with metal on each surface. These capacitors were charged with a static generator and discharged by touching metal to their electrode. Linking them together in a \"battery\" gave a stronger discharge. Originally having the generic meaning of \"a group of two or more similar objects functioning together\", as in an artillery battery, the term came to be used for voltaic piles and similar devices in which many electrochemical cells were connected together in the manner of Franklin's capacitors. Today even a single electrochemical cell, e.g. a dry cell, is commonly called a battery.\n\nThere is a disputed claim for a very early battery, the Baghdad Battery. If it was real, it was soon forgotten.\nIn 1780, Luigi Galvani was dissecting a frog affixed to a brass hook. When he touched its leg with his iron scalpel, the leg twitched. Galvani believed the energy that drove this contraction came from the leg itself, and called it \"animal electricity\".\n\nHowever, Alessandro Volta, a friend and fellow scientist, disagreed, believing this phenomenon was caused by two different metals joined together by a moist intermediary. He verified this hypothesis through experiment, and published the results in 1791. In 1800, Volta invented the first true battery, which came to be known as the voltaic pile. The voltaic pile consisted of pairs of copper and zinc discs piled on top of each other, separated by a layer of cloth or cardboard soaked in brine (i.e., the electrolyte). Unlike the Leyden jar, the voltaic pile produced a continuous electricity and stable current, and lost little charge over time when not in use, though his early models could not produce a voltage strong enough to produce sparks. He experimented with various metals and found that zinc and silver gave the best results.\n\nVolta believed the current was the result of two different materials simply touching each other—an obsolete scientific theory known as contact tension—and not the result of chemical reactions. As a consequence, he regarded the corrosion of the zinc plates as an unrelated flaw that could perhaps be fixed by changing the materials somehow. However, no scientist ever succeeded in preventing this corrosion. In fact, it was observed that the corrosion was faster when a higher current was drawn. This suggested that the corrosion was actually integral to the battery's ability to produce a current. This, in part, led to the rejection of Volta's contact tension theory in favor of electrochemical theory. Volta's illustrations of his Crown of Cups and voltaic pile have extra metal disks, now known to be unnecessary, on both the top and bottom. The figure associated with this section, of the zinc-copper voltaic pile, has the modern design, an indication that \"contact tension\" is not the source of electromotive force for the voltaic pile.\n\nVolta's original pile models had some technical flaws, one of them involving the electrolyte leaking and causing short-circuits due to the weight of the discs compressing the brine-soaked cloth. A Scotsman named William Cruickshank solved this problem by laying the elements in a box instead of piling them in a stack. This was known as the trough battery. Volta himself invented a variant that consisted of a chain of cups filled with a salt solution, linked together by metallic arcs dipped into the liquid. This was known as the Crown of Cups. These arcs were made of two different metals (e.g., zinc and copper) soldered together. This model also proved to be more efficient than his original piles, though it did not prove as popular.\n\nAnother problem with Volta's batteries was short battery life (an hour's worth at best), which was caused by two phenomena. The first was that the current produced electrolysed the electrolyte solution, resulting in a film of hydrogen bubbles forming on the copper, which steadily increased the internal resistance of the battery (this effect, called \"polarization\", is counteracted in modern cells by additional measures). The other was a phenomenon called \"local action\", wherein minute short-circuits would form around impurities in the zinc, causing the zinc to degrade. The latter problem was solved in 1835 by William Sturgeon, who found that amalgamated zinc, whose surface had been treated with some mercury, didn't suffer from local action.\n\nDespite its flaws, Volta's batteries provided a steadier current than Leyden jars, and made possible many new experiments and discoveries, such as the first electrolysis of water by Anthony Carlisle and William Nicholson.\n\nA British chemist named John Frederic Daniell found a way to solve the hydrogen bubble problem in the Voltaic Pile by using a second electrolyte to consume the hydrogen produced by the first. In 1836 he invented the Daniell cell, which consisted of a copper pot filled with a copper sulfate solution, in which was immersed an unglazed earthenware container filled with sulfuric acid and a zinc electrode. The earthenware barrier was porous, which allowed ions to pass through but kept the solutions from mixing.\n\nThe Daniell cell was a great improvement over the existing technology used in the early days of battery development and was the first practical source of electricity. It provided a longer and more reliable current than the Voltaic cell. It was also safer and less corrosive. It had an operating voltage of roughly 1.1 volts. It soon became the industry standard for use, especially with the new telegraph networks.\n\nThe Daniell cell was also used as a working standard for definition of the volt, which is the unit of electromotive force. \n\nA version of the Daniell cell was invented in 1837 by the Guy's hospital physician Golding Bird who used a plaster of Paris barrier to keep the solutions separate. Bird's experiments with this cell were of some importance to the new discipline of electrometallurgy.\n\nThe porous pot version of the Daniell cell was invented by John Dancer, a Liverpool instrument maker, in 1838. It consists of a central zinc anode dipped into a porous earthenware pot containing a zinc sulfate solution. The porous pot is, in turn, immersed in a solution of copper sulfate contained in a copper can, which acts as the cell's cathode. The use of a porous barrier allows ions to pass through but keeps the solutions from mixing.\n\nIn the 1860s, a Frenchman named Callaud invented a variant of the Daniell cell called the gravity cell. This simpler version dispensed with the porous barrier. This reduced the internal resistance of the system and, thus, the battery yielded a stronger current. It quickly became the battery of choice for the American and British telegraph networks, and was used until the 1950s. \n\nThe gravity cell consisted of a glass jar, in which a copper cathode sat on the bottom and a zinc anode was suspended beneath the rim. Copper sulfate crystals would be scattered around the cathode and then the jar would be filled with distilled water. As the current was drawn, a layer of zinc sulfate solution would form at the top around the anode. This top layer was kept separate from the bottom copper sulfate layer by its lower density and by the polarity of the cell.\n\nThe zinc sulfate layer was clear in contrast to the deep blue copper sulfate layer, which allowed a technician to measure the battery life with a glance. On the other hand, this setup meant the battery could be used only in a stationary appliance, else the solutions would mix or spill. Another disadvantage was that a current had to be continually drawn to keep the two solutions from mixing by diffusion, so it was unsuitable for intermittent use.\n\nThe German scientist Johann Christian Poggendorff overcame the problems with separating the electrolyte and the depolariser using a porous earthenware pot in 1842. In the Poggendorff cell, sometimes called Grenet Cell due to the works of Eugene Grenet around 1859, the electrolyte was dilute sulphuric acid and the depolariser was chromic acid. The two acids were physically mixed together eliminating the porous pot. The positive electrode (cathode) was two carbon plates, with a zinc plate (negative or anode) positioned between them. Because of the tendency of the acid mixture to react with the zinc, a mechanism was provided to raise the zinc electrode clear of the acids.\n\nThe cell provided 1.9 volts. It proved popular with experimenters for many years due to its relatively high voltage; greater ability to produce a consistent current and lack of any fumes, but the relative fragility of its thin glass enclosure and the necessity of having to raise the zinc plate when the cell was not in use eventually saw it fall out of favour. The cell was also known as the 'chromic acid cell', but principally as the 'bichromate cell'. This latter name came from the practice of producing the chromic acid by adding sulphuric acid to potassium dichromate, even though the cell itself contained no dichromate.\n\nThe Fuller cell was developed from the Poggendorff cell. Although the chemistry was principally the same, the two acids were once again separated by a porous container and the zinc was treated with mercury to form an amalgam.\n\nThe Grove cell was invented by Welshman William Robert Grove in 1839. It consisted of a zinc anode dipped in sulfuric acid and a platinum cathode dipped in nitric acid, separated by porous earthenware. The Grove cell provided a high current and nearly twice the voltage of the Daniell cell, which made it the favoured cell of the American telegraph networks for a time. However, it gave off poisonous nitric oxide fumes when operated. The voltage also dropped sharply as the charge diminished, which became a liability as telegraph networks grew more complex. Platinum was also very expensive.\n\nAlfred Dun 1885, nitro-muriatic acid (\"aqua regis\") - Iron & Carbon\nIn the new element there can be used advantageously as exciting-liquid in the first case such solutions as have in a concentrated condition great depolarizing-power, which effect the whole depolarization chemically without necessitating the mechanical expedient of increased carbon surface. It is preferred to use iron as the positive electrode, and as exciting-liquid nitro muriatic acid, (\"aqua regis\",) the mixture consisting of muriatic and nitric acids. The nitro-muriatic acid, as explained above, serves for filling both cells. For the carbon-cells it is used strong or very slightly diluted, but for the other cells very diluted, (about one-twentieth, or at the most one-tenth.) The element containing in one cell carbon and concentrated nitro-muriatic acid and in the other cell iron and dilute nitro-muriatic acid remains constant for at least twenty hours when employed for electric incandescent lighting. (p. 80 at Google Books)\n\nUp to this point, all existing batteries would be permanently drained when all their chemical reactions were spent. In 1859, Gaston Planté invented the lead–acid battery, the first-ever battery that could be recharged by passing a reverse current through it. A lead acid cell consists of a lead anode and a lead dioxide cathode immersed in sulfuric acid. Both electrodes react with the acid to produce lead sulfate, but the reaction at the lead anode releases electrons whilst the reaction at the lead dioxide consumes them, thus producing a current. These chemical reactions can be reversed by passing a reverse current through the battery, thereby recharging it.\n\nPlanté's first model consisted of two lead sheets separated by rubber strips and rolled into a spiral. His batteries were first used to power the lights in train carriages while stopped at a station. In 1881, Camille Alphonse Faure invented an improved version that consisted of a lead grid lattice into which a lead oxide paste was pressed, forming a plate. Multiple plates could be stacked for greater performance. This design was easier to mass-produce.\n\nCompared to other batteries, Planté's was rather heavy and bulky for the amount of energy it could hold. However, it could produce remarkably large currents in surges. It also had very low internal resistance, meaning that a single battery could be used to power multiple circuits.\n\nThe lead-acid battery is still used today in automobiles and other applications where weight is not a big factor. The basic principle has not changed since 1859. In the early 1930s, a gel electrolyte (instead of a liquid) produced by adding silica to a charged cell was used in the LT battery of portable vacuum-tube radios. In the 1970s, \"sealed\" versions became common (commonly known as a \"gel cell\" or \"SLA\"), allowing the battery to be used in different positions without failure or leakage.\n\nToday cells are classified as \"primary\" if they produce a current only until their chemical reactants are exhausted, and \"secondary\" if the chemical reactions can be reversed by recharging the cell. The lead-acid cell was the first \"secondary\" cell.\n\nIn 1866, Georges Leclanché invented a battery that consisted of a zinc anode and a manganese dioxide cathode wrapped in a porous material, dipped in a jar of ammonium chloride solution. The manganese dioxide cathode had a little carbon mixed into it as well, which improved conductivity and absorption. It provided a voltage of 1.4 volts. This cell achieved very quick success in telegraphy, signalling and electric bell work.\n\nThe dry cell form was used to power early telephones—usually from an adjacent wooden box affixed to the wall—before telephones could draw power from the telephone line itself. The Leclanché cell could not provide a sustained current for very long. In lengthy conversations, the battery would run down, rendering the conversation inaudible. This was because certain chemical reactions in the cell increased the internal resistance and, thus, lowered the voltage. These reactions reversed themselves when the battery was left idle, so it was good only for intermittent use.\n\nMany experimenters tried to immobilize the electrolyte of an electrochemical cell to make it more convenient to use. The Zamboni pile of 1812 was a high-voltage dry battery but capable of delivering only minute currents. Various experiments were made with cellulose, sawdust, spun glass, asbestos fibers, and gelatine.\n\nIn 1886, Carl Gassner obtained a German patent on a variant of the Leclanché cell, which came to be known as the dry cell because it did not have a free liquid electrolyte. Instead, the ammonium chloride was mixed with plaster of Paris to create a paste, with a small amount of zinc chloride added in to extend the shelf life. The manganese dioxide cathode was dipped in this paste, and both were sealed in a zinc shell, which also acted as the anode. In November 1887, he obtained for the same device.\n\nUnlike previous wet cells, Gassner's dry cell was more solid, did not require maintenance, did not spill, and could be used in any orientation. It provided a potential of 1.5 volts. The first mass-produced model was the Columbia dry cell, first marketed by the National Carbon Company in 1896. The NCC improved Gassner's model by replacing the plaster of Paris with coiled cardboard, an innovation that left more space for the cathode and made the battery easier to assemble. It was the first convenient battery for the masses and made portable electrical devices practical, and led directly to the invention of the flashlight.\n\nThe zinc–carbon battery (as it came to be known) is still manufactured today.\n\nIn parallel, in 1887 Wilhelm Hellesen developed his own dry cell design. It has been claimed that Hellesen's design preceded that of Gassner.\n\nIn 1887, a dry-battery was developed by Yai Sakizō () of Japan, then patented in 1892. In 1893, Yai Sakizō's dry-battery was exhibited in World's Columbian Exposition and commanded considerable international attention.\n\nIn 1899, a Swedish scientist named Waldemar Jungner invented the nickel–cadmium battery, a rechargeable battery that had nickel and cadmium electrodes in a potassium hydroxide solution; the first battery to use an alkaline electrolyte. It was commercialized in Sweden in 1910 and reached the United States in 1946. The first models were robust and had significantly better energy density than lead-acid batteries, but were much more expensive.\n\nJungner had invented a nickel–iron battery in 1899, the same year as his Ni-Cad battery, but found it to be inferior to its cadmium counterpart and, as a consequence, never bothered patenting it. It produced a lot more hydrogen gas when being charged, meaning it could not be sealed, and the charging process was less efficient (it was, however, cheaper). Thomas Edison picked up Jungner's nickel-iron battery design, patented it himself and sold it in 1903. Edison wanted to commercialise a more lightweight and durable substitute for the lead-acid battery that powered some early automobiles, and hoped that by doing so electric cars would become the standard, with his firm as its main battery vendor. However, customers found his first model to be prone to leakage and short battery life, and it did not outperform the lead-acid cell by much either. Although Edison was able to produce a more reliable and powerful model seven years later, by this time the inexpensive and reliable Model T Ford had made gasoline engine cars the standard. Nevertheless, Edison's battery achieved great success in other applications such as electric and diesel-electric rail vehicles, providing backup power for railroad crossing signals, or to provide power for the lamps used in mines.\n\nUntil the late 1950s the zinc–carbon battery continued to be a popular primary cell battery, but its relatively low battery life hampered sales. In 1955, an engineer named Lewis Urry, working for Union Carbide at the National Carbon Company Parma Research Laboratory, was tasked with finding a way to extend the life of zinc-carbon batteries, but Urry decided instead that alkaline batteries held more promise. Until then, longer-lasting alkaline batteries were unfeasibly expensive. Urry's battery consisted of a manganese dioxide cathode and a powdered zinc anode with an alkaline electrolyte. Using powdered zinc gave the anode a greater surface area. These batteries hit the market in 1959.\n\nThe nickel–hydrogen battery entered the market as an energy-storage subsystem for commercial communication satellites.\n\nThe first consumer grade nickel–metal hydride batteries (NiMH) for smaller applications appeared on the market in 1989 as a variation of the 1970s nickel–hydrogen battery. NiMH batteries tend to have longer lifespans than NiCd batteries (and their lifespans continue to increase as manufacturers experiment with new alloys) and, since cadmium is toxic, NiMH batteries are less damaging to the environment.\n\nLithium is the metal with lowest density and with the greatest electrochemical potential and energy-to-weight ratio. The low atomic weight and small size of its ions also speeds its diffusion, suggesting that it would make an ideal material for batteries. Experimentation with lithium batteries began in 1912 under G.N. Lewis, but commercial lithium batteries did not come to market until the 1970s. Three volt lithium primary cells such as the CR123A type and three volt button cells are still widely used, especially in cameras and very small devices. \n\nThree important developments marked the 1980s. In 1980 an American chemist, John B. Goodenough, discovered the LiCoO cathode (positive lead) and a Moroccan research scientist, Rachid Yazami, discovered the graphite anode (negative lead) with the solid electrolyte. In 1981, Japanese chemists Tokio Yamabe and Shizukuni Yata discovered a novel nano-carbonacious-PAS (polyacene,) and found that it was very effective for the anode in the conventional liquid electrolyte. This led a research team managed by Akira Yoshino of Asahi Chemical, Japan, to build the first lithium-ion battery prototype in 1985, a rechargeable and more stable version of the lithium battery; Sony commercialized the lithium-ion battery in 1991. \n\nIn 1997, the lithium polymer battery was released by Sony and Asahi Kasei. These batteries hold their electrolyte in a solid polymer composite instead of in a liquid solvent, and the electrodes and separators are laminated to each other. The latter difference allows the battery to be encased in a flexible wrapping instead of in a rigid metal casing, which means such batteries can be specifically shaped to fit a particular device. This advantage has favored lithium polymer batteries in the design of portable electronic devices such as mobile phones and personal digital assistants, and of radio-controlled aircraft, as such batteries allow for more flexible and compact design. They generally have a lower energy density than normal lithium-ion batteries.\n"}
{"id": "19661218", "url": "https://en.wikipedia.org/wiki?curid=19661218", "title": "John Cain Carter", "text": "John Cain Carter\n\nJohn Cain Carter (born 1966 in San Antonio, Texas) is a cattle rancher and conservationist who started the Brazilian rainforest conservation organization, Aliança da Terra.\nCarter moved to Brazil from Texas in 1996, where he and his wife managed a 8200 hectare cattle ranch between the Xingu and Amazon Rivers in the Brazilian state of Mato Grosso.\nShocked by the rapid deforestation occurring in the Amazon Rainforest, Carter started Aliança da Terra to provide economic incentives for farmers and ranchers to preserve the forest land.\n\n"}
{"id": "2624309", "url": "https://en.wikipedia.org/wiki?curid=2624309", "title": "Lanthanum gallium silicate", "text": "Lanthanum gallium silicate\n\nLanthanum gallium silicate (referred to as LGS in this article), also known as langasite, has a chemical formula of the form \"ABCDO\", where \"A\", \"B\", \"C\" and \"D\" indicate particular cation sites. \"A\" is a decahedral (Thomson cube) site coordinated by 8 oxygen atoms. \"B\" is octahedral site coordinated by 6 oxygen atoms, and \"C\" and \"D\" are tetrahedral sites coordinated by 4 oxygen atoms. In this material, lanthanum occupied the \"A\"-sites, gallium the \"B\", \"C\" and half of \"D\"-sites, and, silicon the other half of \"D\"-sites. The crystal structure is shown below:\n\nLGS is a piezoelectric material, with no phase transitions up to its melting point of 1470 °C. Single crystal LGS can be grown via the Czochralski method, in which crystallization is initiated on a rotating seed crystal lowered into the melt followed by pulling from the melt. The growth atmosphere is usually argon or nitrogen with up to 5% of oxygen. The use of oxygen in the growth environment is reported to suppress gallium loss from the melt; however, too high an oxygen level can lead to platinum (crucible material used for the melt) dissolution in the melt. The growth of LGS is primarily along the z direction. Currently the 3-inch (76 mm) langasite boules produced commercially have growth rates of 1.5 to 5 mm/h. The quality of the crystals tends to improve as the growth rate is reduced.\n\n\n"}
{"id": "13730906", "url": "https://en.wikipedia.org/wiki?curid=13730906", "title": "Larval food plants of Lepidoptera", "text": "Larval food plants of Lepidoptera\n\nCaterpillars (larvae) of Lepidoptera species (i.e. of butterflies and moths) are mostly (though not exclusively) herbivores, often oligophagous, i.e. feeding on a narrow variety of plant species (mostly on their leaves, but sometimes on fruit or other parts. \n\nLepidopteran larvae often require specific species of food plants. It also makes some of them important pests in agriculture or forestry. The host plants have yet to be determined for some species. There is not always consensus among lepidopterists over the listing of suitable plants.\n\nAdult females normally lay their eggs on or near specific food plants (which often have to be abundant enough). Lepidopteran larvae can often be raised on a variety food plants and commercial mixtures. Closely related Lepidoptera tend to have similar food plant preferences. Many caterpillars sequester the toxins from their food plants and use them as a defense against predators. Though it is common for Lepidoptera to prefer a certain plant genus or family, some species feed on a narrow selection of unrelated taxa. The choice is unrelated to nectar plant preferences of adult Lepidoptera, which are much less strict.\n\n\n"}
{"id": "1598497", "url": "https://en.wikipedia.org/wiki?curid=1598497", "title": "Lauric acid", "text": "Lauric acid\n\nLauric acid or systematically, dodecanoic acid, is a saturated fatty acid with a 12-carbon atom chain, thus having many properties of medium-chain fatty acids, is a bright white, powdery solid with a faint odor of bay oil or soap. The salts and esters of lauric acid are known as laurates.\n\nLauric acid, as a component of triglycerides, comprises about half of the fatty-acid content in coconut milk, coconut oil, laurel oil, and palm kernel oil (not to be confused with palm oil), Otherwise, it is relatively uncommon. It is also found in human breast milk (6.2% of total fat), cow's milk (2.9%), and goat's milk (3.1%).\n\n\nAlthough 95% of medium-chain triglycerides are absorbed through the portal vein, only 25-30% of lauric acid is absorbed through it.\n\nLike many other fatty acids, lauric acid is inexpensive, has a long shelf-life, is nontoxic, and is safe to handle. It is used mainly for the production of soaps and cosmetics. For these purposes, lauric acid is reacted with sodium hydroxide to give sodium laurate, which is a soap. Most commonly, sodium laurate is obtained by saponification of various oils, such as coconut oil. These precursors give mixtures of sodium laurate and other soaps.\n\nIn the laboratory, lauric acid may be used to investigate the molar mass of an unknown substance via the freezing-point depression. The choice of lauric acid is convenient because the melting point of the pure compound is relatively high (43.8°C). Its cryoscopic constant is 3.9°C·kg/mol. By melting lauric acid with the unknown substance, allowing it to cool, and recording the temperature at which the mixture freezes, the molar mass of the unknown compound may be determined.\n\n\"In vitro\" experiments have suggested that some fatty acids including lauric acid could be a useful component in a treatment for acne, but no clinical trials have yet been conducted to evaluate this potential benefit in humans.\n\nLauric acid increases total serum cholesterol more than many other fatty acids, but most of the increase is attributable to an increase in high-density lipoprotein (HDL) (the \"good\" blood cholesterol). As a result, lauric acid has been characterized as having \"a more favorable effect on total HDL cholesterol than any other fatty acid [examined], either saturated or unsaturated\". In general, a lower total/HDL serum cholesterol ratio correlates with a decrease in atherosclerotic risk. Nonetheless, an extensive meta-analysis on foods affecting the total LDL /serum cholesterol ratio found in 2003 that the net effects of lauric acid on coronary artery disease outcomes remained uncertain. A 2016 review of coconut oil (which is nearly half lauric acid) was similarly inconclusive about the effects on cardiovascular disease risk.\n\n\n"}
{"id": "11809149", "url": "https://en.wikipedia.org/wiki?curid=11809149", "title": "List of Lessepsian migrant species", "text": "List of Lessepsian migrant species\n\nLessepsian migrants, named after Ferdinand de Lesseps, the French engineer in charge of the Suez Canal's construction, are marine species that are native to the waters on one side of the Suez Canal, and which have been introduced by passage through the canal to the waters on its other side, giving rise to new colonies there and often becoming invasive.\n\nMost Lessepsian migrations are of Red Sea species invading the Mediterranean Sea; few occur in the opposite direction.\n\nThe year given denotes first record in the Mediterranean.\n\n"}
{"id": "49282028", "url": "https://en.wikipedia.org/wiki?curid=49282028", "title": "List of data breaches", "text": "List of data breaches\n\nThis is a list of data breaches, using data compiled from various sources, including press reports, government news releases and mainstream news articles. The list includes those involving the theft or compromise of 30,000 or more records, although many more smaller breaches occur continually. Breaches of large organizations where the number of records is still unknown are also listed. The various methods used in the breaches are also listed, with hacking being the most common.\n\nMost breaches occur in North America. It is estimated that the average cost of a data breach will be over $150 million by 2020, with the global annual cost forecast to be $2.1 trillion. It is estimated that in first half of 2018 alone, about 4.5 billion records were exposed as a result of data breaches.\n"}
{"id": "15652989", "url": "https://en.wikipedia.org/wiki?curid=15652989", "title": "Maralinga: Australia's Nuclear Waste Cover-up", "text": "Maralinga: Australia's Nuclear Waste Cover-up\n\nMaralinga: Australia’s Nuclear Waste Cover-up is a book by Alan Parkinson about the clean-up of the British atomic bomb test site at Maralinga in South Australia, published in 2007. Parkinson, a nuclear engineer, explains that the clean-up of Maralinga in the late 1990s was compromised by cost-cutting and simply involved dumping hazardous radioactive debris in shallow holes in the ground. Parkinson states that \"What was done at Maralinga was a cheap and nasty solution that wouldn't be adopted on white-fellas land.\"\n\n\n"}
{"id": "17553405", "url": "https://en.wikipedia.org/wiki?curid=17553405", "title": "Material failure theory", "text": "Material failure theory\n\nFailure theory is the science of predicting the conditions under which solid materials fail under the action of external loads. The failure of a material is usually classified into brittle failure (fracture) or ductile failure (yield). Depending on the conditions (such as temperature, state of stress, loading rate) most materials can fail in a brittle or ductile manner or both. However, for most practical situations, a material may be classified as either brittle or ductile. Though failure theory has been in development for over 200 years, its level of acceptability is yet to reach that of continuum mechanics.\n\nIn mathematical terms, failure theory is expressed in the form of various failure criteria which are valid for specific materials. Failure criteria are functions in stress or strain space which separate \"failed\" states from \"unfailed\" states. A precise physical definition of a \"failed\" state is not easily quantified and several working definitions are in use in the engineering community. Quite often, phenomenological failure criteria of the same form are used to predict brittle failure and ductile yield.\n\nIn materials science, material failure is the loss of load carrying capacity of a material unit. This definition introduces to the fact that material failure can be examined in different scales, from microscopic, to macroscopic. In structural problems, where the structural response may be beyond the initiation of nonlinear material behaviour, material failure is of profound importance for the determination of the integrity of the structure. On the other hand, due to the lack of globally accepted fracture criteria, the determination of the structure's damage, due to material failure, is still under intensive research.\n\nMaterial failure can be distinguished in two broader categories depending on the scale in which the material is examined:\n\nMicroscopic material failure is defined in terms of crack propagation and initiation. Such methodologies are useful for gaining insight in the cracking of specimens and simple structures under well defined global load distributions. Microscopic failure considers the initiation and propagation of a crack. Failure criteria in this case are related to microscopic fracture. Some of the most popular failure models in this area are the micromechanical failure models, which combine the advantages of continuum mechanics and classical fracture mechanics. Such models are based on the concept that during plastic deformation, microvoids nucleate and grow until a local plastic neck or fracture of the intervoid matrix occurs, which causes the coalescence of neighbouring voids. Such a model, proposed by Gurson and extended by Tvergaard and Needleman, is known as GTN. Another approach, proposed by Rousselier, is based on continuum damage mechanics (CDM) and thermodynamics. Both models form a modification of the von Mises yield potential by introducing a scalar damage quantity, which represents the void volume fraction of cavities, the porosity \"f\".\n\nMacroscopic material failure is defined in terms of load carrying capacity or energy storage capacity, equivalently. Li presents a classification of macroscopic failure criteria in four categories:\n\nFive general levels are considered, at which the meaning of deformation and failure is interpreted differently: the structural element scale, the macroscopic scale where macroscopic stress and strain are defined, the mesoscale which is represented by a typical void, the microscale and the atomic scale. The material behavior at one level is considered as a collective of its behavior at a sub-level. An efficient deformation and failure model should be consistent at every level.\n\nFailure of brittle materials can be determined using several approaches:\n\nThe failure criteria that were developed for brittle solids were the maximum stress/strain criteria. The maximum stress criterion assumes that a material fails when the maximum principal stress formula_1 in a material element exceeds the uniaxial tensile strength of the material. Alternatively, the material will fail if the minimum principal stress formula_2 is less than the uniaxial compressive strength of the material. If the uniaxial tensile strength of the material is formula_3 and the uniaxial compressive strength is formula_4, then the safe region for the material is assumed to be\nNote that the convention that tension is positive has been used in the above expression.\n\nThe maximum strain criterion has a similar form except that the principal strains are compared with experimentally determined uniaxial strains at failure, i.e.,\nThe maximum principal stress and strain criteria continue to be widely used in spite of severe shortcomings.\n\nNumerous other phenomenological failure criteria can be found in the engineering literature. The degree of success of these criteria in predicting failure has been limited. For brittle materials, some popular failure criteria are:\n\nThe approach taken in linear elastic fracture mechanics is to estimate the amount of energy needed to grow a preexisting crack in a brittle material. The earliest fracture mechanics approach for unstable crack growth is Griffiths' theory. When applied to the mode I opening of a crack, Griffiths' theory predicts that the critical stress (formula_7) needed to propagate the crack is given by\nwhere formula_9 is the Young's modulus of the material, formula_10 is the surface energy per unit area of the crack, and formula_11 is the crack length for edge cracks or formula_12 is the crack length for plane cracks. The quantity formula_13 is postulated as a material parameter called the fracture toughness. The mode I fracture toughness for plane strain is defined as\nwhere formula_4 is a critical value of the far field stress and formula_16 is a dimensionless factor that depends on the geometry, material properties, and loading condition. The quantity formula_17 is related to the stress intensity factor and is determined experimentally. Similar quantities formula_18 and formula_19 can be determined for mode II and model III loading conditions.\n\nThe state of stress around cracks of various shapes can be expressed in terms of their stress intensity factors. Linear elastic fracture mechanics predicts that a crack will extend when the stress intensity factor at the crack tip is greater than the fracture toughness of the material. Therefore, the critical applied stress can also be determined once the stress intensity factor at a crack tip is known.\n\nThe linear elastic fracture mechanics method is difficult to apply for anisotropic materials (such as composites) or for situations where the loading or the geometry are complex. The strain energy release rate approach has proved quite useful for such situations. The strain energy release rate for a mode I crack which runs through the thickness of a plate is defined as\nwhere formula_21 is the applied load, formula_22 is the thickness of the plate, formula_23 is the displacement at the point of application of the load due to crack growth, and formula_11 is the crack length for edge cracks or formula_12 is the crack length for plane cracks. The crack is expected to propagate when the strain energy release rate exceeds a critical value formula_26 - called the critical strain energy release rate.\n\nThe fracture toughness and the critical strain energy release rate for plane stress are related by\nwhere formula_9 is the Young's modulus. If an initial crack size is known, then a critical stress can be determined using the strain energy release rate criterion.\n\nCriteria used to predict the failure of ductile materials are usually called yield criteria. Commonly used failure criteria for ductile materials are:\n\nThe yield surface of a ductile material usually changes as the material experiences increased deformation. Models for the evolution of the yield surface with increasing strain, temperature, and strain rate are used in conjunction with the above failure criteria for isotropic hardening, kinematic hardening, and viscoplasticity. Some such models are:\n\nThere is another important aspect to ductile materials - the prediction of the ultimate failure strength of a ductile material. Several models for predicting the ultimate strength have been used by the engineering community with varying levels of success. For metals, such failure criteria are usually expressed in terms of a combination of porosity and strain to failure or in terms of a damage parameter.\n\n"}
{"id": "5188196", "url": "https://en.wikipedia.org/wiki?curid=5188196", "title": "NatureKids", "text": "NatureKids\n\nNatureKids is a Costa Rican foundation that works with low-income families, offering various educational programs including English language and environmental education programs.\n"}
{"id": "14668917", "url": "https://en.wikipedia.org/wiki?curid=14668917", "title": "Oil megaprojects (2019)", "text": "Oil megaprojects (2019)\n\nThis page summarizes projects that propose to bring more than of new liquid fuel capacity to market with the first production of fuel beginning in 2019. This is part of the Wikipedia summary of Oil Megaprojects. \n"}
{"id": "21950694", "url": "https://en.wikipedia.org/wiki?curid=21950694", "title": "Parvez Butt", "text": "Parvez Butt\n\nParvez Butt (or Pervez Butt) (born 4 October 1942) is a Pakistani nuclear engineer and the former chairman of the Pakistan Atomic Energy Commission (PAEC) from 2001 to 2006. A nuclear engineer by profession, Parvez Butt played an important role in the development of the Pakistan's nuclear power and weapons program. As chairman of PAEC, Parvez Butt helped establishing the particle accelerators, nuclear power plants, and started the research program to the field of Nuclear medicines. Parvez Butt is currently serving as an assistant professor of nuclear engineering at the Pakistan Institute of Engineering and Applied Sciences, and also serving as a \"Science and Technology\" member at the Planning Commission of Pakistan.\n\nBorn in Gujranwala, Punjab, Pakistan. At age 16, Butt was admitted to University of Engineering and Technology, Lahore, with a full engineering scholarship. In 1962, Butt graduated and received his BS in Mechanical engineering from University of Engineering and Technology. After his graduation, Butt was employed in Pakistan Atomic Energy Commission (PAEC), and joined PAEC-Lahore Center, where he did his post-graduate research at Pakistan Atomic Energy Commission. Butt was awarded the PAEC scholarship in 1964 by the PAEC chairman Dr. I. H. Usmani, and fled to Canada on the advice of Dr. Ishfaq Ahmad. He was enrolled in Engineering School of the University of Toronto, where he completed and received his MS in nuclear engineering from the University of Toronto in 1965.\n\nPervez Butt joined the Pakistan Atomic Energy Commission in 1963 as \"Principal Engineer\" (PE), and following his master's degree, he was upgraded to become a \"Chief Engineer\" (CE) in 1966. Butt was part of the Pakistani delegation that led to a successful agreement with General Electric to supply country's first CANDU type-nuclear reactor, under the name of Karachi Nuclear Power Plant (KANUPP). Butt was involved in the project's design, installation, operation and maintenance since very beginning of the project. From 1965 to 1970, he worked in the design office of Canadian General Electric, Peterborough, Ontario. Canada.\n\nIn 1971, after the 1971 war, Butt came back to Pakistan and re-joined PAEC. The PAEC, under Munir Ahmed Khan, began to developed nuclear weapons by the prime minister Zulfikar Ali Bhutto, Parvez Butt was made part of the nuclear weapons design and manufacture program with the full backing of Munir Ahmad Khan. He was also a part of the PAEC team that was mandated to buy and purchase equipment and machinery from abroad. In 1974, Butt was transferred to the Pakistan Institute of Nuclear Science and Technology (PINSTECH), where he was made director-general of \"Nuclear Engineering Division\" (NED). NED, along with NPD (Nuclear Physics Division), developed the first nuclear device, codename \"HMX-3\". In 1974, Parvez Butt was part of the PAEC team that had indigenously designed built the PARR-II Reactor. While the project was led by Munir Ahmad Khan and Hafeez Qureshi, Butt took part in designing of the reactor. In 1975, Munir Ahmad Khan assigned another project to Parvez Butt. In 1975, Parvez Butt was the head of the team that was responsible for producing the first UF. Pervez Butt also played an important role in designing neutron particle accelerator at PARR-II. He had worked closely with well-known Pakistani nuclear scientist Dr. Samar Mubarakmand in establishing a neutron particle accelerator. He worked closely with his mentor and former PAEC chairman, Munir Ahmad Khan in building nuclear power plants.\n\nTo develop indigenous capability, Butt was assigned the responsibility for the establishment of large infrastructure organisation, consisting of design and development and testing facilities and large manufacturing workshops. The national industrial training centres for welding (Pakistan Welding Institute) and non-destructive testing (NCNDT), to ensure high quality production, are among other organisations established by him.\n\nHe also designed the Chashma Nuclear Power Plant. He was appointed director of the PAEC, in 1984 and chairman of PAEC in 2001. On 26 July, as new Chairman of Pakistan Atomic Energy Commission, he established five more nuclear medicines centres in the country and trained the staff of other health institutions in nuclear medicines and methods.\n\nDuring his tenure of 5 years as chairman, he successfully took actions to accelerate the growth in various areas of activities in which PAEC is involved and created many new organisations. These included research organisations in agriculture, establishing more nuclear medical centres, expansion in nuclear power generation, search and mining of uranium, industrialisation, self-reliance, human resource development etc.\n\nButt was actively involved in Pakistan's civilian nuclear program. He, along with other Pakistani nuclear scientists, visited People's Republic of China in 1986 to lead a Civilian Nuclear Technology agreement between Pakistan and People's Republic of China. He was actively involved in the technical and contractual negotiations for the Chinese nuclear power plants and as chairman PAEC signed the contract for the second unit in 2004.\n\nParvez Butt as PAEC chairman faced an important environmental issue at Baghalchur, a former uranium exploration site, which was used in 1978 to 2000. The site was closed in 2000. In an interview with BBC, Butt has stated: \"It is being done in keeping with the international standards for storing nuclear waste\". PAEC chairman was summoned by the Supreme Court of Pakistan on the nuclear waste issue in which he gave assurance to supreme court of Pakistan that the radioactive and nuclear waste material has been taken care under the international standard.\n\nPervez Butt headed PAEC at IAEA meeting where he, as a chairman of PAEC, signed a contract with IAEA. On 3 September 2004, International Atomic Energy Agency (IAEA) has mandated Pakistan to extensively use nuclear energy for civilian purposes in agriculture, industrial, health, education and environment sectors. IAEA had decided to offer substantial funding for 24 research projects, findings of which would be shared with other Asian countries.\n\nPakistan had become the \"highest recipient of IAEA's financial and technical assistance\" and that the relevant international agencies and Islamabad's bilateral supporters had been taken into confidence about the application of nuclear energy for civilian purposes. Mr. Parvez Butt also said the IAEA had allowed the Pakistan Atomic Energy Commission to amply use nuclear energy for improving the performance of agriculture, industrial, health, education and environment sectors.\n\nButt was very close to then-Prime Minister of Pakistan, Shaukat Aziz. Parvez Butt maintained a close association with Prime Minister Aziz until Butt's term to be completed. Parvez Butt submitted a long-term nuclear power plant project to Prime Minister Aziz, who allowed PAEC to established more both civilian and military purpose nuclear power plants. Butt also assisted Prime Minister Aziz to launch work on the 325-megawatt plant in Chashma, which is the second to be built at the site with Chinese help.\n\nOn 28 December 2005, Prime Minister Aziz inaugurated Chasma nuclear power plant, where both Chinese and Pakistani nuclear scientists attended. In an inauguration, Aziz said \"a milestone\" in the history of nuclear technology in Pakistan.\n\nHe has been awarded Pakistan's highest civil awards Hilal-e-Imtiaz and Sitara-e-Imtiaz in 1999 from the President of Pakistan, Justice (r). Muhammad Rafiq Tarar. Pervez Butt has been an active member of the Board of Government of World Association of Nuclear Operators (WANO), World Nuclear Association (WNA) and the International Atomic Energy Agency (IAEA). He was elected the Vice Chair of the IAEA Board of Government in 2005.\n\nOn 4 July 2006, Parvez Butt retired from PAEC as a Chief Scientist. Dr. Anwar Ali, a known nuclear physicist and his fellow nuclear engineer, took over as Chairman of the Pakistan Atomic Energy Commission in a simple ceremony held at PAEC headquarters. He was later transferred to Ministry of Science and Technology where he was elevated as the Science and Technology secretary. He is also serving as a member of Higher Education Commission.\n\nPervez Butt was a distinguished associate and team member of late Pakistani nuclear engineer Munir Ahmad Khan and also worked under Dr. I. H. Usmani and Dr. Ishfaq Ahmad. Like his mentors, he kept a lower profile. That is why he is also considered as the \"Unsung Hero of Nuclear Pakistan\".\n\n\n\n\n"}
{"id": "51211134", "url": "https://en.wikipedia.org/wiki?curid=51211134", "title": "Peter von Bilderling", "text": "Peter von Bilderling\n\nBaron Peter von Bilderling (born in St Peterburg May 26, 1844 – died in Zapolie September 25, 1900), was an engineer and an officer in the Engineering Corps of the Imperial Russian Army.\n\nHe was a humanist known for his writings, notably his book on the military horse, and his reorganisation of the Ijevsk arms factory, the manufacture of Berdanka carbines, and the creation of the no 4 line carbine. He founded the Tsaritsin refinery with Robert Nobel and the Branobel oil company in Baku with Ludwig Nobel. Finally, he established an agricultural and meteorological station in Zapolie, where he invented the roséomètre, an instrument for measuring the dew point. He was the brother of Baron Alexandre von Bilderling, the general who participated in the Russo-Japanese War.\n\nPeter von Bilderling was born to a noble Baltic German family from the Duchy of Courland and Semigallia. His father, Alexandre Otto Hermann Grigoriévitch von Bilderling, was a lieutenant-general in the Engineering Corps. His grandfather, Georges Sigismond von Bilterlings, (1767–1829), was a Lutheran pastor in Mitau, Courland, today known as Jelgava, Latvia. He was a professor, theologian, philosopher and writer. His mother came from an untitled family of the Polish nobility, the Doliwo-Dobrowolski.\n\nPeter von Bilderling was born in Saint Petersburg on May 26, 1844, and died the 25th of September 1900 at his Zapolie property near Louga, where he is buried. He left the Corps of Pages as a major with honors in 1861, then the Michel artillery academy in the first rank. He was attached to the Uhlan regiment of the guards of the Grand Duke Nicholas. He was an aide de camp to General Kartzof in the Caucasus. In 1870, he was sent on a mission by the Imperial government to Great Britain and the United States to study gun manufacture.\n\nHe married twice:\n\nWith an imperial government lease, he and Baron Standertskjöld, directed and reorganised the Ijevsk weapons factory from 1871 to l879 to require only domestic raw materials. In 1872, he signed an agreement with Ludvig Nobel, who furnished the machine tools. Under his direction, more than 400,000 Berdanka guns were produced. He created the #4 line carbine. A telegraph office opened at the factory. The factory was re-supplied by rail. It was during these years (with production starting of the Berdan rifle) that the division of labour and machine tool production of gun mechanism components came into its own. The Kalashnikov was invented and manufactured at Ijevsk. The carbine #4 underwent a baptism of fire during the Russo-Turk War. Von Bilderling personally visited the front to evaluate of the effectiveness of this weapon. He was later awarded the right to bear the arms of St George for his actions on the Danube. Wounded in the leg and head, he joined the reserve corps in September 1880.\n\nInstead of using his time and the assets of his mission to find raw materials for the Ijevsk factory, in 1876 Robert Nobel used the 25,000 rubles he had to buy an oil refinery in Tsaritsyne, now known as Volgograd. He saw the refinery as an investment with a great future.\n\nA total of 450,000 rubles were invested in BRANOBEL. Peter von Bilderling put in 300,000 and Baron Standertskjöld 150,000. The business began to show a profit.\n\nLudwig Nobel convinced his older brother Robert in May 1879 to set up a company, the \"Tovarichtchestvo Nephtanavo Proïsvodtsva Bratiev Nobel\" (Nobel Brothers Oil Production Company), named BRANOBEL in telegraph communications, and soon, simply NOBEL. Peter von Bilderling remained president of its administrative council until he died in 1900. The capital of the business founded in St Petersburg (3 million rubles in 600 shares of 5000 rubles each) was held as follows:\n\nThis 3-million-ruble capitalization was followed by another million rubles en 1880, 2 million and 4 million in 1881 and finally 5 million en 1884 (in 250-rubles shares to allow investors to multiply). BRANOBEL was, at the end of the 19th century, one of the biggest oil companies in the world. Pipeline technology for oil transportation was perfected near Baku by Vladimir Shukhov and Branobel in the years 1878–1880.\n\nOf the money bequeathed by Alfred Nobel to establish the Nobel Prize, 12% came from his shares in BRANOBEL. At that point he was its biggest individual investor. Bolsheviks took Baku on April 28, 1920, and nationalized BRANOBEL.\n\nAround 1881, after much searching in Courland, he bought from a property from General Borivoje Mirković, a property of 400 mas de Zapolie et de 490 mas de Boussany (Luga District, Saint-Petersburg). In 1889, he organised and became very interested in the model agricultural station in Zapolie with a special meteorology station, reputed even abroad. After completely organizing the work, he turned these stations over to the minister of agriculture.\n\nPeter von Bilderling's family was originally from Westphalia. In 1350 Otbert Boldring owned the fiefs of Cotwik and Overdik in Raalte parish and were notable members of the Hanseatic League. One branch of the family came from the Baltic countries with Johan Buldrink, in Courland, who became the vassal of the Teutonic Order when he received a fief near Riga named Bilderlingshof from Wolter von Plettenberg, the grand master of the Order. Between the 17th century and 1940 (currently Bulduri). Hermann von Bilderling had his family entered in the Nobiliaire de Courlande in 1634 (first class) and is cited in the Baltic genealogical literature as Uradel (noblesse immémoriale). The branch of Peter von Bilderling came through [[Livonia with Friedrich, grandson of Johan, and the fiefs of Karrinem and Taifer, then Lithuania with Johan then Melchior and the fief of Miszany. The family's title of baron was recognized in the Russian \"Nobiliaire\" through an edict of the Imperial Senate in 1903.\n\nThe family arms are: \"D'argent à une aigle de sable, languée de gueules, la poitrine d'argent bearing a green poplar.\" Cimier un peuplier arraché de sinople entre un vol de sable à dextre et d'argent à sénestre. Sable [[lambrequin]]s doublé d'argent.\n\nThe poplar is a reference to the original name, Boldring. (Bolder meant poplar in Old German. [[dictionnaire de Grimm]]).\n\n\n\n[[Category:1844 births]]\n[[Category:1900 deaths]]\n[[Category:People from Saint Petersburg]]\n[[Category:Russian engineers]]\n[[Category:Imperial Russian military personnel]]\n[[Category:Baltic nobility]]\n[[Category:People in the petroleum industry]]\n[[Category:Recipients of the Order of St. Vladimir, 3rd class]]\n[[Category:Recipients of the Order of St. Vladimir, 4th class]]\n[[Category:Recipients of the Order of St. Anna]]"}
{"id": "6737988", "url": "https://en.wikipedia.org/wiki?curid=6737988", "title": "Photobioreactor", "text": "Photobioreactor\n\nA photobioreactor is a bioreactor that utilizes a light source to cultivate phototrophic microorganisms. These organisms use photosynthesis to generate biomass from light and carbon dioxide and include plants, mosses, macroalgae, microalgae, cyanobacteria and purple bacteria. Within the artificial environment of a photobioreactor, specific conditions are carefully controlled for respective species. Thus, a photobioreactor allows much higher growth rates and purity levels than anywhere in nature or habitats similar to nature. Hypothetically, phototropic biomass could be derived from nutrient-rich wastewater and flue gas carbon dioxide in a photobioreactor.\n\nThe first approach for the controlled production of phototrophic organisms was and still is a natural open pond or artificial raceway pond. Therein, the culture suspension, which contains all necessary nutrients and carbon dioxide, is pumped around in a cycle, being directly illuminated from sunlight via the liquid’s surface. This construction principle is the simplest way of production for phototrophic organisms. But due to their depth (up to 0.3 m) and the related reduced average light supply, open systems only reach limited areal productivity rates. In addition, the consumption of energy is relatively high, as high amounts of water containing low product concentration have to be processed. Open space is expensive in areas with a dense population, while water is rare in others. Using open technologies causes high losses of water due to evaporation into the atmosphere.\n\nSince the 1950s several approaches have been conducted to develop closed systems, which theoretically provide higher cell densities of phototrophic organisms and therefore a lower demand of water to be pumped than open systems. In addition, closed construction avoids system-related water losses and the risk of contamination through landing water birds or dust is minimized. All modern photobioreactors have tried to balance between a thin layer of culture suspension, optimized light application, low pumping energy consumption, capital expenditure and microbial purity. Many different systems have been tested, but only a few approaches were able to perform at an industrial scale.\n\nThe simplest approach is the redesign of the well-known glass fermenters, which are state of the art in many biotechnological research and production facilities worldwide. The moss reactor for example shows a standard glass vessel, which is externally supplied with light. The existing head nozzles are used for sensor installation and for gas exchange. This type is quite common in laboratory scale, but it has never been established in bigger scale, due to its limited vessel size.\n\nMade from glass or plastic tubes, this photobioreactor type has succeeded within production scale. The tubes are oriented horizontally or vertically and are supplied from a central utilities installation with pump, sensors, nutrients and CO. Tubular photobioreactors are established worldwide from laboratory up to production scale, e.g. for the production of the carotenoid Astaxanthine from the green algae \"Haematococcus pluvialis\" or for the production of food supplement from the green algae \"Chlorella vulgaris\". These photobioreactors take advantage from the high purity levels and their efficient outputs. The biomass production can be done at a high quality level and the high biomass concentration at the end of the production allows energy efficient downstream processing. Due to the recent prices of the photobioreactors, economically feasible concepts today can only be found within high-value markets, e.g. food supplement or cosmetics.\n\nThe advantages of tubular photobioreactors at production scale are also transferred to laboratory scale. A combination of the mentioned glass vessel with a thin tube coil allows relevant biomass production rates a laboratory research scale. Being controlled by a complex process control system the regulation of the environmental conditions reaches a high level.\n\nAn alternative approach is shown by a photobioreactor, which is built in a tapered geometry and which carries a helically attached, translucent double hose circuit system. The result is a layout similar to a Christmas tree. The tubular system is constructed in modules and can theoretically be scaled outdoors up to agricultural scale. A dedicated location is not crucial, similar to other closed systems, and therefore non-arable land is suitable as well. The material choice should prevent biofouling and ensure high final biomass concentrations. The combination of turbulence and the closed concept should allow a clean operation and a high operational availability.\n\nAnother development approach can be seen with the construction based on plastic or glass plates. Plates with different technical design are mounted to form a small layer of culture suspension, which provides an optimized light supply. In addition, the simpler construction compared to tubular reactors allows the use of less expensive plastic materials. From the pool of different concepts e.g. meandering flow designs or bottom gassed systems have been realized and shown good output results. Some unsolved issues are material life time stability or the biofilm forming. Applications at industrial scale are limited by the scalability of plate systems.\n\nIn April 2013, the IBA in Hamburg, Germany, a building with an integrated glass plate photobioreactor facade, was commissioned.\n\nThis photobioreactor type consists of a plate-shaped basic geometry with peaks and valleys arranged in regular distance. This geometry causes the distribution of incident light over a larger surface which corresponds to a dilution effect. This also helps solving a basic problem in phototrophic cultivation, because most microalgae species react sensitively to high light intensities. Most microalgae experience light saturation already at light intensities, ranging substantially below the maximum daylight intensity of approximately 2000 W/m. Simultaneously, a larger light quantity can be exploited in order to improve photoconversion efficiency. The mixing is accomplished by a rotary pump, which causes a cylindrical rotation of the culture broth. In contrast to vertical designs, horizontal reactors contain only thin layers of media with a correspondingly low hydrodynamic pressure. This has a positive impact on the necessary energy input and reduces material costs at the same time.\n\nThe pressure of market prices has led the development of foil-based photobioreactor types. Inexpensive PVC or PE foils are mounted to form bags or vessels which cover algae suspensions and expose them to light. The pricing ranges of photobioreactor types have been enlarged with the foil systems. It has to be kept in mind, that these systems have a limited sustainability as the foils have to be replaced from time to time. For full balances, the investment for required support systems has to be calculated as well.\n\nPorous Substrate Bioreactor (PSBR), being developed at University of Cologne, also known as the twin-layer system, uses a new principle to separate the algae from a nutrient solution by means of a porous reactor surface on which the microalgae are trapped in biofilms. This new procedure reduces by a factor of up to one hundred the amount of liquid needed for operation compared to the current technology, which cultivates algae in suspensions. As such, the PSBR procedure significantly reduces the energy needed while increasing the portfolio of algae that can be cultivated.\n\nThe discussion around microalgae and their potentials in carbon dioxide sequestration and biofuel production has caused high pressure on developers and manufacturers of photobioreactors. Today, none of the mentioned systems is able to produce phototrophic microalgae biomass at a price which is able to compete with crude oil. New approaches test e.g. dripping methods to produce ultra-thin layers for maximal growth with application of flue gas and waste water. Further on, much research is done worldwide on genetically modified and optimized microalgae.\n\n\n"}
{"id": "34004553", "url": "https://en.wikipedia.org/wiki?curid=34004553", "title": "Photoelectrowetting", "text": "Photoelectrowetting\n\nPhotoelectrowetting is a modification of the wetting properties of a surface (typically a hydrophobic surface) using incident light. Whereas ordinary electrowetting is observed in surfaces consisting of a liquid/insulator/conductor stack, photoelectrowetting can be observed by replacing the conductor with a semiconductor to form a liquid/insulator/semiconductor stack. This has electrical and optical properties similar to the metal/insulator/semiconductor stack used in metal-oxide-semiconductor field effect transistors (MOSFETs) and charge-coupled devices (CCDs). Replacing the conductor with a semiconductor results in asymmetrical electrowetting behavior (in terms of voltage polarity), depending on the semiconductor doping type and density.\n\nIncident light above the semiconductor's band gap creates photo-induced carriers via electron-hole pair generation in the depletion region of the underlying semiconductor. This leads to a modification of the capacitance of the insulator/semiconductor stack, resulting in a modification of the contact angle of a liquid droplet resting on the surface of the stack in a continuous way which can also be non-reversible. The photoelectrowetting effect can be interpreted by a modification of the Young-Lippmann equation.\n\nThe figure illustrates the principle of the photoelectrowetting effect. At zero bias (0V) the conducting droplet has a large contact angle (left image) if the insulator is hydrophobic. As the bias is increased (positive for a p-type semiconductor, negative for an n-type semiconductor) the droplet spreads out – i.e. the contact angle decreases (middle image). In the presence of light (having an energy superior to the band gap of the semiconductor) the droplet spreads out more due to the reduction of the thickness of the space charge region at the insulator/semiconductor interface (right image).\n\nPhoto-actuation of microelectromechanical systems (MEMS) has been demonstrated using photoelectrowetting., A microcantilever is placed on top of the liquid-insulator-photoconductor junction. As light is shined on the junction, the capillary force from the droplet on the cantilever, due to the contact angle change, deflects the cantilever. This wireless actuation can be used as a substitute for complex circuit-based systems currently used for optical addressing and control of autonomous wireless sensors\n\nPhotoelectrowetting can be used to circulate aqueous solution-based sessile droplets on a silicon wafer covered with silicon dioxide and Teflon – the latter providing a hydrophobic surface. Droplet transport is achieved by focusing a laser at the leading edge of the droplet. Droplet speeds of more than 10 mm/s can be achieved without the necessity of underlying patterned electrodes.\n\n\n"}
{"id": "13340625", "url": "https://en.wikipedia.org/wiki?curid=13340625", "title": "RWE Supply &amp; Trading CZ", "text": "RWE Supply &amp; Trading CZ\n\nRWE Supply & Trading CZ is the largest natural gas trading company in the Czech Republic. It is owned by the German energy company RWE.\n\nThe group used to own pipelines from Lanžhot on Czech-Slovak border to Germany. Its market position deteriorated in 1997 when competing Yamal-Europe pipeline was put into operation. Another threat was the Nord Stream linking Russia and Germany via Baltic sea. Its past subsidiaries include RWE Gas Storage (owns and operates several underground storages located in the Czech Republic).\n\n"}
{"id": "885860", "url": "https://en.wikipedia.org/wiki?curid=885860", "title": "Red Adair", "text": "Red Adair\n\nPaul Neal \"Red\" Adair (June 18, 1915 – August 7, 2004) was an American oil well firefighter. He became notable as an innovator in the highly specialized and hazardous profession of extinguishing and capping oil well blowouts, both land-based and offshore.\n\nAdair was born in Houston, Texas, the son of an Irish blacksmith, and attended Reagan High School. He began fighting oil well fires after returning from serving in an Army bomb disposal unit during World War II. He started his career working for Myron Kinley, the \"original\" blowout/oil firefighting pioneer. They pioneered the technique of using a V-shaped charge of high explosives (the Munroe effect being used during the war and used in bazookas and the atom bomb), the high velocity blast of which would snuff the fire. He founded Red Adair Co. Inc. in 1959, and over the course of his career battled more than 2,000 land and offshore oil well, natural gas well, and similar spectacular fires. Adair gained global attention in 1962 when he tackled a fire at the Gassi Touil gas field in the Algerian Sahara nicknamed the Devil's Cigarette Lighter, a pillar of flame that burned from 12:00 PM November 13, 1961 to 9:30 AM on April 28, 1962. In December 1968, Adair sealed a large gas leak at an Australian gas and oil platform off Victoria's southeast coast.\n\nIn 1977, he and his crew (including Asger \"Boots\" Hansen and Manohar \"Man\" Dhumtara-Kejriwal) contributed to the capping of the biggest oil well blowout to have occurred in the North Sea (and at the time the largest offshore blowout worldwide, in terms of volume of crude oil spilled), at the Ekofisk Bravo platform, located in the Norwegian sector and operated by Phillips Petroleum Company (now ConocoPhillips). In 1978, Adair's top lieutenants Hansen and Ed \"Coots\" Matthews left to found competitor Boots & Coots International Well Control Inc. In 1988, Adair was again in the North Sea where he helped to put out the UK sector Piper Alpha oil platform fire. At age 75, Adair took part in extinguishing the oil well fires in Kuwait set by retreating Iraqi troops after the Gulf War in 1991.\n\nAdair retired in 1993, and sold The Red Adair Service and Marine Company to Global Industries. His top employees (Brian Krause, Raymond Henry, Rich Hatteberg) left in 1994 and formed their own company, International Well Control (IWC). \n\nAdair died in 2004 at age 89, survived by his wife, a son and a daughter.\n\n"}
{"id": "533199", "url": "https://en.wikipedia.org/wiki?curid=533199", "title": "S-Methylmethionine", "text": "S-Methylmethionine\n\n\"S\"-Methylmethionine (SMM) is a derivative of methionine with the chemical formula (CH)SCHCHCH(NH)CO. This cation is an intermediate in many biosynthetic pathways owing to the sulfonium functional group. The natural derivative \"S\"-methylmethionine is biosynthesized from -methionine which is first converted to \"S\"-adenosylmethionine. The subsequent conversion, involving replacement of the adenosyl group by a methyl group is catalyzed by the enzyme methionine S-methyltransferase. \"S\"-methylmethionine is particularly abundant in plants, being more abundant than methionine.\n\n\"S\"-Methylmethionine is sometimes referred to as \"vitamin U\", but it is not considered a true vitamin. The term was coined in 1950 by Garnett Cheney for uncharacterized anti-ulcerogenic factors in raw cabbage juice that may help speed healing of peptic ulcers.\n\n\"S\"-Methylmethionine arises via the methylation of methionine by \"S\"-adenosyl methionine (SAM). The coproduct is \"S\"-adenosyl homocysteine.\n\nThe biological roles of \"S\"-methylmethionine are not well understood. Speculated roles include methionine storage, use as a methyl donor, regulation of SAM. A few plants use \"S\"-methylmethionine as a precursor to the osmolyte dimethylsulfoniopropionate (DMSP). Intermediates include dimethylsulfoniumpropylamine and dimethylsulfoniumpropionaldehyde.\n\n\"S\"-Methylmethionine is claimed to have protective effects in the gastrointestinal mucosa and in the liver.\n\n\"S\"-Methylmethionine is found in barley and is further created during the malting process. SMM can be subsequently converted to DMS during the malt kilning process. Lightly kilned malts such as pilsner or lager malts retain much of their SMM content while higher kilned malt such as pale ale malt has substantially more of the SMM converted to DMS in the malt. Darker kilned malts such as Munich malt have virtually no SMM content since most has been converted to DMS. Other crystal malts and roasted malts have no SMM content and often no DMS content since the kilning also drives that compound out of the malt. \n"}
{"id": "27119", "url": "https://en.wikipedia.org/wiki?curid=27119", "title": "Silver", "text": "Silver\n\nSilver is a chemical element with symbol Ag (from the Latin \"\", derived from the Proto-Indo-European \"h₂erǵ\": \"shiny\" or \"white\") and atomic number 47. A soft, white, lustrous transition metal, it exhibits the highest electrical conductivity, thermal conductivity, and reflectivity of any metal. The metal is found in the Earth's crust in the pure, free elemental form (\"native silver\"), as an alloy with gold and other metals, and in minerals such as argentite and chlorargyrite. Most silver is produced as a byproduct of copper, gold, lead, and zinc refining.\n\nSilver has long been valued as a precious metal. Silver metal is used in many bullion coins, sometimes alongside gold: while it is more abundant than gold, it is much less abundant as a native metal. Its purity is typically measured on a per-mille basis; a 94%-pure alloy is described as \"0.940 fine\". As one of the seven metals of antiquity, silver has had an enduring role in most human cultures.\n\nOther than in currency and as an investment medium (coins and bullion), silver is used in solar panels, water filtration, jewellery, ornaments, high-value tableware and utensils (hence the term silverware), in electrical contacts and conductors, in specialized mirrors, window coatings, in catalysis of chemical reactions, as a colorant in stained glass and in specialised confectionery. Its compounds are used in photographic and X-ray film. Dilute solutions of silver nitrate and other silver compounds are used as disinfectants and microbiocides (oligodynamic effect), added to bandages and wound-dressings, catheters, and other medical instruments.\n\nSilver is similar in its physical and chemical properties to its two vertical neighbours in group 11 of the periodic table, copper and gold. Its 47 electrons are arranged in the configuration [Kr]4d5s, similarly to copper ([Ar]3d4s) and gold ([Xe]4f5d6s); group 11 is one of the few groups in the d-block which has a completely consistent set of electron configurations. This distinctive electron configuration, with a single electron in the highest occupied s subshell over a filled d subshell, accounts for many of the singular properties of metallic silver.\n\nSilver is an extremely soft, ductile and malleable transition metal, though it is slightly less malleable than gold. Silver crystallizes in a face-centered cubic lattice with bulk coordination number 12, where only the single 5s electron is delocalized, similarly to copper and gold. Unlike metals with incomplete d-shells, metallic bonds in silver are lacking a covalent character and are relatively weak. This observation explains the low hardness and high ductility of single crystals of silver.\n\nSilver has a brilliant white metallic luster that can take a high polish, and which is so characteristic that the name of the metal itself has become a colour name. Unlike copper and gold, the energy required to excite an electron from the filled d band to the s-p conduction band in silver is large enough (around 385 kJ/mol) that it no longer corresponds to absorption in the visible region of the spectrum, but rather in the ultraviolet; hence silver is not a coloured metal. Protected silver has greater optical reflectivity than aluminium at all wavelengths longer than ~450 nm. At wavelengths shorter than 450 nm, silver's reflectivity is inferior to that of aluminium and drops to zero near 310 nm.\n\nVery high electrical and thermal conductivity is common to the elements in group 11, because their single s electron is free and does not interact with the filled d subshell, as such interactions (which occur in the preceding transition metals) lower electron mobility. The electrical conductivity of silver is the greatest of all metals, greater even than copper, but it is not widely used for this property because of the higher cost. An exception is in radio-frequency engineering, particularly at VHF and higher frequencies where silver plating improves electrical conductivity because those currents tend to flow on the surface of conductors rather than through the interior. During World War II in the US, tons of silver were used in electromagnets for enriching uranium, mainly because of the wartime shortage of copper. Pure silver has the highest thermal conductivity of any metal, although the conductivity of carbon (in the diamond allotrope) and superfluid helium-4 are even higher. Silver also has the lowest contact resistance of any metal.\n\nSilver readily forms alloys with copper and gold, as well as zinc. Zinc-silver alloys with low zinc concentration may be considered as face-centred cubic solid solutions of zinc in silver, as the structure of the silver is largely unchanged while the electron concentration rises as more zinc is added. Increasing the electron concentration further leads to body-centred cubic (electron concentration 1.5), complex cubic (1.615), and hexagonal close-packed phases (1.75).\n\nNaturally occurring silver is composed of two stable isotopes, Ag and Ag, with Ag being slightly more abundant (51.839% natural abundance). This almost equal abundance is rare in the periodic table. The atomic weight is 107.8682(2) u; this value is very important because of the importance of silver compounds, particularly halides, in gravimetric analysis. Both isotopes of silver are produced in stars via the s-process (slow neutron capture), as well as in supernovas via the r-process (rapid neutron capture).\n\nTwenty-eight radioisotopes have been characterized, the most stable being Ag with a half-life of 41.29 days, Ag with a half-life of 7.45 days, and Ag with a half-life of 3.13 hours. Silver has numerous nuclear isomers, the most stable being Ag (\"t\" = 418 years), Ag (\"t\" = 249.79 days) and Ag (\"t\" = 8.28 days). All of the remaining radioactive isotopes have half-lives of less than an hour, and the majority of these have half-lives of less than three minutes.\n\nIsotopes of silver range in relative atomic mass from 92.950 u (Ag) to 129.950 u (Ag); the primary decay mode before the most abundant stable isotope, Ag, is electron capture and the primary mode after is beta decay. The primary decay products before Ag are palladium (element 46) isotopes, and the primary products after are cadmium (element 48) isotopes.\n\nThe palladium isotope Pd decays by beta emission to Ag with a half-life of 6.5 million years. Iron meteorites are the only objects with a high-enough palladium-to-silver ratio to yield measurable variations in Ag abundance. Radiogenic Ag was first discovered in the Santa Clara meteorite in 1978. The discoverers suggest the coalescence and differentiation of iron-cored small planets may have occurred 10 million years after a nucleosynthetic event. Pd–Ag correlations observed in bodies that have clearly been melted since the accretion of the solar system must reflect the presence of unstable nuclides in the early solar system.\n\nSilver is a rather unreactive metal. This is because its filled 4d shell is not very effective in shielding the electrostatic forces of attraction from the nucleus to the outermost 5s electron, and hence silver is near the bottom of the electrochemical series (\"E\"(Ag/Ag) = +0.799 V). In group 11, silver has the lowest first ionization energy (showing the instability of the 5s orbital), but has higher second and third ionization energies than copper and gold (showing the stability of the 4d orbitals), so that the chemistry of silver is predominantly that of the +1 oxidation state, reflecting the increasingly limited range of oxidation states along the transition series as the d-orbitals fill and stabilize. Unlike copper, for which the larger hydration energy of Cu as compared to Cu is the reason why the former is the more stable in aqueous solution and solids despite lacking the stable filled d-subshell of the latter, with silver this effect is swamped by its larger second ionisation energy. Hence, Ag is the stable species in aqueous solution and solids, with Ag being much less stable as it oxidizes water.\n\nMost silver compounds have significant covalent character due to the small size and high first ionization energy (730.8 kJ/mol) of silver. Furthermore, silver's Pauling electronegativity of 1.93 is higher than that of lead (1.87), and its electron affinity of 125.6 kJ/mol is much higher than that of hydrogen (72.8 kJ/mol) and not much less than that of oxygen (141.0 kJ/mol). Due to its full d-subshell, silver in its main +1 oxidation state exhibits relatively few properties of the transition metals proper from groups 4 to 10, forming rather unstable organometallic compounds, forming linear complexes showing very low coordination numbers like 2, and forming an amphoteric oxide as well as Zintl phases like the post-transition metals. Unlike the preceding transition metals, the +1 oxidation state of silver is stable even in the absence of π-acceptor ligands.\n\nSilver does not react with air, even at red heat, and thus was considered by alchemists as a noble metal along with gold. Its reactivity is intermediate between that of copper (which forms copper(I) oxide when heated in air to red heat) and gold. Like copper, silver reacts with sulfur and its compounds; in their presence, silver tarnishes in air to form the black silver sulfide (copper forms the green sulfate instead, while gold does not react). Unlike copper, silver will not react with the halogens, with the exception of fluorine gas, with which it forms the difluoride. While silver is not attacked by non-oxidizing acids, the metal dissolves readily in hot concentrated sulfuric acid, as well as dilute or concentrated nitric acid. In the presence of air, and especially in the presence of hydrogen peroxide, silver dissolves readily in aqueous solutions of cyanide.\n\nThe three main forms of deterioration in historical silver artifacts are tarnishing, formation of silver chloride due to long-term immersion in salt water, as well as reaction with nitrate ions or oxygen. Fresh silver chloride is pale yellow, becoming purplish on exposure to light; it projects slightly from the surface of the artifact or coin. The precipitation of copper in ancient silver can be used to date artifacts, as copper is nearly always a constituent of silver alloys.\n\nSilver metal is attacked by strong oxidizers such as potassium permanganate () and potassium dichromate (), and in the presence of potassium bromide (). These compounds are used in photography to bleach silver images, converting them to silver bromide that can either be fixed with thiosulfate or redeveloped to intensify the original image. Silver forms cyanide complexes (silver cyanide) that are soluble in water in the presence of an excess of cyanide ions. Silver cyanide solutions are used in electroplating of silver.\n\nThe common oxidation states of silver are (in order of commonness): +1 (the most stable state; for example, silver nitrate, AgNO); +2 (highly oxidising; for example, silver(II) fluoride, AgF); and even very rarely +3 (extreme oxidising; for example, potassium tetrafluoroargentate(III), KAgF). The +1 state is by far the most common, followed by the easily reducible +2 state. The +3 state requires very strong oxidising agents to attain, such as fluorine or peroxodisulfate, and some silver(III) compounds react with atmospheric moisture and attack glass. Indeed, silver(III) fluoride is usually obtained by reacting silver or silver monofluoride with the strongest known oxidizing agent, krypton difluoride.\n\nSilver and gold have rather low chemical affinities for oxygen, lower than copper, and it is therefore expected that silver oxides are thermally quite unstable. Soluble silver(I) salts precipitate dark-brown silver(I) oxide, AgO, upon the addition of alkali. (The hydroxide AgOH exists only in solution; otherwise it spontaneously decomposes to the oxide.) Silver(I) oxide is very easily reduced to metallic silver, and decomposes to silver and oxygen above 160 °C. This and other silver(I) compounds may be oxidized by the strong oxidizing agent peroxodisulfate to black AgO, a mixed silver(I,III) oxide of formula AgAgO. Some other mixed oxides with silver in non-integral oxidation states, namely AgO and AgO, are also known, as is AgO which behaves as a metallic conductor.\n\nSilver(I) sulfide, AgS, is very readily formed from its constituent elements and is the cause of the black tarnish on some old silver objects. It may also be formed from the reaction of hydrogen sulfide with silver metal or aqueous Ag ions. Many non-stoichiometric selenides and tellurides are known; in particular, AgTe is a low-temperature superconductor.\n\nThe only known dihalide of silver is the difluoride, AgF, which can be obtained from the elements under heat. A strong yet thermally stable and therefore safe fluorinating agent, silver(II) fluoride is often used to synthesize hydrofluorocarbons.\n\nIn stark contrast to this, all four silver(I) halides are known. The fluoride, chloride, and bromide have the sodium chloride structure, but the iodide has three known stable forms at different temperatures; that at room temperature is the cubic zinc blende structure. They can all be obtained by the direct reaction of their respective elements. As the halogen group is descended, the silver halide gains more and more covalent character, solubility decreases, and the color changes from the white chloride to the yellow iodide as the energy required for ligand-metal charge transfer (XAg → XAg) decreases. The fluoride is anomalous, as the fluoride ion is so small that it has a considerable solvation energy and hence is highly water-soluble and forms di- and tetrahydrates. The other three silver halides are highly insoluble in aqueous solutions and are very commonly used in gravimetric analytical methods. All four are photosensitive (though the monofluoride is so only to ultraviolet light), especially the bromide and iodide which photodecompose to silver metal, and thus were used in traditional photography. The reaction involved is:\n\nThe process is not reversible because the silver atom liberated is typically found at a crystal defect or an impurity site, so that the electron's energy is lowered enough that it is \"trapped\".\n\nWhite silver nitrate, AgNO, is a versatile precursor to many other silver compounds, especially the halides, and is much less sensitive to light. It was once called \"lunar caustic\" because silver was called \"luna\" by the ancient alchemists, who believed that silver was associated with the moon. It is often used for gravimetric analysis, exploiting the insolubility of the heavier silver halides which it is a common precursor to. Silver nitrate is used in many ways in organic synthesis, e.g. for deprotection and oxidations. Ag binds alkenes reversibly, and silver nitrate has been used to separate mixtures of alkenes by selective absorption. The resulting adduct can be decomposed with ammonia to release the free alkene.\n\nYellow silver carbonate, AgCO can be easily prepared by reacting aqueous solutions of sodium carbonate with a deficiency of silver nitrate. Its principal use is for the production of silver powder for use in microelectronics. It is reduced with formaldehyde, producing silver free of alkali metals:\n\nSilver carbonate is also used as a reagent in organic synthesis such as the Koenigs-Knorr reaction. In the Fétizon oxidation, silver carbonate on celite acts as an oxidising agent to form lactones from diols. It is also employed to convert alkyl bromides into alcohols.\n\nSilver fulminate, AgCNO, a powerful, touch-sensitive explosive used in percussion caps, is made by reaction of silver metal with nitric acid in the presence of ethanol. Other dangerously explosive silver compounds are silver azide, AgN, formed by reaction of silver nitrate with sodium azide, and silver acetylide, AgC, formed when silver reacts with acetylene gas in ammonia solution. In its most characteristic reaction, silver azide decomposes explosively, releasing nitrogen gas: given the photosensitivity of silver salts, this behaviour may be induced by shining a light on its crystals.\n\nSilver complexes tend to be similar to those of its lighter homologue copper. Silver(III) complexes tend to be rare and very easily reduced to the more stable lower oxidation states, though they are slightly more stable than those of copper(III). For instance, the square planar periodate [Ag(IOOH)] and tellurate [Ag{TeO(OH)}] complexes may be prepared by oxidising silver(I) with alkaline peroxodisulfate. The yellow diamagnetic [AgF] is much less stable, fuming in moist air and reacting with glass.\n\nSilver(II) complexes are more common. Like the valence isoelectronic copper(II) complexes, they are usually square planar and paramagnetic, which is increased by the greater field splitting for 4d electrons than for 3d electrons. Aqueous Ag, produced by oxidation of Ag by ozone, is a very strong oxidising agent, even in acidic solutions: it is stabilized in phosphoric acid due to complex formation. Peroxodisulfate oxidation is generally necessary to give the more stable complexes with heterocyclic amines, such as [Ag(py)] and [Ag(bipy)]: these are stable provided the counterion cannot reduce the silver back to the +1 oxidation state. [AgF] is also known in its violet barium salt, as are some silver(II) complexes with \"N\"- or \"O\"-donor ligands such as pyridine carboxylates.\n\nBy far the most important oxidation state for silver in complexes is +1. The Ag cation is diamagnetic, like its homologues Cu and Au, as all three have closed-shell electron configurations with no unpaired electrons: its complexes are colourless provided the ligands are not too easily polarized such as I. Ag forms salts with most anions, but it is reluctant to coordinate to oxygen and thus most of these salts are insoluble in water: the exceptions are the nitrate, perchlorate, and fluoride. The tetracoordinate tetrahedral aqueous ion [Ag(HO)] is known, but the characteristic geometry for the Ag cation is 2-coordinate linear. For example, silver chloride dissolves readily in excess aqueous ammonia to form [Ag(NH)]; silver salts are dissolved in photography due to the formation of the thiosulfate complex [Ag(SO)]; and cyanide extraction for silver (and gold) works by the formation of the complex [Ag(CN)]. Silver cyanide forms the linear polymer {Ag–C≡N→Ag–C≡N→}; silver thiocyanate has a similar structure, but forms a zigzag instead because of the sp-hybridized sulfur atom. Chelating ligands are unable to form linear complexes and thus silver(I) complexes with them tend to form polymers; a few exceptions exist, such as the near-tetrahedral diphosphine and diarsine complexes [Ag(L–L)].\n\nUnder standard conditions, silver does not form simple carbonyls, due to the weakness of the Ag–C bond. A few are known at very low temperatures around 6–15 K, such as the green, planar paramagnetic Ag(CO), which dimerizes at 25–30 K, probably by forming Ag–Ag bonds. Additionally, the silver carbonyl [Ag(CO)] [B(OTeF)] is known. Polymeric AgLX complexes with alkenes and alkynes are known, but their bonds are thermodynamically weaker than even those of the platinum complexes (though they are formed more readily than those of the analogous gold complexes): they are also quite unsymmetrical, showing the weak \"π\" bonding in group 11. Ag–C \"σ\" bonds may also be formed by silver(I), like copper(I) and gold(I), but the simple alkyls and aryls of silver(I) are even less stable than those of copper(I) (which tend to explode under ambient conditions). For example, poor thermal stability is reflected in the relative decomposition temperatures of AgMe (−50 °C) and CuMe (−15 °C) as well as those of PhAg (74 °C) and PhCu (100 °C).\n\nThe C–Ag bond is stabilized by perfluoroalkyl ligands, for example in AgCF(CF). Alkenylsilver compounds are also more stable than their alkylsilver counterparts. Silver-NHC complexes are easily prepared, and are commonly used to prepare other NHC complexes by displacing labile ligands. For example, the reaction of the bis(NHC)silver(I) complex with bis(acetonitrile)palladium dichloride or chlorido(dimethyl sulfide)gold(I):\n\nSilver forms alloys with most other elements on the periodic table. The elements from groups 1–3, except for hydrogen, lithium, and beryllium, are very miscible with silver in the condensed phase and form intermetallic compounds; those from groups 4–9 are only poorly miscible; the elements in groups 10–14 (except boron and carbon) have very complex Ag–M phase diagrams and form the most commercially important alloys; and the remaining elements on the periodic table have no consistency in their Ag–M phase diagrams. By far the most important such alloys are those with copper: most silver used for coinage and jewellery is in reality a silver–copper alloy, and the eutectic mixture is used in vacuum brazing. The two metals are completely miscible as liquids but not as solids; their importance in industry comes from the fact that their properties tend to be suitable over a wide range of variation in silver and copper concentration, although most useful alloys tend to be richer in silver than the eutectic mixture (71.9% silver and 28.1% copper by weight, and 60.1% silver and 28.1% copper by atom).\n\nMost other binary alloys are of little use: for example, silver–gold alloys are too soft and silver–cadmium alloys too toxic. Ternary alloys have much greater importance: dental amalgams are usually silver–tin–mercury alloys, silver–copper–gold alloys are very important in jewellery (usually on the gold-rich side) and have a vast range of hardnesses and colours, silver–copper–zinc alloys are useful as low-melting brazing alloys, and silver–cadmium–indium (involving three adjacent elements on the periodic table) is useful in nuclear reactors because of its high thermal neutron capture cross-section, good conduction of heat, mechanical stability, and resistance to corrosion in hot water.\n\nThe word \"silver\" appears in Anglo-Saxon in various spellings, such as \"seolfor\" and \"siolfor\". A similar form is seen throughout the Germanic languages (compare Old High German \"silabar\" and \"silbir\"). The chemical symbol Ag is from the Latin word for \"silver\", \"argentum\" (compare Ancient Greek ἄργυρος, \"árgyros\"), from the Proto-Indo-European root *\"h₂erǵ-\" (formerly reconstructed as \"*arǵ-\"), meaning \"white\" or \"shining\": this was the usual Proto-Indo-European word for the metal, whose reflexes are missing in Germanic and Balto-Slavic. The Balto-Slavic words for silver are quite similar to the Germanic ones (e.g. Russian \"серебро\" [\"serebro\"], Polish \"srebro\", Lithuanian \"sidabras\") and they may have a common origin, although this is uncertain: some scholars have suggested the Akkadian \"sarpu\" \"refined silver\" as this origin, related to the word \"sarapu\" \"to refine or smelt\".\n\nSilver was one of the seven metals of antiquity that were known to prehistoric humans and whose discovery is thus lost to history. In particular, the three metals of group 11, copper, silver, and gold, occur in the elemental form in nature and were probably used as the first primitive forms of money as opposed to simple bartering. However, unlike copper, silver did not lead to the growth of metallurgy on account of its low structural strength, and was more often used ornamentally or as money. Since silver is more reactive than gold, supplies of native silver were much more limited than those of gold. For example, silver was more expensive than gold in Egypt until around the fifteenth century BC: the Egyptians are thought to have separated gold from silver by heating the metals with salt, and then reducing the silver chloride produced to the metal.\n\nThe situation changed with the discovery of cupellation, a technique that allowed silver metal to be extracted from its ores. While slag heaps found in Asia Minor and on the islands of the Aegean Sea indicate that silver was being separated from lead as early as the 4th millennium BC, and one of the earliest silver extraction centres in Europe was Sardinia in early the Chalcolithic period, these techniques did not spread widely until later,\nwhen it spread throughout the region and beyond. The origins of silver production in India, China, and Japan were almost certainly equally ancient, but are not well-documented due to their great age.\nWhen the Phoenicians first came to what is now Spain, they obtained so much silver that they could not fit it all on their ships, and as a result used silver to weight their anchors instead of lead. By the time of the Greek and Roman civilizations, silver coins were a staple of the economy: the Greeks were already extracting silver from galena by the 7th century BC, and the rise of Athens was partly made possible by the nearby silver mines at Laurium, from which they extracted about 30 tonnes a year from 600 to 300 BC. The stability of the Roman currency relied to a high degree on the supply of silver bullion, mostly from Spain, which Roman miners produced on a scale unparalleled before the discovery of the New World. Reaching a peak production of 200 tonnes per year, an estimated silver stock of 10000 tonnes circulated in the Roman economy in the middle of the second century AD, five to ten times larger than the combined amount of silver available to medieval Europe and the Abbasid Caliphate around AD 800. The Romans also recorded the extraction of silver in central and northern Europe in the same time period. This production came to a nearly complete halt with the fall of the Roman Empire, not to resume until the time of Charlemagne: by then, tens of thousands of tonnes of silver had already been extracted.\n\nCentral Europe became the centre of silver production during the Middle Ages, as the Mediterranean deposits exploited by the ancient civilisations had been exhausted. Silver mines were opened in Bohemia, Saxony, Erzgebirge, Alsace, the Lahn region, Siegerland, Silesia, Hungary, Norway, Steiermark, Salzburg, and the southern Black Forest. Most of these ores were quite rich in silver and could simply be separated by hand from the remaining rock and then smelted; some deposits of native silver were also encountered. Many of these mines were soon exhausted, but a few of them remained active until the Industrial Revolution, before which the world production of silver was around a meagre 50 tonnes per year. In the Americas, high temperature silver-lead cupellation technology was developed by pre-Inca civilizations as early as AD 60–120; silver deposits in India, China, Japan, and pre-Columbian America continued to be mined during this time.\n\nWith the discovery of America and the plundering of silver by the Spanish conquistadors, Central and South America became the dominant producers of silver until around the beginning of the 18th century, particularly Peru, Bolivia, Chile, and Argentina: the last of these countries later took its name from that of the metal that composed so much of its mineral wealth. The silver trade this was a part of gave way to a global network of exchange. As one historian put it, silver \"went round the world and made the world go round.\" Much of this silver ended up in the hands of the Chinese. A Portuguese merchant in 1621 noted that silver \"wanders throughout all the world... before flocking to China, where it remains as if at its natural center.\" Still, much of it went to Spain, allowing Spanish rulers to pursue military and political ambitions in both Europe and the Americas. \"New World mines,\" concluded several historians, \"supported the Spanish empire.\"\n\nIn the 19th century, primary production of silver moved to North America, particularly Canada, Mexico, and Nevada in the United States: some secondary production from lead and zinc ores also took place in Europe, and deposits in Siberia and the Russian Far East as well as in Australia were mined. Poland emerged as an important producer during the 1970s after the discovery of copper deposits that were rich in silver, before the centre of production returned to the Americas the following decade. Today, Peru and Mexico are still among the primary silver producers, but the distribution of silver production around the world is quite balanced and about one-fifth of the silver supply comes from recycling instead of new production.\n\nSilver plays a certain role in mythology and has found various usage as a metaphor and in folklore. The Greek poet Hesiod's \"Works and Days\" (lines 109–201) lists different ages of man named after metals like gold, silver, bronze and iron to account for successive ages of humanity. Ovid's \"Metamorphoses\" contains another retelling of the story, containing an illustration of silver's metaphorical use of signifying the second-best in a series, better than bronze but worse than gold:\n\nIn folklore, silver was commonly thought to have mystic powers: for example, a bullet cast from silver is often supposed in such folklore the only weapon that is effective against a werewolf, witch, or other monsters. From this the idiom of a silver bullet developed into figuratively referring to any simple solution with very high effectiveness or almost miraculous results, as in the widely discussed software engineering paper \"No Silver Bullet\".\n\nSilver production has also inspired figurative language. Clear references to cupellation occur throughout the Old Testament of the Bible, such as in Jeremiah's rebuke to Judah: \"The bellows are burned, the lead is consumed of the fire; the founder melteth in vain: for the wicked are not plucked away. Reprobate silver shall men call them, because the Lord hath rejected them.\" (Jeremiah 6:19–20) Jeremiah was also aware of sheet silver, exemplifying the malleability and ductility of the metal: \"Silver spread into plates is brought from Tarshish, and gold from Uphaz, the work of the workman, and of the hands of the founder: blue and purple is their clothing: they are all the work of cunning men.\" (Jeremiah 10:9)\n\nSilver also has more negative cultural meanings: the idiom thirty pieces of silver, referring to a reward for betrayal, references the bribe Judas Iscariot is said in the New Testament to have taken from Jewish leaders in Jerusalem to turn Jesus of Nazareth over to soldiers of the high priest Caiaphas. Ethically, silver also symbolizes greed and degradation of consciousness; this is the negative aspect, the perverting of its value.\n\nThe abundance of silver in the Earth's crust is 0.08 parts per million, almost exactly the same as that of mercury. It mostly occurs in sulfide ores, especially acanthite and argentite, AgS. Argentite deposits sometimes also contain native silver when they occur in reducing environments, and when in contact with salt water they are converted to chlorargyrite (including horn silver), AgCl, which is prevalent in Chile and New South Wales. Most other silver minerals are silver pnictides or chalcogenides; they are generally lustrous semiconductors. Most true silver deposits, as opposed to argentiferous deposits of other metals, came from Tertiary period vulcanism.\n\nThe principal sources of silver are the ores of copper, copper-nickel, lead, and lead-zinc obtained from Peru, Bolivia, Mexico, China, Australia, Chile, Poland and Serbia. Peru, Bolivia and Mexico have been mining silver since 1546, and are still major world producers. Top silver-producing mines are Cannington (Australia), Fresnillo (Mexico), San Cristóbal (Bolivia), Antamina (Peru), Rudna (Poland), and Penasquito (Mexico). Top near-term mine development projects through 2015 are Pascua Lama (Chile), Navidad (Argentina), Jaunicipio (Mexico), Malku Khota (Bolivia), and Hackett River (Canada). In Central Asia, Tajikistan is known to have some of the largest silver deposits in the world.\n\nSilver is usually found in nature combined with other metals, or in minerals that contain silver compounds, generally in the form of sulfides such as galena (lead sulfide) or cerussite (lead carbonate). So the primary production of silver requires the smelting and then cupellation of argentiferous lead ores, a historically important process. Lead melts at 327 °C, lead oxide at 888 °C and silver melts at 960 °C. To separate the silver, the alloy is melted again at the high temperature of 960 °C to 1000 °C in an oxidizing environment. The lead oxidises to lead monoxide, then known as litharge, which captures the oxygen from the other metals present. The liquid lead oxide is removed or absorbed by capillary action into the hearth linings.\n\nToday, silver metal is primarily produced instead as a secondary byproduct of electrolytic refining of copper, lead, and zinc, and by application of the Parkes process on lead bullion from ore that also contains silver. In such processes, silver follows the non-ferrous metal in question through its concentration and smelting, and is later purified out. For example, in copper production, purified copper is electrolytically deposited on the cathode, while the less reactive precious metals such as silver and gold collect under the anode as the so-called \"anode slime\". This is then separated and purified of base metals by treatment with hot aerated dilute sulfuric acid and heating with lime or silica flux, before the silver is purified to over 99.9% purity via electrolysis in nitrate solution.\n\nCommercial-grade fine silver is at least 99.9% pure, and purities greater than 99.999% are available. In 2014, Mexico was the top producer of silver (5,000 tonnes or 18.7% of the world's total of 26,800 t), followed by China (4,060 t) and Peru (3,780 t).\n\nThe earliest known coins were minted in the kingdom of Lydia in Asia Minor around 600 BC. The coins of Lydia were made of electrum, which is a naturally occurring alloy of gold and silver, that was available within the territory of Lydia. Since that time, silver standards, in which the standard economic unit of account is a fixed weight of silver, have been widespread throughout the world until the 20th century. Notable silver coins through the centuries include the Greek drachma, the Roman denarius, the Islamic dirham, the karshapana from ancient India and rupee from the time of the Mughal Empire (grouped with copper and gold coins to create a trimetallic standard), and the Spanish dollar.\n\nThe ratio between the amount of silver used for coinage and that used for other purposes has fluctuated greatly over time; for example, in wartime, more silver tends to have been used for coinage to finance the war.\n\nToday, silver bullion has the ISO 4217 currency code XAG, one of only four precious metals to have one (the others being palladium, platinum, and gold). Silver coins are produced from cast rods or ingots, rolled to the correct thickness, heat-treated, and then used to cut blanks from. These blanks are then milled and minted in a coining press; modern coining presses can produce 8000 silver coins per hour.\n\nAs of July 2018, silver is valued at around $495 per kilogram, or about $15.5 per ounce.\n\nSilver prices are normally quoted in Troy ounces. One troy ounce is equal to 31.1034 grams. In 2015 China reverted to the metric system and currently prices silver (and gold) in grams. The London silver fix is published once daily at noon London time. This price is determined by several major international banks and is used by London bullion market members for trading that day. Prices are most commonly shown as the United States dollar (USD), the Pound sterling (GBP), and the Euro (EUR).\n\nThe major use of silver besides coinage throughout most of history was in the manufacture of jewellery and other general-use items, and this continues to be a major use today. Examples include table silver for cutlery, for which silver is highly suited due to its antibacterial properties. Western concert flutes are usually plated with or made out of sterling silver; in fact, most silverware is only silver-plated rather than made out of pure silver; the silver is normally put in place by electroplating. Silver-plated glass (as opposed to metal) is used for mirrors, vacuum flasks, and Christmas tree decorations.\n\nBecause pure silver is very soft, most silver used for these purposes is alloyed with copper, with finenesses of 925/1000, 835/1000, and 800/1000 being common. One drawback is the easy tarnishing of silver in the presence of hydrogen sulfide and its derivatives. Including precious metals such as palladium, platinum, and gold gives resistance to tarnishing but is quite costly; base metals like zinc, cadmium, silicon, and germanium do not totally prevent corrosion and tend to affect the lustre and colour of the alloy. Electrolytically refined pure silver plating is effective at increasing resistance to tarnishing. The usual solutions for restoring the lustre of tarnished silver are dipping baths that reduce the silver sulfide surface to metallic silver, and cleaning off the layer of tarnish with a paste; the latter approach also has the welcome side effect of polishing the silver concurrently. A simple chemical approach to removal of the sulfide tarnish is to bring silver items into contact with aluminium foil whilst immersed in water containing a conducting salt, such as sodium chloride.\n\nIn medicine, silver is incorporated into wound dressings and used as an antibiotic coating in medical devices. Wound dressings containing silver sulfadiazine or silver nanomaterials are used to treat external infections. Silver is also used in some medical applications, such as urinary catheters (where tentative evidence indicates it reduces catheter-related urinary tract infections) and in endotracheal breathing tubes (where evidence suggests it reduces ventilator-associated pneumonia). The silver ion is bioactive and in sufficient concentration readily kills bacteria \"in vitro\". They interfere with enzymes in the bacteria that transport nutrients, form structures, synthesise cell walls, and bond with the bacteria's genetic material. Microbes cannot develop resistance to silver as they can to antibiotics, and hence silver and silver nanoparticles are used as an antimicrobial in a variety of industrial, healthcare, and domestic application: for example, infusing clothing with nanosilver particles thus allows them to stay odourless for longer. Silver compounds are taken up by the body like mercury compounds, but lack the toxicity of the latter. Silver and its alloys are used in cranial surgery to replace bone, and silver–tin–mercury amalgams are used in dentistry. Silver diammine fluoride, the fluoride salt of a coordination complex with the formula [Ag(NH)]F, is a topical medicament (drug) used to treat and prevent dental caries (cavities) and relieve dentinal hypersensitivity.\n\nSilver is very important in electronics for conductors and electrodes on account of its high electrical conductivity even when tarnished. Bulk silver and silver foils were used to make vacuum tubes, and continue to be used today in the manufacture of semiconductor devices, circuits, and their components. For example, silver is used in high quality connectors for RF, VHF, and higher frequencies, particularly in tuned circuits such as cavity filters where conductors cannot be scaled by more than 6%. Printed circuits and RFID antennas are made with silver paints, Powdered silver and its alloys are used in paste preparations for conductor layers and electrodes, ceramic capacitors, and other ceramic components.\n\nSilver-containing brazing alloys are used for brazing metallic materials, mostly cobalt, nickel, and copper-based alloys, tool steels, and precious metals. The basic components are silver and copper, with other elements selected according to the specific application desired: examples include zinc, tin, cadmium, palladium, manganese, and phosphorus. Silver provides increased workability and corrosion resistance during usage.\n\nSilver is useful in the manufacture of chemical equipment on account of its low chemical reactivity, high thermal conductivity, and being easily workable. Silver crucibles (alloyed with 0.15% nickel to avoid recrystallisation of the metal at red heat) are used for carrying out alkaline fusion. Copper and silver are also used when doing chemistry with fluorine. Equipment made to work at high temperatures is often silver-plated. Silver and its alloys with gold are used as wire or ring seals for oxygen compressors and vacuum equipment.\n\nSilver metal is a good catalyst for oxidation reactions; in fact it is somewhat too good for most purposes, as finely divided silver tends to result in complete oxidation of organic substances to carbon dioxide and water, and hence coarser-grained silver tends to be used instead. For instance, 15% silver supported on α-AlO or silicates is a catalyst for the oxidation of ethylene to ethylene oxide at 230–270 °C. Dehydrogenation of methanol to formaldehyde is conducted at 600–720 °C over silver gauze or crystals as the catalyst, as is dehydrogenation of isopropanol to acetone. In the gas phase, glycol yields glyoxal and ethanol yields acetaldehyde, while organic amines are dehydrated to nitriles.\n\nThe photosensitivity of the silver halides allowed for their use in traditional photography, although digital photography, which does not use silver, is now dominant. The photosensitive emulsion used in black-and-white photography is a suspension of silver halide crystals in gelatin, possibly mixed in with some noble metal compounds for improved photosensitivity, developing, and tuning. Colour photography requires the addition of special dye components and sensitisers, so that the initial black-and-white silver image couples with a different dye component. The original silver images are bleached off and the silver is then recovered and recycled. Silver nitrate is the starting material in all cases.\n\nThe use of silver nitrate and silver halides in photography has rapidly declined with the advent of digital technology. From the peak global demand for photographic silver in 1999 (267,000,000 troy ounces or 8304.6 metric tonnes) the market contracted almost 70% by 2013.\n\nNanosilver particles, between 10 and 100 nanometres in size, are used in many applications. They are used in conductive inks for printed electronics, and have a much lower melting point than larger silver particles of micrometre size. They are also used medicinally in antibacterials and antifungals in much the same way as larger silver particles. In addition, according to the European Union Observatory for Nanomaterials (EUON), silver nanoparticles are used both in pigments, as well as cosmetics. \n\nPure silver metal is used as a food colouring. It has the E174 designation and is approved in the European Union. Traditional Pakistani and Indian dishes sometimes include decorative silver foil known as \"vark\", and in various other cultures, silver \"dragée\" are used to decorate cakes, cookies, and other dessert items.\n\nPhotochromic lenses include silver halides, so that ultraviolet light in natural daylight liberates metallic silver, darkening the lenses. The silver halides are reformed in lower light intensities. Colourless silver chloride films are used in radiation detectors. Zeolite sieves incorporating Ag ions are used to desalinate seawater during rescues, using silver ions to precipitate chloride as silver chloride. Silver is also used for its antibacterial properties for water sanitisation, but the application of this is limited by limits on silver consumption. Colloidal silver is similarly used to disinfect closed swimming pools; while it has the advantage of not giving off a smell like hypochlorite treatments do, colloidal silver is not effective enough for more contaminated open swimming pools. Small silver iodide crystals are used in cloud seeding to cause rain.\n\nSilver compounds have low toxicity compared to those of most other heavy metals, as they are poorly absorbed by the human body when digested, and that which does get absorbed is rapidly converted to insoluble silver compounds or complexed by metallothionein. However, silver fluoride and silver nitrate are caustic and can cause tissue damage, resulting in gastroenteritis, diarrhoea, falling blood pressure, cramps, paralysis, and respiratory arrest. Animals repeatedly dosed with silver salts have been observed to experience anaemia, slowed growth, necrosis of the liver, and fatty degeneration of the liver and kidneys; rats implanted with silver foil or injected with colloidal silver have been observed to develop localised tumours. Parenterally admistered colloidal silver causes acute silver poisoning. Some waterborne species are particularly sensitive to silver salts and those of the other precious metals; in most situations, however, silver does not pose serious environmental hazards.\n\nIn large doses, silver and compounds containing it can be absorbed into the circulatory system and become deposited in various body tissues, leading to argyria, which results in a blue-grayish pigmentation of the skin, eyes, and mucous membranes. Argyria is rare, and so far as is known, does not otherwise harm a person's health, though it is disfiguring and usually permanent. Mild forms of argyria are sometimes mistaken for cyanosis.\n\nMetallic silver, like copper, is an antibacterial agent, which was known to the ancients and first scientifically investigated and named the oligodynamic effect by Carl Nägeli. Silver ions damage the metabolism of bacteria even at such low concentrations as 0.01–0.1 milligrams per litre; metallic silver has a similar effect due to the formation of silver oxide. This effect is lost in the presence of sulfur due to the extreme insolubility of silver sulfide.\n\nSome silver compounds are very explosive, such as the nitrogen compounds silver azide, silver amide, and silver fulminate, as well as silver acetylide, silver oxalate, and silver(II) oxide. They can explode on heating, force, drying, illumination, or sometimes spontaneously. To avoid the formation of such compounds, ammonia and acetylene should be kept away from silver equipment. Salts of silver with strongly oxidising acids such as silver chlorate and silver nitrate can explode on contact with materials that can be readily oxidised, such as organic compounds, sulfur and soot.\n\n\n\n"}
{"id": "24898017", "url": "https://en.wikipedia.org/wiki?curid=24898017", "title": "Solar Settlement at Schlierberg", "text": "Solar Settlement at Schlierberg\n\nThe Solar Settlement at Schlierberg () is a 59-home PlusEnergy housing community in Freiburg, Germany. Solar architect Rolf Disch wanted to apply his PlusEnergy concept, created originally with his Heliotrope home, to mass residential production. This residential complex won such awards as House of the Year (2002), Residential PV solar integration award (2002), Germany’s most beautiful housing community (2006). It is the first housing community in the world in which all the homes produce a positive energy balance and which is emissions-free and neutral. Built between 2000 and 2005 in the Vauban quarter of Freiburg the design of each home offers maximum energy efficiency and turns energy bills into energy income. The Solar Settlement has proved Disch’s vision of a “fundamental environmental imperative” as these homes have exhibited more than 8 years of full occupancy and each produced more than 4,000 Euros ($5,600) each year in solar energy profits.\nDisch believes the image his community exhibits is equally as important as the eco-savings it brings. Disch says 50 homes that eliminate energy bills and feed clean sustainable renewable energy into the city’s grid is the imperative we need. They contribute towards the goal of sustainable ecological and economic development and show the entire world that communities like this are economical, beautiful, comfortable and in fact preferable. The tenants at the Solar Settlement do not claim to have made any compromises in their living situations but rather that they have both environmentally and economically improved. Made from Black Forest timber, the wood interior and natural lighting provide for happily lit spaces and a natural flow from room to room.\n\nPlusEnergy is a coined concept developed by Rolf Disch that indicates a structure’s extreme energy efficiency so that it holds a positive energy balance, actually producing more energy than it uses. With the completion of his private residence, the Heliotrope, in 1994, Disch had created the first PlusEnergy house in the world. The sheer logic of a home that creates more energy than it consumes made perfect sense to Disch. His next goal in its development was thus the mass application of the concept to residential, commercial and retail space. As the concept further developed and gained financial backing as well, Disch built several more projects with PlusEnergy certifications. PlusEnergy is a simple concept that has been materialized in a technical design. “PlusEnergy is a fundamental environmental imperative,” Disch claims. Disch believes that passive building isn’t enough because passive homes still emit into the atmosphere – we can solar activate our homes!\n\nCommunity planning goes hand in hand with sustainable development and thus Rolf Disch dedicates much of his design and planning to the symbiosis of his projects' surroundings. A solar community generates identification and an enormous public image. Disch says he attracts great tenants, innovative undertakings and creative work places through his design. \nDisch always aims to create a community with combined uses for an ecological urban planning concept like traffic management – with wide, attractive walkways, bike routes and connections to public transportation. At the Solar Settlement for example, tenants and owners incorporate bike- and carsharing and the neighborhood has an extensive car-free zone with many public transportation connections. Disch also has incorporated community solutions for energy. For example, a biomass cogeneration unit is extremely expedient, which meets the (reduced) additional heating demands for his PlusEnergy homes.\n\n\n"}
{"id": "14169590", "url": "https://en.wikipedia.org/wiki?curid=14169590", "title": "Super-LumiNova", "text": "Super-LumiNova\n\nSuper-LumiNova is a brand name under which strontium aluminate–based non-radioactive and nontoxic photoluminescent or afterglow pigments for illuminating markings on watch dials, hands and bezels, etc. in the dark are marketed. This technology offers up to ten times higher brightness than previous zinc sulfide–based materials.\n\nSuper-LumiNova is based on LumiNova pigments, invented in 1993 by Nemoto & Co., Ltd. of Japan as a safe replacement for radium-based luminous paints. Nemoto & Co. was founded in December 1941 as a luminous paint processing company and has supplied paint to the watch and clock industry for over 70 years.\n\nBesides being used in timepieces, Super-LumiNova is also marketed for application on:\n\n\nThese types of phosphorescent pigments, often called lume, operate like a light battery. After sufficient activation by sunlight or artificial light, they glow in the dark for hours. Larger markings are visible for the whole night. This activation and subsequent light emission process can be repeated again and again, and the material does not suffer any practical aging. Strontium aluminate–based pigments have to be protected against contact with water or moisture, since this degrades the light emitting quality.\n\nTritium-based devices called \"gaseous tritium light source\" (GTLS), that are an alternative for afterglow pigments, have the advantage of being self-powered and producing a consistent luminosity that does not fade during the night. However, they are radioactive and have a half-life of slightly over 12 years. This means the intensity of the tritium-powered light source will gradually fade, generally becoming too dim to be useful after 20 to 30 years.\n\n\n"}
{"id": "144755", "url": "https://en.wikipedia.org/wiki?curid=144755", "title": "Tactical High Energy Laser", "text": "Tactical High Energy Laser\n\nThe Tactical High-Energy Laser, or THEL, was a laser developed for military use, also known as the Nautilus laser system. The mobile version is the Mobile Tactical High-Energy Laser, or MTHEL. In 1996, the United States and Israel entered into an agreement to produce a cooperative THEL called the Demonstrator, which would utilize deuterium fluoride chemical laser technologies. In 2000 and 2001 THEL shot down 28 Katyusha artillery rockets and five artillery shells. On November 4, 2002, THEL shot down an incoming artillery shell. The prototype weapon was roughly the size of six city buses, made up of modules that held a command center, radar and a telescope for tracking targets, the chemical laser itself, fuel and reagent tanks, and a rotating mirror to reflect its beam toward speeding targets. It was discontinued in 2005.\n\nOn July 18, 1996, the United States and Israel entered into an agreement to produce a cooperative THEL, called the Advanced Concept Technology Demonstrator, which would utilize deuterium fluoride chemical laser technologies. Primary among the four contractors awarded the project on September 30, 1996 was Northrop Grumman (formerly TRW). THEL conducted test firing in FY1998, and Initial Operating Capability (IOC) was planned in FY1999. However, this was significantly delayed due to reorienting the project as a mobile, not fixed, design, called Mobile Tactical High Energy Laser (MTHEL). The original fixed location design eliminates most weight, size and power restrictions, but is not compatible with the fluid, mobile nature of modern combat. The initial MTHEL goal was a mobile version the size of three large semi trailers. Ideally it would be further downsized to a single semi trailer size. However, doing this while maintaining the original performance characteristics is difficult. Furthermore, the Israeli government, which had been providing significant funding, decreased their financial support in 2004, postponing the IOC date to at least 2010.\n\nIn 2000 and 2001 THEL shot down 28 Katyusha artillery rockets and five artillery shells.\n\nOn November 4, 2002, THEL shot down an incoming artillery shell. A mobile version completed successful testing. During a test conducted on August 24, 2004 the system successfully shot down multiple mortar rounds. The test represented actual mortar threat scenarios. Targets were intercepted by the THEL testbed and destroyed. Both single mortar rounds and salvo were tested. Many military experts, such as the former head of the Administration for the Development of Weapons and the Technological Industry, Aluf Yitzhak Ben Yisrael, considered THEL to be a success and called for its implementation. However, in 2005, the US and Israel decided to discontinue developing the THEL after the project budget had surpassed $300 million. The decision came as a result of \"its bulkiness, high costs and poor anticipated results on the battlefield.\" During the 2006 Lebanon War, Ben Yisrael, currently the chairman of the Israeli Space Agency, renewed his calls to implement the THEL against high-trajectory fire.\n\nIn 2007, Ehud Barak requested to reconsider project Skyguard (the next phase of THEL) in order to fight Qassam attacks.\n\n\n"}
{"id": "8871017", "url": "https://en.wikipedia.org/wiki?curid=8871017", "title": "Ted Smith (environmentalist)", "text": "Ted Smith (environmentalist)\n\nTed Smith (born July 15, 1945) is the founder and former executive director of the Silicon Valley Toxics Coalition, co-founder of the International Campaign for Responsible Technology, and chair of the Electronics TakeBack Coalition steering committee.\n\nSmith is a former VISTA Volunteer, a 1967 graduate of Wesleyan University, and a 1972 graduate of Stanford Law School.\n\nIn 2001, Smith was recognized for his environmental leadership and in 2006 co-edited the book, \"Challenging the Chip: Labor Rights and Environmental Justice in the Global Electronics Industry\".\nhttp://www.temple.edu/tempress/authors/1788_qa.html\n\n"}
{"id": "53078620", "url": "https://en.wikipedia.org/wiki?curid=53078620", "title": "The Guns and the Fury", "text": "The Guns and the Fury\n\nThe Guns and the Fury is a 1981 film starring Peter Graves and Cameron Mitchell.\n\nAfter the turn of the century, two fortune hunters discover oil in Persia, when they try to claim it serious resistance stands in their way.\n\n\n"}
{"id": "67046", "url": "https://en.wikipedia.org/wiki?curid=67046", "title": "Thermal mass", "text": "Thermal mass\n\nIn building design, thermal mass is a property of the mass of a building which enables it to store heat, providing \"inertia\" against temperature fluctuations. It is sometimes known as the thermal flywheel effect. For example, when outside temperatures are fluctuating throughout the day, a large thermal mass within the insulated portion of a house can serve to \"flatten out\" the daily temperature fluctuations, since the thermal mass will absorb thermal energy when the surroundings are higher in temperature than the mass, and give thermal energy back when the surroundings are cooler, without reaching thermal equilibrium. This is distinct from a material's insulative value, which reduces a building's thermal conductivity, allowing it to be heated or cooled relatively separate from the outside, or even just retain the occupants' thermal energy longer.\n\nScientifically, thermal mass is equivalent to thermal capacitance or heat capacity, the ability of a body to store thermal energy. It is typically referred to by the symbol \"C\" and measured in units of J/°C or J/K (which are equivalent). Thermal mass may also be used for bodies of water, machines or machine parts, living things, or any other structure or body in engineering or biology. In those contexts, the term \"heat capacity\" is typically used instead.\n\nThe equation relating thermal energy to thermal mass is:\nwhere \"Q\" is the thermal energy transferred, \"C\" is the thermal mass of the body, and Δ\"T\" is the change in temperature.\n\nFor example, if 250 J of heat energy is added to a copper gear with a thermal mass of 38.46 J/°C, its temperature will rise by 6.50 °C.\nIf the body consists of a homogeneous material with sufficiently known physical properties, the thermal mass is simply the mass of material present times the specific heat capacity of that material. For bodies made of many materials, the sum of heat capacities for their pure components may be used in the calculation, or in some cases (as for a whole animal, for example) the number may simply be measured for the entire body in question, directly.\n\nAs an extensive property, heat capacity is characteristic of an object; its corresponding intensive property is specific heat capacity, expressed in terms of a measure of the amount of material such as mass or number of moles, which must be multiplied by similar units to give the heat capacity of the entire body of material. Thus the heat capacity can be equivalently calculated as the product of the mass \"m\" of the body and the specific heat capacity \"c\" for the material, or the product of the number of moles of molecules present \"n\" and the molar specific heat capacity formula_2. For discussion of \"why\" the thermal energy storage abilities of pure substances vary, see factors that affect specific heat capacity.\n\nFor a body of uniform composition, formula_3 can be approximated by\nwhere formula_5 is the mass of the body and formula_6 is the isobaric specific heat capacity of the material averaged over temperature range in question. For bodies composed of numerous different materials, the thermal masses for the different components can just be added together.\n\nThermal mass is effective in improving building comfort in any place that experiences these types of daily temperature fluctuations—both in winter as well as in summer.\nWhen used well and combined with passive solar design, thermal mass can play an important role in major reductions to energy use in active heating and cooling systems.\nThe terms \"heavy-weight\" and \"light-weight\" are often used to describe buildings with different thermal mass strategies, and affects the choice of numerical factors used in subsequent calculations to describe their thermal response to heating and cooling.\nIn building services engineering, the use of dynamic simulation computational modelling software has allowed for the accurate calculation of the environmental performance within buildings with different constructions and for different annual climate data sets. This allows the architect or engineer to explore in detail the relationship between heavy-weight and light-weight constructions, as well as insulation levels, in reducing energy consumption for mechanical heating or cooling systems, or even removing the need for such systems altogether.\n\nIdeal materials for thermal mass are those materials that have:\nAny solid, liquid, or gas that has mass will have some thermal mass. A common misconception is that only concrete or earth soil has thermal mass; even air has thermal mass (although very little).\n\nA table of volumetric heat capacity for building materials is available, but note that their definition of thermal mass is slightly different.\n\nThe correct use and application of thermal mass is dependent on the prevailing climate in a district.\n\nThermal mass is ideally placed within the building and situated where it still can be exposed to low-angle winter sunlight (via windows) but insulated from heat loss. In summer the same thermal mass should be obscured from higher-angle summer sunlight in order to prevent overheating of the structure.\n\nThe thermal mass is warmed passively by the sun or additionally by internal heating systems during the day. Thermal energy stored in the mass is then released back into the interior during the night. It is essential that it be used in conjunction with the standard principles of passive solar design.\n\nAny form of thermal mass can be used. A concrete slab foundation either left exposed or covered with conductive materials, e.g. tiles, is one easy solution. Another novel method is to place the masonry facade of a timber-framed house on the inside ('reverse-brick veneer'). Thermal mass in this situation is best applied over a large area rather than in large volumes or thicknesses. 7.5–10 cm (3-4\") is often adequate.\n\nSince the most important source of thermal energy is the Sun, the ratio of glazing to thermal mass is an important factor to consider. Various formulas have been devised to determine this. As a general rule, additional solar-exposed thermal mass needs to be applied in a ratio from 6:1 to 8:1 for any area of sun-facing (north-facing in Southern Hemisphere or south-facing in Northern Hemisphere) glazing above 7% of the total floor area. For example, a 200 m house with 20 m of sun-facing glazing has 10% of glazing by total floor area; 6 m of that glazing will require additional thermal mass. Therefore, using the 6:1 to 8:1 ratio above, an additional 36–48 m of solar-exposed thermal mass is required. The exact requirements vary from climate to climate.\n\nThermal mass is ideally placed within a building where it is shielded from direct solar gain but exposed to the building occupants. It is therefore most commonly associated with solid concrete floor slabs in naturally ventilated or low-energy mechanically ventilated buildings where the concrete soffit is left exposed to the occupied space.\n\nDuring the day heat is gained from the sun, the occupants of the building, and any electrical lighting and equipment, causing the air temperatures within the space to increase, but this heat is absorbed by the exposed concrete slab above, thus limiting the temperature rise within the space to be within acceptable levels for human thermal comfort. In addition the lower surface temperature of the concrete slab also absorbs radiant heat directly from the occupants, also benefiting their thermal comfort.\n\nBy the end of the day the slab has in turn warmed up, and now, as external temperatures decrease, the heat can be released and the slab cooled down, ready for the start of the next day. However this \"regeneration\" process is only effective if the building ventilation system is operated at night to carry away the heat from the slab. In naturally ventilated buildings it is normal to provide automated window openings to facilitate this process automatically.\n\nThis is a classical use of thermal mass. Examples include adobe or rammed earth houses. Its function is highly dependent on marked diurnal temperature variations. The wall predominantly acts to retard heat transfer from the exterior to the interior during the day. The high volumetric heat capacity and thickness prevents thermal energy from reaching the inner surface. When temperatures fall at night, the walls re-radiate the thermal energy back into the night sky. In this application it is important for such walls to be massive to prevent heat transfer into the interior.\n\nThe use of thermal mass is the most challenging in this environment where night temperatures remain elevated. Its use is primarily as a temporary heat sink. However, it needs to be strategically located to prevent overheating. It should be placed in an area that is not directly exposed to solar gain and also allows adequate ventilation at night to carry away stored energy without increasing internal temperatures any further. If to be used at all it should be used in judicious amounts and again not in large thicknesses.\n\n\nIf enough mass is used it can create a seasonal advantage. That is, it can heat in the winter and cool in the summer. This is sometimes called passive annual heat storage or PAHS. The PAHS system has been successfully used at 7000 ft. in Colorado and in a number of homes in Montana. The Earthships of New Mexico utilize passive heating and cooling as well as using recycled tires for foundation wall yielding a maximum PAHS/STES. It has also been used successfully in the UK at Hockerton Housing Project.\n\n\n"}
{"id": "24691507", "url": "https://en.wikipedia.org/wiki?curid=24691507", "title": "Tocoma Dam", "text": "Tocoma Dam\n\nThe Manuel Piar Hydroelectric Power Plant (Tocoma Dam) is under construction and it is the last hydroelectric development project in the Lower Caroní River Basin of Venezuela. The project includes the installation of MW to generate an annual average energy of . Ten Kaplan generator units, of , manufactured by Argentinian company IMPSA, were predicted to begin operations between 2012 and 2014. The first generator was installed but not yet commissioned in April 2012. Behind scheduled, the dam began to impound its reservoir on 16 November 2015. These units had the world record as of 2012 in power generation at nominal head for Kaplan turbines. The diameter of the runner is and nominal head is with claimed output up to .\n"}
{"id": "42340183", "url": "https://en.wikipedia.org/wiki?curid=42340183", "title": "WeWOOD", "text": "WeWOOD\n\nWeWOOD is a watch brand that manufacturers wooden watches. The brand was developed in 2009 in Italy and its trademark was registered in 2010 under the company name Fratelli Diversi. The headquarters of the company is located in Lamporecchio and a North American regional office is located in Los Angeles.\n\nAccording to WeWOOD, the company plants a tree for every watch sold. As of December 2013, the company has planted 250,000 trees in America and other countries where the watch is sold. WeWOOD has partnered with American Forests, Trees for the Future and Treedom for planting trees.\n\nThe brand was developed in 2009 in Italy. Its trademark was registered by Alessandro Rosano, Daniele Guidi and Emma Bogren in 2010 under the company name Fratelli Diversi. Prior to starting the company Guidi was an accessories distributor and Rosano was a shoe designer. After a year of operations, the company opened a regional office in the United States in Fall 2011.\n\nThe designs of the watches are prepared in Italy and the watches are manufactured in Indonesia and China. The company has offices in Los Angeles and Lamporecchio. WeWOOD watches were launched in the United Kingdom in March 2011. WeWOOD in the UK is distributed by Winshport LTD based in Poole, Dorset. WeWOOD in Australia/Pacific region is distributed by Branched, located in Melbourne Victoria. WeWOOD in Brazil is distributed by Grupo Zarco®, located in Rio de Janeiro.\n\nWeWOOD manufacturers watches completely from wood with the exception of the glass, movement, battery, pins and clasp. The watches use Japanese Miyota movement pieces. The company uses different woods to derive different colors for the watches and does not use dyes to alter the color of the wood. WeWOOD also manufactures multi-wood watches. The wood used in watches is hypo-allergenic and is not treated with chemicals. Therefore, the watches are not waterproof.\n\nThe company uses recycled wood for manufacturing watches. Initially the company sold two designs, Date (analog) and Crono (digital). However, WeWOOD started manufacturing several other designs later on.\n\nWeWOOD promises to plant one tree for each watch sold. For this purpose, WeWOOD works with American Forests to plant trees in North America and Trees for the Future to plant trees in other countries. By the end of 2013, the company had planted 250,000 trees. Of these 50,000 trees were planted in Ghana, 7,500 in Texas and 5,000 in the Bitterroot National Forest.\n\nWeWOOD provides a quarterly financial statement to American Forests to document the number of trees planted.\n\nSince its launch WeWOOD has been featured in multiple national and international media outlets. ELLE wrote about WeWOOD watches, \"Completely absent of artificial and toxic materials, these timber timepieces are as natural as your wrist – the perfect eco arm candy!\" and Vogue Italy wrote \"Technically, no one can ever tell you that you have his same watch. Every wood, in fact, has a unique grain pattern as the DNA of a person.\"\n\nIn 2011, WeWOOD's watch, \"Jupiter\" was featured in \"Three accessories for eco-friendly travel\" in The Globe and Mail. WeWOOD watches were also featured in OK! Magazine's 2012 Christmas Gift Guide, in New York Post's \"50 great gifts (for everyone on your list!)\" and in \"6 Watches that Will Make You Want to Wear One\" in Fox News Magazine.\n"}
{"id": "51971", "url": "https://en.wikipedia.org/wiki?curid=51971", "title": "Yarn", "text": "Yarn\n\nYarn is a long continuous length of interlocked fibres, suitable for use in the production of textiles, sewing, crocheting, knitting, weaving, embroidery, or ropemaking. Thread is a type of yarn intended for sewing by hand or machine. Modern manufactured sewing threads may be finished with wax or other lubricants to withstand the stresses involved in sewing. Embroidery threads are yarns specifically designed for needlework.\n\nThe word yarn comes from Middle English, from the Old English \"gearn\", akin to Old High German's \"garn\" yarn, Greek's \"chordē\" string, and Sanskrit's \"hira\" band.\nYarn can be made from a number of natural or synthetic fibers. Many types of yarn are made differently though. There are two main types of yarn: spun and filament.\n\nThe most common plant fiber is cotton, which is typically spun into fine yarn for mechanical weaving or knitting into cloth.\n\nCotton and polyester are the most commonly spun fibers in the world. Cotton is grown throughout the world. After harvesting it is ginned and prepared for yarn spinning. Polyester is extruded from polymers derived from natural gas and oil. Synthetic fibers are generally extruded in continuous strands of gel-state materials. These strands are drawn (stretched), annealed (hardened), and cured to obtain properties desirable for later processing.\n\nSynthetic fibers come in three basic forms: staple, tow, and filament. Staple is cut fibers, generally sold in lengths up to 120mm. Tow is a continuous \"rope\" of fibers consisting of many filaments loosely joined side-to-side. Filament is a continuous strand consisting of anything from 1 filament to many. Synthetic fiber is most often measured in a weight per linear measurement basis, along with cut length. Denier and Dtex are the most common weight to length measures. Cut-length only applies to staple fiber.\n\nFilament extrusion is sometimes referred to as \"spinning\" but most people equate spinning with spun yarn production. \nThe most commonly spun animal fiber is wool harvested from sheep. For hand knitting and hobby knitting, thick, wool and acrylic yarns are frequently used.\n\nOther animal fibers used include alpaca, angora, mohair, llama, cashmere, and silk. More rarely, yarn may be spun from camel, yak, possum, musk ox, cat, dog, wolf, rabbit, or buffalo hair, and even turkey or ostrich feathers. Natural fibers such as these have the advantage of being slightly elastic and very breathable, while trapping a great deal of air, making for a fairly warm fabric.\n\nOther natural fibers that can be used for yarn include linen and cotton. These tend to be much less elastic, and retain less warmth than the animal-hair yarns, though they can be stronger in some cases. The finished product will also look rather different from the woollen yarns. Other plant fibers which can be spun include bamboo, hemp, corn, nettle, and soy fiber.\n\nIn general, natural fibers tend to require more careful handling than synthetics because they can shrink, felt, stain, shed, fade, stretch, wrinkle, or be eaten by moths more readily, unless special treatments such as mercerization or superwashing are performed to strengthen, fix color, or otherwise enhance the fiber's own properties.\n\nProtein yarns (i.e., hair, silk, feathers) may also be irritating to some people, causing contact dermatitis, hives, wheezing, or other reactions. Plant fibers tend to be better tolerated by people with sensitivities to the protein yarns, and allergists may suggest using them or synthetics instead to prevent symptoms. Some people find that they can tolerate organically grown and processed versions of protein fibers, possibly because organic processing standards preclude the use of chemicals that may irritate the skin.\n\nWhen natural hair-type fibers are burned, they tend to singe and have a smell of burnt hair; this is because many, as human hair, are protein-derived. Cotton and viscose (rayon) yarns burn as a wick. Synthetic yarns generally tend to melt though some synthetics are inherently flame-retardant. Noting how an unidentified fiber strand burns and smells can assist in determining if it is natural or synthetic, and what the fiber content is.\n\nBoth synthetic and natural yarns can pill. Pilling is a function of fiber content, spinning method, twist, contiguous staple length, and fabric construction. Single ply yarns or using fibers like merino wool are known to pill more due to the fact that in the former, the single ply is not tight enough to securely retain all the fibers under abrasion, and the merino wool's short staple length allows the ends of the fibers to pop out of the twist more easily.\n\nYarns combining synthetic and natural fibers inherit the properties of each parent, according to the proportional composition. Synthetics are added to lower cost, increase durability, add unusual color or visual effects, provide machine washability and stain resistance, reduce heat retention or lighten garment weight.\n\nSpun yarn is made by twisting staple fibres together to make a cohesive thread, or \"single.\" Twisting fibres into yarn in the process called spinning can be dated back to the Upper Paleolithic, and yarn spinning was one of the very first processes to be industrialized. Spun yarns may contain a single type of fibre, or be a blend of various types. Combining synthetic fibres (which can have high strength, lustre, and fire retardant qualities) with natural fibres (which have good water absorbency and skin comforting qualities) is very common. The most widely used blends are cotton-polyester and wool-acrylic fibre blends. Blends of different natural fibres are common too, especially with more expensive fibres such as alpaca, angora and cashmere.\n\nYarn is selected for different textiles based on the characteristics of the yarn fibres, such as warmth (wool), light weight (cotton or rayon), durability (nylon is added to sock yarn, for example), or softness (cashmere, alpaca).\n\nYarn is composed of twisted strands of fiber, which are known as plies when grouped together. These strands of yarn are twisted together (plied) in the opposite direction to make a thicker yarn. Depending on the direction of this final twist, the yarn will have either \"s‑twist\" (the threads appear to go \"up\" to the left) or \"z‑twist\" (to the right). For a single ply yarn, the direction of the final twist is the same as its original twist. The twist direction of yarn can affect the final properties of the fabric, and combined use of the two twist directions can nullify skewing in knitted fabric.\n\nThe mechanical integrity of yarn is derived from frictional contacts between its composing fibers. The science behind this was first studied by Galileo.\n\nFilament yarn consists of filament fibres (very long continuous fibres) either twisted together or only grouped together. Thicker monofilaments are typically used for industrial purposes rather than fabric production or decoration. Silk is a natural filament, and synthetic filament yarns are used to produce silk-like effects.\n\nTexturized yarns are made by a process of air texturizing filament yarns (sometimes referred to as \"taslanizing\"), which combines multiple filament yarns into a yarn with some of the characteristics of spun yarns.\nSlub Effect means a yarn with thick and thin sections alternating regularly or irregularly.It was invented by Mr. Rizwanul Karim from Bangladesh.\n\nYarn may be used undyed, or may be coloured with natural or artificial dyes. Most yarns have a single uniform hue, but there is also a wide selection of variegated yarns:\n\n\nYarn quantities for handcrafts are usually measured and sold by weight in ounces or grams. Common sizes include 25 g, 50 g, and 100 g skeins. Some companies also primarily measure in ounces with common sizes being three-ounce, four-ounce, six-ounce, and eight-ounce skeins. Textile measurements are taken at a standard temperature and humidity, because fibers can absorb moisture from the air. The actual length of the yarn contained in a ball or skein can vary due to the inherent heaviness of the fibre and the thickness of the strand; for instance, a 50 g skein of lace weight mohair may contain several hundred metres, while a 50 g skein of bulky wool may contain only 60 metres.\n\nThere are several thicknesses of craft yarn, also referred to as weight. This is not to be confused with the measurement and/or weight listed above. The Craft Yarn Council of America is making an effort to promote a standardized industry system for measuring this, numbering the weights from 1 (finest) to 6 (heaviest). Some of the names for the various weights of yarn from finest to thickest are called lace, fingering, sport, double-knit (or DK), worsted, aran (or heavy worsted), bulky, and super-bulky. This naming convention is more descriptive than precise; fibre artists disagree about where on the continuum each lies, and the precise relationships between the sizes.\n\nAnother measurement of yarn weight, often used by weavers, is wraps per inch (WPI). The yarn is wrapped snugly around a ruler and the number of wraps that fit in an inch are counted.\n\nLabels on yarn for handicrafts often include information on gauge, known in the UK as tension, which is a measurement of how many stitches and rows are produced per inch or per cm on a specified size of knitting needle or crochet hook. The proposed standardization uses a four-by-four inch/ten-by-ten cm knitted or crocheted square, with the resultant number of stitches across and rows high made by the suggested tools on the label to determine the gauge.\n\nIn Europe, textile engineers often use the unit tex, which is the weight in grams of a kilometre of yarn, or decitex, which is a finer measurement corresponding to the weight in grams of 10 km of yarn. Many other units have been used over time by different industries.\n\nBelow are the images taken by a digital USB microscope. These show how the yarn looks in different kinds of clothes when magnified.\n\n"}
