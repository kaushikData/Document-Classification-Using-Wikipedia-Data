{"id": "58927278", "url": "https://en.wikipedia.org/wiki?curid=58927278", "title": "10 Billion - What's on your plate?", "text": "10 Billion - What's on your plate?\n\n10 Billion - What's on your plate? (Original german title: 10 Milliarden – Wie werden wir alle satt?) is a documentary film released in 2015 by Valentin Thurn about solutions to supply the future world population with food. It was the most viewed documentary film in 2015 in German cinemas. \n\nIn 2050 the Earth's population will most probably reach approximately 10 billion people. To look for an answer to the question of whether it is possible to supply food for the whole world population, Valentin Thurn travels to different spots in the world in search of ecologically and economically responsible alternatives to the mass means by which most of our food is currently produced.\n\nHe visits a seed farm in India, an insect farm in Thailand, food cooperatives in the United States, Germany and the United Kingdom, both small and large farming projects in Africa and industrial food producers in Japan.\nHe discovers the complexity of producing and transporting food and its dependence on vulnerable markets.\n\nSmall farmers, especially in newly industrialized countries and developing countries, stand in competition with the industrial food production. The message that arises from all these interactions is about the huge difference that small-scale agriculture and eating locally produced food as often as possible can make. According to Thurn, these are the only viable options in resolving the problems of the future.\n\nAcademic Brandon West praised the film, mentioning that while it \"could have easily used scare tactics to pique viewer interest, [...] Thurn presents an interesting and well-balanced perspective\". He goes on to recommend it for academic purposes, claiming that the \"narration of the events and concepts provide ample details, while respecting the intelligence of the audience.\" Writing for Glam Adelaide, Jo Vabolis gave the film a 10/10 rating, while describing it as \"equal parts enthralling and confronting\" and \"essential viewing\" for all those being kept awake at night by the ramifications of population growth.\n\nThe film has been used as academic material, with Planet in Focus even releasing a teacher's guide that provides \"a brief social and scientific background on the issues covered in the film\".\n\n\n\n"}
{"id": "33791505", "url": "https://en.wikipedia.org/wiki?curid=33791505", "title": "Agoniates", "text": "Agoniates\n\nAgoniates is genus of characiform fishes from tropical South America.\n\nThere are currently two recognized species in this genus:\n"}
{"id": "187701", "url": "https://en.wikipedia.org/wiki?curid=187701", "title": "Amnion", "text": "Amnion\n\nThe amnion is a membrane that closely covers the embryo when first formed. It fills with the amniotic fluid which causes the amnion to expand and become the amniotic sac which serves to provide a protective environment for the developing embryo or fetus. The amnion, along with the chorion, the yolk sac and the allantois form a protective sac around the embryo. In birds, reptiles and monotremes, the protective sac is enclosed in a shell. In marsupials and placental mammals, it is enclosed in a uterus.\n\nThe term amnion is from Ancient Greek ἀμνίον 'little lamb', diminutive of ἀμνός 'lamb'. it is cognate with the English verb 'yean', bring forth young (usually lambs).\n\nThe amnion is a feature of the vertebrate clade Amniota, which includes reptiles, birds, and mammals. Amphibians and fish are not amniotes and thus lack the amnion. The amnion stems from the extra-embryonic somatic mesoderm on the outer side and the extra-embryonic ectoderm or trophoblast on the inner side.\n\nIn the human embryo, the earliest stages of the formation of the amnion have not been observed; in the youngest embryo which has been studied the amnion was already present as a closed sac, and appears in the inner cell-mass as a cavity. This cavity is roofed in by a single stratum of flattened, ectodermal cells, the amniotic ectoderm, and its floor consists of the prismatic ectoderm of the embryonic disk—the continuity between the roof and floor being established at the margin of the embryonic disk. Outside the amniotic ectoderm is a thin layer of mesoderm, which is continuous with that of the somatopleure and is connected by the body-stalk with the mesodermal lining of the chorion.\n\nWhen first formed, the amnion is in contact with the body of the embryo, but about the fourth or fifth week amniotic fluid (also called \"liquor amnii\") begins to accumulate within it. This fluid increases in quantity and causes the amnion to expand and ultimately to adhere to the inner surface of the chorion, so that the extra-embryonic part of the coelom is obliterated. The amniotic fluid increases in quantity up to the sixth or seventh month of pregnancy, after which it diminishes somewhat; at the end of pregnancy it amounts to about 1 liter.\n\nThe amniotic fluid allows the free movements of the fetus during the later stages of pregnancy, and also protects it by diminishing the risk of injury from without. It contains less than two percent solids, consisting of urea and other extractives, inorganic salts, a small amount of protein, and frequently a trace of sugar. That some of the liquor amnii is swallowed by the fetus is proved by the fact that epidermal debris and hairs have been found among the contents of the fetal alimentary canal.\n\nExtra-amniotic pregnancy is a rare condition that result from a rupture of the amnion, leading to development of the fetus within the extraembryonic coelom.\n\nIn reptiles, birds, and many mammals the amnion is developed in the following manner:\n\nAt the point of constriction where the primitive digestive tube of the embryo joins the yolk sac a reflection or folding upward of the somatopleure takes place.\n\nThis, the amniotic fold, first makes its appearance at the cephalic extremity, and subsequently at the caudal end and sides of the embryo, and gradually rising more and more, its different parts meet and fuse over the dorsal aspect of the embryo, and enclose a cavity, the amniotic cavity. This kind of amnion is known as pleuroamnion (formed by folding), as opposed to schyzoamnion (formed by delamination).\n\nAfter the fusion of the edges of the amniotic fold, the two layers of the fold become completely separated, the inner forming the amnion, the outer the false amnion or serosa.\n\nThe space between the amnion and the serosa constitutes the extra-embryonic celom, and for a time communicates with the embryonic celom.\n\nCats and dogs are born inside of the amnion; it is cut open by the mother and eaten.\n\nIn elephants, \"The amnios is continued from the base of the umbilical cord upon the allantois, which is of considerable size, and is so interposed between the chorion and amnios, as to prevent any part of the amnios attaining the inner surface of the placenta. The amnios consists of two layers:one is the granular layer, continued upon the inner or foetal surface of the allantois, and thence upon the umbilical cord; the other is the smooth outer layer, continued upon the outer or chorional surface of the allantois, and thence upon the inner surface of the chorion.\"\n\n\n"}
{"id": "901", "url": "https://en.wikipedia.org/wiki?curid=901", "title": "Astatine", "text": "Astatine\n\nAstatine is a radioactive chemical element with symbol At and atomic number 85. It is the rarest naturally occurring element in the Earth's crust, occurring only as the decay product of various heavier elements. All of astatine's isotopes are short-lived; the most stable is astatine-210, with a half-life of 8.1 hours. A sample of the pure element has never been assembled, because any macroscopic specimen would be immediately vaporized by the heat of its own radioactivity.\n\nThe bulk properties of astatine are not known with any certainty. Many of them have been estimated based on the element's position on the periodic table as a heavier analog of iodine, and a member of the halogens (the group of elements including fluorine, chlorine, bromine, and iodine). Astatine is likely to have a dark or lustrous appearance and may be a semiconductor or possibly a metal; it probably has a higher melting point than that of iodine. Chemically, several anionic species of astatine are known and most of its compounds resemble those of iodine. It also shows some metallic behavior, including being able to form a stable monatomic cation in aqueous solution (unlike the lighter halogens).\n\nThe first synthesis of the element was in 1940 by Dale R. Corson, Kenneth Ross MacKenzie, and Emilio G. Segrè at the University of California, Berkeley, who named it from the Greek \"astatos\" (ἄστατος), meaning \"unstable\". Four isotopes of astatine were subsequently found to be naturally occurring, although much less than one gram is present at any given time in the Earth's crust. Neither the most stable isotope astatine-210, nor the medically useful astatine-211, occur naturally; they can only be produced synthetically, usually by bombarding bismuth-209 with alpha particles.\n\nAstatine is an extremely radioactive element; all its isotopes have short half-lives of 8.1 hours or less, decaying into other astatine isotopes, bismuth, polonium or radon. Most of its isotopes are very unstable with half-lives of one second or less. Of the first 101 elements in the periodic table, only francium is less stable, and all the astatine isotopes more stable than francium are in any case synthetic and do not occur in nature.\n\nThe bulk properties of astatine are not known with any certainty. Research is limited by its short half-life, which prevents the creation of weighable quantities. A visible piece of astatine would immediately vaporize itself because of the heat generated by its intense radioactivity. It remains to be seen if, with sufficient cooling, a macroscopic quantity of astatine could be deposited as a thin film. Astatine is usually classified as either a nonmetal or a metalloid; metal formation has also been predicted.\n\nMost of the physical properties of astatine have been estimated (by interpolation or extrapolation), using theoretically or empirically derived methods. For example, halogens get darker with increasing atomic weight – fluorine is nearly colorless, chlorine is yellow-green, bromine is red-brown, and iodine is dark gray/violet. Astatine is sometimes described as probably being a black solid (assuming it follows this trend), or as having a metallic appearance (if it is a metalloid or a metal). The melting and boiling points of astatine are also expected to follow the trend seen in the halogen series, increasing with atomic number. On this basis they are estimated to be , respectively. Some experimental evidence suggests astatine may have lower melting and boiling points than those implied by the halogen trend. Astatine sublimes less readily than does iodine, having a lower vapor pressure. Even so, half of a given quantity of astatine will vaporize in approximately an hour if put on a clean glass surface at room temperature. The absorption spectrum of astatine in the middle ultraviolet region has lines at 224.401 and 216.225 nm, suggestive of 6p to 7s transitions.\n\nThe structure of solid astatine is unknown. As an analogue of iodine it may have an orthorhombic crystalline structure composed of diatomic astatine molecules, and be a semiconductor (with a band gap of 0.7 eV). Alternatively, if condensed astatine forms a metallic phase, as has been predicted, it may have a monatomic face-centered cubic structure; in this structure it may well be a superconductor, like the similar high-pressure phase of iodine. Evidence for (or against) the existence of diatomic astatine (At) is sparse and inconclusive. Some sources state that it does not exist, or at least has never been observed, while other sources assert or imply its existence. Despite this controversy, many properties of diatomic astatine have been predicted; for example, its bond length would be , dissociation energy , and heat of vaporization (∆H) 54.39 kJ/mol. The latter figure means that astatine may (at least) be metallic in the liquid state on the basis that elements with a heat of vaporization greater than ~42 kJ/mol are metallic when liquid; diatomic iodine, with a value of 41.71 kJ/mol, falls just short of the threshold figure.\n\nThe chemistry of astatine is \"clouded by the extremely low concentrations at which astatine experiments have been conducted, and the possibility of reactions with impurities, walls and filters, or radioactivity by-products, and other unwanted nano-scale interactions.\" Many of its apparent chemical properties have been observed using tracer studies on extremely dilute astatine solutions, typically less than 10 mol·L. Some properties – such as anion formation – align with other halogens. Astatine has some metallic characteristics as well, such as plating onto a cathode, coprecipitating with metal sulfides in hydrochloric acid, and forming a stable monatomic cation in aqueous solution. It forms complexes with EDTA, a metal chelating agent, and is capable of acting as a metal in antibody radiolabeling; in some respects astatine in the +1 state is akin to silver in the same state. Most of the organic chemistry of astatine is, however, analogous to that of iodine.\n\nAstatine has an electronegativity of 2.2 on the revised Pauling scale – lower than that of iodine (2.66) and the same as hydrogen. In hydrogen astatide (HAt) the negative charge is predicted to be on the hydrogen atom, implying that this compound could be referred to as astatine hydride according to certain nomenclatures, That would be consistent with the electronegativity of astatine on the Allred–Rochow scale (1.9) being less than that of hydrogen (2.2). However, official IUPAC stoichiometric nomenclature is based on an idealized convention of determining the relative electronegativities of the elements by the mere virtue of their position within the periodic table. According to this convention, astatine is handled as though it is more electronegative than hydrogen, irrespective of its true electronegativity. The electron affinity of astatine is predicted to be reduced by one-third because of spin–orbit interactions.\n\nLess reactive than iodine, astatine is the least reactive of the halogens, although its compounds have been synthesized in microscopic amounts and studied as intensively as possible before their radioactive disintegration. The reactions involved have been typically tested with dilute solutions of astatine mixed with larger amounts of iodine. Acting as a carrier, the iodine ensures there is sufficient material for laboratory techniques (such as filtration and precipitation) to work. Like iodine, astatine has been shown to adopt odd-numbered oxidation states ranging from −1 to +7.\n\nOnly a few compounds with metals have been reported, in the form of astatides of sodium, palladium, silver, thallium, and lead. Some characteristic properties of silver and sodium astatide, and the other hypothetical alkali and alkaline earth astatides, have been estimated by extrapolation from other metal halides.\n\nThe formation of an astatine compound with hydrogen – usually referred to as hydrogen astatide – was noted by the pioneers of astatine chemistry. As mentioned, there are grounds for instead referring to this compound as astatine hydride. It is easily oxidized; acidification by dilute nitric acid gives the At or At forms, and the subsequent addition of silver(I) may only partially, at best, precipitate astatine as silver(I) astatide (AgAt). Iodine, in contrast, is not oxidized, and precipitates readily as silver(I) iodide.\n\nAstatine is known to bind to boron, carbon, and nitrogen. Various boron cage compounds have been prepared with At–B bonds, these being more stable than At–C bonds. Astatine can replace a hydrogen atom in benzene to form astatobenzene CHAt; this may be oxidized to CHAtCl by chlorine. By treating this compound with an alkaline solution of hypochlorite, CHAtO can be produced. The dipyridine-astatine(I) cation, [At(CHN)], forms ionic compounds with perchlorate (a non-coordinating anion) and with nitrate, [At(CHN)]NO. This cation exists as a coordination complex in which two dative covalent bonds separately link the astatine(I) centre with each of the pyridine rings via their nitrogen atoms.\n\nWith oxygen, there is evidence of the species AtO and AtO in aqueous solution, formed by the reaction of astatine with an oxidant such as elemental bromine or (in the last case) by sodium persulfate in a solution of perchloric acid. The species previously thought to be has since been determined to be , a hydrolysis product of AtO (another such hydrolysis product being AtOOH). The well characterized anion can be obtained by, for example, the oxidation of astatine with potassium hypochlorite in a solution of potassium hydroxide. Preparation of lanthanum triastatate La(AtO), following the oxidation of astatine by a hot NaSO solution, has been reported. Further oxidation of , such as by xenon difluoride (in a hot alkaline solution) or periodate (in a neutral or alkaline solution), yields the perastatate ion ; this is only stable in neutral or alkaline solutions. Astatine is also thought to be capable of forming cations in salts with oxyanions such as iodate or dichromate; this is based on the observation that, in acidic solutions, monovalent or intermediate positive states of astatine coprecipitate with the insoluble salts of metal cations such as silver(I) iodate or thallium(I) dichromate.\n\nAstatine may form bonds to the other chalcogens; these include SAt and with sulfur, a coordination selenourea compound with selenium, and an astatine–tellurium colloid with tellurium.\n\nAstatine is known to react with its lighter homologs iodine, bromine, and chlorine in the vapor state; these reactions produce diatomic interhalogen compounds with formulas AtI, AtBr, and AtCl. The first two compounds may also be produced in water – astatine reacts with iodine/iodide solution to form AtI, whereas AtBr requires (aside from astatine) an iodine/iodine monobromide/bromide solution. The excess of iodides or bromides may lead to and ions, or in a chloride solution, they may produce species like or via equilibrium reactions with the chlorides. Oxidation of the element with dichromate (in nitric acid solution) showed that adding chloride turned the astatine into a molecule likely to be either AtCl or AtOCl. Similarly, or may be produced. The polyhalides PdAtI, CsAtI, TlAtI, and PbAtI are known or presumed to have been precipitated. In a plasma ion source mass spectrometer, the ions [AtI], [AtBr], and [AtCl] have been formed by introducing lighter halogen vapors into a helium-filled cell containing astatine, supporting the existence of stable neutral molecules in the plasma ion state. No astatine fluorides have been discovered yet. Their absence has been speculatively attributed to the extreme reactivity of such compounds, including the reaction of an initially formed fluoride with the walls of the glass container to form a non-volatile product. Thus, although the synthesis of an astatine fluoride is thought to be possible, it may require a liquid halogen fluoride solvent, as has already been used for the characterization of radon fluoride.\n\nIn 1869, when Dmitri Mendeleev published his periodic table, the space under iodine was empty; after Niels Bohr established the physical basis of the classification of chemical elements, it was suggested that the fifth halogen belonged there. Before its officially recognized discovery, it was called \"eka-iodine\" (from Sanskrit \"eka\" – \"one\") to imply it was one space under iodine (in the same manner as eka-silicon, eka-boron, and others). Scientists tried to find it in nature; given its extreme rarity, these attempts resulted in several false discoveries.\n\nThe first claimed discovery of eka-iodine was made by Fred Allison and his associates at the Alabama Polytechnic Institute (now Auburn University) in 1931. The discoverers named element 85 \"alabamine\", and assigned it the symbol Ab, designations that were used for a few years. In 1934, H. G. MacPherson of University of California, Berkeley disproved Allison's method and the validity of his discovery. There was another claim in 1937, by the chemist Rajendralal De. Working in Dacca in British India (now Dhaka in Bangladesh), he chose the name \"dakin\" for element 85, which he claimed to have isolated as the thorium series equivalent of radium F (polonium-210) in the radium series. The properties he reported for dakin do not correspond to those of astatine; moreover, astatine is not found in the thorium series, and the true identity of dakin is not known.\n\nIn 1936, a team of Romanian physicist Horia Hulubei and French physicist Yvette Cauchois claimed to have discovered element 85 via X-ray analysis. In 1939, they published another paper which supported and extended previous data. In 1944, Hulubei published a summary of data he had obtained up to that time, claiming it was supported by the work of other researchers. He chose the name \"dor\", presumably from the Romanian for \"longing\" [for peace], as World War II had started five years earlier. As Hulubei was writing in French, a language which does not accommodate the \"ine\" suffix, dor would likely have been rendered in English as \"dorine\", had it been adopted. In 1947, Hulubei's claim was effectively rejected by the Austrian chemist Friedrich Paneth, who would later chair the IUPAC committee responsible for recognition of new elements. Even though Hulubei's samples did contain astatine, his means to detect it were too weak, by current standards, to enable correct identification. He had also been involved in an earlier false claim as to the discovery of element 87 (francium) and this is thought to have caused other researchers to downplay his work.\nIn 1940, the Swiss chemist Walter Minder announced the discovery of element 85 as the beta decay product of radium A (polonium-218), choosing the name \"helvetium\" (from , the Latin name of Switzerland). Karlik and Bernert were unsuccessful in reproducing his experiments, and subsequently attributed Minder's results to contamination of his radon stream (radon-222 is the parent isotope of polonium-218). In 1942, Minder, in collaboration with the English scientist Alice Leigh-Smith, announced the discovery of another isotope of element 85, presumed to be the product of thorium A (polonium-216) beta decay. They named this substance \"anglo-helvetium\", but Karlik and Bernert were again unable to reproduce these results.\n\nLater in 1940, Dale R. Corson, Kenneth Ross MacKenzie, and Emilio Segrè isolated the element at the University of California, Berkeley. Instead of searching for the element in nature, the scientists created it by bombarding bismuth-209 with alpha particles in a cyclotron (particle accelerator) to produce, after emission of two neutrons, astatine-211. The discoverers, however, did not immediately suggest a name for the element. The reason for this was that at the time, an element created synthetically in \"invisible quantities\" that had not yet discovered in nature was not seen as a completely valid one; in addition, chemists were reluctant to recognize radioactive isotopes as legitimately as stable ones. In 1943, astatine was found as a product of two naturally occurring decay chains by Berta Karlik and Traude Bernert, first in the so-called uranium series, and then in the actinium series. (Since then, astatine has been determined in a third decay chain, the neptunium series.) Friedrich Paneth in 1946 called to finally recognize synthetic elements, quoting, among other reasons, recent confirmation of their natural occurrence, and proposed that the discoverers of the newly discovered unnamed elements name these elements. In early 1947, \"Nature\" published the discoverers' suggestions; a letter from Corson, MacKenzie, and Segrè suggested the name \"astatine\" coming from the Greek \"astatos\" (αστατος) meaning \"unstable\", because of its propensity for radioactive decay, with the ending \"-ine\", found in the names of the four previously discovered halogens. The name was also chosen to continue the tradition of the four stable halogens, where the name referred to a property of the element.\n\nCorson and his colleagues classified astatine as a metal on the basis of its analytical chemistry. Subsequent investigators reported iodine-like, cationic, or amphoteric behavior. In a 2003 retrospective, Corson wrote that \"some of the properties [of astatine] are similar to iodine … it also exhibits metallic properties, more like its metallic neighbors Po and Bi.\"\n\nThere are 39 known isotopes of astatine, with atomic masses (mass numbers) of 191–229. Theoretical modeling suggests that 37 more isotopes could exist. No stable or long-lived astatine isotope has been observed, nor is one expected to exist.\n\nAstatine's alpha decay energies follow the same trend as for other heavy elements. Lighter astatine isotopes have quite high energies of alpha decay, which become lower as the nuclei become heavier. Astatine-211 has a significantly higher energy than the previous isotope, because it has a nucleus with 126 neutrons, and 126 is a magic number corresponding to a filled neutron shell. Despite having a similar half-life to the previous isotope (8.1 hours for astatine-210 and 7.2 hours for astatine-211), the alpha decay probability is much higher for the latter: 41.81% against only 0.18%. The two following isotopes release even more energy, with astatine-213 releasing the most energy. For this reason, it is the shortest-lived astatine isotope. Even though heavier astatine isotopes release less energy, no long-lived astatine isotope exists, because of the increasing role of beta decay (electron emission). This decay mode is especially important for astatine; as early as 1950 it was postulated that all isotopes of the element undergo beta decay. Beta decay modes have been found for all astatine isotopes except astatine-213, -214, -215, and -216m. Astatine-210 and lighter isotopes exhibit beta plus decay (positron emission), astatine-216 and heavier isotopes exhibit beta (minus) decay, and astatine-212 decays via both modes, while astatine-211 undergoes electron capture.\n\nThe most stable isotope is astatine-210, which has a half-life of 8.1 hours. The primary decay mode is beta plus, to the relatively long-lived (in comparison to astatine isotopes) alpha emitter polonium-210. In total, only five isotopes have half-lives exceeding one hour (astatine-207 to -211). The least stable ground state isotope is astatine-213, with a half-life of 125 nanoseconds. It undergoes alpha decay to the extremely long-lived bismuth-209.\n\nAstatine has 24 known nuclear isomers, which are nuclei with one or more nucleons (protons or neutrons) in an excited state. A nuclear isomer may also be called a \"meta-state\", meaning the system has more internal energy than the \"ground state\" (the state with the lowest possible internal energy), making the former likely to decay into the latter. There may be more than one isomer for each isotope. The most stable of these nuclear isomers is astatine-202m1, which has a half-life of about 3 minutes, longer than those of all the ground states bar those of isotopes 203–211 and 220. The least stable is astatine-214m1; its half-life of 265 nanoseconds is shorter than those of all ground states except that of astatine-213.\n\nAstatine is the rarest naturally occurring element. The total amount of astatine in the Earth's crust (quoted mass 2.36 × 10 grams) is estimated to be less than one gram at any given time.\n\nAny astatine present at the formation of the Earth has long since disappeared; the four naturally occurring isotopes (astatine-215, -217, -218 and -219) are instead continuously produced as a result of the decay of radioactive thorium and uranium ores, and trace quantities of neptunium-237. The landmass of North and South America combined, to a depth of 16 kilometers (10 miles), contains only about one trillion astatine-215 atoms at any given time (around 3.5 × 10 grams). Astatine-217 is produced via the radioactive decay of neptunium-237. Primordial remnants of the latter isotope—due to its relatively short half-life of 2.14 million years—are no longer present on Earth. However, trace amounts occur naturally as a product of transmutation reactions in uranium ores. Astatine-218 was the first astatine isotope discovered in nature. Astatine-219, with a half-life of 56 seconds, is the longest lived of the naturally occurring isotopes.\n\nIsotopes of astatine are sometimes not listed as naturally occurring because of misconceptions that there are no such isotopes, or discrepancies in the literature. Astatine-216 has been counted as a naturally occurring isotope but reports of its observation (which were described as doubtful) have not been confirmed.\n\nAstatine was first produced by bombarding bismuth-209 with energetic alpha particles, and this is still the major route used to create the relatively long-lived isotopes astatine-209 through astatine-211. Astatine is only produced in minuscule quantities, with modern techniques allowing production runs of up to 6.6 giga becquerels (about 86 nanograms or 2.47 × 10 atoms). Synthesis of greater quantities of astatine using this method is constrained by the limited availability of suitable cyclotrons and the prospect of melting the target. Solvent radiolysis due to the cumulative effect of astatine decay is a related problem. With cryogenic technology, microgram quantities of astatine might be able to be generated via proton irradiation of thorium or uranium to yield radon-211, in turn decaying to astatine-211. Contamination with astatine-210 is expected to be a drawback of this method.\n\nThe most important isotope is astatine-211, the only one in commercial use. To produce the bismuth target, the metal is sputtered onto a gold, copper, or aluminium surface at 50 to 100 milligrams per square centimeter. Bismuth oxide can be used instead; this is forcibly fused with a copper plate. The target is kept under a chemically neutral nitrogen atmosphere, and is cooled with water to prevent premature astatine vaporization. In a particle accelerator, such as a cyclotron, alpha particles are collided with the bismuth. Even though only one bismuth isotope is used (bismuth-209), the reaction may occur in three possible ways, producing astatine-209, astatine-210, or astatine-211. In order to eliminate undesired nuclides, the maximum energy of the particle accelerator is set to a value (optimally 29.17 MeV) above that for the reaction producing astatine-211 (to produce the desired isotope) and below the one producing astatine-210 (to avoid producing other astatine isotopes).\n\nSince astatine is the main product of the synthesis, after its formation it must only be separated from the target and any significant contaminants. Several methods are available, \"but they generally follow one of two approaches—dry distillation or [wet] acid treatment of the target followed by solvent extraction.\" The methods summarized below are modern adaptations of older procedures, as reviewed by Kugler and Keller. Pre-1985 techniques more often addressed the elimination of co-produced toxic polonium; this requirement is now mitigated by capping the energy of the cyclotron irradiation beam.\n\nThe astatine-containing cyclotron target is heated to a temperature of around 650 °C. The astatine volatilizes and is condensed in (typically) a cold trap. Higher temperatures of up to around 850 °C may increase the yield, at the risk of bismuth contamination from concurrent volatilization. Redistilling the condensate may be required to minimize the presence of bismuth (as bismuth can interfere with astatine labeling reactions). The astatine is recovered from the trap using one or more low concentration solvents such as sodium hydroxide, methanol or chloroform. Astatine yields of up to around 80% may be achieved. Dry separation is the method most commonly used to produce a chemically useful form of astatine.\n\nThe bismuth (or sometimes bismuth trioxide) target is dissolved in, for example, concentrated nitric or perchloric acid. Astatine is extracted using an organic solvent such as butyl or isopropyl ether, or thiosemicarbazide. A separation yield of 93% using nitric acid has been reported, falling to 72% by the time purification procedures were completed (distillation of nitric acid, purging residual nitrogen oxides, and redissolving bismuth nitrate to enable liquid–liquid extraction). Wet methods involve \"multiple radioactivity handling steps\" and are not well suited for isolating larger quantities of astatine. They can enable the production of astatine in a specific oxidation state and may have greater applicability in experimental radiochemistry.\n\nNewly formed astatine-211 is the subject of ongoing research in nuclear medicine. It must be used quickly as it decays with a half-life of 7.2 hours; this is long enough to permit multistep labeling strategies. Astatine-211 has potential for targeted alpha particle radiotherapy, since it decays either via emission of an alpha particle (to bismuth-207), or via electron capture (to an extremely short-lived nuclide, polonium-211, which undergoes further alpha decay), very quickly reaching its stable granddaughter lead-207. Polonium X-rays emitted as a result of the electron capture branch, in the range of 77–92 keV, enable the tracking of astatine in animals and patients. Although astatine-210 has a slightly longer half-life, it is wholly unsuitable because it usually undergoes beta plus decay to the extremely toxic polonium-210.\n\nThe principal medicinal difference between astatine-211 and iodine-131 (a radioactive iodine isotope also used in medicine) is that iodine-131 emits high-energy beta particles, and astatine does not. Beta particles have much greater penetrating power through tissues than do the much heavier alpha particles. An average alpha particle released by astatine-211 can travel up to 70 µm through surrounding tissues; an average-energy beta particle emitted by iodine-131 can travel nearly 30 times as far, to about 2 mm. The short half-life and limited penetrating power of alpha radiation through tissues offers advantages in situations where the \"tumor burden is low and/or malignant cell populations are located in close proximity to essential normal tissues.\" Significant morbidity in cell culture models of human cancers has been achieved with from one to ten astatine-211 atoms bound per cell.\nSeveral obstacles have been encountered in the development of astatine-based radiopharmaceuticals for cancer treatment. World War II delayed research for close to a decade. Results of early experiments indicated that a cancer-selective carrier would need to be developed and it was not until the 1970s that monoclonal antibodies became available for this purpose. Unlike iodine, astatine shows a tendency to dehalogenate from molecular carriers such as these, particularly at sp carbon sites (less so from sp sites). Given the toxicity of astatine accumulated and retained in the body, this emphasized the need to ensure it remained attached to its host molecule. While astatine carriers that are slowly metabolized can be assessed for their efficacy, more rapidly metabolized carriers remain a significant obstacle to the evaluation of astatine in nuclear medicine. Mitigating the effects of astatine-induced radiolysis of labeling chemistry and carrier molecules is another area requiring further development. A practical application for astatine as a cancer treatment would potentially be suitable for a \"staggering\" number of patients; production of astatine in the quantities that would be required remains an issue.\n\nAnimal studies show that astatine, similarly to iodine, although to a lesser extent, is preferentially concentrated in the thyroid gland. Unlike iodine, astatine also shows a tendency to be taken up by the lungs and spleen, possibly because of in-body oxidation of At to At. If administered in the form of a radiocolloid it tends to concentrate in the liver. Experiments in rats and monkeys suggest that astatine-211 causes much greater damage to the thyroid gland than does iodine-131, with repetitive injection of the nuclide resulting in necrosis and cell dysplasia within the gland. Early research suggested that injection of astatine into female rodents caused morphological changes in breast tissue; this conclusion remained controversial for many years. General agreement was later reached that this was likely caused by the effect of breast tissue irradiation combined with hormonal changes due to irradiation of the ovaries. Trace amounts of astatine can be handled safely in fume hoods if they are well-aerated; biological uptake of the element must be avoided.\n\n\n"}
{"id": "29469931", "url": "https://en.wikipedia.org/wiki?curid=29469931", "title": "Bernard H. Lavenda", "text": "Bernard H. Lavenda\n\nBernard Howard Lavenda (born September 18, 1945) is a retired professor of chemical physics at the University of Camerino and expert on irreversible thermodynamics. He has contributed to many areas of physics, including that of Brownian motion, and in the establishment of the statistical basis of thermodynamics, and non-Euclidean geometrical theories of relativity. He was the scientific coordinator of the \"European Thermodynamics Network\" in the European Commission Program of Human Capital and Mobility. He was also a proponent for the establishment of, and scientific director of, a National (Italian) Centre for Thermodynamics, and has acted as scientific consultant to companies such as the ENI Group, where he helped to found TEMA, a consulting firm for SNAM Progetti, ENEA (Italian National Agency for New Technologies, Energy and the Environment), and the Solar Energy Research Institute in Golden, Colorado. He has had over 130 scientific papers published in international journals, some critical of the new fashions and modes in theoretical physics.\n\nProfessor Lavenda currently lives in Trevignano Romano near Rome, is married with two adult children and two grandchildren, for whom his textbook \"A New Perspective on Thermodynamics\" is dedicated.\n\nBernard H. Lavenda was born in New York City. After completing secondary school in North Adams, Massachusetts, he attended Clark University where he graduated \"cum laude\" in 1966 with a B.Sc in chemistry. Having passed the entrance examination for the doctoral program at the Weizmann Institute of Science, he began experimental work on enzymes under the direction of Ephraim Katzir, who was later to become the President of Israel. Realizing that he was not made out for experimental work, he came under the influence of Ephraim's brother, Aaron, after reading his book \"Nonequilibrium Thermodynamics in Biophysics\", coauthored with Peter Curran.\n\nAfter the Six Days War, Aaron Katchalsky helped him secure a studentship for a doctoral degree in Ilya Prigogine's group in Brussels.\n\nHis doctoral thesis, \"Kinetic analysis and thermodynamic interpretation of nonequilibrium unstable transitions in open systems\", showed that when homogeneous nonlinear chemical reactions far from equilibrium on the thermodynamic branch, which is an extension of the law of mass action at equilibrium, become unstable they make transitions to kinetic branches with lower entropy production than the thermodynamic branch.\nThis result was initially contested by Prigogine who reasoned from hydrodynamic instabilities, like the Rayleigh-Benard instability, which show a larger entropy production beyond the critical point in order to maintain spatial structures. Prigogine later considered these spatial structures to be produced by unstable chemically diffusing systems, based on Alan Turing's morphological models, calling them 'dissipative structures' and for which he received the Nobel Prize in Chemistry in 1977.\n\nPrigogine later acknowledged that such transitions to lower states of entropy reduction were possible since no spatial structural changes were involved, and later incorporated Lavenda's work into a chapter of his new book Thermodynamic Theory of Structure, Stability, and Fluctuations, co-authored with Paul Glansdorff. \nAfter receiving his doctorate from the Universite Libre de Bruxelles, with la plus grande distinction, he returned to Israel in 1970 to work as a post-doctoral student in the Physical Chemistry Department of the Hebrew University.\nDuring that period he published a short note in the Italian physics journal, Lettere Al Nuovo Cimento [3 (1972) 385-390] criticizing the Glansdorff-Prigogine universal criteria of evolution which attributes an inequality to a potential which is a function only of intensive variables, the forces. He pointed out that no such thermodynamic potential could exist for it would be devoid of all information regarding how large the system is, or how many particles it contains. The inequality would be a criterion of stability, but, on account of the assumption of local (stable) equilibrium of the components that the system is broken up into, the sum of stable components can hardly become unstable. The note would probably have gone unnoticed were it not for Peter Landsberg's citation of it in his \"Nature\" review of the Glansdorff-Prigogine book [P. T. Landsberg, \"The fourth law of thermodynamics\"], where he predicted \"the occasional lack of lucidity in the book which may give rise to some discussion within the next few years\".\n\nAfter the murder of Aharon Katzir in Lod Airport massacre in May 1972, Lavenda accepted a position of consultant at Nuovo Pignone in Florence Italy together with a teaching position at the University of Pisa.\nThrough the vice President of Nuovo Pignone, he came into contact with Vicenzo Gervasio who was later to become President of ENI Data, and the idea crystallized of setting up a company dedicated to the analysis and dynamic modeling of fouling processes in refineries and reactors. He established relations between ENI and Northwest Research, Boeing, and SERI (Solar Energy Research Institute). \nHe helped form a new company within the ENI group, TEMA, which was supported by SNAM Progetti. While retaining an unpaid lectureship in Thermodynamics at the University of Naples, Lavenda published his critical appraisal of the then current theories of irreversible thermodynamics, Thermodynamics of Irreversible Processes, in 1978. It was originally published by the Macmillan Press and later became a Dover Classic of Science and Mathematics.\n\nIn 1980 he won a chair in Physical Chemistry. Transferring to Camerino, he was to spend more than three decades there. \nHis first book during this period, \"Nonequilibrium Statistical Thermodynamics\", published by Wiley in 1985, developed the nonlinear generalization of the Onsager-Machlup formulation of nonequilibrium fluctuations which was restricted to linear (Gaussian) processes. \nJust as equilibrium is characterized by the state of maximum entropy, corresponding to maximum probability, nonequlibrium states are characterized by the principle of least dissipation of energy, corresponding to the maximum probability of a transition between nonequilibrium states that are not well-separated in time. \nThis principle can be generalized to non-Gaussian fluctuations in the limit of small thermal noise and constitutes a kinetic analog to Boltzmann's principle.\n\nDuring a sabbatical year in 1986 in Porto Alegre, Lavenda had ample time to browse through the well-furnished library at the Universidade Federale di Rio Grande del Sud. \nHe was impressed by the parallelism between statistical inference and statistical thermodynamics: two distinct and separate branches that are essentially working on the same problems but with no apparent connection. \nHis work, summarized in Statistical Physics: A Probabilistic Approach, published by Wiley-Interscience in 1991, completes Boltzmann's principle, which expresses the entropy as the logarithm of a combinatorial factor, by showing that the entropy is the potential that determines Gauss’ law of error for which the average value is the most probable value.\nJust as there are frequency and degree- of-belief (Bayes' theorem) interpretations of statistical inference, the same should apply to statistical thermodynamics. The frequency interpretation applies to extensive variables, like energy and volume which can be sampled, while the degree-of-belief interpretations applies to the intensive variables, like temperature and pressure, for which sampling has no meaning. The connection between the two branches translates the Cramer-Rao inequality into thermodynamic uncertainty relations, analogous to quantum mechanical uncertainty relations, where the more knowledge we have about a thermodynamic variable the less we know about its conjugate. \nSince the lack of a probability distribution means the absence of its statistics, the possibility of an intermediate statistics, or what is referred to as parastatistics, between Bose–Einstein statistics and Fermi–Dirac statistics is nonexistent.\n\nStatistical thermodynamics is usually concerned with most probable behavior which becomes almost certainty if large enough samples are taken. \nBut sometimes surprises are in store where extreme behavior becomes the prevalent one.\nTurning his attention to such rare events Lavenda published Thermodynamics of Extremes in 1995, whose real interest lies in the formulation of a thermodynamics of earthquakes that was subsequently published in Annali di Geofisica (Extreme value statistics and thermodynamics of earthquakes: \"Large earthquakes\"; \"aftershock sequences\"), and which is gaining increasing attention.\nBy properly defining entropy and energy, a temperature can be associated to an aftershock sequence giving it an additional means of characterization. \nA new magnitude-frequency relation is predicted which applies to clustered after-shocks in contrast to the [Gutenberg-Richter law] which treats them as independent and identically distributed random events.\n\nIn the nineties, Lavenda saw thermodynamics as a cultural heritage that could have a place in Italian society, and would be pertinent to both industrial research and to the preservation of its artistic patrimony. \nHe was a proponent for the establishment of a National Centre of Thermodynamics for which financial funding was unavailable. Capturing the interest of the ENEA, or the Italian agency for alternative energy resources, he applied for funding in the European Commission of Human Capital and Mobility Programme. \nHis project, \"Thermodynamics of Complex Systems\", came in sixth place in Chemistry section with maximum funding in 1992.\nThis led to the formation of a European Thermodynamics Network consisting of 16 partners in the EU and Switzerland.\nIt was later extended to the Eastern European Countries in the European Commission PECO Programme. This eventually led to the establishment of a National Centre for Thermodynamics that was brought into existence by the ENEA, but lasted only several months, because European funds were absorbed by other projects\n\nOften critical of new fashions and modes in thermodynamics, Lavenda wrote \"A New Perspective on Thermodynamics\", published in 2009, by returning to Carnot's original conception that work can only be done when there is a difference in temperature, and the necessity of closing the cycle before that work can be assessed.\nMore recently Lavenda has directed his interests to relativity by providing it with a new foundation based on non-Euclidean geometries. \nRather than measuring distances in terms of the usual Euclidean metric, distances are defined in terms of what is known as a cross-ratio, a perspective invariant of four points, which, for the space of velocities, just happens to be the compounding of longitudinal Doppler shifts. Doppler shifts are fundamental to relativity: oblique Doppler shifts describe aberration, while second order ones describe length contraction, but rather than being in the direction of the motion are perpendicular to it.\nThe uniformly rotating disc, which is considered by some to be the missing link in Einstein's formulation of general relativity, is exactly described by the hyperbolic metric in polar coordinates, named after the nineteenth century Italian geometer Eugenio Beltrami, which predicts the circumference of the disc to be greater when in motion than when it is at rest.\nThus a uniformly rotating disc belongs to hyperbolic, and not Euclidean, space and so, too, does relativity.\n\n\n\n\n"}
{"id": "48912755", "url": "https://en.wikipedia.org/wiki?curid=48912755", "title": "Bismuth antimonide", "text": "Bismuth antimonide\n\nBismuth antimonides, Bismuth-antimonys, or Bismuth-antimony alloys, (BiSb) are binary alloys of bismuth and antimony in various ratios.\n\nSome, in particular BiSb, were the first experimentally-observed three-dimensional topological insulators, materials that have conducting surface states but have an insulating interior.\n\nVarious BiSb alloys also superconduct at low temperatures, are semiconductors, and are used in thermoelectric devices.\n\nBismuth antimonide itself (see box to right) is sometimes described as BiSb.\n\nCrystals of bismuth antimonides are synthesized by melting bismuth and antimony together under inert gas or vacuum. Zone melting is used to decrease the concentration of impurities. When synthesizing single crystals of bismuth antimonides, it is important that impurities are removed from the samples, as oxidation occurring at the impurities leads to polycrystalline growth.\n\nPure bismuth is a semimetal, containing a small band gap, which leads to it having a relatively high conductivity (7.7*10 S/m at 20 °C). When the bismuth is doped with antimony, the conduction band decreases in energy and the valence band increases in energy. At an Sb concentration of 4%, the two bands intersect, forming a Dirac point (which is defined as a point where the conduction and valence bands intersect). Further increases in the concentration of antimony result in a band inversion, in which the energy of the valence band becomes greater than that of the conduction band at specific momenta. Between Sb concentrations of 7 and 22%, the bands no longer intersect, and the BiSb becomes an inverted-band insulator. It is at these higher concentrations of Sb that the band gap in the surface states vanishes, and the material thus conducts at its surface.\n\nThe highest temperatures at which BiSb thin film of thicknesses 150-1350A superconduct, the critical temperature T, is approximately 2K. Single crystal BiSb can superconduct at slightly higher temperatures, and at 4.2K, its critical magnetic field B (the maximum magnetic field that the superconductor can expel) of 1.6T at 4.2K.\n\nElectron mobility is one important parameter describing semiconductors because it describes the rate at which electrons can travel through the semiconductor. At 40K, electron mobility ranged from 0.49*10 cm/Vs at an Sb concentration of 0 to .24*10 cm/Vs at a Sb concentration of 7.2%. This is much greater than the electron mobility of other common semiconductors like Si, which is 1400 cm/Vs at room temperature.\n\nAnother important parameter of BiSb is the effective electron mass (EEM), a measure of the ratio of the acceleration of an electron to the force applied to an electron. The effective electron mass is .002m for x=.11 and .0009m at x=.06. This is much less than the electron effective mass in many common semiconductors (1.09 in Si at 300K, .55 in Ge, and .067 in GaAs). A low EEM is good for Thermophotovoltaic applications.\n\nBismuth antimonides are used as the n-type legs in many thermoelectric devices below room temperature. The thermoelectric efficiency, given by its figure of merit zT = σS2T/λ, where S is the Seebeck coefficient, λ is the thermal conductivity, and σ is the electrical conductivity, describes the ratio of the energy provided by the thermoelectric to the heat absorbed by the device. At 80K, the figure of merit (zT) for BiSb peaks at 6.5*10/K when x = 15%. Also, The Seebeck coefficient (the ratio of the potential difference between ends of a material to the temperature difference between the sides) at 80K of BiSb is -140μV/K, much smaller than the Seebeck coefficient of pure bismuth, -50μV/K.\n"}
{"id": "8144345", "url": "https://en.wikipedia.org/wiki?curid=8144345", "title": "Bolton WtE", "text": "Bolton WtE\n\nThe Bolton WtE is a waste power station constructed in 1971 in Bolton, and is a major landmark of its skyline. The incinerator burns up to of household waste per hour or per year, and can generate up to 11 MW of electricity. The plant is operated by Viridor. The Bolton incinerator is the only household waste incinerator in Greater Manchester.\n\n\n"}
{"id": "21235839", "url": "https://en.wikipedia.org/wiki?curid=21235839", "title": "Cemented carbide", "text": "Cemented carbide\n\nCemented carbide is a hard material used extensively as cutting tool material, as well as other industrial applications. It consists of fine particles of carbide cemented into a composite by a binder metal. Cemented carbides commonly use tungsten carbide (WC), titanium carbide (TiC), or tantalum carbide (TaC) as the aggregate. Mentions of \"carbide\" or \"tungsten carbide\" in industrial contexts usually refer to these cemented composites.\n\nMost of the time, carbide cutters will leave a better surface finish on the part, and allow faster machining than high-speed steel or other tool steels. Carbide tools can withstand higher temperatures at the cutter-workpiece interface than standard high-speed steel tools (which is a principal reason for the faster machining). Carbide is usually superior for the cutting of tough materials such as carbon steel or stainless steel, as well as in situations where other cutting tools would wear away faster, such as high-quantity production runs.\n\nCemented carbides are metal matrix composites where carbide particles act as the aggregate and a metallic binder serves as the matrix (like gravel aggregate in a matrix of cement makes concrete). Its structure is thus conceptually similar to that of a grinding wheel, except that the abrasive particles are much smaller; macroscopically, the material of a carbide cutter looks homogeneous.\n\nThe process of combining the carbide particles with the binder is referred to as sintering or hot isostatic pressing (HIP). During this process, the binder eventually will be entering the liquid stage and carbide grains (much higher melting point) remain in the solid stage. As a result of this process, the binder is embedding/cementing the carbide grains and thereby creates the metal matrix composite with its distinct material properties. The naturally ductile metal binder serves to offset the characteristic brittle behavior of the carbide ceramic, thus raising its toughness and durability. By controlling various parameters, including grain size, cobalt content, dotation (e.g., alloy carbides) and carbon content, a carbide manufacturer can tailor the carbide's performance to specific applications.\n\nThe first cemented carbide developed was tungsten carbide (introduced in 1927) which uses tungsten carbide particles held together by a cobalt metal binder. Since then other cemented carbides have been developed, such as titanium carbide, which is better suited for cutting steel, and tantalum carbide, which is tougher than tungsten carbide.\n\nThe coefficient of thermal expansion of cemented tungsten carbide is found to vary with the amount of cobalt used as a metal binder. For 5.9% of cobalt a coefficient of 4.4 µm·m·K is found, whereas the coefficient is around 5.0 µm·m·K for a cobalt content of 13%. Both values are only valid from to , but more data is available from Hidnert.\n\nCarbide is more expensive per unit than other typical tool materials, and it is more brittle, making it susceptible to chipping and breaking. To offset these problems, the carbide cutting tip itself is often in the form of a small insert for a larger tipped tool whose shank is made of another material, usually carbon tool steel. This gives the benefit of using carbide at the cutting interface without the high cost and brittleness of making the entire tool out of carbide. Most modern face mills use carbide inserts, as well as many lathe tools and endmills. In recent decades, though, solid-carbide endmills have also become more commonly used, wherever the application's characteristics make the pros (such as shorter cycle times) outweigh the cons (mentioned above).\n\nTo increase the life of carbide tools, they are sometimes coated. Four such coatings are TiN (titanium nitride), TiC (titanium carbide), Ti(C)N (titanium carbide-nitride), and TiAlN (titanium aluminium nitride). (Newer coatings, known as DLC (diamond-like carbon) are beginning to surface, enabling the cutting power of diamond without the unwanted chemical reaction between real diamond and iron.) Most coatings generally increase a tool's hardness and/or lubricity. A coating allows the cutting edge of a tool to cleanly pass through the material without having the material gall (stick) to it. The coating also helps to decrease the temperature associated with the cutting process and increase the life of the tool. The coating is usually deposited via thermal CVD and, for certain applications, with the mechanical PVD method. However, if the deposition is performed at too high temperature, an \"eta phase\" of a CoWC tertiary carbide forms at the interface between the carbide and the cobalt phase, which may lead to adhesion failure of the coating.\n\nMining and tunneling cutting tools are most often fitted with Cemented Carbide tips, the so-called \"button bits\". Only artificial diamond can replace the cemented carbide buttons when conditions are ideal, but as rock drilling is a tough job the Cemented Carbide button bits remain the most used type throughout the world.\n\nSince the mid-1960s, steel mills around the world have applied cemented carbide to the rolls of their rolling mills for both hot and cold rolling of tubes, bars, and flats.\n\nThis category contains a countless number of applications, but can be split into three main areas:\n\nSome key areas where cemented carbide components are used:\n\nTungsten carbide has become a popular metal in the bridal jewellery industry, due to its extreme hardness and high resistance to scratching. Given its brittleness, it is prone to chip, crack, or shatter in jewellery applications. Once fractured, it cannot be repaired.\n\nThe initial development of cemented and sintered carbides occurred in Germany in the 1920s. ThyssenKrupp says [in historical present tense], \"Sintered tungsten carbide was developed by the \"Osram study society for electrical lighting\" to replace diamonds as a material for machining metal. Not having the equipment to exploit this material on an industrial scale, Osram sells the license to Krupp at the end of 1925. In 1926 Krupp brings sintered carbide onto the market under the name WIDIA (acronym for WIe DIAmant = like diamond).\" Machinery's Handbook gives the date of carbide tools' commercial introduction as 1927. Burghardt and Axelrod give the date of their commercial introduction in the United States as 1928. Subsequent development occurred in various countries.\n\nAlthough the marketing pitch was slightly hyperbolic (carbides being not entirely equal to diamond), carbide tooling offered an improvement in cutting speeds and feeds so remarkable that, like high-speed steel had done two decades earlier, it forced machine tool designers to rethink every aspect of existing designs, with an eye toward yet more rigidity and yet better spindle bearings.\n\nDuring World War II there was a tungsten shortage in Germany. It was found that tungsten in carbide cuts metal more efficiently than tungsten in high-speed steel, so to economise on the use of tungsten, carbides were used for metal cutting as much as possible.\n\nThe Widia name became a genericized trademark in various countries and languages, including English (widia, ), although the genericized sense was never especially widespread in English (\"carbide\" is the normal generic term). Since 2009, the name has been revived as a brand name by Kennametal, and the brand subsumes numerous popular brands of cutting tools. For the sake of clear communication, the reviving of the Widia brand may naturally further discourage use of the generic sense..\n\nUncoated tips brazed to their shanks were the first form. Clamped indexable inserts and today's wide variety of coatings are advances made in the decades since. With every passing decade, the use of carbide has become less \"special\" and more ubiquitous.\n\nRegarding fine-grained hardmetal, an attempt has been made to follow the scientific and technological steps associated with its production; this task is not easy, though, because of the restrictions placed by commercial, and in some cases research, organisations, in not publicising relevant information until long after the date of the initial work. Thus, placing data in an historical, chronological order is somewhat difficult. However, it has been possible to establish that as far back as 1929, approximately 6 years after the first patent was granted, Krupp/Osram workers had identified the positive aspects of tungsten carbide grain refinement. By 1939, they had also discovered the beneficial effects of adding a small amount of vanadium and tantalum carbide. This effectively controlled discontinuous grain growth.\n\nWhat was considered ‘fine’ in one decade was considered not so fine in the next. Thus, a grain size in the range 0.5–3.0 μm was considered fine in the early years, but by the 1990s, the era of the nano-crystalline material had arrived, with a grain size of 20–50 nm.\n\n\"Pobedit\" () is a sintered carbide alloy of about 90% tungsten carbide as a hard phase, and about 10% cobalt (Co) as a binder phase, with a small amount of additional carbon. It was developed in the Soviet Union in 1929, it is described as a material from which cutting tools are made. Later a number of similar alloys based on tungsten and cobalt were developed, and the name of 'pobedit' was retained for them as well.\n\nPobedit is usually produced by powder metallurgy in the form of plates of different shapes and sizes. The manufacturing process is as follows: a fine powder of tungsten carbide (or other refractory carbide) and a fine powder of binder material such as cobalt or nickel both get intermixed and then pressed into the appropriate forms. Pressed plates are sintered at a temperature close to the melting point of the binder metal, which yields a very tight and solid substance.\n\nThe plates of this superhard alloy are applied to manufacturing of metal-cutting and drilling tools; they are usually soldered on the cutting tool tips. Heat post-treatment is not required. The pobedit inserts at the tips of drill bits are still very widespread in Russia.\n\n\n"}
{"id": "6229024", "url": "https://en.wikipedia.org/wiki?curid=6229024", "title": "Civic amenity site", "text": "Civic amenity site\n\nA civic amenity site (CA site) or household waste recycling centre (HWRC) (both terms are used in the United Kingdom) is a facility where the public can dispose of household waste and also often containing recycling points. Civic amenity sites are run by the local authority in a given area. Collection points for recyclable waste such as green waste, metals, glass and other waste types (including WVO) are available. Items that cannot be collected by local waste collection schemes such as bulky waste are also accepted.\n\nIn the United Kingdom, civic amenity sites are informally called \"tips\" or \"dumps\".\n\nIn continental Europe, there are usually several types of collection sites: \n\n"}
{"id": "10630682", "url": "https://en.wikipedia.org/wiki?curid=10630682", "title": "Coffin corner (aerodynamics)", "text": "Coffin corner (aerodynamics)\n\nCoffin corner (also known as the aerodynamic ceiling or Q corner) is the region of flight where a fast fixed-wing aircraft's stall speed is near the critical Mach number, at a given gross weight and G-force loading. In this region of flight, it is very difficult to keep the airplane in stable flight. Because the stall speed is the minimum speed required to maintain level flight, any reduction in speed will cause the airplane to stall and lose altitude. Because the critical Mach number is the maximum speed at which air can travel over the wings without losing lift due to flow separation and shock waves, any increase in speed will cause the airplane to lose lift, or to pitch heavily nose-down, and lose altitude. The \"corner\" refers to the triangular shape at the top of a flight envelope chart where the stall speed and critical Mach number are within a few knots of each other. The speed where they meet is the ceiling of the aircraft. This is distinct from the same term used for helicopters when outside the auto-rotation envelope as seen in the height-velocity diagram. \n\nConsideration of statics shows that when a fixed-wing aircraft is in straight, level flight at constant-airspeed the lift on the main wing plus the force (in the negative sense if downward) on the horizontal stabilizer is equal to the aircraft's weight; and its thrust is equal to its drag. In most circumstances this equilibrium can occur at a range of airspeeds. The minimum such speed is the stall speed, or \"V\". The indicated airspeed at which a fixed-wing aircraft stalls varies with the weight of the aircraft but does not vary significantly with altitude. At speeds close to the stall speed the aircraft's wings are at a high angle of attack.\n\nAt higher altitudes, the air density is lower than at sea level. Because of the progressive reduction in air density, as the aircraft’s altitude increases its true airspeed is progressively greater than its indicated airspeed. For example, the indicated airspeed at which an aircraft stalls can be considered constant, but the true airspeed at which it stalls increases with altitude.\n\nAir conducts sound at a certain speed, the \"speed of sound\". This becomes slower as the air becomes cooler. Because the temperature of the atmosphere generally decreases with altitude (until the tropopause), the speed of sound also decreases with altitude. (See the International Standard Atmosphere for more on temperature as a function of altitude.)\n\nA given airspeed, divided by the speed of sound in that air, gives a ratio known as the Mach number. A Mach number of 1.0 indicates an airspeed equal to the speed of sound in that air. Because the speed of sound increases with air temperature, and air temperature generally decreases with altitude, the true airspeed for a given Mach number generally decreases with altitude.\n\nAs an airplane moves through the air faster, the airflow over parts of the wing will reach speeds that approach Mach 1.0. At such speeds, shock waves form in the air passing over the wings, drastically increasing the drag due to drag divergence, causing Mach buffet, or drastically changing the center of pressure, resulting in a nose-down moment called \"mach tuck\". The aircraft Mach number at which these effects appear is known as its critical Mach number, or M. The true airspeed corresponding to the critical Mach number generally decreases with altitude.\n\nThe flight envelope is a plot of various curves representing the limits of the aircraft's true airspeed and altitude. Generally, the top-left boundary of the envelope is the curve representing stall speed, which increases as altitude increases. The top-right boundary of the envelope is the curve representing critical Mach number in true airspeed terms, which decreases as altitude increases. These curves typically intersect at some altitude. This intersection is the \"coffin corner\", or more formally the \"Q corner\".\n\nThe above explanation is based on level, constant speed, flight with a given gross weight and load factor of 1.0 G. The specific altitudes and speeds of the coffin corner will differ depending on weight, and the load factor increases caused by banking and pitching maneuvers. Similarly, the specific altitudes at which the stall speed meets the critical Mach number will differ depending on the actual atmospheric temperature.\n\nWhen an aircraft slows to below its stall speed, it is unable to generate enough lift in order to cancel out the forces that act on the aircraft (such as weight and centripetal force). This will cause the aircraft to drop in altitude. The drop in altitude may cause the pilot to increase the angle of attack by pulling up on the stick, because normally increasing the angle of attack puts the aircraft in a climb. However, when the wing exceeds its critical angle of attack, an increase in angle of attack will lead to a loss of lift and a further loss of airspeed - the wing stalls. The reason why the wing stalls when it exceeds its critical angle of attack is that the airflow over the top of the wing separates.\n\nWhen the airplane exceeds its critical Mach number (such as during stall prevention or recovery), then drag increases or Mach tuck occurs, which can cause the aircraft to upset, lose control, and lose altitude. In either case, as the airplane falls, it could gain speed and then structural failure could occur, typically due to excessive \"g\" forces during the pullout phase of the recovery.\n\nAs an airplane approaches its coffin corner, the margin between stall speed and critical Mach number becomes smaller and smaller. Small changes could put one wing or the other above or below the limits. For instance, a turn causes the inner wing to have a lower airspeed, and the outer wing, a higher airspeed. The aircraft could exceed both limits at once. Or, turbulence could cause the airspeed to change suddenly, to beyond the limits. Some aircraft, such as the Lockheed U-2, routinely operate in the \"coffin corner\". In the case of the U-2, the aircraft was equipped with an autopilot, though it was unreliable. The U-2's speed margin, at high altitude, between 1-G stall and Mach buffet can be as small as 5 knots.\n\nAircraft capable of flying close to their critical Mach number usually carry a machmeter, an instrument which indicates speed in Mach number terms. As part of certifying aircraft in the United States of America, the Federal Aviation Administration (FAA) certifies a maximum Mach number for operation, or M. \n\nFollowing a series of crashes of high performance aircraft operating at high altitudes to which no definite cause could be attributed, as the aircraft involved suffered near total destruction, the FAA published an Advisory Circular establishing guidelines for improved aircrew training in high altitude operations in high performance aircraft. The circular includes a comprehensive explanation of aerodynamic effects of, and operations near coffin corner.\n\n\n"}
{"id": "70570", "url": "https://en.wikipedia.org/wiki?curid=70570", "title": "Combined nuclear and steam propulsion", "text": "Combined nuclear and steam propulsion\n\nCombined nuclear and steam propulsion system (CONAS) is used on the guided missile cruisers.\n\nComplementary to the nuclear component there are two conventional boilers, installed as a backup in case of reactor failure. Both components are capable of driving two geared steam turbines, generating 120,000 hp (89 MW) at two prop shafts.\n"}
{"id": "37011244", "url": "https://en.wikipedia.org/wiki?curid=37011244", "title": "Conductor clashing", "text": "Conductor clashing\n\nConductor clashing is the phenomenon where conductors come in contact with one another during high wind speeds or gusts.\n\nConductor clashing may be assisted by the following external forces:\n"}
{"id": "58690", "url": "https://en.wikipedia.org/wiki?curid=58690", "title": "Crystal structure", "text": "Crystal structure\n\nIn crystallography, crystal structure is a description of the ordered arrangement of atoms, ions or molecules in a crystalline material. Ordered structures occur from the intrinsic nature of the constituent particles to form symmetric patterns that repeat along the principal directions of three-dimensional space in matter.\n\nThe smallest group of particles in the material that constitutes the repeating pattern is the unit cell of the structure. The unit cell completely defines the symmetry and structure of the entire crystal lattice, which is built up by repetitive translation of the unit cell along its principal axes. The repeating patterns are said to be located at the points of the Bravais lattice.\n\nThe lengths of the principal axes, or edges, of the unit cell and the angles between them are the lattice constants, also called \"lattice parameters\". The symmetry properties of the crystal are described by the concept of space groups. All possible symmetric arrangements of particles in three-dimensional space may be described by the 230 space groups.\n\nThe crystal structure and symmetry play a critical role in determining many physical properties, such as cleavage, electronic band structure, and optical transparency.\n\nCrystal structure is described in terms of the geometry of arrangement of particles in the unit cell. The unit cell is defined as the smallest repeating unit having the full symmetry of the crystal structure. The geometry of the unit cell is defined as a parallelepiped, providing six lattice parameters taken as the lengths of the cell edges (\"a\", \"b\", \"c\") and the angles between them (α, β, γ). The positions of particles inside the unit cell are described by the fractional coordinates (\"x\", \"y\", \"z\") along the cell edges, measured from a reference point. It is only necessary to report the coordinates of a smallest asymmetric subset of particles. This group of particles may be chosen so that it occupies the smallest physical space, which means that not all particles need to be physically located inside the boundaries given by the lattice parameters. All other particles of the unit cell are generated by the symmetry operations that characterize the symmetry of the unit cell. The collection of symmetry operations of the unit cell is expressed formally as the space group of the crystal structure.\n\nVectors and planes in a crystal lattice are described by the three-value Miller index notation. This syntax uses the indices \"ℓ\", \"m\", and \"n\" as directional orthogonal parameters, which are separated by 90°.\n\nBy definition, the syntax (\"ℓmn\") denotes a plane that intercepts the three points \"a\"/\"ℓ\", \"a\"/\"m\", and \"a\"/\"n\", or some multiple thereof. That is, the Miller indices are proportional to the inverses of the intercepts of the plane with the unit cell (in the basis of the lattice vectors). If one or more of the indices is zero, it means that the planes do not intersect that axis (i.e., the intercept is \"at infinity\"). A plane containing a coordinate axis is translated so that it no longer contains that axis before its Miller indices are determined. The Miller indices for a plane are integers with no common factors. Negative indices are indicated with horizontal bars, as in (13). In an orthogonal coordinate system for a cubic cell, the Miller indices of a plane are the Cartesian components of a vector normal to the plane.\n\nConsidering only (\"ℓmn\") planes intersecting one or more lattice points (the \"lattice planes\"), the distance \"d\" between adjacent lattice planes is related to the (shortest) reciprocal lattice vector orthogonal to the planes by the formula\n\nThe crystallographic directions are geometric lines linking nodes (atoms, ions or molecules) of a crystal. Likewise, the crystallographic planes are geometric \"planes\" linking nodes. Some directions and planes have a higher density of nodes. These high density planes have an influence on the behavior of the crystal as follows:\n\n\n\nSome directions and planes are defined by symmetry of the crystal system. In monoclinic, rhombohedral, tetragonal, and trigonal/hexagonal systems there is one unique axis (sometimes called the principal axis) which has higher rotational symmetry than the other two axes. The basal plane is the plane perpendicular to the principal axis in these crystal systems. For triclinic, orthorhombic, and cubic crystal systems the axis designation is arbitrary and there is no principal axis.\n\nFor the special case of simple cubic crystals, the lattice vectors are orthogonal and of equal length (usually denoted \"a\"); similarly for the reciprocal lattice. So, in this common case, the Miller indices (\"ℓmn\") and [\"ℓmn\"] both simply denote normals/directions in Cartesian coordinates. For cubic crystals with lattice constant \"a\", the spacing \"d\" between adjacent (ℓmn) lattice planes is (from above):\nBecause of the symmetry of cubic crystals, it is possible to change the place and sign of the integers and have equivalent directions and planes:\n\n\nFor face-centered cubic (fcc) and body-centered cubic (bcc) lattices, the primitive lattice vectors are not orthogonal. However, in these cases the Miller indices are conventionally defined relative to the lattice vectors of the cubic supercell and hence are again simply the Cartesian directions.\n\nThe spacing d between adjacent (\"hkl\") lattice planes is given by:\n\nThe defining property of a crystal is its inherent symmetry, by which we mean that under certain 'operations' the crystal remains unchanged. All crystals have translational symmetry in three directions, but some have other symmetry elements as well. For example, rotating the crystal 180° about a certain axis may result in an atomic configuration that is identical to the original configuration. The crystal is then said to have a twofold rotational symmetry about this axis. In addition to rotational symmetries like this, a crystal may have symmetries in the form of mirror planes and translational symmetries, and also the so-called \"compound symmetries,\" which are a combination of translation and rotation/mirror symmetries. A full classification of a crystal is achieved when all of these inherent symmetries of the crystal are identified.\n\nThese lattice systems are a grouping of crystal structures according to the axial system used to describe their lattice. Each lattice system consists of a set of three axes in a particular geometric arrangement. There are seven lattice systems. They are similar to but not quite the same as the seven crystal systems. \nThe simplest and most symmetric, the cubic (or isometric) system, has the symmetry of a cube, that is, it exhibits four threefold rotational axes oriented at 109.5° (the tetrahedral angle) with respect to each other. These threefold axes lie along the body diagonals of the cube. The other six lattice systems, are hexagonal, tetragonal, rhombohedral (often confused with the trigonal crystal system), orthorhombic, monoclinic and triclinic.\n\nBravais lattices, also referred to as \"space lattices\", describe the geometric arrangement of the lattice points, and therefore the translational symmetry of the crystal. The three dimensions of space afford 14 distinct Bravais lattices describing the translational symmetry. All crystalline materials recognized today, not including quasicrystals, fit in one of these arrangements. The fourteen three-dimensional lattices, classified by lattice system, are shown above.\n\nThe crystal structure consists of the same group of atoms, the \"basis\", positioned around each and every lattice point. This group of atoms therefore repeats indefinitely in three dimensions according to the arrangement of one of the Bravais lattices. The characteristic rotation and mirror symmetries of the unit cell is described by its crystallographic point group.\n\nA crystal system is a set of point groups in which the point groups themselves and their corresponding space groups are assigned to a lattice system. Of the 32 point groups that exist in three dimensions, most are assigned to only one lattice system, in which case the crystal system and lattice system both have the same name. However, five point groups are assigned to two lattice systems, rhombohedral and hexagonal, because both lattice systems exhibit threefold rotational symmetry. These point groups are assigned to the trigonal crystal system. \n\nIn total there are seven crystal systems: triclinic, monoclinic, orthorhombic, tetragonal, trigonal, hexagonal, and cubic.\n\nThe crystallographic point group or \"crystal class\" is the mathematical group comprising the symmetry operations that leave at least one point unmoved and that leave the appearance of the crystal structure unchanged. These symmetry operations include\n\n\nRotation axes (proper and improper), reflection planes, and centers of symmetry are collectively called \"symmetry elements\". There are 32 possible crystal classes. Each one can be classified into one of the seven crystal systems.\n\nIn addition to the operations of the point group, the space group of the crystal structure contains translational symmetry operations. These include:\n\nThere are 230 distinct space groups.\n\nBy considering the arrangement of atoms relative to each other, their coordination numbers (or number of nearest neighbors), interatomic distances, types of bonding, etc., it is possible to form a general view of the structures and alternative ways of visualizing them.\n\nThe principles involved can be understood by considering the most efficient way of packing together equal-sized spheres and stacking close-packed atomic planes in three dimensions. For example, if plane A lies beneath plane B, there are two possible ways of placing an additional atom on top of layer B. If an additional layer was placed directly over plane A, this would give rise to the following series:\nThis arrangement of atoms in a crystal structure is known as hexagonal close packing (hcp).\n\nIf, however, all three planes are staggered relative to each other and it is not until the fourth layer is positioned directly over plane A that the sequence is repeated, then the following sequence arises:\nThis type of structural arrangement is known as cubic close packing (ccp).\n\nThe unit cell of a ccp arrangement of atoms is the face-centered cubic (fcc) unit cell. This is not immediately obvious as the closely packed layers are parallel to the {111} planes of the fcc unit cell. There are four different orientations of the close-packed layers.\n\nThe packing efficiency can be worked out by calculating the total volume of the spheres and dividing by the volume of the cell as follows:\n\nThe 74% packing efficiency is the maximum density possible in unit cells constructed of spheres of only one size. Most crystalline forms of metallic elements are hcp, fcc, or bcc (body-centered cubic). The coordination number of atoms in hcp and fcc structures is 12 and its atomic packing factor (APF) is the number mentioned above, 0.74. This can be compared to the APF of a bcc structure, which is 0.68.\n\nGrain boundaries are interfaces where crystals of different orientations meet. A grain boundary is a single-phase interface, with crystals on each side of the boundary being identical except in orientation. The term \"crystallite boundary\" is sometimes, though rarely, used. Grain boundary areas contain those atoms that have been perturbed from their original lattice sites, dislocations, and impurities that have migrated to the lower energy grain boundary.\n\nTreating a grain boundary geometrically as an interface of a single crystal cut into two parts, one of which is rotated, we see that there are five variables required to define a grain boundary. The first two numbers come from the unit vector that specifies a rotation axis. The third number designates the angle of rotation of the grain. The final two numbers specify the plane of the grain boundary (or a unit vector that is normal to this plane).\n\nGrain boundaries disrupt the motion of dislocations through a material, so reducing crystallite size is a common way to improve strength, as described by the Hall–Petch relationship. Since grain boundaries are defects in the crystal structure they tend to decrease the electrical and thermal conductivity of the material. The high interfacial energy and relatively weak bonding in most grain boundaries often makes them preferred sites for the onset of corrosion and for the precipitation of new phases from the solid. They are also important to many of the mechanisms of creep.\n\nGrain boundaries are in general only a few nanometers wide. In common materials, crystallites are large enough that grain boundaries account for a small fraction of the material. However, very small grain sizes are achievable. In nanocrystalline solids, grain boundaries become a significant volume fraction of the material, with profound effects on such properties as diffusion and plasticity. In the limit of small crystallites, as the volume fraction of grain boundaries approaches 100%, the material ceases to have any crystalline character, and thus becomes an amorphous solid.\n\nReal crystals feature defects or irregularities in the ideal arrangements described above and it is these defects that critically determine many of the electrical and mechanical properties of real materials. When one atom substitutes for one of the principal atomic components within the crystal structure, alteration in the electrical and thermal properties of the material may ensue. Impurities may also manifest as spin impurities in certain materials. Research on magnetic impurities demonstrates that substantial alteration of certain properties such as specific heat may be affected by small concentrations of an impurity, as for example impurities in semiconducting ferromagnetic alloys may lead to different properties as first predicted in the late 1960s. Dislocations in the crystal lattice allow shear at lower stress than that needed for a perfect crystal structure.\n\nThe difficulty of predicting stable crystal structures based on the knowledge of only the chemical composition has long been a stumbling block on the way to fully computational materials design. Now, with more powerful algorithms and high-performance computing, structures of medium complexity can be predicted using such approaches as evolutionary algorithms, random sampling, or metadynamics.\n\nThe crystal structures of simple ionic solids (e.g., NaCl or table salt) have long been rationalized in terms of Pauling's rules, first set out in 1929 by Linus Pauling, referred to by many since as the \"father of the chemical bond\". Pauling also considered the nature of the interatomic forces in metals, and concluded that about half of the five d-orbitals in the transition metals are involved in bonding, with the remaining nonbonding d-orbitals being responsible for the magnetic properties. He, therefore, was able to correlate the number of d-orbitals in bond formation with the bond length as well as many of the physical properties of the substance. He subsequently introduced the metallic orbital, an extra orbital necessary to permit uninhibited resonance of valence bonds among various electronic structures.\n\nIn the resonating valence bond theory, the factors that determine the choice of one from among alternative crystal structures of a metal or intermetallic compound revolve around the energy of resonance of bonds among interatomic positions. It is clear that some modes of resonance would make larger contributions (be more mechanically stable than others), and that in particular a simple ratio of number of bonds to number of positions would be exceptional. The resulting principle is that a special stability is associated with the simplest ratios or \"bond numbers\": , , , , , etc. The choice of structure and the value of the axial ratio (which determines the relative bond lengths) are thus a result of the effort of an atom to use its valency in the formation of stable bonds with simple fractional bond numbers.\n\nAfter postulating a direct correlation between electron concentration and crystal structure in beta-phase alloys, Hume-Rothery analyzed the trends in melting points, compressibilities and bond lengths as a function of group number in the periodic table in order to establish a system of valencies of the transition elements in the metallic state. This treatment thus emphasized the increasing bond strength as a function of group number. The operation of directional forces were emphasized in one article on the relation between bond hybrids and the metallic structures. The resulting correlation between electronic and crystalline structures is summarized by a single parameter, the weight of the d-electrons per hybridized metallic orbital. The \"d-weight\" calculates out to 0.5, 0.7 and 0.9 for the fcc, hcp and bcc structures respectively. The relationship between d-electrons and crystal structure thus becomes apparent.\n\nIn crystal structure predictions/simulations, the periodicity is usually applied, since the system is imagined as unlimited big in all directions. Starting from a triclinic structure with no further symmetry property assumed, the system may be driven to show some additional symmetry properties by applying Newton's Second Law on particles in the unit cell and a recently developed dynamical equation for the system period vectors\n\nPolymorphism is the occurrence of multiple crystalline forms of a material. It is found in many crystalline materials including polymers, minerals, and metals. According to Gibbs' rules of phase equilibria, these unique crystalline phases are dependent on intensive variables such as pressure and temperature. Polymorphism is related to allotropy, which refers to elemental solids. The complete morphology of a material is described by polymorphism and other variables such as crystal habit, amorphous fraction or crystallographic defects. Polymorphs have different stabilities and may spontaneously convert from a metastable form (or thermodynamically unstable form) to the stable form at a particular temperature. They also exhibit different melting points, solubilities, and X-ray diffraction patterns.\n\nOne good example of this is the quartz form of silicon dioxide, or SiO. In the vast majority of silicates, the Si atom shows tetrahedral coordination by 4 oxygens. All but one of the crystalline forms involve tetrahedral {SiO} units linked together by shared vertices in different arrangements. In different minerals the tetrahedra show different degrees of networking and polymerization. For example, they occur singly, joined together in pairs, in larger finite clusters including rings, in chains, double chains, sheets, and three-dimensional frameworks. The minerals are classified into groups based on these structures. In each of the 7 thermodynamically stable crystalline forms or polymorphs of crystalline quartz, only 2 out of 4 of each the edges of the {SiO} tetrahedra are shared with others, yielding the net chemical formula for silica: SiO.\n\nAnother example is elemental tin (Sn), which is malleable near ambient temperatures but is brittle when cooled. This change in mechanical properties due to existence of its two major allotropes, α- and β-tin. The two allotropes that are encountered at normal pressure and temperature, α-tin and β-tin, are more commonly known as \"gray tin\" and \"white tin\" respectively. Two more allotropes, γ and σ, exist at temperatures above 161 °C and pressures above several GPa. White tin is metallic, and is the stable crystalline form at or above room temperature. Below 13.2 °C, tin exists in the gray form, which has a diamond cubic crystal structure, similar to diamond, silicon or germanium. Gray tin has no metallic properties at all, is a dull gray powdery material, and has few uses, other than a few specialized semiconductor applications. Although the α–β transformation temperature of tin is nominally 13.2 °C, impurities (e.g. Al, Zn, etc.) lower the transition temperature well below 0 °C, and upon addition of Sb or Bi the transformation may not occur at all.\n\nTwenty of the 32 crystal classes are piezoelectric, and crystals belonging to one of these classes (point groups) display piezoelectricity. All piezoelectric classes lack a center of symmetry. Any material develops a dielectric polarization when an electric field is applied, but a substance that has such a natural charge separation even in the absence of a field is called a polar material. Whether or not a material is polar is determined solely by its crystal structure. Only 10 of the 32 point groups are polar. All polar crystals are pyroelectric, so the 10 polar crystal classes are sometimes referred to as the pyroelectric classes.\n\nThere are a few crystal structures, notably the perovskite structure, which exhibit ferroelectric behavior. This is analogous to ferromagnetism, in that, in the absence of an electric field during production, the ferroelectric crystal does not exhibit a polarization. Upon the application of an electric field of sufficient magnitude, the crystal becomes permanently polarized. This polarization can be reversed by a sufficiently large counter-charge, in the same way that a ferromagnet can be reversed. However, although they are called ferroelectrics, the effect is due to the crystal structure (not the presence of a ferrous metal).\n\n"}
{"id": "8715293", "url": "https://en.wikipedia.org/wiki?curid=8715293", "title": "Curb feeler", "text": "Curb feeler\n\nCurb feelers or curb finders are springs or wires installed on a vehicle which act as \"whiskers\" to alert drivers when they are at the right distance from the curb while parking. \n\nThe devices are fitted low on the body, close to the wheels. As the vehicle approaches the curb, the protruding feelers scrape against the curb, making a noise and alerting the driver in time to avoid damaging the wheels or hubcaps. The feelers are manufactured to be flexible and do not break easily.\n\nCurb feelers are still used on some hot rods when a 1950s look is wanted. They are especially popular for cars with whitewall tires, which easily lose their white coating when scraped against the curb. Sometimes curb feelers are found only on the passenger side of the car, since that is most commonly near the curb when parking. Sometimes they are added only next to the front wheels. Some curb feelers have a single wire or spring, while others have two to increase the area that can be protected. Any particular car may have just one curb feeler installed (as on the pictures) or more if attached near the front and rear, as well as on both sides of the vehicle.\n\nRecreational vehicles sometimes have rubber feelers or metal, antenna-like rods mounted on the lower part of the body that act as feelers so that drivers are warned if they are approaching a curb or other obstruction, thus reducing the chances of gouging or even cutting the tire sidewalls and generally increasing the safety of vehicle operation.\n\nBuses are sometimes fitted with curb feelers, which can assist the driver in ensuring that the bus is close enough to the curb to allow passengers to step to and from the curb easily.\n\nToday, the U.S. Department of Labor Mine Safety and Health Administration mentions that users of heavy equipment can benefit from an accessory:\n\n...Using a piece of 48 inch conveyor belt, 4 to 5 feet long by 4 to 6 inches wide and a couple of pieces of angle iron, you can make a pinch-point feeler, a warning device for the corners of a continuous miner. This will give a warning nudge to anyone in the danger area, giving him or her about a two-foot running start to stop the machine or to yell at the operator to stop. The belting is stiff enough to hold its shape but flexible enough to give if it runs into a miner or vice versa. The flexibility also allows this \"curb feeler\" to drag against the rib or be smacked by a shuttle car with little or no damage. A little spray from a can of reflective paint will make the belt a visual warning device as well. One or two on each corner will help or put as many as you want.\nCurb feelers based on optical technology are designed to function the same way but work in the proximity of an obstruction rather than having to come into physical contact with it. As described by one United States patent:\nAn electronic curb feeler system uses two pairs of optical sensor units to detect an object located near the front end of a vehicle during parking. One pair of optical sensor units detects an object directly in front of a left portion of the front end of the vehicle while another pair of optical sensors detects an object directly in front of a right portion of the front end of the vehicle. By supplying the operator of the vehicle with the location of the object as well as the exact distance the object is from the front end of the vehicle the operator can avoid hitting the object while parking very close to the object.\n\n"}
{"id": "245963", "url": "https://en.wikipedia.org/wiki?curid=245963", "title": "Distributed generation", "text": "Distributed generation\n\nDistributed generation, also distributed energy, on-site generation (OSG) or district/decentralized energy is electrical generation and storage performed by a variety of small, grid-connected devices referred to as distributed energy resources (DER).\n\nConventional power stations, such as coal-fired, gas, and nuclear powered plants, as well as hydroelectric dams and large-scale solar power stations, are centralized and often require electric energy to be transmitted over long distances. By contrast, DER systems are decentralized, modular, and more flexible technologies, that are located close to the load they serve, albeit having capacities of only 10 megawatts (MW) or less. These systems can comprise multiple generation and storage components; in this instance they are referred to as hybrid power systems.\n\nDER systems typically use renewable energy sources, including small hydro, biomass, biogas, solar power, wind power, and geothermal power, and increasingly play an important role for the electric power distribution system. A grid-connected device for electricity storage can also be classified as a DER system and is often called a distributed energy storage system (DESS). By means of an interface, DER systems can be managed and coordinated within a smart grid. Distributed generation and storage enables collection of energy from many sources and may lower environmental impacts and improve security of supply.\n\nMicrogrids are modern, localized, small-scale grids, contrary to the traditional, centralized electricity grid (macrogrid). Microgrids can disconnect from the centralized grid and operate autonomously, strengthen grid resilience, and help mitigate grid disturbances. They are typically low-voltage AC grids, often use diesel generators, and are installed by the community they serve. Microgrids increasingly employ a mixture of different distributed energy resources, such as solar hybrid power systems, which reduce the amount of emitted carbon significantly.\n\nHistorically, central plants have been an integral part of the electric grid, in which large generating facilities are specifically located either close to resources or otherwise located far from populated load centers. These, in turn, supply the traditional transmission and distribution (T&D) grid that distributes bulk power to load centers and from there to consumers. These were developed when the costs of transporting fuel and integrating generating technologies into populated areas far exceeded the cost of developing T&D facilities and tariffs. Central plants are usually designed to take advantage of available economies of scale in a site-specific manner, and are built as \"one-off,\" custom projects.\n\nThese economies of scale began to fail in the late 1960s and, by the start of the 21st century, Central Plants could arguably no longer deliver competitively cheap and reliable electricity to more remote customers through the grid, because the plants had come to cost less than the grid and had become so reliable that nearly all power failures originated in the grid. Thus, the grid had become the main driver of remote customers’ power costs and power quality problems, which became more acute as digital equipment required extremely reliable electricity. Efficiency gains no longer come from increasing generating capacity, but from smaller units located closer to sites of demand.\n\nFor example, coal power plants are built away from cities to prevent their heavy air pollution from affecting the populace. In addition, such plants are often built near collieries to minimize the cost of transporting coal. Hydroelectric plants are by their nature limited to operating at sites with sufficient water flow.\n\nLow pollution is a crucial advantage of combined cycle plants that burn natural gas. The low pollution permits the plants to be near enough to a city to provide district heating and cooling.\n\nDistributed energy resources are mass-produced, small, and less site-specific. Their development arose out of:\n\nCapital markets have come to realize that right-sized resources, for individual customers, distribution substations, or microgrids, are able to offer important but little-known economic advantages over central plants. Smaller units offered greater economies from mass-production than big ones could gain through unit size. These increased value—due to improvements in financial risk, engineering flexibility, security, and environmental quality—of these resources can often more than offset their apparent cost disadvantages. DG, vis-à-vis central plants, must be justified on a life-cycle basis. Unfortunately, many of the direct, and virtually all of the indirect, benefits of DG are not captured within traditional utility cash-flow accounting.\n\nWhile the levelized cost of distributed generation (DG) is typically more expensive than conventional, centralized sources on a kilowatt-hour basis, this does not consider negative aspects of conventional fuels. The additional premium for DG is rapidly declining as demand increases and technology progresses, and sufficient and reliable demand may bring economies of scale, innovation, competition, and more flexible financing, that could make DG clean energy part of a more diversified future. \n\nDistributed generation reduces the amount of energy lost in transmitting electricity because the electricity is generated very near where it is used, perhaps even in the same building. This also reduces the size and number of power lines that must be constructed.\n\nTypical DER systems in a feed-in tariff (FIT) scheme have low maintenance, low pollution and high efficiencies. In the past, these traits required dedicated operating engineers and large complex plants to reduce pollution. However, modern embedded systems can provide these traits with automated operation and renewables, such as sunlight, wind and geothermal. This reduces the size of power plant that can show a profit.\n\nGrid parity occurs when an alternative energy source can generate electricity at a levelized cost (LCOE) that is less than or equal to the end consumer's retail price. Reaching grid parity is considered to be the point at which an energy source becomes a contender for widespread development without subsidies or government support. Since the 2010s, grid parity for solar and wind has become a reality in a growing number of markets, including Australia, several European countries, and some states in the U.S.\n\nDistributed energy resource (DER) systems are small-scale power generation or storage technologies (typically in the range of 1 kW to 10,000 kW) used to provide an alternative to or an enhancement of the traditional electric power system. DER systems typically are characterized by high initial capital costs per kilowatt. DER systems also serve as storage device and are often called \"Distributed energy storage systems\" (DESS).\n\nDER systems may include the following devices/technologies:\n\n\nDistributed cogeneration sources use steam turbines, natural gas-fired fuel cells, microturbines or reciprocating engines to turn generators. The hot exhaust is then used for space or water heating, or to drive an absorptive chiller for cooling such as air-conditioning. In addition to natural gas-based schemes, distributed energy projects can also include other renewable or low carbon fuels including biofuels, biogas, landfill gas, sewage gas, coal bed methane, syngas and associated petroleum gas.\n\nDelta-ee consultants stated in 2013 that with 64% of global sales, the fuel cell micro combined heat and power passed the conventional systems in sales in 2012. 20.000 units were sold in Japan in 2012 overall within the Ene Farm project. With a Lifetime of around 60,000 hours. For PEM fuel cell units, which shut down at night, this equates to an estimated lifetime of between ten and fifteen years. For a price of $22,600 before installation. For 2013 a state subsidy for 50,000 units is in place.\n\nIn addition, molten carbonate fuel cell and solid oxide fuel cells using natural gas, such as the ones from FuelCell Energy and the Bloom energy server, or waste-to-energy processes such as the Gate 5 Energy System are used as a distributed energy resource.\n\nPhotovoltaics, by far the most important solar technology for distributed generation of solar power, uses solar cells assembled into solar panels to convert sunlight into electricity. It is a fast-growing technology doubling its worldwide installed capacity every couple of years. PV systems range from distributed, residential, and commercial rooftop or building integrated installations, to large, centralized utility-scale photovoltaic power stations.\n\nThe predominant PV technology is crystalline silicon, while thin-film solar cell technology accounts for about 10 percent of global photovoltaic deployment. In recent years, PV technology has improved its sunlight to electricity conversion efficiency, reduced the installation cost per watt as well as its energy payback time (EPBT) and levelised cost of electricity (LCOE), and has reached grid parity in at least 19 different markets in 2014.\n\nAs most renewable energy sources and unlike coal and nuclear, solar PV is variable and non-dispatchable, but has no fuel costs, operating pollution, as well as greatly reduced mining-safety and operating-safety issues. It produces peak power around local noon each day and its capacity factor is around 20 percent.\n\nWind turbines can be distributed energy resources or they can be built at utility scale. These have low maintenance and low pollution, but distributed wind unlike utility-scale wind has much higher costs than other sources of energy. As with solar, wind energy is variable and non-dispatchable. Wind towers and generators have substantial insurable liabilities caused by high winds, but good operating safety. Distributed generation from wind hybrid power systems combines wind power with other DER systems. One such example is the integration of wind turbines into solar hybrid power systems, as wind tends to complement solar because the peak operating times for each system occur at different times of the day and year.\n\nHydroelectricity is the most widely used form of renewable energy and its potential has already been explored to a large extent or is compromised due to issues such as environmental impacts on fisheries, and increased demand for recreational access. However, using modern 21st century technology, such as wave power, can make large amounts of new hydropower capacity available, with minor environmental impact.\n\nModular and scalable \"Next generation kinetic energy turbines\" can be deployed in arrays to serve the needs on a residential, commercial, industrial, municipal or even regional scale. \"Microhydro kinetic generators\" neither require dams nor impoundments, as they utilize the kinetic energy of water motion, either waves or flow. No construction is needed on the shoreline or sea bed, which minimizes environmental impacts to habitats and simplifies the permitting process. Such power generation also has minimal environmental impact and non-traditional microhydro applications can be tethered to existing construction such as docks, piers, bridge abutments, or similar structures.\n\nMunicipal solid waste (MSW) and natural waste, such as sewage sludge, food waste and animal manure will decompose and discharge methane-containing gas that can be collected and used as fuel in gas turbines or micro turbines to produce electricity as a distributed energy resource. Additionally, a California-based company, Gate 5 Energy Partners, Inc. has developed a process that transforms natural waste materials, such as sewage sludge, into biofuel that can be combusted to power a steam turbine that produces power. This power can be used in lieu of grid-power at the waste source (such as a treatment plant, farm or dairy).\n\nA distributed energy resource is not limited to the generation of electricity but may also include a device to store distributed energy (DE). Distributed energy storage systems (DESS) applications include several types of battery, pumped hydro, compressed air, and thermal energy storage. Access to energy storage for commercial applications is easily accessible through programs such as Energy Storage as a Service (ESaaS).\n\nFor reasons of reliability, distributed generation resources would be interconnected to the same transmission grid as central stations. Various technical and economic issues occur in the integration of these resources into a grid. Technical problems arise in the areas of power quality, voltage stability, harmonics, reliability, protection, and control. Behavior of protective devices on the grid must be examined for all combinations of distributed and central station generation. A large scale deployment of distributed generation may affect grid-wide functions such as frequency control and allocation of reserves. As a result, smart grid functions, virtual power plants and grid energy storage such as power to gas stations are added to the grid.\n\nEach distributed generation resource has its own integration issues. Solar PV and wind power both have intermittent and unpredictable generation, so they create many stability issues for voltage and frequency. These voltage issues affect mechanical grid equipment, such as load tap changers, which respond too often and wear out much more quickly than utilities anticipated. Also, without any form of energy storage during times of high solar generation, companies must rapidly increase generation around the time of sunset to compensate for the loss of solar generation. This high ramp rate produces what the industry terms the \"duck curve\" (example) that is a major concern for grid operators in the future. Storage can fix these issues if it can be implemented. Flywheels have shown to provide excellent frequency regulation. Short term use batteries, at a large enough scale of use, can help to flatten the duck curve and prevent generator use fluctuation and can help to maintain voltage profile. However, cost is a major limiting factor for energy storage as each technique is prohibitively expensive to produce at scale and comparatively not energy dense compared to liquid fossil fuels. Finally, another necessary method of aiding in integration of photovoltaics for proper distributed generation is in the use of intelligent hybrid inverters.\n\nAnother approach does not demand grid integration: stand alone hybrid systems.\n\nIt is now possible to combine technologies such as photovoltaics, batteries and cogen to make stand alone distributed generation systems.\n\nRecent work has shown that such systems have a low levelized cost of electricity.\n\nMany authors now think that these technologies may enable a mass-scale grid defection because consumers can produce electricity using off grid systems primarily made up of solar photovoltaic technology. For example, the Rocky Mountain Institute has proposed that there may wide scale grid defection. This is backed up by studies in the Midwest.\n\nCogenerators are also more expensive per watt than central generators. They find favor because most buildings already burn fuels, and the cogeneration can extract more value from the fuel . Local production has no electricity transmission losses on long distance power lines or energy losses from the Joule effect in transformers where in general 8-15% of the energy is lost (see also cost of electricity by source).\n\nSome larger installations utilize combined cycle generation. Usually this consists of a gas turbine whose exhaust boils water for a steam turbine in a Rankine cycle. The condenser of the steam cycle provides the heat for space heating or an absorptive chiller. Combined cycle plants with cogeneration have the highest known thermal efficiencies, often exceeding 85%.\n\nIn countries with high pressure gas distribution, small turbines can be used to bring the gas pressure to domestic levels whilst extracting useful energy. If the UK were to implement this countrywide an additional 2-4 GWe would become available. (Note that the energy is already being generated elsewhere to provide the high initial gas pressure - this method simply distributes the energy via a different route.)\n\nA \"microgrid\" is a localized grouping of electricity generation, energy storage, and loads that normally operates connected to a traditional centralized grid (macrogrid). This single point of common coupling with the macrogrid can be disconnected. The microgrid can then function autonomously. Generation and loads in a microgrid are usually interconnected at low voltage and it can operate in DC, AC, or the combination of both. From the point of view of the grid operator, a connected microgrid can be controlled as if it were one entity.\n\nMicrogrid generation resources can include stationary batteries, fuel cells, solar, wind, or other energy sources. The multiple dispersed generation sources and ability to isolate the microgrid from a larger network would provide highly reliable electric power. Produced heat from generation sources such as microturbines could be used for local process heating or space heating, allowing flexible trade off between the needs for heat and electric power.\n\nMicro-grids were proposed in the wake of the July 2012 India blackout:\n\nGTM Research forecasts microgrid capacity in the United States will exceed 1.8 gigawatts by 2018.\n\nMicro-grids have seen implementation in a number of communities over the world. For example, Tesla has implemented a solar micro-grid in the Samoan island of Ta'u, powering the entire island with solar energy. This localized production system has helped save over 100,000 gallons of diesel fuel. It is also able to sustain the island for three whole days if the sun were not to shine at all during that period. This is a great example of how micro-grid systems can be implemented in communities to encourage renewable resource usage and localized production.\n\nTo plan and install Microgrids correctly, engineering modelling is needed. Multiple simulation tools and optimization tools exist to model the economic and electric effects of Microgrids. A widely used economic optimization tool is the Distributed Energy Resources Customer Adoption Model (DER-CAM) from Lawrence Berkeley National Laboratory. Another frequently used commercial economic modelling tool is Homer Energy, originally designed by the National Renewable Laboratory. There are also some power flow and electrical design tools guiding the Microgrid developers. The Pacific Northwest National Laboratory designed the public available GridLAB-D tool and the Electric Power Research Institute (EPRI) designed OpenDSS to simulate the distribution system (for Microgrids). A professional integrated DER-CAM and OpenDSS version is available via BankableEnergy. A European tool that can be used for electrical, cooling, heating, and process heat demand simulation is EnergyPLAN from the Aalborg University, Denmark.\n\n\nIn 2010 Colorado enacted a law requiring that by 2020 that 3% of the power generated in Colorado utilize distributed generation of some sort.\n\nOn 11 October 2017, California Governor Jerry Brown signed into law a bill, SB 338, that makes utility companies plan \"carbon-free alternatives to gas generation\" in order to meet peak demand. The law requires utilities to evaluate issues such as energy storage, efficiency, and distributed energy resources.\n\n\n"}
{"id": "44227694", "url": "https://en.wikipedia.org/wiki?curid=44227694", "title": "Douglas Point Conservation Park", "text": "Douglas Point Conservation Park\n\n"}
{"id": "45704946", "url": "https://en.wikipedia.org/wiki?curid=45704946", "title": "ETC Group (energy efficiency consultant)", "text": "ETC Group (energy efficiency consultant)\n\nETC Group provides energy efficiency consulting for commercial and industrial businesses. Energy savings are accomplished through engineering, analysis, education and advocacy.\n\nMark Case founded ETC Group in 1988.\n\n\n\n\n\n\n"}
{"id": "355802", "url": "https://en.wikipedia.org/wiki?curid=355802", "title": "Egg white", "text": "Egg white\n\nEgg white is the clear liquid (also called the albumen or the glair/glaire) contained within an egg. In chickens it is formed from the layers of secretions of the anterior section of the hen's oviduct during the passage of the egg. It forms around fertilized or unfertilized egg yolks. The primary natural purpose of egg white is to protect the yolk and provide additional nutrition for the growth of the embryo (when fertilized).\nEgg white consists primarily of about 90% water into which is dissolved about 10% proteins (including albumins, mucoproteins, and globulins). Unlike the yolk, which is high in lipids (fats), egg white contains almost no fat, and carbohydrate content is less than 1%. Egg whites contain about 56% of the protein in the egg. Egg white has many uses in food (e.g. meringue, mousse) and also many other uses (e.g. in the preparation of vaccines such as those for influenza).\n\nEgg white makes up around two-thirds of a chicken egg by weight. Water constitutes about 90% of this, with protein, trace minerals, fatty material, vitamins, and glucose contributing the remainder. A raw U.S. large egg contains around 33 grams of egg white with 3.6 grams of protein, 0.24 grams of carbohydrate and 55 milligrams of sodium. It contains no cholesterol and the energy content is about 17 Calories. Egg white is an alkaline solution and contains around 148 proteins. The table below lists the major proteins in egg whites by percentage and their natural functions.\nOvalbumin is the most abundant protein in albumen. Classed as phosphoglycoprotein, during storage, it converts into s-ovalbumin (5% at the time of laying) and can reach up to 80% after six months of cold storage. Ovalbumin in solution is heat-resistant. Denaturation temperature is around 84°C, but it can be easily denatured by physical stresses.\nConalbumin/ovotransferrin is a glycoprotein which has the capacity to bind the bi- and trivalent metal cations into a complex and is more heat sensitive than ovalbumin. At its isoelectric pH (6.5), it can bind two cations and assume a red or yellow color. These metal complexes are more heat stable than the native state. Ovomucoid is the major allergen from egg white and is a heat-resistant glycoprotein found to be a trypsin inhibitor. Lysozyme is a holoprotein which can lyse the wall of certain Gram-positive bacteria and is found at high levels in the chalaziferous layer and the chalazae which anchor the yolk towards the middle of the egg. Ovomucin is a glycoprotein which may contribute to the gel-like structure of thick albumen. The amount of ovomucin in the thick albumen is four times greater than in the thin albumen.\n\nThe physical stress of beating egg whites can create a foam. Two types of physical stress are caused by beating them with a whisk, the first of which occurs as the whisk drags the liquid through itself, creating a force that unfolds the protein molecules. This process is called denaturation. The second stress comes from the mixing of air into the whites, which causes the proteins to come out of their natural state. These denatured proteins gather together where the air and water meet and create multiple bonds with the other unraveled proteins, and thus become a foam, holding the incorporated air in place, because the proteins consist of amino acids; some are hydrophilic (attracted to water) and some are hydrophobic (repelled by water). This process is called coagulation.\n\nWhen beating egg whites, they are classified in three stages according to the peaks they form when the beater is lifted: soft, firm, and stiff peaks. Overbeaten eggs take on a dry appearance, and eventually collapse. Egg whites do not beat up correctly if they are exposed to any form of fat, such as cooking oils or the fats contained in egg yolk.\n\nCopper bowls have been used in France since the 18th century to stabilize egg foams. The copper in the bowl assists in creating a tighter bond in reactive sulfur items such as egg whites. The bond created is so tight that the sulfurs are prevented from reacting with any other material. A silver-plated bowl has the same result as the copper bowl, as will a pinch of powdered copper supplement from a health store used in a glass bowl. Drawbacks of the copper bowl include the expense of the bowl itself, and the bowls are difficult to keep clean. Copper contamination from the bowl is minimal, as a cup of foam contains a tenth of a human's normal daily intake level.\nAlthough egg whites are prized as a source of low-fat, high-protein nutrition, a small number of people cannot eat them. Egg allergy is more common among infants than adults, and most children will outgrow it by the age of five. Allergic reactions against egg white are more common than reactions against egg yolks. In addition to true allergic reactions, some people experience a food intolerance to egg whites.\n\nEggs are susceptible to \"Salmonella\" contamination. Thorough cooking eliminates the direct threat (i.e. cooked egg whites that are solid and not runny), the threat of cross-contamination remains if people handle contaminated eggs and then touch other foods or items in the kitchen, thus spreading the bacteria. In August 2010, the FDA ordered the recall of 380 million eggs because of possible \"Salmonella\" contamination.\n\nEgg white is a fining agent that can be used in the clarification and stabilization of wine. Egg white can also be added to shaken cocktails to create a delicate froth. Some protein powders also use egg whites as a primary source of protein.\n\nIn the 1750s, egg whites were believed to prevent swelling, and were used for that purpose. To help soothe areas of skin that were afflicted, egg white mixed with Armenic bole could help restore the fibers. Egg whites are also used in bookbinding during the gilding process, where it is referred to as 'glaire', and to give a book cover shine.\n\n\n\n"}
{"id": "11298808", "url": "https://en.wikipedia.org/wiki?curid=11298808", "title": "Energy Globe Award", "text": "Energy Globe Award\n\nThe International Energy Globe Awards (the World Awards for Sustainability or Nature's Nobel Prize) have been awarded by the \"Energy Globe Foundation\" annually to recognise projects that 'make careful and economical use of resources and employ alternative energy sources.' The winners, in the categories of Earth, Fire, Water, Air and Youth, are selected by a panel including members from the United Nations Industrial Development Organization, World Bank and the European Renewable Energy Council. In 2016, the category Sustainable Plastics was added to the over five categories. The awards were an initiative by Austrian engineer and environmentalist Wolfgang Neumann. The herculean status of the Award often equates it to Nobel Prize.\n\nIt distinguishes projects regionally, nationally and globally every year since 2000.\n\nThe goal of the Energy Globe Award is to create the necessary awareness concerning solutions to our environmental problems and to demonstrate that each of us can make a positive contribution. The winning ENERGY GLOBE projects serve as examples, which are presented on the Energy Globe website and in the Energy Globe project database. The monetary prize for first place in the international award is 10,000 euro which is distributed among the six categories.\n\nThe (inter)national ENERGY GLOBE winner projects are honored at festive ceremonies and presented in the media and on television. Large-scale awards were held in 2007 and 2008 in the Plenary Hall of the European Parliament in Brussels with the support of prominent people from all over the world. 2009 the Energy Globe gala was the opening event of the informal meeting of the EU Environmental Ministers in Prague. 2010 Energy Globe together with UNEP has opened the UN World Environment Day\nin Kigali/Rwanda. The 2014/2015 celebration took place in Tehran/Iran. More than 1000 projects from all over the world are submitted each year. In all some 7000 projects from 177 countries have been submitted for the Energy Globe Award.\n\nThe ENERGY GLOBE Jury is headed by Maneka Gandhi (Member of Parliament), incumbent Indian minister for Ministry of Women and Child Development and former Indian Minister for Environment.\n\nInternational personalities such as former Secretary General of the UN Kofi Annan, former Russian President Mikhail Gorbachev, the President of the European Parliament and EU Commission, world stars like actor Martin Sheen, “Columbo” Peter Falk, Bollywood star Aamir Khan, musicians like Alanis Morissette, Dionne Warwick, Zucchero, Robin Gibb (Bee Gees), etc., offer their services for Energy Globe.\n\nHridith Sudev, a then ninth grader from Indian School Salalah in Oman won the Energy Globe National Award 2014 for his initiative Project GreenOman, setting record as the youngest ever to do so (at age 14). The project later grew into Project GreenWorld International, the World's biggest children's environmental organization.\nOther recipients include:\nOther national and international winners can be seen here.\n\n\n\n"}
{"id": "43489576", "url": "https://en.wikipedia.org/wiki?curid=43489576", "title": "First Wind", "text": "First Wind\n\nFounded in 2002, First Wind was an independent renewable energy company that develops and operates utility-scale power projects in the United States. It grew to employ more than 220 people across the country.\n\nHeadquartered in Boston, the company develops or operates renewable energy projects in Maine, Vermont, Massachusetts, New York, Utah, Idaho, Washington and Hawaii with a combined capacity of nearly 1,300 megawatts. In addition to renewable energy projects, the company developed and operates two generator leads.\n\nFirst Wind began as a utility-scale wind power developer and operator, but in 2014 the company brought its first solar energy projects online.\n\nIn November 2014 First Wind was purchased by SunEdison and its yieldco TerraForm Power for $2.4 billion.\n\nIn February 2015, as part of the merger integration, First Wind CEO Paul Gaynor, who led the company since 2004, was appointed EVP Of North America Utility And Global Wind at SunEdison.\n\n"}
{"id": "29003420", "url": "https://en.wikipedia.org/wiki?curid=29003420", "title": "Ford Comuta", "text": "Ford Comuta\n\nThe Ford Comuta was an experimental electric vehicle designed by Ford in the 1960s. The vehicle was powered by four 12-volt 85-Ah lead batteries.\n\nWhen it was fully charged, the car had a range of at a speed of , and was capable of a maximum speed of . Only a handful Comutas were produced, as the vehicle was an experiment.\n"}
{"id": "4487998", "url": "https://en.wikipedia.org/wiki?curid=4487998", "title": "Forest integrated pest management", "text": "Forest integrated pest management\n\nForest integrated pest management or Forest IPM is the practice of monitoring and managing pest and environmental information with pest control methods to prevent pest damage to forests and forest habitats by the most economical means. Forest IPM practices vary from region to region and particularly by state, according to the habitat and forests present. Forest integrated pest management or Forest IPM combines cultural, biological and chemical technologies to reduce pest damage to levels below those that of economic damage. Forest IPM is utilized for the whole life of the tree, from site prep to harvest. An IPM treatment is utilized before the cost of the treatment is equal to the reduction in crop value due to past injury, which is called the economic injury level. Forest integrated pest management has a strong emphasis on intensive forest management. \n\nThe main components of forest integrated pest management are how pest populations change over time, forest stand susceptibility and resistance to pests, pest impact on crop value, and control strategies. Forest IPM is designed to provide the information needed to deal with multiple pest problems in a way that promotes forest management objectives.\nIn the state of Vermont, two common pests are of particular significance, the Hemlock Woolly Adelgid or HWA, and thrips.\n\nPreventative practices of forest integrated pest management include training, detection, diagnosis and evaluation and exclusion. These are actions that can be done to prevent pest infestations from reaching levels of concern. Training employees to find early signs of pest occurrence will help landowners find and remove infected trees before the infestation spreads. Cleaning equipment before moving to different stands and when first brought onto the property will remove any contaminates that could affect the forest.\n\nIntensive forest management can be described as the cultural practices in forest integrated pest management. Cultural practices are done during the growth of the crop trees and also in the form of site prep and harvest practices. These practices range from choosing a good site with adequate drainage to the having an appropriate tree per acer (TPA). Seedling density is very important in tree nurseries. Having seedlings with a too high density promotes foliage disease fungi since airflow is reduced. The seedlings will also have more competition between themselves with a higher density, and this will reduce the quality of each seedling. The recommended density for longleaf pine is 50-90 per square meter and loblolly and slash pine at less than 215 seedlings per square meter. \n\nThe act of releasing native or nonnative predators of pest species is uncommon in forest integrated pest management. It would not be economically feasible to utilize this method in a forestry scenario. Instead, the forest and its surrounding area is managed in a way to promote natural enemies of pest species. In a study of eucalyptus plantations in Brazil, the closer the plantation is to a \"natural\" more diverse forest, the more diverse and balanced the insect populations are. This more diverse environment is less likely for produce an outbreak of pest species. Bacillus thuringiensis (Bt) is a bacterium that is used to kill larvae of many pest species. This can be applied aerially or on the ground over large surfaces. Bt is safe for humans and other wildlife since it only infects certain species of insects.\n\nIntegrated pest management was designed after Rachael Carson's \"Silent Spring\" as a way to manage for pests without overusing pesticides. Chemicals are still used but in a way that focuses on proper use of pesticide application so overuse does not occur. The fungicide triadimefon is applied to loblolly and slash pine seeds to prevent fusiform rust. If the seeds used are not treated for fusiform rust, the stand can lose 1-30% of their trees to the rust. Longleaf pine is naturally resistant to fusiform rust so their seeds are not treated with triadimefon. Southern pine nurseries fumigate the soil every four years and hardwood nurseries fumigate yearly. Fumigation promotes growth of beneficial fungus Trichoderma and less of the harmful Phythium fungus. In forest integrated pest management, insecticides are not applied until an infestation is observed. Herbicides are used to control weeds in nurseries. Low rates and frequent application intervals of herbicide is what is recommended.\n\nEucalyptus plantations provide a unique management challenge since they are commonly grown out of their native range and also in monocultures of clones. Large homogeneous areas of one species of tree are more susceptible of pest attacks. This is due to an abundant and predictable amount of food for the pest, and an absence of their natural enemies. There are less predator species in monocultures because the plants they need to forage, rest and lay eggs on are not there. This combination is why monoculture environments favor certain species and amplify their numbers to pest levels. The biggest pest concern with eucalyptus plantations in Brazil are leaf-cutting ants and Lepidopteran (moth and butterfly) defoliators. Leaf-cutting ants contribute the most amount of damage to these plantations. The insecticide methyl bromide is needed to fight leaf-cutting ants, with application done within a month of planting. Periodic applications during the rest of the rotation may be necessary. Monitoring leaf-cutting ant activity is crucial in eucalyptus plantations. Keeping a vegetative understory can also lower leaf-cutting ant numbers. Lepidoptera larva can be controlled with Bt application. Using a tractor is a more efficient than aerially applying Bt since the canopy is so closed and the Bt needs to reach the ground where the larva are. Traps are used to monitor larva levels to help better manage for them.\n\n\n"}
{"id": "33109056", "url": "https://en.wikipedia.org/wiki?curid=33109056", "title": "Germanide", "text": "Germanide\n\nA germanide is any binary compound of germanium and a more electropositive element. The composition of most germanides is analogous to that of the corresponding silicides and does not follow formal valence rules. The germanides of alkali and alkaline earth metals, are readily decomposed by water and acids to give germanium hydrides; most germanides of the transition metals resist the action of acids and alkalies. The main method of producing germanides is the melting or sintering of the components.\n\n"}
{"id": "3669984", "url": "https://en.wikipedia.org/wiki?curid=3669984", "title": "Grafting wax", "text": "Grafting wax\n\nGrafting wax is a composition of rosin, beeswax, tallow, and similar materials, used in gluing and sealing the wounds of newly grafted trees or shrubs to protect them from infection. The current formulation typically used in the northwestern portion of the United States for fruit trees, is based on a mixture created by Albert Sak, a German-from-Russia immigrant. The exact original composition is a closely guarded family secret.\n"}
{"id": "5292034", "url": "https://en.wikipedia.org/wiki?curid=5292034", "title": "Indium(III) sulfide", "text": "Indium(III) sulfide\n\nIndium(III) sulfide (Indium sesquisulfide, Indium sulfide (2:3), Indium (3+) sulfide) is the inorganic compound with the formula InS.\n\nIt has a \"rotten egg\" odor characteristic of sulfur compounds, and produces hydrogen sulfide gas when reacted with mineral acids.\n\nThree different structures (\"polymorphs\") are known: yellow, α-InS has a defect cubic structure, red β-InS has a defect spinel, tetragonal, structure, and γ-InS has a layered structure. The red, β, form is considered to be the most stable form at room temperature, although the yellow form may be present depending on the method of production. InS is attacked by acids and by sulfide. It is slightly soluble in NaS.\n\nIndium sulfide was the first indium compound ever described, being reported in 1863. Reich and Richter determined the existence of indium as a new element from the sulfide precipitate.\n\nInS features tetrahedral In(III) centers linked to four sulfido ligands.\n\nα-InS has a defect cubic structure. The polymorph undergoes a phase transition at 420 °C and converts to the spinel structure of β-InS. Another phase transition at 740 °C produces the layered γ-InS polymorph.\n\nβ-InS has a defect spinel structure. The sulfide anions are closely packed in layers, with octahedrally-coordinated In(III) cations present within the layers, and tetrahedrally-coordinated In(III) cations between them. A portion of the tetrahedral interstices are vacant, which leads to the defects in the spinel.\n\nβ-InS has two subtypes. In the T-InS subtype, the tetragonally-coordinated vacancies are in an ordered arrangement, whereas the vacancies in C-InS are disordered. The disordered subtype of β-InS shows activity for photocatalytic H production with a noble metal cocatalyst, but the ordered subtype does not.\n\nβ-InS is an N-type semiconductor with an optical band gap of 2.1 eV. It has been proposed to replace the hazardous cadmium sulfide, CdS, as a buffer layer in solar cells, and as an additional semiconductor to increase the performance of TiO-based photovoltaics.\n\nThe unstable γ-InS polymorph has a layered structure.\n\nIndium sulfide is usually prepared by direct combination of the elements.\n\nProduction from volatile complexes of indium and sulfur, for example dithiocarbamates (e.g. EtInSCNEt), has been explored for vapor deposition techniques.\n\nThin films of the beta complex can be grown by chemical spray pyrolysis. Solutions of In(III) salts and organic sulfur compounds (often thiourea) are sprayed onto preheated glass plates, where the chemicals react to form thin films of indium sulfide. Changing the temperature at which the chemicals are deposited and the In:S ratio can affect the optical band gap of the film.\n\nSingle-walled indium sulfide nanotubes can be formed in the laboratory, by the use of two solvents (one in which the compound dissolves poorly and one in which it dissolves well). There is partial replacement of the sulfido ligands with O, and the compound forms thin nanocoils, which self-assemble into arrays of nanotubes with diameters on the order of 10 nm, and walls approximately 0.6 nm thick. The process mimics protein crystallization.\nThe β-InS polymorph, in powdered form, can irritate eyes, skin and respiratory organs. It is toxic if swallowed, but can be handled safely under conventional laboratory conditions. It should be handled with gloves, and care should be taken to keep from inhaling the compound, and to keep it from contact with the eyes.\n\nThere is considerable interest in using InS to replace the semiconductor CdS (cadmium sulfide) in photoelectronic devices. β-InS has a tunable band gap, which makes it attractive for photovoltaic applications, and it shows promise when used in conjunction with TiO in solar panels, indicating that it could replace CdS in that application as well. Cadmium sulfide is toxic and must be deposited with a chemical bath, but indium(III) sulfide shows few adverse biological effects and can be deposited as a thin film through less hazardous methods.\n\nThin films β-InS can be grown with varying band gaps, which make them widely applicable as photovoltaic semiconductors, especially in heterojunction solar cells.\n\nPlates coated with beta-InS nanoparticles can be used efficiently for PEC (photoelectrochemical) water splitting.\n\nA preparation of indium sulfide made with the radioactive In can be used as a lung scanning agent for medical imaging. It is taken up well by lung tissues, but does not accumulate there.\n\nInS nanoparticles luminesce in the visible spectrum. Preparing InS nanoparticles in the presence of other heavy metal ions creates highly efficient blue, green, and red phosphors, which can be used in projectors and instrument displays.\n\n"}
{"id": "25986184", "url": "https://en.wikipedia.org/wiki?curid=25986184", "title": "Integral Urban House", "text": "Integral Urban House\n\nThe Integral Urban House was a pioneering 1970s experiment in self-reliant urban homesteading. The house was located 1516 5th St. in Berkeley, California. The founders were California State Architect Sim Van der Ryn and Bill & Helga Olkowski, authors of the \"City People's Guide to Raising Food\", and the project was run by the Farallones Institute (which Van der Ryn founded).\n\nThe Sierra Club published a book about the experiment in 1979. Elements of the home included a vegetable garden, chickens, rabbits, a fish pond, beehives, a composting toilet, solar power and more.\n\n\"Mother Earth News\" published an article on the house in 1976.\n"}
{"id": "42935859", "url": "https://en.wikipedia.org/wiki?curid=42935859", "title": "Ketchikan Pulp Company", "text": "Ketchikan Pulp Company\n\nKetchikan Pulp Company was a pulp mill located on the north shore of Ward Cove, from Ketchikan, in the U.S. state of Alaska. Owned by Louisiana-Pacific, it operated between 1954 and 1997. It was the last pulp mill to operate in the state.\n\nAlong with the Sitka pulp mill, the mill was built as part of a U.S. Forest Service economic development program for Southeast Alaska. Feedstock for the mill was harvested from the Tongass National Forest under guaranteed 50-year supply contracts that enabled the private companies to commit the large development investments in an area with only one log supplier (the USFS). This became controversial in the late 1980s, due to environmental concerns with the scale of old-growth forest harvesting and uninformed criticism of the alleged multimillion-dollar subsidies. In 1990, the Tongass Timber Reform Act directed the agency to terminate the long-term timber contracts, and both mills closed shortly thereafter. The last bale of pulp paper to come off the mill is on display at the Southeast Alaska Discovery Center in Ketchikan.\n"}
{"id": "284289", "url": "https://en.wikipedia.org/wiki?curid=284289", "title": "List of rock types", "text": "List of rock types\n\nThe following is a list of rock types recognized by geologists. There is no agreed number of specific types of rocks. Any unique combination of chemical composition, mineralogy, grain size, texture, or other distinguishing characteristics can describe rock types. Additionally, different classification systems exist for each major type of rock. There are three major types of rock: igneous, sedimentary, and metamorphic. They are all identified by their texture, streak, and location, among other factors.\n\n\n\n\nThe following are terms for rocks that are not petrographically or genetically distinct but are defined according to various other criteria; most are specific classes of other rocks, or altered versions of existing rocks. Some archaic and vernacular terms for rocks are also included.\n\n\n\n"}
{"id": "43225941", "url": "https://en.wikipedia.org/wiki?curid=43225941", "title": "Marsh Award for Conservation Biology", "text": "Marsh Award for Conservation Biology\n\nThe Marsh Award for Conservation Biology, established 1991, is an Award run in partnership between the Zoological Society of London (ZSL) and the Marsh Christian Trust that recognises an individual for his or her \"contributions of fundamental science to the conservation of animal species and habitats\".\n\n"}
{"id": "44446909", "url": "https://en.wikipedia.org/wiki?curid=44446909", "title": "National Commission for State Regulation of Energy and Public Utilities", "text": "National Commission for State Regulation of Energy and Public Utilities\n\nThe National Commission for State Regulation of Energy and Public Utilities is the state collegial body subordinated to the President of Ukraine and accountable to the Parliament of Ukraine. The Commission was created by Presidential Decree №715/2014 on 10 September 2014. The commission was created to combine the previously separate National Commission for State Regulation of Public Utilities and National Commission for State Regulation of Energy and Utilities.\n\nAccording to the Decree of the President, the Commission:\n\n\n"}
{"id": "45394", "url": "https://en.wikipedia.org/wiki?curid=45394", "title": "Neotropical realm", "text": "Neotropical realm\n\nThe Neotropical realm is one of the eight biogeographic realms constituting the Earth's land surface. Physically, it includes the tropical terrestrial ecoregions of the Americas and the entire South American temperate zone.\n\nIn biogeography, the Neotropic or Neotropical realm is one of the eight terrestrial realms. This realm includes South and the North American regions of Central America; in Mexico, the Yucatan Peninsula and southern lowlands, and most of the east and west coastlines, including the southern tip of the Baja California Peninsula; the Caribbean islands, southern Florida, and the coastal portion of the Río Grande Valley in South Texas, because these regions share a large number of plant and animal groups.\n\nThe realm also includes temperate southern South America. In contrast, the Neotropical Floristic Kingdom excludes southernmost South America, which instead is placed in the Antarctic kingdom.\n\nThe Neotropic is delimited by similarities in fauna or flora. Its fauna and flora are distinct from the Nearctic (which includes most of North America) because of the long separation of the two continents. The formation of the Isthmus of Panama joined the two continents two to three million years ago, precipitating the Great American Interchange, an important biogeographical event.\n\nThe Neotropic includes more tropical rainforest (tropical and subtropical moist broadleaf forests) than any other realm, extending from southern Mexico through Central America and northern South America to southern Brazil, including the vast Amazon Rainforest. These rainforest ecoregions are one of the most important reserves of biodiversity on Earth. These rainforests are also home to a diverse array of indigenous peoples, who to varying degrees persist in their autonomous and traditional cultures and subsistence within this environment. The number of these peoples who are as yet relatively untouched by external influences continues to decline significantly, however, along with the near-exponential expansion of urbanization, roads, pastoralism and forest industries which encroach on their customary lands and environment. Nevertheless, amidst these declining circumstances this vast \"reservoir\" of human diversity continues to survive, albeit much depleted. In South America alone, some 350–400 indigenous languages and dialects are still living (down from an estimated 1,500 at the time of first European contact), in about 37 distinct language families and a further number of unclassified and isolate languages. Many of these languages and their cultures are also endangered. Accordingly, conservation in the Neotropical realm is a hot political concern, and raises many arguments about development versus indigenous versus ecological rights and access to or ownership of natural resources.\n\nThe WWF subdivides the realm into \"bioregions\", defined as \"geographic clusters of ecoregions that may span several habitat types, but have strong biogeographic affinities, particularly at taxonomic levels higher than the species level (genus, family).\"\n\nLaurel forest and other cloud forest are subtropical and mild temperate forest, found in areas with high humidity and relatively stable and mild temperatures. Tropical rainforest, tropical and subtropical moist broadleaf forests are highlight in Southern North America, Amazonia, Caribbean, Central America, Northern Andes and Central Andes.\n\nThe Amazonia bioregion is mostly covered by tropical moist broadleaf forest, including the vast Amazon rainforest, which stretches from the Andes mountains to the Atlantic Ocean, and the lowland forests of the Guianas. The bioregion also includes tropical savanna and tropical dry forest ecoregions.\n\nEastern South America includes the Caatinga xeric shrublands of northeastern Brazil, the broad Cerrado grasslands and savannas of the Brazilian Plateau, and the Pantanal and Chaco grasslands. The diverse Atlantic forests of eastern Brazil are separated from the forests of Amazonia by the Caatinga and Cerrado, and are home to a distinct flora and fauna.\n\nThe Orinoco is a region of humid forested broadleaf forest and wetland primarily comprising the drainage basin for the Orinoco River and other adjacent lowland forested areas. This region includes most of Venezuela and parts of Colombia.\n\nThe temperate forest ecoregions of southwestern South America, including the temperate rain forests of the Valdivian temperate rain forests and Magellanic subpolar forests ecoregions, and the Juan Fernández Islands and Desventuradas Islands, are a refuge for the ancient Antarctic flora, which includes trees like the southern beech (\"Nothofagus\"), podocarps, the alerce (\"Fitzroya cupressoides\"), and Araucaria pines like the monkey-puzzle tree (\"Araucaria araucana\"). These magnificent rainforests are endangered by extensive logging and their replacement by fast-growing non-native pines and eucalyptus.\n\nSouth America was originally part of the supercontinent of Gondwana, which included Africa, Australia, India, New Zealand, and Antarctica, and the Neotropic shares many plant and animal lineages with these other continents, including marsupial mammals and the Antarctic flora.\n\nAfter the final breakup of the Gondwana about 110 million years ago, South America was separated from Africa and drifted north and west. Much later, about two to three million years ago, South America was joined with North America by the formation of the Isthmus of Panama, which allowed a biotic exchange between the two continents, the Great American Interchange. South American species like the ancestors of the Virginia opossum (\"Didelphis virginiana\") and the armadillo moved into North America, and North Americans like the ancestors of South America's camelids, including the llama (\"Lama glama\"), moved south. The long-term effect of the exchange was the extinction of many South American species, mostly by outcompetition by northern species.\n\nThere are 31 bird families that are endemic to the Neotropical realm, over twice the number of any other realm. They include tanagers, rheas, tinamous, curassows, antbirds, ovenbirds, and toucans. Bird families originally unique to the Neotropics include hummingbirds (family Trochilidae) and wrens (family Troglodytidae).\n\nMammal groups originally unique to the Neotropics include:\n\nThere are 63 fish families and subfamilies are endemic to the Neotropical realm, more than any other realm (van der Sleen and Albert, 2018 [van der Sleen, Peter, and James S. Albert, eds. Field Guide to the Fishes of the Amazon, Orinoco, and Guianas. Princeton University Press, 2017]). Neotropical fishes include more than 5,700 species, and represent at least 66 distinct lineages in continental freshwaters (Albert and Reis, 2011). The well-known red-bellied piranha is endemic to the Neotropic realm, occupying a larger geographic area than any other piranha species. Some fish groups originally unique to the Neotropics include:\n\n\nExamples of other animal groups that are entirely or mainly restricted to the Neotropical region include:\n\n\nPlant families endemic and partly subendemic to the realm are, according to Takhtajan (1978), Hymenophyllopsidaceae, Marcgraviaceae, Caryocaraceae, Pellicieraceae, Quiinaceae, Peridiscaceae, Bixaceae, Cochlospermaceae, Tovariaceae, Lissocarpaceae (\"Lissocarpa\"), Brunelliaceae, Dulongiaceae, Columelliaceae, Julianiaceae, Picrodendraceae, Goupiaceae, Desfontainiaceae, Plocospermataceae, Dialypetalanthaceae (\"Dialypetalanthus\"), Nolanaceae (\"Nolana\"), Calyceraceae, Heliconiaceae, Cannaceae, Thurniaceae and Cyclanthaceae.\n\nPlant families that originated in the Neotropic include Bromeliaceae, Cannaceae and Heliconiaceae. \n\nPlant species with economic importance originally unique to the Neotropic include:\n\n\n"}
{"id": "26096318", "url": "https://en.wikipedia.org/wiki?curid=26096318", "title": "Nord Pool AS", "text": "Nord Pool AS\n\nNord Pool AS runs the largest market for electrical energy in Europe, measured in volume traded (TWh) and in market share. It operates in Norway, Denmark, Sweden, Finland, Estonia, Latvia, Lithuania, Germany and the UK and is a Nominated Electricity Market Operator (NEMO) in 15 European countries, while also servicing power markets in Croatia and Bulgaria. More than 80% of the total consumption of electrical energy in the Nordic market is traded through Nord Pool. It was the world's first multinational exchange for trading electric power.\n\nNord Pool offers both day-ahead and intraday markets.\n\nNord Pool traces its origin to ', a power exchange formed by eastern Norwegian electricity companies in 1932 on the initiative of Augustin Paus, and which soon encompassed all the electricity companies of eastern Norway. In 1971 the exchange merged with the regional exchanges in other parts of Norway, and became ', with 118 power companies as members as of 1988.\n\nNord Pool Spot AS was established in 2002, when the derivatives and energy markets were separated from Nord Pool ASA (now NASDAQ OMX Commodities Europe). All Nord Pool's physical energy market activities were transferred into Nord Pool Spot AS. In 2005, Nord Pool Spot expanded its activities to Germany by opening the KONTEK bidding area. \n\nOn 12 January 2010, Nord Pool Spot in cooperation with NASDAQ OMX Commodities launched the N2EX power market in the United Kingdom. On 2 February 2010, Nord Pool Spot signed an agreement with the Estonian national grid company Elering to create the Nord Pool Spot Estlink bidding area starting from 1 April 2010. Nord Pool Spot also delivered the technical solution for the Lithuanian market place Baltpool. On 9 June 2010, APX-ENDEX, Belpex and Nord Pool Spot agreed to create a cross-border intraday electricity market based on Nord Pool Spot's Elbas technology. Today, the common intraday market includes the Nordic countries, the Baltic countries, Germany, the Netherlands, and Belgium.\nIn 2012 Nord Pool Spot opens a bidding area in Lithuania, and in 2013 the Latvian market is opened.\n\nIn 2016 Nord Pool Spot was rebranded to Nord Pool.\n\nIt is owned by the national grid companies Fingrid, Energinet.dk, Statnett, Svenska Kraftnät, Elering, Litgrid and AST. Nord Pool has the main office in Lysaker (Oslo) and offices in Stockholm, Helsinki, Tallinn, Berlin and London. It has wholly owned subsidiaries Nord Pool AB and Nord Pool Finland Oy.\n\nNord Pool AS has 380 members in about 20 countries. Members are public and private energy producers, energy intensive industries, large consumers, distributors, funds, investment companies, banks, brokers, utility companies and financial institutions.\n\nNord Pool AS is the world's largest exchange for electrical energy measured in volume (512 TWh) in 2017.\nNord Pool operates markets in the Nordic and Baltic countries, Germany and the UK, and is a Nominated Electricity Market Operator (NEMO) in 15 European countries, while also servicing power markets in Croatia and Bulgaria.\n\n\n"}
{"id": "53646049", "url": "https://en.wikipedia.org/wiki?curid=53646049", "title": "Ohmic plasma", "text": "Ohmic plasma\n\nAn ohmic plasma is a plasma that is maintained and/or replenished by the heat produced when a plasma current flows through its resistance as in the induced poloidal magnetic field of a tokamak. Efficiency decreases as the plasma temperature increases. The mechanism is similar to the heat created with an electric current flowing through a resistance (ohmic).\n\n\n"}
{"id": "17293433", "url": "https://en.wikipedia.org/wiki?curid=17293433", "title": "Oil Industry Safety Directorate", "text": "Oil Industry Safety Directorate\n\nThe Oil Industry Safety Directorate (OISD) is a regulatory and technical directorate in India. It was established in 1986 by Ministry of Petroleum and Natural Gas. The OISD formulates and implements safety standards for the oil industry.\n\nThe main responsibilities OISD are:\n\nOISD has framed rules and guidelines for safe distances to be observed for various facilities in an oil installation. All the new liquefied petroleum gas (LPG) bottling plants in India are designed based on the guidelines of OISD. Further, The LPG plants can be started only after the approval of OISD. OISD has also issued guidelines for the safe operations of petrol stations and standards related to petroleum installations.\n\n"}
{"id": "11440843", "url": "https://en.wikipedia.org/wiki?curid=11440843", "title": "Plutonium-242", "text": "Plutonium-242\n\nPlutonium-242 is one of the isotopes of plutonium, the second longest-lived, with a half-life of 373,300 years.\nPu's halflife is about 15 times as long as Pu-239's halflife; therefore, it is one-fifteenth as radioactive and not one of the larger contributors to nuclear waste radioactivity.\nPu's gamma ray emissions are also weaker than those of the other isotopes.\n\nIt is not fissile (though it is fissionable by fast neutrons) and its neutron capture cross section is also low.\n\nPlutonium-242 is produced by successive neutron capture on Pu, Pu, and Pu. The odd-mass isotopes Pu and Pu have about a 3/4 chance of undergoing fission on capture of a thermal neutron and about a 1/4 chance of retaining the neutron and becoming the following isotope. The proportion of Pu is low at low burnup but increases nonlinearly.\n\nPlutonium-242 has a particularly low cross section for thermal neutron capture; and it takes four neutron absorptions to become another fissile isotope (either curium-245 or plutonium-241) and undergo fission. Even then, there is a chance either of those two fissile isotopes will fail to fission but instead absorb the fourth neutron, becoming curium-246 (on the way to even heavier actinides like californium, which is a neutron emitter by spontaneous fission and difficult to handle) or becoming Pu again; so the mean number of neutrons absorbed before fission is even higher than 4. Therefore, Pu-242 is particularly unsuited to recycling in a thermal reactor and would be better used in a fast reactor where it can be fissioned directly. However, Pu's low cross section means that relatively little of it will be transmuted during one cycle in a thermal reactor.\n\nPlutonium-242 primarily decays into uranium-238 via alpha decay, before continuing along the Uranium series. Plutonium-242 will occasionally decay via spontaneous fission with a rate of 5.5 × 10%.\n"}
{"id": "17153638", "url": "https://en.wikipedia.org/wiki?curid=17153638", "title": "Polymer stabilizers", "text": "Polymer stabilizers\n\nStabilizers are a class of chemical additives commonly added to polymeric materials, such as plastics, to inhibit or retard their degradation. Polymers can be subject to various degradation processes, including oxidation, UV-damage, thermal degradation, ozonolysis, or combinations thereof like photo-oxidation. These processes all degrade the polymer on a chemical level, leading to chain scission that can adversely affect its mechanical properties such as strength and malleability, as well as its appearance and colour.\n\nA vast number of chemically distinct polymers exist, with their degradation pathways varying according to their chemical structure, as such an equally wide range of stabilizers exists. The market for antioxidant stabilisers alone was estimated at US$1.69 billion for 2017.\n\nStabilizers for polymers are used directly or by combinations to prevent the various effects such as oxidation, chain scission and uncontrolled recombinations and cross-linking reactions that are caused by photo-oxidation of polymers. Polymers are considered to get weathered due to the direct or indirect impact of heat and ultraviolet light. The effectiveness of the stabilizers against weathering depends on solubility, ability to stabilize in different polymer matrix, the distribution in matrix, evaporation loss during processing and use. The effect on the viscosity is also an important concern for processing.\n\nHeat stabilizers are mainly used for construction products made of polyvinyl chloride, for instance window profiles, pipes and cable ducts. Light stabilizers, for instance HALS, are especially needed for polypropylene and polyethylene. The environmental impact of stabilizers for polymers can be problematic because of heavy metal content. In Europe lead stabilizers are increasingly replaced by other types, for example calcium-zinc stabilizers.\n\nAntioxidants inhibit autoxidation that occurs when polymers reacts with atmospheric oxygen. For some compounds this can happen gradually at room temperature but almost all polymers are at risk of thermal-oxidation when they are processed at high temperatures. The molding or casting of plastics (e.g. injection molding) require them to be above their melting point or glass transition temperature (~200-300 °C) and under these conditions reactions with oxygen occur much more rapidly. Once initiated, autoxidation proceeds via a free radical chain reaction which can be autocatalytic. As such, even though efforts are usually made to reduce oxygen levels, total exclusion is often not achievable and even exceedingly low concentrations of oxygen can be sufficient to initiate degradation. Sensitivity to oxidation varies significantly depending on the polymer in question; without stabilizers polypropylene and unsaturated polymers such as rubber will slowly degrade at room temperature where as polystyrene can be stable even at high temperatures. Antioxidants are of great importance during the process stage, with long-term stability at ambient temperature increasingly being supplied by hindered amine light stabiizers (HALs). Antioxidants are often referred to as being primary or secondary depending on their mechanism of action.\n\nPrimary antioxidants act as radical scavengers and remove peroxy radicals (ROO•), as well as to a lesser extent alkoxy radicals (RO•), hydroxyl radicals (HO•) and alkyl radials (R•). Oxidation begins with the formation of alkyl radials, which react very rapidly with molecular oxygen (rate constants ≈ to give peroxy radicals, these in turn abstract hydrogen from a fresh section of polymer in a chain propagation step to give new alkyl radials. The overall process is exceedingly complex and will vary between polymers but the first few steps are shown below in general:\n\nDue to its rapid reaction with oxygen the scavenging of the initial alkyl radical (R•) is exceedingly difficult and can only be achieved using specialised antioxidants baring reactive groups, such as acryloyls, the majority of primary antioxidants react instead with the longer lasting peroxy radicals (ROO•). Hydrogen abstraction is usually the rate determining step in the polymer degradation and the peroxy radicals can be scavenged by hydrogen donation from an alternative source, which converts them into organic hydroperoxides (ROOH). The most important commercial stabilzers for this are hindered phenols such as BHT or analogues thereof and secondary aromatic amines such as alkylated-diphenylamine. Amines are typically more effective, but tend to cause discoloration, which is often undesirable (i.e., in food packaging, clothing). The overall reaction with phenols is shown below:\n\nThe end products of these reactions are typically quinones, which may also impart unwanted colour. Modern phenolic antioxidants often have a propionate-group at the para position of the phenol (i.e. they are ortho-alkylated analogues of phloretic acid) as the quinones of these can\nrearrange once to give a hydroxycinnamate, regenerating the phenolic antioxidant group and allowing further radicals to be scavenged. Ultimately however, primary antioxidants are sacrificial and once they are fully consumed the polymer will being to degrade.\n\nSecondary antioxidants act to remove organic hydroperoxides (ROOH) formed by the action of primary antioxidants. Hydroperoxides are less reactive than radical species but can undergo hemolytic bond breaking to form new radicals. As they are less chemically active they require a more reactive antioxidant. The most commonly employed class are phosphite esters, often of hindered phenols e.g. Tris(2,4-di-tert-butylphenyl)phosphite. These will convert polymer hydroperoxides to alcohols, becoming oxidized to organophosphates in the process:\n\nTransesterification can then take place, in which the hydroxylated polymer is exchanged for a phenol:\n\nThis exchange further stabilizes the polymer by releasing a primary antioxidant, because of this phosphites are sometimes considered multi-functional antioxidants as they can combine both types of activity.\nOrganosulfur compounds are also efficient hydroperoxide decomposers, with thioethers being particularly effective against long-term thermal aging, they are ultimately oxidise up to sulfoxides and sulfones.\n\nAntiozonants prevent or slow down the degradation of material caused by ozone. This is naturally present in the air at very low concentrations but is exceedingly reactive, particularly towards unsaturated polymers such as rubber, where it causes ozone cracking. The mechanism of ozonolysis is different from other forms of oxidation and hence requires its own class of antioxidant stabilizers.\nThese are primarily based on p-phenylenediamine and work by reacting with ozone faster than it can react with vulnerable functional groups in the polymer (typically alkene groups). They achieve this by having a low ionization energy which allows them to react with ozone via electron transfer, this converts them into radical cations that are stabilized by aromaticity. Such species remain reactive and will react further, giving products such as 1,4-benzoquinone, phenylenediamine-dimers and nitroxyl radicals. Some of these products can then be scavenged by antioxidants.\n\nLight stabilizer are used to inhibit polymer photo-oxidation, which is the combined result of the action of light and oxygen. Like autoxidation this is a free radical process, hence the antioxidants described above are effective inhibiting agents, however additional classes of additives are also beneficial, such as UV absorbers, quenchers of excited states and HALS.\n\nThe UV absorbers dissipate the absorbed light energy from UV rays as heat by reversible intramolecular proton transfer. This reduces the absorption of UV rays by the polymer matrix and hence reduces the rate of weathering. Typical UV-absorbers are oxanilides for polyamides, benzophenones for PVC, benzotriazoles and hydroxyphenyltriazines for polycarbonate.\n\nStrongly light-absorbing PPS is difficult to stabilize. Even antioxidants fail in this electron-rich polymer. The acids or bases in the PPS matrix can disrupt the performance of the conventional UV absorbers such as HPBT. PTHPBT, which is a modification of HPBT are shown to be effective, even in these conditions.\n\nPhoto-oxidation can begin with the absorption of light by a chromophore within the polymer (which may be a dye or impurity) causing it to enter an excited state. This can then react with ambient oxygen, converting it into highly reactive singlet oxygen. Quenchers are able to absorb energy from excited molecules via a Förster mechanism and then dissipate it harmlessly as either heat or lower frequency fluorescent light. Singlet oxygen can be quenched by metal chelates, with nickel phenolates being a common example.\n\nThe ability of hindered amine light stabilizers (HALS or HAS) to scavenge radicals produced by weathering, may be explained by the formation of aminoxyl radicals through a process known as the Denisov Cycle. The aminoxyl radical (N-O•) combines with free radicals in polymers:\n\nN-O• + R• → N-O-R\n\nAlthough they are traditionally considered as light stabilizers, they can also stabilize thermal degradation.\n\nEven though HALS are extremely effective in polyolefins, polyethylene and polyurethane, they are ineffective in polyvinyl chloride (PVC). It is thought that their ability to form nitroxyl radicals is disrupted. HALS act as a base and become neutralized by hydrochloric acid (HCl) that is released by photooxidation of PVC. The exception is the recently developed NOR HALS, which is not a strong base and is not deactivated by HCl.\n\nIn addition to the effects of light and oxygen polymers may also be degraded by various other environmental factors\n\nAcid scavengers, also referred to as antacids, neutralize acidic impurities, especially those that can act as a source of HCl. They are important stabilizers in many types of polymer, particularly PVC, as well as those produced using Ziegler–Natta catalysts, or that use brominated or chlorinated flame retardants. Common examples include metallic soaps, such as calcium stearate, mineral agents like hydrotalcite and hydrocalumite, and basic metal oxides such as zinc oxide.\n\nHeat (or thermal) stabilizers are mostly used for PVC, as unstabilized material is particularly prone to thermal degradation. These agents minimize loss of HCl, a degradation process that starts above 70 °C. Once dehydrochlorination starts, it is autocatalytic. Many diverse agents have been used including, traditionally, derivatives of heavy metals (lead, cadmium). Increasingly, metallic soaps (metal \"salts\" of fatty acids) are favored, species such as calcium stearate.\nAddition levels vary typically from 2% to 4%.\nThe choice of the best heat stabilizer depends on its cost effectiveness in the end use application, performance specification requirements, processing technology and regulatory approvals.\n\nFlame retardants are a broad range of compounds added to polymers to improve fire resistance.\n\nDegradation resulting from microorganisms (biodegradation) involves its own class of special bio-stabilizers and biocides (e.g. isothiazolinones).\n\n\n"}
{"id": "945530", "url": "https://en.wikipedia.org/wiki?curid=945530", "title": "Pulse forming network", "text": "Pulse forming network\n\nA pulse forming network (PFN) is an electric circuit that accumulates electrical energy over a comparatively long time, then releases the stored energy in the form of a relatively square pulse of comparatively brief duration for various pulsed power applications. In a PFN, energy storage components such as capacitors, inductors or transmission lines are charged by means of a high voltage power source, then rapidly discharged into a load via a high voltage switch, such as a spark gap or hydrogen thyratron. Repetition rates range from single pulses to about 10 per second. PFNs are used to produce precise nanosecond-length pulses of electricity to power devices such as klystron or magnetron tube oscillators in radar sets, pulsed lasers, particle accelerators, flashtubes, and high voltage utility test equipment.\n\nMuch high energy research equipment is operated in a pulsed mode, both to keep heat dissipation down and because high energy physics often occurs at short time scales, so large PFNs are widely used in high energy research. They have been used to produce nanosecond length pulses with voltages of up to 10 - 10 volts and currents up to 10 amps, with peak power in the terawatt range, similar to lightning bolts.\n\nA PFN consists of a series of high voltage energy storage capacitors and inductors. These components are interconnected as a \"\"ladder network\" that behaves similarly to a length of transmission line. For this reason, a PFN is sometimes called an \"artificial, or synthetic, transmission line\"\". Electrical energy is initially stored within the charged capacitors of the PFN by a high voltage DC power supply. When the PFN is discharged the capacitors discharge in sequence, producing an approximately rectangular pulse. The pulse is conducted to the load through a transmission line. The PFN must be impedance matched to the load impedance to prevent the energy reflecting back toward the PFN.\n\nA length of transmission line can be used as a pulse forming network. This can give substantially flat topped pulses at the inconvenience of using of a large length of cable.\n\nIn a simple charged transmission line pulse generator \"(animation, right)\" a length of transmission line such as a coaxial cable is connected through a switch to a matched load \"R\" at one end, and at the other end to a DC voltage source \"V\" through a resistor \"R\" which is large compared to the characteristic impedance \"Z\" of the line. When the power supply is connected it slowly charges up the capacitance of the line through \"R\". When the switch is closed, a voltage equal to \"V\"/2 is applied to the load, the charge stored in the line begins to discharge through the load a current of \"V\"/2\"Z\" and a voltage step travels up the line toward the source. The source end of the line is approximately an open circuit due to the high \"R\", so the step is reflected uninverted and travels back down the line toward the load. The result is that a pulse of voltage is applied to the load with a duration equal to 2\"D\"/\"c\" where \"D\" is the length of the line and \"c\" is the propagation velocity of the pulse in the line. The propagation velocity in typical transmission lines is within 50% of the speed of light. For example, in most types of coaxial cable the propagation velocity is approximately 2/3 the speed of light, or 20 cm per nanosecond.\n\nHigh power PFNs generally use specialized transmission lines consisting of pipes filled with oil or deionized water as a dielectric to handle the high power dissipation.\n\nA disadvantage of simple PFN pulse generators is that because the transmission line must be matched to the load resistance \"R\" to prevent reflections, the voltage stored on the line is divided equally between the load resistance and the characteristic impedance of the line, so the voltage pulse applied to the load is only one-half the power supply voltage.\n\nA transmission line circuit which circumvented the above problem, producing an output pulse equal to the power supply voltage \"V\", was invented in 1937 by British engineer Alan Blumlein and is widely used today in PFNs. In the Blumlein generator \"(animation, right)\", the load is connected in series between two equal length transmission lines, which are charged by a DC power supply at one end (note that the righthand line is charged through the impedance of the load). To trigger the pulse, a switch short-circuits the line at the power supply end, causing a negative voltage step to travel toward the load. Since the characteristic impedance \"Z\" of the line is made equal to half the load impedance \"R\", the voltage step is half reflected and half transmitted, resulting in two symmetrical opposite polarity voltage steps which propagate away from the load, creating between them a voltage drop of \"V\"/2 − (−\"V\"/2) = \"V\" across the load. The voltage steps reflect from the ends and return, ending the pulse. As in other charge line generators, the pulse duration is equal to 2\"D\"/\"c\", where \"D\" is the length of the individual transmission lines. A second advantage of the Blumlein geometry is that the switching device can be grounded, rather than located in the high voltage side of the transmission line as in the typical charged line, which complicates the triggering electronics.\n\nUpon command, a high voltage switch transfers the energy stored within the PFN into the load. When the switch \"\"fires\" (closes), the network of capacitors and inductors within the PFN creates an approximately square output pulse of short duration and high power. This high power pulse becomes a brief source of high power to the load.\n\nSometimes a specially designed pulse transformer is connected between the PFN and load. This technique improves the impedance match between the PFN and the load so as to improve power transfer efficiency. A pulse transformer is typically required when driving higher impedance devices such as klystrons or magnetrons from a PFN. Because the PFN is charged over a relatively long time and then discharged over a very short time, the output pulse may have a peak power of megawatts or even terawatts.\n\nThe combination of high voltage source, PFN, HV switch, and pulse transformer (when required) is sometimes called a \"power modulator\" or \"pulser\".\n\n\n"}
{"id": "15121977", "url": "https://en.wikipedia.org/wiki?curid=15121977", "title": "Płock refinery", "text": "Płock refinery\n\nThe Płock refinery is a large oil refinery and petrochemical complex located in Płock, Poland. It is owned by PKN Orlen. The refinery has a Nelson complexity index of 9.5 and a capacity is 276 kbpd of crude oil.\n\nThe tallest flarestick of the refinery is 250 metres and the tallest chimney 220 metres tall.\n\n"}
{"id": "27254488", "url": "https://en.wikipedia.org/wiki?curid=27254488", "title": "Recycled fuel", "text": "Recycled fuel\n\nRecycled fuel is fuel made of residues as CO2 produced by using a primary fuel.\n\nFor example, CO2 pollution in the atmosphere, produced by petrol burning or other sources, can be extracted to produce fuel through an artificial photosynthesis based in nanotechnology, which helps to mitigate pollution, climate change and energy issues.\n"}
{"id": "15233345", "url": "https://en.wikipedia.org/wiki?curid=15233345", "title": "Square pyramidal molecular geometry", "text": "Square pyramidal molecular geometry\n\nIn molecular geometry, square pyramidal geometry describes the shape of certain compounds with the formula ML where L is a ligand. If the ligand atoms were connected, the resulting shape would be that of a pyramid with a square base. The point group symmetry involved is of type C. The geometry is common for certain main group compounds that have a stereochemically active lone pair, as described by VSEPR theory. Certain compounds crystallize in both the trigonal bipyramidal and the square pyramidal structures, notably [Ni(CN)].\n\nAs a trigonal bipyramidal molecule undergoes Berry pseudorotation, it proceeds via an intermediary stage with the square planar geometry. Thus even though the geometry is rarely seen as the ground state, it is accessed by a low energy distortion from a trigonal bipyramid.\n\nPseudorotation also occurs in square pyramidal molecules. Molecules with this geometry, as opposed to trigonal bipyramidal, exhibit heavier vibration. The mechanism used is similar to the Berry mechanism.\n\nSome molecular compounds that adopt square pyramidal geometry are XeOF, and various halogen pentafluorides (XF, where X = Cl, Br, I). Complexes of vanadium(IV), such as [VO(acac)] are square pyramidal (acac = acetylacetonate, the deprotonated anion of acetylacetone (2,4-pentanedione)).\n\n\n"}
{"id": "44493625", "url": "https://en.wikipedia.org/wiki?curid=44493625", "title": "Stabilized soil mixing plant", "text": "Stabilized soil mixing plant\n\nA stabilized soil mixing plant is a combination of kinds of machines used for mixing stabilized soil, which is used for highway construction, municipal road projects, and fertile airport areas. The plant produces stabilized soil with different gradings in a continuous way. Such a plant usually contains a cement silo, measuring and conveying system, and mixing devices.\n\nStabilized soil is a mixture of lime, cement, coal ash, soil, sand, and other aggregates.\n\nStabilized soil mixing plants are of two kinds: the portable stabilized soil mixing plant and the stationary stabilized soil mixing plant. The portable stabilized soil mixing plant has wheels on each part and can be driven by a trailer, but has low productivity. The stationary plant has larger productivity but is less flexible, and needs a firm groundwork.\n\nAll aggregates like lime, sand, soil, coal ash, and other materials are loaded into batching hoppers by a loading machine. After measuring, the belt feeder transports the aggregates into a mixing device. Meanwhile, stabilizing powders like lime or cement are transferred from a powder material warehouse to the batch hopper by a spiral conveyor, and then moved to the belt feeder by a powder material feeder. All ingredients then go into the mixing device for final processing. Finally, the feeding belt conveyor takes the final product and delivers it to the storage warehouse.\n"}
{"id": "43546125", "url": "https://en.wikipedia.org/wiki?curid=43546125", "title": "Subcoal", "text": "Subcoal\n\nSubcoal technology is used to process paper plastic waste fractions into a substitute for coal or lignite. The fuel pellets can be used as secondary energy source in industrial furnaces, such as lime kilns and cement kilns, coal-fired power stations and blast furnaces. Subcoal has a caloric value comparable with lignite. The technology was developed by the Dutch chemical company DSM. A study by CE Delft revealed that for the paper-plastic fraction of household waste, the Subcoal route has a better climate and overall environmental score as compared to the incineration in a waste incineration plant.\n\nCurrently, a lot of waste ends up in incineration plants or is dumped to landfill. Depending on the region, waste collection systems have advanced to separate collections for recycling, which allows easier production of refuse-derived fuel. \nThe technology consists of a number of different treatment stages, depending on the input waste streams. The waste streams are typically purified, with recyclables being collected and sorted out of the waste streams. The waste is dried if necessary. The material is shredded to the required size and turned into pellets, with a diameter of approximately 8mm. Terms of calorific value, ash content, chlorine contact and moisture content are important to rate the quality of the pellets. The calorific value is comparable with (and sometimes higher than) a few types of coal. The percentage of biomass is fairly high as well; it is more than 50%. Because it is in pellets, Subcoal shows hydrophobic behaviour and does not dissolve when the moisture content is increased, which is important when considering external storage. Subcoal is mostly dosed as a ground (pulverized) fuel. Subcoal is commercially used via various grinding media (direct firing); via a hammer mill (12 tonnes per hour), air-rotor mill (6 tonnes per hour) and via a bowl mill (4 tonnes per hour). Cement manufacturers, lime producers and energy providers spray the Subcoal powder with coal powder into their kilns.\n\nCurrently, there are two Subcoal producing facilities in the Netherlands, with have a combined output capacity of 80 to 90 thousand tonnes per year. One is implemented at a cardboard mill in the southern part of the Netherlands, with an annual output of about 15.000 tonnes. The biggest is a standalone production facility located in the northern part of the Netherlands, with an annual output of about 70.000 tonnes per year.\n\nOther alternative fuels tend to have greater quality fluctuations due to seasonal influences. Subcoal is hydrophobic and can thus be stored outside. This allows Subcoal to be mixed with the primary fuel in uncovered areas e.g. in the external coal storage area. The existing dosing equipment and grinding media can be used to transport the Subcoal into the power station.\n\nThe gate fee for the waste streams and the costs for logistics have to be taken into account. Typically, in Europe, the price for Subcoal (CIF) fluctuates between €1 and €2.50 per gigajoule. The fluctuation is dependent on the input prices for the waste streams which are used and the production steps.\n\n\n"}
{"id": "9592061", "url": "https://en.wikipedia.org/wiki?curid=9592061", "title": "Tank (unit)", "text": "Tank (unit)\n\nA tank is an obsolete unit of mass in India approximately equal to 4.4 g (69 gr). After metrication in the mid-20th century, the unit became obsolete.\n\nIn Mumbai (formerly Bombay), the tank equalled 17 1/72 grains (about 1.1 grams), and 72 tanks equalled 30 pice. In the 16th century, the tank was reported to be 20.96 g (323.46 grain).\n\n"}
{"id": "14190135", "url": "https://en.wikipedia.org/wiki?curid=14190135", "title": "Terri Swearingen", "text": "Terri Swearingen\n\nTerri Swearingen is a nurse from the state of Ohio.\n\nShe was awarded the Goldman Environmental Prize in 1997, for organizing the protests against Waste Technologies Industries' toxic waste incinerator in the Appalachian town of East Liverpool, Ohio.\n\nOne of her quotes were \"We are living on this planet as if we had another one to go to.\"\n"}
{"id": "18880417", "url": "https://en.wikipedia.org/wiki?curid=18880417", "title": "Tetratriacontanoic acid", "text": "Tetratriacontanoic acid\n\nTetratriacontanoic acid (or gheddic acid, geddic acid) is a 34 carbon long carboxylic acid.\n"}
{"id": "1010088", "url": "https://en.wikipedia.org/wiki?curid=1010088", "title": "Total suspended solids", "text": "Total suspended solids\n\nTotal suspended solids (TSS) is the dry-weight of suspended particles, that are not dissolved, in a sample of water that can be trapped by a filter that is analyzed using a filtration apparatus. It is a water quality parameter used to assess the quality of a specimen of any type of water or water body, ocean water for example, or wastewater after treatment in a wastewater treatment plant. It is listed as a conventional pollutant in the U.S. Clean Water Act. Total dissolved solids is another parameter acquired through a separate analysis which is also used to determine water quality based on the total substances that are fully dissolved within the water, rather than undissolved suspended particles.\n\nTSS was previously called non-filterable residue (NFR), but was changed to TSS because of ambiguity in other scientific disciplines.\n\nTSS of a water or wastewater sample is determined by pouring a carefully measured volume of water (typically one litre; but less if the particulate density is high, or as much as two or three litres for very clean water) through a pre-weighed filter of a specified pore size, then weighing the filter again after the drying process that removes all water on the filter. Filters for TSS measurements are typically composed of glass fibres. The gain in weight is a dry weight measure of the particulates present in the water sample expressed in units derived or calculated from the volume of water filtered (typically milligrams per litre or mg/L).\n\nIf the water contains an appreciable amount of dissolved substances (as certainly would be the case when measuring TSS in seawater), these will add to the weight of the filter as it is dried. Therefore it is necessary to \"wash\" the filter and sample with deionized water after filtering the sample and before drying the filter. Failure to add this step is a fairly common mistake made by inexperienced laboratory technicians working with sea water samples, and will completely invalidate the results as the weight of salts left on the filter during drying can easily exceed that of the suspended particulate matter.\n\nAlthough turbidity purports to measure approximately the same water quality property as TSS, the latter is more useful because it provides an actual weight of the particulate material present in the sample. In water quality monitoring situations, a series of more labor-intensive TSS measurements will be paired with relatively quick and easy turbidity measurements to develop a site-specific correlation. Once satisfactorily established, the correlation can be used to estimate TSS from more frequently made turbidity measurements, saving time and effort. Because turbidity readings are somewhat dependent on particle size, shape, and color, this approach requires calculating a correlation equation for each location. Further, situations or conditions that tend to suspend larger particles through water motion (e.g., increase in a stream current or wave action) can produce higher values of TSS not necessarily accompanied by a corresponding increase in turbidity. This is because particles above a certain size (essentially anything larger than silt) are not measured by a bench turbidity meter (they settle out before the reading is taken), but contribute substantially to the TSS value.\n\nAlthough TSS appears to be a straightforward measure of particulate weight obtained by separating particles from a water sample using a filter, it suffers as a defined quantity from the fact that particles occur in nature in essentially a continuum of sizes. At the lower end, TSS relies on a cut-off established by properties of the filter being used. At the upper end, the cut-off should be the exclusion of all particulates too large to be \"suspended\" in water. However, this is not a fixed particle size but is dependent upon the energetics of the situation at the time of sampling: moving water suspends larger particles than does still water. Usually it is the case that the additional suspended material caused by the movement of the water is of interest.\n\nThese problems in no way invalidate the use of TSS; consistency in method and technique can overcome short-comings in most cases. But comparisons between studies may require a careful review of the methodologies used to establish that the studies are in fact measuring the same thing.\n\nTSS in mg/L can be calculated as:\n\n\n"}
{"id": "23440656", "url": "https://en.wikipedia.org/wiki?curid=23440656", "title": "Viareggio train derailment", "text": "Viareggio train derailment\n\nThe Viareggio derailment was the derailment of a freight train and subsequent fire which occurred on 29 June 2009 in a railway station in Viareggio, Lucca, a city in Central Italy's Tuscany region. Thirty-two people were killed, and a further twenty-six were injured\n\nFreight train No. 50325 from Trecate to Gricignano, hauled by Class E655 locomotive E 655 175 with 14 tank wagons was derailed at Viareggio at 23:48 local time (21:48 UTC) on 29 June 2009. Of the 14 wagons, the first wagon was registered by Polskie Koleje Państwowe, the other 13 wagons by Deutsche Bahn (DB) The first DB-registered wagon, No. 338078182106, which was owned by GATX Rail Austria GmbH derailed on plain track in Viareggio station. The wagon hit the platform of the station and overturned to the left. The next four wagons also overturned and the two following were derailed but remained upright. The last seven wagons were not derailed, remaining intact on the track. The derailed wagons crashed into houses alongside the railway line.\n\nSome of the wagons were owned by KVG Kesselwagen, a division of GATX, and leased to ExxonMobil and ERG (the owners of the oil refinery where the train left), were reported to have been carrying Liquefied Petroleum Gas (LPG). Two of these exploded and caught fire. Seven people were reported to have been killed when a house collapsed. An eighth person who was killed was reported to have been riding a scooter on a road adjoining the railway. A child was found carbonised in a car in front of the house where he lived with his parents. It is speculated that his parents put him in the car to save him and then returned to the house to save other two children.\n\nThe two members of the train crew suffered minor injuries in the accident. A large area of Viareggio was damaged in the subsequent fires caused by the wagons carrying LPG self-combusting. Twenty-six people were reported to have been injured in the accident. The accident is the worst rail accident in Italy since the collision between two trains in Murazze di Vado near Bologna on 15 April 1978, which killed 48 people. It was reported that a whole street had been destroyed in the explosion and fire.\n\nA state of emergency was declared by local authorities. Around 1,000 residents of Viareggio were evacuated from their homes as a result of the accident. Italian Prime Minister Silvio Berlusconi visited Viareggio \"to take control of the situation\", but he received boos and cries of \"go home\". Dr Enrico Petri, an eyewitness and local hospital physician, said that 36 people had been taken to Versilia Hospital in Viareggio suffering from 80–90% burns. He compared the aftermath to a terrorist attack. The accident left around 100 people homeless. The accident resulted in the disruption of rail services between Rome and Genoa. Viareggio railway station was partially reopened on 3 July 2009.\n\nThe Direzione Generale per le Investigazioni Ferroviarie, a section of Ferrovie dello Stato (FS) opened an investigation into the cause of the accident. Italian police said that the accident may have been caused by damaged tracks or a problem with the brakes on the train. Italian union CGIL is reported to have blamed the decrepit state of the rolling stock, the maintenance of the wagon was the responsibility of GATX. The failure of an axle on the wagon that derailed is being investigated as a possible cause. \nPending the official conclusions of the commissions of inquiry the probable cause of the accident is attributable to structural failure of an axis of the carriage of the first tank wagon derailed. Italian Transport Minister Altero Matteoli informed the Italian Parliament on 1 July that a defective axle may have caused the accident.\n\nOn 29 July 2009, an Extraordinary Network Meeting of the Network of National Safety Authorities was held. It invited members to disseminate information related to problems related to Type A Axles to railway operators, owners and keepers of freight wagons.\n\nIn 2016–17, thirty three people were tried in Lucca in connection with the derailment. Ten were acquitted. Former Rete Ferroviaria Italiana (RFI) and current Leonardo CEO Mauro Moretti and former RFI CEO Michele Mario Ella, were convicted. Moretti was sentenced to seven years in prison for his role as CEO of RFI (2001-2006) but acquitted as CEO of Ferrovie dello Stato Italiane (FS) (2006-2014). Ella was sentenced to seven and a half years.\n\n\n"}
{"id": "9620168", "url": "https://en.wikipedia.org/wiki?curid=9620168", "title": "Windstar turbine", "text": "Windstar turbine\n\nThe Windstar vertical-axis turbine is a lift-type device with straight blades attached at each end to a central rotating shaft. Windstar turbines were invented by Robert Nason Thomas and developed by Wind Harvest International, Inc., formerly the Wind Harvest Company, based in Point Reyes, California. Windstar turbines are operated as Linear Array Vortex Turbine Systems (LAVTS). Each rotor unit has a dual braking system of pneumatic disc brakes and blade pitch. All Windstar models use off-the-shelf generators, gearboxes, bearings and other components.\n\nIn 2001 and 2002, three guyed prototype versions of the Windstar Model 530G were installed in Palm Springs, California. The Model 530G prototype used extruded aluminum blades, was supported by a guyed cable system and was rated at 25 kW. The three 530G turbines were placed in close proximity, creating the first Linear Array Vortex Turbine System (LAVTS). The 530G LAVTS was rated at 75 KW and was run continuously for five years to collect performance data.\n\nIn 2006, Wind Harvest International began design work on a series of Windstar turbine models to take advantage of the properties demonstrated in the 530G LAVTS prototype: Model 636, 1500 and 3000 turbines. The Model 636 is a direct evolution of the 530G, with an increase in blade length, in order to increase swept area, and including a modified bearing design. The 1500 and 3000 turbine models stand high with a rotor and 50-75 kW generators. The Windstar 1500 is wide and is designed for higher wind speed locations. The Windstar 3000 will be wide to operate better in lower wind speed locations (14-16 mph). Model 636 continues to use extruded aluminum blades, similar to the 530G. Models 1500 and 3000 use pultruded fiberglass blades.\n\nBy 2010, Windstar Model 636 turbines were placed on the transitional list for approved wind energy generation systems qualifying for the FIT (Feed-in-Tariff) energy pricing in the UK. In 2011, Wind Harvest International, Inc. began the process for MCS certification of the Model 636 LAVTS.\n\nThe Linear Array Vortex Turbine System (LAVTS) consists of three or more Windstar vertical-axis rotors, each with their own 25-75 kW generator, placed in a linear array with each rotor's blades passing within two feet of its neighbor. In this configuration, the center rotors gain an increase in energy output and efficiency. Observation of this effect (the coupled vortex effect) on Windstar turbines in field tests led to the issuance of US patent 6784566.\n\nWindstar turbines are built in arrays of three or more turbines in order to take advantage of the \"coupled vortex effect.\" The coupled vortex effect was identified through field tests of Windstar 530G turbines and is made possible by the fact that straight-bladed vertical axis wind turbine blades passing in close proximity to one another in opposite directions create lift and torque on their neighboring blades, boosting the energy production of an array in comparison with a single turbine's performance. See patent . A mathematical model of the phenomenon and its effect on Windstar turbine performance was made possible by grant funds from the California Energy Commission in 2010., Research on similar vertical axis turbine configurations have independently confirmed the effect, linking the phenomenon's boost of performance to the fluid efficiency observed in schools of fish.\n\nWindstar Model 636 is a commercial model of the Windstar turbine offered by Wind Harvest International, Inc. It is a direct evolution of their earlier guyed model the 530G. It uses similar components with some modifications geared toward the commercial market. Modifications of the design include longer blades and a new bearing assembly.\n\nThese two models were designed based on field data from the 530G prototype field testing in the LAVTS configuration. Mathematical modeling of these turbines was conducted by IOPARA, Inc., made possible by a grant from the California Energy Commission in 2010. Model 3000 will stand 50 ft high with a 40 ft tall rotor. The Windstar Model 3000 will be 75 ft wide and used for lower wind speed locations (14-16 mph). The Windstar Model 1500 will have a similar rotor height and will be 37.5 ft wide. The Model 1500 is designed for higher wind speed locations.\n\n Coupled vortex vertical-axis wind turbine.\n"}
