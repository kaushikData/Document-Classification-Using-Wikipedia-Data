{"id": "46297885", "url": "https://en.wikipedia.org/wiki?curid=46297885", "title": "12-Hydroxyheptadecatrienoic acid", "text": "12-Hydroxyheptadecatrienoic acid\n\n12-Hydroxyheptadecatrenoic acid (also termed 12-HHT, 12(\"S\")-hydroxyheptadeca-5\"Z\",8\"E\",10\"E\"-trienoic acid, or 12(S)-HHTrE) is a 17 carbon metabolite of the 20 carbon polyunsaturated fatty acid, arachidonic acid. It was first detected and structurally defined by P. Wlodawer, Bengt I. Samuelsson, and M. Hamberg as a product of arachidonic acid metabolism made by microsomes (i.e. endoplasmic reticulum) isolated from sheep seminal vesicle glands and by intact human platelets. 12-HHT is less ambiguously termed 12-(\"S\")-hydroxy-5\"Z\",8\"E\",10\"E\"-heptadecatrienoic acid to indicate the \"S\" stereoisomerism of its 12-hydroxyl residue and the \"Z\", \"E\", and \"E\" cis-trans isomerism of its three double bonds. The metabolite was for many years thought to be merely a biologically inactive byproduct of prostaglandin synthesis. More recent studies, however, have attached potentially important activity to it.\n\nCyclooxygenase-1 and cyclooxygenase-2 metabolize arachidonic acid to the 15-hydroperoxy, 20 carbon prostaglandin (PG) intermediate, PGG2, and then to the 15-hydroxy, 20 carbon intermediate, prostaglandin H2 (PGH2). Thromboxane synthase further metabolizes PGH2 to the 20 carbon product, Thromboxane A2, the 17 carbon product, 12-HHT, and the 3 carbon product Malonyldialdehyde. Platelets express cycloxygenase and thromboxane synthase enzymes, producing PGG2, PGH2, and TXA2 in response to platelet aggregating agents such as thrombin; these metabolites act as autocrines by feeding back to promote further aggregation of their cells of origin and as paracrines by recruiting nearby platlets into the response as well as exerting effects on other nearby tissues such as contracting blood vessels. These effects combine to trigger blood clotting and limiting blood loss. 12-HHT is a particularly abundant product of these pro-clotting responses, accounting for about one third of the total amount of arachidonic acid metabolites formed by physiologically stimulated human platelets. Its abundant production during blood clotting, the presence of cyclooxygenases and to a lesser extent thromboxane synthase in a wide range of cell types and tissue, and its production by other pathways imply that 12-HHT has one or more important bioactivities relevant to clotting and, perhaps, other responses.\n\nVarious cytochrome P450 enzymes (e.g. CYP1A1, CYP1A2, CYP1B1, CYP2E1, CYP2S1, and CYP3A4) metabolize PGG2 and PGH2 to 12-HHT and MDA. While the latter studies were conducted using recombinant cytochrome enzymes or sub-fractions of disrupted cells, the human monocyte, a form of blood circulating leukocyte, increases its expression of CYP2S1 when forced to differentiate into a macrophage phenotype by interferon gamma or lipopolysaccharide (i.e. endotoxin); associated with these changes, the differentiated macrophage metabolized arachidonic acid to 12-HHT by a CYP2S1-dependent mechanism. Future studies, therefore may show that cytochromes are responsible for 12-HHT and MDA production in vivo.\n\nPGH2, particularly in the presence of ferrous iron (FeII), ferric iron (FeIII), or hemin, rearranges non-enzymatically to a mixture of 12-HHT and 12-HHT's 8-cis isomer, i.e., 12-(\"S\")-hydroxy-5\"Z\",8\"Z\",10\"E\"-heptadecatrienoic acid. This non-enzymatic pathway may explain findings that cells can make 12-HHT in excess of TXA2 and also in the absence of active cycloxygenase and/or thromboxane synthase enzymes.\n\n12-HHT is further metabolized by 15-hydroxyprostaglandin dehydrogenase (NAD+) in a wide variety of human and other vertebrate cells to its 12-oxo (also termed 12-keto) derivative, 12-oxo-5\"Z\",8\"E\",10\"E\"-heptadecatrienoic acid (12-oxo-HHT or 12-keto-HHT). Pig kidney tissue also converted 12-HHT to 12-keto-5\"Z\",8\"E\"-heptadecadienoic acid (12-oxo-5\"Z\",8\"E\"-heptadecadienoic acid) and 12-hydroxy-heptadecadienoic acid.\n\nAcidic conditions (pH~1.1-1.5) cause 12-HHT to rearrange in a time- and temperature-dependent process to its 5-cis isomer, 12-hydroxy-5\"E\",8\"E\",10\"E\"-heptadecatrienoic acid.\n\nFourteen years after the first publication on its detection in 1973, 12-HHT was reported to stimulate fetal bovine aortic and human umbilical vein endotheleal cells to metabolize arachidonic acid to Prostacyclin I2 (PGI2), a powerful inhibitor of platelet activation and stimulator of Vasodilation (see Prostacyclin synthase); it did not, however, alter arachidonic acid metabolism in human platelets. Shortly thereafter, 12-HHT was reported to inhibit the chemotaxis-blocking effect of a human monocyte-derived factor on human moncytes while the immediate metabolite of 12-HHT, 12-oxo-HT, was reported to stimulate the chemotasis of human neutrophils. and to inhibit platelet aggregation responses to various agents by stimulating platelets to raise their levels of Cyclic adenosine monophosphate (cAMP), an intracellular signal that serves broadly to inhibit platelet activation. These studies were largely overlooked; in 1998 and 2007 publications, for example, 12-HHT was regarded as either inactive or without significant biological activity. Nonetheless, this early work suggested that 12-HHT may serve as a contributor to monocyte- and neutrophil-based inflammatory responses and 12-oxo-HT may serve as a counterpoise to platelet aggregation responses elicited or promoted by TXA2. Relevant to the latter activity, a later study showed that this inhibitory effect was due to the ability of 12-oxo-HT to act as a partial antagonist of the Thromboxane receptor: it blocks TXA2 binding to its receptor and thereby the responses of platelets and possibly other tissues to TXA2 as well as agents that depend on stimulating TXA2 production for their activity. Thus, 12-HHT forms simultaneously with, and by stimulating PGI2 production, inhibits TXA2-mediated platelet activation responses while 12-oxo-HT blocks TXA2 receptor binding to reduce not only TXA2-induced thrombosis and blood clotting but possibly also vasospasm and other actions of TXA2. In this view, thromboxane synthase leads to the production of a broadly active arachidonic acid metabolite, TXA2, plus two other arachidonic acid metabolites, 12-HHT and 12-oxo-HT, that serve indirectly to stimulate PGI2 production or directly as a receptor antagonist to moderate TXA2's action, respectively. This strategy may be essential for limiting the deleterious thrombotic and vasospastic activities of TXA2. \nLeukotriene B4 (i.e. LTB4) is an arachidonic acid metabolite made by the 5-lipoxygenase enzyme pathway. It activates cells through both its high affinity (Dissociation constant [Kd] of 0.5-1.5 nM) Leukotriene B4 receptor 1 (BLT1 receptor) and its low affinity BLT2 receptor (Kd=23 nM); both receptors are G protein coupled receptors that, when ligand-bound, activate cells by releasing the Gq alpha subunit and pertussis toxin-sensitive Gi alpha subunit from Heterotrimeric G proteins. BLT1 receptor has a high degree of ligand-binding specificity: among a series of hydroxylated eicosanoid metabolites of arachidonic acid, it binds only LTB4, 20-hydroxy-LTB4, and 12-epi-LTB4; among this same series, BLT2 receptor has far less specificity in that it binds not only LTB4, 20-hydroxy-LTB4, and 12-epi-LTB4 but also 12(\"R\")-HETE and 12(\"S\")-HETE (i.e. the two stereoisomers of 12-Hydroxyeicosatetraenoic acid) and 15(\"S\")-HpETE and 15(\"S\")-HETE (i.e. the two stereoisomers of 15-Hydroxyicosatetraenoic acid). The BLT2 receptor's relative affinities for finding LTB4, 12(\"S\")-HETE, 12(\"S\")-HpETE, 12(\"R\")-HETE, 15(\"S\")-HETE, and 20-hydroxy-LTB4 are ~100, 10, 10, 3, 3, and 1, respectively. All of these binding affinities are considered to be low and therefore indicating that some unknown ligand(s) might bind BLT2 with high affinity. In 2009, 12-HHT was found to bind to the BLT2 receptor with ~10-fold higher affinity than LTB4; 12-HHT did not bind to the BLT1 receptor. Thus, the BLT1 receptor exhibits exquisite specificity, binding 5(\"S\"),12(\"R\")-dihydroxy-6\"Z\",8\"E\",10\"E\",14\"Z\"-eicosatetraenoic acid (i.e. LTB4) but not LTB4's 12(\"S\") or 6\"Z\" isomers while the BLT2 receptor exhibits a promiscuous finding pattern. Formyl peptide receptor 2 is a relevant and well-studied example of promiscuous receptors. Initially thought to be a second and low affinity receptor for the neutrophil tripeptide chemotactic factor, \"N\"-formyl-met-leu-phe, subsequent studies showed that it was a high affinity receptor for the arachidonic acid metabolite, lipoxin A4, but also bound and was activated by a wide range of peptides, proteins, and other agents. BLT2 may ultimately prove to have binding specificity for a similarly broad range of agents.\n\nThe production of LTB4 and expression of BLT1 by human tissues are largely limited to bone marrow-derived cells such as the neutrophil, eosinophil, mast cell, and various types of lymphocytes and accordingly are regarded primarily as contributing to the many human defensive and pathological (ulcerative colitis, arthritis, asthma, etc.) inflammatory responses which are mediated by these cell types. Drugs that inhibit LTB4 production or binding to BLT1 are in use or development for the latter diseases. In contrast, the production of 12-HH2 and expression of BLT2 receptors by human tissues is far wider and more robust than that of the LTB4/BLT2 receptor axis. Recent studies indicate that the role(s) of the 12-HHT/BLT2 receptor axis in human physiology and pathology may be very different than those of the LTB4/BLT1 axis.\n\n12-HHT stimulates chemotactic responses in mouse bone marrow mast cells, which naturally express BLT2 receptors, as well as in Chinese hamster ovary cells made to express these receptors by transfection. These findings suggest that the 12-HHT/BLT2 receptor pathway may support the pro-inflammatory (i.e. chemotactic) actions of the LTB4/BLT1 pathway.\n\nOn the other hand, the immortalized human skin cell line HaCaT expresses BLT2 receptors and responds to ultraviolet B (UVB) radiation by generating toxic reactive oxygen species which in turn cause the HaCaT cells to die by activating apoptotic pathways in a BLT2 receptor-dependent reaction. Topical treatment of mouse skin with a BLT2 receptor antagonist, LY255283, protects against UVB radiation-induced apoptosis and BLT2-overexpressing transgenic mice exhibited significantly more extensive skin apoptosis in response to UVB irradiation. Furthermore, 12-HHT inhibits HaCaT cells from synthesizing interleukin-6 (IL-6), a pro-inflammatory cytokine associated with cutaneous inflammation, in response to UVB radiation. These results suggest that the 12-HHT/BLT2 axis can act to suppress inflammation by promoting the orderly death of damaged cells and blocking IL-6 production. Opposition between the pro-inflammatory LTB4/BLT1 and anti-inflammatory actions of the 12-HHT/BLT2 axes occurs in another setting. In a mice model of ovalbumin-induced allergic airway disease, 12-HHT and its companion cyclooxygenase metabolites, Prostaglandin E2 and Prostaglandin D2, but not 12 other lipoxygenase or cycloxygenase mebolites showed a statistically significantly increase in bronchoalveolar lavage fluid levels after intratracheal ovalbumin challenge; after this challenge, only 12-HHT, among the monitored BLT2 receptor-activating ligands (LTB4, the 12(\"S\") stereoisomer of 12-HETE, and 15(\"S\")-HETE) attained levels capable of activating BLT2 receptors. Also, BLT2 knockout mice exhibited a greatly \"enhanced\" response to ovalabumin challenge. Finally, BLT2 receptor expression was significantly reduced in allergy-regulating CD4+ T cells from patients with asthma compared to healthy control subjects. Unlike LTB4 and its BLT1 receptor, which are implicated in contributing to allergen-based airway disease in mice and humans, 12-HHT and its BLT2 receptor appear to suppress this disease in mice and may do so in humans. While further studies to probe the role of the 12-HHT/BLT2 axis in human inflammatory and allergic diseases, the current studies indicate that 12-HHT, acting through BLT2, may serve to promote or limit, inflammatory and to promote allergic responses.\n\nHigh dose aspirin treatment (aspirin, at these doses, inhibits cyclooxygenases-1 and -2 to block their production of 12-HHT), thromboxane synthase knockout, and BLT2 receptor knockout but not TXA2 receptor knockout impair keratinocyte-based re-epithelialization and thereby closure of experimentally induced wounds in mice while a synthetic BLT2 receptor agonist accelerats wound closure not only in this mouse model but also in the db/db mouse model of obesity, diabetes, and dyslipidemia due to leptin receptor deficiency. 12-HHT accumulated in the wounds of the former mouse model. Companion studies using an in vitro scratch test assay indicated that 12-HHT stimulated human and mouse keratinocyte migration by a BLT2 receptor-dependent mechanism that involved the production of tumor necrosis factor α and metalloproteinases. These results indicate that the 12-HHT/BLT2 receptor axis is a critical contributor to wound healing in mice and possibly humans. The axis operates by recruiting the movement of keratinocytes to close the wound. This mechanism may underlie the suppression of wound healing that accompanies the high dose intake of aspirin and, based on mouse studies, other non-steroidal anti-inflammatory agents (NSAID) in humans. Synthetic BLT2 agonists may be useful for speeding the healing of chronic ulcerative wounds, particularly in patients with, for example diabetics, that have impaired wound healing.\n\nA large number of studies have associated BLT2 and, directly or by assumption, 12-HHT in the survival, growth, and/or spread of various human cancers. See \"'Leukotriene B4 receptor 2\" for the associations of BLT2 and/or 12-HHT with stimulating the malignant behavior of prostate cancer, urinary bladder cancer, breast cancer, thyroid gland follicular carcinoma, kidney renal cell carcinoma, bladder transitional cell carcinoma, esophageal cancer, pancreatic cancer, and colon cancer.\n\n"}
{"id": "48510837", "url": "https://en.wikipedia.org/wiki?curid=48510837", "title": "Ace Matara Power Station", "text": "Ace Matara Power Station\n\nThe Ace Matara Power Station was a power station located in Matara, Sri Lanka. During its operations from 2002-2012, the plant utilized four generating units. The plant was decommissioned in 2012 after its 10-year PPA expired and recommissioned in 2017.\n\n"}
{"id": "417036", "url": "https://en.wikipedia.org/wiki?curid=417036", "title": "Agency for Nuclear Projects", "text": "Agency for Nuclear Projects\n\nThe Agency for Nuclear Projects (Nuclear Waste Project Office) is a part of the Nevada state government, under the administration of the Governor of Nevada. The organization is based in Carson City.\n\nThe Agency, created within the Office of the Governor, works to ensure the health, safety and welfare of Nevada's citizens, environment, and its economy with regard to disposal and transportation of nuclear waste throughout the State of Nevada. The Agency is responsible for fulfilling the federal oversight responsibilities called for in the Nuclear Waste Policy Act of 1982. The NWPA was amended in 1987 to select Yucca Mountain in Southern Nevada as the nation's permanent nuclear waste repository.\n\nCreated in 1985 to help the state fight the siting of the Yucca Mountain nuclear waste repository it was headed by Robert Loux until he resigned on September 29, 2008, to be replaced by Bruce Breslow.\n\n"}
{"id": "5580444", "url": "https://en.wikipedia.org/wiki?curid=5580444", "title": "Amorphous carbonia", "text": "Amorphous carbonia\n\nAmorphous carbonia, also called a-carbonia or a-CO, is an exotic amorphous solid form of carbon dioxide that is analogous to amorphous silica glass. It was first made in the laboratory in 2006 by subjecting dry ice to high pressures (40-48 gigapascal, or 400,000 to 480,000 atmospheres), in a diamond anvil cell. Amorphous carbonia is not stable at ordinary pressures—it quickly reverts to normal CO.\n\nWhile normally carbon dioxide forms molecular crystals, where individual molecules are bound by Van der Waals forces, in amorphous carbonia a covalently bound three-dimensional network of atoms is formed, in a structure analogous to silicon dioxide or germanium dioxide glass.\n\nMixtures of a-carbonia and a-silica may be a prospective very hard and stiff glass material stable at room temperature. Such glass may serve as protective coatings, e.g. in microelectronics.\n\nThe discovery has implications for astrophysics, as interiors of massive planets may contain amorphous solid carbon dioxide.\n\n\n"}
{"id": "20193516", "url": "https://en.wikipedia.org/wiki?curid=20193516", "title": "Amtrak's 25 Hz traction power system", "text": "Amtrak's 25 Hz traction power system\n\nAmtrak's 25 Hz traction power system is a traction power grid operated by Amtrak along the southern portion of its Northeast Corridor (NEC): the 225 route miles (362 km) between Washington, D.C. and New York City and the 104 route miles (167 km) between Philadelphia and Harrisburg, Pennsylvania. The Pennsylvania Railroad constructed it between 1915 and 1938. Amtrak inherited the system from Penn Central, the successor to Pennsylvania Railroad, in 1976 along with the Northeast Corridor. This is the reason for using 25 Hz, as opposed to 60 Hz – which is the standard for power transmission in North America. In addition to serving the NEC, the system provides power to New Jersey Transit Rail Operations (NJT), the Southeastern Pennsylvania Transportation Authority (SEPTA) and the Maryland Area Regional Commuter Train (MARC). Only about half of the system's electrical capacity is used by Amtrak. The remainder is sold to the commuter railroads who operate their trains along the corridor.\n\nThe Pennsylvania Railroad (PRR) began experimenting with electric traction in 1910, coincident with their completion of the trans-Hudson tunnels and New York Penn Station. These initial systems were low-voltage direct current (DC) third rail systems. While they performed adequately for tunnel service, the PRR ultimately determined them to be inadequate for long distance, high-speed electrification.\n\nOther railroads had by this time experimented with low frequency (less than 60 Hz) alternating current (AC) systems. These low-frequency systems had the AC advantage of higher transmission voltages, reducing resistive losses over long distances, as well as the typically DC advantage of easy motor control as universal motors could be employed with transformer tap changer control gear. Pantograph contact with trolley wire is also more tolerant of high speeds and variations in track geometry. The New York, New Haven and Hartford Railroad had already electrified a portion of its Main Line in 1908 at 11 kV AC 25 Hz and this served as a template for the PRR, which installed its own trial main line electrification between Philadelphia and Paoli, Pennsylvania in 1915. Power was transmitted along the tops of the catenary supports using four single phase, 2 wire 44 kV distribution circuits. Tests on the line using experimental electric locomotives such as the PRR FF1 revealed that the 44 kV distribution lines would be insufficient for heavier loads over longer distances.\n\nIn the 1920s the PRR decided to electrify major portions of its eastern rail network, and because a commercial electric grid did not exist at the time, the railroad constructed its own distribution system to transmit power from generating sites to trains, possibly hundreds of miles distant. To accomplish this the PRR implemented a pioneering system of single-phase high voltage transmission lines at 132 kV, stepped down to the 11 kV at regularly spaced substations along the tracks.\n\nThe first line to be electrified using this new system was between Philadelphia and Wilmington, Delaware in the late 1920s. By 1930, catenary extended from Philadelphia to Trenton, New Jersey, by 1933 to New York City, and by 1935 south to Washington, D.C. Finally in 1939 the main line from Paoli west to Harrisburg was completed along with several freight-only lines. Also included were the Trenton Cutoff and the Port Road Branch. Superimposed on these electrified lines was an independent power grid delivering 25 Hz current from the point of generation to electric locomotives anywhere on nearly 500 route miles (800 km) of track, all under the control of electric power dispatchers in Harrisburg, Baltimore, Philadelphia and New York City.\n\nNortheast railroads atrophied in the years following World War II; the PRR was no exception. The infrastructure of the northeast corridor remained essentially unchanged through the series of mergers and bankruptcies which ended in Amtrak's creation and acquisition of the former PRR lines which came to be known as the Northeast Corridor. The circa 1976 Northeast Corridor Improvement Project had originally planned to convert the PRR's system to the utility grid standard of 60 Hz. Ultimately, this plan was shelved as economically unfeasible and the electrical traction infrastructure was left largely unchanged with the exception of a general traction power voltage increase to 12 kV and a corresponding transmission voltage increase to 138 kV.\n\nDuring the 1970s, several of the original converter or power stations which originally supplied power to the system were shut down. Also the end of electrified through-freight service on the Main Line to Paoli allowed the original 1915 substations and their 44 kV distribution lines to be decommissioned with that section of track being fed from 1930s-era substations on either end. In the decade between 1992 and 2002, several static converter stations were commissioned to replace stations that had or were being shut down. Jericho Park, Richmond, and Sunnyside Yard converters were all installed during this period. This replaced much of the electrical frequency conversion equipment, but the lineside transmission and distribution equipment were unchanged.\n\nIn 2003, Amtrak commenced a capital improvement plan that involved planned replacement of much of the lineside network including 138/12 kV transformers, circuit breakers, and catenary wire. Statistically, this capital improvement has resulted in significantly fewer delays, although dramatic system shutdowns have still occurred.\n\nThe 25 Hz system was built by the Pennsylvania Railroad with a nominal voltage of 11.0 kV. The nominal operating voltages were raised in 1948 and are now:\n\n\nAs of 1997, the system included:\n\nOver 550 GWh of energy are consumed annually by locomotives on the system. If this were consumed at a constant rate over the entire year (although it is not in practice), the average system load would be approximately 63 MW.\n\nThe system power factor varies between 0.75 and around 0.85.\n\nElectrical power originates at seven generation or conversion facilities. The nameplate capacity of all the power sources in the system is about 354 MW. The instantaneous peak loading on the system is 210–220 MW (as of 2009) during the morning rush hour, and up to 225 MW during afternoon. Peak load has risen significantly in the last decade – in 1997 the peak load was 148 MW. As a point of comparison, an HHP-8 electric locomotive is rated for a 6 MW (equivalent to 8,000 hp) mechanical output, after conversion and Head End Power losses.\n\nRegardless of the source, all converter and generator plants supply power to the transmission system at 138 kV, 25 Hz, single phase, using two wires. Typically at least two separate 138 kV circuits follow each right of way to supply the lineside substations.\n\nCurrently, the following converter and generating plants are operable, although all are rarely in operation simultaneously due to maintenance shutdowns and overhaul:\n\nThree types of equipment are currently in operation: hydroelectric generators, motor-generators (sometimes called rotary frequency converters), and static frequency converters.\n\n\nThe 25 Hz machines at the dam are scheduled by Amtrak but owned and operated by Safe Harbor Water Power Company. Amtrak typically uses this source as a peaking power source, similarly to most hydroelectric plants in the US . Like other hydroelectric plants, it also has excellent black start capability. This was most recently demonstrated during the 2006 blackout. After a cascade shutdown of converters had left the network de-energized, it was recovered using Safe Harbor's generators, and the other converters subsequently brought back online.\n\nDuring the twelve-month period ending August 2009, Safe Harbor supplied about 133 GWh of energy to the Amtrak substation at Perryville. Typically, two thirds of the Safe Harbor output is routed through Perryville, the remainder being sent through Harrisburg or Parkesburg. This suggests that Safe Harbor supplies around 200 GWh of energy annually into the 25 Hz network. \n\nMotor-generators and steam turbine generators were the original power sources on the PRR traction power network. The last steam turbine shut down in 1954, but some of the original motor generators remain. Although the converting machines are frequently called 'rotary converters' or 'rotary frequency converters', they are not the rotary converter used frequently by subways to convert low-frequency alternating current to DC power. The converters used are more precisely described as motor generators and consist of two synchronous AC machines on a common shaft with different ratios of poles; they are not electrically connected as in a true rotary converter.\n\nPrincipal advantages of motor generators include very high fault current ratings and clean output current. Solid state electronics can be damaged very quickly, so the microprocessor control systems react very quickly to over-correct conditions to place the converter in a safe, idle mode; or to trip the output circuit breaker. Motor generators, being of 1930s design, are heavily overbuilt. These rugged machines can absorb large load transients and demanding fault conditions, while continuing to remain online. Their output waveform is also perfectly sinusoidal without noise or higher harmonic output. They can actually absorb harmonic noise produced by solid-state devices, effectively serving as a filter. These attributes, combined with their high fault-current capability, make them desirable in a stabilizing role within the power system. Amtrak has retained two of the original converter plants and plans to overhaul them and continue their operation indefinitely.\n\nDisadvantages of motor generators include lower efficiency, generally between 83% (lightly loaded machine) and 92% (fully loaded machine). In comparison, cycloconverter efficiency can exceed 95%. Also, motor generators require more routine maintenance due their nature as rotating machines, given the bearings and slip rings. Today, the outright replacement of motor generators would also be difficult due to the high manufacturing cost and limited demand for these large 25 Hz machines.\n\n\nThe static converters in the system were commissioned during the decade between 1992 and around 2002. Static converters use high-power solid-state electronics, with few moving parts. Chief advantages of static converters over motor generators include lower capital cost, lower operating costs, and higher conversion efficiency. The Jericho Park converter exceeds its efficiency design criteria of 95%. Major disadvantages of solid state converters include harmonic frequency generation on both the 25 Hz and 60 Hz sides, and lower overload capability.\n\n\nThe majority of power sources in the original Pennsylvania Railroad electrification were built prior to 1940. Some have been retired out-right, others have been replaced with co-located static frequency converters, and others remain in service and will be refurbished and operated indefinitely. The following tables lists sources which are no longer in service.\n\nDuring the beginning of the 20th century, 25 Hz power was much more readily available from commercial electrical utilities. The vast majority of urban subway systems used 25 Hz power to supply their lineside rotary converters used to generate the DC voltage supplied to the trains. Since rotary converters work more efficiently with lower frequency supplies, 25 Hz was a common supply frequency for these machines. Rotary converters have been steadily replaced over the past 70 years with, at first, mercury arc rectifiers and more lately solid-state rectifiers. Thus, the need for special frequency power for urban traction has disappeared, along with the financial motivation for utilities to operate generators at these frequencies.\n\nLong Island City Power Station in Hunter's Point, NY was built by the Pennsylvania Railroad in 1906 in preparation for the North River Tunnels and the opening of Pennsylvania Station in Manhattan. The station consisted of 64 coal-fired boilers and three steam turbine generators with a total capacity of 16 MW. In 1910, the station was expanded with two additional turbine generators for a total capacity of 32.5 MW. Power was transmitted to rotary converters (AC to DC machines) for use in the PRR's original third rail electrification scheme. Like most DC electric distribution systems of the time (Thomas Edison's being the most famous), 25 Hz power was used to drive rotary converters at substations along the line. Some sources state that the station was largely dormant by the 1920s. When AC overhead electrification was extended in the 1930s, Long Island City connected to the 11 kV catenary distribution system. Operation of the station was transferred to Consolidated Edison in 1938, although ConEd began supplying power from the adjacent Waterside Generating Station, most likely due to declining overall demand for 25 Hz power. The station was disused and sold in the mid-1950s. \n\nOriginally constructed by Consolidated Edison to supply power to their DC distribution system in Manhattan, Waterside began supplying power to the PRR's AC system around 1938 when ConEd assumed operation of the Long Island City Station. The single-phase turbine generators were retired in the mid-1970s due to safety concerns. Two transformers were installed to supply catenary power from remaining (three-phase) portions of ConEd's still relatively extensive 25 Hz system. Power flow management problems prevented usage of this source under other than emergency conditions. \n\nIn 1986, Baltimore Gas and Electric elected to not renew the contract under which it had operated the Benning Power Station frequency changer on behalf of Amtrak. They proposed a static frequency changer which was built at Jericho Park (Bowie, Maryland) and placed on service in the spring of 1992. \n\nAlthough reactive power has primarily been supplied along with real power by the steam turbines and motor generators of the system, the PRR briefly used two synchronous condensers. Shortly after commissioning the 1915 electrification, the railroad discovered that the 44 kV feeders and large inductive loads on the system were causing significant voltage sag. The supplying electric utility (Philadelphia Electric) also discovered that power factor correction was needed. In 1917, the PRR installed two 11 kV, 4.5 MVA synchronous converters at Radnor, the approximate center point of the system load. This substation was located at the site of water tanks used to supply water to track pans which supplied water to conventional locomotives. At some later time, the converters were shut down and removed. Dedicated machines for reactive power support have not been used subsequently by either the PRR or Amtrak. \n\nThe PRR's original 1915 electrification made use of four substations, at Arsenal Bridge, West Philadelphia, Bryn Mawr, and Paoli. The Arsenal Bridge substation stepped-up 13.2 kV, 25 Hz power supplied from PECO's Schuylkill power station on Christian Street to 44 kV for distribution. The remaining three substations reduced the 44 kV distribution voltage to 11 kV catenary voltage. The substations were operated from adjacent signal towers. They used typical period concrete buildings to house the transformers and switchgear while the line terminals were on the roof. From 1918 onward, outdoor stations were used and when the main line electrification began in 1928 the stations became large open air structures using lattice steel frameworks to mount the 132 kV terminations and switchgear. By 1935 new stations were connected to remote supervision systems allowing power directors to open and close switches and breakers from central offices without having to go through the tower operators.\n\nToday about 55 substations are part of Amtrak's network. Substations are spaced on average 8 miles apart and feed 12 kV catenary circuits in both directions along the line. Thus the catenary is segmented (via section breaks, also called 'sectionalizations' by the PRR) at each substation, and each substation feeds both sides of a catenary's section break. A train traveling between two substations draws power through both transformers.\n\nA typical substation includes two to four 138/12 kV transformers, 138 kV air switches that permit isolation of individual transformers, shutdown one of the two 138 kV feeders, or cross-connection from one feeder to another. The output of the transformers is routed to the catenary via 12 kV circuit breakers and air disconnect switches. Cross-connect switches allow one transformer to feed all catenary lines.\n\nThe PRR substation architecture was based on a long distance, high speed railway. The substation spacing ensures that any train is never more than 4 or 5 miles from the nearest substation, which minimized voltage drop. One disadvantage to the substation design as originally built by the PRR concerns its lack of 138 kV circuit breakers. Essentially all segmentation of the 138 kV system must be manually accomplished, making rapid isolation of a fault on the 138 kV line difficult.\n\nFaults in one part of the line also affect the entire distribution system since it is impossible for the 138 kV transmission system to protect or reconfigure itself during a fault condition. High voltage faults generally are cleared by opening converter output breakers, which causes a concurrent loss of the converter. The system does not degrade gracefully under high voltage faults. Rather than isolating, for example, the south 138 kV feeder between Washington and Perryville, the system would require opening converter output breakers at Jericho Park and Safe Harbor. This results in loss of much more of the network than is required to simply isolate the fault.\n\nAll transmission lines within the 25 Hz system are two-wire, single-phase, 138 kV. The center tap of each 138 kV/12 kV transformer is connected to ground, thus the two transmission lines are tied to ±69 kV with respect to ground and 138 kV relative to each other.\n\nGenerally two separate two-wire circuits travel along the rail line between substations. One circuit is mounted at the top of the catenary poles on one side of the track; the second circuit runs along the other side.\n\nThe arrangement of catenary supports and transmission wires gives the overhead structure along former Pennsylvania Railroad lines its characteristic -tall 'H'-shaped structure. They are much taller than the overhead electrification structures on other electrified American railroads due to the 138 kV transmission lines. Catenary towers and transmission lines along former New York, New Haven and Hartford Railroad lines and Amtrak's New England division are much shorter, and are recognizable due to different design and construction.\n\nWhile a majority of the transmission infrastructure is located directly above the rail lines on the same structure that supports the catenary system, some lines are either located above lines that have been de-electrified or abandoned or in a few cases on completely independent rights of way.\n\nThe following is a list of all major segments of the 25 Hz 138 kV transmission infrastructure listing substations (SS or Sub) or high-tension switching stations (HT Sw'g) as termini. For clarity, positions of substations are not repeated in this table. A listing of the high-tension switching stations follows.\n\nAmtrak's capital improvement program which began in 2003 has continued to the present day and has since 2009, received added support from economic stimulus funding sources (American Recovery and Reinvestment Act of 2009 or ARRA).\n\nMajor improvements in 2010 included:\n\nMajor improvements planned for the future include:\n\nThe Ivy City substation project marked the first extension of 138 kV transmission line since Safe Harbor Dam was constructed in 1938. In the original PRR electrification scheme, the 138 kV transmission lines went south from Landover to the Capital South substation rather than following the line through Ivy City to the northern approach to Union Station. The two tracks between Landover and Union Station had no high voltage transmission line above them; Union Station catenary was fed at 12 kV from the Landover and Capitol substations (the latter via the First Street Tunnels). When the Capitol South substation was abandoned, coincident with the de-electrification of the track between Landover and Potomac Yard, Union Station and its approaches became a single-end fed section of track. This combined with rising traffic levels resulted in low voltage conditions on the approaches to Union Station and decreased system reliability.\n\nThe Ivy City project resulted in the installation of two 4.5 MVA transformers in a 138/12 kV substation on the northeast edge of the Ivy City yard complex and of 138 kV transmission line to augment the overstretched facilities at Landover. Since the original catenary supports along this section of track were only high enough for the 12 kV catenary wire, the 138 kV lines were installed on new steel monopod poles installed along the right-of-way. Except for the fact that the new poles only carry four conductors rather than the typical six for a utility line, the new line appears as a typical medium voltage power line rather than the typical PRR style H-shaped structure.\n\nIn 2011, Amtrak replaced the transmission lines that tie the Conestoga Substation to Parkesburg via Atglen. These lines were originally installed over the Atglen and Susquehanna Branch. The line was subsequently abandoned by Conrail and the tracks removed, but Amtrak has retained an easement to operate its 138 kV transmission lines over the roadbed. Towers and conductors and wire over of the route were replaced; work was completed in September 2011. The scope of work included:\nFunding for this project was included under the ARRA program. The specified number of poles, spaced approximately per tower, is approximately twice as far apart as the span length between the 1930s structures, which averaged .\n\nIn late 2010, Amtrak solicited design services for new transmission lines between Paoli and Zoo substations. Primary objectives of this expansion include improving reliability of transmission between Safe Harbor and Philadelphia, and reducing maintenance costs. This project complements the Safe Harbor to Atglen transmission line replacement, which has already been completed.\n\nThe Zoo to Paoli transmission line would replace the current supply scheme which uses 138 kV lines which run circuitously along the SEPTA Cynwyd Line, the Schuylkill Branch rail-trails and the Trenton Cut-off between the Zoo and Frazer substations. The new routing will reduce maintenance costs, as Amtrak must maintain transmission poles and control vegetation along right-of-way which it neither owns nor uses for revenue service. The conceptual line will run from the existing Paoli substation to the junction of the Harrisburg to Philadelphia main line and SEPTA's Cynwyd Line at 52nd Street in West Philadelphia. .\n\nThe new lines would connect to the existing 1ED and 2ED circuits, which would be abandoned between the junction and their current terminus at the Earnest Junction HT Switch. The plan also includes construction of a 138/12 kV substation at Bryn Mawr to replace the existing switching station. The existing 1915 catenary structures are planned for replacement, and new transmission supports will be compatible with catenary replacement.\n\nA new substation (Number 34A) called Hamilton was constructed in Mercer County, NJ. Work on the site began in early 2013, and the substation sap put into service in early 2015.\n\nThe Morton #01 and Lenni #02 substations are owned by SEPTA and supply the Media/Elwyn Line; therefore, they are not covered by Amtrak capital funding programs. SEPTA's own capital improvement plan, formulated in late 2013 after passage of funding legislation in Pennsylvania, allowed for the renewal of all components at Morton and Lenni.\n\nIn October 2014 SEPTA requested interested contractors to submit bids for the rehabilitation of Lenni substation. In December 2014 SEPTA awarded a $6.82 million contract to Vanalt Electrical for the work. The work was completed by the end of fall 2016.\n\nIn February 2014 SEPTA awarded a $6.62 million contract to Philips Brothers Electrical Contractors Inc. for the rehabilitation of Morton substation. The work was completed by the end of fall 2016.\n\nDespite the recent capital improvements throughout the system, several high-profile power failures have occurred along the NEC in recent years.\n\nOn May 25, 2006, during restoration from maintenance on one of the Richmond inverter modules, a command to restore the module to full output capability was not executed. The system tolerated this reduced capacity for about 36 hours, during which time the problem went unnoticed. During rush hour the next morning (May 26), the overall capacity became overloaded:\n\n\nBy 8:03 am, the entire 25 Hz system, stretching from Washington, D.C. to Queens, New York, was shut down. About 52,000 people were stranded on trains or otherwise affected. Two New Jersey Transit trains stranded under the Hudson River were retrieved by diesel locomotives. Restoration was hampered by policies which allowed the converter stations to operate unattended during rush hour periods. The 25 Hz system was restored by a 'black start' using the Safe Harbor water turbines, and most service along the system returned to normal by mid-afternoon. Amtrak subsequently improved its system of maintaining 'rescue' diesel locomotives near the Hudson River tunnels.\n\nLow system voltage around New York City caused a halt of trains in and around the New York area at 8:45 am on Wednesday, December 23, 2009. Power was never fully lost, and full voltage was restored by 11:30 am. Amtrak stated that an electrical problem in North Bergen, New Jersey (near the western portal and the Union City substation) caused the problem, but did not further elaborate on the nature of the malfunction.\n\nLow system voltages beginning at 7:45 am on Tuesday, August 24, 2010, caused Amtrak to order an essentially system-wide stoppage of trains within the 25 Hz traction network. Slow-speed service was gradually restored, and the power problem was corrected by 9:00 am, although delays persisted the remainder of the morning.\n\nOn October 29, 2012, Hurricane Sandy struck the northeast coast of the U.S. Augmented by a nor'easter, the storm surge from Sandy raced through the Hackensack Meadows, severely damaging (among other railroad infrastructure) Kearney Substation #41 and knocking it offline. This loss of electrical capacity forced Amtrak and New Jersey Transit to operate fewer trains, using modified weekend schedules. With assistance from the U.S. Army Corps of Engineers, the substation was isolated from floodwaters and then dewatered. After testing the substation's components, the degree of damage was determined to be less than initially feared, and after further repairs, Kearney Substation came back on-line on Friday, November 16, allowing the immediate return of all Amtrak and gradual return of all NJ Transit electric trains into Penn Station through the dewatered North River Tunnels.\n\nAmtrak has since requested federal funding to upgrade Kearny substation so it is high enough to not be affected by flood water.\n\n"}
{"id": "6590445", "url": "https://en.wikipedia.org/wiki?curid=6590445", "title": "Anaerobic digester types", "text": "Anaerobic digester types\n\nThe following is a partial list of types of anaerobic digesters. These processes and systems harness anaerobic digestion for purposes such as sewage treatment and biogas generation. Anaerobic digesters can be categorized according to two main criteria: by whether the biomass is fixed to a surface (\"attached growth\") or can mix freely with the reactor liquid (\"suspended growth\"); and by the organic loading rate (the influent mass rate of chemical oxygen demand per unit volume). The widely used UASB reactor, for example, is a suspended-growth high-rate digester, with its biomass clumped into granules that will settle relatively easily and with typical loading rates in the range 5-10 kgCOD/m/d.\n\nExamples of anaerobic digesters include:\n"}
{"id": "16465743", "url": "https://en.wikipedia.org/wiki?curid=16465743", "title": "Banana production in the Caribbean", "text": "Banana production in the Caribbean\n\nBanana production in the Caribbean is widespread. Bananas are cultivated by both small farmers and large land holders. The plant is perennial and is planted either in pure stands or in mixed cultivation, such as in Jamaica. Countries where bananas are a main export crop are Belize, Costa Rica, Dominican Republic, Honduras, Jamaica, Guadeloupe, Dominica, Martinique, Saint Lucia, Saint Vincent and the Grenadines, Grenada, Trinidad and Tobago, Nicaragua, Panama, Suriname and Colombia.\n\nMechanical tilling of the land is undertaken on gentle slopes. On flat land, harrowing is carried out to renew the soil fertility.\nThe plant is propagated by roots or from suckers and takes one year to reach maturity. Each planting area consists of one bearing plant. When the crop is bearing, bunch sleeving is carried out. Each bunch is covered with a polythene bag and treated with insecticide. \nWhen the mature fruit is about 12 months old, it is cut directly from the tree in a green condition. A cutter grooves and cuts the tree near the base while a backer catches the cut bunch. The upper part of the pseudo-stem is not removed as it is suitable for mulching. New plants or suckers are left to grow from the rhizomes. The bananas are carried on the heads of laborers to the nearest road, then transported by trucks and motor trailers to the packing shed. From the plantations, bunches of bananas are loaded onto cable ways, leading to the packing shed where the choicest fruit is selected for export. As the crop is perishable, timing is of utmost importance. Fruit must be cut within a week's time to meet a shipping load.\n\nAt the boxing plant, the bananas selected are hung on a conveyor and removed by hand from the stalk. They are then placed in tanks of water for at least four minutes to wash off the latex and other impurities. Every precautionary method must be taken against damage to the skin of the fruit. Wad wrapping and polythene bags are commonly used. The latex flow tends to be absorbed by the wad wrapping. \n\nAt the port, the fruit is graded according to quality. It is weighed and placed in refrigerated compartments of ships at temperatures of . It undergoes regular changes of air to discharge ethylene gases produced by the chemistry of the fruit. In this way, the fruit can withstand long sea journeys and arrive in good condition. Shipping schedules are arranged about eight weeks in advance.\n\nThe industry provides a very valuable source of foreign exchange. Dominica earned about EC$40 million in 1998 and produced 30,000 tons in 1999; St.Lucia produced 80,000 tons and earned about EC$87.6 million in 1999; St.Vincent about EC$38.9 million in 1997 and produced 43,000 tons in 1999: Belize earned about BZ$43.16 million in 1998 and produced 78,000 tons in 1999; Jamaica earned about US$29.6 million in 1999 and produced about 130,000 tons and Dominican Republic produced 432,000 tons. \n\nOther Caribbean producing countries are: Haiti produced 290,000 tons in 1999; Cuba produced 115,000 tons in 1999; Colombia produced 1,600,000 tons in 2005; Costa Rica produced 2,200,000 tons in 2005; Mexico produced 2,000,000 tons in 2005; Guatemala produced 733,000 tons in 1999; Honduras produced 861,000 tons in 1999; Nicaragua produced 69,000 tons in 1999; Panama produced 650,000 tons and Venezuela produced 1,000,000 tons.\n\nBanana cultivation is a major employer of rural labor as it is a labor-intensive industry. In Dominica, it is the second largest employer after the government, providing work for 6,000 farmers with another 700 employed at boxing plants. In St. Lucia, it provides employment for about 10,000 workers. In St.Vincent, there are about 5,000 banana growers.\n\n\nThe small size of farms makes for inefficient production and contributes to a low standard of living. In Dominica, 8% of the fruit is supplied by 51% of the banana farming population, medium growers supply 41% and over 51% is supplied by large land holders. In the Windward group generally, the majority of the farming population owns less than a third of the banana acreage. In Grenada, there are a large number of peasant plots given to intercrop cultivation with bananas. In St. Vincent, the pattern is the same, although output is greater from the estates. The small size of farms provides a low income to farmers as they are unable to apply fertilizers or minimize the risk of insect pests and diseases.\n\nLow income returns to farmers are experienced when low prices are offered though shipping and marketing agents. As a result, farmers are unable to effectively increase their production. They are unable to apply fertilizers or to minimize risks caused by diseases. The industry is plagued with high costs and low prices. Bananas are also plentiful in Latin America and Africa, thus imposing a threat to the Caribbean's position on the British market. The United Kingdom imports a high percentage of their fruit from the Caribbean. Too great a supply reduces the price of the fruit. This is an economic disadvantage since the fruit is perishable.\n\nIn Grenada, the Moko disease (Ralstonia solanacearum) has caused much destruction in the northern part of the island while it has eradicated the industry in Trinidad and Tobago out of commercial production during the 1960s. Black sigatoka (Mycosphaerella fijiensis) causes premature ripening of the fruit. It has wiped out 40% of Dominica's of banana cultivation. Even when the fruit is exported there is a high incidence of ship-ripe fruit which arrive at their destination in poor condition.\n\nPanama disease (Fusarium oxysporum) is caused by tiny worms which feed on the roots of the plants, block the water vessels and reduce production. Production is also affected by pests which scar the banana peel. The placing of sleeves over the bunches and the removal of the buds are two controlling measures.\n\n\n"}
{"id": "2081252", "url": "https://en.wikipedia.org/wiki?curid=2081252", "title": "Beneficiation", "text": "Beneficiation\n\nIn the mining industry or extractive metallurgy, beneficiation is any process that improves (benefits) the economic value of the ore by removing the gangue minerals, which results in a higher grade product (concentrate) and a waste stream (tailings). Examples of beneficiation processes include froth flotation and gravity separation.\n\nOther economic uses of the term are in use. For example, in the diamond industry, the \"beneficiation imperative\" argues that cutting and polishing (processes within the diamond value chain) should be conducted in-country to maximise the local economic contribution.\n\n\n"}
{"id": "6944639", "url": "https://en.wikipedia.org/wiki?curid=6944639", "title": "Brittle–ductile transition zone", "text": "Brittle–ductile transition zone\n\nSections of fault zones once active in the transition zone, and now exposed at the surface, typically have a complex overprinting of brittle and ductile rock types. Cataclasites or pseudotachylite breccias with mylonite clasts are common, as are ductily deformed cataclasites and pseudotachylites.\n\n\n"}
{"id": "11111587", "url": "https://en.wikipedia.org/wiki?curid=11111587", "title": "China Southern Airlines Flight 3456", "text": "China Southern Airlines Flight 3456\n\nChina Southern Airlines Flight 3456 (CZ3456/CSN3456) was a scheduled domestic passenger flight from Chongqing to Shenzhen Huangtian Airport (now Shenzhen Bao'an International Airport), Guangdong, China. On May 8, 1997, the Boeing 737 performing this route crashed during the second attempt to land in a thunderstorm. \nThe flight number 3456 is still used by China Southern for the Chongqing-Shenzhen route but with Airbus A320 family aircraft.\n\nThe aircraft was a Boeing 737-31B registered as B-2925 and with serial number 27288. The aircraft was delivered to China Southern on February 2, 1994, and had recorded over 8,500 hours before the crash. The aircraft was powered by 2 CFM International CFM56-3C1 turbofan engine.\n\nThe captain in command was 45-year-old Yougui Lin (Chinese: 林友贵), he had logged over 15,500 hours of total flying time, of which 11,200 hours as flight engineer and 4,300 hours as a pilot. The first officer was 36-year-old Dexin Kong (Chinese: 孔德新), he had logged more than 12,700 hours of total flying time, including 9,100 hours as flight engineer and 3,600 hours as a pilot.\n\nThe weather reported by Shenzhen Airport from 17:00 of 8 May to 02:00 of 9 May was: \"170 degrees wind at 7 with rain, visibility , overcast at , variable winds at , thunderstorm may appear.\"\n\nAt 18:00, 8 May, severe weather warning was issued: \"report to airports, air traffic controls and airline companies: Thunderstorm with strong winds will appear, all departments including the crew who will be taking off should be notified.\" At 21:33, the weather recorded was 290 degrees wind at , visibility , showers, low clouds at , cumulonimbus at , temperature at 23 ℃.\n\nOn May 8, 1997, Flight 3456 took off from Chongqing Jiangbei International Airport at 19:45 local time (UTC+8), expected to arrive Shenzhen Huangtian Airport at 21:30. At 21:07, the Shenzhen Airport approach controller cleared the flight to the approach of Runway 33. At 21:17, the Tower informed the crew \"heavy rain on final, advise when spotting the runway\". At 21:18:07, the crew stated they have established ILS approach. At 21:18:53, the crew advised ATC that they spotted the approach lightings, and the controller cleared the aircraft to land. The controller was able to see the landing light of the plane, but it was not clear due to the rain. At 21:19:33, the aircraft touched down on the south of the runway, bounced three times, damaged the aircraft's nose gear, hydraulic systems and flaps. The crew decided to go around.\n\nThe aircraft made a left turn while climbing up to . The crew were asked to turn on the transponder to show the ATC their position, but the secondary surveillance radar did not receive any signal from the aircraft, indicated the transponder was off. At 21:23:57, the crew informed the ATC they are on the downwind side, and requested other aircraft to clear off the airspace for Flight 3456's landing. At 21:23:40, the crew declared an emergency and requested to clear the approach again. At the time, the main warning, hydraulic system warning and the gear warning were all triggered in the cockpit. At 21:24:58, the crew asked for a full emergency airfield support. The aircraft then turned around, reported will land towards the south, which was approved. At 21:28:30, the aircraft skidded off the runway, broke into three pieces and caught on fire, killing 33 passengers and 2 crew members.\n\nThe first landing attempt was toward north. Debris from the nose gear was found scattered near the southern end of the runway, indicating the left front tyre had exploded during the first touch down. Fallouts including rivets, metal sheets, rubber tube and retaining clip could also be found on the runway surface.\n\nThe second landing attempt was toward south. A clear surface scratch from the fuselage was found from the runway threshold. The aircraft disintegrated after rolling approximately across the runway and burst into flames. The central part of the fuselage and the trailing edge of the right wing received the most severe burning damage. The front section of the fuselage was long with nose pointing north, partially damaged, showing rolling and rotating trace but no signs of burning. A large amount of mire was filled in the deformed cockpit. The rear section was relatively intact, and was the only section not destroyed. The left main gear and the right engine were scattered on the left side of the runway.\n\nOn May 5, 1997 (???Wrong date???), , a national news show aired at TVB Jade, provided a casualty list for the accident.\nIn June 2007, an audio recording reputed to be the last 12 min. 27 sec. recorded by the cockpit voice recorder of Flight 3456 was leaked on the Internet. According to an expert from the Civil Aviation Administration of China, the recording is unlikely to be fake.\n\n\n"}
{"id": "3053507", "url": "https://en.wikipedia.org/wiki?curid=3053507", "title": "Crystal growth", "text": "Crystal growth\n\nCrystal growth is the process where a pre-existing crystal becomes larger as more molecules or ions add in their positions in the crystal lattice or a solution is developed into a crystal and further growth is processed . A crystal is defined as being atoms, molecules, or ions arranged in an orderly repeating pattern, a crystal lattice, extending in all three spatial dimensions. So crystal growth differs from growth of a liquid droplet in that during growth the molecules or ions must fall into the correct lattice positions in order for a well-ordered crystal to grow. The schematic shows a very simple example of a crystal with a simple cubic lattice growing by the addition of one additional molecule.\n\nWhen the molecules or ions fall into the positions different from those in a perfect crystal lattice, crystal defects are formed. Typically, the molecules or ions in a crystal lattice are trapped in the sense that they cannot move from their positions, and so crystal growth is often irreversible, as once the molecules or ions have fallen into place in the growing lattice, they are fixed in place.\n\nCrystallization is a common process, both in industry and in the natural world, and crystallization is typically understood as consisting of two processes. If there is no pre-existing crystal, then a new crystal must nucleate, and then this crystal must undergo crystal growth.\n\nThe interface between a crystal and its vapor can be molecularly sharp at temperatures well below the melting point. An ideal crystalline surface grows by the spreading of single layers, or equivalently, by the lateral advance of the growth steps bounding the layers. For perceptible growth rates, this mechanism requires a finite driving force (or degree of supercooling) in order to lower the nucleation barrier sufficiently for nucleation to occur by means of thermal fluctuations. In the theory of crystal growth from the melt, Burton and Cabrera have distinguished between two major mechanisms:\n\nThe surface advances by the lateral motion of steps which are one interplanar spacing in height (or some integral multiple thereof). An element of surface undergoes no change and does not advance normal to itself except during the passage of a step, and then it advances by the step height. It is useful to consider the step as the transition between two adjacent regions of a surface which are parallel to each other and thus identical in configuration — displaced from each other by an integral number of lattice planes. Note here the distinct possibility of a step in a diffuse surface, even though the step height would be much smaller than the thickness of the diffuse surface.\n\nThe surface advances normal to itself without the necessity of a stepwise growth mechanism. This means that in the presence of a sufficient thermodynamic driving force, every element of surface is capable of a continuous change contributing to the advancement of the interface. For a sharp or discontinuous surface, this continuous change may be more or less uniform over large areas each successive new layer. For a more diffuse surface, a continuous growth mechanism may require change over several successive layers simultaneously.\n\nNon-uniform lateral growth is a geometrical motion of steps — as opposed to motion of the entire surface normal to itself. Alternatively, uniform normal growth is based on the time sequence of an element of surface. In this mode, there is no motion or change except when a step passes via a continual change. The prediction of which mechanism will be operative under any set of given conditions is fundamental to the understanding of crystal growth. Two criteria have been used to make this prediction:\n\nWhether or not the surface is \"diffuse\": a diffuse surface is one in which the change from one phase to another is continuous, occurring over several atomic planes. This is in contrast to a sharp surface for which the major change in property (e.g. density or composition) is discontinuous, and is generally confined to a depth of one interplanar distance.\n\nWhether or not the surface is \"singular\": a singular surface is one in which the surface tension as a function of orientation has a pointed minimum. Growth of singular surfaces is known to requires steps, whereas it is generally held that non-singular surfaces can continuously advance normal to themselves.\n\nConsider next the necessary requirements for the appearance of lateral growth. It is evident that the lateral growth mechanism will be found when any area in the surface can reach a metastable equilibrium in the presence of a driving force. It will then tend to remain in such an equilibrium configuration until the passage of a step. Afterward, the configuration will be identical except that each part of the step but will have advanced by the step height. If the surface cannot reach equilibrium in the presence of a driving force, then it will continue to advance without waiting for the lateral motion of steps.\n\nThus, Cahn concluded that the distinguishing feature is the ability of the surface to reach an equilibrium state in the presence of the driving force. He also concluded that for every surface or interface in a crystalline medium, there exists a critical driving force, which, if exceeded, will enable the surface or interface to advance normal to itself, and, if not exceeded, will require the lateral growth mechanism.\n\nThus, for sufficiently large driving forces, the interface can move uniformly without the benefit of either a heterogeneous nucleation or screw dislocation mechanism. What constitutes a sufficiently large driving force depends upon the diffuseness of the interface, so that for extremely diffuse interfaces, this critical driving force will be so small that any measurable driving force will exceed it. Alternatively, for sharp interfaces, the critical driving force will be very large, and most growth will occur by the lateral step mechanism.\n\nNote that in a typical solidification or crystallization process, the thermodynamic driving force is dictated by the degree of supercooling.\n\nIt is generally believed that the mechanical and other properties of the crystal are also pertinent to the subject matter, and that crystal morphology provides the missing link between growth kinetics and physical properties. The necessary thermodynamic apparatus was provided by Josiah Willard Gibbs'study of heterogeneous equilibrium. He provided a clear definition of surface energy, by which the concept of surface tension is made applicable to solids as well as liquids. He also appreciated that \"an anisotropic surface free energy implied a non-spherical equilibrium shape\", which should be thermodynamically defined as \"the shape which minimizes the total surface free energy\".\n\nIt may be instructional to note that whisker growth provides the link between the mechanical phenomenon of high strength in whiskers and the various growth mechanisms which are responsible for their fibrous morphologies. (Prior to the discovery of carbon nanotubes, single-crystal whiskers had the highest tensile strength of any materials known). Some mechanisms produce defect-free whiskers, while others may have single screw dislocations along the main axis of growth — producing high strength whiskers.\n\nThe mechanism behind whisker growth is not well understood, but seems to be encouraged by compressive mechanical stresses including mechanically induced stresses, stresses induced by diffusion of different elements, and thermally induced stresses. Metal whiskers differ from metallic dendrites in several respects. Dendrites are fern-shaped like the branches of a tree, and grow across the surface of the metal. In contrast, whiskers are fibrous and project at a right angle to the surface of growth, or substrate.\n\nVery commonly when the supersaturation (or degree of supercooling) is high, and sometimes even when it is not high, growth kinetics may be diffusion-controlled. Under such conditions, the polyhedral crystal form will be unstable, it will sprout protrusions at its corners and edges where the degree of supersaturation is at its highest level. The tips of these protrusions will clearly be the points of highest supersaturation. It is generally believed that the protrusion will become longer (and thinner at the tip) until the effect of interfacial free energy in raising the chemical potential slows the tip growth and maintains a constant value for the tip thickness.\n\nIn the subsequent tip-thickening process, there should be a corresponding instability of shape. Minor bumps or \"bulges\" should be exaggerated — and develop into rapidly growing side branches. In such an unstable (or metastable) situation, minor degrees of anisotropy should be sufficient to determine directions of significant branching and growth. The most appealing aspect of this argument, of course, is that it yields the primary morphological features of dendritic growth.\n\n"}
{"id": "32932764", "url": "https://en.wikipedia.org/wiki?curid=32932764", "title": "D&amp;D KM-IT", "text": "D&amp;D KM-IT\n\nThe Deactivation and Decommissioning Knowledge Management Information Tool (D&D KM-IT) is a centralized web-based knowledge management information tool built for the D&D user community by the Applied Research Center (ARC) at Florida International University (FIU) in collaboration with the United States Department of Energy (DOE), the Energy Facility Contractors Group (EFCOG), and the former ALARA (As-Low-As-Reasonably-Achievable) Centers at the Hanford Site and Savannah River Site.\n\nKnowledge management is the process of gathering, analyzing, storing and sharing knowledge and information within an organization or community. The purpose of this process is to improve efficiency by reducing the need to rediscover the knowledge and to promote reuse of the existing knowledge. To prevent the loss of nuclear decommissioning and deactivation (D&D) knowledge and expertise that has been gained over the years by employees and contractors of the DOE, a knowledge management system has been created specifically for the D&D community to collect, consolidate, and share the information.\n\nDeactivation and decommissioning work is a high priority across the DOE complex. Subject matter specialists associated with the different former ALARA Centers, DOE sites and the D&D community have gained extensive knowledge and experience over the years. To prevent the D&D knowledge and expertise from being lost over time, an approach is needed to capture and maintain this valuable information.\n\n"}
{"id": "944638", "url": "https://en.wikipedia.org/wiki?curid=944638", "title": "Earth's energy budget", "text": "Earth's energy budget\n\nEarth's energy budget accounts for the balance between the energy Earth receives from the Sun, the energy Earth radiates back into outer space after having been distributed throughout the five components of Earth's climate system and having thus powered the so-called Earth’s heat engine. This system is made up of earth's water, ice, atmosphere, rocky crust, and all living things.\n\nQuantifying changes in these amounts is required to accurately model the Earth's climate. \n\nReceived radiation is unevenly distributed over the planet, because the Sun heats equatorial regions more than polar regions. The atmosphere and ocean work non-stop to even out solar heating imbalances through evaporation of surface water, convection, rainfall, winds, and ocean circulation. Earth is very close to being in radiative equilibrium, the situation where the incoming solar energy is balanced by an equal flow of heat to space; under that condition, global temperatures will be \"relatively\" stable. Globally, over the course of the year, the Earth system—land surfaces, oceans, and atmosphere—absorbs and then radiates back to space an average of about 240 watts of solar power per square meter. Anything that increases or decreases the amount of incoming or outgoing energy will change global temperatures in response.\n\nHowever, Earth's energy balance and heat fluxes depend on many factors, such as atmospheric composition (mainly aerosols and greenhouse gases), the albedo (reflectivity) of surface properties, cloud cover and vegetation and land use patterns.\n\nChanges in surface temperature due to Earth's energy budget do not occur instantaneously, due to the inertia of the oceans and the cryosphere. The net heat flux is buffered primarily by becoming part of the ocean's heat content, until a new equilibrium state is established between radiative forcings and the climate response.\n\nIn spite of the enormous transfers of energy into and from the Earth, it maintains a relatively constant temperature because, as a whole, there is little net gain or loss: Earth emits via atmospheric and terrestrial radiation (shifted to longer electromagnetic wavelengths) to space about the same amount of energy as it receives via insolation (all forms of electromagnetic radiation).\n\nTo quantify Earth's \"heat budget\" or \"heat balance\", let the insolation received at the top of the atmosphere be 100 units (100 units = about 1,360 watts per square meter facing the sun), as shown in the accompanying illustration. Called the albedo of Earth, around 35 units are reflected back to space: 27 from the top of clouds, 2 from snow and ice-covered areas, and 6 by other parts of the atmosphere. The 65 remaining units are absorbed: 14 within the atmosphere and 51 by the Earth’s surface. These 51 units are radiated to space in the form of terrestrial radiation: 17 directly radiated to space and 34 absorbed by the atmosphere (19 through latent heat of condensation, 9 via convection and turbulence, and 6 directly absorbed). The 48 units absorbed by the atmosphere (34 units from terrestrial radiation and 14 from insolation) are finally radiated back to space. These 65 units (17 from the ground and 48 from the atmosphere) balance the 65 units absorbed from the sun in order to maintain zero net gain of energy by the Earth.\n\nThe total amount of energy received per second at the top of Earth's atmosphere (TOA) is measured in watts and is given by the solar constant times the cross-sectional area of the Earth. Because the surface area of a sphere is four times the cross-sectional surface area of a sphere (i.e. the area of a circle), the average TOA flux is one quarter of the solar constant and so is approximately 340 W/m². Since the absorption varies with location as well as with diurnal, seasonal and annual variations, the numbers quoted are long-term averages, typically averaged from multiple satellite measurements.\n\nOf the ~340 W/m² of solar radiation received by the Earth, an average of ~77 W/m² is reflected back to space by clouds and the atmosphere and ~23 W/m² is reflected by the surface albedo, leaving ~240 W/m² of solar energy input to the Earth's energy budget. This gives the earth a mean net albedo of 0.29.\n\nThe geothermal heat flux from the Earth's interior is estimated to be 47 terawatts. This comes to 0.087 watt/square metre, which represents only 0.027% of Earth's total energy budget at the surface, which is dominated by 173,000 terawatts of incoming solar radiation.\n\nHuman production of energy is even lower, at an estimated 18 TW.\n\nPhotosynthesis has a larger effect: photosynthetic efficiency turns up to 2% of incoming sunlight into biomass, for a total photosynthetic productivity of earth between ~1500–2250 TW (~1%+/-0.26% solar energy hitting the Earth's surface).\n\nOther minor sources of energy are usually ignored in these calculations, including accretion of interplanetary dust and solar wind, light from stars other than the Sun and the thermal radiation from space. Earlier, Joseph Fourier had claimed that deep space radiation was significant in a paper often cited as the first on the greenhouse effect.\n\nLongwave radiation is usually defined as outgoing infrared energy leaving the planet. However, the atmosphere absorbs parts initially, or cloud cover can reflect radiation. Generally, heat energy is transported between the planet's surface layers (land and ocean) to the atmosphere, transported via evapotranspiration and latent heat fluxes or conduction/convection processes. Ultimately, energy is radiated in the form of longwave infrared radiation back into space.\n\nRecent satellite observations indicate additional precipitation, which is sustained by increased energy leaving the surface through evaporation (the latent heat flux), offsetting increases in longwave flux to the surface.\n\nIf the incoming energy flux is not equal to the outgoing energy flux, net heat is added to or lost by the planet (if the incoming flux is larger or smaller than the outgoing respectively).\n\nAn imbalance must show in something on Earth warming or cooling (depending on the direction of the imbalance), and the ocean being the larger thermal reservoir on Earth, is a prime candidate for measurements.\n\nEarth's energy imbalance measurements provided by Argo floats have detected an accumulation of ocean heat content (OHC). The estimated imbalance was measured during a deep solar minimum of 2005–2010 to be 0.58 ± 0.15 W/m². This level of detail cannot be inferred directly from measurements of surface energy fluxes, which have combined uncertainties of the order of ± 17 W/m².\n\nSeveral satellites indirectly measure the energy absorbed and radiated by Earth and by inference the energy imbalance. The NASA Earth Radiation Budget Experiment (ERBE) project involves three such satellites: the Earth Radiation Budget Satellite (ERBS), launched October 1984; NOAA-9, launched December 1984; and NOAA-10, launched September 1986.\n\nToday NASA's satellite instruments, provided by CERES, part of the NASA's Earth Observing System (EOS), are designed to measure both solar-reflected and Earth-emitted radiation.\n\nThe major atmospheric gases (oxygen and nitrogen) are transparent to incoming sunlight but are also transparent to outgoing thermal (infrared) radiation. However, water vapor, carbon dioxide, methane and other trace gases are opaque to many wavelengths of thermal radiation. The Earth's surface radiates the net equivalent of 17 percent of the incoming solar energy in the form of thermal infrared. However, the amount that directly escapes to space is only about 12 percent of incoming solar energy. The remaining fraction, 5 to 6 percent, is absorbed by the atmosphere by greenhouse gas molecules.\nWhen greenhouse gas molecules absorb thermal infrared energy, their temperature rises. Those gases then radiate an increased amount of thermal infrared energy in all directions. Heat radiated upward continues to encounter greenhouse gas molecules; those molecules also absorb the heat, and their temperature rises and the amount of heat they radiate increases. The atmosphere thins with altitude, and at roughly 5–6 kilometres, the concentration of greenhouse gases in the overlying atmosphere is so thin that heat can escape to space.\n\nBecause greenhouse gas molecules radiate infrared energy in all directions, some of it spreads downward and ultimately returns to the Earth's surface, where it is absorbed. The Earth's surface temperature is thus higher than it would be if it were heated only by direct solar heating. This supplemental heating is the natural greenhouse effect. It is as if the Earth is covered by a blanket that allows high frequency radiation (sunlight) to enter, but slows the rate at which the low frequency infrared radiant energy emitted by the Earth leaves.\n\nA change in the incident radiated portion of the energy budget is referred to as a radiative forcing.\n\nClimate sensitivity is the steady state change in the equilibrium temperature as a result of changes in the energy budget.\n\nClimate forcings are changes that cause temperatures to rise or fall, disrupting the energy balance. Natural climate forcings include changes in the Sun's brightness, Milankovitch cycles (small variations in the shape of Earth's orbit and its axis of rotation that occur over thousands of years) and volcanic eruptions that inject light-reflecting particles as high as the stratosphere. Man-made forcings include particle pollution (aerosols) that absorb and reflect incoming sunlight; deforestation, which changes how the surface reflects and absorbs sunlight; and the rising concentration of atmospheric carbon dioxide and other greenhouse gases, which decreases the rate at which heat is radiated to space.\n\nA forcing can trigger feedbacks that intensify (positive feedback) or weaken (negative feedback) the original forcing. For example, loss of ice at the poles, which makes them less reflective, causes greater absorption of energy and so increases the rate at which the ice melts, is an example of a positive feedback.\n\nThe observed planetary energy imbalance during the recent solar minimum shows that solar forcing of climate, although natural and significant, is overwhelmed by anthropogenic climate forcing.\n\nIn 2012, NASA scientists reported that to stop global warming atmospheric CO content would have to be reduced to 350 ppm or less, assuming all other climate forcings were fixed. The impact of anthropogenic aerosols has not been quantified, but individual aerosol types are thought to have substantial heating and cooling effects.\n\n\n"}
{"id": "53574774", "url": "https://en.wikipedia.org/wiki?curid=53574774", "title": "Ellevio", "text": "Ellevio\n\nEllevio is an electricity distribution company in Sweden. It dominates in 35 municipalities.\n\nIt appeared as a company when Fortum sold its distribution network in Sweden in 2015.\nIt is owned by Tredje AP-fonden (20%), Folksam (17.5%), Första AP-fonden (12.5%) and Canadian Borealis Infrastructure Management (50%).\n"}
{"id": "33157882", "url": "https://en.wikipedia.org/wiki?curid=33157882", "title": "Energy efficiency gap", "text": "Energy efficiency gap\n\n\"This article is about the energy efficiency gap.\"\n\nEnergy efficiency gap refers to the improvement potential of energy efficiency or the difference between the cost-minimizing level of energy efficiency and the level of energy efficiency actually realized. It has attracted considerable attention among energy policy analysts, because its existence suggests that society has forgone cost-effective investments in energy efficiency, even though they could significantly reduce energy consumption at low cost. This term was first \"coined\" by Eric Hirst and Marilyn Brown in a paper entitled \"Closing the Efficiency Gap: Barriers to the Efficient Use of Energy\" in 1990.\n\nEnergy efficiency refers to changes in equipment and behavior that result in increased energy services per unit of energy consumed, while behavioral changes that reduce energy use are often referred to as energy conservation. Energy intensity which measures energy consumption per Gross Domestic Product (GDP) is one indicator of energy efficiency.\nMany people have attempted to measure the energy efficiency gap, and their approaches differ based on the definitions of the optimal level of energy use. A popular theme is Hirst and Brown (1990)'s definition: energy efficiency gap is the unexploited economic potential for energy efficiency, in other words, it emphasizes the technically feasible energy efficiency measures that are cost-effective but are not being deployed. Many other studies have used this definition, such as International Energy Agency (2007) and Koopmans and Velde.\nJaffe and Stavins (1994) identify five types of optimality and the corresponding definitions of the energy-efficiency gap: the economists' economic potential, the technologists’ economic potential, hypothetical potential, the narrow social optimum and the true social optimum. In particular, economists' economic potential could be achieved by eliminating market failures in the energy efficiency technology market, while technologists' economic potential could be achieved by eliminating both market and non-market failures. Achieving the hypothetical potential would require the elimination of market failures in the whole energy market, for instance, having energy prices that reflect all externalities. The society can achieve the narrow social optimum by implementing all available cost-effective programs, and the true social optimum can be achieved if the environmental effects of energy generation and consumption is taken into consideration.\n\nEnergy efficiency gaps exist because market failures exist. It is important to identify and understand those barriers in order to achieve desirable government policy interventions. According to Hirst and Brown (1990), various barriers that prevent the society from successfully closing energy efficiency gap can be divided into two categories: structural barriers and behavioral barriers.\nStructural barriers result from the actions of public and private organizations, and are usually beyond the control of the individual energy end user. Some examples are presented as following:\n\nDistortion in fuel prices. The fuel prices that consumers pay do not reflect the social and environmental costs associated with fuel production, distribution and consumption. Consumers tend not to invest on energy efficiency technologies due to this distortion.\n\nUncertainty about future fuel prices. There have been great uncertainties with the prices for fuels, such as electricity and petroleum. More stringent environmental regulations and global warming concerns also increase the volatility of fuel prices. These uncertainties prevent consumers from making rational purchase decisions of new energy-using systems.\n\nLimited access to capital. Consumers often face high up-front costs for energy-efficient systems. In addition, high discount rates are used to make tradeoffs between the initial capital investment and reduced operating costs, which also hinder the investments in energy-efficiency technologies.\n\nGovernment fiscal and regulatory policies. Government policies tend to encourage energy consumption, rather than energy efficiency. For instance, government support has focused more on energy production, and the profit of electric utilities is a function of sales.\n\nCodes and standards. The development of codes and standards often lag behind the development of technologies. It also takes a long time to adopt and modify standards, which becomes a barrier for energy efficiency technological innovation.\n\nSupply infrastructure limitations. The deployment of energy efficiency technologies is highly restricted by factors such as geography, infrastructure and human resources.\n\nBehavioral barriers are problems that characterize the end-user’s decision-marking relating to energy consumption. Four examples are discussed below.\n\nAttitudes toward energy efficiency. Public’s awareness of and attitudes toward energy efficiency could greatly affect their energy-related purchase and consumption behaviors.\n\nPerceived risk of energy-efficiency investments. Consumers and businesses can be very risk-averse in terms of investing in energy efficiency technologies. The uncertainties of fuel prices and high discount rate for operating costs have both made energy-efficiency investments even more \"risky” for many decision makers.\n\nInformation gaps. There is often a lack of information on the performance of energy-efficient technologies. Consumers tend not to change their energy consumption behavior if little information is provided.\n\nMisplaced incentives. The principal-agent problem and a lack of life-cycle thinking on costs and savings have imposed barriers for energy conservation.\n\nJaffe and Stavins (1994) categorize the barriers differently. They think that both market failures and non-market failures could account for the limited market success of the cost-effective energy-efficiency technologies. One important source of market failure is imperfect information, for instance the public good attributes of information and information asymmetry. Non-market failure may include the heterogeneity and inertia of consumers, and uncertainty about future energy prices and actual savings from energy efficiency investments.\n\nEnergy efficiency gap exists in various sectors, ranging from households, small businesses, corporations, and governments. Many policies and programs have been developed to overcome those barriers and close the energy efficiency gap.\n\nSubsidies and incentives for energy-efficient technologies. Insufficient capital investment could be overcome by more aggressive tax subsidies, loan guarantees, and low-interest government loans for energy efficient technologies.\n\nMinimum building and equipment efficiency standards. Minimum building and equipment efficiency standards are cost-effective approaches to save energy. Effective implementation and upgrading of building energy efficiency standards could improve the energy integrity of new buildings, while equipment efficiency standards could help reduce energy consumption and pollution during the life-cycle of equipment.\n\nInformation programs. Research has proved that providing accurate and trustworthy information on energy use and energy efficiency choices could help narrow the gap. Three forms of information programs can be implemented to help producers and consumers make more informed and rational decisions. The first one is generic information applicable to all energy decision, such as forecasts of future energy pricies; the second type of program is to provide comparative information to facilitate technology and product choices, such as product rating and labeling systems; the third type of program is to offer specific recommendations for producers’ and consumers’ investment choices or behavior changes.\n\nGovernment procurement programs for energy-efficient technologies. Government agencies could be required to procure energy-efficient products. This would help improve the energy efficiency of government sector, and the “learning by doing” impact would create early markets for energy-efficient technologies.\nSome real-world examples of those measures include the following: EU's energy consumption labeling scheme, U.S. DOE's building energy codes program, and EPA's and DOE's ENERGY STAR® voluntary labeling programs.\n\n"}
{"id": "11493", "url": "https://en.wikipedia.org/wiki?curid=11493", "title": "Fallout shelter", "text": "Fallout shelter\n\nA fallout shelter is an enclosed space specially designed to protect occupants from radioactive debris or fallout resulting from a nuclear explosion. Many such shelters were constructed as civil defense measures during the Cold War.\n\nDuring a nuclear explosion, matter vaporized in the resulting fireball is exposed to neutrons from the explosion, absorbs them, and becomes radioactive. When this material condenses in the rain, it forms dust and light sandy materials that resemble ground pumice. The fallout emits alpha and beta particles, as well as gamma rays.\n\nMuch of this highly radioactive material falls to earth, subjecting anything within the line of sight to radiation, becoming a significant hazard. A fallout shelter is designed to allow its occupants to minimize exposure to harmful fallout until radioactivity has decayed to a safer level.\n\nDuring the Cold War, many countries built fallout shelters for high-ranking government officials and crucial military facilities, such as Project Greek Island and the Cheyenne Mountain nuclear bunker in the United States and Canada's Emergency Government Headquarters. Plans were made, however, to use existing buildings with sturdy below-ground-level basements as makeshift fallout shelters. These buildings were placarded with the orange-yellow and black trefoil sign designed by United States Army Corps of Engineers director of administrative logistics support function Robert W. Blakeley in 1961.\n\nThe National Emergency Alarm Repeater (NEAR) program was developed in the United States in 1956 during the Cold War to supplement the existing siren warning systems and radio broadcasts in the event of a nuclear attack. The NEAR civilian alarm device was engineered and tested but the program was not viable and was terminated in 1967. In the U.S. in September 1961, under the direction of Steuart L. Pittman, the federal government started the Community Fallout Shelter Program. A letter from President Kennedy advising the use of fallout shelters appeared in the September 1961 issue of \"Life\" magazine.\n\nIn November 1961, in \"Fortune\" magazine, an article by Gilbert Burck appeared that outlined the plans of Nelson Rockefeller, Edward Teller, Herman Kahn, and Chet Holifield for an enormous network of concrete lined underground fallout shelters throughout the United States sufficient to shelter millions of people to serve as a refuge in case of nuclear war.\n\nThe United States ended federal funding for the shelters in the 1970s. In 2017, New York City began removing the iconic yellow signs since members of the public are unlikely to find viable food and medicine inside those rooms.\n\nSimilar projects have been undertaken in Finland, which requires all buildings with area over 600 m² to have an NBC shelter, and Norway, which requires all buildings with an area over 1000 m² to have a shelter.\n\nThe former Soviet Union and other Eastern Bloc countries often designed their underground mass-transit and subway tunnels to serve as bomb and fallout shelters in the event of an attack.\n\nGermany has protected shelters for 3% of its population, Austria for 30%, Finland for 70%, Sweden for 81%, and Switzerland for 114%.\n\nSwitzerland built an extensive network of fallout shelters, not only through extra hardening of government buildings such as schools, but also through a building regulation requiring nuclear shelters in residential buildings since the 1960s (the first legal basis in this sense dates from 4 October 1963). Later, the law ensured that all residential buildings built after 1978 contained a nuclear shelter able to withstand a blast from a 12 megaton explosion at a distance of 700 metres. The \"Federal Law on the Protection of the Population and Civil Protection\" still requires that every inhabitant should have a place in a shelter close to where they live.\n\nThe Swiss authorities maintained large communal shelters (such as the Sonnenberg Tunnel until 2006) stocked with over four months of food and fuel. The reference \"Nuclear War Survival Skills\" declared that, as of 1986, \"Switzerland has the best civil defense system, one that already includes blast shelters for over 85% of all its citizens.\" As of 2006, there were about 300,000 shelters built in private homes, institutions and hospitals, as well as 5,100 public shelters for a total of 8.6 million places, a level of coverage equal to 114% of the population.\n\nIn Switzerland, most residential shelters are no longer stocked with the food and water required for prolonged habitation and a large number have been converted by the owners to other uses (e.g., wine cellars, ski rooms, gyms). But the owner still has the obligation to ensure the maintenance of the shelter.\n\nA basic fallout shelter consists of shields that reduce gamma ray exposure by a factor of 1000. The required shielding can be accomplished with 10 times the thickness of any quantity of material capable of cutting gamma ray exposure in half. Shields that reduce gamma ray intensity by 50% (1/2) include 1 cm (0.4 inch) of lead, 6 cm (2.4 inches) of concrete, 9 cm (3.6 inches) of packed earth or 150 m (500 ft) of air. When multiple thicknesses are built, the shielding multiplies. Thus, a practical fallout shield is ten halving-thicknesses of packed earth, reducing gamma rays by approximately 1024 times (2).\n\nUsually, an expedient purpose-built fallout shelter is a trench; with a strong roof buried by 1 m (3 ft) of earth. The two ends of the trench have ramps or entrances at right angles to the trench, so that gamma rays cannot enter (they can travel only in straight lines). To make the overburden waterproof (in case of rain), a plastic sheet may be buried a few inches below the surface and held down with rocks or bricks.\n\nBlast doors are designed to absorb the shock wave of a nuclear blast, bending and then returning to their original shape.\n\nDry earth is a reasonably good thermal insulator, and over several weeks of habitation, a shelter will become dangerously hot. The simplest form of effective fan to cool a shelter is a wide, heavy frame with flaps that swing in the shelter's doorway and can be swung from hinges on the ceiling. The flaps open in one direction and close in the other, pumping air. (This is a Kearny air pump, or KAP, named after the inventor, Cresson Kearny)\n\nUnfiltered air is safe, since the most dangerous fallout has the consistency of sand or finely ground pumice. Such large particles are not easily ingested into the soft tissues of the body, so extensive filters are not required. Any exposure to fine dust is far less hazardous than exposure to the fallout outside the shelter. Dust fine enough to pass the entrance will probably pass through the shelter. Some shelters, however, incorporate NBC-filters for additional protection.\n\nEffective public shelters can be the middle floors of some tall buildings or parking structures, or below ground level in most buildings with more than 10 floors. The thickness of the upper floors must form an effective shield, and the windows of the sheltered area must not view fallout-covered ground that is closer than 1.5 km (1 mi). One of Switzerland's solutions is to use road tunnels passing through the mountains, with some of these shelters being able to protect tens of thousands.\n\nFallout shelters are not always underground. Above ground buildings with walls and roofs dense enough to afford a meaningful protection factor can be used as a fallout shelter.\n\nA battery-powered radio may be helpful to get reports of fallout patterns and clearance. However, radio and other electronic equipment may be disabled by electromagnetic pulse. For example, even at the height of the cold war, EMP protection had been completed for only 125 of the approximately 2,771 radio stations in the United States Emergency Broadcast System. Also, only 110 of 3,000 existing Emergency Operating Centers had been protected against EMP effects. The Emergency Broadcast System has since been supplanted in the United States by the Emergency Alert System.\n\nThe reference \"Nuclear War Survival Skills\" includes the following supplies in a list of \"Minimum Pre-Crisis Preparations\": one or more shovels, a pick, a bow-saw with an extra blade, a hammer, and 4-mil polyethylene film (also any necessary nails, wire, etc.); a homemade shelter-ventilating pump (a KAP); large containers for water; a plastic bottle of sodium hypochlorite bleach; one or two KFMs and the knowledge to operate them; at least a 2-week supply of compact, nonperishable food; an efficient portable stove; wooden matches in a waterproof container; essential containers and utensils for storing, transporting, and cooking food; a hose-vented 5-gallon can, with heavy plastic bags for liners, for use as a toilet; tampons; insect screen and fly bait; any special medications needed by family members; pure potassium iodide, a 2-oz bottle, and a medicine dropper; a first-aid kit and a tube of antibiotic ointment; long-burning candles (with small wicks) sufficient for at least 14 nights; an oil lamp; a flashlight and extra batteries; and a transistor radio with extra batteries and a metal box to protect it from electromagnetic pulse.\n\nInhabitants should have water on hand, 1-2 gallons per person per day. Water stored in bulk containers requires less space than water stored in smaller bottles.\n\nCommercially made Geiger counters are expensive and require frequent calibration. It is possible to construct an electrometer-type radiation meter called the Kearny fallout meter, which does not require batteries or professional calibration, from properly-scaled plans with just a coffee can or pail, gypsum board, monofilament fishing line, and aluminum foil. Plans are freely available in the public domain in the reference \"Nuclear War Survival Skills\" by Cresson Kearny.\n\nInhabitants should plan to remain sheltered for at least two weeks (with an hour out at the end of the first week – see Swiss Civil Defense guidelines), then work outside for gradually increasing amounts of time, to four hours a day at three weeks. The normal work is to sweep or wash fallout into shallow trenches to decontaminate the area. They should sleep in a shelter for several months. Evacuation at three weeks is recommended by official authorities.\n\nIf available, inhabitants may take potassium iodide at the rate of 130 mg/day per adult (65 mg/day per child) as an additional measure to protect the thyroid gland from the uptake of dangerous radioactive iodine, a component of most fallout and reactor waste.\n\nIn the vast majority of accidents, and in all atomic bomb blasts, the threat due to beta and gamma emitters is greater than that posed by the alpha emitters in the fallout. Alpha particles are identical to a helium-4 nucleus (two protons and two neutrons), and travel at speeds in excess of 5% of the speed of light. Alpha particles have little penetrating power; most cannot penetrate through human skin. Avoiding direct exposure with fallout particles will prevent injury from alpha radiation.\n\nBeta radiation consists of particles (high-speed electrons) given off by some fallout. Most beta particles cannot penetrate more than about 10 feet (3 metres) of air or about inch (3 millimetres) of water, wood, or human body tissue; or a sheet of aluminum foil. Avoiding direct exposure with fallout particles will prevent most injuries from beta radiation.\n\nThe primary dangers associated with beta radiation are internal exposure from ingested fallout particles and beta burns from fallout particles no more than a few days old. Beta burns can result from contact with highly radioactive particles on bare skin; ordinary clothing separating fresh fallout particles from the skin can provide significant shielding.\n\nGamma radiation penetrates further through matter than alpha or beta radiation. Most of the design of a typical fallout shelter is intended to protect against gamma rays. Gamma rays are better absorbed by materials with high atomic numbers and high density, although neither effect is important compared to the total mass per area in the path of the gamma ray. Thus, lead is only modestly better as a gamma shield than an equal mass of another shielding material such as aluminum, concrete, water or soil.\n\nSome gamma radiation from fallout will penetrate into even the best shelters. However, the radiation dose received while inside a shelter can be significantly reduced with proper shielding. Ten halving thicknesses of a given material can reduce gamma exposure to less than of unshielded exposure.\n\nThe bulk of the radioactivity in nuclear accident fallout is more long-lived than that in weapons fallout. A good table of the nuclides, such as that provided by the Korean Atomic Energy Research Institute, includes the fission yields of the different nuclides. From this data it is possible to calculate the isotopic mixture in the fallout (due to fission products in bomb fallout).\n\nWhile a person's home may not be a purpose-made shelter, it could be thought of as one if measures are taken to improve the degree of fallout protection.\n\nThe main threat of beta radiation exposure comes from \"hot particles\" in contact with or close to the skin of a person. Also, swallowed or inhaled hot particles could cause beta burns. As it is important to avoid bringing hot particles into the shelter, one option is to remove one's outer clothing, or follow other decontamination procedures, on entry. Fallout particles will cease to be radioactive enough to cause beta burns within a few days following a nuclear explosion. The danger of gamma radiation will persist for far longer than the threat of beta burns in areas with heavy fallout exposure.\n\nThe gamma dose rate due to the contamination brought into the shelter on the clothing of a person is likely to be small (by wartime standards) compared to gamma radiation that penetrates through the walls of the shelter. The following measures can be taken to reduce the amount of gamma radiation entering the shelter:\n\nFallout shelters feature prominently in the Robert A. Heinlein novel \"Farnham's Freehold\" (Heinlein built a fairly extensive shelter near his home in Colorado Springs in 1963), \"Pulling Through\" by Dean Ing, \"A Canticle for Leibowitz\" by Walter M. Miller and \"Earth\" by David Brin.\n\nThe 1961 \"Twilight Zone\" episode \"The Shelter\", from a Rod Serling script, deals with the consequences of actually using a shelter. Another episode of the series called \"One More Pallbearer\" featured a fallout shelter owned by millionaire. The 1985 adaption of the series had the episode \"Shelter Skelter\" that featured a fallout shelter.\n\nIn the \"Only Fools and Horses\" episode \"The Russians are Coming\", Derek Trotter buys a lead fallout shelter, then decides to construct it in fear of an impending nuclear war caused by the Soviet Union (who were still active during the episode's creation).\n\nIn 1999 the film \"Blast from the Past\" was released. It is a romantic comedy film about a nuclear physicist, his wife, and son that enter a well-equipped, spacious fallout shelter during the 1962 Cuban Missile Crisis. They do not emerge until 35 years later, in 1997. The film shows their reaction to contemporary society.\n\nThe \"Fallout\" series of computer games depicts the remains of human civilization after an immensely destructive global nuclear war; the United States of America had built underground vaults that were advertised to protect the population against a nuclear attack, but almost all of them were in fact meant to lure subjects for long-term human experimentation.\n\n\"Paranoia\", a role-playing game, takes place in a form of fallout shelter, which has become ruled by an insane computer.\n\nThe \"Metro 2033\" book series by Russian author Dmitry Glukhovsky depicts survivors' life in the subway systems below Moscow and Saint-Petersburg after a nuclear exchange between the Russian Federation and the United States of America.\n\nFallout shelters are often featured on the reality television show \"Doomsday Preppers\".\n\nThe Silo series of novellas by Hugh Howey feature extensive fallout-style shelters that protect the inhabitants from an initially unknown disaster.\n\n\nNation specific:\n\nGeneral:\n\n\nPublications:\n\n"}
{"id": "26351099", "url": "https://en.wikipedia.org/wiki?curid=26351099", "title": "Garuda Indonesia Flight 708", "text": "Garuda Indonesia Flight 708\n\nGaruda Indonesia Airways Flight 708 was a scheduled passenger flight on 16 February 1967 which crashed upon landing at Sam Ratulangi Airport, Manado, North Sulawesi, Indonesia. 22 of the 84 passengers on board were killed. All eight crew members survived.\n\nFlight 708 departed Jakarta for a flight to Manado via Surabaya and Makassar. On the second leg of the flight bad weather at Makassar forced the crew to return to Surabaya. The flight continued the next day to Makassar and on to Manado. The weather at Manado was cloud base at 900 feet and 2 km visibility. An approach to runway 18 was made, but after passing a hill 200 feet above runway elevation and 2720 feet short of the threshold, the pilot realized he was too high and left of the centerline. The nose was lowered and the aircraft banked right to intercept the glide path. The speed decreased below the 125 knots target threshold speed and the aircraft, still banked to the right, landed heavily 156 feet short of the runway threshold. The undercarriage collapsed and the aircraft skidded and caught fire.\n\nThe probable cause of the accident was determined to be an awkward landing technique resulting in an excessive rate of sink on touchdown. Among the contributing factors were the uneven pavement of the runway and marginal weather at time of the landing.\n"}
{"id": "47162717", "url": "https://en.wikipedia.org/wiki?curid=47162717", "title": "HTR-PM", "text": "HTR-PM\n\nThe HTR-PM is a small modular nuclear reactor under development in China.\nIt is a high-temperature gas-cooled (HTGR) pebble-bed generation IV reactor partly based on the earlier HTR-10 prototype reactor.\nThe reactor unit has a thermal capacity of 250 MW, and two reactors are connected to a single steam turbine to generate 210 MW of electricity.\n\nWork on the first demonstration HTR-PM power plant, composed of two reactors driving a single steam turbine, began in December 2012 in Shidao Bay Nuclear Power Plant. \nThe pressure vessels of the two reactors were installed in 2016, and the plant is expected to start generation in 2018. \nThe HTR-PM is expected to be the first generation IV reactor to enter operation.\n\n"}
{"id": "10872064", "url": "https://en.wikipedia.org/wiki?curid=10872064", "title": "Hardmask", "text": "Hardmask\n\nA hardmask is a material used in semiconductor processing as an etch mask instead of a polymer or other organic \"soft\" resist material. The idea is that polymers tend to be etched easily by oxygen, fluorine, chlorine or other reactive gases to the extent that a pattern defined using a polymeric mask is rapidly degraded during plasma etching. \n"}
{"id": "7609098", "url": "https://en.wikipedia.org/wiki?curid=7609098", "title": "Hundred-Year Winter", "text": "Hundred-Year Winter\n\nThe Hundred-Year Winter is a time period in the fictional Narnia universe created by C.S. Lewis. It takes place from 900–1000 Narnia time. The White Witch Jadis cast a spell to make it Winter all year round, but never reaches Christmas. But throughout the story, Aslan is entering Narnia and his presence weakens The White Witch, Jadis, causing Spring and Father Christmas to slowly appear. Aslan also brings Peter, Lucy, Susan, and Edmund to Narnia to fulfill the Prophecy of The Four Thrones (\"When two daughters of Eve and two sons of Adam sit together in throne at the Cair Paravel, the reign of the White Witch will be over and done.) This would put an end to White Witch's plan and her reign and the endless winter would come to an end. (The final days of the Hundred Year Winter occur during \"The Lion, the Witch, and the Wardrobe\".)\n\nThe Hundred-Year Winter bears strong parallels with the Norse Fimbulwinter, and may have also been an inspiration for the Long Night, a generation-long winter that occurred at some point in the history of Westeros in George R. R. Martin's \"A Song of Ice and Fire\".\n"}
{"id": "38957192", "url": "https://en.wikipedia.org/wiki?curid=38957192", "title": "Isotopic ratio outlier analysis", "text": "Isotopic ratio outlier analysis\n\nIsotopic ratio outlier analysis (IROA) iIsotopic Ratio Outlier Analysis (IROA) is a simple and direct LC-MS metabolic profiling technique that uses \"95% and 5% 13C\" to label biochemical metabolites resulting in unique signatures (see figure below). LC-MS and software algorithms are then used find, identify and quantitate these specific signatures.  IROA is used to detect metabolites, even at low concentrations in any sample and can be applied to many life science applications (toxicology, diagnostics, drug development, bioprocess, etc.).\n\nTraditionally, stable isotope labeling analyses use molecules labeled at 99% 13C. IROA utilizes both 5% C and 95%C and compounds are randomly and universally labeled to achieve this effect. The relative abundances of the isotopes of carbon are altered; i.e. enriched in one particular isotope and depleted in its other isotopic form. For example, 95% U-13C6-Glucose is not actually 95% 13C6 and 5% 12C6. In this case, each carbon atom position (1,2,3,4,5,6) has a 95% chance of being 13C labeled and a 5% chance of being 12C. \n\nCost effective simultaneous metabolite identification, quantitation and platform QC. \n\nUnique IROA patterns discriminate peaks of biological origin from artefactual peaks allowing the removal of natural abundance non-biological artefacts and noise (false data).\n\nFacilitates detections of low intensity features - at natural abundance more than one isotopic peak is often not detectable and excluded as noise.\n\nRemoves variances - as a composite sample, sample-to-sample and analytical variance is removed and during MS analysis the identical compounds (unlabeled or labeled with either U-95% 13C or U-5% 13C) experience the same ionization efficiency and suppression. \n\nTriply redundant identification – the number of carbons for each metabolite is determined by both the height of the M+1 and M-1 peaks and the distance between the monoisotopic peaks; carbon number and mass together greatly restrict the number of possible molecular formulae. \n\nAccurate compound formula ID for MS alone; complete ID with addition of SWATH, or IM, even at low concentrations.\n\nMathematically calculable - enables computational analysis; ClusterFinder software solution builds libraries, IDs/quantitates compounds and normalizes data. \n\nAll IROA-based fragments have the IROA ratio pattern of their parent peaks and can be identified as such using the “peak correlation” ClusterFinder module.\n\nBecause all molecules (biochemicals, proteins, RNA and DNA) are labeled by the IROA protocol it can be used in all 'omic sciences, however biochemical profiling or metabolomics is an especially useful case. The natural abundance of carbon is approximately 98.9% and 1.1% . Because of this, during mass spectrometric analysis carbon-based molecules have both a monoisotopic peak and a second peak, the \"M+1\" peak, that is caused by the presence of the isotopes of not only , but also , , and others. The key to IROA is that analysis is done using a specific mixture of and . One isotope is present at approximately 95% and the other at 5%. This the concentrated isotope's corresponding peak dominates the dilute isotope.\n\nA homogeneous cell population is divided into equal-sized \"experimental\" and \"control\" groups. The biological compounds in these groups are labeled using an isotopically defined growth media in which all of the carbon components in the experimental and control samples are replaced with randomly and universally enriched 5% or 95%, respectively. After at least 5 subsequent cell divisions, the experiment group is exposed to a stressor (chemical, genetic, environmental, etc.) When the experiment has concluded, the experimental sample is mixed with the control sample and analyzed using mass spectrometry. The admixing of samples increases data quality as sample-to-sample variance is reduced and the identification of all biological compounds is enhanced.\n\nClusterFinder software fully automates the processing of IROA-based data files to identify and quantify all compounds of biological origin and to remove artifacts.\n\nBasic IROA is fundamentally an unbiased analysis. This leads to clean, high resolution data sets that clearly define the biological response of a biological system.\nThis workflow utilizes both 95%- and 5%-C13 signatures. Like Stable Isotope Labeling by/with Amino acids in Cell culture (SILAC), cells are differentially labeled by growing them in heavy medium (medium in which all carbon sources are labeled with 95% C13) or light medium (medium in which all carbon sources are labeled with 5% C13). When the cells are growing in this medium, they incorporate the heavy or light carbon into all of their metabolites. The Control and Experimental sample media are chemically identical but isotopically different, and importantly, the isotopes are universally and randomly incorporated into all carbon positions. \nThe Control (heavy medium) and the Experimental (light medium) cell samples are grown in the IROA isotopically-defined media for a sufficient time to replace their original natural abundance carbon so that all of their contents will demonstrate distinctive isotopic patterns. Once labeled, the Experimental sample is treated with a stressing regimen, and the Control sample is treated with only vehicle (e.g. if drug & DMSO vehicle is added to the Experimental sample, DMSO should be added to the Control sample for the same time period). The stressing agent may be chemical (e.g. toxin), genetic (e.g. mutant), environmental (e.g. UV exposure), or any element or combination of elements that induce physiological alteration. Experimental samples are individually pooled with Control samples, and the resulting composite samples analyzed using LC-MS. This has the effect of reducing sample-to-sample analytical variance, and increasing data quality. In the Basic IROA protocol, Control sample metabolic pools are fully labeled with U-95%13C, and Experimental sample metabolic pools labeled with U-5%13C. Control and Experimental metabolite peaks are easily identified according to the presence of characteristic M-1 and M-1 IROA peak patterns, respectively. When pooled cells are processed, the signals for the compounds from both the Control and Experimental cells may be distinguished and differences between the ratio of their areas are directly indicative of the ratio of the respective sizes of their metabolic pools. Outliers to the normalized ratios are metabolic pools that are impacted by the experimental treatment.\n\nThe IROA Phenotypic workflow utilizes only 95%- C13 signatures. There are many cases where it is not practical or possible to label the Experimental sample, for example, when working with tissue samples, performing large fermentation runs, or propagating field-grown plants. In these instances, the IROA “Phenotypic” experiment may be applied. In the Phenotypic Protocol the Control or complex \"Internal Standard\" is isotopically labeled with IROA 95% U-13C media (media in which all the carbon sources are randomly and universally composed of approximately 95%12C with 5%13C) and used to spike into Experimental samples.\nThe unique IROA labeling pattern again ensures that the monoisotopic peaks and the carbon envelope of the associated isotopic peaks (M-1 etc.) can be detected during LC-MS. The carbon envelope differentiates the IROA-IS from natural abundance peaks (see figure below) and is used to identify compounds of interest and exclude artifacts the may look otherwise similar. \nThe IROA Phenotypic Experiment. \na) The material to be phenotyped i.e. biopsy tissue, plant material, cells, etc. is mixed with the labeled 13C IROA- Internal Standard. An ideal Standard is one that represents the entire metabolome of the fluid or tissue under study. For example, if interrogating the metabolome in pancreatic tissue, quantification of metabolites can be achieved using an IROA-grown/labeled pancreatic cell line as a complex Internal Standard. The IROA ClusterFinder software will find all IROA-labeled compounds and use these peaks to locate and identify the associated paired natural abundance peaks in the Experimental samples. This allows for a complex targeted analysis.\nb) Using an IROA-labeled Standard, all of peaks may be easily identified according to presence of their characteristic M-1 peak. The natural abundance peaks corresponding to each Standard peak may be readily identified because even though they do not carry any isotopic labeling, their exact mass and position are established relative to the Standard.\nc) If the pooled Standard is well characterized and the compounds that are present in it have already been identified, then it will be able to be used as a point of comparison for all of its contained compounds. Artifacts will have no match in the Standard and are eliminated by ClusterFinder in a final dataset. In a typical experimental IROA dataset the ratio of the peak areas represent the relative deviation of the metabolic pool sizes brought about by the experimental condition. In a Phenotypic experiment, all experimental samples are measured relative to the Standard; therefore, the phenotype is defined by the deviation from the Standard.\nd) The Phenotypic IROA signal is made up of two halves. The C13 side tells the ClusterFinder software where to find the corresponding 12C peak. The C12 side is unlabeled and has only natural abundance carbon. The pairing makes the identity of all peaks measured unambiguous.\n\nThe basic IROA protocol relies on the creation of isotopic patterns by growing cells on media wherein all the carbon sources contain defined isotopic balances. With reasonable resolution, the mass and the number of carbons will unambiguously indicate a formula for monoisotopic peak. Since this is true for both the and peak sets, then either peak carries all of the information needed to find the other (and the formula), the pair of peaks provides a triply redundant mechanism to identify all compounds of biological origin. Since non-biologically derived compounds will never have IROA patterns (like H-NA), all artifactual peaks may be identified and removed from consideration. These characteristics greatly simplify and strengthen the quality for the interpretation of a mass spectrum of a biological sample.\n\nIROA software algorithms were created based on the fact that IROA peaks are mathematically calculable. Each set of carbon isotopomers ( and ) reliably and accurately account for the other set, providing a redundant quality control check. The ClusterFinder software achieves a data reduction of complex raw data to concise, high value information by performing these steps: characterization of peaks according to source (artifactual-no label, experimental, or control, removal of artifacts, alignment and pairing of remaining peaks across scans, pair normalization and identification, and determination of the relative / ratios of analytes in each sample.\n\nThe IROA approach is applicable to measuring an organism's biological response to any stressor, such as disease, environment, drugs or toxins. As an example IROA was used to label biosynthetic pathways, specifically outlining the glutathione pathway, during the fermentation process of \"Saccharomyces cerevisiae (S. cerevisiae).\" \"S. cerevisiae\" was profiled and measured using liquid chromatography/mass spectrometry.\n"}
{"id": "649485", "url": "https://en.wikipedia.org/wiki?curid=649485", "title": "JT-60", "text": "JT-60\n\nJT-60 (short for Japan Torus-60) is large research tokamak, the flagship of Japan's magnetic fusion program, previously run by the Japan Atomic Energy Research Institute (JAERI) and currently run by the Japan Atomic Energy Agency's (JAEA) Naka Fusion Institute in Ibaraki Prefecture. It is properly an advanced tokamak, including a D-shaped plasma cross-section and active feedback control.\n\nFirst designed in the 1970s as the \"Breakeven Plasma Test Facility\" (BPTF), the goal of the system was to reach breakeven, a goal also set for the US's TFTR, the UK's JET and the Soviet T-15. JT-60 began operations in 1985, and like the TFTR and JET that began operations only shortly before it, JT-60 demonstrated performance far below predictions.\n\nOver the next two decades, JET and JT-60 led the effort to regain the performance originally expected of these machines. JT-60 underwent two major modifications during this time, producing JT-60A, and then JT-60U (for \"upgrade\"). These changes resulted in significant improvements in plasma performance. , JT-60 currently holds the record for the highest value of the fusion triple product achieved: = .\n\nDuring deuterium (D–D fuel) plasma experiments in 1998, plasma conditions were achieved which would have achieved break-even—the point where the power produced by the fusion reactions equals the power supplied to operate the machine—if the D–D fuel were replaced with a 1:1 mix of deuterium and tritium (D–T fuel). \nJT-60 does not have the facilities to handle tritium; only the JET tokamak in the United Kingdom has such facilities as of 2018. \nIn fusion terminology, JT-60 achieved conditions which in D–T would have provided a fusion energy gain factor (the ratio of fusion power to input power) \"Q\" = 1.25.\nA self-sustaining nuclear fusion reaction would need a value of \"Q\" that is greater than 5.\n\nIn 2005, ferritic steel (ferromagnet) tiles were installed in the vacuum vessel to correct the magnetic field structure and hence reduce the loss of fast ions.\nOn May 9, 2006, the JAEA announced that the JT-60 had achieved a 28.6 second plasma duration time. \nThe JAEA used new parts in the JT-60, having improved its capability to hold the plasma in its powerful toroidal magnetic field. \nThe main future objective of JT-60 is to realize high-beta steady-state operation in the use of reduced radio-activation ferritic steel in a collision-less regime.\n\nIt was planned for JT-60 to be disassembled and then upgraded to JT-60SA by adding niobium-titanium superconducting coils by 2010. \nIt is intended to be able to run with the same shape plasma as ITER. The central solenoid will use niobium-tin (because of the higher (9 T) field). \n\nConstruction of the tokamak officially began in 2013, and will continue until 2020 with first plasma planned in September 2020.\n\n"}
{"id": "2213288", "url": "https://en.wikipedia.org/wiki?curid=2213288", "title": "Jim Bohlen", "text": "Jim Bohlen\n\nJim Bohlen (July 4, 1926 – July 5, 2010) was an American engineer who worked on the Atlas ICBM missile program and later emigrated to Canada after becoming disillusioned with the US government's nuclear policy during the Cold War. He became one of the founders of Greenpeace.\n\nBohlen, one of the approximately half-dozen founders of Greenpeace, arguably made the most lasting impression with his brilliant backroom dealings. He effectively out-manoeuvered more high-profile members of the group such as Paul Watson within the organization's inner workings and can be primarily credited with transforming the organization from a small, vociferous Canadian pressure group into the highly efficient international movement it is today. Bohlen was a strong supporter of the creation of Greenpeace International and the move of the group's headquarters from Vancouver to New York City, essentially giving birth to Greenpeace as we know it today. He was also instrumental in developing the group's hierarchical military-style command structure.\n\nHe made a similar and even less-acknowledged contribution to Green politics in Canada by negotiating a deal that ended the party's 10-year prohibition against Green Parties having leaders at a meeting of the Green Party of BC in 1992. (Between 1982 and 1992, some Green Parties were required by law to have registered leaders but party constitutions prohibited these individuals from speaking for these parties.) Within four years of Bohlen's breakthrough agreement, all Canadian Green parties had moved to adopt conventional political structures in which their leaders functioned as primary spokespeople.\n\nBohlen ran for the Greens federally in 1988. Some credit friction between him and long-time enemy Paul Watson for Watson's defection to the NDP during that campaign.\n\n\n"}
{"id": "29810194", "url": "https://en.wikipedia.org/wiki?curid=29810194", "title": "John Sherman Cooper Power Station", "text": "John Sherman Cooper Power Station\n\nThe John Sherman Cooper Power Station is a coal-fired power plant owned and operated by the East Kentucky Cooperative near Somerset, Kentucky. It is actually closest to the smaller city of Burnside. It is named after John Sherman Cooper, a US Senator from Kentucky.\n\n\n\n"}
{"id": "34301700", "url": "https://en.wikipedia.org/wiki?curid=34301700", "title": "Kamrun Nahar", "text": "Kamrun Nahar\n\nKamrun Nahar is a Bangladeshi soil scientist and environmentalist. A prominent biofuels researcher of Bangladesh, her research and publications also aimed to lower dependence on petroleum based foreign oil by producing low carbon and sulphur emitting biofuels from the second generation energy crops cultivated in the unused wastelands of Bangladesh for use in home generators to supplement power.\n\nAn elected member of the Asiatic Society of Bangladesh, she was also the former secretary of the Institute of Environmental Professionals – Bangladesh in 2003, and is an Associate professor and Faculty of Environmental Science and Management at North South University and held similar teaching tenures at BRAC University and at the Department of Population Environment at IUB since 2000.\n\nShe was born in 1961, to the Munshibari family of Comilla, where author Saleh Uddin was her older brother. She is the sister-in-law of Raihanul Abedin. Upon graduation from Eden College, she attended the University of Dhaka's Department of Soil, Water and Environment in 1977. In 1978 she studied Soil Chemistry under Dr. Iajuddin Ahmed. She graduated with a BS degree in Soil Science in 1981 and a MS degree in Soil Chemistry in 1982. She was married to Muhammad Shahid Sarwar in 1981. The same year she was awarded the First Class Honours Award by the Dhaka Education Board.\n\nIn 1997 she travelled to Europe to attend the Department of Applied Plant Sciences and Plant Biotechnology for a PhD at the University of Natural Resources and Life Sciences, Vienna in Austria (Institut f. Pflanzenzuchtung u. Pflanzenbau Uni. F. Bodenkultur Wien, Osterreich) as an . She has also been a visiting scholar at University of Florida and Washington State University in the United States.\n\nNahar first proposed to cultivate a non-food bioenergy crop, \"Jatropha curcas\" L. in Bangladesh as it did not need arable lands and does not compete with food. Her work focused on the increasing water deficit conditions due to global climate change and its relationship to fruit yield, particularly in combating world hunger. Her publications also center around bioenergy and food production in both the cyclical water deficit lands and the highly flooded plains of Bangladesh. Emphasizing on land use patterns and possible cultivation areas of Bangladesh, she stressed the uses and socioeconomic benefits of the plant citing minimal production costs and ease for the production of biodiesel and other useful byproducts, compared to conventional fossil fuel. Carbon sequestration was also implied in the national scheme.\n\nIn the early 1980s, Nahar began analysing the soils of different areas of the country. Twenty soil samples belonging to four pedons from Bhola District were analysed for their profile morphology, particle size distribution, and mineral composition in the clay fraction. Fine to medium-sized mottles with distinct contrast were present in almost all the horizons. Structural B (cambic) horizon has developed in all the pedons where clay content ranged from 17–42%. The texture of the soils ranged from silt loam to silty clay loam. Mica and kaolinite were the two other minerals whose abundance was nearly equal. Occurrence of small quantities of mica-vermiculite intergrades and some interstratified clay minerals was suspected. A small portion of smectites was considered to be formed authigenically in the soils from Bhola.\n\nLater, a total of twenty one soil samples belonging to five representative soil series were collected on horizon basis from the three distinct vegetative zones of Raojan Rubber garden, Chittagong and analysed for their different properties. The difference horizons of the profiles studied were truly pedogenetic. Sand was the dominant fraction of the soil which might indicate that the parent materials were arenaceous in nature. The texture of the soil ranged from loamy sand to sandy loam at the surface and sandy loam to sandy clay loam at the subsurface. The results of sand/silt ration indicate that the studied profiles did not form on uniform parent materials. Moisture percentage of air dry soils ranged from 0.3 to 2.6. A positive correlation existed between percent clay and hygroscopic moisture of the soils.\n\nSince then, Nahar's research focused on increasing water deficit conditions due to global climate change and its relationship to fruit yield, particularly in combating world hunger. Her publications also center around bioenergy and food production in both the cyclical water deficit lands and the highly flooded plains of Bangladesh. The influence of water stress on tomato plants and fruit quality was investigated in a pot experiment. The uptake of nitrogen, sodium, potassium, sulphur, calcium and magnesium was significantly reduced by water stress in the plants. Significant increases in glucose, fructose, sucrose in fruits and proline content in leaves showed some tendency of this crop to adjust osmotically to water stress. Water stress increased the sugar and acid contents (ascorbic, malic and citric acid) of the tomato fruits and thus improved the fruit quality. This study investigates the effects of water stress on moisture content distribution at different soil layers (pot) and on morphological characters of tomato plants. Moisture content distribution was higher at the surface and decreased with increasing stress at all growth stages. Yield and related morphological characters responded better at certain of the field capacity compared when with other treatments. A study was conducted in the experimental field of Sher-e-Bangla Agricultural University, in Dhaka, Bangladesh to study the effect of water stress on fruit quality and osmotic adjustment in different types. The plants had a tendency to adjust against drop in potential in soil by producing organic solutes such as glucose, fructose, sucrose and proline. The quality of fruits was improved as a result of the synthesis of ascorbic acid, citric acid and malic acid. No physical damage due to stress was observed in fruits, which were over 90% red. Another such test conducted studied the effect of water stress on the height, dry matter and yield of few cultivars where it was noticed that the yield was reduced due to stress but no significant difference was observed. \n\nNahar also worked on energy crops, especially their adaptability in different soil types of Bangladesh, especially wastelands choosing second generation energy crops not in high demand in the global food market and thus has little impact on food prices and food security thereby negating the food-fuel dilemma. Second generation biofuel crops can help supply fuel and alleviate the energy crisis with greater environmental benefits. Biofuel production uses biomass generally consisting of energy crops which usually also produces food. Second generation biofuel crops are ones that are not typically used for food purposes. Some grains are can be effectively grown on already-farmed dry lands that are low in carbon storage capacity, eliminating any concerns of clearing of rainforests. Breeds not requiring much irrigation will be an important consideration in the dry areas. As a stress tolerant plant it is well adapted to tropical, arid and semi-arid regions. It can be easily propagated and grows on a wide variety of soils, including marginal lands, wastelands and also even when the soil is considered infertile, meaning it can be grown on lands where food crops are not grown, hence it does not compete with the latter for space. She explored the potential of biofuel production on a limited land space for sustaining a growing population like Bangladesh. With rapidly growing urban and national population growth rate, Bangladesh's growing demand for energy with urban expansion has led to deforestation and a steady loss of arable lands, which may result in future food shortages, where she proposes a simple required land use per capita model .\\sum_{i=1}^{n}\\left[\nfor establishing a relationship between the biomass production, associated crop yields, the biomass to biofuel conversion methods and the overall fuel demand, as a plan to meet the national energy and habitable land demands while considering the looming environmental effects related to energy usage.\n\n"}
{"id": "180234", "url": "https://en.wikipedia.org/wiki?curid=180234", "title": "Kilowatt hour", "text": "Kilowatt hour\n\nThe kilowatt hour (symbol kWh, kW⋅h or kW h) is a unit of energy equal to 3.6 megajoules. If energy is transmitted or used at a constant rate (power) over a period of time, the total energy in kilowatt hours is equal to the power in kilowatts multiplied by the time in hours. The kilowatt hour is commonly used as a billing unit for energy delivered to consumers by electric utilities.\n\nThe kilowatt hour (symbolized kW⋅h as per SI) is a composite unit of energy equivalent to one kilowatt (1 kW) of power sustained for one hour.\nOne watt is equal to 1 J/s. One kilowatt hour is 3.6 megajoules, which is the amount of energy converted if work is done at an average rate of one thousand watts for one hour.\n\nThe derived unit of energy within the International System of Units (SI) is the joule. The hour is a unit of time \"outside the SI\", making the kilowatt hour a non-SI unit of energy. The kilowatt hour is not listed among the non-SI units accepted by the BIPM for use with the SI, although the hour, from which the kilowatt hour is derived, is.\n\nAn electric heater consuming 1000 watts (1 kilowatt), and operating for one hour uses one kilowatt hour of energy. A television consuming 100 watts operating for 10 hours continuously uses one kilowatt hour. A 40-watt electric appliance operating continuously for 25 hours uses one kilowatt hour. In terms of human power, a healthy adult male manual laborer will perform work equal to about half a kilowatt hour over an eight-hour day.\n\nElectrical energy is typically sold to consumers in kilowatt hours. The cost of running an electric device is calculated by multiplying the device's power consumption in kilowatts by the running time in hours and then by the price per kilowatt hour. The unit price of electricity may depend upon the rate of consumption and the time of day. Prices vary considerably by locality. In the United States prices in different states can vary by a factor of three.\n\nWhereas individual homes have historically only paid for the kilowatt hours consumed and the rated capacity, commercial buildings and institutions also pay for peak power consumption, the greatest power recorded in a fairly short time, such as 15 minutes. (However the installation of smart meters now enables the supplier also to vary the charging rate more flexibly for individual homes also.) This compensates the power company for maintaining the infrastructure needed to provide peak power. These charges are billed as demand charges. Industrial users may also have extra charges according to the power factor of their load.\n\nMajor energy production or consumption is often expressed as terawatt hours (TW⋅h) for a given period that is often a calendar year or financial year. A 365-day year equals 8,760 hours, so over a period of one year, power of one gigawatt equates to 8.76 terawatt hours of energy. Conversely, one terawatt hour is equal to a sustained power of about 114 megawatts for a period of one year.\n\nThe symbol \"kWh\" is commonly used in commercial, educational, scientific and media publications, and is the usual practice in electrical power engineering.\n\nOther abbreviations and symbols may be encountered:\n\nTo convert a quantity measured in a unit in the left column to the units in the top row, multiply by the factor in the cell where the row and column intersect.\nAll the SI prefixes are commonly applied to the watt hour: a kilowatt hour is 1,000 W⋅h (symbols kW⋅h, kWh or kW h; a megawatt hour is 1 million W⋅h, (symbols MW⋅h, MWh or MW h); a milliwatt hour is 1/1000 W⋅h (symbols mW⋅h, mWh or mW h) and so on.\nThe kilowatt hour is commonly used by electrical distribution providers for purposes of billing, since the monthly energy consumption of a typical residential customer ranges from a few hundred to a few thousand kilowatt hours. Megawatt hours (MWh), gigawatt hours (GWh), and terawatt hours (TWh) are often used for metering larger amounts of electrical energy to industrial customers and in power generation. The terawatt hour and petawatt hour (PWh) units are large enough to conveniently express the annual electricity generation for whole countries and the world energy consumption.\n\nPetawatt hours can describe the output of nuclear power plants across decades. For example, the Gravelines Nuclear Power Station in France became in 2010 the first power plant to ever deliver a cumulative petawatt-hour of electricity.\n\nThe terms power and energy are frequently confused. Power is the rate of delivery of energy. Power is work performed per unit of time. Energy is the work performed (over a period of time).\n\nPower is measured using the unit \"watts\", or \"joules per second\". Energy is measured using the unit \"watt seconds\", or \"joules\".\n\nA common household battery contains energy. When the battery delivers its energy, it does so at a certain power level, that is, the rate of delivery of the energy. The higher the power level, the quicker the battery's stored energy is delivered. If the power is higher, the battery's stored energy will be depleted in a shorter time period.\n\nFor a given period of time, a higher level of power causes more energy to be used. For a given power level, a longer run period causes more energy to be used. For a given amount of energy, a higher level of power causes that energy to be used in less time.\n\nPower units measure the rate of energy per unit time. Many compound units for rates explicitly mention units of time, for example, miles per hour, kilometers per hour, dollars per hour. Kilowatt hours are a product of power and time, not a rate of change of power with time.\nWatts per hour (W/h) is a unit of a \"change\" of power per hour. It might be used to characterize the ramp-up behavior of power plants. For example, a power plant that reaches a power output of 1 MW from 0 MW in 15 minutes has a ramp-up rate of 4 MW/h. Hydroelectric power plants have a very high ramp-up rate, which makes them particularly useful in peak load and emergency situations.\n\nThe proper use of terms such as \"watts per hour\" is uncommon, whereas misuse may be widespread.\n\nBy definition of the units, a consumption of 1 kWh/100 km is exactly equivalent to a resistance force of 36 N (newtons), an idea taken up by the von Kármán–Gabrielli diagram.\n\nSeveral other units are commonly used to indicate power or energy capacity or use in specific application areas.\nAverage annual power production or consumption can be expressed in kilowatt hours per year; for example, when comparing the energy efficiency of household appliances whose power consumption varies with time or the season of the year, or the energy produced by a distributed power source. One kilowatt hour per year equals about 114.08 milliwatts applied constantly during one year.\n\nThe energy content of a battery is usually expressed indirectly by its capacity in ampere-hours; to convert ampere-hour (A⋅h) to watt hours (W⋅h), the ampere-hour value must be multiplied by the voltage of the power source. This value is approximate, since the battery voltage is not constant during its discharge, and because higher discharge rates reduce the total amount of energy that the battery can provide. In the case of devices that output a different voltage than the battery, it is the battery voltage (typically 3.7 V for Li-ion) that must be used to calculate rather than the device output (for example, usually 5.0 V for USB portable chargers). This results in a 500 mA USB device running for about 3.7 hours on a 2500 mAh battery, not five hours.\n\nThe \"Board of Trade unit\" (BOTU) is an obsolete UK synonym for kilowatt hour. The term derives from the name of the Board of Trade which regulated the electricity industry until 1942 when the Ministry of Power took over.\n\nThe British thermal unit or BTU (not to be confused with BOTU), is a unit of thermal energy with several definitions, all about 1055 Joule or 0.293 watt hour. The quad, short for quadrillion BTU, or 10 BTU, is sometimes used in national-scale energy discussions in the United States. One quad is approximately 293 TWh or 1.055 exajoule (EJ).\n\nA TNT equivalent is a measure of energy released in the detonation of trinitrotoluene. A tonne of TNT equivalent is approximately 4.184 gigajoules or 1,163 kilowatt hours.\n\nA tonne of oil equivalent is the amount of energy released by burning one tonne of crude oil. It is approximately 41.84 gigajoules or 11,630 kilowatt hours.\n\nIn India, the kilowatt hour is often simply called a \"Unit\" of energy. A million units, designated \"MU\", is a gigawatt hour and a BU (billion units) is a terawatt hour.\n\nBurnup of nuclear fuel is normally quoted in megawatt days per tonne (MW⋅d/MTU), where tonne refers to a metric ton of uranium metal or its equivalent, and megawatt refers to the entire thermal output, not the fraction which is converted to electricity.\n\n\n"}
{"id": "52824147", "url": "https://en.wikipedia.org/wiki?curid=52824147", "title": "Korean brining salt", "text": "Korean brining salt\n\nKorean brining salt, also called Korean sea salt, is a variety of edible salt with a larger grain size compared to common kitchen salt. It is called \"wang-sogeum\" (; \"king/queen salt\") or \"gulgeun-sogeum\" (; \"coarse salt\") in Korean. The salt is used mainly for salting napa cabbages when making kimchi. Being minimally processed, it serves to help developing flavours in fermented foods.\n"}
{"id": "37019524", "url": "https://en.wikipedia.org/wiki?curid=37019524", "title": "List of equations in fluid mechanics", "text": "List of equations in fluid mechanics\n\nThis article summarizes equations in the theory of fluid mechanics.\n\nHere formula_1 is a unit vector in the direction of the flow/current/flux.\n\n\n\n"}
{"id": "18254249", "url": "https://en.wikipedia.org/wiki?curid=18254249", "title": "Load factor (electrical)", "text": "Load factor (electrical)\n\nIn electrical engineering the load factor is defined as the average load divided by the peak load in a specified time period. It is a measure of the utilization rate, or efficiency of electrical energy usage; a low load factor indicates that load is not putting a strain on the electric system, whereas consumers or generators with that put more of a strain on the electric distribution will have a high load factor.\n\nformula_1\n\nAn example, using a large commercial electrical bill:\n\n\nHence:\n\n\nIt can be derived from the load profile of the specific device or system of devices. Its value is always less than one because maximum demand is never lower than average demand, since facilities likely never operate at full capacity for the duration of an entire 24-hour day. A high load factor means power usage is relatively constant. Low load factor shows that occasionally a high demand is set. To service that peak, capacity is sitting idle for long periods, thereby imposing higher costs on the system. Electrical rates are designed so that customers with high load factor are charged less overall per kWh. This process along with others is called load balancing or peak shaving.\n\nThe load factor is closely related to and often confused with the demand factor.\n\nformula_2\n\nThe major difference to note is that the denominator in the demand factor is fixed depending on the system. Because of this, the demand factor cannot be derived from the load profile but needs the addition of the full load of the system in question.\n\n"}
{"id": "32758610", "url": "https://en.wikipedia.org/wiki?curid=32758610", "title": "Lysebotn Hydroelectric Power Station", "text": "Lysebotn Hydroelectric Power Station\n\nThe Lysebotn Power Station is a hydroelectric power station located in the municipality Forsand in Rogaland, Norway. The facility operates at an installed capacity of . The average annual production is 1,242 GWh. It has produced 63 TWh since it started in 1953. \nA new NOK 1.8 billion powerplant called Lysebotn II with 370 MW Francis turbines is being built nearby to replace it, with an expected life of 6070 years. The tunnels are 7.8 km long, 45 m wide, and transporting 60 m/second. \n\nThe reservoir has a capacity of water located at 636686 m altitude.\n\n"}
{"id": "17596012", "url": "https://en.wikipedia.org/wiki?curid=17596012", "title": "Minimum mass", "text": "Minimum mass\n\nIn astronomy, minimum mass is the lower-bound calculated mass of observed objects such as planets, stars and binary systems, nebulae, and black holes.\n\nMinimum mass is a widely cited statistic for extrasolar planets detected by the radial velocity method, and is determined using the binary mass function. This method reveals planets by measuring changes in the movement of stars in the line-of-sight, so the real orbital inclinations and true masses of the planets are generally unknown.\n\nIf inclination can be determined, the true mass can be obtained from the calculated minimum mass using the following relationship:\n\nFor orbiting bodies in extrasolar stellar and planetary systems, an inclination of 0° or 180° corresponds to a face-on orbit (which cannot be observed by radial velocity), whereas an inclination of 90° corresponds to an edge-on orbit (for which the true mass equals the minimum mass).\n\n"}
{"id": "48476037", "url": "https://en.wikipedia.org/wiki?curid=48476037", "title": "Nala Danavi Wind Farm", "text": "Nala Danavi Wind Farm\n\nThe Nala Danavi Wind Farm (also known as the Erumbukkudal Wind Farm after its location) is a onshore wind farm in Erumbukkudal, on the Kalpitiya Peninsula, Sri Lanka. The wind farm is operated by , which is a subsidiary of . The facility consists of six wind turbines measuring each.\n\n"}
{"id": "53675156", "url": "https://en.wikipedia.org/wiki?curid=53675156", "title": "New York's Reforming the Energy Vision", "text": "New York's Reforming the Energy Vision\n\nNew York’s Reforming the Energy Vision (REV) is a set of multi-year regulatory proceedings and policy initiatives launched in New York state in 2014. REV is intended to transform the way electricity is produced, bought and sold in New York and enable the integration of renewable energy generation and smart grid technologies on the electric grid. REV is ongoing with no predefined end date, and will impact all New York utilities and ratepayers.\n\nREV encompasses numerous policies and proceedings in the state of New York, as well as a restructuring of state utility ratemaking and revenue models. REV's utility reform component is part of a new trend toward utility reform that coincides with the drop in renewable energy prices.\n\nThe Andrew Cuomo administration, the New York Public Service Commission (PSC), and energy policy lead Richard Kauffman announced REV in April 2014. When they introduced REV, the administration cited the need to modernize New York's utility infrastructure to be more resilient in light of the impact of Hurricane Sandy. They also cited trends such as the aging of the electric grid, the need to control energy prices, and the threat of climate change.\n\nIn February 2015, the New York Public Service Commission (PSC) issued the REV Track One order, which adopted the official policy framework that serves as the basis of the REV proceedings.\n\nOverall, REV aims to make it easier for New York consumers and utilities to invest in distributed energy resources (DERs) and smart grid technology. Cuomo's administration hope that this shift will in turn create a more energy-efficient and resilient energy grid and a strong market for new energy technologies in New York State.\n\nREV's utility reform proceedings are intended to change the way that electric utilities make money. Utilities will maintain their former status as energy distributors, but will also assume the role of \"market operators,\" facilitating transactions between those who provide energy and those who use it. Utilities will be incentivized to use DER in their grid planning efforts.\n\nIn this new role, utilities will own the distributed service platform (DSP) that DER sellers and retail customers use to buy and sell electricity. REV envisions that current utilities in New York state will become a sort of “mini-ISO” as it relates to DERs. Utilities will be incentivized to use DER in their grid planning efforts.\n\nNew York's utilities are required to cooperate with the public service commission and other bodies to develop the new earnings mechanisms.\n\nThe detailed and necessary actions that are required of all stakeholders to execute the goals of REV have only recently been more clearly defined. Many utilities are in the process of determining how they will respond the new policy. It is unclear how long the transition to the new model will take.\n\nThe REV policy is being executed in two tracks. Both tracks seek to meet the same three goals:\n\n\nTrack One is described in an order released on February 26, 2015 focuses on shaping the new utility vision and DER ownership challenges.\n\nTrack Two described in an order released on May 16, 2016 focuses on the necessary changes in the current regulatory, tariff, market, and incentive structures.\n\nThe New York State Energy Plan is well integrated with REV and is intended to provide a roadmap for executing the REV policy. The New York State Energy Plan was released by the Cuomo administration in 2015. The Plan lays out New York's clean energy ambitions and includes the REV proceedings.\n\nGovernor Cuomo has cited ratepayer interested as part of the impetus for REV. Successful implementation of REV initiatives has the potential to save residents of New York state between $1.4 to $2.1 billion per year.\n\nState leaders are aware that low income customers and renters may be less likely to participate in distributed energy solutions like solar, that often require financing. As a partial solution, NY-SUN’s Community Distribution Generation program supports these customers. REV is also poised to make changes slowly such that utilities can remain profitable and effective operators of the grid through the transition.\n\nThe creators of REV believe the new utility model will allow ratepayers to better manage their energy usage and reduce their energy bills. For example, implementing Time of Use (TOU) rates and creating transparency that shows customers how much they could save by shifting electricity usage to lower peak demand parts of the day.\n\nHowever, the costs of overhauling the current utility model could theoretically be passed down to consumers. One of the largest costs will be creating the two way grid to support DERs.\n\nSupporters of REV reference Consolidated Edison’s (ConEd) Brooklyn/Queens Demand Management Program (BQDM) as an early success of REV. In place of requesting funds for capital expenditures to construct a $1 billion substation in Brooklyn to meet increasing load demand, ConEd utilized solar, batteries and energy efficiency solutions and was compensated in the same way a capital expenditure would be.\n\nNew York’s current electricity grid is not currently set up to handle bi-directional resource generation. This is critical to the success of REV and must be resolved if REV policies are expected to be fully implemented.\n\nThe Alliance for a Green Economy, an environmental group is advocating that the new DSP should be operated by an entity independent from the current utilities. They do not believe that allowing the current utilities to be responsible for the DSP platform will put customers on the same level playing field as the utilities.\n\nNew York is a leader in taking steps to update the most common utility model currently used throughout the United States. Regulators, utilities and stakeholders in other states and jurisdictions are closely watching the progress of REV implementation. Seven states (California, Hawaii, Minnesota, Michigan, Massachusetts, Missouri, and Wisconsin) have open dockets related to utility business models.\n"}
{"id": "11997299", "url": "https://en.wikipedia.org/wiki?curid=11997299", "title": "Nonode", "text": "Nonode\n\nA nonode is a type of thermionic valve that has nine active electrodes. The term most commonly applies to a seven-grid vacuum tube, also sometimes called an enneode. An example was the EQ80/UQ80, which was used as an FM quadrature detector. It was developed during the introduction of TV and FM radio and delivered an output voltage large enough to directly drive an end pentode while still allowing for some negative feedback. As most of the grids were tied together, even an 8-pin Rimlock base was sufficient in the case of the EQ40. \n"}
{"id": "2002708", "url": "https://en.wikipedia.org/wiki?curid=2002708", "title": "Oil Storm", "text": "Oil Storm\n\nOil Storm is a 2005 television mockumentary portraying a future oil-shortage crisis in the United States, precipitated by a hurricane destroying key parts of the United States' oil infrastructure. The program was an attempt to depict what would happen if the highly oil-dependent country was suddenly faced with gasoline costing upwards of $7 to $8 per gallon (as opposed to the national average of around $2 per gallon when the show first aired). Directed by James Erskine and written by Erskine and Caroline Levy, it originally aired on FX Networks on 5 June 2005, at 8 p.m. ET.\n\nThe crisis arises from a hurricane destroying an important pipeline at Port Fourchon in Louisiana, a tanker collision closing a busy port, terrorist attacks and tension with Saudi Arabia over the oil trade, and other fictional events. The program followed the lives of several people - the owners of a mom-and-pop convenience store, a paramedic, stock market and oil analysts, government officials, and others - and includes a substantial amount of human drama.\n\nHurricane Julia, a Category 4 hurricane, strikes the Gulf of Mexico, making a direct hit on New Orleans, Louisiana, killing thousands of people and causing severe damage. Large numbers of offshore oil rigs in the Gulf, and a major pipeline and the primary nerve center of the Gulf Coast petroleum industry at Port Fourchon, Louisiana are destroyed. It shows how the effects of that disaster could have significant consequences throughout the United States, even in areas far removed from landfall.\n\nWhile the loss of life and property in the storm is staggering (with the death toll in the thousands and the damage in the billions), the greater impact is on the crippled energy industry. Due to the destruction at Port Fourchon and in the Gulf, oil prices skyrocket from $55 to $77 per barrel, and the United States government is forced to take immediate action to rebuild the Gulf's energy infrastructure. Once the storm passes, the government starts to rebuild the infrastructure at Port Fourchon (requiring a minimum of eight months) and repair or replace damaged offshore rigs (requiring a similar amount of time). Also, shipping that normally goes through Port Fourchon is rerouted to the Port of Houston; Houston's port facilities are open around-the-clock with higher-than-usual throughput, with attendant higher risk of accident.\n\nWith widespread gas lines and prices over $3.00 per gallon, the U.S. persuades Saudi Arabia to increase its oil production by 1m barrels a day. The Saudi decision to aid America causes a backlash among a restive Muslim population already energized because of the 2003 invasion of Iraq. Local terrorists stage an attack on an upscale shopping mall in Riyadh which (after intervention by Saudi special forces) kills about 300 Americans associated with multinational oil companies. This attack leads the U.S. to send troops to Saudi Arabia. In the meantime, the oil crisis escalates when two large tankers collide in the narrow Houston Ship Channel, shutting down the Channel.\n\nOnce winter sets in, gas lines take a back seat to critical shortages of heating oil during a bitterly cold winter, with thousands dying in the cold. Some good news comes after the Houston Ship Channel is reopened, but on Christmas Day, the same Saudi terrorists blow up sections of the mammoth Ras Tanura refinery complex, killing 142 U.S. soldiers who were protecting the Saudi oil infrastructure; among the casualties is the eldest son of the convenience store owners. Oil prices reach $153 per barrel, and gas prices top $8 per gallon.\n\nMeanwhile, with a government budget crisis due to military and economic pressures, farm spending is cut dramatically, leading to a subplot in which the social and political effects of this are explored. A well-renowned agricultural leader organizes a 750,000-farmer march on Washington, leading to clashes with police and the leader being taken into custody.\n\nIn the spring, the U.S. makes a deal with Russia to send of oil by tanker, but the oil companies involved subsequently make a deal with China, which, equally hungry for oil and with greater financial reserves, outbids the U.S. This leaves America in a state of chaos, as well leading to soul-searching on whether China has now become the world's economic superpower. The country considers fast-tracking development of alternative energy sources, but there is little that can be done in the short-term to alter an economy structurally dependent on cheap foreign oil. Later, the U.S. government, showing unexpected diplomatic skill, resurrects the Russian oil deal by agreeing to a $16 billion long-term investment in its oil industry, and the China-bound tankers change course to the United States.\n\nThe crisis finally eases a year after Hurricane Julia, with Port Fourchon back at 80-percent capacity, the strategic petroleum reserve being replenished and tensions in Saudi Arabia stabilized. Oil prices drop from their highs at the height of the crisis to $73 per barrel, and gas prices nearing the $4 per gallon mark and dropping. However, the country has been through a stress as great as the Stock Market Crash of 1929, and now knowing that relying on OPEC for most of its foreign oil imports makes the United States vulnerable, Americans will never take cheap oil for granted again.\n\nThe events of Hurricane Katrina, its economic impact and the ensuing price increases nearly parallel some of the events in the movie. However, the damage to US oil infrastructure was less severe than in the film, and the lack of the compounding events (shutdown of Port of Houston, loss of some of Saudi Arabia's supply) means the consequences of Katrina are much less than of the fictional Julia. However, especially given the coincidence of dates (in the film, Julia strikes in early September 2005), the similarity of the early impact of the two storms has been noted.\n\nOn 28 August 2005, Hurricane Katrina was on a direct path to hit Port Fourchon and New Orleans. Many of the initial scenes of Hurricane Julia were playing out in real life with Hurricane Katrina, such as the mandatory evacuation of New Orleans, the opening of the Superdome as shelter, and the changing of traffic to contraflow. On 29 August 2005, Hurricane Katrina did not directly hit Port Fourchon but across Barataria Bay at Buras, but nonetheless some oil rigs were damaged. Saudi Arabia agreed to increase oil production to help. \n\nOn 30 August 2005, many gas stations raised prices by a considerable amount putting most of America over $3.00/gallon, as shown in the movie. On 1 September 2005, gas stations throughout the country began to run out of fuel due to worries of mass shortages. Some stations in Atlanta, Georgia sold gas at nearly $6/gallon. Most of this was due to panic buying, as noted in the film, rather than physical shortage. Subsequently, as the extent of the damage became clearer, prices eased.\n\nThe casualty numbers shown for New Orleans is also startlingly close to the casualties of Katrina. The Movies shows a little more than 1700 while the official number for Katrina is 1833.\n\nBoth Ford and GM announced layoffs and plant closings in late 2005 and early 2006. However, while one reason for the layoffs was increased gasoline prices, it was not the sole reason for the layoffs. In addition, the layoffs were on a much smaller scale than the ones shown in the film.\n\nThere was also the 2009 GM crisis in which General Motors requested and received Federal assistance. Unlike the mocumentary, GM did not die, it seemed \"too big to fail\" because maintaining automotive competition and the status quo seemed more important.\n\nIn the film, al-Qaeda makes a terrorist attack on the oil facilities at Ras Tanura. On 24 February 2006 al-Qaeda attacked the nearby facility at Abqaiq, though no damage was inflicted on the facilities themselves.\n"}
{"id": "44575747", "url": "https://en.wikipedia.org/wiki?curid=44575747", "title": "Oil sands tailings ponds", "text": "Oil sands tailings ponds\n\nOil sands tailings ponds are engineered dam and dyke systems used to capture oil sand tailings. Oil sand tailings contain a mixture of salts, suspended solids and other dissolvable chemical compounds such as acids, benzene, hydrocarbons residual bitumen, fine silts and water. Large volumes of tailings are a byproduct of bitumen extraction from the oil sands and managing these tailings is one of the most difficult environmental challenges facing the oil sands industry.\n\nIn Canada there are three major oil sand deposits. They are known as Athabasca Oil Sands, Cold Lake oil sands, and Peace River oil sands. These settling basins were meant to be temporary, but with no concrete strategy on how to clean them up and make them safe for the environment, they have become permanent waste dumps.\n\nOil sand tailings is a highly complex mixture of compounds and only some of its chemical components are known. The concentrations of chemicals may be harmful to fish and oil on the surface of the ponds may be harmful to birds.\n\nThe lack of knowledge and identification of individual compounds has become a major hindrance to the handling and monitoring of oil sands tailings. A better understanding of the chemical makeup, including naphthenic acids, it may be possible to monitor rivers for leachate and also to remove toxic components. The identification of individual acids has for many years proved to be impossible but a breakthrough in 2011 in analysis began to reveal what is in the oil sands tailings ponds. Theoretically, as much as ninety percent of the water in the tailings could be reused for further oil extraction.\n\nBy 2009 as tailing ponds continued to proliferate and volumes of fluid tailings increased, the Energy Resources Conservation Board of Alberta issued Directive 074 to force oil companies to manage tailings based on new aggressive criteria. The Government of Alberta reported in 2013 that tailings ponds in the Alberta oil sands covered an area of about .\n\nThe Syncrude Tailings Dam or Mildred Lake Settling Basin (MLSB) is an embankment dam that is, by volume of construction material, the largest earth structure in the world in 2001. It is located north of Fort McMurray, Alberta, Canada at the northern end of the Mildred Lake lease owned by Syncrude Canada Ltd.. The dam and the tailings artificial lake within it are constructed and maintained as part of ongoing operations by Syncrude in extracting oil from the Athabasca Oil Sands. Other tailings dams constructed and operated in the same area by Syncrude include the Southwest Sand Storage (SWSS), which is the third largest dam in the world by volume of construction material after the Tarbela Dam.\n\nAs of 2010, according to the \"Mature Fine Tailings Inventory from mine operator tailings plans submitted in October 2009, Canadian Natural Resources's (CNRL) mine, Horizon mine had of mature fine tailings (MFT) in their tailings ponds. However COSIA argues that CNRL's Horizon External Tailings Facility (ETF) is a relatively young pond with a configuration that minimizes the \"Pond Centre (PC) depositional environment\". It has a \"side hill\" facility with a three-sided dyke impounding fluid against the natural ground that rises away from the containment dyke.\"\n\nThe Government of Alberta released the Tailings Management Framework for Mineable Oil Sands as part of Alberta’s Progressive Reclamation Strategy for the oil sands to ensure that tailings are reclaimed as quickly as possible.\n\nIn March 2015 in response to the Tailings Management Framework for Mineable Athabasca Oil Sands, Alberta Energy Regulator (AER) suspended Directive 074: Tailings Performance Criteria and Requirements for Oil Sands Mining Schemes.\n\nOn July 14, 2016 the Alberta Energy Regulator - following consultations with \"consultations with First Nations, local communities, environmental groups and industry itself\" - issued Directive 85 with new guidelines and a phased-in approach on oil sands producers' management of their tailings ponds. Under Directive 85 \"fluid tailings\" must be \"ready to reclaim\" within ten years of the closing of an oil sands mine.\n\nSuncor invested $1.2 billion in their Tailings Reduction Operations (TROTM) method that treats mature fine tails (MFT) from tailings ponds with chemical flocculant, an anionic Polyacrylamide, commonly used in water treatment plants to improve removal of total organic content (TOC), to speed their drying into more easily reclaimable matter. Mature tailings dredged from a pond bottom in suspension were mixed with a polymer flocculant and spread over a \"beach\" with a shallow grade where the tailings would dewater and dry under ambient conditions. The dried MFT can then be reclaimed in situ or moved to another location for final reclamation. Suncor hoped this would reduce the time for water reclamation from tailings to weeks rather than years, with the recovered water being recycled into the oil sands plant. Suncor claimed the mature fines tailings process would reduce the number of tailing ponds and shorten the time to reclaim a tailing pond from 40 years at present to 7–10 years, with land rehabilitation continuously following 7 to 10 years behind the mining operations. For the reporting periods from 2010 to 2012, Suncor had a lower-than-expected fines capture performance from this technology.\n\nSyncrude used the older composite tailings (CT) technology to capture fines at its Mildred Lake project. Syncrude had a lower-than-expected fines capture performance in 2011/2012 but exceeded expectations in 2010/2011. Shell used atmospheric fines drying (AFD) technology combined \"fluid tailings and flocculants and deposits the mixture in a sloped area to allow the water to drain and the deposit to dry\" and had a lower-than-expected fines capture performance.\n\nBy 2010 Suncor had transformed their first tailings pond, Pond One, into Wapisiw Lookout, the first reclaimed settling basin in the oil sands. In 2007 the area was a 220-hectare pond of toxic effluent but several years later there was firm land planted with black spruce and trembling aspen. Wapisiw Lookout represents only one percent of tailings ponds in 2011 but Pond One was the first effluent pond in the oil sands industry in 1967 and was used until 1997. By 2011 only 65 square kilometres were cleaned up and about one square kilometre was certified by Alberta as a self-sustaining natural environment. Wapisiw Lookout has not yet been certified. Closure operations of Pond One began in 2007. The jello-like mature fine tails (MFT) were pumped and dredged out of the pond and relocated to another tailings pond for long-term storage and treatment. The MFT was then replaced with 30 million tonnes clean sand and then topsoil that had been removed from the site in the 1960s. The 1.2 million cubic meters of topsoil over the surface, to a depth of 50 centimetres, was placed on top of the sand in the form of hummocks and swales. It was then planted with reclamation plants.\n\nIn 2008 Syncrude Canada Ltd. began construction of Sandhill Fen project, a 57-hectare research watershed- creating a mix of forest and wetland- on top of sand-capped composite tailings at its former 60-metre deep East Mine.\n\nThe Pembina Institute suggested that the huge investments by many companies in Canadian oil sands was leading to increased production results in excess bitumen with no place to store it. It added that by 2022 a month’s output of waste-water could result in a 11-feet deep toxic reservoir the size of New York City’s Central Park [840.01 acres (339.94 ha) (3.399 km²)].\n\nThe oil sands industry may build a series of up to thirty lakes by pumping water into old mine pits when they have finished excavation leaving toxic effluent at their bottoms and letting biological processes restore it to health. It is less expensive to fill abandoned open pit mines with water instead of dirt. In 2012 the Cumulative Environmental Management Association (CEMA) described End Pit Lakes (EPL) as CEMA acknowledged that the \"main concern is the potential for EPLs to develop a legacy of toxicity and thus reduce the land use value of the oil sands region in the future.\" Syncrude Canada was planning the first end pit lake in 2013 with the intention of \"pumping fresh water over 40 vertical metres of mine effluent that it has deposited in what it calls 'base mine lake.'\" David Schindler argued that no further end pit lakes should be approved until we \"have some assurance that they will eventually support a healthy ecosystem.\" There is to date no “evidence to support their viability, or the ‘modelled’ results suggesting that outflow from the lakes will be non-toxic.\"\n\nIn March 2012 an alliance of oil companies called Canada’s Oil Sands Innovation Alliance (COSIA) was launched with a mandate to share research and technology to decrease the negative environmental impact of oil sands production focusing on tailings ponds, greenhouse gases, water and land. Almost all the water used to produce crude oil using steam methods of production ends up in tailings ponds. Recent enhancements to this method include Tailings Oil Recovery (TOR) units which recover oil from the tailings, Diluent Recovery Units to recover naphtha from the froth, Inclined Plate Settlers (IPS) and disc centrifuges. These allow the extraction plants to recover well over 90% of the bitumen in the sand.\n\nIn January 2013, scientists from Queen's University published a report analyzing lake sediments in the Athabasca region over the past fifty years. They found that levels of polycyclic aromatic hydrocarbons (PAHs) had increased as much as 23-fold since bitumen extraction began in the 1960s. Levels of carcinogenic, mutagenic, and teratogenic PAHs were substantially higher than guidelines for lake sedimentation set by the Canadian Council of Ministers of the Environment in 1999. The team discovered that the contamination spread farther than previously thought.\n\n\n"}
{"id": "23698052", "url": "https://en.wikipedia.org/wiki?curid=23698052", "title": "P. K. Iyengar", "text": "P. K. Iyengar\n\nPadmanabha Krishnagopala Iyengar (29 June 1931 – 21 December 2011; best known as P. K. Iyengar), was an Indian nuclear physicist who is widely known for his central role in the development of the nuclear program of India. Iyengar previously served as the director of BARC and former chairman of the Atomic Energy Commission of India, he raised his voice and opposition against the nuclear agreement between India and the United States and expressed that the deal favoured the United States.\n\nDuring his last years of his life, Iyenger engaged in peace activism and greatly exhorted the normalization of bilateral relations between India and Pakistan.\n\nIyengar joined the Tata Institute for Fundamental Research, Department of Atomic Energy in 1952 as a junior research scientist, undertaking a wide variety of research in neutron scattering. He later got shifted to Atomic Energy Establishment (later renamed as Bhabha Atomic Research Centre) when it was formed in 1954. In 1956, Iyengar was trained in Canada working under Nobel laureate in Physics Bertram Neville Brockhouse, contributing to path-breaking research on lattice dynamics in germanium. At the DAE, he built up and headed the team of physicists and chemists that gained international recognition for their original research contributions in this field. In 1960s, he indigenously designed the PURNIMA reactor and headed the team that successfully commissioned the reactor on 18 May 1972 at BARC.\n\nWhen Ramanna took over as director of Bhabha Atomic Research Centre in 1972, the mantle of directorship of the Physics Group (PG) was handed over to Iyengar. He was one of the key scientist in the development of India's first nuclear device. The team, under Raja Ramanna tested the device under the code name Smiling Buddha on 18 May 1974. Iyengar played a leading role in the peaceful nuclear explosion at Pokharan-I, for which he was conferred the Padma Bhushan in 1975.\n\nIyengar took over as Director of the Bhabha Atomic Research Centre in 1984. As director, one of his first tasks was to take charge of the construction of the Dhruva reactor, the completion of which was then in question, and bring it to a successful conclusion under his leadership. Recognizing the importance of transferring newly developed technology from research institutes to industry, he introduced a Technology Transfer Cell at the BARC to assist and speed the process. He motivated basic research in fields ranging from molecular biology, to chemistry and material science. He nucleated new technologies like lasers and accelerators, which led to the establishment of a new Centre for Advanced Technology, at Indore.\n\nIyengar was appointed chairman of the Atomic Energy Commission of India and secretary to the Department of Atomic Energy in 1990. He was also appointed as chairman of the Nuclear Power Corporation of India. Under his leadership the Department of Atomic Energy vigorously pursued the nuclear power programme with the commissioning of two new power reactors at Narora and Kakrapar, and continued with the development of new reactor systems, such as liquid-sodium based fast reactors. Equal emphasis was laid on enhanced production of heavy water, nuclear fuel and special nuclear materials. He also initiated proposals for the export of heavy-water, research reactors, hardware for nuclear applications to earn precious foreign exchange.\n\nRegarding Iyengar's involvement in Indian cold fusion research, the Indian newspaper \"Daily News and Analysis\" wrote: \"Iyengar also pioneered cold fusion experiments in the 1980s to prove the hypothesis that nuclear fusion can occur at ordinary temperatures under certain scenarios. The experiments were discontinued after Iyengar's exit from the nuclear establishment by some conservative scientists.\" \n\nIyengar has been the recipient of many high civilian awards and honours. After retirement Iyengar served in various positions such as member of the Atomic Energy Commission, scientific advisor to the Government of Kerala, on the board of the Global Technology Development Centre, president of the Indian Nuclear Society, and a member of the Inter-governmental Indo-French Forum, besides serving on various national committees. Iyengar’s later interests focused on advances in nuclear technology for nuclear applications, issues of nuclear policy and national security, science education and the application of science in nation-building. He participated in various international meetings on non-proliferation issues. Most recently, as a founder trustee of the Agastya International Foundation, he focused on rural education and instilling creativity and scientific temperament in rural children.\n\n\n"}
{"id": "4612642", "url": "https://en.wikipedia.org/wiki?curid=4612642", "title": "PPS 10", "text": "PPS 10\n\nPlanning Policy Statement 10: Planning for Sustainable Waste Management commonly abbreviated as PPS 10, is a document produced by the British Government and forms part of the national waste management plan for the United Kingdom. The current version was introduced in July 2005 and replaced Revised PPG 10: Planning and Waste Management (published 1999).\n\n"}
{"id": "24570568", "url": "https://en.wikipedia.org/wiki?curid=24570568", "title": "Paul E. Simons", "text": "Paul E. Simons\n\nPaul E. Simons served as the Ambassador Extraordinary and Plenipotentiary of the United States to Chile from 9 November 2007 to 2010.\n\nIn April 2015, Paul Simons had been selected new Deputy Executive Director of the International Energy Agency (IEA). He assumed office in July 2015.\n\n"}
{"id": "5777994", "url": "https://en.wikipedia.org/wiki?curid=5777994", "title": "Petroleum pricing in Prince Edward Island", "text": "Petroleum pricing in Prince Edward Island\n\nPetroleum pricing in Prince Edward Island is regulated by the\nPrince Edward Island Regulatory and Appeals Commission, the regulatory arm of the Government of Prince Edward Island. The commission is commonly shortened to the \"Island Regulatory and Appeals Commission and is usually referred to by its acronym \"IRAC\".\n\nPetroleum marketers buy fuel on the basis of the \"rack price\" and the price set by refiners willing to supply Prince Edward Island retailers, mainly the Irving Oil Refinery in Saint John, New Brunswick, which, since August 2013 is the only oil refinery remaining in the Maritimes. IRAC petroleum regulators determine a \"wholesale price\" or \"Approved Dealer Base Price\" based on commodity prices usually derived from the NYEX. IRAC petroleum regulators then sets a \"tank truck price\" which includes Federal Excise Tax (10¢/L for gasoline and 6¢/L for diesel, as well as the provincial petroleum tax (13.1¢/L for gasoline and 20.2¢/L for diesel). IRAC petroleum regulators also establish a \"retail price\" which provides a profit margin for retailers established at 5¢/L to 7.5¢/L for full serve fuel and 4¢/L to 5.5¢/L for self serve fuel. All prices have an additional 15% Harmonized Sales Tax (HST) added.\n\nPrices are revised on the 1st and 15th day of each month, or whenever the price raises or falls on the market sufficiently to interrupt the cycle by triggering an interrupting formula.\n\nConsumers in Prince Edward Island are notified of any changes in the price via the IRAC website and through the media. Price changes are closely guarded by IRAC to prevent runs on supply at local service stations.\n\n"}
{"id": "23941767", "url": "https://en.wikipedia.org/wiki?curid=23941767", "title": "Prosolvable group", "text": "Prosolvable group\n\nIn mathematics, more precisely in algebra, a prosolvable group (less common: prosoluble group) is a group that is isomorphic to the inverse limit of an inverse system of solvable groups. Equivalently, a group is called prosolvable, if, viewed as a topological group, every open neighborhood of the identity contains a normal subgroup whose corresponding quotient group is a solvable group.\n\n\n"}
{"id": "12045340", "url": "https://en.wikipedia.org/wiki?curid=12045340", "title": "Skipping tornado", "text": "Skipping tornado\n\nA skipping tornado is a vaguely defined term which refers to a process tornado (or a series of tornadoes) which has a discontinuous damage path. Often the term \"skipping\" is used only as an adjective in describing the process.\n\nThere are several possible causes for this phenomenon:\n\nSkipping historically was a term used to describe breaks in the damage path of what was considered as a single longer track tornado. With the discovery of cyclic tornadogenesis with some supercell thunderstorms, it was learned that successive tornadoes form with new mesocyclones and the resulting series of tornadoes is referred to as a tornado family. Especially when a tornado is first developing there may be small gaps in damage due to the circulation briefly ceasing at the surface or becoming too weak to cause damage even as the parent mesocyclone (and possibly a tornadic circulation aloft) is continuous and this is typically, but not always, considered a single tornado. In these cases, usually only tornadoes emanating from new mesocyclones (i.e. new tornadogenesis cycle and new wall cloud) are counted as a separate tornado. Before it was recognized that tornadoes can be multivortex, the phenomenon of subvortices also caused confusion both during a tornado and for surveying damage paths. Some forms of tornadoes, again especially in the process of formation prior to congealing their wind fields, may be multivortex tornadoes with dispersed subvortices with subdamaging wind in the intervening space within the larger circulation, which can give the appearance of skipping.\n\n\n"}
{"id": "42713377", "url": "https://en.wikipedia.org/wiki?curid=42713377", "title": "Solar Ship, Inc.", "text": "Solar Ship, Inc.\n\nSolar Ship Inc. is a company based out of Toronto, Ontario, Canada in order to develop hybrid aircraft to deliver critical cargo to cut-off places. The solarship gains lift from both buoyant gas and aerodynamics and uses power from solar panels. The aircraft is a new concept of transport that does not rely on fossil fuels or ground infrastructure.\n\nThe concept behind Solar Ship was based on airships and the Canadian bush plane, ad devised by Jay Godsall in 1983. G In 2006, Solar Ship Inc. was officially established.\n\nThe solarship's wing-ship design allows for extreme short takeoff and landing (XSTOL), such as in a soccer field. Its design provides a large surface area for solar electric power, allowing long, self- sufficient range. A solarship does not need to be solar powered and can be powered by traditional combustion. However, the goal is to develop a new mode of transportation that does not depend on fossil fuels, roads, or runways. The solarship can access areas where planes, trucks, ships and airships cannot, delivering cargo to the places that are currently cut off from the benefits of the connected world. Each solar plane is designed and built to the requirements of a mission. Currently, there are three initial missions with specific requirements: the Wolverine, the Caracal, and the Nanuq. Solar Ship's active mission, as of December 2015, is Mission Burundi.\n\nOn August 29, 2014, a prototype aircraft owned, designed, and operated by Solar Ship crashed during a test flight in a tobacco field near Brantford Airport, seriously injuring its two pilots, Mark Taylor and Mark Marshall. One of the pilots had to be cut out of the aircraft by rescuers. The crash was attributed to pilot error. The incident resulted in the temporary closure of Brantford Airport.\n\n\n"}
{"id": "1105907", "url": "https://en.wikipedia.org/wiki?curid=1105907", "title": "Spark-gap transmitter", "text": "Spark-gap transmitter\n\nA spark-gap transmitter is a device that generates radio frequency electromagnetic waves using an intermittent electric discharge across a spark gap to produce radio frequency current oscillations in a tuned circuit. These are fed to an aerial to transmit a radio signal through the atmosphere. \n\nSpark gap transmitters were the first devices to demonstrate practical radio transmission, and were the standard technology for the first three decades of radio (1887–1916). Later, more efficient transmitters were developed based on rotary machines like the high-speed Alexanderson alternators and the static Poulsen arc generators.\n\nMost operators, however, still preferred spark transmitters because of their uncomplicated design and because the carrier wave (carrier) stopped when the telegraph key was released, which let the operator \"listen through\" for a reply. With other types of transmitter, the carrier could not be controlled so easily, and they required elaborate measures to modulate the carrier and to prevent transmitter leakage from de-sensitizing the receiver.\n\nAfter WWI, greatly improved transmitters based on vacuum tubes became available, which overcame these problems, and by the late 1920s the only spark transmitters still in regular operation were \"legacy\" installations on naval vessels. Even when vacuum tube based transmitters had been installed, many vessels retained their crude but reliable spark transmitters as an emergency backup. However, by 1940, the technology was no longer used for communication. Use of the spark-gap transmitter led to many radio operators being nicknamed \"Sparks\" long after they ceased using spark transmitters. Even today, the German verb \"funken\", literally, \"to spark\", also means \"to send a radio message or signal\".\n\nThe effects of sparks causing unexplained \"action at a distance\", such as inducing sparks in nearby devices, had been noticed by scientists and experimenters well before the invention of radio. Extensive experiments were conducted by Joseph Henry (1842), Thomas Edison (1875) and David Edward Hughes (1878). With no other theory to explain the phenomenon, it was usually written off as electromagnetic induction.\n\nIn 1886, after noticing unusual induced sparking in a Riess spiral, physicist Heinrich Hertz concluded this phenomenon could be used to scientifically verify James Clerk Maxwell's predictions on electromagnetism. Hertz used a tuned spark gap transmitter and a tuned spark gap detector (consisting of a loop of wire connected to a small spark gap) located a few meters from the source. In a series of experiments, Hertz verified that electromagnetic waves were being produced by the transmitter: when the transmitter sparked, small sparks also appeared across the receiver's spark gap, which could be seen under a microscope.\nMany experimenters used the spark gap setup to further investigate the new \"Hertzian wave\" (radio) phenomenon, including Oliver Lodge and other \"Maxwellian\" investigators. The Serbian American engineer Nikola Tesla proposed methods to synchronise sparks with the peak output of an alternator, which he patented in 1896, while pursuing a wireless lighting and power distribution system based on his own conduction/ether theories.\n\nThe Italian inventor Guglielmo Marconi used a spark-gap transmitter in his experiments to develop the radio phenomenon into a wireless telegraphy system in the early 1890s. In 1895 he succeeded in transmitting over a distance of 1 1/4 miles. His first transmitter consisted of an induction coil connected between a wire antenna and ground, with a spark gap across it. Every time the induction coil pulsed, the antenna was momentarily charged up to tens (sometimes hundreds) of thousands of volts until the spark gap started to arc. This acted as a switch, essentially connecting the charged antenna to ground and producing a brief burst of electromagnetic radiation.\n\nWhile the various early systems of spark transmitters worked well enough to prove the concept of wireless telegraphy, the primitive spark gap assemblies used had some severe shortcomings. The biggest problem was that the maximum power that could be transmitted was directly determined by how much electrical charge the antenna could hold. Because the capacitance of practical antennas is quite small, the only way to get a reasonable power output was to charge it up to very high voltages. However, this made transmission impossible in rainy or even damp conditions. Also, it necessitated a quite wide spark gap, with a very high electrical resistance, with the result that most of the electrical energy was used simply to heat up the air in the spark gap.\n\nAnother problem with the spark transmitter was a result of the shape of the waveform produced by each burst of electromagnetic radiation. These transmitters radiated an extremely \"dirty\" wide band signal that could greatly interfere with transmissions on nearby frequencies. Receiving sets relatively close to such a transmitter had entire sections of a band masked by this wide band noise.\n\nDespite these flaws, Marconi was able to generate sufficient interest from the British Admiralty in these originally crude systems to eventually finance the development of a commercial wireless telegraph service between United States and Europe using vastly improved equipment.\n\nReginald Fessenden's first attempts to transmit voice employed a spark transmitter operating at approximately 10,000 sparks/second. To modulate this transmitter he inserted a carbon microphone in series with the supply lead. He experienced great difficulty in achieving intelligible sound. At least one high-powered audio transmitter used water cooling for the microphone.\n\nIn 1905 a \"state of the art\" spark gap transmitter generated a signal having a wavelength between 250 meters (1.2 MHz) and 550 meters (545 kHz). 600 meters (500 kHz) became the international distress frequency. The receivers were simple unamplified magnetic detectors or electrolytic detectors. This later gave way to the famous and more sensitive galena crystal sets. Tuners were primitive or nonexistent. Early amateur radio operators built low power spark gap transmitters using the spark coil from Ford Model T automobiles. But a typical commercial station in 1916 might include a 1/2 kW transformer that supplied 14,000 volts, an eight section capacitor, and a rotary gap capable of handling a peak current of several hundred amperes.\n\nShipboard installations usually used a DC motor (usually run off the ship's DC lighting supply) to drive an alternator whose AC output was then stepped up to 10,000–14,000 volts by a transformer. This was a very convenient arrangement, since the signal could be easily modulated by simply connecting a relay between the relatively low voltage alternator output and the transformer's primary winding, and activating it with the telegraph key. (Lower-powered units sometimes used the telegraph key to directly switch the AC, but this required a heavier key making it more difficult to operate).\n\nSpark gap transmitters generate fairly broad-band signals. As the more efficient transmission mode of continuous waves (CW) became easier to produce and band crowding and interference worsened, spark-gap transmitters and damped waves were legislated off the new shorter wavelengths by international treaty, and replaced by Poulsen arc converters and high frequency alternators, which developed a sharply defined transmitter frequency. These approaches later yielded to vacuum tube technology and the 'electric age' of radio ended. Long after operators no longer used spark gap transmitters for communications, the military used them for radio jamming. As late as 1955, a Japanese radio-controlled toy bus used a spark transmitter and coherer receiver; the spark was visible behind a sheet of blue transparent plastic.\n\nSpark gap oscillators are still used to generate high frequency high voltage to initiate welding arcs in gas tungsten arc welding. Powerful spark gap pulse generators are still used to simulate EMPs. Most high power gas-discharge street lamps (mercury and sodium vapor) still use modified spark transmitters as switch-on ignitors.\n\nThe function of the spark gap is to initially present a high resistance to the circuit so that the C1 capacitor is allowed to charge. When the breakdown voltage of the gap is reached, the air in the gap ionizes, the resistance across the gap is dramatically lower and a current pulse flows across the arc to the rest of the circuit. The gap is set so that the discharge coincides with a maximum or near maximum of charge in C1 and it is as if a high speed switch is turned on at just the right moment to allow the C1 capacitor to discharge its stored energy into the other circuit elements. This pulse of energy is rapidly exchanged back and forth between the C2 and L elements and takes the form of a damped oscillation at a radio frequency. The back and forth exchange of energy is in the form of an alternating current and voltage wave with much of the energy flowing out to the antenna.\n\nThese waves are called \"damped waves\" because the wave tends to \"die out\" or \"dampen out\" between discharges of the spark gap as opposed to modern continuous waves (CW), which don't die out. Because the \"damped waves\" are a train of regularly spaced radio frequency triangle waves that occur at an audio rate, early crystal, magnetic and Fleming valve detectors heard them as a musical note, rich in harmonics, making it easy for the human ear to \"copy\" messages and identify stations by their unique sound, even under adverse conditions.\n\nThe exchange of energy in this kind of oscillator occurs at a rate or frequency determined by the resonant frequency of its \"tank circuit\" which is composed of the combined capacitance of C1 and C2 and the inductance of L, famously known as an LC circuit. Capacitance of C2 was generally small and generally not shown in most diagrams. C2 represents the stray circuit capacitance, but C1 was relatively huge both in size and capacity so that it could store the large amount of high voltage energy necessary for high power transmission (P=EI). Some installations had whole buildings devoted to the C1 capacitor (as in the Cape Breton Transmitter). The inductance coils (L) were relatively small so that the entire circuit could resonate at a reasonably \"high\" frequency, given the large value of C1. Frequencies much above 1 MHz were impractical, because L could not get electrically smaller and not enough energy could be stored in a small C1—though a small C1 would have been necessary because of the resonance characteristics of \"shortwave\" frequencies.\n\nIn addition to the size and robustness of the oscillator components, the lower frequency components were likewise robust. This is because a very large induced EMF occurs when the spark is struck, causing a strain on the insulation in the primary transformer. To overcome this, the construction of even low-power sets was very solid and a radio frequency choke coil or a resistor (R shown in this diagram) was necessary to protect the transformer or induction coil. The telegraph key (essentially an easy to operate on/off switch) many times had to carry large currents and high voltages and so it also was generally quite robust too.\n\nThough ubiquitous in early radio, the spark gap transmitter was finally doomed by its extremely broad frequency spectrum and damped wave output. Damped waves were excellent for radiotelegraph with early radio detectors, but are very wasteful of bandwidth. This limited the number of stations that could effectively use a band, because of the interference. Also, wide bandwidth meant the transmitter spread useful intelligence over a large spectrum, and only a fraction of the transmit power was useful for communications. Finally, the damped wave is already a form of amplitude modulation (AM) and cannot be further modulated for voice with any intelligibility. Only the continuous wave oscillators made possible by vacuum tube technology could provide high frequency (HF) and beyond, and only their advent made efficient radiotelegraph and voice/data transmissions possible.\n\nA simple spark gap consists of two conducting electrodes separated by a gap immersed within a gas (typically air). When a sufficiently high voltage is applied, a spark bridges the gap, ionizing the gas and drastically reducing its electrical resistance. An electric current then flows until the path of ionized gas is broken or the current is reduced below a minimum value called the 'holding current'. This usually occurs when the voltage across the gap drops sufficiently, but the process may also be assisted by cooling the spark channel or by physically separating the electrodes. This breaks the conductive filament of ionized gas, allowing the capacitor to recharge, and permitting the recharging/discharging cycle to repeat. The action of ionizing the gas is quite sudden and violent (\"disruptive\"), and it creates a sharp sound (ranging from a \"snap\" for a spark plug, to a loud \"bang\" for a wider gap). The noise from the spark mechanism, especially from the higher powered transmitters, was so loud that it could seriously interfere with the operator's ability to receive messages after transmitting. Higher powered spark gap mechanisms were isolated from the operator's station in an insulated space called a \"silent room\", which, when the radio was transmitting, was anything but silent inside. The spark gap also produces light and heat.\n\nQuenching refers to the act of extinguishing the previously established arc within the spark gap. This is considerably more difficult than initiating spark breakdown in the gap. As transmitter power was increased, the problem of quenching arose.\n\nA cold, non-firing spark gap contains no ionized gases. Once the voltage across the gap reaches its breakdown voltage, gas molecules in the gap are very quickly ionized along a path, creating a hot electric arc, or plasma, that consists of large numbers of ions and free electrons between the electrodes. The arc also heats part of the electrodes to incandescence. The incandescent regions contribute free electrons via thermionic emission, and (easily ionized) metal vapor. The mixture of ions and free electrons in the plasma is highly conductive, resulting in a sharp drop in the gap's electrical resistance. This highly conductive arc supports efficient tank circuit oscillations. However, the oscillating current also sustains the arc and, until it can be extinguished, the tank capacitor cannot be recharged for the next pulse.\n\nSeveral methods were applied to quench the arc.\n\nSpark gaps in early radio transmitters varied in construction, depending on the power they had to handle. Some were fairly simple, consisting of one or more fixed (\"static\") gaps connected in series, while others were significantly more complex. Because sparks were quite hot and erosive, electrode wear and cooling were constant problems.\n\nThe need to extinguish arcs in increasingly higher power transmitters led to development of the rotating spark gap. These devices used an alternating current power supply, produced a more regular spark, and could handle more power than conventional static spark gaps. An inner rotating metal disc typically had a number of studs on its outer edge. A discharge took place when two studs lined up with two outer contacts that carried the high voltage. The resulting arcs rapidly stretched, cooled, and broke as the disk rotated.\n\nRotary gaps operated in two modes, synchronous and asynchronous. A synchronous gap was driven by a synchronous AC motor so that it ran at a fixed speed, and the gap fired in direct relation to the waveform of the A.C. supply that recharged the tank capacitor. The operator changed the point in the waveform where the gaps were closest by adjusting the rotor position on the motor shaft relative to the stator's studs. By properly adjusting the synchronous gap, it was possible to have the gap fire only at the voltage peaks of the input current. This technique made the tank circuit fire only at successive voltage peaks, thereby delivering maximum energy from the fully charged tank capacitor each time the gap fired. The \"break rate\" was thus fixed at twice the incoming power frequency (typically, 100 or 120 breaks/second, corresponding to 50 Hz or 60 Hz supply). When properly engineered and adjusted, synchronous spark gap systems delivered the largest amount of power to the antenna. However, electrode wear progressively changed the gap's \"firing point\", so synchronous gaps were somewhat temperamental and difficult to maintain.\n\nAsynchronous gaps were considerably more common. In an asynchronous gap, the rotation of the motor had no fixed relationship relative to the incoming AC waveform. Asynchronous gaps worked quite well and were much easier to maintain. By using a larger number of rotating studs or a higher rotational speed, many asynchronous gaps operated at break rates in excess of 400 breaks/second. Since the gap could fire more often than the input waveform could switch polarity, the tank capacitor charged and discharged more rapidly than a synchronous gap. However, each discharge occurred at a different voltage, which was almost always lower than the consistent peak voltage of a synchronous gap.\n\nRotary gaps also altered the tone of the transmitter, since changing either the number of studs or rotational speed changed the spark discharge frequency. This was audible in receivers with detectors that could detect the modulation on the spark signal—which enabled listeners to distinguish between different transmitters that were nominally tuned to the same frequency. A typical high-power multiple spark system (as it was also called) used a rotating commutator with six to twelve studs per wheel, typically switching several thousand volts.\n\nThe output of a rotary spark gap transmitter was turned on and off by the operator using a special kind of telegraph key that switched power going to the high voltage power supply. The key was designed with large contacts to carry the heavy current that flowed into the low voltage (primary) side of the high voltage transformer (often in excess of 20 amps). Alternatively a relay was used to do the actual switching.\n\n\n\n"}
{"id": "13886058", "url": "https://en.wikipedia.org/wiki?curid=13886058", "title": "Standby of death", "text": "Standby of death\n\nIn computing, the term standby of death (also SOD) refers to a problem with some electronic devices where they cannot resume after entering standby. This usually requires the user to forcibly restart the device, typically resulting in the loss of any unsaved work. Although such problems have plagued standby-capable devices since their inception, the specific term \"standby of death\" is relatively new and uncommon.\n"}
{"id": "88138", "url": "https://en.wikipedia.org/wiki?curid=88138", "title": "Thunderbolt", "text": "Thunderbolt\n\nA thunderbolt or lightning bolt is a symbolic representation of lightning when accompanied by a loud thunderclap. In Indo-European mythology, the thunderbolt was identified with the 'Sky Father'; this association is also found in later Hellenic representations of Zeus and Vedic descriptions of the \"vajra\" wielded by the god Indra. It may have been a symbol of cosmic order, as expressed in the fragment from Heraclitus describing \"the Thunderbolt that steers the course of all things\".\n\nIn its original usage the word may also have been a description of the consequences of a close approach between two planetary cosmic bodies, as Plato suggested in \"Timaeus\", or, according to Victor Clube, meteors, though this is not currently the case. As a divine manifestation the thunderbolt has been a powerful symbol throughout history, and has appeared in many mythologies. Drawing from this powerful association, the thunderbolt is often found in military symbolism and semiotic representations of electricity.\n\nLightning plays a role in many mythologies, often as the weapon of a sky god and weather god. As such, it is an unsurpassed method of dramatic instantaneous retributive destruction: thunderbolts as divine weapons can be found in many mythologies.\n\nThe name \"thunderbolt\" or \"thunderstone\" has also been traditionally applied to the fossilised rostra of belemnoids. The origin of these bullet-shaped stones was not understood, and thus a mythological explanation of stones created where a lightning struck has arisen.\n\nThe thunderbolt or lightning bolt continues into the modern world as a prominent symbol; it has entered modern heraldry and military iconography.\n\n\n\n\n\n"}
{"id": "1089891", "url": "https://en.wikipedia.org/wiki?curid=1089891", "title": "Travois", "text": "Travois\n\nA travois (Canadian French, from French \"travail\", a frame for restraining horses; also obsolete travoy or travoise) is a historical frame structure that was used by indigenous peoples, notably the Plains Indians of North America, to drag loads over land.\n\nThere is evidence to support the thesis that travois were used in other parts of the world before the invention of the wheel.\n\nThe basic construction consists of a platform or netting mounted on two long poles, lashed in the shape of an elongated isosceles triangle; the frame was dragged with the sharply pointed end forward. Sometimes the blunt end of the frame was stabilized by a third pole bound across the two poles.\n\nThe travois was dragged by hand, sometimes fitted with a shoulder harness for more efficient dragging, or dragged by dogs or horses (after the 16th-century introduction of horses by the Spanish).\n\nA travois could either be loaded by piling goods atop the bare frame and tying them in place, or by first stretching cloth or leather over the frame to hold the load to be dragged.\n\nAlthough considered more primitive than wheel-based forms of transport, on the type of territory where the travois was used (forest floors, soft soil, snow, etc.), rather than roadways, wheels would have encountered difficulties which would have made them less efficient. As such the travois was employed by \"coureurs des bois\" in New France's fur trade with the Plains Tribes.\n\nIt is possible for a person to transport more weight on a travois than can be carried on the back.\n\nThe basic dog travois consists of two aspen or cottonwood poles notched and lashed together at one end with buffalo sinew; the other ends rest splayed apart. Cross-bars are lashed between the poles near the splayed ends, and the finished frame looks like a large letter A with extra cross-bars. The apex of the A, wrapped in buffalo skin to prevent friction burns, rests on a dog’s shoulders, while the splayed ends drag over the ground ... First Nations women both built the travois and managed the dogs, sometimes using toy travois to train the puppies. Buffalo meat and firewood were typical travois loads.\n\n\"The dog travois of pre-European times was small, capable of pulling not more than 20 to 30 kg.\" Travel by dog travois was slower in hot weather, which is tiring for dogs. The dog travois can be seen in the paintings of Karl Bodmer.\n\nBy the mid-nineteenth century, the dog travois had given way to the horse travois. When dogs were replaced by horses, the greater pulling power allowed tipis to increase in size and household goods to multiply.\"\n\nAfter horses were introduced to North America, many Plains Indian tribes began to make larger horse-drawn travois. Instead of making specially constructed travois sleds, they would simply cross a pair of tepee poles across the horse's back and attach a burden platform between the poles behind the horse. This served two purposes at once, as the horses could then simultaneously carry the tepee poles and some additional baggage. Horses, of course, could pull much greater weight than dogs. Children often rode in the back of horse travois.\nWhen traveling with a travois, it was traditional for Salish people to leave the tipi poles behind at the camp \"for use by the next tribe or family to camp there.\"\n\nA horse travois can be made with either A-frame or H-frame construction.\nThe travois served the Native Americans in a number of ingenious ways. Before the use of horses, Blackfoot women made a curved fence of dog travois’ tied together, front end up, to hold driven animals enclosed until the hunters could kill them. When the women put up a tipi, they placed an upright horse travois against a tipi pole and used it as a ladder so they could attach the two upper sides of the lodge cover with wooden pins. A travois leaned against a branch of a tree functioned as a simple burial scaffold for a dead Crow baby tied to it. \n\nWhat today is known as the Lewis and Clark Trail-Travois Road, and Montana's Lewis and Clark Pass were areas heavily traveled where travois \"were dragged over the trail, causing deep, parallel tracks to mark the earth,\" which are still visible today. Remains of travois tracks can also be seen at the Knife River Indian Villages National Historic Site.\n\n\n"}
{"id": "37413263", "url": "https://en.wikipedia.org/wiki?curid=37413263", "title": "United States–Mexico Convention relating to the Final Adjustment of Certain Unsettled Claims", "text": "United States–Mexico Convention relating to the Final Adjustment of Certain Unsettled Claims\n\nThe United States–Mexico Convention relating to the Final Adjustment of Certain Unsettled Claims (in Spanish, \"Convencion entre los Estados Unidos de America y Mexico Relativa al Arreglo de Ciertas Reclamaciones Pendientes de Resolucion\") was a bilateral agreement concluded between the US and Mexican governments in Washington, D.C. on November 19, 1941. It annulled most of claims by US and Mexican nationals against the other party's government, except for claims by US oil companies for property nationalized by the Mexican government, which remained to be determined by future agreements. The convention came into effect on April 2, 1942. It was registered in \"United Nations Treaty Series\" on March 26, 1952.\n\nEver since the Mexican revolution of 1910, the US and Mexican governments were at odds over the issue of oil concessions in Mexico. On the eve of the US entry into the Second World War, Washington's position was becoming more difficult, as the Mexican government led a policy of nationalizing the oil industry, and assets of foreign oil companies were confiscated by it. The US government wished to avoid further confrontation with Mexico at a time President Franklin D. Roosevelt was preparing to enter the war against Hitler, and in an attempt to appease the Mexican government while assisting US oil industry, decided to remit all debts by the Mexican government to the US, which were mostly related to agricultural products.\n"}
{"id": "52375044", "url": "https://en.wikipedia.org/wiki?curid=52375044", "title": "VolturnUS (floating wind turbine)", "text": "VolturnUS (floating wind turbine)\n\nThe VolturnUS is a floating concrete structure that supports a wind turbine, designed by University of Maine Advanced Structures and Composites Center and deployed by DeepCwind Consortium in 2013. The VolturnUS can support wind turbines in water depths of 150 feet or more. The DeepCwind Consortium and its partners deployed a 1/8th scale VolturnUS in 2013. Efforts are now underway by Maine Aqua Ventus 1, GP, LLC, to deploy to full-scale VolturnUS structures off the coast of Monhegan Island, Maine, in the UMaine Deepwater Offshore Wind Test Site. This demonstration project, known as New England Aqua Ventus I, will deploy two 6 MW wind turbines by 2020 (planned).\n\nThe University of Maine announced in September 2017 that its VolturnUS design became the first floating offshore wind turbine to meet American Bureau of Shipping requirements for floating offshore wind turbines, demonstrating feasibility of the VolturnUS concept. The design review was conducted against the ABS Guide for Building and Classing Floating Offshore Wind Turbine Installations. \n\nNorth America’s first floating grid-connected wind turbine was lowered into the Penobscot River in Maine on 31 May 2013 by the University of Maine Advanced Structures and Composites Center and its partners. The VolturnUS 1:8 was towed down the Penobscot River where it was deployed for 18 months in Castine, ME, along with a UMaine-developed floating LiDAR.\n\nIn June 2013, the University of Maine made history with its 20 kW Renewegy VP-20 wind turbine with a 9.6m rotor named VolturnUS 1:8, a 65-foot-tall floating turbine prototype that is 1:8th the scale of a 6-megawatt (MW), 450-foot rotor diameter design. VolturnUS 1:8 was the first grid-connected offshore wind turbine deployed in the Americas. The VolturnUS design utilizes a concrete semisubmersible floating hull and a composite materials tower designed to reduce both capital and Operation & Maintenance costs, and to allow local manufacturing throughout the US and the World. The VolturnUS technology is the culmination of collaborative research and development conducted by the University of Maine-led DeepCwind Consortium.\n\nDuring its deployment, it experienced numerous storm events representative of design environmental conditions prescribed by the American Bureau of Shipping (ABS) Guide for Building and Classing Floating Offshore Wind Turbines, 2013. It was taken out of the water in November 2014.\n\nThe patented VolturnUS floating concrete hull technology can support wind turbines in water depths of 45 meters or more, and has the potential to significantly reduce the cost of offshore wind. With 12 independent cost estimates from around the U.S. and the world, it has been found to significantly reduce costs compared to existing floating systems. The design has also received a complete third-party engineering review.\n\nIn June 2016, the UMaine-led New England Aqua Ventus I project won top tier status from the US Department of Energy (DOE) Advanced Technology Demonstration Program for Offshore Wind. This means that the New England Aqua Ventus project is now automatically eligible for an additional $39.9 Million in construction funding from the DOE, as long as the project continues to meet its milestones. The developer asserts that the New England Aqua Ventus I project will likely become the first commercial scale floating wind project in the Americas.\n\nU.S. Senators Susan Collins and Angus King announced in June 2016 that Maine’s New England Aqua Ventus I floating offshore wind demonstration project was selected by the U.S. Department of Energy to participate in the Offshore Wind Advanced Technology Demonstration program. The project is opposed by Senator Dow with Bill LR1613.\n\nNew England Aqua Ventus I is one of two leading projects that are each eligible for up to $39.9 million in additional funding over three years for the construction phase of the demonstration program.\n\n\n\n"}
