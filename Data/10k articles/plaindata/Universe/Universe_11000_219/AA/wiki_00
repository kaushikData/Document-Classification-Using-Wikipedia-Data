{"id": "28379140", "url": "https://en.wikipedia.org/wiki?curid=28379140", "title": "1783 Great Meteor", "text": "1783 Great Meteor\n\nThe 1783 Great Meteor was an unusually bright bolide observed on 18 August 1783, from the British Isles at a time when such phenomena were not well understood. The meteor was the subject of much discussion in the \"Philosophical Transactions of the Royal Society\" and was the subject of a detailed study by Charles Blagden.\n\nThe event occurred between 21:15 and 21:30 on 18 August 1783, a clear, dry night. Analysis of observations has indicated that the meteor entered the Earth's atmosphere over the North Sea, before passing over the east coast of Scotland and England and the English Channel; it finally broke up, after a passage within the atmosphere of around a thousand miles (1610 km), over south-western France or northern Italy.\n\nThere were many witnesses. Perhaps the most prominent was Tiberius Cavallo, an Italian natural philosopher who had happened to be amongst a group of people on the terrace at Windsor Castle at the time the meteor appeared. Cavallo published his account of the phenomenon in v. 74 of the \"Philosophical Transactions\" :\n\nSome flashes of lambent light, much like the \"aurora borealis\", were first observed on the northern part of the heavens, which were soon perceived to proceed from a roundish luminous body, whose apparent diameter equaled half that of the moon, and almost stationary in the same point of the heavens [...] This ball at first appeared of a faint bluish light, perhaps from appearing just kindled, or from its appearing through the haziness; but it gradually increased its light, and soon began to move, at first ascending above the horizon in an oblique direction towards the east. Its course in this direction was very short, perhaps of five or six degrees; after which it directed its course towards the east [...] Its light was prodigious. Every object appeared very distinct; the whole face of the country, in that beautiful prospect before the terrace, being instantly illuminated.\n\nCavallo noted both that the meteor, which was visible for around thirty seconds in total, appeared to split into several smaller bodies immediately following the main mass and that a rumbling noise, \"as it were of thunder at a great distance\", was heard around ten minutes after the meteor appeared, which he speculated \"was the report of the meteor's explosion\". Other accounts, such as those of Alexander Aubert and Richard Lovell Edgeworth, noted red and blue colour tints in the fireball.\n\nSome accounts appeared rather more fanciful; the \"London Magazine\" mentioned a letter by a lieutenant on a British warship which had been positioned north of Ireland \"who relates he saw the same meteor moving along the north-east quarter [...] but he adds something singular enough, namely, that a little time afterwards, he saw it moving back again, the contrary way to which it came\". The author added that \"several other observations of this meteor have come into my hands, but they are so inconsistent with these already related, as well as with one another, that I forebear to mention them\".\n\nGilbert White, writing in 1787, was to remember the \"amazing and portentous\" summer of 1783 as \"full of horrible phaenomena [...] alarming meteors and tremendous thunder-storms that affrighted and distressed the different counties of this kingdom\".\n\nOne of Cavallo's five companions on the terrace was the artist Thomas Sandby, who in collaboration with his brother Paul based a now well-known engraving on the event. A print of this engraving is in the collection of the Hunterian Museum and Art Gallery at Glasgow University. A second engraving was produced by a schoolmaster, Henry Robinson, who observed the meteor from the village of Winthorpe, Nottinghamshire. Further engravings, based on the drawings of the authors and presented in a fold-out form, were included with articles by Cavallo and Nathaniel Pigott in the \"Philosophical Transactions\".\n\nA painting traditionally thought to be of the 1759 apparition of Halley's Comet and attributed to the \"English Canaletto\", Samuel Scott, has in more recent years been interpreted as depicting a large fireball meteor given its generally uncometary appearance. Further work by Jay Pasachoff and Roberta Olson has suggested that the painting is not in fact by Scott, and that it depicts the third stage of the 1783 fireball, viewed over the Thames.\n\nIt has been speculated that the Hambleton Pallasite, a rare type of meteorite found in 2005 in Hambleton, North Yorkshire, may be related to the 1783 Great Meteor, based on the latter's track, and on weathering on the pallasite's surface. In support of this, in 2008 the terrestrial age of the Hambleton meteorite was determined to be 225 years (+/-).\n\n\n"}
{"id": "4941810", "url": "https://en.wikipedia.org/wiki?curid=4941810", "title": "Alexey Limanzo", "text": "Alexey Limanzo\n\nAlexey Limanzo, President of the Association of Indigenous Peoples of North Sakhalin Region, is the chairman of the council of the indigenous people plenipotentiary of Sakhalin Island \n\n\n"}
{"id": "20280063", "url": "https://en.wikipedia.org/wiki?curid=20280063", "title": "Algaenan", "text": "Algaenan\n\nAlgaenan is the resistant biopolymer in the cell walls of unrelated groups of green algae, and facilitates their preservation in the fossil record.\n"}
{"id": "1159331", "url": "https://en.wikipedia.org/wiki?curid=1159331", "title": "Aluminium nitride", "text": "Aluminium nitride\n\nAluminium nitride (AlN) is a nitride of aluminium. Its wurtzite phase (w-AlN) is a wide band gap (6.01-6.05 eV at room temperature) semiconductor material, giving it potential application for deep ultraviolet optoelectronics.\n\nAlN was first synthesized in 1877, but it was not until the middle of the 1980s that its potential for application in microelectronics was realized due to its relatively high thermal conductivity for an electrically insulating ceramic (70–210 W·m·K for polycrystalline material, and as high as 285 W·m·K for single crystals).\n\nAluminium nitride is stable at high temperatures in inert atmospheres and melts about 2200 °C. In a vacuum, AlN decomposes at ~1800 °C. In the air, surface oxidation occurs above 700 °C, and even at room temperature, surface oxide layers of 5-10 nm have been detected. This oxide layer protects the material up to 1370 °C. Above this temperature bulk oxidation occurs. Aluminium nitride is stable in hydrogen and carbon dioxide atmospheres up to 980 °C.\n\nThe material dissolves slowly in mineral acids through grain boundary attack, and in strong alkalies through attack on the aluminium nitride grains. The material hydrolyzes slowly in water. Aluminium nitride is resistant to attack from most molten salts, including chlorides and cryolite. \n\nAluminum nitride can be patterned with a Cl-based reactive ion etch. \n\nAlN is synthesized by the carbothermal reduction of aluminium oxide in the presence of gaseous nitrogen or ammonia or by direct nitridation of aluminium. The use of sintering aids, such as YO or CaO, and hot pressing is required to produce a dense technical grade material.\n\nEpitaxially grown thin film crystalline aluminium nitride is used for surface acoustic wave sensors (SAWs) deposited on silicon wafers because of AlN's piezoelectric properties. One application is an RF filter which is widely used in mobile phones, which is called a thin film bulk acoustic resonator (FBAR). This is a MEMS device that uses aluminium nitride sandwiched between two metal layers.\n\nAluminium nitride is also used to build piezoelectric micromachined ultrasound transducers, which emit and receive ultrasound and which can be used for in-air rangefinding over distances of up to a meter.\n\nMetallization methods are available to allow AlN to be used in electronics applications similar to those of alumina and beryllium oxide. AlN nanotubes as inorganic quasi-one-dimensional nanotubes, which are isoelectronic with carbon nanotubes, have been suggested as chemical sensors for toxic gases.\n\nCurrently there is much research into developing light-emitting diodes to operate in the ultraviolet using gallium nitride based semiconductors and, using the alloy aluminium gallium nitride, wavelengths as short as 250 nm have been achieved. In May 2006, an inefficient AlN LED emission at 210 nm was reported.\n\nThere are also multiple research efforts in industry and academia to use aluminum nitride in piezoelectric MEMS applications. These include resonators, gyroscopes and microphones.\n\nAmong the applications of AlN are\n\n"}
{"id": "18788528", "url": "https://en.wikipedia.org/wiki?curid=18788528", "title": "Amberlite", "text": "Amberlite\n\nAmberlite is the tradename of a range of ion-exchange resins.\n"}
{"id": "1302124", "url": "https://en.wikipedia.org/wiki?curid=1302124", "title": "Baseboard", "text": "Baseboard\n\nIn architecture, a baseboard (also called skirting board, skirting, mopboard, floor molding, or base molding) is usually wooden or vinyl board covering the lowest part of an interior wall. Its purpose is to cover the joint between the wall surface and the floor. It covers the uneven edge of flooring next to the wall; protects the wall from kicks, abrasion, and furniture; and can serve as a decorative molding.\n\nAt its simplest, baseboard consists of a simple plank nailed, screwed or glued to the wall; however, particularly in older houses, it can be made up of a number of mouldings for decoration. A baseboard differs from a wainscot; a wainscot typically covers from the floor to around 1-1.5m high (waist or chest height), whereas a baseboard is typically under 0.2m high (ankle height).\n\nPlastic baseboard comes in various plastic compounds, the most common of which is UPVC. It is usually available in white or a flexible version in several colors and is usually glued to the wall. Vinyl baseboard is glued with adhesive and can be difficult to remove or to replace. It has a long lifespan, which can mean lower maintenance.\n\nWooden baseboard can be available in untreated, lacquered or prepainted versions. Prepainted baseboards can be made from a single piece or finger jointed wood, often softwoods, while hardwoods are either lacquered, or raw for staining and made from a single piece of wood.\n\nRadiators are sometimes installed inside or in front of baseboards (baseboard radiators). These radiators rely on hot water as their heat source. Electric heating is also used in this manner.\n\nBaseboards / skirting boards generally have typical variations depending on the country. For example, in China the baseboards are usually very low in height, are made of plastic or redwood, and have a very simple or unprofiled design. In contrast, in the UK, where they are normally referred to as skirting board not baseboard, there are a vast number of profiles available. These profiles are frequently named after the period when they were developed, such as Victorian or Edwardian.\n\n"}
{"id": "6259941", "url": "https://en.wikipedia.org/wiki?curid=6259941", "title": "Biomaterial", "text": "Biomaterial\n\nA biomaterial is any substance that has been engineered to interact with biological systems for a medical purpose - either a therapeutic (treat, augment, repair or replace a tissue function of the body) or a diagnostic one. As a science, biomaterials is about fifty years old. The study of biomaterials is called biomaterials science or biomaterials engineering. It has experienced steady and strong growth over its history, with many companies investing large amounts of money into the development of new products. Biomaterials science encompasses elements of medicine, biology, chemistry, tissue engineering and materials science.\n\nNote that a biomaterial is different from a biological material, such as bone, that is produced by a biological system. Additionally, care should be exercised in defining a biomaterial as biocompatible, since it is application-specific. A biomaterial that is biocompatible or suitable for one application may not be biocompatible in another.\n\nBiomaterials can be derived either from nature or synthesized in the laboratory using a variety of chemical approaches utilizing metallic components, polymers, ceramics or composite materials. They are often used and/or adapted for a medical application, and thus comprises whole or part of a living structure or biomedical device which performs, augments, or replaces a natural function. Such functions may be relatively passive, like being used for a heart valve, or may be bioactive with a more interactive functionality such as hydroxy-apatite coated hip implants. Biomaterials are also used every day in dental applications, surgery, and drug delivery. For example, a construct with impregnated pharmaceutical products can be placed into the body, which permits the prolonged release of a drug over an extended period of time. A biomaterial may also be an autograft, allograft or xenograft used as a transplant material.\n\nThe ability of an engineered biomaterial to induce a physiological response that is supportive of the biomaterial's function and performance is known as bioactivity. Most commonly, in bioactive glasses and bioactive ceramics this term refers to the ability of implanted materials to bond well with surrounding tissue in either osseoconductive or osseoproductive roles . Bone implant materials are often designed to promote bone growth while dissolving into surrounding body fluid . Thus for many biomaterials good biocompatibility along with good strength and dissolution rates are desirable. Commonly, bioactivity of biomateirals is gauged by the surface biomineralisation in which a native layer of hydroxyapatite is formed at the surface.\n\nSelf-assembly is the most common term in use in the modern scientific community to describe the spontaneous aggregation of particles (atoms, molecules, colloids, micelles, etc.) without the influence of any external forces. Large groups of such particles are known to assemble themselves into thermodynamically stable, structurally well-defined arrays, quite reminiscent of one of the 7 crystal systems found in metallurgy and mineralogy (e.g. face-centered cubic, body-centered cubic, etc.). The fundamental difference in equilibrium structure is in the spatial scale of the unit cell (or lattice parameter) in each particular case.\n\nMolecular self-assembly is found widely in biological systems and provides the basis of a wide variety of complex biological structures. This includes an emerging class of mechanically superior biomaterials based on microstructural features and designs found in nature. Thus, self-assembly is also emerging as a new strategy in chemical synthesis and nanotechnology. Molecular crystals, liquid crystals, colloids, micelles, emulsions, phase-separated polymers, thin films and self-assembled monolayers all represent examples of the types of highly ordered structures which are obtained using these techniques. The distinguishing feature of these methods is self-organization.\n\nNearly all materials could be seen as hierarchically structured, especially since the changes in spatial scale bring about different mechanisms of deformation and damage. However, in biological materials this hierarchical organization is inherent to the microstructure. One of the first examples of this, in the history of structural biology, is the early X-ray scattering work on the hierarchical structure of hair and wool by Astbury and Woods. In bone, for example, collagen is the building block of the organic matrix — a triple helix with diameter of 1.5 nm. These tropocollagen molecules are intercalated with the mineral phase (hydroxyapatite, a calcium phosphate) forming fibrils that curl into helicoids of alternating directions. These \"osteons\" are the basic building blocks of bones, with the volume fraction distribution between organic and mineral phase being about 60/40.\n\nIn another level of complexity, the hydroxyapatite crystals are mineral platelets that have a diameter of approximately 70–100 nm and thickness of 1 nm. They originally nucleate at the gaps between collagen fibrils.\n\nSimilarly, the hierarchy of abalone shell begins at the nanolevel, with an organic layer having a thickness of 20–30 nm. This layer proceeds with single crystals of aragonite (a polymorph of CaCO) consisting of \"bricks\" with dimensions of 0.5 and finishing with layers approximately 0.3 mm (mesostructure).\n\nCrabs are arthropods whose carapace is made of a mineralized hard component (which exhibits brittle fracture) and a softer organic component composed primarily of chitin. The brittle component is arranged in a helical pattern. Each of these mineral ‘rods’ (1 μm diameter) contains chitin–protein fibrils with approximately 60 nm diameter. These fibrils are made of 3 nm diameter canals which link the interior and exterior of the shell.\n\nBiomaterials are used in:\n\nBiomaterials must be compatible with the body, and there are often issues of biocompatibility which must be resolved before a product can be placed on the market and used in a clinical setting. Because of this, biomaterials are usually subjected to the same requirements as those undergone by new drug therapies.\n\nAll manufacturing companies are also required to ensure traceability of all of their products so that if a defective product is discovered, others in the same batch may be traced.\n\nIn the United States, 45% of the 250,000 valve replacement procedures performed annually involve a mechanical valve implant. The most widely used valve is a bileaflet disc heart valve, or St. Jude valve. The mechanics involve two semicircular discs moving back and forth, with both allowing the flow of blood as well as the ability to form a seal against backflow. The valve is coated with pyrolytic carbon, and secured to the surrounding tissue with a mesh of woven fabric called Dacron (du Pont's trade name for polyethylene terephthalate). The mesh allows for the body's tissue to grow while incorporating the valve.\n\nMost of the time, ‘artificial’ tissue is grown from the patient’s own cells. However, when the damage is so extreme that it is impossible to use the patient's own cells, artificial tissue cells are grown. The difficulty is in finding a scaffold that the cells can grow and organize on. The characteristics of the scaffold must be that it is biocompatible, cells can adhere to the scaffold, mechanically strong and biodegradable. One successful scaffold is a copolymer of lactic acid and glycolic acid.\n\nBiocompatibility is related to the behavior of biomaterials in various environments under various chemical and physical conditions. The term may refer to specific properties of a material without specifying where or how the material is to be used. For example, a material may elicit little or no immune response in a given organism, and may or may not able to integrate with a particular cell type or tissue. The ambiguity of the term reflects the ongoing development of insights into how biomaterials interact with the human body and eventually how those interactions determine the clinical success of a medical device (such as pacemaker or hip replacement). Modern medical devices and prostheses are often made of more than one material—so it might not always be sufficient to talk about the biocompatibility of a specific material.\n\nBiopolymers are polymers produced by living organisms. Cellulose and starch, proteins and peptides, and DNA and RNA are all examples of biopolymers, in which the monomeric units, respectively, are sugars, amino acids, and nucleotides.\nCellulose is both the most common biopolymer and the most common organic compound on Earth. About 33% of all plant matter is cellulose.\n\n\n"}
{"id": "752897", "url": "https://en.wikipedia.org/wiki?curid=752897", "title": "Buffer gas", "text": "Buffer gas\n\nA buffer gas is an inert or nonflammable gas. In the Earth's atmosphere, nitrogen acts as a buffer gas. A buffer gas adds pressure to a system and controls the speed of combustion with any oxygen present. Any inert gas such as helium, neon, or argon will serve as a buffer gas.\n\nBuffer gases are commonly used in many applications from high pressure discharge lamps to reduce line width of microwave transitions in alkali atoms. A buffer gas usually consists of atomically inert gases such as helium, argon, and nitrogen which are the primary gases used. Krypton, neon, and xenon are also used, primarily for lighting. In most scenarios, buffer gases are used in conjunction with other molecules for the main purpose of causing collisions with the other co-existing molecules.\n\nIn fluorescent lamps, mercury is used as the primary ion from which light is emitted. Krypton is the buffer gas used in conjunction with the mercury which is used to moderate the momentum of collisions of mercury ions in order to reduce the damage done to the electrodes in the fluorescent lamp. Generally speaking, the longest lasting lamps are those with the heaviest noble gases as buffer gases.\nBuffer gas loading techniques have been developed for use in cooling paramagnetic atoms and molecules at ultra-cold temperatures. The buffer gas most commonly used in this sort of application is helium. Buffer gas cooling can be used on just about any molecule, as long as the molecule is capable of surviving multiple collisions with low energy helium atoms, which most molecules are capable of doing. Buffer gas cooling is allowing the molecules of interest to be cooled through elastic collisions with a cold buffer gas inside a chamber. If there are enough collisions between the buffer gas and the other molecules of interest before the molecules hit the walls of the chamber and are gone, the buffer gas will sufficiently cool the atoms. Of the two isotopes of helium (He and He), the rarer He is sometimes used over He as it provides significantly higher vapor pressures and buffer gas density at sub-kelvin temperatures.\nBuffer gases are also commonly used in compressors used in power plants for supplying gas to gas turbines. The buffer gas fills the spaces between seals in the compressor. This space is usually about 2 micrometres wide. The gas must be completely dry and free of any contaminants. Contaminants can potentially lodge in the space between the seal and cause metal to metal contact in the compressor, leading to compressor failure (above right). In this case the buffer gas acts in a way much like oil does in an automotive engine’s bearings.\n\n"}
{"id": "12661539", "url": "https://en.wikipedia.org/wiki?curid=12661539", "title": "Carr index", "text": "Carr index\n\nThe Carr index (also: Carr's index or Carr's Compressibility Index) is an indication of the compressibility of a powder. It is named after the scientist Ralph J. Carr, Jr.\nThe Carr index is calculated by the formula formula_1, where formula_2 is the volume that a given mass of powder would occupy if let settled freely, and formula_3 is the volume of the same mass of powder would occupy after \"tapping down\". It can also be expressed as formula_4, where formula_5 is the freely settled bulk density of the powder, and formula_6 is the tapped bulk density of the powder.\n\nThe Carr index is frequently used in pharmaceutics as an indication of the flowability of a powder. In a free-flowing powder, the bulk density and tapped density would be close in value, therefore, the Carr index would be small. On the other hand, in a poor-flowing powder where there are greater interparticle interactions, the difference between the bulk and tapped density observed would be greater, therefore, the Carr index would be larger. A Carr index greater than 25 is considered to be an indication of poor flowability, and below 15, of good flowability.\n\nAnother way to measure the flow of a powder is the Hausner ratio, which can be expressed as formula_7.\n\nBoth the Hausner ratio and the Carr index are sometimes criticized, despite their relationships to flowability being established empirically, as not having a strong theoretical basis. Use of these measures persists, however, because the equipment required to perform the analysis is relatively cheap and the technique is easy to learn.\n\nCarrs index formula to change\n\n(Tapped density - Bulk density) / tapped density X 100\n"}
{"id": "14646706", "url": "https://en.wikipedia.org/wiki?curid=14646706", "title": "Charge ordering", "text": "Charge ordering\n\nCharge ordering (CO) is a (first- or second-order) phase transition occurring mostly in strongly correlated materials such as transition metal oxides or organic conductors. Due to the strong interaction between electrons, charges are localized on different sites leading to a disproportionation and an ordered superlattice. It appears in different patterns ranging from vertical to horizontal stripes to a checkerboard–like pattern \n, and it is not limited to the two-dimensional case. The charge order transition is accompanied by symmetry breaking and may lead to ferroelectricity. It is often found in close proximity to superconductivity and colossal magnetoresistance.\nBending and breaking of stripes in a charge ordered manganite\n\nThis long range order phenomena was first discovered in magnetite (FeO) by Verwey in 1939.\nHe observed an increase of the electrical resistivity by two orders of magnitude at T=120K, suggesting a phase transition which is now well known as the Verwey transition. He was the first to propose the idea of an ordering process in this context. The charge ordered structure of magnetite was solved in 2011 by a group led by Paul Attfield with the results published in \"Nature\". Periodic lattice distortions associated with charge order were later mapped in the manganite lattice to reveal striped domains containing topological disorder.\n\nThe extended one-dimensional Hubbard model delivers a good description of the charge order transition with the on-site and nearest neighbor Coulomb repulsion U and V. It emerged that V is a crucial parameter and important for developing the charge order state. Further model calculations try to take the temperature and an interchain interaction into account. \nThe extended Hubbard model for a single chain including inter-site and on-site interaction V and U as well as the parameter formula_1 for a small dimerization which can be typically found in the (TMTTF)X compounds is presented as follows:\n\nformula_2\nwhere t describes the transfer integral or the kinetic energy of the electron and formula_3 and formula_4 are the creation and annihilation operator, respectively, for an electron with the spin formula_5 at the formula_6th or formula_7th site. formula_8 denotes the density operator. For non-dimerized systems, formula_1 can be set to zero Normally, the on-site Coulomb repulsion U stays unchanged only t and V can vary with pressure.\n\nOrganic conductors consist of donor and acceptor molecules building separated planar sheets or columns. The energy difference in the ionization energy acceptor and the electron affinity of the donor leads to a charge transfer and consequently to free carriers whose number is normally fixed. The carriers are delocalized throughout the crystal due to the overlap of the molecular orbitals being also reasonable for the high anisotropic conductivity. That is why it will be distinct between different dimensional organic conductors. They possess a huge variety of ground states, for instance, charge ordering, spin-Peierls, spin-density wave, antiferromagnetic state, superconductivity, charge-density wave to name only some of them.\n\nThe model system of one-dimensional conductors is the Bechgaard-Fabre salts family, (TMTTF)X and (TMTSF)X, where in the latter one sulfur is substituted by selenium leading to a more metallic behavior over a wide temperature range and exhibiting no charge order. While the TMTTF compounds depending on the counterions X show the conductivity of a semiconductor at room temperature and are expected to be more one-dimensional than (TMTSF)X.\nThe transition temperature T for the TMTTF subfamily was registered over two order of magnitudes for the centrosymmetric anions X = Br, PF, AsF, SbF and the non-centrosymmetric anions X= BF and ReO.\nIn the middle of the eighties, a new \"structureless transition\" was discovered by Coulon et al. conducting transport and thermopower measurements. They observed a suddenly rise of the resistivity and the thermopower at T while x-ray measurements showed no evidence for a change in the crystal symmetry or a formation of a superstructure. The transition was later confirmed by C-NMR and dielectric measurements.\n\nDifferent measurements under pressure reveal a decrease of the transition temperature T by increasing the pressure. According to the phase diagram of that family, an increasing pressure applied to the TMTTF compounds can be understood as a shift from the semiconducting state (at room temperature) to a higher dimensional and metallic state as you can find for TMTSF compounds without a charge order state. \nA dimensional crossover can be induced not only by applying pressure, but also be substituting the donor molecules by other ones. From a historical point of view, the main aim was to synthesize an organic superconductor with a high T. The key to reach that aim was to increase the orbital overlap in two dimension. With the BEDT-TTF and its huge π-electron system, a new family of quasi-two-dimensional organic conductors were created exhibiting also a great variety of the phase diagram and crystal structure arrangements.\n\nAt the turn of the 20th century, first NMR measurements on the θ-(BEDT-TTF)RbZn(SCN) compound uncovered the known metal to insulator transition at T= 195 K as an charge order transition. \nThe most prominent transition metal oxide revealing a CO transition is the magnetite FeO being a mixed-valence oxide where the iron atoms have a statistical distribution of Fe and Fe above the transition temperature. Below 122 K, the combination of 2+ and 3+ species arrange themselves in a regular pattern, whereas above that transition temperature (also referred to as the Verwey temperature in this case) the thermal energy is large enough to destroy the order.\n\n"}
{"id": "43466266", "url": "https://en.wikipedia.org/wiki?curid=43466266", "title": "Christmas Hurricane of 1902", "text": "Christmas Hurricane of 1902\n\nThe Christmas Hurricane of 1902 (, ) was a strong European windstorm which struck Denmark and Sweden on 25 December 1902.\n\nIn Sweden, it hit Scania, parts of southern Kronoberg County and the west coast of Sweden. Circa 50 fishermen were killed, while very few people on land were killed. Material damages, were measured in millions of Swedish crowns as of 1902, were reported. The storm destroyed, among other things, open-air baths in Varberg, Mölle and Malmö (one in Ribersborgsstranden - the Ribersborgs open-air bath, and one in Sibbarp), and the Örgryte Church tower spire fell off.\n"}
{"id": "20159769", "url": "https://en.wikipedia.org/wiki?curid=20159769", "title": "Droop speed control", "text": "Droop speed control\n\nIn electrical power generation, droop speed control is a speed control mode of a prime mover driving a synchronous generator connected to an electrical grid. This mode allows synchronous generators to run in parallel, so that loads are shared among generators in proportion to their power rating.\n\nThe frequency of a synchronous generator is given by\n\nwhere\n\nThe frequency (F) of a synchronous generator is directly proportional to its speed (N). When multiple synchronous generators are connected in parallel to electrical grid, the frequency is fixed by the grid, since individual power output of each generator will be small compared to the load on a large grid. Synchronous generators connected to the grid run at various speeds but they all run at the same frequency because they differ in the number of poles (P).\n\nA speed reference as percentage of actual speed is set in this mode. As the generator is loaded from no load to full load, the actual speed of the prime mover tends to decrease. In order to increase the power output in this mode, the prime mover speed reference is increased. Because the actual prime mover speed is fixed by the grid, this difference in speed reference and actual speed of the prime mover is used to increase the flow of working fluid (fuel, steam, etc.) to the prime mover, and hence power output is increased. The reverse will be true for decreasing power output. The prime mover speed reference is always greater than actual speed of the prime mover. The actual speed of the prime mover is allowed to \"droop\" or decrease with respect to the reference, and so the name.\n\nFor example, if the turbine is rated at 3000 rpm, and the machine speed reduces from 3000 rpm to 2880 rpm when it is loaded from no load to base load, then the droop % is given by\n\nIn this case, speed reference will be 104% and actual speed will be 100%. For every 1% change in the turbine speed reference, the power output of the turbine will change by 25% of rated for a unit with a 4% droop setting. Droop is therefore expressed as the percentage change in (design) speed required for 100% governor action.\n\nAs frequency is fixed on the grid, and so actual turbine speed is also fixed, the increase in turbine speed reference will increase the error between reference and actual speed. As the difference increases, fuel flow is increased to increase power output, and vice versa. This type of control is referred to as \"straight proportional\" control. If the entire grid tends to be overloaded, the grid frequency and hence actual speed of generator will decrease. All units will see an increase in the speed error, and so increase fuel flow to their prime movers and power output. In this way droop speed control mode also helps to hold a stable grid frequency. The amount of power produced is strictly proportional to the error between the actual turbine speed and speed reference.\n\nIt can be mathematically shown that if all machines synchronized to a system have the same droop speed control, they will share load proportionate to the machine ratings.\n\nFor example, how fuel flow is increased or decreased in a GE-design heavy duty gas turbine can be given by the formula,\n\nFSRN = (FSKRN2 * (TNR-TNH)) + FSKRN1\n\nWhere,\n\nFSRN = Fuel Stroke Reference (Fuel supplied to Gas Turbine) for droop mode\n\nTNR = Turbine Speed Reference\n\nTNH = Actual Turbine Speed\n\nFSKRN2 = Constant\n\nFSKRN1 = Constant\n\nAs frequency is fixed on the grid, and so actual turbine speed is also fixed, the increase in turbine speed reference will increase the error between reference and actual speed. As the difference increases, fuel flow is increased to increase power output, and vice versa. This type of control is referred to as \"straight proportional\" control. If the entire grid tends to be overloaded, the grid frequency and hence actual speed of generator will decrease. All units will see an increase in the speed error, and so increase fuel flow to their prime movers and power output. In this way droop speed control mode also helps to hold a stable grid frequency. The amount of power produced is strictly proportional to the error between the actual turbine speed and speed reference.\n\nThe above formula is nothing but the equation of a straight line (y = mx + b).\n\nMultiple synchronous generators having equal % droop setting connected to a grid will share the change in grid load in proportion of their base load.\n\nFor stable operation of the electrical grid of North America, power plants typically operate with a four or five percent speed droop. By definition, with 5% droop the full-load speed is 100% and the no-load speed is 105%.\n\nNormally the changes in speed are minor due to inertia of the total rotating mass of all generators and motors running on the grid. Adjustments in power output for a particular primer mover and generator combination are made by slowly raising the droop curve by increasing the spring pressure on a centrifugal governor or by an engine control unit adjustment, or the analogous operation for an electronic speed governor. All units to be connected to a grid should have the same droop setting, so that all plants respond in the same way to the instantaneous changes in frequency without depending on outside communication.\n\nNext to the inertia given by the parallel operation of synchronous generators, the frequency speed droop is the primary instantaneous parameter in control of an individual power plant's power output (kW).\n\n\n"}
{"id": "740501", "url": "https://en.wikipedia.org/wiki?curid=740501", "title": "Elastin", "text": "Elastin\n\nElastin is a highly elastic protein in connective tissue and allows many tissues in the body to resume their shape after stretching or contracting. Elastin helps skin to return to its original position when it is poked or pinched. Elastin is also an important load-bearing tissue in the bodies of vertebrates and used in places where mechanical energy is required to be stored. In humans, elastin is encoded by the \"ELN\" gene.\n\nThe \"ELN\" gene encodes a protein that is one of the two components of elastic fibers. The encoded protein is rich in hydrophobic amino acids such as glycine and proline, which form mobile hydrophobic regions bounded by crosslinks between lysine residues. Multiple transcript variants encoding different isoforms have been found for this gene. Elastin's soluble precursor is tropoelastin. The characterization of disorder is consistent with an entropy-driven mechanism of elastic recoil. It is concluded that conformational disorder is a constitutive feature of elastin structure and function.\n\nDeletions and mutations in this gene are associated with supravalvular aortic stenosis (SVAS) and the autosomal dominant cutis laxa. Other associated defects in elastin include Marfan syndrome, emphysema caused by α-antitrypsin deficiency, atherosclerosis, Buschke-Ollendorff syndrome, Menkes syndrome, pseudoxanthoma elasticum, and Williams syndrome.\n\nIn the body, elastin is usually associated with other proteins in connective tissues. Elastic fiber in the body is a mixture of amorphous elastin and fibrous fibrillin. Both components are primarily made of smaller amino acids such as glycine, valine, alanine, and proline. The total elastin ranges from 58 to 75% of the weight of the dry defatted artery in normal canine arteries. Comparison between fresh and digested tissues shows that, at 35% strain, a minimum of 48% of the arterial load is carried by elastin, and a minimum of 43% of the change in stiffness of arterial tissue is due to the change in elastin stiffness.\n\nElastin serves an important function in arteries as a medium for pressure wave propagation to help blood flow and is particularly abundant in large elastic blood vessels such as the aorta. Elastin is also very important in the lungs, elastic ligaments, elastic cartilage, the skin, and the bladder. It is present in all vertebrates above the jawless fish.\n\nElastin is made by linking together many small soluble precursor tropoelastin protein molecules (50-70 kDa), to make the final massive insoluble, durable complex. The unlinked tropoelastin molecules are not normally available in the cell, since they become crosslinked into elastin fibres immediately after their synthesis by the cell and during their export into the extracellular matrix.\n\nEach tropoelastin consists of a string of 36 small domains, each weighing about 2 kDa in a random coil conformation. The protein consists of alternating hydrophobic and hydrophilic domains, which are encoded by separate exons, so that the domain structure of tropoelastin reflects the exon organization of the gene. The hydrophilic domains contain Lys-Ala (KA) and Lys-Pro (KP) motifs that are involved in crosslinking during the formation of mature elastin. In the KA domains, lysine residues occur as pairs or triplets separated by two or three alanine residues (e.g. AAAKAAKAA) whereas in KP domains the lysine residues are separated mainly by proline residues (e.g. KPLKP).\n\nTropoelastin aggregates at physiological temperature due to interactions between hydrophobic domains in a process called coacervation. This process is reversible and thermodynamically controlled and does not require protein cleavage. The coacervate is made insoluble by irreversible crosslinking.\n\nTo make mature elastin fibres, the tropoelastin molecules are cross-linked via their lysine residues with desmosine and isodesmosine cross-linking molecules. The enzyme that performs the crosslinking is lysyl oxidase, using an \"in vivo\" Chichibabin pyridine synthesis reaction.\n\nIn mammals, the genome only contains one gene for tropoelastin, called \"ELN\". The human \"ELN\" gene is a 45 kb segment on chromosome 7, and has 34 exons interrupted by almost 700 introns, with the first exon being a signal peptide assigning its extracellular localization. The large number of introns suggests that genetic recombination may contribute to the instability of the gene, leading to diseases such as SVAS. The expression of tropoelastin mRNA is highly regulated under at least eight different transcription start sites.\n\nTissue specific variants of elastin are produced by alternative splicing of the tropoelastin gene. There are at least 11 known human tropoelastin isoforms. these isoforms are under developmental regulation, however there are minimal differences among tissues at the same developmental stage.\n\n\n"}
{"id": "31773280", "url": "https://en.wikipedia.org/wiki?curid=31773280", "title": "Environmental product declaration", "text": "Environmental product declaration\n\nIn life cycle assessment, an Environmental Product Declaration (EPD) is a standardized way of quantifying the environmental impact of a product or system. Declarations include information on the environmental impact of raw material acquisition, energy use and efficiency, content of materials and chemical substances, emissions to air, soil and water and waste generation. Product and company information is also included.\n\nAn EPD is created and verified in accordance with the International Standard ISO 14025, developed by the International Organization for Standardization (ISO). EPDs are based on a Life-cycle assessment according to ISO 14040 and ISO 14044.\n\nWhile EPDs do educate consumers about the product and its environmental impact, consumers should know that it is for disclosure purposes only, and does not mean that the product meets any environmental performance standards.\n\nIn Europe, the European Committee for Standardization has published EN 15804, a common \"Product Category Rules\" (PCR) for EPD development in the construction sector. Other complementary standards, for example for environmental building assessment (EN 15978) were also published by this Technical Committee.\n\nIn order to enhance harmonization, the main Programme Operators for EPD verification in the construction sector has created the Association ECO Platform, with members from different European countries. \n\nThe Programme Operators approved to issue EPD with the ECO Platform verified logo are:\n\n\nECO Platform also include Associations, for example:\n\nSome of these Programme Operators are under bilateral mutual recognition agreements like IBU (Germany), EPD International (Sweden) and AENOR GlobalEPD (Spain).\n\n\n"}
{"id": "7209279", "url": "https://en.wikipedia.org/wiki?curid=7209279", "title": "Glass fiber reinforced concrete", "text": "Glass fiber reinforced concrete\n\nGlass fiber reinforced concrete or GFRC is a type of fiber-reinforced concrete. The product is also known as glassfibre reinforced concrete or GRC in British English. Glass fiber concretes are mainly used in exterior building façade panels and as architectural precast concrete. Somewhat similar materials are fiber cement siding and cement boards.\n\nGlass fiber-reinforced concrete consists of high-strength, alkali-resistant glass fiber embedded in a concrete matrix. In this form, both fibers and matrix retain their physical and chemical identities, while offering a synergistic combination of properties that cannot be achieved with either of the components acting alone. In general, fibers are the principal load-carrying members, while the surrounding matrix keeps them in the desired locations and orientation, acting as a load transfer medium between the fibers and protecting them from environmental damage. The fibers provide reinforcement for the matrix and other useful functions in fiber-reinforced composite materials. Glass fibers can be incorporated into a matrix either in continuous or discontinuous (chopped) lengths.\n\nDurability was poor with the original type of glass fibers since the alkalinity of cement reacts with its silica. In the 1970s alkali-resistant glass fibers were commercialized. Alkali resistance is achieved by adding zirconia to the glass. The higher the zirconia content the better the resistance to alkali attack. The best fibers have zirconia contents of 19% or higher.\n\nA widely used application for fiber-reinforced concrete is structural laminate, obtained by adhering and consolidating thin layers of fibers and matrix into the desired thickness. The fiber orientation in each layer as well as the stacking sequence of various layers can be controlled to generate a wide range of physical and mechanical properties for the composite laminate. GFRC cast without steel framing is commonly used for purely decorative applications such as window trims, decorative columns, exterior friezes, or limestone-like wall panels.\n\nThe design of glass-fiber-reinforced concrete panels uses a knowledge of its basic properties under tensile, compressive, bending and shear forces, coupled with estimates of behavior under secondary loading effects such as creep, thermal response and moisture movement.\n\nThere are a number of differences between structural metal and fiber-reinforced composites. For example, metals in general exhibit yielding and plastic deformation, whereas most fiber-reinforced composites are elastic in their tensile stress-strain characteristics. However, the dissimilar nature of these materials provides mechanisms for high-energy absorption on a microscopic scale comparable to the yielding process. Depending on the type and severity of external loads, a composite laminate may exhibit gradual deterioration in properties but usually does not fail in a catastrophic manner. Mechanisms of damage development and growth in metal and composite structure are also quite different. Other important characteristics of many fiber-reinforced composites are their non-corroding behavior, high damping capacity and low coefficients of thermal expansion.\n\nGlass-fiber-reinforced concrete architectural panels have the general appearance of pre-cast concrete panels, but differ in several significant ways. For example, the GFRC panels, on average, weigh substantially less than pre-cast concrete panels due to their reduced thickness. Their low weight decreases loads superimposed on the building’s structural components making construction of the building frame more economical.\n\nA sandwich panel is a composite of three or more materials bonded together to form a structural panel. It takes advantage of the shear strength of a low density core material and the high compressive and tensile strengths of the GFRC facing to obtain high strength-to-weight ratios.\nThe theory of sandwich panels and functions of the individual components may be described by making an analogy to an I-beam. The core in a sandwich panel is comparable to the web of an I-beam, which supports the flanges and allows them to act as a unit. The web of the I-beam and the core of the sandwich panels carry the beam shear stresses. The core in a sandwich panel differs from the web of an I-beam in that it maintains a continuous support for the facings, allowing the facings to be worked up to or above their yield strength without crimping or buckling. Obviously, the bonds between the core and facings must be capable of transmitting shear loads between these two components, thus making the entire structure an integral unit.\n\nThe load-carrying capacity of a sandwich panel can be increased dramatically by introducing light steel framing. Light steel stud framing is similar to conventional steel stud framing for walls, except that the frame is encased in a concrete product. Here, the sides of the steel frame are covered with two or more layers of GFRC, depending on the type and magnitude of external loads. The strong and rigid GFRC provides full lateral support on both sides of the studs, preventing them from twisting and buckling laterally. The resulting panel is lightweight in comparison with traditionally reinforced concrete, yet is strong and durable and can be easily handled.\n\n"}
{"id": "1437020", "url": "https://en.wikipedia.org/wiki?curid=1437020", "title": "Guiding center", "text": "Guiding center\n\nIn physics, the motion of an electrically charged particle such as an electron or ion in a plasma in a magnetic field can be treated as the superposition of a relatively fast circular motion around a point called the guiding center and a relatively slow drift of this point. The drift speeds may differ for various species depending on their charge states, masses, or temperatures, possibly resulting in electric currents or chemical separation.\n\nIf the magnetic field is uniform and all other forces are absent, then the Lorentz force will cause a particle to undergo a constant acceleration perpendicular to both the particle velocity and the magnetic field. This does not affect particle motion parallel to the magnetic field, but results in circular motion at constant speed in the plane perpendicular to the magnetic field. This circular motion is known as the gyromotion. For a particle with mass formula_1 and charge formula_2 moving in a magnetic field with strength formula_3, it has a frequency, called the gyrofrequency or cyclotron frequency, of\n\nFor a speed perpendicular to the magnetic field of formula_5, the radius of the orbit, called the gyroradius or Larmor radius, is\n\nSince the magnetic Lorentz force is always perpendicular to the magnetic field, it has no influence (to lowest order) on the parallel motion. In a uniform field with no additional forces, a charged particle will gyrate around the magnetic field according to the perpendicular component of its velocity and drift parallel to the field according to its initial parallel velocity, resulting in a helical orbit. If there is a force with a parallel component, the particle and its guiding center will be correspondingly accelerated.\n\nIf the field has a parallel gradient, a particle with a finite Larmor radius will also experience a force in the direction away from the larger magnetic field. This effect is known as the magnetic mirror. While it is closely related to guiding center drifts in its physics and mathematics, it is nevertheless considered to be distinct from them.\n\nGenerally speaking, when there is a force on the particles perpendicular to the magnetic field, then they drift in a direction perpendicular to both the force and the field. If formula_7 is the force on one particle, then the drift velocity is\n\nThese drifts, in contrast to the mirror effect and the non-uniform \"B\" drifts, do not depend on finite Larmor radius, but are also present in cold plasmas. This may seem counterintuitive. If a particle is stationary when a force is turned on, where does the motion perpendicular to the force come from and why doesn't the force produce a motion parallel to itself? The answer is the interaction with the magnetic field. The force initially results in an acceleration parallel to itself, but the magnetic field deflects the resulting motion in the drift direction. Once the particle is moving in the drift direction, the magnetic field deflects it back against the external force, so that the average acceleration in the direction of the force is zero. There is, however, a one-time displacement in the direction of the force equal to (\"f\"/\"m\")ω, which should be considered a consequence of the polarization drift (see below) while the force is being turned on. The resulting motion is a cycloid. More generally, the superposition of a gyration and a uniform perpendicular drift is a trochoid.\n\nAll drifts may be considered special cases of the force drift, although this is not always the most useful way to think about them. The obvious cases are electric and gravitational forces. The grad-B drift can be considered to result from the force on a magnetic dipole in a field gradient. The curvature, inertia, and polarisation drifts result from treating the acceleration of the particle as fictitious forces. The diamagnetic drift can be derived from the force due to a pressure gradient. Finally, other forces such as radiation pressure and collisions also result in drifts.\n\nA simple example of a force drift is a plasma in a gravitational field, e.g. the ionosphere. The drift velocity is\n\nBecause of the mass dependence, the gravitational drift for the electrons can normally be ignored.\n\nThe dependence on the charge of the particle implies that the drift direction is opposite for ions as for electrons, resulting in a current. In a fluid picture, it is this current crossed with the magnetic field that provides that force counteracting the applied force.\n\nThis drift, often called the formula_10 (\"E\"-cross-\"B\") drift, is a special case because the electric force on a particle depends on its charge (as opposed, for example, to the gravitational force considered above). As a result, ions (of whatever mass and charge) and electrons both move in the same direction at the same speed, so there is no net current (assuming quasineutrality of the plasma). In the context of special relativity, in the frame moving with this velocity, the electric field vanishes. The value of the drift velocity is given by\n\nIf the electric field is not uniform, the above formula is modified to read\n\nGuiding center drifts may also result not only from external forces but also from non-uniformities in the magnetic field. It is convenient to express these drifts in terms of the parallel and perpendicular kinetic energies\n\nIn that case, the explicit mass dependence is eliminated. If the ions and electrons have similar temperatures, then they also have similar, though oppositely directed, drift velocities.\n\nWhen a particle moves into a larger magnetic field, the curvature of its orbit becomes tighter, transforming the otherwise circular orbit into a cycloid. The drift velocity is\n\nIn order for a charged particle to follow a curved field line, it needs a drift velocity out of the plane of curvature to provide the necessary centripetal force. This velocity is\n\nwhere formula_17 is the radius of curvature pointing outwards, away from the center of the circular arc which best approximates the curve at that point.\n\nwhere formula_19 is the unit vector in the direction of the magnetic field. This drift can be decomposed into the sum of the curvature drift and the term\n\nIn the important limit of stationary magnetic field and weak electric field, the inertial drift is dominated by the curvature drift term.\n\nIn the limit of small plasma pressure, Maxwell's equations provide a relationship between gradient and curvature that allows the corresponding drifts to be combined as follows\n\nFor a species in thermal equilibrium, formula_22 can be replaced by formula_23 (formula_24 for formula_25 and formula_26 for\nformula_27).\n\nThe expression for the grad-B drift above can be rewritten for the case when formula_28 is due to the curvature.\nThis is most easily done by realizing that in a vacuum, Ampere's Law is \nformula_29. In cylindrical coordinates chosen such that the azimuthal direction is parallel to the magnetic field and the radial direction is parallel to the gradient of the field, this becomes\n\nSince formula_31 is a constant, this implies that\n\nand the grad-B drift velocity can be written\n\nA time-varying electric field also results in a drift given by\n\nObviously this drift is different from the others in that it cannot continue indefinitely. Normally an oscillatory electric field results in a polarization drift oscillating 90 degrees out of phase. Because of the mass dependence, this effect is also called the inertia drift. Normally the polarization drift can be neglected for electrons because of their relatively small mass.\n\nThe diamagnetic drift is not actually a guiding center drift. A pressure gradient does not cause any single particle to drift. Nevertheless, the fluid velocity is defined by counting the particles moving through a reference area, and a pressure gradient results in more particles in one direction than in the other. The net velocity of the fluid is given by\n\nWith the important exception of the E-cross-B drift, the drift velocities of different species will be different. The differential velocity of charged particles results in a current, while the mass dependence of the drift velocity can result in chemical separation.\n\n\nT.G. Northrop, The guiding center approximation to charged particle motion, Annals of Physics 15, p.79-101, 1961\n\nH.J. de Blank, Guiding center motion, Fusion Science and Technology / Volume 61 / Number 2T / February 2012 / Pages 61-68\n\n\"Cosmic Plasma\" (1981), Hannes Alfvén\n"}
{"id": "47524120", "url": "https://en.wikipedia.org/wiki?curid=47524120", "title": "Hastatic order", "text": "Hastatic order\n\nHastatic order is a fundamental way of breaking double \"time-reversal\" symmetry. It is present in the heavy-fermion compound URuSi. This order was dubbed \"hastatic\" from \"hasta\", the Latin word for \"spear\". Its cycle is twice as complex as magnetism.\n\nHastatic order was first reported in January 2013 when the heavy-fermion uranium compound URuSi was cooled to nearly . It was said to produce extra heat and the heat was the main mystery. After the extra heat was released, particles were arranged at this way, making the hastatic order present on that reaction.\n\n"}
{"id": "53311", "url": "https://en.wikipedia.org/wiki?curid=53311", "title": "Hermann von Helmholtz", "text": "Hermann von Helmholtz\n\nHermann Ludwig Ferdinand von Helmholtz (August 31, 1821 – September 8, 1894) was a German physician and physicist who made significant contributions in several scientific fields. The largest German association of research institutions, the Helmholtz Association, is named after him.\n\nIn physiology and psychology, he is known for his mathematics of the eye, theories of vision, ideas on the visual perception of space, color vision research, and on the sensation of tone, perception of sound, and empiricism in the physiology of perception.\n\nIn physics, he is known for his theories on the conservation of energy, work in electrodynamics, chemical thermodynamics, and on a mechanical foundation of thermodynamics.\n\nAs a philosopher, he is known for his philosophy of science, ideas on the relation between the laws of perception and the laws of nature, the science of aesthetics, and ideas on the civilizing power of science.\n\nHelmholtz was born in Potsdam the son of the local Gymnasium headmaster, Ferdinand Helmholtz, who had studied classical philology and philosophy, and who was a close friend of the publisher and philosopher Immanuel Hermann Fichte. Helmholtz's work was influenced by the philosophy of Johann Gottlieb Fichte and Immanuel Kant. He tried to trace their theories in empirical matters like physiology.\n\nAs a young man, Helmholtz was interested in natural science, but his father wanted him to study medicine at the Charité because there was financial support for medical students.\n\nTrained primarily in physiology, Helmholtz wrote on many other topics, ranging from theoretical physics, to the age of the Earth, to the origin of the Solar System.\n\nHelmholtz's first academic position was as a teacher of Anatomy at the Academy of Arts in Berlin in 1848. He then moved to take a post of associate professor of physiology at the Prussian University of Königsberg, where he was appointed in 1849. In 1855 he accepted a full professorship of anatomy and physiology at the University of Bonn. He was not particularly happy in Bonn, however, and three years later he transferred to the University of Heidelberg, in Baden, where he served as professor of physiology. In 1871 he accepted his final university position, as professor of physics at the Humboldt University in Berlin.\n\nHis first important scientific achievement, an 1847 treatise on the conservation of energy, was written in the context of his medical studies and philosophical background. He discovered the principle of conservation of energy while studying muscle metabolism. He tried to demonstrate that no energy is lost in muscle movement, motivated by the implication that there were no \"vital forces\" necessary to move a muscle. This was a rejection of the speculative tradition of \"Naturphilosophie\" which was at that time a dominant philosophical paradigm in German physiology.\n\nDrawing on the earlier work of Sadi Carnot, Benoît Paul Émile Clapeyron and James Prescott Joule, he postulated a relationship between mechanics, heat, light, electricity and magnetism by treating them all as manifestations of a single \"force\" (energy in modern terms). He published his theories in his book \"Über die Erhaltung der Kraft\" (\"On the Conservation of Force\", 1847). \n\nIn the 1850s and 60s, building on the publications of William Thomson, Helmholtz and William Rankine popularized the idea of the heat death of the universe.\n\nIn fluid dynamics, Helmholtz made several contributions, including Helmholtz's theorems for vortex dynamics in inviscid fluids. \n\nHelmholtz was a pioneer in the scientific study of human vision and audition. He coined the term \"psychophysics,\" to capture the distinction between the measurement of physical stimuli and their effect on human perception. For example, the amplitude of a sound wave can be varied, causing the sound to appear louder or softer, but a linear step in sound pressure amplitude does not result in a linear step in perceived loudness. The physical sound needs to be increased exponentially in order for equal steps to seem linear, a fact that is used in current electronic devices to control volume. Helmholtz paved the way in experimental studies on the relationship between the physical energy (physics) and its appreciation (psychology), with the goal in mind to develop \"psychophysical laws.\"\n\nThe sensory physiology of Helmholtz was the basis of the work of Wilhelm Wundt, a student of Helmholtz, who is considered one of the founders of experimental psychology. More explicitly than Helmholtz, Wundt described his research as a form of empirical philosophy and as a study of the mind as something separate. Helmholtz had, in his early repudiation of Naturphilosophie, stressed the importance of materialism, and was focusing more on the unity of \"mind\" and body.\n\nIn 1851, Helmholtz revolutionized the field of ophthalmology with the invention of the ophthalmoscope; an instrument used to examine the inside of the human eye. This made him world-famous overnight. Helmholtz's interests at that time were increasingly focused on the physiology of the senses. His main publication, titled \"Handbuch der Physiologischen Optik\" (\"Handbook of Physiological Optics\" or \"Treatise on Physiological Optics\"), provided empirical theories on depth perception, color vision, and motion perception, and became the fundamental reference work in his field during the second half of the nineteenth century. In the third and final volume, published in 1867, Helmholtz described the importance of unconscious inferences for perception. The \"Handbuch\" was first translated into English under the editorship of James P. C. Southall on behalf of the Optical Society of America in 1924-5. His theory of accommodation went unchallenged until the final decade of the 20th century.\n\nHelmholtz continued to work for several decades on several editions of the handbook, frequently updating his work because of his dispute with Ewald Hering who held opposite views on spatial and color vision. This dispute divided the discipline of physiology during the second half of the 1800s.\n\nIn 1849, while at Königsberg, Helmholtz measured the speed at which the signal is carried along a nerve fibre. At that time most people believed that nerve signals passed along nerves immeasurably fast. He used a recently dissected sciatic nerve of a frog and the calf muscle to which it attached. He used a galvanometer as a sensitive timing device, attaching a mirror to the needle to reflect a light beam across the room to a scale which gave much greater sensitivity. Helmholtz reported transmission speeds in the range of 24.6 - 38.4 meters per second.\n\nIn 1863, Helmholtz published \"Sensations of Tone\", once again demonstrating his interest in the physics of perception. This book influenced musicologists into the twentieth century. Helmholtz invented the Helmholtz resonator to identify the various frequencies or pitches of the pure sine wave components of complex sounds containing multiple tones.\n\nHelmholtz showed that different combinations of resonator could mimic vowel sounds: Alexander Graham Bell in particular was interested in this but, not being able to read German, misconstrued Helmholtz' diagrams as meaning that Helmholtz had transmitted multiple frequencies by wire—which would allow multiplexing of telegraph signals—whereas, in reality, electrical power was used only to keep the resonators in motion. Bell failed to reproduce what he thought Helmholtz had done but later said that, had he been able to read German, he would not have gone on to invent the telephone on the harmonic telegraph principle. \n\nThe translation by Alexander J. Ellis was first published in 1875 (the first English edition was from the 1870 third German edition; Ellis's second English edition from the 1877 fourth German edition was published in 1885; the 1895 and 1912 third and fourth English editions were reprints of the second).\n\nHelmholtz studied the phenomena of electrical oscillations from 1869 to 1871, and in a lecture delivered to the Naturhistorisch-medizinischen Vereins zu Heidelberg (Natural History and Medical Association of Heidelberg) on April 30, 1869, titled \"On Electrical Oscillations\" he indicated that the perceptible damped electrical oscillations in a coil joined up with a Leyden jar were about 1/50th of a second in duration.\n\nIn 1871, Helmholtz moved from Heidelberg to Berlin to become a professor in physics. He became interested in electromagnetism and the Helmholtz equation is named for him. Although he did not make major contributions to this field, his student Heinrich Rudolf Hertz became famous as the first to demonstrate electromagnetic radiation. Oliver Heaviside criticised Helmholtz's electromagnetic theory because it allowed the existence of longitudinal waves. Based on work on Maxwell's equations, Heaviside pronounced that longitudinal waves could not exist in a vacuum or a homogeneous medium. Heaviside did not note, however, that longitudinal electromagnetic waves can exist at a boundary or in an enclosed space.\n\nThere is even a topic by the name \"Helmholtz optics\", based on the Helmholtz equation.\n\nWhoever, in the pursuit of science, seeks after immediate practical utility may rest assured that he seeks in vain.\n— \"Academic Discourse\" (Heidelberg 1862)\n\nOther students and research associates of Helmholtz at Berlin included Max Planck, Heinrich Kayser, Eugen Goldstein, Wilhelm Wien, Arthur König, Henry Augustus Rowland, Albert A. Michelson, Wilhelm Wundt, Fernando Sanford and Michael I. Pupin. Leo Koenigsberger, who was his colleague 1869–1871 in Heidelberg, wrote the definitive biography of him in 1902.\n\n\n\n\n\n"}
{"id": "2256335", "url": "https://en.wikipedia.org/wiki?curid=2256335", "title": "Huangbaiyu", "text": "Huangbaiyu\n\nHuangbaiyu () is a model sustainable village in Benxi, Liaoning, China. As of 2006, over 40 individual houses had been built; however, the construction methods, costs, materials used and the design of each house has come under great criticism.\n\nHuangbaiyu was conceived by William McDonough and Partners in conjunction with Tongji University in Shanghai, the Benxi Design Institute, and China-U.S. Center for Sustainable Development. The town is being built in stages and is to be model of sustainable development using principles laid out by McDonough. His main thesis is that instead of trying to reduce waste you eliminate it by having everything be capable of being broken down into technical or biological nutrition that can be reused so that no waste is created and no waste needs to be disposed.\n\nIn April 2006, the project was encountering some difficulties: some housing was completed, but no residents had moved in.\nBy September 2006, 42 houses had been built. The cost of each individual dwelling is estimated to be around 28,000 yuan (A$4,600).\n\n\n"}
{"id": "20240232", "url": "https://en.wikipedia.org/wiki?curid=20240232", "title": "In Our Water", "text": "In Our Water\n\nIn Our Water is a 1982 American documentary film directed by Meg Switzgable, about a family in South Brunswick, New Jersey, who discover their drinking water is contaminated by a nearby landfill. It was nominated for an Academy Award for Best Documentary Feature, nominated for an Emmy Award and won a Columbia/DuPont Award for Journalistic Excellence.\n\nThe film focuses on the Frank Kaler family of South Brunswick, New Jersey. Kaler is a housepainter and the family gets their water from their own well, as do their neighbors. The Kalers realize that their water is being contaminated from a nearby landfill and seek to get government to acknowledge and fix the problem. Their attempts to contact local and then the state Department of Environmental Protection are rebuffed. Mr. Kaler eventually goes to Washington DC to testify at Congressional hearings. The film follows the 5-year radicalization of Kaler as he realizes what is happening to groundwater and why.\n\n"}
{"id": "2517513", "url": "https://en.wikipedia.org/wiki?curid=2517513", "title": "Jolly balance", "text": "Jolly balance\n\nThe Jolly balance is an instrument for determining specific gravities. Invented by the German physicist Philipp von Jolly in 1864, it consists of a spring fastened at the top to a movable arm. At the lower end, the spring is provided with two small pans, one suspended beneath the other. The lower pan is kept immersed to the same depth in water, while the other one hangs in the air. On the upright stand behind the spring is a mirror on which is engraved or painted a scale of equal parts. The specific gravity of an object, typically a solid, is determined by noting how much the spring lengthens when the object is resting in the upper pan in air (formula_1), and then when the object is moved to the lower pan and immersed in water (formula_2). The specific gravity is formula_3.\n\n"}
{"id": "50305933", "url": "https://en.wikipedia.org/wiki?curid=50305933", "title": "Karatepe bilingual", "text": "Karatepe bilingual\n\nThe Karatepe Bilingual (8th century BCE), also known as the Azatiwada inscription, is a bilingual inscription on stone slabs consisting of Phoenician language and Luwian language text each, which enabled the decryption of the Anatolian hieroglyphs. The artifacts were discovered at Karatepe, southern Turkey by the archaeologists Helmuth Theodor Bossert (1889–1961) and Halet Çambel (1916–2014) in 1946.\n\nThe stones featuring Karatepe Bilingual are situated along with many other statues and reliefs in stone at the Karatepe-Aslantaş Open-Air Museum, which is in turn part of the Karatepe-Aslantaş National Park.\n\nPlaced at the fortress gates, the stones presenting the Karatepe Bilıngıal inscriptions feature the \"Call of Azatiwada\" in the following text:\n\n"}
{"id": "23183287", "url": "https://en.wikipedia.org/wiki?curid=23183287", "title": "Lamb–Mössbauer factor", "text": "Lamb–Mössbauer factor\n\nIn physics, the Lamb–Mössbauer factor (LMF, after Willis Lamb and Rudolf Mössbauer) or elastic incoherent structure factor (EISF) is the ratio of elastic to total incoherent neutron scattering, or the ratio of recoil-free to total nuclear resonant absorption in Mössbauer spectroscopy. The corresponding factor for coherent neutron or X-ray scattering is the Debye–Waller factor; often, that term is used in a more generic way to include the incoherent case as well.\n\nWhen first reporting on recoil-free resonance absorption, Mössbauer (1959) cited relevant theoretical work by Lamb (1939). The first use of the term \"Mössbauer–Lamb factor\" seems to be by Tzara (1961); from 1962 on, the form \"Lamb–Mössbauer factor\" came into widespread use.\n\nSingwi and Sjölander (1960) pointed out the close relation to incoherent neutron scattering. With the invention of backscattering spectrometers, it became possible to measure the Lamb–Mössbauer factor as a function of the wavenumber (whereas Mössbauer spectroscopy operates at a fixed wavenumber). Subsequently, the term \"elastic incoherent structure factor\" became more frequent.\n"}
{"id": "7276057", "url": "https://en.wikipedia.org/wiki?curid=7276057", "title": "List of NGC objects (4001–5000)", "text": "List of NGC objects (4001–5000)\n\nThis is a list of NGC objects 4001–5000 from the New General Catalogue (NGC). The astronomical catalogue is composed mainly of star clusters, nebulae, and galaxies. Other objects in the catalogue can be found in the other subpages of the list of NGC objects.\n\nThe constellation information in these tables is taken from \"The Complete New General Catalogue and Index Catalogue of Nebulae and Star Clusters by J. L. E. Dreyer\", which was accessed using the \"VizieR Service\". Galaxy types are identified using the \"NASA/IPAC Extragalactic Database\". The other data of these tables are from the SIMBAD Astronomical Database unless otherwise stated.\n"}
{"id": "48067427", "url": "https://en.wikipedia.org/wiki?curid=48067427", "title": "MV Isla Bella", "text": "MV Isla Bella\n\nIsla Bella and her sister ship \"Perla del Caribe\" are the world's first liquefied natural gas (LNG) powered container ships. \"Isla Bella\" is currently used as part of the Florida-Puerto Rico trade, sailing out of Jacksonville, Florida and arriving in San Juan, Puerto Rico on a weekly basis. Both ships were built by NASSCO for TOTE Maritime Puerto Rico and replaced the last of the ponce-class ships that were previously used for the route. They cost $324 million US dollars to built and each is propelled by a single slow speed engine capable of 25,191 KW at 104 rpm, which propels the ships at a maximum speed of 22 knots. \n\nThe \"Isla Bella\" has a length of 764 feet and is 106 feet in width. It has a capacity of 3,100 TEU but has the ability to carry 20 foot, 30 foot, 40 foot, 45 foot and 53 foot containers. The \"Isla Bella\" was christened of April 18 before a crowd of 3,400 people. The name \"Isla Bella\" is Spanish for “beautiful island.” The name of the vessel comes from Paola Dominguez as part of a partnership with The Puerto Rico Boys & Girls Club and TOTE.\n\n\"Isla Bella\"'s engine is powered by a single slow speed MAN B&W 8L70ME-GI dual fuel gas engine. This engine operates on a mixture of LNG and diesel fuel oil, the latter of which is used to ignite the natural gas. The engines are designed to run on as little as 3 percent pilot oil. The result is a 97 percent reduction in sulfur oxides and a 98 percent nitrogen oxide reduction in particulate matter coming from the exhausts. To fuel this engine two 900 cubic meter cryogenic tanks are located aft of the house.The combined weight of these two tanks is 760 tons. These eighty-four-foot-long tanks hold a combined total of 475,000 gallons of LNG.\n\nFor the first few years, the ship was refueled from trucks. As of 2018 a dedicated LNG bunker barge and a LNG liquification plant are being built for the \"Isla Bella\" and \"Perla del Caribe\".\n"}
{"id": "699123", "url": "https://en.wikipedia.org/wiki?curid=699123", "title": "Marfa lights", "text": "Marfa lights\n\nThe Marfa lights, also known as the Marfa ghost lights, have been observed near U.S. Route 67 on Mitchell Flat east of Marfa, Texas, in the United States. They have gained some fame as onlookers have ascribed them to paranormal phenomena such as ghosts, UFOs, or will-o'-the-wisp, etc. However, scientific research suggests that most, if not all, are atmospheric reflections of automobile headlights and campfires.\n\nAccording to Judith Brueske, \"The 'Marfa Lights of west Texas have been called many names over the years, such as ghost lights, weird lights, mystery lights, or Chinati lights. The favorite place from which to view the lights is a widened shoulder on Highway 90 about nine miles east of Marfa...at this 'official Marfa Lights viewing site'. The lights are most often reported as rather distant bright lights distinguishable from ranch lights and automobile headlights on Highway 67 (between Marfa and Presidio, to the south) primarily by their aberrant movements.\"\n\nRobert and Judy Wagers define \"Classic Marfa Lights\" as being seen south-southwest of the Marfa Lights Viewing Center (MLVC). They define the left margin of the viewing area as being aligned along the Big Bend Telephone Company tower as viewed from the MLVC, and the right margin defined by Chinati Peak as viewed from the MLVC.\n\nReferring to the Marfa Lights View Park east of Marfa, James Bunnell states, \"you might just see mysterious orbs of light suddenly appear above desert foliage. These balls of light may remain stationary as they pulse on and off with intensity varying from dim to almost blinding brilliance. Then again, these ghostly lights may dart across the desert...or perform splits and mergers. Light colors are usually yellow-orange but other hues, including green, blue and red are also seen. Marfa Mystery Lights (MLs) usually fly above desert vegetation but below background mesas.\"\n\nThe first published account of the lights appeared in the July 1957 issue of \"Coronet\" magazine. In 1976, Elton Miles' \"Tales of the Big Bend\" included stories dating to the 19th century, and a photograph of the Marfa lights taken by a local rancher.\n\nThe earliest anecdote commonly cited as an observation of the Marfa lights is that of the cowboy Robert Reed Ellison in March 1883. This was while he was herding cattle through the Paisano Pass southwest across the Marfa plain. The lights were next reported in 1885 by Joe and Anne Humphreys. Both stories appear in Cecilia Thompson's book \"History of Marfa and Presidio County, Texas 1535-1946\".\n\nBunnell lists 34 Marfa lights sightings from 1945 through 2008. Monitoring stations were put in place starting in 2003. He has identified \"an average of 9.5 MLs on 5.25 nights per year\", but thinks the monitoring stations may only be finding half of the Marfa lights in Mitchell Flat.\n\nSkeptic Brian Dunning notes that the designated \"View Park\" for the lights, a roadside park on the south side of U.S. Route 90 about 9 miles (14 km) east of Marfa, is located at the site of Marfa Army Airfield, where tens of thousands of personnel were stationed between 1942 and 1947, training American and Allied pilots. This massive field was then used for years as a regional airport, with daily airline service. Between Marfa AAF and its satellite fields — each constantly patrolled by sentries — they consider it unlikely that any unusual phenomena would have remained unobserved and unmentioned. According to Dunning, the dominant explanation is that the lights are a sort of mirage caused by sharp temperature gradients between cold and warm layers of air. Marfa is located at an altitude of 4,688 ft (1,429 m) above sea level, and temperature differentials of 40–50 °F (22–28 °C) between high and low temperatures are quite common.\n\nIn May 2004, a group from the Society of Physics Students at the University of Texas at Dallas spent four days investigating and recording lights observed southwest of the view park using traffic volume-monitoring equipment, video cameras, binoculars, and chase cars. Their report made the following conclusions:\n\nThey came to the conclusion that all of the lights observed over a four-night period southwest of the view park could be reliably attributed to automobile headlights traveling along U.S. 67 between Marfa and Presidio, Texas.\n\nFor 20 nights in May 2008, scientists from Texas State University used spectroscopy equipment to observe lights from the Marfa lights viewing station. They recorded a number of lights that \"could have been mistaken for lights of unknown origin\" but, in each case, the movements of the lights and the data from their equipment could be easily explained as automobile headlights or small fires.\n\n\n\n"}
{"id": "21391466", "url": "https://en.wikipedia.org/wiki?curid=21391466", "title": "Musselroe Wind Farm", "text": "Musselroe Wind Farm\n\nMusselroe Wind Farm is a wind farm at Cape Portland, Tasmania, Australia. It is third wind farm in the state, being owned and operated by Hydro Tasmania. It consists of 56 Vestas V90-3MW wind turbines, with a generating capacity of 168 MW. The energy output from the Musselroe Wind Farm will be sufficient to supply electricity to around 50,000 households and abate 450,000 tonnes of emissions annually.\n\nThe first 37 turbines have been connected to the grid in 2013, and the full farm was completed in January 2014.\n\n\n"}
{"id": "2506410", "url": "https://en.wikipedia.org/wiki?curid=2506410", "title": "NACA duct", "text": "NACA duct\n\nA NACA duct, also sometimes called a NACA scoop or NACA inlet, is a common form of low-drag air inlet design, originally developed by the U.S. National Advisory Committee for Aeronautics (NACA), the precursor to NASA, in 1945.\n\nPrior submerged inlet experiments showed poor pressure recovery due to the slow-moving boundary layer entering the inlet. This design is believed to work because the combination of the gentle ramp angle and the curvature profile of the walls creates counter-rotating vortices which deflect the boundary layer away from the inlet and draws in the faster moving air, while avoiding the form drag and flow separation that can occur with protruding scoop designs. \n\nWhen properly implemented, a NACA duct allows air to flow into an internal duct, often for cooling purposes, with a minimal disturbance to the flow. The design was originally called a submerged inlet, since it consists of a shallow ramp with curved walls recessed into the exposed surface of a streamlined body, such as an aircraft. \n\nThis type of flush inlet generally cannot achieve the larger ram pressures and flow volumes of an external design, and so is rarely used for the jet engine intake application for which it was originally designed, such as the North American YF-93 and Short SB.4 Sherpa. It is, however, common for piston engine and ventilation intakes.\n\nIt is especially favored in racing car design. Famous sports cars featuring prominent NACA ducts include the Ferrari F40, the Lamborghini Countach, the 1971-1973 Ford Mustang, the 1973 Pontiac GTO, Porsche 911 GT2\n\n\n"}
{"id": "36647269", "url": "https://en.wikipedia.org/wiki?curid=36647269", "title": "Newar window", "text": "Newar window\n\nNewār window refers to the elaborately carved wooden window which is the distinguishing feature of traditional Nepalese architecture. The ornate windows have been described as a symbol of Newar culture and artistry. The level of design and carving of the Newar window reached its peak in the mid-18th century. They are found on palaces, private residences and sacred houses across Nepal Mandala.\n\nThe lintel, sill and jamb are ornamented with figures of deities, mythical beings, dragons, peacocks, auspicious jars and other elements. The window is surmounted by ritual parasols. Traditional Newar houses are usually of four stories and built of brick. Different types of windows are used on each floor according to their function.\n\nNewar windows and bare-brick facade in the traditional style are making a comeback as an architectural trend due to the tourism industry and growing heritage awareness.\n\nAmong the many window designs, the following are the most common.\n\n\nA number of traditional carved windows in the Kathmandu Valley are celebrated for their uniqueness.\n\n"}
{"id": "21278", "url": "https://en.wikipedia.org/wiki?curid=21278", "title": "Nobelium", "text": "Nobelium\n\nNobelium is a synthetic chemical element with symbol No and atomic number 102. It is named in honor of Alfred Nobel, the inventor of dynamite and benefactor of science. A radioactive metal, it is the tenth transuranic element and is the penultimate member of the actinide series. Like all elements with atomic number over 100, nobelium can only be produced in particle accelerators by bombarding lighter elements with charged particles. A total of twelve nobelium isotopes are known to exist; the most stable is No with a half-life of 58 minutes, but the shorter-lived No (half-life 3.1 minutes) is most commonly used in chemistry because it can be produced on a larger scale.\n\nChemistry experiments have confirmed that nobelium behaves as a heavier homolog to ytterbium in the periodic table. The chemical properties of nobelium are not completely known: they are mostly only known in aqueous solution. Before nobelium's discovery, it was predicted that it would show a stable +2 oxidation state as well as the +3 state characteristic of the other actinides: these predictions were later confirmed, as the +2 state is much more stable than the +3 state in aqueous solution and it is difficult to keep nobelium in the +3 state.\n\nIn the 1950s and 1960s, many claims of the discovery of nobelium were made from laboratories in Sweden, the Soviet Union, and the United States. Although the Swedish scientists soon retracted their claims, the priority of the discovery and therefore the naming of the element was disputed between Soviet and American scientists, and it was not until 1997 that International Union of Pure and Applied Chemistry (IUPAC) credited the Soviet team with the discovery, but retained nobelium, the Swedish proposal, as the name of the element due to its long-standing use in the literature.\n\nThe discovery of element 102 was a complicated process and was claimed by groups from Sweden, the United States, and the Soviet Union. The first complete and incontrovertible report of its detection only came in 1966 from the Joint Institute of Nuclear Research at Dubna (then in the Soviet Union).\n\nThe first announcement of the discovery of element 102 was announced by physicists at the Nobel Institute in Sweden in 1957. The team reported that they had bombarded a curium target with carbon-13 ions for twenty-five hours in half-hour intervals. Between bombardments, ion-exchange chemistry was performed on the target. Twelve out of the fifty bombardments contained samples emitting (8.5 ± 0.1) MeV alpha particles, which were in drops which eluted earlier than fermium (atomic number \"Z\" = 100) and californium (\"Z\" = 98). The half-life reported was 10 minutes and was assigned to either 102 or 102, although the possibility that the alpha particles observed were from a presumably short-lived mendelevium (\"Z\" = 101) isotope created from the electron capture of element 102 was not excluded. The team proposed the name \"nobelium\" (No) for the new element, which was immediately approved by IUPAC, a decision which the Dubna group characterized in 1968 as being hasty. The following year, scientists at the Lawrence Berkeley National Laboratory repeated the experiment but were unable to find any 8.5 MeV events which were not background effects.\n\nIn 1959, the Swedish team attempted to explain the Berkeley team's inability to detect element 102 in 1958, maintaining that they did discover it. However, later work has shown that no nobelium isotopes lighter than No (no heavier isotopes could have been produced in the Swedish experiments) with a half-life over 3 minutes exist, and that the Swedish team's results are most likely from thorium-225, which has a half-life of 8 minutes and quickly undergoes triple alpha decay to polonium-213, which has a decay energy of 8.53612 MeV. This hypothesis is lent weight by the fact that thorium-225 can easily be produced in the reaction used and would not be separated out by the chemical methods used. Later work on nobelium also showed that the divalent state is more stable than the trivalent one and hence that the samples emitting the alpha particles could not have contained nobelium, as the divalent nobelium would not have eluted with the other trivalent actinides. Thus, the Swedish team later retracted their claim and associated the activity to background effects.\n\nThe Berkeley team, consisting of Albert Ghiorso, Glenn T. Seaborg, John R. Walton and Torbjørn Sikkeland, then claimed the synthesis of element 102 in 1958. The team used the new heavy-ion linear accelerator (HILAC) to bombard a curium target (95% Cm and 5% Cm) with C and C ions. They were unable to confirm the 8.5 MeV activity claimed by the Swedes but were instead able to detect decays from fermium-250, supposedly the daughter of 102 (produced from the curium-246), which had an apparent half-life of ~3 s. Later 1963 Dubna work confirmed that 102 could be produced in this reaction, but that its half-life was actually . In 1967, the Berkeley team attempted to defend their work, stating that the isotope found was indeed Fm but the isotope that the half-life measurements actually related to was californium-244, granddaughter of 102, produced from the more abundant curium-244. Energy differences were then attributed to \"resolution and drift problems\", although these had not been previously reported and should also have influenced other results. 1977 experiments showed that 102 indeed had a 2.3-second half-life. However, 1973 work also showed that the Fm recoil could have also easily been produced from the isomeric transition of Fm (half-life 1.8 s) which could also have been formed in the reaction at the energy used. Given this, it is probable that no nobelium was actually produced in this experiment.\n\nIn 1959 the team continued their studies and claimed that they were able to produce an isotope that decayed predominantly by emission of an 8.3 MeV alpha particle, with a half-life of 3 s with an associated 30% spontaneous fission branch. The activity was initially assigned to 102 but later changed to 102. However, they also noted that it was not certain that nobelium had been produced due to difficult conditions. The Berkeley team decided to adopt the proposed name of the Swedish team, \"nobelium\", for the element.\n\nMeanwhile, in Dubna, experiments were carried out in 1958 and 1960 aiming to synthesize element 102 as well. The first 1958 experiment bombarded plutonium-239 and -241 with oxygen-16 ions. Some alpha decays with energies just over 8.5 MeV were observed, and they were assigned to 102, although the team wrote that formation of isotopes from lead or bismuth impurities (which would not produce nobelium) could not be ruled out. While later 1958 experiments noted that new isotopes could be produced from mercury, thallium, lead, or bismuth impurities, the scientists still stood by their conclusion that element 102 could be produced from this reaction, mentioning a half-life of under 30 seconds and a decay energy of (8.8 ± 0.5) MeV. Later 1960 experiments proved that these were background effects. 1967 experiments also lowered the decay energy to (8.6 ± 0.4) MeV, but both values are too high to possibly match those of No or No. The Dubna team later stated in 1970 and again in 1987 that these results were not conclusive.\n\nIn 1961, Berkeley scientists claimed the discovery of element 103 in the reaction of californium with boron and carbon ions. They claimed the production of the isotope 103, and also claimed to have synthesized an alpha decaying isotope of element 102 that had a half-life of 15 s and alpha decay energy 8.2 MeV. They assigned this to 102 without giving a reason for the assignment. The values do not agree with those now known for No, although they do agree with those now known for No, and while this isotope probably played a part in this experiment its discovery was inconclusive.\n\nWork on element 102 also continued in Dubna, and in 1964, experiments were carried out there to detect alpha-decay daughters of element 102 isotopes by synthesizing element 102 from the reaction of a uranium-238 target with neon ions. The products were carried along a silver catcher foil and purified chemically, and the isotopes Fm and Fm were detected. The yield of Fm was interpreted as evidence that its parent 102 was also synthesized: as it was noted that Fm could also be produced directly in this reaction by the simultaneous emission of an alpha particle with the excess neutrons, steps were taken to ensure that Fm could not go directly to the catcher foil. The half-life detected for 102 was 8 s, which is much higher than the more modern 1967 value of (3.2 ± 0.2) s. Further experiments were conducted in 1966 for 102, using the reactions Am(N,4n)102 and U(Ne,6n)102, finding a half-life of (50 ± 10) s: at that time the discrepancy between this value and the earlier Berkeley value was not understood, although later work proved that the formation of the isomer Fm was less likely in the Dubna experiments than at the Berkeley ones. In hindsight, the Dubna results on 102 were probably correct and can be now considered a conclusive detection of element 102.\n\nOne more very convincing experiment from Dubna was published in 1966, again using the same two reactions, which concluded that 102 indeed had a half-life much longer than the 3 seconds claimed by Berkeley. Later work in 1967 at Berkeley and 1971 at the Oak Ridge National Laboratory fully confirmed the discovery of element 102 and clarified earlier observations. In December 1966, the Berkeley group repeated the Dubna experiments and fully confirmed them, and used this data to finally assign correctly the isotopes they had previously synthesized but could not yet identify at the time, and thus claimed to have discovered nobelium in 1958 to 1961.\n\nIn 1969, the Dubna team carried out chemical experiments on element 102 and concluded that it behaved as the heavier homologue of ytterbium. The Russian scientists proposed the name \"joliotium\" (Jo) for the new element after Irène Joliot-Curie, who had recently died, creating an element naming controversy that would not be resolved for several decades, which each group using its own proposed names.\n\nIn 1992, the IUPAC-IUPAP Transfermium Working Group (TWG) reassessed the claims of discovery and concluded that only the Dubna work from 1966 correctly detected and assigned decays to nuclei with atomic number 102 at the time. The Dubna team are therefore officially recognised as the discoverers of nobelium although it is possible that it was detected at Berkeley in 1959. This decision was criticized by Berkeley the following year, calling the reopening of the cases of elements 101 to 103 a \"futile waste of time\", while Dubna agreed with the IUPAC's decision.\n\nIn 1994, as part of an attempted resolution to the element naming controversy, the IUPAC ratified names for elements 101–109. For element 102, it ratified the name \"nobelium\" (No) on the basis that it had become entrenched in the literature over the course of 30 years and that Alfred Nobel should be commemorated in this fashion. Because of outcry over the 1994 names, which mostly did not respect the choices of the discoverers, a comment period ensued, and in 1995 IUPAC named element 102 \"flerovium\" (Fl) as part of a new proposal, after either Georgy Flyorov or his eponymous Flerov Laboratory of Nuclear Reactions. This proposal was also not accepted, and in 1997 the name \"nobelium\" was restored. Today the name \"flerovium\", with the same symbol, refers to element 114.\n\nIn the periodic table, nobelium is located to the right of the actinide mendelevium, to the left of the actinide lawrencium, and below the lanthanide ytterbium. Nobelium metal has not yet been prepared in bulk quantities, and bulk preparation is currently impossible. Nevertheless, a number of predictions and some preliminary experimental results have been done regarding its properties.\n\nThe lanthanides and actinides, in the metallic state, can exist as either divalent (such as europium and ytterbium) or trivalent (most other lanthanides) metals. The former have fs configurations, whereas the latter have fds configurations. In 1975, Johansson and Rosengren examined the measured and predicted values for the cohesive energies (enthalpies of crystallization) of the metallic lanthanides and actinides, both as divalent and trivalent metals. The conclusion was that the increased binding energy of the [Rn]5f6d7s configuration over the [Rn]5f7s configuration for nobelium was not enough to compensate for the energy needed to promote one 5f electron to 6d, as is true also for the very late actinides: thus einsteinium, fermium, mendelevium, and nobelium were expected to be divalent metals, although for nobelium this prediction has not yet been confirmed. The increasing predominance of the divalent state well before the actinide series concludes is attributed to the relativistic stabilization of the 5f electrons, which increases with increasing atomic number: an effect of this is that nobelium is predominantly divalent instead of trivalent, unlike all the other lanthanides and actinides. In 1986, nobelium metal was estimated to have an enthalpy of sublimation between 126 kJ/mol, a value close to the values for einsteinium, fermium, and mendelevium and supporting the theory that nobelium would form a divalent metal. Like the other divalent late actinides (except the once again trivalent lawrencium), metallic nobelium should assume a face-centered cubic crystal structure. Divalent nobelium metal should have a metallic radius of around 197 pm. Nobelium's melting point has been predicted to be 827 °C, the same value as that estimated for the neighboring element mendelevium. Its density is predicted to be around 9.9 ± 0.4 g/cm.\n\nThe chemistry of nobelium is incompletely characterized and is known only in aqueous solution, in which it can take on the +3 or +2 oxidation states, the latter being more stable. It was largely expected before the discovery of nobelium that in solution, it would behave like the other actinides, with the trivalent state being predominant; however, Seaborg predicted in 1949 that the +2 state would also be relatively stable for nobelium, as the No ion would have the ground-state electron configuration [Rn]5f, including the stable filled 5f shell. It took nineteen years before this prediction was confirmed.\n\nIn 1967, experiments were conducted to compare nobelium's chemical behavior to that of terbium, californium, and fermium. All four elements were reacted with chlorine and the resulting chlorides were deposited along a tube, along which they were carried by a gas. It was found that the nobelium chloride produced was strongly adsorbed on solid surfaces, proving that it was not very volatile, like the chlorides of the other three investigated elements. However, both NoCl and NoCl were expected to exhibit nonvolatile behavior and hence this experiment was inconclusive as to what the preferred oxidation state of nobelium was. Determination of nobelium's favoring of the +2 state had to wait until the next year, when cation-exchange chromatography and coprecipitation experiments were carried out on around fifty thousand No atoms, finding that it behaved differently from the other actinides and more like the divalent alkaline earth metals. This proved that in aqueous solution, nobelium is most stable in the divalent state when strong oxidizers are absent. Later experimentation in 1974 showed that nobelium eluted with the alkaline earth metals, between Ca and Sr. Nobelium is the only f-block element for which the +2 state is the most common and stable one in aqueous solution. This occurs because of the large energy gap between the 5f and 6d orbitals at the end of the actinide series.\n\nThe relativistic stability of the 7s subshell greatly destabilizes nobelium dihydride, NoH, and relativistic stabilisation of the 7p spinor over the 6d spinor mean that excited states in nobelium atoms have 7s and 7p contribution instead of the expected 6d contribution. The long No–H distances in the NoH molecule and the significant charge transfer lead to extreme ionicity with a dipole moment of 5.94 D for this molecule. In this molecule, nobelium is expected to exhibit main-group-like behavior, specifically acting like an alkaline earth metal with its \"n\"s valence shell configuration and core-like 5f orbitals.\n\nNobelium's complexing ability with chloride ions is most similar to that of barium, which complexes rather weakly. Its complexing ability with citrate, oxalate, and acetate in an aqueous solution of 0.5 M ammonium nitrate is between that of calcium and strontium, although it is somewhat closer to that of strontium.\n\nThe standard reduction potential of the \"E\"°(No→No) couple was estimated in 1967 to be between +1.4 and +1.5 V; it was later found in 2009 to be only about +0.75 V. The positive value shows that No is more stable than No and that No is a good oxidizing agent. While the quoted values for the \"E\"°(No→No) and \"E\"°(No→No) vary among sources, the accepted standard estimates are −2.61 and −1.26 V. It has been predicted that the value for the \"E\"°(No→No) couple would be +6.5 V. The Gibbs energies of formation for No and No are estimated to be −342 and −480 kJ/mol, respectively.\n\nA nobelium atom has 102 electrons, of which three can act as valence electrons. They are expected to be arranged in the configuration [Rn]5f7s (ground state term symbol S), although experimental verification of this electron configuration had not yet been made as of 2006. In forming compounds, all the three valence electrons may be lost, leaving behind a [Rn]5f core: this conforms to the trend set by the other actinides with their [Rn]5f electron configurations in the tripositive state. Nevertheless, it is more likely that only two valence electrons may be lost, leaving behind a stable [Rn]5f core with a filled 5f shell. The first ionization potential of nobelium was measured to be at most (6.65 ± 0.07) eV in 1974, based on the assumption that the 7s electrons would ionize before the 5f ones; this value has since not yet been refined further due to nobelium's scarcity and high radioactivity. The ionic radius of hexacoordinate and octacoordinate No had been preliminarily estimated in 1978 to be around 90 and 102 pm respectively; the ionic radius of No has been experimentally found to be 100 pm to two significant figures. The enthalpy of hydration of No has been calculated as 1486 kJ/mol.\n\nTwelve isotopes of nobelium are known, with mass numbers 250–260 and 262; all are radioactive. Additionally, nuclear isomers are known for mass numbers 251, 253, and 254. Of these, the longest-lived isotope is No with a half-life of 58 minutes, and the longest-lived isomer is No with a half-life of 1.7 seconds. However, the still undiscovered isotope No is predicted to have a still longer half-life of 170 min. Additionally, the shorter-lived No (half-life 3.1 minutes) is more often used in chemical experimentation because it can be produced in larger quantities from irradiation of californium-249 with carbon-12 ions. After No and No, the next most stable nobelium isotopes are No (half-life 1.62 minutes), No (51 seconds), No (25 seconds), No (2.91 seconds), and No (2.57 seconds). All of the remaining nobelium isotopes have half-lives that are less than a second, and the shortest-lived known nobelium isotope (No) has a half-life of only 0.25 milliseconds. The isotope No is especially interesting theoretically as it is in the middle of a series of prolate nuclei from Pa to Rg, and the formation of its nuclear isomers (of which two are known) is controlled by proton orbitals such as 2f which come just above the spherical proton shell; it can be synthesized in the reaction of Pb with Ca.\n\nThe half-lives of nobelium isotopes increase smoothly from No to No. However, a dip appears at No, and beyond this the half-lives of even-even nobelium isotopes drop sharply as spontaneous fission becomes the dominant decay mode. For example, the half-life of No is almost three seconds, but that of No is only 1.2 milliseconds. This shows that at nobelium, the mutual repulsion of protons poses a limit to the region of long-lived nuclei in the actinide series. The even-odd nobelium isotopes mostly continue to have longer half-lives as their mass numbers increase, with a dip in the trend at No.\n\nThe isotopes of nobelium are mostly produced by bombarding actinide targets (uranium, plutonium, curium, californium, or einsteinium), with the exception of nobelium-262, which is produced as the daughter of lawrencium-262. The most commonly used isotope, No, can be produced from bombarding curium-248 or californium-249 with carbon-12: the latter method is more common. Irradiating a 350 μg cm target of californium-249 with three trillion (3 × 10) 73 MeV carbon-12 ions per second for ten minutes can produce around 1200 nobelium-255 atoms.\n\nOnce the nobelium-255 is produced, it can be separated out in a similar way as used to purify the neighboring actinide mendelevium. The recoil momentum of the produced nobelium-255 atoms is used to bring them physically far away from the target from which they are produced, bringing them onto a thin foil of metal (usually beryllium, aluminium, platinum, or gold) just behind the target in a vacuum: this is usually combined by trapping the nobelium atoms in a gas atmosphere (frequently helium), and carrying them along with a gas jet from a small opening in the reaction chamber. Using a long capillary tube, and including potassium chloride aerosols in the helium gas, the nobelium atoms can be transported over tens of meters. The thin layer of nobelium collected on the foil can then be removed with dilute acid without completely dissolving the foil. The nobelium can then be isolated by exploiting its tendency to form the divalent state, unlike the other trivalent actinides: under typically used elution conditions (bis-(2-ethylhexyl) phosphoric acid (HDEHP) as stationary organic phase and 0.05 M hydrochloric acid as mobile aqueous phase, or using 3 M hydrochloric acid as an eluant from cation-exchange resin columns), nobelium will pass through the column and elute while the other trivalent actinides remain on the column. However, if a direct \"catcher\" gold foil is used, the process is complicated by the need to separate out the gold using anion-exchange chromatography before isolating the nobelium by elution from chromatographic extraction columns using HDEHP.\n\n"}
{"id": "450883", "url": "https://en.wikipedia.org/wiki?curid=450883", "title": "Northeastern United States blizzard of 1978", "text": "Northeastern United States blizzard of 1978\n\nThe Northeastern United States blizzard of 1978 was a catastrophic, historic nor'easter that struck New England, New Jersey, and the New York metropolitan area. The \"Blizzard of '78\" formed on Sunday, February 5, 1978, and broke up on February 7. The storm was primarily known as \"Storm Larry\" in Connecticut, following the local convention promoted by the Travelers Weather Service on television and radio stations there. Snow fell mostly from Monday morning, February 6, to the evening of Tuesday, February 7. Connecticut, Rhode Island, and Massachusetts were hit especially hard by this storm.\n\nBoston received a record-breaking of snow; Providence also broke a record, with of snow; Atlantic City broke an all-time storm accumulation, with . Nearly all economic activity was disrupted in the worst-hit areas. The storm killed about 100 people in the Northeast and injured about 4,500. It caused more than (US$ in terms) in damage.\n\nThe storm was formed from an extratropical cyclone off the coast of South Carolina on February 5. An Arctic cold front and a cold air mass then merged with the storm, creating the perfect ingredients for a large and intense low-pressure system.\n\nThis storm system made its way up the coast and approached southern New England late February 6 and early February 7. Since it developed during a new moon, an unusually large high tide occurred, and the storm brought a massive amount of water along coastal communities. The huge storm surge resulted in broken sea walls and massive property loss.\n\nStrong winds and extremely heavy precipitation brought zero visibility for travelers, and numerous power outages ensued. The precipitation changed to rain on Cape Cod, reducing the total snowfall, but snow continued in the west. By the time it ended, thousands of people were stranded and homeless as a result of the storm.\n\nThe storm's power was made apparent by its sustained hurricane-force winds of approximately with gusts to and the formation of an eye-like structure in the middle. While a typical nor'easter brings steady snow for six to twelve hours, the Blizzard of '78 brought heavy snow for an unprecedented full 33 hours as it was blocked from heading into the North Atlantic by a strong Canadian high pressure area. In many areas in Central and Southern New England, the snow falling at night turned to an icy mix that left a notable layer of solid ice on every external surface. This ice greatly complicated recovery efforts in subsequent days, as it added considerable weight to power lines and tree limbs. Trees that survived the daytime snow did not survive the nighttime ice storm.\n\nAn atypical vertical development of storm clouds brought unusual thundersnow to southern New England and Long Island. These storms resulted in lightning and thunder accompanying the snowfall as it fell at an hour at times.\n\nOne of the major problems with the Blizzard of 1978 was the lack of foreknowledge about the storm's severity. Weather forecasting in New England is difficult, and meteorologists had developed a reputation as being inaccurate. Forecasting techniques and technology had improved dramatically in the 1970s, but the public was still quite skeptical. Snow failed to arrive in Monday's pre-dawn hours as predicted, and many locals felt it to be another failed forecast—despite the accuracy of National Weather Service (NWS) forecasters' predictions concerning the Great Blizzard—and they went to work and school as normal. Because of this, people had neither time nor incentive to prepare. The region was already reeling after storms in January 1978 that left nearly two feet of snow in some areas of New England, and had caused the collapse of the roof of the Hartford Civic Center.\nThe government of Massachusetts had a system for notifying major employers to send employees home early in the event of heavy storms. Thousands of employees were sent home starting in the early afternoon of February 6, but thousands more were still caught by the storm. Some did not make it home for several days. Many people were stranded in their cars along roads throughout New England. Fourteen people died on I-95 near Boston because snow piled high enough to prevent poisonous exhaust fumes from escaping from their idling vehicles. I-95 eventually had to be evacuated by cross-country skiers and snowmobilers. More than 3,500 cars were found abandoned and buried on roads during the clean-up. This number excludes the countless other vehicles buried in driveways, on the sides of streets, and in parking lots. Other transportation links were disrupted and shut down throughout the region, stranding public-transit commuters in city centers.\n\nSnowplows were also stranded in traffic as the snow continued to fall. At one point on I-93 north of Boston, a jackknifed tractor trailer blocked traffic in both directions, with a similar event occurring on Route 128 near Route 138 in Canton. The Neponset River also flooded I-93 in Milton, causing the highway's complete closure.\n\nA massive effort was made to clear Logan Airport runways for 200 National Guard troops' arrivial on 27 C-130 and C-141 military flights from Fort Bragg and Fort Devens, who were called out by the governor.\n\nSome 11,666 college-hockey fans in Boston Garden, then the site of the 26th edition of the annual \"Beanpot\" college ice hockey tournament, held at the time of the blizzard's outbreak, found weather much different from what they had expected. Some spectators spent the next few days living at the arena, eating hot dogs, and sleeping in the bleachers and locker rooms. Because of the Blizzard, the second round of the Beanpot that year was not held until March 1, 1978, the latest date ever for the tournament's concluding games.\n\nThroughout eastern Massachusetts, automobile traffic was banned for the remainder of the week. Thousands of people walked and skied on the quiet city streets and over the frozen Charles River.\n\nThis blizzard was one of the worst in Rhode Island's history, catching off guard many residents and the state government. Although Governor J. Joseph Garrahy had ordered an emergency evacuation of all public buildings, shortly before noon on February 6, too many people had lagged. Providence County, Rhode Island, was the hardest hit by the blizzard; the towns of Lincoln, Smithfield, Woonsocket, and North Smithfield all reported totals of at least snow.\n\nIn New York City, it was one of the rare times that a snowstorm closed the schools; the New York City Board of Education closed schools for snow again only once in the next 18 years, on April 7, 1982. Most suburban districts in the area close for snow several times each winter, but they rarely do in the city itself because of relatively easy access to subways, whose ability to run is not appreciably affected by moderate snowstorms.\n\nMany people were caught in the storm while driving, and many others were trapped in their homes and workplaces, with snow drifts of up to , in some places blocking the exits. In many cases, those who had become ill or had been injured during the storm had to be taken to hospitals by snowmobile. Other people left their homes and went for help by cross-country skis and sleds.\n\nThe storm caused coastal flooding. The fierce northeast winds from the storm—with the low-pressure area stalled off the island of Martha's Vineyard—combined with high tides and storm surge, resulting from the storm's low pressure. This sent water over low land along the shores of Long Island Sound, Cape Cod Bay, and other bodies of water, causing some of the worst recorded coastal flooding. The flood continued through two days of tide cycles, a total of four successive high tides. Thousands of homes throughout coastal Massachusetts were damaged or destroyed, as was \"Motif Number 1\", in Rockport, an often painted fisherman's shack renowned in art circles. The \"Peter Stuyvesant\", a former Hudson River Day Line boat turned into a floating restaurant, was sunk in Boston Harbor. The region's fishing fleet was damaged by the storm.\n\nThe storm's straight-line surface winds destroyed buildings along the coast, often aided by flooding and waves. Wind gusts of were recorded in Plum Island and at First Cliff in Scituate, Massachusetts. Duxbury Beach was hit with gusts and in Chatham.\n\nBoston and Providence recorded all-time highs for 24-hour and storm snowfall. Many people were left without heat, water, food, and electricity for over a week after the storm finished. Approximately 10,000 people moved into emergency shelters. Some 2,500 houses were reported as seriously damaged or destroyed and 54 people were killed, many because of fallen electrical wires. Several people were found dead in downtown Providence, near the central police station; they may have been seeking shelter. Ten-year-old Peter Gosselin, of Uxbridge, Massachusetts, disappeared in the deep snow just feet from his home's front door and was not found until three weeks later. Most of the Interstate highway system in the region was shut down, with some stretches not reopening to traffic until the following week. Air and rail traffic also were shut down.\n\nThe snow fell too quickly for plow trucks to keep up. Plows were further hampered by the number of cars stuck on the roads. In Boston, the deep snow overwhelmed the city's sanitation department, because there was no more room along streets and sidewalks to put the snow; much of it was hauled and dumped in nearby harbors. Throughout the region, the high winds caused enormous drifts.\n\nA state of emergency was declared by governors in the affected states, and the United States National Guard was called out to help clear the roads. Additional troops were flown into Boston to help. It took six days to clear the roads of snow and of the cars and trucks buried in it. Governor Ella T. Grasso ordered all roads in Connecticut closed except for emergency travel, for three days; Governor Michael Dukakis, of Massachusetts, did the same for his state. The parking lot of Fenway Park was used for the National Guard to stage its efforts. In Massachusetts, there was no travel ban again until 35 years later, when governor Deval Patrick announced a travel ban on February 8, 2013, running from 4 pm that day until 4 pm the next day, because of the February 2013 nor'easter, whose snowfall rivaled and, in some places, beat that of the Blizzard of '78; in the \"Blizzard of '13\", the ban was declared before the worst hit; in the Blizzard of '78 this happened after the storm's worst.\n\nExtensive beach erosion occurred on the east coast of Massachusetts. Especially hard-hit were Cape Cod and Cape Ann, both on the eastern shore of Massachusetts. In Truro, on Cape Cod, the Atlantic Ocean broke through to the Pamet River for the first time during this storm, completely washing away the link between the North and South Pamet roads. The town chose not to reconstruct the link, though the right-of-way is open to pedestrians. Monomoy Island was split into north and south parts.\n\nMany homes along the New England and Long Island coastlines were destroyed or washed into the ocean. Many roof collapses occurred across New England from snow load.\n\n"}
{"id": "11516862", "url": "https://en.wikipedia.org/wiki?curid=11516862", "title": "Phytotope", "text": "Phytotope\n\nPhytotope is the total habitat available for colonisation within any certain ecotope or biotope by plants and fungi. The community of plants and fungi so established constitutes the phytocoenosis of that ecotope.\n\nAll these words (ecotope, biotope, phytotope and others) describe environmental niches at very small scales of consideration. A suburban garden or village park or wilderness ravine would each be deserving of the label.\n\n\n"}
{"id": "38822", "url": "https://en.wikipedia.org/wiki?curid=38822", "title": "Power transmission", "text": "Power transmission\n\nPower transmission is the movement of energy from its place of generation to a location where it is applied to perform useful work.\n\nPower is defined formally as units of energy per unit time. In SI units:\n\nSince the development of technology, transmission and storage systems have been of immense interest to technologists and technology users.\n\nWith the widespread establishment of electrical grids, power transmission is usually associated most with electric power transmission. Alternating current is normally preferred as its voltage may be easily stepped up by a transformer in order to minimize resistive loss in the conductors used to transmit power over great distances; another set of transformers is required to step it back down to safer or more usable voltage levels at destination.\n\nPower transmission is usually performed with overhead lines as this is the most economical way to do so. Underground transmission by high-voltage cables is chosen in crowded urban areas and in high-voltage direct-current (HVDC) submarine connections.\n\nPower might also be transmitted by changing electromagnetic fields or by radio waves; microwave energy may be carried efficiently over short distances by a waveguide or in free space via wireless power transfer.\n\nElectrical power transmission has replaced mechanical power transmission in all but the very shortest distances. \n\nFrom the 16th century through the industrial revolution to the end of the 19th century mechanical power transmission was the norm. The oldest long-distance power transmission technology involved systems of push-rods or jerker lines (\"stängenkunst\" or \"feldstängen\") connecting waterwheels to distant mine-drainage and brine-well pumps. A surviving example from 1780 exists at Bad Kösen that transmits power approximately 200 meters from a waterwheel to a salt well, and from there, an additional 150 meters to a brine evaporator. This technology survived into the 21st century in a handful of oilfields in the US, transmitting power from a central pumping engine to the numerous pump-jacks in the oil field.\n\nMechanical power may be transmitted directly using a solid structure such as a driveshaft; transmission gears can adjust the amount of torque or force vs. speed in much the same way an electrical transformer adjusts voltage vs current. Factories were fitted with overhead line shafts providing rotary power. Short line-shaft systems were described by Agricola, connecting a waterwheel to numerous ore-processing machines. While the machines described by Agricola used geared connections from the shafts to the machinery, by the 19th century, drivebelts would become the norm for linking individual machines to the line shafts. One mid 19th century factory had 1,948 feet of line shafting with 541 pulleys.\n\nHydraulic systems use liquid under pressure to transmit power; canals and hydroelectric power generation facilities harness natural water power to lift ships or generate electricity. Pumping water or pushing mass uphill with (windmill pumps) is one possible means of energy storage. London had a hydraulic network powered by five pumping stations operated by the London Hydraulic Power Company, with a total effect of 5 MW.\n\nPneumatic systems use gasses under pressure to transmit power; compressed air is commonly used to operate pneumatic tools in factories and repair garages. A pneumatic wrench (for instance) is used to remove and install automotive tires far more quickly than could be done with standard manual hand tools. A pneumatic system was proposed by proponents of Edison's direct current as the basis of the power grid. Compressed air generated at Niagara Falls would drive far away generators of DC power. The War of Currents ended with alternating current (AC) as the only means of long distance power transmission.\n\nThermal power can be transported in pipelines containing a high heat capacity fluid such as oil or water as used in district heating systems, or by physically transporting material items, such as bottle cars, or in the ice trade.\n\nPower (and energy) may be transmitted by physically transporting chemical or nuclear fuels. Possible artificial fuels include radioactive isotopes, wood alcohol, grain alcohol, methane, synthetic gas, hydrogen gas (H), cryogenic gas, and liquefied natural gas (LNG).\n\n"}
{"id": "13580475", "url": "https://en.wikipedia.org/wiki?curid=13580475", "title": "Pylons of Pearl River Crossing", "text": "Pylons of Pearl River Crossing\n\nThe Pylons of Pearl River Crossing is a 500 kV power line crossing over the Pearl River in China's Guangdong Province. The power lines are suspended from tall towers.\n\n"}
{"id": "1430672", "url": "https://en.wikipedia.org/wiki?curid=1430672", "title": "Rebaptism (Mormonism)", "text": "Rebaptism (Mormonism)\n\nRebaptism is a practice of in the Latter Day Saint movement.\n\nIn late 1839, the Church of Jesus Christ of Latter Day Saints (by an 1838 revelation) was relocated to Nauvoo, Illinois. Many who were already baptized members of the church, were rebaptised either to show a renewal of their commitment to the movement or as part of a healing ordinance.\n\nThe Church of Jesus Christ of Latter-day Saints does not recognize baptisms performed by any other denomination. All converts to the LDS Church must be baptized under the direction of local church leaders. In this sense, the LDS Church practices rebaptism.\n\nIn addition, while scripture makes it clear that baptism is necessary for salvation, early church leaders noticed that there is no scriptural prohibition against being baptized more than once. Members would often be rebaptized before serving missions or when getting married to show determination to follow the gospel during their mission or marriage.\n\nAfter the death of Joseph Smith, the founder of the Latter Day Saint movement, in 1844, rebaptism became a more important ordinance in The Church of Jesus Christ of Latter-day Saints (LDS Church), as led by Brigham Young. Young led his group to the Great Basin in what is now Utah, and most of his followers were rebaptised soon after arriving as a sign that they would rededicate their lives to Christ. During the \"Mormon Reformation\" of 1856–57, rebaptism became an extremely important ordinance, signifying that the church member confessed their sins and would live a life of a Latter-day Saint. Church members were rebaptized prior to new covenants and ordinances, such as ordination to a new office of the priesthood, receiving temple ordinances, getting married, or entering plural marriage.\n\nCurrent church policy prohibits rebaptism for these purposes. Rebaptism of somebody already known to have been baptized according to LDS doctrine is practiced only when a previously excommunicated member rejoins the church. In such cases, the wording of the ordinance is identical to that of the first baptismal ordinance.\n\nAmong the Latter Day Saints who remained in the Midwest, rebaptism generally has been practiced only when an excommunicate rejoins the church. When Joseph Smith III and his mother Emma Hale Smith Bidamon joined with the \"New Organization\" of the church in 1860, their original baptisms were considered sufficient. This organization, now known as the Community of Christ, occasionally cited their avoidance of rebaptism as proof that theirs was the true continuation of the original Latter Day Saint church.\n\n\n"}
{"id": "2531433", "url": "https://en.wikipedia.org/wiki?curid=2531433", "title": "Regional Greenhouse Gas Initiative", "text": "Regional Greenhouse Gas Initiative\n\n'The Regional Greenhouse Gas Initiative (RGGI, pronounced \"Reggie\") is the first mandatory market based program in the United States to reduce greenhouse gas emissions. RGGI is a cooperative effort among the states of Connecticut, Delaware, Maine, Maryland, Massachusetts, New Hampshire, New Jersey, New York, Rhode Island, and Vermont to cap and reduce carbon dioxide (CO) emissions from the power sector. RGGI compliance obligations apply to fossil-fueled power plants 25MW and larger within the ten-state region.\n\nRGGI establishes a regional cap on the amount of CO pollution that power plants can emit by issuing a limited number of tradable CO allowances. Each allowance represents an authorization for a regulated power plant to emit one short ton of CO. Individual CO budget trading programs in each RGGI state together create a regional market for CO allowances.\n\nThe RGGI states distribute over 90 percent of allowances through quarterly auctions. These allowance auctions generate proceeds, which participating states are able to invest in strategic energy and consumer benefit programs. Programs funded through RGGI have included energy efficiency, clean and renewable energy, greenhouse gas abatement, and direct bill assistance.\n\nAn initial milestone program's development occurred in 2005, when seven states signed a Memorandum of Understanding (MOU) announcing an agreement to implement RGGI. The RGGI states then established individual CO budget trading programs, based on the RGGI Model Rule. The first pre-compliance RGGI auction took place in September 2008, and the program became effective on January 1, 2009. The RGGI program is currently in its fourth three-year compliance period, which began January 1, 2018.\n\nRGGI states have reduced their carbon emissions while still experiencing economic growth. Power sector carbon emissions in the RGGI states have declined by over 40% since 2005, while state economies have grown 8%. Media have reported on RGGI's success as a nationally relevant example showing that economic growth can coincide with pollution reductions. In a report on RGGI, the Congressional Research Service has also said that \"experiences in RGGI may be instructive for policymakers seeking to craft a national program.\n\nWhile multiple factors contribute to emissions trends, a 2015 peer-reviewed study found that RGGI has contributed significantly to the decline in emissions in the nine-state region. Alternate factors considered by the study included state Renewable Portfolio Standard (RPS) programs, economic trends, and natural gas prices.\n\nOther independent reports have analyzed RGGI's economic impact. For example, two reports by the Analysis Group studied RGGI's first and second three-year compliance periods. They found that the effects of RGGI's first three years are generating in $1.6 billion in net economic benefit and 16,000 job-years, and RGGI's second three years are generating $1.3 billion in net economic benefit and 14,700 job-years. These figures do not include co-benefits such as public health improvements or avoided climate change impacts.\n\nA Clean Air Task Force (CATF) study investigated public health benefits arising from the RGGI states' shift to cleaner power generation. The study found that the RGGI states transition to cleaner energy is saving hundreds of lives, preventing thousands of asthma attacks and reducing medical impacts and expenses by billions of dollars.\n\nThe RGGI CO cap represents a regional budget for CO emissions from the power sector. The RGGI states include two interim adjustments to the RGGI cap to account for banked CO allowances. The cap declines 2.5 percent each year until 2020. Initial reductions were planned as follows:\n\nThe RGGI caps and adjusted caps for the years 2014-2020 are as follows:\nThe RGGI states also established a Cost Containment Reserve (CCR) of CO allowances that creates a fixed additional supply of CO allowances that are only available for sale if CO allowances prices exceed certain price levels - $4 in 2014, $6 in 2015, $8 in 2016, and $10 in 2017, rising by 2.5 percent each year thereafter. The CCR was 5 million CO allowances in 2014, and 10 million CO allowances each year thereafter.\n\nIn August 2017, RGGI states \"agreed to reduce power plant emissions by another 30 percent from 2020 to 2030. The plan, which must be approved by each state...would lower these emissions by more than 65 percent since 2009, when the states began setting annual caps.\" The RGGI began negotiating this move before the 2016 presidential election. The new caps for 2021-2030 are as follows:\n\nRGGI compliance obligations apply to fossil-fueled power plants 25MW and larger within the RGGI region. As of 2016, there were 163 such covered sources.\n\nUnder RGGI, sources are required to possess CO allowances equal to their CO emissions over a three-year control period. A CO allowance represents a limited authorization to emit one ton of CO. The first three-year control period took effect on January 1, 2009 and extended through December 31, 2011. The second three-year control period took effect on January 1, 2012 and extended through December 31, 2014. The third three-year period took effect on January 1, 2015 and extends through December 31, 2017.\n\nAs of June 2015, 96 percent of regulated power plants had met their compliance obligations for the second control period.\n\nThe first pre-compliance auction of RGGI CO allowances took place in September 2008. Regional auctions are held on a quarterly basis and are conducted using a sealed-bid, uniform price format. Since 2008, the RGGI states have held 31 auctions generating over $2.4 billion in proceeds. Auction clearing prices have ranged from $1.86 to $7.50.\n\nAny party can participate in the RGGI CO allowance auctions, provided they meet qualification requirements, including provision of financial security. Auction rules limit the number of CO allowances that associated entities may purchase in a single auction to 25 percent of the CO allowances offered for sale in that auction.\n\nThe RGGI auctions are monitored by an independent market monitor, Potomac Economics. Potomac Economics monitors the RGGI allowance market in order to protect and foster competition, as well as to increase the confidence of participants and the public in the allowance market. The independent market monitor has found no evidence of anti-competitive conduct, and no material concerns regarding the auction process, barriers to participation in the auctions, competitiveness of the auction results, or the competitiveness of the secondary market for RGGI CO allowances.\n\nMarket participants can also obtain CO allowances in secondary markets, such as the Intercontinental Exchange (ICE), or in over-the-counter transactions. The independent market monitor provides quarterly reports on the secondary market for RGGI allowances.\n\nThe RGGI states have discretion over how they invest RGGI auction proceeds. They have reinvested proceeds, generated by RGGI auctions in a wide variety of programs. Programs funded through RGGI investment in energy efficiency, renewable energy, direct bill assistance, and greenhouse gas abatement have benefited more than 3.7 million participating households and 17,800 participating businesses. These investments have saved participants money on their energy bills, created jobs, and reduced pollution. Over their lifetime, programs funded by RGGI investments will avoid the use of 11.5 million MWh of electricity, 48.7 million MMBtu of fossil fuel, and the release of 10.3 million short tons of carbon dioxide.\n\nEnergy efficiency represents a large portion of RGGI investments. Ultimately, all electricity consumers, not only those who make upgrades, benefit from energy efficiency programs. For example, investing in efficiency programs - such as weatherizing houses - reduces the amount of electricity used. The decrease in electricity demand actually reduces the overall price of electricity. That means the costs go down for everyone, not just someone who installed new, efficient windows.\n\nThe RGGI participating states have committed to comprehensive, periodic program review to consider program successes, impacts, and design elements. The RGGI states are currently undergoing a 2016 Program Review, which includes regularly scheduled public stakeholder meetings.\n\nThe previous RGGI Program Review took place in 2012, and resulted in several updates to the program. These changes included a 45 percent reduction in the RGGI cap, and the introduction of the Cost Containment Reserve (CCR). The CCR and the reduced cap took effect in 2014.\n\nIn 2003, governors from Connecticut, Delaware, Maine, Massachusetts, New Hampshire, New Jersey, New York, Rhode Island, and Vermont began discussions to develop a regional cap-and-trade program addressing carbon dioxide emissions from power plants.\n\nOn December 20, 2005, seven of those states announced an agreement to implement the Regional Greenhouse Gas Initiative, as outlined in a Memorandum of Understanding (MOU) signed by the Governors of Connecticut, Delaware, Maine, New Hampshire, New Jersey, New York, and Vermont. The MOU, as amended, provides the outlines of RGGI, including the framework for a Model Rule.\n\nIn August 2006, the original seven MOU signatory states published a Model Rule, which provided a regulatory framework for the development of individual state regulatory and/or statutory proposals. The model set of regulations detailed the proposed program, as outlined in the MOU.\n\nIn early 2007, Massachusetts and Rhode Island, which had participated in the early development of RGGI, signed the MOU, as did Maryland later that year.\n\nThrough statuses or regulations based on the RGGI Model Rule, each state established individual CO Budget Trading Programs. Together, these composed a regional cap and market for allowances. Each state's CO Budget Trading Programs limits emissions of CO from electric power plants, issues CO allowances, and establishes participation in regional CO allowance auctions.\n\nThe first compliance period for each state's linked CO Budget Trading Program began January 1, 2009.\nOn November 29, 2011, New Jersey withdrew from the MOU, effective January 1, 2012. Groups such as Acadia Center have since reported on lost revenue resulting from New Jersey's departure, and argued for renewed participation. After the election of governor Phil Murphy in 2017, New Jersey began to make preliminary moves to rejoin RGGI. New Jersey reentered the RGGI under an executive order on January 29, 2018.\n\nAfter the election of Ralph Northam in the 2017 Virginia gubernatorial election Virginia began to make preliminary moves to join RGGI.\n\n\n"}
{"id": "4825691", "url": "https://en.wikipedia.org/wiki?curid=4825691", "title": "Reverse osmosis plant", "text": "Reverse osmosis plant\n\nA reverse osmosis plant is a manufacturing plant where the process of reverse osmosis takes place. An average modern reverse osmosis plant needs six kilowatt-hours of electricity to desalinate one cubic metre of water. The process also results in an amount of salty briny waste. The challenge for these plants is to find ways to reduce energy consumption, use sustainable energy sources, improve the process of desalination and to innovate in the area of waste management to deal with the waste. Self-contained water treatment plants using reverse osmosis, called reverse osmosis water purification units, are normally used in a military context.\n\nIn 1977 Cape Coral, Florida became the first municipality in the United States to use the RO process on a large scale with an initial operating capacity of 11,356 cubic metres (3 million gallons) per day. By 1985, due to the rapid growth in population of Cape Coral, the city had the largest low pressure reverse osmosis plant in the world, capable of producing 56,782 cubic metres (15 million gallons) per day.\n\nIn Israel at Ashkelon on the Mediterranean coast, the world's largest reverse osmosis plant is producing 396,000 cubic metres of water a day at around possibly $0.50 USD per cubic metre.\n\nIn western Saudi Arabia at Yanbu, production started in 1999 at 106,904 cubic metres of water a day. Later in 2009 with some expansion the production reached to 132,000 cubic metres of water a day.\n\nIn Sindh Province Pakistan the provincial government has installed 382 reverse osmosis plants in the province out of which 207 are installed in backward areas of Sindh which includes districts of Thar, Thatta, Badin, Sukkur, Shaheed, Benazirabad, Noshero, Feroz, and others while 726 are on the final stage of their completion.\n\nIn China a desalination plant was planned for Tianjin in 2010, to produce 100,000 cubic metres of desalinated seawater a day. In Spain in 2004, 20 reverse osmosis plants were planned to be built along the Costas, expecting to meet slightly over 1% of Spain's total water needs.\n\n\n"}
{"id": "33192846", "url": "https://en.wikipedia.org/wiki?curid=33192846", "title": "Rickshaw", "text": "Rickshaw\n\nA Rickshaw originally denoted a two or three-wheeled passenger cart, now known as a pulled rickshaw, which is generally pulled by one man carrying one passenger. The first known use of the term was in 1879. Over time, cycle rickshaws (also known as pedicabs or trishaws), auto rickshaws, and electric rickshaws were invented, and have replaced the original pulled rickshaws, with a few exceptions for their use in tourism.\n\nPulled rickshaws created a popular form of transportation, and a source of employment for male laborers, within Asian cities in the 19th century. Their appearance was related to newly acquired knowledge of ball-bearing systems. Their popularity declined as cars, trains and other forms of transportation became widely available.\n\nAuto rickshaws are becoming more popular in some cities in the 21st century as an alternative to taxis because of their low cost.\n\"Rickshaw\" originates from the Japanese word \"jinrikisha\" (, \"jin\" = human, \"riki\" = power or force, \"sha\" = vehicle), which literally means \"human-powered vehicle\".\n\nRickshaws were invented in Japan circa 1869, after the lifting of a ban on wheeled vehicles from the Tokugawa period (1603–1868), and at the beginning of a rapid period of technical advancement in Japan.\n\nThere are lots of theories about the inventor, with the most likely and widely accepted theory describing the Rickshaw as having been invented in Japan in 1869, by Izumi Yosuke, who formed a partnership with Suzuki Tokujiro and Takayama Kosuke to build the vehicles, having been \"inspired by the horse carriages that had been introduced to the streets of Tokyo a few years earlier\".\n\nOther theories about the inventor of the Rickshaw include:\n\nJapan historian Seidensticker wrote of the theories:\nThough the origins of the rickshaw are not entirely clear, they seem to be Japanese, and of Tokyo specifically. The most widely accepted theory offers the name of three inventors, and gives 1869 as the date of invention.\n\nThe vehicle had a wooden carriage that rode on \"superior Western wheels\" and was a dramatic improvement over earlier modes of transportation. Whereas the earlier sedan chairs required two people, the rickshaw generally only required one. More than one person was required for hilly or mountainous areas. It also provided a smoother ride for the passenger. Other forms of vehicles at the time were drawn by animals or were wheelbarrows.\n\nThe Powerhouse Museum in Sydney, Australia, has had a rickshaw in its collection for over 120 years. It was made about 1880 and is described as:\nA rickshaw, or Jinrikisha, is a light, two-wheeled cart consisting of a doorless, chairlike body, mounted on springs with a collapsible hood and two shafts. Finished in black lacquer-ware over timber, it was drawn by a single rickshaw runner.\n\nIn the Late 19th century, hand-pulled Rickshaws became an inexpensive, popular mode of transportation across Asia. Peasants who migrated to large Asian cities often worked first as a rickshaw runner. It was \"the deadliest occupation in the East, [and] the most degrading for human beings to pursue.\"\n\nStarting in 1870, the Tokyo government issued a permit to build and sell 人力車　(jinrikisha : rickshaws in Japanese) to the trio that are believed in Asia to be the rickshaw's inventors: Izumi Yosuke, Takayama Kosuke, and Suzuki Tokujiro. In order to operate a rickshaw in Tokyo, a seal was required from these men. By 1872, they replaced the kago and norimono, becoming the main mode of transportation in Japan, with about 40,000 rickshaws in service. At that time man-power was much cheaper than horse-power; horses were generally only used by the military. Some of the rickshaws were artistically decorated with paintings and rear elevations. In this time, the more exuberant styles of decorations were banned. If the families were well-off financially they might have their own rickshaw runner. Generally, runners covered in a day, at an average traveling speed of per hour.\n\nJapanese rickshaw manufacturers produced and exported rickshaws to Asian countries and South Africa.\n\nSingapore received its first rickshaws in 1880 and soon after they were prolific, making a \"noticeable change in the traffic on Singapore's streets.\" Bullock carts and gharries were used before rickshaws were introduced.\n\nMany of the poorest individuals in Singapore in the late nineteenth century were poverty-stricken, unskilled people of Chinese ancestry. Sometimes called coolies, the hardworking men found that pulling a rickshaw was a new opportunity for employment.\n\nIn 1897, martial law was declared to end a four-day rickshaw workers' strike.\n\nIn China, the rickshaw was first seen in 1873 and was used for public transportation the following year. Within a year there were 10,000 rickshaws in operation. Around 1880 rickshaws appeared in India, first introduced in Simla by Reverend J. Fordyce. At the turn of the century they were introduced in Calcutta, India, and by 1914 were a conveyance for hire. The rickshaw was also introduced to Korea in the late 19th century.\n\nAfter World War II, there was a major shift in the use of man-powered rickshaws:\nHand-pulled rickshaws became an embarrassment to modernizing urban elites in the Third World, and were widely banned, in part because they were symbolic, not of modernity, but of a feudal world of openly marked class distinctions. Perhaps the seated rickshaw passenger is too close to the back of the laboring driver, who, besides, is metaphorically a draught animal harnessed between shafts.\n\nThe cycle rickshaw was built in the 1880s and was first used with regularity starting in 1929 in Singapore. They were found in every south and east Asian country by 1950. By the late 1980s there were estimated 4 million cycle rickshaws in the world.\n\nRickshaws were introduced to Durban, South Africa, and by 1904 there were about 2,000 registered rickshaw pullers.\n\nThe rickshaw's popularity in Japan had declined by the 1930s with the advent of automated forms of transportation like automobiles and trains. After World War II, when gasoline and automobiles were scarce, they made a temporary come-back. The rickshaw tradition has stayed alive in Kyoto and Tokyo's geisha districts. In the 1990s, German-made cycle rickshaws called \"velotaxis\" were introduced in Japanese cities, including Kobe.\n\nIn China, the rickshaw's popularity began to decline in the 1920s and particularly as a mode of passenger transportation by the 1950s. A rough form of a rickshaw is sometimes used for hauling coal, building materials or other material. Both motorized and pedal-power cycle rickshaws, or pedicabs, were used for short distance passenger travel. There are still many rickshaws in many cities for either touring purposes (in big cities such as Beijing and Shanghai, with traditional Chinese rickshaws) or short range transportation in some counties.\n\nIn Singapore, the rickshaw's popularity increased into the 20th century. There were approximately 50,000 rickshaws in 1920 and that number had doubled by 1930. Cycle rickshaws were used in Singapore beginning in 1929. Within six years pulled rickshaws were outnumbered by cycle rickshaws, which were also used by sightseeing tourists.\nIn the 1930s, cycle rickshaws were used in Kolkata, India; Jakarta, Indonesia; and Dhaka, Bangladesh. By 1950 they could be found in many South and East Asian countries. By the end of the 20th century, there were 300,000 such vehicles in Dhaka. By the end of 2013, there were about 100,000 electric rickshaws in Delhi.\n\nPedicabs were introduced in North America in 1962, where they were a means of transportation at the Seattle World's Fair in the state of Washington.\n\nThe 21st century has seen a resurgence in rickshaws, particularly in motorized rickshaws and cycle rickshaws. Auto rickshaws, also called velotaxis, have resurged as they are about 1/3 to 1/2 the cost of regular taxis. German velotaxis are three-wheeled, powered vehicles with a space for a driver and, behind the driver, space for two passengers. Cycle rickshaws are used in many North American, European, and Asian cities. They are increasingly being used as an eco-friendly way of short-range transportation, particularly in urban areas. Along with auto rickshaws, they are also used (particularly by Asian cities) for tourism, because of their \"novelty value as an entertaining form of transportation\". \n\nIn Madagascar, pulled cycle and auto rickshaws are a common form of transportation in a number of cities, especially Antsirabe. They are known as \"pousse-pousse\", meaning \"push-push\".\n\nMacau - Still uses tri-wheels bicycle rickshaw, or Riquexó in Portuguese, as Macau was a Portuguese colony in the past, and this kind of transportation were very famous till late 20th century, due to the fact of being a small city and few cars, not so much motorcycle, very bad public transport and none other transport such as train or subway.\nYou can go around Macau peninsula and the two island on rickshaw, and visit the Riquexó Museum and see the evolution of rickshaw since 18th till nowadays.\n\nAutomated cycle rickshaws, called velotaxis, are popular in Kyoto and Tokyo, Japan. Their use is growing at a rate of about 20-30% a year in Japanese cities. The traditional rickshaws are still alive for travelers in some tourist places in Japan. Rickshaws are found in Hong Kong. In China, automated and pedal-power cycle rickshaws, or pedicabs, are used for short distance passenger travel in large cities and many medium-sized cities. Most Indian cities offer auto rickshaw service; Hand-pulled rickshaws do exist in some areas, such as Kolkata (Calcutta) as a part of their transport system which also includes cycle rickshaws.\n\nIn Australia, Cycle rickshaws or trishaws (3 wheels) are used in Melbourne and St Kilda. They are also seen in Cowaramup, Western Australia at Bakehouse '38.\n\nCycle rickshaws or trishaws (3 wheels) are used in most large continental European cities, such as:\nWithin the United Kingdom, pedicabs operate in:\n\n\n\nSpain: Barcelona and Valencia\n\nTypes of rickshaws include:\n\n\nDesigner\n"}
{"id": "44716981", "url": "https://en.wikipedia.org/wiki?curid=44716981", "title": "SAL electrolytic capacitor", "text": "SAL electrolytic capacitor\n\nSAL electrolytic capacitors (SAL = Solid Aluminum) are aluminum electrolytic capacitors with anodic oxidized aluminum oxide as dielectric and with the semiconducting solid manganese dioxide as electrolyte. They are made of etched and formed aluminum anodes, which are folded for the dipped pearl types or wound for the axial style. The solid manganese dioxide electrolyte is analogously prepared in a pyrolytic process like for solid tantalum capacitors.\n\nSAL-capacitors were developed and introduced in the market in the 1960s by Philips. Up until December 30, 2015, it was a single source product manufactured by Vishay. As of December 31, 2015 these are now end-of-life and have ceased production.\n\nBasic anode material of solid aluminum capacitors exists of highly purified aluminum with a purity of at least 99.99%. In an electrochemical process the anode material is etched (roughened) to increase the effective electrode surface. After that the roughened aluminum becomes oxidized or formed by an anodic oxidizing process. Thereby an electrical insulating oxide layer AlO is formed on the aluminum surface by applying an electric current in correct polarity in an electrolytic bath.\n\nThis process of oxide formation is carried out into two reaction steps:\n\n\nThe aluminum oxide layer acts as a dielectric. After forming the dielectric the aluminum foils are folded for the dipped style or wound for the axial style, and than provided with electrolyte, the capacitors cathode. The electrolyte used in SAL capacitors is a solid state oxide semiconductor, manganese dioxide (MnO2). This electrolyte is formed by pyrolysing of the liquid manganese nitrate into the solid manganese dioxide.\n\n\nAfter the pyrolising process the capacitor cell is reformed again to heal all impurities or cracks caused during the pyrolising process.\n\nManganese dioxide is a hard, black crystalline substance. It has a fairly good electrical conductivity and has an excellent long-term stability. In an ideal case it covers 100% of the dielectric layer and acts as a solid cathode in the solid electrolytic capacitor.\n\nFor contact purposes, a layer of carbon from a graphite dispersion is put on the MnO2 coating on the surface of the capacitor cell. Onto this a metallic conductive silver epoxy lacquer is applied. The graphite layer also prevents a direct contact between manganese dioxide and silver. Direct contact between these two materials forces a chemical reaction which oxidizes the silver and reduces manganese dioxide into high resistive manganese (III) oxide resulting in increasing ESR of the capacitor. This silver layer now can be contacted with the cathode terminal of the capacitor.\n\nSolid aluminum electrolytic capacitors have no known inherent wear-out failure mechanism. In addition the solid electrolyte offers a very long time stability of the electrical and thermal characteristics. They remain constant throughout a very long time without time-depending changes. The dependence of the impedance and ESR at lower temperatures is very low compared with non-solid electrolytes. The capacitors are insensible against high inrush or switch-off currents and can be operated without a series resistor, whereby the SAL electrolytic capacitors at high current loads have a much higher reliability with respect to tantalum electrolytic capacitors. In addition, the dielectric aluminum oxide in combination with the electrolyte manganese dioxide has a relatively high voltage resistance against wrong polarity.\n\nSAL electrolytic capacitors are used for filtering, smoothing coupling and decoupling applications in industrial, medical and automotive equipment. \nThe axial style of SAL capacitors has military and high professional applications up to 200 °C.\n\nCompared to non-solid electrolytic capacitors, SAL capacitors\n\nCompared to polymer electrolytic capacitors, SAL capacitors\n\nThe standardization for all electrical, electronic components and related technologies follows the rules given by the International Electrotechnical Commission (IEC), a non-profit, non-governmental international standards organization. The definition of the characteristics and the procedure of the test methods for aluminum electrolytic capacitors for use in electronic equipment are set out in the generic specification:\n\nThe tests and requirements to be met by capacitors for use in electronic equipment for approval as standardized types are set out in the following sectional specifications\n\n"}
{"id": "2426926", "url": "https://en.wikipedia.org/wiki?curid=2426926", "title": "Snowsquall", "text": "Snowsquall\n\nA snowsquall (or snow squall) is a sudden moderately heavy snow fall with blowing snow and strong, gusty surface winds. It is often referred to as a whiteout and is similar to a blizzard but is localized in time or in location and snow accumulations may or may not be significant.\n\nThere are two primary types of snowsqualls: lake effect and frontal.\n\nWhen arctic air moves over large expanses of warmer open waters in winter, convective clouds develop which cause heavy snow showers due to the large amount of moisture available. This occurs southwest of extratropical cyclones, with the curved cyclonic wind flow bringing cold air across the relatively warm Great Lakes which then leads to narrow lake-effect snow bands that can produce significant localized snowfall. Whiteout conditions will affect narrow corridors from shores to inland areas aligned along the prevailing wind direction. This will be enhanced when the moving air mass is uplifted by higher elevations. The name originates from the Great Lakes area of North America, however any body of water can produce them. Regions in lee of oceans, such as the Canadian Maritimes could experience such snowsqualls.\n\nThe areas affected by lake-effect snow are called snowbelts and deposition rate of many inches (centimetres) of snow per hour are common in these situations. In order for lake-effect snow to form, the temperature difference between the water and 850 mbar should be at least 23 °F (13 °C), surface temperature be around the freezing mark, the lake unfrozen, the path over the lake at least 100 km, and the directional wind shear with height should be less than 30° from the surface to 850 millibars. Extremely cold air over still warm water in early winter can even produce thundersnow, snow showers accompanied by lightning and thunder.\n\nA frontal snowsquall is an intense frontal convective line (similar to a rainband), when temperature is near freezing at the surface. The strong convection that develops has enough moisture to produce whiteout conditions at places which line passes over as the wind causes intense blowing snow. This type of snowsquall generally lasts less than 30 minutes at any point along its path but the motion of the line can cover large distances. Frontal squalls may form a short distance ahead of the surface cold front or behind the cold front in situations where there are other contributing factors such as dynamic lifting from a deepening low pressure system or a series of trough lines which act similar to a traditional cold frontal passage. In situations where squalls develop post-frontally it is not unusual to have two or three linear squall bands pass in rapid succession only separated by 25 miles (40 kilometers) with each passing the same point in roughly 30 minutes apart.\n\nThis is similar to a line of thunderstorms in the summer but the tops of the clouds are only 5,000 to 10,000 feet (1,500 to 3,000 m), often difficult to see on radar. Forecasting these types of events is equivalent to summer severe weather forecast for squall lines: presence of a sharp frontal trough with wind shift and low level jet of more than 30 knots (55.58 km/h). However, the cold dome behind the trough is at 850 mbar instead of a higher level and must be at least -13 °F (-25 °C). The presence of surface moisture from bodies of water or preexisting liquid precipitation is also a significant contributing factor helping to raise the dew point temperature and saturate the boundary layer. This saturate can significantly increase the amount of convective available potential energy leading to deeper vertical growth and higher precipitable water levels increasing the volume of snow which can be produced by the squall. In cases where there is a large amount of vertical growth and mixing the squall may develop embedded cumulonimbus clouds resulting in lightning and thunder which is dubbed thundersnow.\n\nBoth types of snowsqualls are very dangerous for motorists and airplanes or generally any traveler unfortunate enough to get stuck in one. The change in conditions is very sudden, and slippery conditions and abrupt loss of visibility due to whiteouts often cause multiple-vehicle collisions. In the case of lake effect snow, heavy amounts of snow can accumulate in short periods of time, possibly causing road closures and paralyzing cities. For instance, on January 9, 2015, a localized, heavy snow squall caused a 193-vehicle pile-up on I-94 highway near Galesburg, Michigan.\n\n\nWarnings about lake effect snow:\n\n\n\n"}
{"id": "33533975", "url": "https://en.wikipedia.org/wiki?curid=33533975", "title": "Soil resistivity", "text": "Soil resistivity\n\nSoil resistivity is a measure of how much the soil resists the flow of electricity. It is a critical factor in design of systems that rely on passing current through the Earth's surface. An understanding of the soil resistivity and how it varies with depth in the soil is necessary to design the grounding system in an electrical substation, or for lightning conductors. It is needed for design of grounding (earthing) electrodes for substations and High-voltage direct current transmission systems. It can also be a useful measure in agriculture as a proxy measurement for moisture content.\n\nIn most substations the earth is used to conduct fault current when there are ground faults on the system. In single wire earth return power transmission systems, the earth itself is used as the path of conduction from the end customers (the power consumers) back to the transmission facility. \nIn general there is some value above which the impedance of the earth connection must not rise, and some maximum step voltage which must not be exceeded to avoid endangering people and livestock.\n\nThe soil resistivity value is subject to great variation, due to moisture, temperature and chemical content. Typical values are:\n\nThe SI unit of resistivity is the Ohm-meter (Ω-m); in the United States the Ohm-centimeter (Ω-cm) is often used instead. One Ω-m is 100 Ω-cm. Sometimes the conductivity, the reciprocal of the resistivity, is quoted instead.\n\nA wide range of typical soil resistivity values can be found in literature. Military Handbook 419 (MIL-HDBK-419A) contains reference tables and formulae for the resistance of various patterns of rods and wires buried in soil of known resistivity. Being copyright free, these numbers are widely copied, sometimes without acknowledgement.\n\nBecause soil quality may vary greatly with depth and over a wide lateral area, estimation of soil resistivity based on soil classification provide only a rough approximation. Actual resistivity measurements are required to fully qualify the resistivity and its effects on the overall transmission system.\n\nSeveral methods of resistivity measurement are frequently employed:\n\nThe Wenner four-pin method, as shown in figure above, is the most commonly used technique for soil resistivity measurements.\nUsing the Wenner method, the apparent soil resistivity value is:\n\nwhere\n\nρ = measured apparent soil resistivity (Ωm)\n\na = electrode spacing (m)\n\nb = depth of the electrodes (m)\n\nR = Wenner resistance measured as \"V/I\" in Figure (Ω)\nIf b is small compared to a, as is the case of probes penetrating the ground only for a short distance (as normally happens), the previous equation can be reduced to:\n\nIn the Schlumberger method the distance between the voltages probe is a and the distances from voltages probe and currents probe are c (see figure above).\n\nUsing the Schlumberger method, if b is small compared to a and c, and c>2a, the apparent soil resistivity value is:\n\nwhere\n\nρ = measured apparent soil resistance (Ωm)\n\na = electrode spacing (m)\n\nb = depth of the electrodes (m)\n\nc = electrode spacing (m)\n\nR = Schlumberger resistance measured as \"V/I\" in Figure (Ω)\n\nThe conversion between values measured using the Schlumberger and Wenner methods is possible only in an approximate way. In any cases, for both Wenner and Schlumberger methods the electrode spacing between the currents probe corresponds to the depth of soil investigation and the measured apparent soil resistivity is referred to a soil volume as in the figure.\n\nThe current tends to flow near the surface for small probe spacing, whereas more current penetrates deeper into the soil for large spacing. The resistivity measured for a given current probe spacing represents, to a first approximation, the apparent resistivity of the soil to a depth equal to that spacing.\n\nIf the apparent soil resistivity measured with Schlumberger method ρ (with the corresponding electrode spacing a and c) is given, assuming that the soil resistivity refers to a volume as in the figure with a=L/3 follows:\n\nwith\n\nwhere:\n\nR = equivalent Wenner resistance (Ω)\n\na = equivalent electrode spacing with Wenner method (m)\n\na = electrode spacing between voltages probe with Schlumberger method (m)\n\nc = electrode spacing between voltages and currents probe with Schlumberger method (m)\n\nIf the measured Schlumberger resistance is given, before calculating the apparent soil resistivity the following factor must be calculated:\n\nThe Wenner method is the most widely used method for measuring soil resistivity for electrical grounding (earthing) purposes. The Schlumberger method was developed to increase the voltage signal for the earlier, less sensitive instruments, by placing the potential probes closer to the current probes.\n\nThe soil resistivity measurements will be affected by existing nearby grounded electrodes. Buried conductive objects in contact with the soil can invalidate readings made by the methods described if they are close enough to alter the test current flow pattern. This is particularly true for large or long objects.\n\nElectrical conduction in soil is essentially electrolytic and for this reason the soil resistivity depends on:\n\nBecause of the variability of soil resistivity, IEC standards require that the seasonal variation in resistivity be accounted for in transmission system design. Soil resistivity can increase by a factor of 10 or more in very cold temperatures. \n\nSoil resistivity is one of the driving factors determining the corrosiveness of soil. The soil corrosiveness is classified based on soil electrical resistivity by the British Standard BS-1377 as follow:\n\n"}
{"id": "3956140", "url": "https://en.wikipedia.org/wiki?curid=3956140", "title": "Stearidonic acid", "text": "Stearidonic acid\n\nStearidonic acid (SDA) is an ω-3 fatty acid, sometimes called moroctic acid. It is biosynthesized from alpha-linolenic acid by the enzyme delta-6-desaturase. Natural sources of this fatty acid are the seed oils of hemp, blackcurrant, corn gromwell and echium (although the plant is a source of stearidonic acid, it is toxic for human consumption), and the cyanobacterium \"Spirulina\".\n\n"}
{"id": "1657562", "url": "https://en.wikipedia.org/wiki?curid=1657562", "title": "Superga air disaster", "text": "Superga air disaster\n\nThe Superga air disaster occurred on 4 May 1949, when a Fiat G.212 of Avio Linee Italiane (Italian Airlines), carrying the entire Torino football team (popularly known as the \"Grande Torino\"), crashed into the retaining wall at the back of the Basilica of Superga, which stands on a hill on the outskirts of Turin. Thirty-one people died; there were no survivors.\n\nThe Avio Linee Italiane Fiat G.212CP was carrying the team home from Lisbon, where they had played a friendly match with S.L. Benfica in honour of the Portuguese captain, Francisco Ferreira. In the incident, the whole Torino team (almost all of the Italy national football team) lost their lives. Club officials and carriers also perished in the accident, as well as the crew and three well-known Italian sports journalists: Renato Casalbore (founder of \"Tuttosport\"); Renato Tosatti (the \"Gazzetta del Popolo\", father of Giorgio Tosatti), and Luigi Cavallero (\"La Stampa\"). The task of identifying the bodies was entrusted to the former manager of the Italy national team, Vittorio Pozzo, who had called up most of Torino's players to the \"Azzurri\".\n\nThe full-back, Sauro Tomà, did not take part in the trip due to an injured meniscus, nor did the reserve goalkeeper, Renato Gandolfi (the third goalkeeper, Dino Ballarin, took his place). Radio commentator Nicolò Carosio, Luigi Giuliano (captain of the Torino youth team), and former manager of the Italy national team, Vittorio Pozzo, were excluded for various reasons. Torino's president, Ferruccio Novo, did not take part in the trip due to influenza.\n\nTorino were proclaimed winners of the 1948–49 Serie A season on 6 May 1949, and the opponents, as well as Torino, fielded their youth teams in the four remaining games. On the day of the funeral, nearly a million people took to the streets of Turin to give a final farewell to the players. The shock was such that the following year, the Italy national team travelled to the 1950 FIFA World Cup in Brazil by ship.\n\nThe three-engined Fiat G.212, with aircraft registration I-ELCE, of Avio Linee Italiane, took off from Lisbon at 09:40 on Wednesday, 4 May 1949. The commander of the aircraft was Lieutenant Colonel Meroni. The flight landed at the airport in Barcelona at 13:00. While the aircraft was refuelled during the stopover, Torino met for lunch with Milan, who were on their way to Madrid.\n\nAt 14:50, I-ELCE set off for the Turin-Aeritalia Airport. The flight's route was to take it over Cap de Creus, Toulon, Nice, Albenga, and Savona. Above Savona, the plane turned north, in the direction of the capital of Piedmont, where it was expected to arrive in 30 minutes. The weather in Turin was poor; at 16:55, the airport of Aeritalia communicated the weather situation to the pilot - clouds almost touching the ground, showers, strong southwest wind gusts, and very poor horizontal visibility ().\n\nAt this point, the tower asked for the pilot's position. After a few minutes of silence (at 16:59) came the reply: \"Quota . QDM on Pino, then we will cut at Superga\". At Pino Torinese, which is located between Chieri and Baldissero Torinese, southeast of Turin, there was a VDF radio station (VHF direction finder), to provide a QDM (magnetic course to be taken on a head on approach as a radio aid) on request.\n\nOn approach, the plane lined up with the runway at Aeritalia about away, at above sea level, with Pino at 290° off its nose. Just north of Pino Torinese was the Basilica of Superga, situated on a hill at above sea level. One theory for the deviation is that due to the strong left crosswinds, the plane could have suffered a drift to starboard, which shifted from the axis of descent and lined up with the hill of Superga. Recent investigations also suggested the possibility that the altimeter had malfunctioned and locked at , which led the pilots to believe that they were at a higher altitude.\n\nAt 17:03, the plane made a turn to the left, returned to level flight, and had aligned to prepare for landing when it crashed into the back of the embankment of the Basilica of Superga. The pilot, who likely believed that the Superga hill was off to his right, would have seen it suddenly emerge directly in front of him (speed , visibility ) and been unable to react. The wreckage did not give any indication of an attempt to go around. The only part of the aircraft which remained partially intact was the empennage.\n\nAt 17:05, Aeritalia Torre called I-ELCE, but received no response. Of the 31 people on board, none survived. Remains of the aircraft, including a propeller, a tire, scattered pieces of the fuselage, and the personal bags of Mazzola, Maroso, and Erbstein, are preserved in a museum in Grugliasco near Turin. The \"Museo del Grande Torino e della Leggenda Granata\", located in the prestigious Villa Claretta Assandri of Grugliasco, was opened on the 4th of May 2008, the anniversary of the tragedy. Eight of the 18 players (as well as two coaches and the journalist Renato Casalbore) are buried at the \"Cimitero Monumentale\" of Turin.\n\n\n\n\n\n\n\n\n"}
{"id": "12185896", "url": "https://en.wikipedia.org/wiki?curid=12185896", "title": "Vibration-powered generator", "text": "Vibration-powered generator\n\nA vibration powered generator is a type of electric generator that converts the kinetic energy from vibration into electrical energy. The vibration may be from sound pressure waves or other ambient sources.\n\nVibration powered generators usually consist of a resonator which is used to amplify the vibration source, and a transducer mechanism which converts the energy from the vibrations into electrical energy. The transducer usually consists of a magnet and coil or a piezoelectric crystal.\n\nElectromagnetic based generators use Faraday's law of induction to convert the kinetic energy of the vibrations into electrical energy. They consist of magnets attached to a flexible membrane or cantilever beam and a coil. The vibrations cause the distance between the magnet and coil to change, causing a change in magnetic flux and resulting in an electromagnetic force being produced. Generally the coil is made using a diamagnetic material as these materials have weaker interactions with the magnet that would dampen the vibration. The main advantage of this type of generator is that it is able to produce more power than the piezoelectric generators.\n\nA miniature electromagnetic vibration energy generator was developed by a team from the University of Southampton in 2007. This particular device consists of a cantilever beam with a magnet attached to the end. The beam moves up and down as the device is subjected to vibrations from surrounding sources. This device allows sensors in hard to access locations to be powered without electrical wires or batteries that need to be replaced. Sensors in inaccessible places can now generate their own power and transmit data to outside receivers. The generator was developed to be used in air compressors, and is able to power things in high vibration environments like sensors on machinery in manufacturing plants, or sensors that monitor the health of bridges. One of the major limitations of the magnetic vibration energy harvester developed at University of Southampton is the size of the generator. At approximately one cubic centimeter, this device would be much too large to be used in modern electronic devices. Future improvements on the size of the device could make it an ideal power source for medically implanted devices such as pacemakers. According to the team that created the device, the vibrations from the heart muscles would be enough to allow the generator to power a pacemaker. This would eliminate the need to replace the batteries surgically.\n\nIn 2012 a group at Northwestern University developed a vibration-powered generator out of polymer in the form of a spring. This device was able to harvest the energy from vibrations at the same frequencies as the University of Southampton groups cantilever based device, but at approximately one third the size of the other device.\n\nPiezoelectric based generators use thin membranes or cantilever beams made of piezoelectric crystals as a transducer mechanism. When the crystal is put under strain by the kinetic energy of the vibration a small amount of current is produced thanks to the piezoelectric effect. These mechanisms are usually very simple with few moving parts, and they tend to have a very long service life. This makes them the most popular method of harvesting the energy from vibrations. These mechanisms can be manufactured using the MEMS fabrication process, which allows them to be created on a very small scale. The ability to make piezoelectric generators on such a small scale is the main advantage of this method over the electromagnetic generators, especially when the generator is being developed to power microelectronic devices.\n\nOne piezoelectric generator being developed uses water droplets at the end of a piezoelectric cantilever beam. The water droplets hang from the end of the beam and are subjected to excitation by the kinetic energy of the vibrations. This results in the water droplet oscillating, which in turn causes the beam they are hanging from to deflect up and down. This deflection is the strain which is converted to energy through the piezoelectric effect. A major advantage to this method is that it can be tailored towards a wide range of excitation frequencies. The natural frequency of the water droplet is a function of its size; therefore changing the size of the water droplet allows for the matching of the natural frequency of the droplet and the frequency of the pressure wave being converted into electrical energy. Matching these frequencies produces the largest amplitude oscillation of the water droplet, resulting in a large force and larger strain on the piezoelectric beam.\n\nAnother application seeks to use the vibrations created during flight in aircraft to power the electronics on the plane that currently rely on batteries. Such a system would allow for a reliable energy source, and reduce maintenance as batteries would no longer need to be replaced and piezoelectric systems have a long service life. This system is used with a resonator, which allows the airflow to form a high amplitude steady tone. The same principle is used in many wind instruments, converting the airflow provided by the musician into a loud steady tone. This tone is used as the vibration that is converted from kinetic to electric energy by the piezoelectric generator. This application is still in the early stages of development; the concept has been proven on a scale model but the system still needs to be optimized before it is tested on a full scale.\n\n"}
{"id": "17699994", "url": "https://en.wikipedia.org/wiki?curid=17699994", "title": "Zinc ricinoleate", "text": "Zinc ricinoleate\n\nZinc ricinoleate is the zinc salt of ricinoleic acid, a major fatty acid found in castor oil. It is used in many deodorants as an odor-adsorbing agent. The mechanism of this activity is unclear.\n"}
