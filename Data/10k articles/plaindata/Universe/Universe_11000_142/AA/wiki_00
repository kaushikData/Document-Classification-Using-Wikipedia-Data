{"id": "1269816", "url": "https://en.wikipedia.org/wiki?curid=1269816", "title": "Ameralik Span", "text": "Ameralik Span\n\nThe Ameralik Span is the longest span of an electrical overhead power line in the world. It is situated near Nuuk on Greenland and crosses Ameralik Fjord with a span width of 5,376 metres at . It was built in 1993 by the Norwegian company NTE Entreprise (Nord-Trøndelag Elektrisitetsverk) and is part of a single-circuit 132 kV powerline running from Buksefjord hydroelectric power plant to Nuuk.\n\nThe span consists of four steel conductors of 40 millimeters diameter whereby one is a spare conductor. The span has a width of 190 metres and a minimum clearance of 128 metres. The pylons on each side of the span carry only one conductor and are situated on mountains 444 metres high on the north and 1013 metres at the south shore.\n\n\n\n\n"}
{"id": "12636614", "url": "https://en.wikipedia.org/wiki?curid=12636614", "title": "Between Pacific Tides", "text": "Between Pacific Tides\n\nBetween Pacific Tides is a 1939 book by Ed Ricketts and Jack Calvin that explores the intertidal ecology of the Pacific coast of the United States. The book was originally titled \"Between Pacific Tides: An Account of the Habits and Habitats of Some Five Hundred of the Common, Conspicuous Seashore Invertebrates of the Pacific Coast Between Sitka, Alaska, and Northern Mexico\".\n\nPrior to Ricketts' work, the standard descriptive text of intertidal species of the Pacific was Myrtle E. Johnson's \"Seashore Animals of the Pacific Coast\", published in 1927 (repr. 1967). \n\n\"Between Pacific Tides\" was out of print from 1942 to 1948, but it has since been revised and updated to keep it current, and is now in its fifth edition with the size increasing around twenty percent from the original. Updated and expanded sections have been added since the original edition was published, including: John Steinbeck's Foreword to the 1948 edition; a new chapter regarding the influence on the distribution of shore organisms; an updated Annotated Systematic Index and General Bibliography comprising 2,300 entries; and the addition of 200 photographs and drawings.\n\nBy 2004, the book had sold around 100,000 copies, making it one of the best-selling books published by Stanford University Press.\n"}
{"id": "40687961", "url": "https://en.wikipedia.org/wiki?curid=40687961", "title": "Broadlands Dam", "text": "Broadlands Dam\n\nThe Broadlands Dam (also known as the Broadlands Hydropower Project by the developers) is a run-of-the-river hydroelectric complex currently under construction in Kitulgala, Sri Lanka. The project is expected to be completed in 2020, and will consist of two dams, and a power station further downstream.\n\nWith an estimated annual generation capacity of , the facility will be the country's last major hydroelectric project, due to the exhaustion of island-wide hydropower potential. Construction of the project was ceremonially inaugurated by the at the auspicious time of 11:01 on .\n\nApproximately project funding was met via credit arrangements made with the Chinese government, with the rest borne via a loan from the local Hatton National Bank. The construction contract of the project was granted to the China National Electric Equipment Corporation (CNEEC).\n\nThe primary gravity dam measuring in height and in length is being constructed across the Maskeliya Oya at Kitulgala, and will supply water to the power station via a penstock measuring in diameter.\n\nA secondary gravity weir measuring and in height and length, is also to be built in the vicinity, over the nearby Kehelgamu Oya, to provide additional hydroelectric capacity. The weir, to be called the Kehelgamu Weir, will create a catchment area of , and will provide additional head to the penstock of the main dam via a tunnel.\n\nThe penstock from the main dam will feed a power station consisting of two turbines, each of and a rated discharge of .\n\n"}
{"id": "25100797", "url": "https://en.wikipedia.org/wiki?curid=25100797", "title": "Bulgartransgaz", "text": "Bulgartransgaz\n\nBulgartransgaz EAD is a natural gas transmission and storage system operator of Bulgaria. It is a subsidiary of Bulgarian Energy Holding EAD. Bulgartransgaz was created on 15 January 2007 in Sofia. In November 2009, the Bulgarian Government decided to list the company at the Bulgarian Stock Exchange - Sofia.\n\n\n"}
{"id": "208896", "url": "https://en.wikipedia.org/wiki?curid=208896", "title": "Buzz Lightyear of Star Command", "text": "Buzz Lightyear of Star Command\n\nBuzz Lightyear of Star Command is an American animated science fiction/adventure/comedy series produced by Walt Disney Television Animation. The series originally aired on UPN and ABC from October 2000 to January 2001 as part of Disney's One Saturday Morning programming block. It follows the adventures of space ranger Buzz Lightyear, who first appeared in the film \"Toy Story\" as an action figure and one of the film's protagonists.\n\n\"Buzz Lightyear of Star Command\" takes place in the far future, and is a pastiche of \"Star Trek\", \"Battlestar Galactica\", \"Babylon 5\", \"Lost in Space\", \"Flash Gordon\", \"Buck Rogers\", \"Doctor Who\" and \"Star Wars\"-style science fiction. Capital Planet is the forefront of the Galactic Alliance, a peaceful union of various planets, home to various alien species that coexist in harmony with one another. Star Command is a peacekeeping organization consisting of Space Rangers, who investigate threats to galactic peace. The primary enemy of Star Command is the Evil Emperor Zurg, an intergalactic crime boss that rules an empire of heavily armed robots and slave races forced to work in opposition to the Galactic Alliance.\n\nThe series features Buzz Lightyear, a famous, experienced Space Ranger who takes a crew of rookies under his wing as he investigates criminal activity across the galaxy, and attempts to bring down Evil Emperor Zurg once and for all.\n\n\"Buzz Lightyear of Star Command\" is a metafictional series that exists within the reality of the film \"Toy Story\", where Buzz Lightyear is a highly popular toyline. One of the two lead characters in the \"Toy Story\" film series is a Buzz Lightyear action figure, voiced by Tim Allen.\n\n\n\n\n\n\nThere are many planets in this series. Among these planets are:\n\n\nThe series was aired during UPN's Disney's One Too programming block from October 2000 to August 2003. It also aired on Disney Channel from October 2000 to August 2003 and again from June 5, 2006 to May 16, 2008, when it was taken off the air in the U.S.. The show also aired on Toon Disney from 2003 to 2007.\n\n\nAs of 2018, the whole show has yet to be released for home media. The promotional VHS of Buzz Lightyear of Star Command \"Planet of the Lost\" was released on VHS as a sampler tape in November 18, 2003.\n\nA videogame named \"Buzz Lightyear of Star Command\" was developed by Traveller's Tales and published by Activision, and released for PlayStation, Game Boy Color, Microsoft Windows, and Dreamcast in 2000. The gameplay revolves around Buzz chasing down the various villains from the show, and defeating them using different color coded weapons.\n\n\n"}
{"id": "15112668", "url": "https://en.wikipedia.org/wiki?curid=15112668", "title": "Central Heat Distribution", "text": "Central Heat Distribution\n\nCentral Heat Distribution Ltd. (CHDL) is a private district heating company located at 720 Beatty Street in Vancouver, British Columbia, Canada that provides heat to the Downtown Core including the Vancouver Public Library Central Branch, B.C. Place, GM Place, Queen Elizabeth Theatre, Pacific Centre and most major hotel/office/condo towers such as Shaw Tower via a 10.5 km network of high-pressure pipes between five centimetres and 50 centimetres in diameter running anywhere from one to five metres below street surfaces. The world-famous steam clock in Gastown is a notable addition to the more than 180 buildings that are served by the natural gas powered boiler located in the Stadium/Entertainment district of downtown.\n\nThe company was founded on November 1, 1968, by group of engineers with a desire to lower heating bills for buildings (no boilers to buy and maintain) and to reduce the amount of pollution being created to provide heat downtown. In 2014, the company was bought by developer Ian Gillespie for $32 million.\n\nThe massive building CHDL occupies at the west end of the Georgia Viaduct was once home to the printing plant for Pacific Press the publishers of The Vancouver Sun and The Province newspapers.\n\n"}
{"id": "41741714", "url": "https://en.wikipedia.org/wiki?curid=41741714", "title": "Coega Wind Farm", "text": "Coega Wind Farm\n\nThe Coega Wind Farm is a wind farm in Coega, an Industrial Development Zone (IDZ) covering of land, that is situated within the Nelson Mandela Metropolitan Municipality near Port Elizabeth, in the Eastern Cape province of South Africa. Installed by the Belgian company , it was the first commercial wind farm built in the country.\n"}
{"id": "21727204", "url": "https://en.wikipedia.org/wiki?curid=21727204", "title": "Crockett Hills Regional Park", "text": "Crockett Hills Regional Park\n\nCrockett Hills Regional Park is a regional park in Contra Costa County, California, just south of Crockett. opened to the public in 2006. Part of the East Bay Regional Park District, it consists of of rolling grasslands, wooded ravines and shoreline along the south bank of the Carquinez Strait. Its elevation ranges from to above sea level. The higher elevations offer good views of San Pablo Bay, the Sacramento - San Joaquin Delta, Mount Tamalpais, and Mount Diablo.\n\nThe portion of park area west of Cummings Skyway from its junction with Crockett Boulevard to the southeastern boundary of the park, and from the junction along Crockett Boulevard to the Crockett Ranch Staging Area in the northwestern corner of the park is open to public use. The remainder of the park area is in Land Bank status, and is closed to the public at all times. The southern boundary of the park reaches nearly to California Highway 4, although there is no entrance to the park from that road.\n\nPopular activities include hiking, running, dog walking and horseback riding. Picnicking is also popular, Tables are available in the park, but they are not reservable (first come, first served). There are no campgrounds.\n\nDogs are welcome. They must be on leash in parking lots and staging areas. Dogs may be off leash elsewhere in the park, provided they are under voice control.\n"}
{"id": "923251", "url": "https://en.wikipedia.org/wiki?curid=923251", "title": "Crystal healing", "text": "Crystal healing\n\nCrystal healing is a pseudoscientific alternative medicine technique that employs stones and crystals. Adherents of the technique claim that these have healing powers, although there is no scientific basis for this claim.\n\nIn one method, the practitioner places crystals on different parts of the body, often corresponding to chakras; or else the practitioner places crystals around the body in an attempt to construct an energy grid, which is purported to surround the client with healing energy. Despite this, scientific investigations have not validated claims that chakras or energy grids actually exist, nor is there any evidence that crystal healing has any greater effect upon the body than any other placebo; for these reasons it is considered a pseudoscience.\n\nPrecious stones have been thought of as healing objects by a variety of cultures worldwide.\n\nCrystal healing is heavily associated with the New Age spiritual movement: \"the middle-class New Age healing activity \"par excellence\"\". In contrast with other forms of complementary and alternative medicine (CAM), participants in crystal healing view the practice as \"individuated\", i.e., dependent on extreme personalization and creative expression.\n\nPractitioners of crystal healing purport that certain physical properties—e.g., shape, color, and markings—determine the ailments that a stone can heal; lists of such links are published in commonly distributed texts. Paradoxically, practitioners also \"hold the view that crystals have no intrinsic qualities but that, instead, their quality changes according to both\" participants. After selecting the stones by color or their believed metaphysical qualities, they place them on parts of the body. Color selection and placement of stones are done according to concepts of grounding, \"chakras\", or energy grids.\n\nMany other cultures have developed traditions of crystal healing over time, including the Hopi Native Americans of Arizona\nand Hawaiian islanders, some of whom continued to use it .\nThe Chinese have traditionally attributed healing powers to microcrystalline jade.\n\nThere is no peer-reviewed scientific evidence that crystal healing has any effect; it is considered a pseudoscience. Alleged successes of crystal healing can be attributed to the placebo effect. Furthermore, there is no scientific basis for the concepts of \"chakras\", being \"blocked\", energy grids requiring grounding, or other such terms; they are widely understood to be nothing more than terms used by adherents to lend credibility to their practices. Energy, as a scientific term, is a very well-defined concept that is readily measurable and bears little resemblance to the esoteric concept of energy used by proponents of crystal healing.\n\nIn 1999, researchers French and Williams conducted a study to investigate the power of crystals compared with a placebo. Eighty volunteers were asked to meditate with either a quartz crystal, or a placebo stone which was indistinguishable from quartz. Many of the participants reported feeling typical \"crystal effects\"; however, this was irrespective of whether the crystals were real or placebo. In 2001 Christopher French, head of the anomalistic psychology research unit at the University of London and colleagues from Goldsmiths College outlined their study of crystal healing at the British Psychological Society Centenary Annual Conference, concluding \"There is no evidence that crystal healing works over and above a placebo effect.”\n\nCrystal healing effects could also be attributed to cognitive bias (which occurs when the believers want the practice to be true and see only things that back up that desire).\n\nCrystal healing techniques are also practiced on animals, although some veterinary organizations, such as the British Veterinary Association, have warned that these methods are not scientifically proven and state that people should seek the advice of a vet before using alternative techniques.\n\n\n\n"}
{"id": "51560627", "url": "https://en.wikipedia.org/wiki?curid=51560627", "title": "Cumulonimbus and aviation", "text": "Cumulonimbus and aviation\n\nNumerous accidents have occurred in the vicinity of thunderstorms. It is often said that the turbulence can be extreme enough inside a cumulonimbus to tear an aircraft into pieces. However, this kind of accident is relatively rare. Moreover, the turbulence \"under\" a thunderstorm can be non-existent and is usually no more than moderate. Actually, most thunderstorm-related crashes occur due to a stall close to the ground when the pilot gets caught by surprise by a thunderstorm-induced wind shift. Moreover, aircraft damage caused by thunderstorms is rarely in the form of structural failure due to turbulence but is typically less severe and the consequence of secondary effects of thunderstorms (e.g., denting by hail or paint removal by high-speed flight in torrential rain).\n\nThus, cumulonimbus are known to be extremely dangerous to air traffic, and it is recommended to avoid them as much as possible. Cumulonimbus can be extremely insidious, and an inattentive pilot can end up in a very dangerous situation while flying in apparently very calm air.\n\nWhile there is a gradation with respect to thunderstorm severity, there is little quantitative difference between a significant shower generated by a cumulus congestus and a small thunderstorm with a few thunderclaps associated with a small cumulonimbus. For this reason, a glider pilot could exploit the rising air under a thunderstorm without recognising the situation - thinking instead that the rising air was due to a more benign variety of cumulus. However, forecasting thunderstorm severity is an inexact science; in numerous occasions, pilots got trapped by underestimating the severity of a thunderstorm that suddenly strengthened.\n\nEven large airliners avoid crossing the path of a cumulonimbus. Two dangerous effects of cumulonimbus have been put forward to explain the crash of flight AF447 that sank into sea on the 31st of May 2009 about northeast of Brazil. It encountered a mesoscale convective system in the intertropical convergence zone (known by sailors as the \"doldrums\"), where cumulonimbus rise to more than in altitude.\nHowever, the aircraft did not disintegrate in flight. A different hypothesis was put forward and later confirmed: accumulation of ice on the aircraft's pitot tubes.\n\nThe inconsistency between the airspeeds measured by the different sensors is one of the causes of the accident according to the final report.\n\nThe US FAA recommends that aircraft (including gliders) stay at least 20 nautical miles away from a severe thunderstorm, while a glider pilot could be tempted to use the updraughts below and inside the cloud. There are two sorts of danger for this type of aircraft. One is related to the shear effects between updraughts and downdraughts inside the cloud - effects that can smash the glider. This shear creates a Kelvin-Helmholtz instability that can generate extremely violent sub-vortices. The second danger is more insidious: the strong updraughts below a supercell cumulonimbus can cover a large area and contain little or no turbulence as explained below. In this case, the glider can be sucked into the cloud, where the pilot can quickly lose visual reference to the ground, causing conditions to quickly become IMC.\nIn these conditions, the aircraft (if not equipped for IMC flight and flown by a pilot experienced in IMC flight) is likely to enter a graveyard spiral and eventually break up by exceeding the wing load limit. In this situation, the cause of the disintegration of the aircraft is not atmospheric turbulence but is the inability of the pilot to control the aircraft following the loss of visual reference to the ground. In the case of an instrument flight, cumulonimbus can catch a pilot by surprise when embedded in a more benign cloud mass. For example, nimbostratus can originate from the spreading of a cumulonimbus (\"nimbostratus cumulonimbogenitus\"), making the presence of active convective cells likely. Small private airplanes are generally not equipped with on-board weather radars; and during an IFR approach, they can be sent accidentally by air traffic control to non-obvious active cells.\n\nThe updraughts under a cumulonimbus can be extremely laminar, extensive, and uniform, this is particularly true during the buildup of the thunderstorm. They can last more than one hour and correspond to a steady state of the cumulonimbus.\nThe updraught under the cloud is mostly due to buoyancy, but there is also a large pressure difference between the base and the top of the cumulonimbus (larger than would be found in this height range outside the cloud) and local low-level mechanical lifting such as the lifting generated by a downburst. The two last phenomena can overcome a stable air zone close to the surface by lifting cooler air parcels to a level where they are eventually warmer than the surrounding air. This can happen if these mechanical phenomena lift the parcel above the lifted condensation level (LCL), above which height the parcel's temperature \"T(z)\" decreases less with height (due to the release of latent heat and at approximately 6.5 K/km) than the surrounding air temperature \"T(z)\" decreases with height in the case of a conditionally unstable lapse rate aloft. In other words, the parcel can be lifted to a height where formula_1, where the former is the cooling rate of the parcel and the latter is the ambient lapse rate. In these conditions, the rising parcel may eventually become warmer than the surrounding air; in other words, there may exist a level above which formula_2. This scenario's conditionally unstable lapse rate aloft is relatively common when thunderstorms exist. In effect, at low level, such air parcels are sucked into the cloud as if by a vacuum cleaner. Soaring pilots refer to this near-base sucking as \"cloud suck\", a phenomenon known to generally be more intense the taller the cumulus cloud - and to thus be at maximum intensity with a cumulonimbus. Since the dynamic updraught is wide, the updraught velocity varies little laterally and thus the turbulence is minimised. So, it is said:In fact, Ellrod and Marwitz's paper is more general. These authors state that in general, the buoyancy beneath the cumulonimbus cloud base is often negative. This explains why updraughts underneath the base of a cumulonimbus are often \"laminar\". This phenomenon is well known by glider pilots. (see below). The phenomenon is enhanced under the weak echo region of a supercell thunderstorm that is \"extremely dangerous\". At approximately these smooth updraughts become suddenly very turbulent.\n\nIn general, updraughts reach their maximum intensity at above the ground. At this altitude, a phase change occurs where water droplets become ice crystals and therefore release energy in the form of latent heat and thus the updraught strength increases. Supercell thunderstorms or derechos can have gigantic updraughts at this altitude, updraughts with speeds that can exceed . Such an updraught speed corresponds to the wind speed of a small hurricane. The speed can even exceed . The maximum number in the Beaufort scale is 12 (\"hurricane force\" wind) and is assigned to wind speeds of 64 knots or greater. If the Beaufort scale were extended, these updraughts would have a Beaufort number of 14 \"in the vertical direction\". The turbulence is then \"extreme\" at this altitude.\n\nMoreover, the diameters of the updraught columns vary between 2 km (air mass thunderstorm) and 10 km (supercell thunderstorm). The height of the cumulonimbus base is extremely variable. It varies from a few tens of metres above the ground to 4000 m above the ground. In the latter case, the updraughts can originate either from the ground (if the air is very dry - typical of deserts) or from aloft (when altocumulus castellanus degenerates into cumulonimbus). When the updraught originates from aloft, this is considered \"elevated convection\".\n\n\"Detailed article: Downburst\n\nDownbursts are dangerous for many reasons. First, downdraughts under cumulonimbus can be severe and extensive. A sailplane flying at 50 knots in a downdraught of 15 knots has an approximate glide ratio of 3, meaning that it covers only about three metres of ground for every metre it descends. Assuming that the glider is at cloud base height at , if it remains in the downdraught the entire time, it will only be able to glide before being forced to land - likely under difficult and dangerous conditions. Even if the glider lands safely, it could be destroyed later by a wind gust. So when a rain curtain shows a downburst, it is of paramount importance to not land in this area.\n\nDowndraughts of 50 knots are possible and can generate wind gusts of 60 knots or more.\nSafely landing a light aircraft in these conditions can be virtually impossible. Moreover, close to the ground, a glider or airplane pilot can be caught by surprise by a sudden reversal of the wind direction and transition from an upwind to a downwind situation. If the airspeed becomes too low, the aircraft will stall and may crash into the ground due to the altitude lost recovering from the stall. As a consequence of famous instances of crashes of this nature in the United States, a network of wind profilers and Terminal Doppler Weather Radars was developed in the vicinity of airports to monitor this wind shear. Based on FAA rules, every pilot must inquire about the wind speed and direction before landing.\n\nCompared to airliners, sailplanes fly at low airspeeds. The usual approach speed of a sailplane is around 50 knots, but let's assume that the pilot is extra \"careful\" and flies his approach at 65 knots. William Cotton claims that the wind shear can be as high as 50 knots. In such a case, if the shear direction is such that the airspeed is reduced by the shear amount, this pilot's airspeed will drop to 15 knots, which is well below his glider's stall speed (typically 35–40 knots). If this airspeed drop occurs during the transition from the base leg to the final approach, the aircraft may enter into a spin from which there isn't enough altitude to recover. The exact quotation is the following: \n\nSo when the pilot encounters benign cumulonimbus, it may be a better choice to stay aloft and use the updraughts under the cumulus in front of the thunderstorm along the flanking line (or even under the cumulonimbus itself in its laminar region) and wait for the thunderstorm to dissipate instead of attempting a landing in the presence of possible downbursts.\n\nIn some countries, sailplanes are permitted to fly inside clouds. For example, during the 1972 World Soaring Championship at Vršac, Yugoslavia, Helmut Reichmann attempted to use the violent updraughts associated with cumulonimbus. Initially, he found an updraught of +8 m/s. After half a circle, he was in a downdraught of −15 m/s. He had to land very shortly afterward. The thunderstorm was in its mature stage. In another example, Terry Delore got trapped in a severe thunderstorm. He entered a seemingly innocuous cumulus at . This cumulus evolved into a large cumulonimbus. At first, the flight inside the cloud was turbulence-free. Then his glider suddenly became uncontrollable. He was either inverted, in a nosedive, or in a chandelle. The airbrakes became stuck open due to hailstones blocking the orifices. When he landed, the airfield was still covered by hailstones. The wind gusts were between 30 and 40 knots. Everyone on the ground feared for the pilot's life. In the same book, the author narrates that an Italian instructor at Rieti had his students climb inside cumulonimbus so that they get accustomed to them.\n\nAs mentioned above, a climb inside a cumulonimbus can be initially very smooth (due to the negative buoyancy of the air parcel) and suddenly become horribly turbulent. As an example, a glider pilot found initially very laminar updraughts and got sucked into the cloud where he encountered accelerations of \"18 g\" and became unconscious.\n\nDue to the phase change of water droplets (to ice), the cumulonimbus top is almost always turbulent. Moreover, the glider can become covered with ice, and the controls can freeze and remain stuck. Many accidents of this kind have occurred. If the pilot bails out and opens his parachute, he may be sucked upward (or at least held aloft) as happened to William Rankin after ejecting from an F-8 fighter jet and falling into a cumulonimbus (within which his parachute opened).\n\nA skydiver or paraglider pilot under a cumulonimbus is exposed to a potentially deadly risk of being rapidly sucked up to the top the cloud and being suffocated, struck by lightning, or frozen. If he survives, he may suffer irreversible brain damage due to lack of oxygen or an amputation as a consequence of frostbite. German paraglider pilot Ewa Wiśnierska barely survived a climb of more than inside a cumulonimbus.\n\nHeavy transportation airplanes may occasionally have to cross a thunderstorm line associated with a cold front or a squall. They may not be able to overfly the cumulonimbus, because at 36000 feet, the aircraft may be in or near what is known as the coffin corner (stall speed = Vne), thus making it structurally dangerous to climb higher. However, some cells can rise to 70000 feet. Another option would be to navigate around the cells. This is strongly discouraged, however, because in the opening, new cells can grow very rapidly and engulf the aircraft. Whenever an aircraft moves to the west and crosses a thunderstorm line, the pilot will first encounter a line of powerful and laminar updraughts (that are not thermal but dynamic). The pilot should refrain from pushing the stick to try to maintain a constant altitude (similar to mountain waves), because pushing the stick can cause his airspeed to increase to the point of hitting the yellow arc (on the airspeed indicator). An airspeed this high is not permissible in turbulent conditions and may lead to break-up of the aircraft. Indeed, when the pilot exits the updraught zone, he will encounter very strong turbulence due to the shear between rising and sinking air. If the airspeed is too high at this point, the airplane will break apart. The crash of Flight AF 447 is indirectly related to this situation: the pilot, being short on fuel, opted for the shortest path while crossing the thunderstorm line associated with the intertropical convergence zone, and the pitot tubes iced over. What followed is known.\n\nOn-board radars can be deceiving. Hail shafts generate weak radar echoes and are significantly more dangerous than cloudbursts. Close to the ground, heavy rain (or snow at altitude) tends to dampen turbulence (it is said that when rain comes, most of the danger is gone). So a counter intuitive recommendation exists: it is better to fly toward the zone of heavy precipitation or toward the darkest area of the thunderstorm line. This recommendation contradicts the usual use of on-board radars to avoid areas of strong precipitation, which is usually the best course of action. There is no \"miracle\" solution, and the best option is to avoid these thunderstorm systems by having enough fuel on board, thus reducing the temptation to take a more dangerous route in the interest of fuel savings.\n\nAlso, St Elmo's fires while flying inside cumulonimbus can burn out the on-board electronic equipment and even pierce a wing by melting the metal skin.\n\n\"See detailed article\" supercell thunderstorm\nThe updraughts inside a cumulonimbus associated with a supercell thunderstorm can reach . This corresponds to the wind speed of a weak hurricane. Moreover, the turbulence inside a cloud can become extreme and break apart an aircraft. Thus, it is extremely dangerous to fly inside such a monster.\n\nThe thunderstorm system can be divided into two zones in the figure to the left: the precipitation-free zone, located on the left where the airmass has a widespread up motion, and the precipitation zone, on the right where the airmass is sinking. At the point where the two zones meet, there is a wall cloud that could initiate tornadoes. Moreover, even the cumulus congestus associated with a supercell thunderstorm can be very dangerous. Tornadoes can be produced up to from the main cell.\n\nIn the updraught area, the air has a negative buoyancy and is sucked up by a low pressure zone at altitude. Turbulence is annihilated. In particular, in the forward area of the supercell, one can find a flanking line made of cumulus congestus or small cumulonimbus. It should be noted that the cloud base of the flanking line is higher than the base of the main cumulonimbus.\n\nSince the updraught under these clouds (in the flanking line) is mainly dynamic, the airmass being smooth and the cloud base higher, a glider pilot could be tempted to fly in this zone. However, conditions can rapidly become dangerous, since the wall cloud can generate a tornado that will pulverise any aircraft. Moreover, since the rising air is widespread, the glider pilot (especially if flying a low-speed, low-performance glider like a paraglider) may be unable to escape and may be sucked into the cloud up to its top. Thus, the FAA recommends that aircraft should never be closer than 20 miles from severe thunderstorms.\n\nAlthough it rarely happens, a glider can be struck by lightning. Metal sailplanes are Faraday cages and thus should not be destroyed by a lightning strike. However, gliders made of wood or fibreglass can be destroyed. Moreover, modern sailplanes are filled with electronic devices that can be damaged by lightning. Also, any winch launch must be prohibited when a thunderstorm is less than away. Indeed, the air is electrified, and the cable will act as a lightning rod.\n\nHail can shred a sailplane canopy and seriously damage the wings and fuselage. Hail is barely visible and can be encountered in the updraught zone under the cloud. On August 5, 1977, an airplane pilot was taken by surprise in the vicinity of Colorado Springs by a supercell thunderstorm that produced 20 tornadoes. The pilot was flying in eerily calm air (the updraught zone can be laminar) when he saw the sky transitioning from pale grey to inky black. The pilot heard a loud sound that reoccurred more and more frequently. Then a hailstone pierced the windshield, rendering the pilot semi-unconscious. Eventually, the pilot landed his shredded airplane in a field.\n\nAn EF5 tornado can generate ground winds of unbelievable speed; common sense dictates that an aircraft should never be close to such a meteorological phenomenon. Indeed the wind speed can reach , and one can easily guess that the aircraft can be torn into pieces in such conditions. However, airline transportation aircraft have overflown tornadoes by more than without damage. The fact that an airliner does not get destroyed can be explained as follows: tornadoes are violent phenomena only close to the ground and become weaker at height. A glider dared to cross a weak tornado during a soaring contest in Texas in 1967. The cumulonimbus base was at . The glider crossed an extremely turbulent zone and ended up in a turbulence-free zone inverted. The controls were not responding, and the pilot contemplated abandoning the aircraft. After some time and a big fright, the controls started to respond again, and the pilot was able to continue his flight. Pilots in the vicinity did not notice anything.\n\nOn the 6th of October 1981 a Fokker aircraft hit a tornado which occurred in a supercell near the town of Moerdijk in the Netherlands, all 17 occupants of the aircraft were killed.\n\nAn empirical criterion for tornado formation has been developed by Dan Sowa from Northwest Orient Airlines as follows: the cumulonimbus overshooting top must enter into the stratosphere by at least 10000 feet. This criterion is, however, incorrect and the is a counter-example. It reached level EF2 while being generated by a small cumulonimbus that did not attain .\n\nAs a result of a faulty generalisation, it is very often incorrectly said that cumulonimbus and the updraughts under them are always turbulent. This fallacy originates from the fact that cumulonimbus are actually extremely turbulent at high altitude, and therefore, one might falsely deduce that cumulonimbus are turbulent at \"all\" altitudes. Reliable studies and glider pilots' experience have demonstrated that updraughts under cumulonimbus were generally smooth. As seen above, updraughts under a cumulonimbus are often \"dynamic\" and thus will be very smooth. The phenomenon is enhanced under the weak echo region of a supercell thunderstorm that is \"extremely dangerous\". However, this phenomenon is little known in the aviation world. Thus, a widespread view in the aeronautical community is that cumulonimbus are always associated with very strong turbulence (at all altitudes) and severe thunderstorms. For example, Gil Roy, in a book endorsed by the , claims that:\n\nAlso, the author talks about cumulo-nimbus [\"sic\"] of gigantic size that can reach a height of \"several thousand metres\". While the word \"several\" isn't very precise, a thickness of 8000 metres is fairly \"typical\" for a cumulonimbus, with some as thick as 20000 metres or more. Moreover, the majority of cumulonimbus are associated with weak pulse thunderstorms or even simple showers without electric phenomena.\n\nThe reference to the thunderstorm front corresponds to the outflow boundary associated with downbursts that are indeed very dangerous and are the site of vortices associated with the Kelvin-Helmholtz instability at the junction between updraughts and downdraughts. However, in front of the thunderstorm, updraughts are generally laminar due to the negative buoyancy of air parcels (see above).\n\nAlso, the LUXORION web site states:\n\nSuch a claim is too broad and again contradicts the fact that updraughts in front of a thunderstorm are often laminar. However, it is true that the upper layers are almost always turbulent. However, in most cases, the aforesaid turbulence is not extreme. Along the same lines, Didier Morieux states:\nDennis Pagen is even more explicit. He states:\n\nThe International cloud atlas soothes these claims: it simply states that \"\"la turbulence is often very strong\" \" below the cloud.\n\nA glider pilot convinced that cumulonimbus are always violent risks getting a nasty surprise. If he flies under the flanking line of a supercell thunderstorm and finds that the air is very smooth and updraughts are moderate, he may falsely infer that he is safe and not under a cumulonimbus; since cumulonimbus are, officially, always turbulent. He may thus not realise when he is under a secondary cumulonimbus that can suck him inside the cloud, and he may encounter a wall cloud that could generate a tornado that could disentegrate his fragile skiff as shown in Figure 5. Dominique Musto cautions paraglider pilots (that might otherwise be swayed by the above myth) against the false sensation of safety in a region of extended updraughts that are rather weak as follows:\n\nThis quotation summarises in three sentences the often-insidious dangers associated with cumulonimbus, dangers that are exacerbated for paraglider pilots, as German paraglider pilot Ewa Wiśnierska experienced. She survived climbing above inside a cumulonimbus. A nearby fellow pilot caught in the same weather event wasn't so fortunate.\n\nAs well, in 2014, the 66 years old general Paolo Antoniazzi died after its paraglider got sucked into a cumulonimbus up to the altitude of .\n\nThe above quotation puts informally the harbingers of a thunderstorm. So a cumulonimbus acts as an enormous thermal machine that sucks up the air in the front (left side of Figure 3) and violently throws it out in the back through downbursts (right side of Figure 3). Consequently, a broad area of updraughts will be located in front of the thunderstorm. Typically, in a humid airmass, the updraughts will be on the order of 1 m/s; and in a dry air mass, they will be on the order of 2 to 3 m/s. Therefore, when a glider pilot is in an area where \"updraughts are everywhere\" and he is close to large clouds (that can be cumulus congestus), he is likely in the vicinity of a building thunderstorm.\n\nThe downbursts associated with cumulonimbus can generate gravity waves far way from thunderstorms. These gravity waves can be felt up to away and in some conditions several hundreds of kilometres away. A severe thunderstorm generating these gravity waves located at more than away (according to Federal Aviation Administration recommendations) should not affect the safety of aircraft this far from the thunderstorm. These gravity waves can be modelled in the same manner as mountain waves and can be usable by a glider pilot.\n\nSmall cumulonimbus can relatively safely be exploited by experienced glider pilots. They generate moderate updraughts that are generally laminar and that are pleasant and enlightening. Thus, pulse-like summer thunderstorms can be used during cross-country flights, since the glider will move away from the cumulonimbus after having (in theory) climbed up to 500 feet below the cloud base (the maximum permissible height in the United States) and the passage of the glider in the proximity of the thunderstorm will be short. For example, during an official contest of the Soaring Society of America, pilots openly played with the cumulonimbus (and even with the updraughts contiguous to downbursts) and boasted about it.\nHowever, a rule of thumb says that the distance between two thermals is equal to three times the height of the cloud. Consequently, a cumulonimbus that is 13 km thick will eliminate any convective activity over a radius of approximately 40 km. Most gliders cannot perform such long glides, and therefore, an encounter with a pulse-like thunderstorm in a glider will often be followed soon by the end of the flight.\n\nFigure 3.22 from this reference shows the presence of a rotor outside a downburst. A more-than-foolhardy pilot could easily locate this updraught and exploit it. However, this photograph will dissuade any sensible pilot from using such monstrosities. Actually, downbursts are the most significant hazard pertaining\nto thunderstorms. Moreover, if for any reason the pilot must land (hail storm or other), he will have to cross the downburst immediately above him and there will be a greatly increased chance of crashing - due to the unpredictable decrease of the airspeed. Moreover, if the glider transitions from the updraught to the downdraught, severe turbulence will occur due to the Kelvin-Helmholtz instability in the shear area. However, pilots have nonetheless exploited such updraughts.\n\nReckless pilots have exploited squalls by flying in front of thunderstorm systems as if flying along a ridge. The pilot really must land at an airport and put the glider in a hangar; the squall line will catch him again soon and imperil the glider if it is not protected. Dennis Pagen performed a similar flight in front of a supercell cumulonimbus during the Preliminaries of the hang glider 1990 World championship in Brazil where he was able to fly 35 km at high speed without a turn. The author acknowledges that his achievement was dubious, since hang gliders (and even more so paragliders) are significantly slower than sailplanes and can much more easily be sucked inside the cloud.\n\nThe only cumulonimbus clouds that could be usable by a glider pilot, subject to all necessary reservations, might be isolated small cumulonimbus or at a pinch the flanking lines associated with strong thunderstorms. However, examples above show that a seemingly innocuous cloud can rapidly become very dangerous. Squalls and supercell thunderstorms are definitely deadly hazards to uninformed pilots. Based on visual flight rules, flights in pre-storm areas must be visual; the pilots must be able to watch the evolution of a thundercloud and take the necessary actions of avoidance or to quickly land when appropriate.\n\nThe above examples demonstrate that the different phenomena associated with cumulonimbus can jeopardise any type of aircraft and its occupants when the pilot flies in the vicinity and especially inside a thundercloud. An airplane pilot should never come near a cumulonimbus.\n\n\n"}
{"id": "10656445", "url": "https://en.wikipedia.org/wiki?curid=10656445", "title": "Derivation of the Navier–Stokes equations", "text": "Derivation of the Navier–Stokes equations\n\nThe intent of this article is to highlight the important points of the derivation of the Navier–Stokes equations as well as its application and formulation for different families of fluids.\n\nThe Navier–Stokes equations are based on the assumption that the fluid, at the scale of interest, is a continuum, in other words is not made up of discrete particles but rather a continuous substance. Another necessary assumption is that all the fields of interest like pressure, flow velocity, density, and temperature are differentiable, weakly at least.\n\nThe equations are derived from the basic principles of continuity of mass, momentum, and energy. For that matter, sometimes it is necessary to consider a finite arbitrary volume, called a control volume, over which these principles can be applied. This finite volume is denoted by formula_1 and its bounding surface formula_2. The control volume can remain fixed in space or can move with the fluid.\n\nChanges in properties of a moving fluid can be measured in two different ways. One can measure a given property by either carrying out the measurement on a fixed point in space as particles of the fluid pass by, or by following a parcel of fluid along its streamline. The derivative of a field with respect to a fixed position in space is called the \"Eulerian\" derivative while the derivative following a moving parcel is called the \"advective\" or \"material\" (\"Lagrangian\" ) derivative.\n\nThe material derivative is defined as the \"nonlinear operator\":\n\nwhere formula_4 is the flow velocity. The first term on the right-hand side of the equation is the ordinary Eulerian derivative (i.e. the derivative on a fixed reference frame, representing changes at a point with respect to time) whereas the second term represents changes of a quantity with respect to position (see advection). This \"special\" derivative is in fact the ordinary derivative of a function of many variables along a path following the fluid motion; it may be derived through application of the chain rule in which all independent variables are checked for change along the path (i.e. the total derivative).\n\nFor example, the measurement of changes in wind velocity in the atmosphere can be obtained with the help of an anemometer in a weather station or by observing the movement of a weather balloon. The anemometer in the first case is measuring the velocity of all the moving particles passing through a fixed point in space, whereas in the second case the instrument is measuring changes in velocity as it moves with the flow.\n\nThe Navier–Stokes equation is a special continuity equation. A continuity equation may be derived from conservation principles of:\n\n\nThis is done via the continuity equation, an integral relation stating that the rate of change of some integrated property formula_5 defined over a control volume formula_1 must be equal to what amount is lost or gained through the boundaries formula_7 of the volume plus what is created or consumed by sources and sinks inside the volume. This is expressed by the following integral continuity equation:\n\nwhere formula_4 is the flow velocity of the fluid, formula_10 is the outward-pointing unit-normal vector, and formula_11 represents the sources and sinks in the flow, taking the sinks as positive.\n\nThe divergence theorem may be applied to the surface integral, changing it into a volume integral:\n\nApplying Reynolds transport theorem to the integral on the left and then combining all of the integrals:\n\nThe integral must be zero for any control volume; this can only be true if the integrand itself is zero, so that:\n\nFrom this valuable relation (a very generic continuity equation), three important concepts may be concisely written: conservation of mass, conservation of momentum, and conservation of energy. Validity is retained if formula_5 is a vector, in which case the vector-vector product in the second term will be a dyad.\n\nA general momentum equation is obtained when the conservation relation is applied to momentum. If the intensive property formula_5; considered is the mass flux (also \"momentum density\"), i.e. the product of mass density and flow velocity formula_17, by substitution in the general continuum equation:\n\nwhere formula_19 is a dyad, a special case of tensor product, which results in a second rank tensor; the divergence of a second rank tensor is again a vector (a first rank tensor). \n\nUsing the formula for the divergence of a dyad, \n\nwe then have\n\nNote that the gradient of a vector is a special case of the covariant derivative, the operation results in second rank tensors; except in Cartesian coordinates, it's important to understand that this isn't simply an element by element gradient. Rearranging and recognizing that formula_22:\n\nThe leftmost expression enclosed in parentheses is, by mass continuity (shown in a moment), equal to zero. Noting that what remains on the left side of the equation is the material derivative:\n\nor, with the use of the material derivative operator previously defined:\n\nThis appears to simply be an expression of Newton's second law (F = \"m\"a) in terms of body forces instead of point forces. Each term in any case of the Navier–Stokes equations is a body force. A shorter though less rigorous way to arrive at this result would be the application of the chain rule to acceleration:\n\nwhere formula_28. The reason why this is \"less rigorous\" is that we haven't shown that the choice of\n\nis correct; however it does make sense since with that choice of path the derivative is \"following\" a fluid \"particle\", and in order for Newton's second law to work, forces must be summed following a particle. For this reason the convective derivative is also known as the particle derivative.\n\nMass may be considered also. Taking formula_30 (no sources or sinks of mass) and putting in density:\n\nwhere formula_32 is the mass density (mass per unit volume), and formula_4 is the flow velocity. This equation is called the mass continuity equation, or simply \"the\" continuity equation. This equation generally accompanies the Navier–Stokes equation.\n\nIn the case of an incompressible fluid, formula_34 (i.e. the density following the path of a fluid element is constant) and the equation reduces to:\n\nwhich is in fact a statement of the conservation of volume.\n\nThe generic density of the momentum source formula_36 seen previously is made specific first by breaking it up into two new terms, one to describe surface forces and one for body forces, such as gravity. By examining the forces acting on a small cube in a fluid, it may be shown that\n\nwhere formula_38 is the Cauchy stress tensor, and formula_39 accounts for body forces present. This equation is called the Cauchy momentum equation and describes the non-relativistic momentum conservation of \"any\" continuum that conserves mass. formula_38 is a rank two symmetric tensor given by its covariant components. In orthogonal coordinates in three dimensions it is represented as the 3x3 matrix:\n\nwhere the formula_42 are normal stresses and formula_43 shear stresses. This matrix is split up into two terms:\n\nwhere formula_45 is the 3 x 3 identity matrix and formula_46 is the deviatoric stress tensor. Note that the mechanical pressure \"p\" is equal to minus the mean normal stress:\n\nThe motivation for doing this is that pressure is typically a variable of interest, and also this simplifies application to specific fluid families later on since the rightmost tensor formula_46 in the equation above must be zero for a fluid at rest. Note that formula_46 is traceless. The Cauchy equation may now be written in another more explicit form:\n\nThis equation is still incomplete. For completion, one must make hypotheses on the forms of formula_46 and formula_52, that is, one needs a constitutive law for the stress tensor which can be obtained for specific fluid families and on the pressure. Some of these hypotheses bring to Euler equations (fluid dynamics), other ones bring to Navier-Stokes equations. Additionally, if the flow is assumed compressible an equation of state will be required, which will likely further require a conservation of energy formulation.\n\nThe general form of the equations of motion is not \"ready for use\", the stress tensor is still unknown so that more information is needed; this information is normally some knowledge of the viscous behavior of the fluid. For different types of fluid flow this results in specific forms of the Navier–Stokes equations.\n\nThe formulation for Newtonian fluids stems from an observation made by Newton that, for most fluids,\n\nIn order to apply this to the Navier–Stokes equations, three assumptions were made by Stokes:\n\nThe above list states the classic argument that the shear strain rate tensor (i.e. the (symmetric) shear part of the velocity gradient) is a pure shear tensor and does not include any inflow/outflow part (i.e. any compression/expansion part). This means that its trace is zero, and this is achieved by subtracting formula_55 in a symmetric way from the diagonal elements of the tensor. The compressional contribution to viscous stress is added as a separate diagonal tensor.\n\nApplying these assumptions will lead to :\n\nThat is, the deviatoric of the deformation rate tensor is identified to the deviatoric of the stress tensor, up to a factor \"μ\".\n\nformula_57 is the Kronecker delta. \"μ\" and \"λ\" are proportionality constants associated with the assumption that stress depends on strain linearly; \"μ\" is called the first coefficient of viscosity or shear viscosity (usually just called \"viscosity\") and \"λ\" is the second coefficient of viscosity or volume viscosity (and it is related to bulk viscosity). The value of \"λ\", which produces a viscous effect associated with volume change, is very difficult to determine, not even its sign is known with absolute certainty. Even in compressible flows, the term involving \"λ\" is often negligible; however it can occasionally be important even in nearly incompressible flows and is a matter of controversy. When taken nonzero, the most common approximation is \"λ\" ≈ - ⅔ \"μ\".\n\nA straightforward substitution of formula_58 into the momentum conservation equation will yield the Navier–Stokes equations, describing a compressible Newtonian fluid:\n\nwhere the transpose has been used. The body force has been decomposed into density and external acceleration, i.e. formula_60. The associated mass continuity equation is:\n\nIn addition to this equation, an equation of state and an equation for the conservation of energy is needed. The equation of state to use depends on context (often the ideal gas law), the conservation of energy will read:\n\nHere, formula_63 is the specific enthalpy, formula_64 is the temperature, and formula_65 is a function representing the dissipation of energy due to viscous effects:\n\nWith a good equation of state and good functions for the dependence of parameters (such as viscosity) on the variables, this system of equations seems to properly model the dynamics of all known gases and most liquids.\n\nFor the special (but very common) case of incompressible flow, the momentum equations simplify significantly. Taking into account the following assumptions:\nthen looking at the viscous terms of the formula_70 momentum equation for example we have:\nSimilarly for the formula_72 and formula_73 momentum directions we have formula_74 and formula_75.\n\nThe above solution is key to deriving Navier-Stokes equations from Equation of motion in fluid dynamics when density and viscosity are constant.\n\nA non-Newtonian fluid is a fluid whose flow properties differ in any way from those of Newtonian fluids. Most commonly the viscosity of non-Newtonian fluids is a function of shear rate or shear rate history. However, there are some non-Newtonian fluids with shear-independent viscosity, that nonetheless exhibit normal stress-differences or other non-Newtonian behaviour. Many salt solutions and molten polymers are non-Newtonian fluids, as are many commonly found substances such as ketchup, custard, toothpaste, starch suspensions, paint, blood, and shampoo. In a Newtonian fluid, the relation between the shear stress and the shear rate is linear, passing through the origin, the constant of proportionality being the coefficient of viscosity. In a non-Newtonian fluid, the relation between the shear stress and the shear rate is different, and can even be time-dependent. The study of the non-Newtonian fluids is usually called rheology. A few examples are given here.\n\nIn Bingham fluids, the situation is slightly different:\n\nThese are fluids capable of bearing some shear before they start flowing. Some common examples are toothpaste and clay.\n\nA power law fluid is an idealised fluid for which the shear stress, formula_43, is given by\n\nThis form is useful for approximating all sorts of general fluids, including shear thinning (such as latex paint) and shear thickening (such as corn starch water mixture).\n\nIn the analysis of a flow, it is often desirable to reduce the number of equations or the number of variables being dealt with, or both. The incompressible Navier-Stokes equation with mass continuity (four equations in four unknowns) can, in fact, be reduced to a single equation with a single dependent variable in 2D, or one vector equation in 3D. This is enabled by two vector calculus identities:\n\nfor any differentiable scalar formula_5 and vector formula_82. The first identity implies that any term in the Navier-Stokes equation that may be represented as the gradient of a scalar will disappear when the curl of the equation is taken. Commonly, pressure \"p\" and external acceleration g are what eliminate, resulting in (this is true in 2D as well as 3D):\n\nwhere it's assumed that all body forces are describable as gradients (for example it is true for gravity), and density has been divided so that viscosity becomes kinematic viscosity.\n\nThe second vector calculus identity above states that the divergence of the curl of a vector field is zero. Since the (incompressible) mass continuity equation specifies the divergence of flow velocity being zero, we can replace the flow velocity with the curl of some vector formula_84 so that mass continuity is always satisfied:\n\nSo, as long as flow velocity is represented through formula_86, mass continuity is unconditionally satisfied. With this new dependent vector variable, the Navier-Stokes equation (with curl taken as above) becomes a single fourth order vector equation, no longer containing the unknown pressure variable and no longer dependent on a separate mass continuity equation:\n\nApart from containing fourth order derivatives, this equation is fairly complicated, and is thus uncommon. Note that if the cross differentiation is left out, the result is a third order vector equation containing an unknown vector field (the gradient of pressure) that may be determined from the same boundary conditions that one would apply to the fourth order equation above.\n\nThe true utility of this formulation is seen when the flow is two dimensional in nature and the equation is written in a general orthogonal coordinate system, in other words a system where the basis vectors are orthogonal. Note that this by no means limits application to Cartesian coordinates, in fact most of the common coordinates systems are orthogonal, including familiar ones like cylindrical and obscure ones like toroidal.\n\nThe 3D flow velocity is expressed as (note that the discussion has been coordinate free up till now):\n\nwhere formula_89 are basis vectors, not necessarily constant and not necessarily normalized, and formula_90 are flow velocity components; let also the coordinates of space be formula_91.\n\nNow suppose that the flow is 2D. This doesn't mean the flow is in a plane, rather it means that the component of flow velocity in one direction is zero and the remaining components are independent of the same direction. In that case (take component 3 to be zero):\n\nThe vector function formula_84 is still defined via:\n\nbut this must simplify in some way also since the flow is assumed 2D. If orthogonal coordinates are assumed, the curl takes on a fairly simple form, and the equation above expanded becomes:\n\nExamining this equation shows that we can set formula_97 and retain equality with no loss of generality, so that:\n\nthe significance here is that only one component of formula_84 remains, so that 2D flow becomes a problem with only one dependent variable. The cross differentiated Navier–Stokes equation becomes two 0 = 0 equations and one meaningful equation.\n\nThe remaining component formula_100 is called the stream function. The equation for formula_101 can simplify since a variety of quantities will now equal zero, for example:\n\nif the scale factors formula_103 and formula_104 also are independent of formula_105. Also, from the definition of the vector Laplacian\n\nManipulating the cross differentiated Navier–Stokes equation using the above two equations and a variety of identities will eventually yield the 1D scalar equation for the stream function:\n\nwhere formula_108 is the biharmonic operator. This is very useful because it is a single self-contained scalar equation that describes both momentum and mass conservation in 2D. The only other equations that this partial differential equation needs are initial and boundary conditions.\n\nThe assumptions for the stream function equation are listed below:\n\n\nThe stream function has some useful properties:\n\n\nThe derivation of the Navier-Stokes equation involves the consideration of forces acting on fluid elements, so that a quantity called the stress tensor appears naturally in the Cauchy momentum equation. Since the divergence of this tensor is taken, it is customary to write out the equation fully simplified, so that the original appearance of the stress tensor is lost.\n\nHowever, the stress tensor still has some important uses, especially in formulating boundary conditions at fluid interfaces. Recalling that formula_112, for a Newtonian fluid the stress tensor is:\n\nIf the fluid is assumed to be incompressible, the tensor simplifies significantly. In 3D cartesian coordinates for example:\n\nformula_115 is the strain rate tensor, by definition:\n\n"}
{"id": "32186312", "url": "https://en.wikipedia.org/wiki?curid=32186312", "title": "Diablo Canyon earthquake vulnerability", "text": "Diablo Canyon earthquake vulnerability\n\nDiablo Canyon (Nuclear) Power Plant, located in San Luis Obispo County California, was originally designed to withstand a 6.75 magnitude earthquake from four faults, including the nearby San Andreas and Hosgri faults, but was later upgraded to withstand a 7.5 magnitude quake. It has redundant seismic monitoring and a safety system designed to shut it down promptly in the event of significant ground motion.\n\nIn 2008 the Shoreline Fault, which passes less than a mile from the plant, was discovered. The fault has the potential of triggering a 6.5-magnitude earthquake. Because the Richter magnitude scale is logarithmic, Diablo Canyon is designed to withstand an earthquake of shaking amplitude ten times larger than that which the Shoreline fault is capable of triggering, based on an analysis by plant owner Pacific Gas & Electric (PG&E).\n\nDiablo Canyon Power Plant (DCNPP/DCPP) is located proximal to the Los Osos, Hosgri, San Andreas and Shoreline faults. The discovery of these faults required design modifications during construction of the plant.\n\nThe Shoreline Fault is described in the November 2008 PG&E report as an \"alignment of microseismicity subparallel to the coastline indicating the possible presence of a previously unidentified fault located about 1 km offshore of DCPP.\" The plant's current operating license expires in 2024 and, in the aftermath of the Tohoku earthquake and tsunami, there is renewed opposition due to public perception that the risk of earthquake or tsunami might make the plant unsafe. Re-licensing is contingent upon consistency with the Coastal Act and thus review by the California Coastal Commission, however seismic issues are more properly within the purview of the Nuclear Regulatory Commission. The NRC in June 2011 announced that it had already completed its Safety Evaluation Report (SER) for the plant. In July, 2016 PG&E announced that it does not plan on relicensing either of its two Units.\n\nCritics contend that the Diablo Canyon (Nuclear) Power Plant was built so close to a set of geological fault lines that it is \"for all practical purposes\" to be regarded as built \"directly over\" a fault. They refer to the Hosgri fault, which was discovered while the plant was under construction. No scientific corroboration for these opinions has been presented, and PG&E maintains that \"new and extensive scientific re-evaluations performed at the direction of the Nuclear Regulatory Commission (NRC) continue to show that Diablo Canyon can safely withstand earthquakes, tsunamis and flooding that could potentially occur in the region.\"\n\nOn July 15, 2011, PBS aired a 17-minute video documenting the controversy over the discovery of the Shoreline fault. It details the differences of opinion between PG&E and a USGS geologist who disagrees with PG&E's assessment of the length of the Shoreline fault, and the potential for shaking should slippage occur at the Hosgri/Shoreline faults simultaneously.\n\nThe documentary also prominently features former California Assemblyman Sam Blakeslee, who after discovery of the Shoreline fault introduced legislation mandating 3-D seismic studies. Blakeslee, a former employee of Exxon-Mobil and an outspoken critic of the seismic evaluation undertaken in-house by PG&E, has come under fire by the California Democratic Party for alleged ties to California's fossil fuel industry. The Party alleges Blakeslee \"led the effort to insert offshore drilling rights along the Santa Barbara coast as part of [2009] state budget negotiations\", and he \"used his Assembly position to rake in more than $56,000 from his friends in Big Oil including Exxon, BP, Chevron and Valero\".\n\nAccording to the documentary, Blakeslee authored a bill in 2006 giving the California Energy Commission (CEC) \"the power to assess the safety of [California's] nuclear plants\", ostensibly to provide a seismic evaluation by an organization without involved financial interest. Others dispute CEC's impartiality. CEC Chairman Robert B. Weisenmiller co-founded and maintained a 24-year association with MRW & Associates, a company with areas of focus including \"Fossil fuel generation\", \"Natural gas pipelines and storage\", and \"Liquified natural gas\". On its website the company notes, \"Because MRW maintains a singular focus on power and gas markets, we are ideally placed to help clients take advantage of the changes in market structure and industry regulation.\" Nuclear energy has been perceived by many as a barrier to the expanding role of natural gas as a fuel for generating electricity, due to nuclear's lack of carbon emissions.\n\nThree Pliocene-Miocene marine sedimentary units dominate the geology of Diablo Canyon: the Pismo Formation, the Monterey Formation, and the Obispo Formation. According to a Lawrence-Berkeley report entitled Geologic Investigation of a Potential Site for a Next-Generation Reactor Neutrino Oscillation Experiment – Diablo Canyon, San Luis Obispo County, CA\nof the northwest trending Pismo Syncline.\"\nThe Obispo Formation is made up of marine and volcaniclastic sedimentary rocks.\nVarious techniques are used to identify faults. These include:\n\nIn 1989, the USGS of the Department of Interior published a report titled \"Empirical prediction of near-source ground motion for the Diablo Canyon power plant site, San Luis Obispo County, California\". Though the report was a primarily deterministic (empirical) analysis, it included a 1000-point Monte Carlo simulation to demonstrate that seven principal coefficients of ground motion were statistically significant at the 90-percent confidence level.\nMore recent studies include a probabilistic seismic hazard assessment. As a result of studies by PG&E, it is determined that \"ground motions from strike-slip earthquakes along the Hosgri fault zone have decreased and the ground motions from the reverse-slip earthquakes on the Los Osos and San Luis Bay fault zones have remained about the same\".\n\nThe Shoreline Fault is a 25 km long vertical strike-slip fault, identified in 2008, which lies approximately three hundred meters from the Diablo Canyon Nuclear Power Plant in California. According to Pacific Gas & Electric, the fault may produce quakes up to 6.5 magnitude. Mandated three-dimensional seismic studies have not been yet completed, and are not prerequisites for reissuance of the operating licenses for the two onsite units.\n\nThe company asserts the facility is able to withstand a 7.5 magnitude quake, and NRC's estimate of the risk each year of an earthquake intense enough to cause core damage to the reactor at Diablo Canyon was 1 in 23,810 according to an NRC study published in August 2010.\n\nThe Office of Nuclear Reactor Regulation of the Nuclear Regulatory Commission provides oversight of Incident response teams and assures provision of an appropriate Incident commander for all adverse events including earthquakes. The company does have in place the following measures:\n\nAccording to information provided by PG&E the plant is designed to accommodate a ground acceleration of 0.75 g – three quarters of gravity. By comparison, the ground tremors experienced by the Fukushima Daiichi power plant was reported as 0.2–0.51g, with the plant certified to withstand 0.18–0.36g. Subsequent inspection showed no significant damage to any of Fukushima Daiichi's four reactors from the earthquake, indicating all four reactors exceeded safety specifications by .02-.15 g.\n\nDiablo Canyon is designed to exceed a maximum oceanic flood level (combined tsunami, storm wave, high tide, and storm surge) of 32.0 feet above mean sea level (MSL) / 34.6 feet mean lower-low water (MLLW). The facility recently conducted a trial application of a Probabilistic Tsunami Hazard Assessment (PTHA) to evaluate lessons learned from the 2004 Sumatra tsunami as part of PG&E's Long-Term Seismic Plan (LTSP). This study shows that the hazard for tsunami waves of up to 3 meters (approximately 10 ft) is dominated by distant earthquakes around the circum-Pacific region, and is consistent with the historic record. The tsunami wave heights observed in San Luis Obispo County from the March 2011 Japanese tsunami were consistent with these results.\n\nSoil liquefaction resulting from earthquake activity has been raised as a concern however the plant and storage fuels are anchored on bedrock.\n\nIn April 2011, in the wake of the Fukushima nuclear incident in Japan, PG&E asked the NRC not to issue license renewals until PG&E can complete new seismic studies, which are expected to take at least three years.\nThe ongoing (as of 6/2011) seismic studies were recommended by the California Energy Commission and are approved and funded by the California Public Utilities Commission.\n\nOn June 21, 2016, PG&E announced a Joint Proposal with labor and environmental organizations to increase investment in energy efficiency, renewables and storage, while phasing out nuclear power. Specifically, the operating licenses for Diablo Canyon Units 1 and 2 will not be renewed. These are set to expire on November 2, 2024 and August 26, 2025 respectively.\n\n\n"}
{"id": "14650683", "url": "https://en.wikipedia.org/wiki?curid=14650683", "title": "Diamond Alkali", "text": "Diamond Alkali\n\nDiamond Alkali Company was an American chemical company incorporated in 1910 in West Virginia by a group of glass industry businessmen from Pittsburgh. The company soon established a large chemical plant at Fairport Harbor, Ohio, which would operate for over sixty years. In 1947, the headquarters of the company was moved from Pittsburgh to Cleveland. In 1967, Diamond Alkali and Shamrock Oil and Gas merged to form the Diamond Shamrock Corporation. Diamond Shamrock would go on to merge with Ultramar Corporation, and the combined company, Ultramar Diamond Shamrock Corporation, would in turn be acquired by Valero Energy Corporation in 2001.\n\nDiamond Alkali was largely responsible for contamination leading to the creation of a Superfund Site in the Ironbound section of Newark, New Jersey. Between 1951 and 1969, Diamond Alkali in Newark produced approximately of the herbicide Agent Orange. The plant had a reputation for accidents and producing the lowest quality (most contaminated with by-products) herbicides. Furthermore, the firm frequently dumped \"bad\" batches of the herbicide into the Passaic River. The former plant property and adjoining portions of the Lower Passaic River were declared a Superfund site in 1984. In 1986, the Diamond Shamrock Corporation agreed to pay $150,000 for a canvas tarpaulin to cover of the contaminated area. Remediation efforts at Diamond Alkali began in 2000 and ecological investigation, dredging, and other cleanup activities are still underway .\n"}
{"id": "18889617", "url": "https://en.wikipedia.org/wiki?curid=18889617", "title": "Diffusion damping", "text": "Diffusion damping\n\nIn modern cosmological theory, diffusion damping, also called photon diffusion damping, is a physical process which reduced density inequalities (anisotropies) in the early universe, making the universe itself and the cosmic microwave background radiation (CMB) more uniform. Around 300,000 years after the Big Bang, during the epoch of \"recombination\", diffusing photons travelled from hot regions of space to cold ones, equalising the temperatures of these regions. This effect is responsible, along with baryon acoustic oscillations, the Doppler effect, and the effects of gravity on electromagnetic radiation, for the eventual formation of galaxies and galaxy clusters, these being the dominant large scale structures which are observed in the universe. It is a damping \"by\" diffusion, not \"of\" diffusion.\nThe strength of diffusion damping is calculated by a mathematical expression for the \"damping factor\", which figures into the Boltzmann equation, an equation which describes the amplitude of perturbations in the CMB. The strength of the diffusion damping is chiefly governed by the distance photons travel before being scattered (diffusion length). The primary effects on the diffusion length are from the properties of the plasma in question: different sorts of plasma may experience different sorts of diffusion damping. The evolution of a plasma may also affect the damping process. The scale on which diffusion damping works is called the Silk scale and its value corresponds to the size of galaxies of the present day. The mass contained within the Silk scale is called the Silk mass and it corresponds to the mass of the galaxies.\n\nDiffusion damping took place about 13.8 billion years ago, during the stage of the early universe called \"recombination\" or matter-radiation \"decoupling\". This period occurred about 320,000 years after the Big Bang. This is equivalent to a redshift of around \"z\" = 1090. Recombination was the stage during which simple atoms, e.g. hydrogen and helium, began to form in the cooling, but still very hot, soup of protons, electrons and photons that composed the universe. Prior to the recombination epoch, this \"soup\", a plasma, was largely opaque to the electromagnetic radiation of photons. This meant that the permanently excited photons were scattered by the protons and electrons too often to travel very far in straight lines. During the recombination epoch, the universe cooled rapidly as free electrons were captured by atomic nuclei; atoms formed from their constituent parts and the universe became transparent: the amount of photon scattering decreased dramatically. Scattering less, photons could diffuse (travel) much greater distances. There was no significant diffusion damping for electrons, which could not diffuse nearly as far as photons could in similar circumstances. Thus all damping by electron diffusion is negligible when compared to photon diffusion damping.\n\nAcoustic perturbations of initial density fluctuations in the universe made some regions of space hotter and denser than others. These differences in temperature and density are called \"anisotropies\". Photons diffused from the hot, overdense regions of plasma to the cold, underdense ones: they dragged along the protons and electrons: the photons pushed electrons along, and these, in turn, pulled on protons by the Coulomb force. This caused the temperatures and densities of the hot and cold regions to be averaged and the universe became less anisotropic (characteristically various) and more \"isotropic\" (characteristically uniform). This reduction in anisotropy is the \"damping\" of diffusion damping. Diffusion damping thus damps temperature and density anisotropies in the early universe. With baryonic matter (protons and electrons) escaping the dense areas along with the photons; the temperature and density inequalities were \"adiabatically\" damped. That is to say the ratios of photons to baryons remained constant during the damping process.\n\nPhoton diffusion was first described in Joseph Silk's 1968 paper entitled \"Cosmic Black-Body Radiation and Galaxy Formation\", which was published in \"The Astrophysical Journal\". As such, diffusion damping is sometimes also called Silk damping, though this term may apply only to one possible damping scenario. Silk damping was thus named after its discoverer.\nThe magnitude of diffusion damping is calculated as a \"damping factor\" or \"suppression factor\", represented by the symbol formula_1, which figures into the Boltzmann equation, an equation which describes the amplitude of perturbations in the CMB. The strength of the diffusion damping is chiefly governed by the distance photons travel before being scattered (diffusion length). What affect the diffusion length are primarily the properties of the plasma in question: different sorts of plasma may experience different sorts of diffusion damping. The evolution of a plasma may also affect the damping process.\n\nWhere:\n\nThe damping factor formula_1, when factored into the Boltzmann equation for the cosmic microwave background radiation (CMB), reduces the amplitude of perturbations:\n\nWhere:\n\nMathematical calculations of the damping factor depend on formula_14, or the \"effective diffusion scale\", which in turn depends on a crucial value, \"the diffusion length\", formula_15. The diffusion length relates how far photons travel during diffusion, and comprises a finite number of short steps in random directions. The average of these steps is the \"Compton mean free path\", and is denoted by formula_16. As the direction of these steps are randomly taken, formula_15 is approximately equal to formula_18, where formula_19 is the number of steps the photon takes before the conformal time at decoupling (formula_10).\n\nThe diffusion length increases at recombination because the mean free path does, with less photon scattering occurring; this increases the amount of diffusion and damping. The mean free path increases because the \"electron ionisation fraction\", formula_21, decreases as ionised hydrogen and helium bind with the free, charged electrons. As this occurs, the mean free path increases proportionally: formula_22. That is, the mean free path of the photons is inversely proportional to the electron ionisation fraction and the baryon number density (formula_23). That means that the more baryons there were, and the more they were ionised, the shorter the average photon could travel before encountering one and being scattered. Small changes to these values before or during recombination can augment the damping effect considerably. This dependence on the baryon density by photon diffusion allows scientists to use analysis of the latter to investigate the former, in addition to the history of ionisation.\n\nThe effect of diffusion damping is greatly augmented by the finite width of the surface of last scattering (SLS). The finite width of the SLS means the CMB photons we see were not all emitted at the same time, and the fluctuations we see are not all in phase. It also means that during recombination, the diffusion length changed dramatically, as the ionisation fraction shifted.\n\nIn general, diffusion damping produces its effects independent of the cosmological model being studied, thereby masking the effects of other, model-\"dependent\" phenomena. This means that without an accurate model of diffusion damping, scientists cannot judge the relative merits of cosmological models, whose theoretical predictions cannot be compared with observational data, this data being obscured by damping effects. For example, the peaks in the power spectrum due to acoustic oscillations are decreased in amplitude by diffusion damping. This deamplification of the power spectrum hides features of the curve, features that would otherwise be more visible.\n\nThough general diffusion damping can damp perturbations in collisionless dark matter simply due to photon dispersion, the term \"Silk damping\" applies only to damping of adiabatic models of baryonic matter, which is coupled to the diffusing photons, not dark matter, and diffuses with them. Silk damping is not as significant in models of cosmological development which posit early isocurvature fluctuations (i.e. fluctuations which do not require a constant ratio of baryons and photons). In this case, increases in baryon density do not require a corresponding increases in photon density, and the lower the photon density, the less diffusion there would be: the less diffusion, the less damping. Photon diffusion is not dependent on the causes of the initial fluctuations in the density of the universe.\n\nDamping occurs at two different scales, with the process working more quickly over short ranges than over longer distances. Here, a short length is one that is lower than the mean free path of the photons. A long distance is one that is greater than the mean free path, if still less than the diffusion length. On the smaller scale, perturbations are damped almost instantaneously. On the larger scale, anisotropies are decreased more slowly, with significant degradation happening within one unit of Hubble time.\n\nDiffusion damping exponentially decreases anisotropies in the CMB on a scale (the Silk scale) much smaller than a degree, or smaller than approximately 3 megaparsecs. This angular scale corresponds to a multipole moment formula_24. The mass contained within the Silk scale is the \"silk mass\". Numerical evaluations of the Silk mass yield results on the order of formula_25 solar masses at recombination and on the order of the mass of a present-day galaxy or galaxy cluster in the current era.\n\nScientists say diffusion damping affects \"small\" angles and corresponding anisotropies. Other effects operate on a scale called \"intermediate\" formula_27 or \"large\" formula_28. Searches for anisotropies on a small scale are not as difficult as those on larger scales, partly because they may employ ground-based telescopes and their results can be more easily predicted by current theoretical models.\n\nScientists study photon diffusion damping (and CMB anisotropies in general) because of the insight the subject provides into the question, \"How did the universe come to be?\". Specifically, primordial anisotropies in the temperature and density of the universe are supposed to be the causes of later large-scale structure formation. Thus it was the amplification of small perturbations in the pre-recombination universe that grew into the galaxies and galaxy clusters of the present era. Diffusion damping made the universe isotropic within distances on the order of the Silk Scale. That this scale corresponds to the size of observed galaxies (when the passage of time is taken into account) implies that diffusion damping is responsible for limiting the size of these galaxies. The theory is that clumps of matter in the early universe became the galaxies that we see today, and the size of these galaxies is related to the temperature and density of the clumps.\n\nDiffusion may also have had a significant effect on the evolution of primordial cosmic magnetic fields, fields which may have been amplified over time to become galactic magnetic fields. However, these cosmic magnetic fields may have been damped by radiative diffusion: just as acoustic oscillations in the plasma were damped by the diffusion of photons, so were magnetosonic waves (waves of ions travelling through a magnetised plasma). This process began before the era of neutrino decoupling and ended at the time of recombination.\n\n\n\n"}
{"id": "39716485", "url": "https://en.wikipedia.org/wiki?curid=39716485", "title": "Dynamical heterogeneity", "text": "Dynamical heterogeneity\n\nDynamical heterogeneity describes the behavior of glass-forming materials when undergoing a phase transition from the liquid state to the glassy state. In dynamical heterogeneity, the dynamics of cooling to a glassy state show variation within the material.\n\nPolymer properties include viscoelasticity and may be synthetic or natural. When a polymeric liquid is cooled below its freezing temperature without crystallizing, it becomes a supercooled liquid. When the supercooled liquid is further cooled, it becomes a glass.\n\nThe temperature at which a polymer becomes a glass by fast cooling is called the glass transition temperature T. At this temperature, viscosity reaches up to 10 poise depending upon cooling-rate.\n\nIt is possible for a phase transition from polymer to glassy state to take place. Polymer glass transitions have many determinants including relaxation time, viscosity and cage size. At low temperatures the dynamics become very slow (sluggish) and relaxation time increases from picoseconds to seconds, minutes, or more. At high temperatures, the correlation function has a ballistic regime for very short times (when particles do not interact) and a microscopic regime. In the microscopic regime, the correlation functions decay exponentially at high temperatures. At low temperatures the correlation functions have an intermediate regime in which particles have both slow and fast relaxations. The slow relaxation is an indication of cages in the glassy system. In glassy state density is not homogeneous i.e. particles are localized in different density distributions in space. It means that density fluctuations are present in the system. Particle dynamics become very slow because temperature is directly proportional to kinetic energy causing the particles trapped in local regions by each other. Particles are doing rattling motion inside these cages and cooperate with each other. These regions in the glassy polymer are called cages. In the intermediate regime each particle has its own and different relaxation time.\n\nThe dynamics in all these cases are different, so at a small scale, there are a large number of cages in the system relative to the size of the whole system. This is known as dynamical heterogeneity in the glassy state of the system. A measurement of dynamical heterogeneity can be done by calculating correlation functions like Non-Gaussian parameter, four point correlation functions(Dynamic Susceptibility) and three time correlation functions.\n"}
{"id": "1700482", "url": "https://en.wikipedia.org/wiki?curid=1700482", "title": "Electrode line", "text": "Electrode line\n\nAn electrode line is used in some high-voltage direct-current (HVDC) power transmission systems which use the ground or sea as the return path for electric current. Many long-distance HVDC systems use sea or ground return for the DC neutral current since this is considerably cheaper than providing a dedicated \"metallic return\" conductor on an overhead wire or cable. \n\nThe connection into the ground requires a specially designed \"ground electrode\" (or \"earth electrode\"). The electrode is usually located several tens of kilometres from the converter station in order to avoid possible problems or corrosion in the converter station grounding system. The electrode line connects the converter station to the grounding electrode. The electrode line can be implemented, depending upon the situation of the electrode (onshore or in the sea), as ground cables, as overhead line, or as a combination of ground cable and overhead line.\n\nHVDC electrodes are used in most bipolar HVDC transmission systems as a means to improve the reliability of the entire system. In the event that one of the poles in the bipolar system is faulted, the current path will switch to ground return, thus allowing the system to continue operating at reduced capacity and reducing the possibility that a pole fault will cause a bipolar outage. Usually these ground return paths are only used for very short durations until the faulted pole can be returned to service. The ground current in such schemes can flow in either direction, and the electrodes have to be designed to be reversible, operating either as an anode or cathode.\n\nHVDC electrodes are also used in some monopolar HVDC systems, for example the Italy–Corsica–Sardinia scheme. In such systems the electrode line permanently carries the same current as the high-voltage conductor; however since the ground current is then only unidirectional, one of the electrodes (the cathode) can be of simpler design since corrosion is not a problem for cathode electrodes.\n\nThe operating voltage of the electrode line usually is in the range of approx. 10-20kV (medium voltage range).\n\n\n"}
{"id": "34016837", "url": "https://en.wikipedia.org/wiki?curid=34016837", "title": "Experiment (horse-powered boat)", "text": "Experiment (horse-powered boat)\n\nExperiment was an early 19th-century boat powered by horses and incorporating the idea of a screw propeller, which was a new idea at the time.\n\n\"Experiment\" was a horse-powered ferry boat. It was a 12-ton, three-masted boat drawing a few feet of water, about long by beam. Its driving mechanism was an in-water screw invented by David Grieve in 1801. The boat was constructed by David Wilkinson (some sources give his name as \"Varnum\") in 1807 to 1810, depending on the source. It was propelled by a \"goose-foot paddle,\" a large mechanical screw propeller in the water instead of a paddle wheel at water surface. The new technology devised by Grieve and Wilkinson was powered by eight horses on a treadmill. The technology to propel the boat upstream was originally invented by David Grieve and granted a patent February 24, 1801 in the category of \"Boats to ascend rivers\". The complete recorded patent was lost in the 1836 U.S. Patent Office fire. The idea of propelling vessels by a mechanical screw in the water is now referred to as Ericsson's propeller.\n\nIt is reported that \"Experiment\" made one unsuccessful voyage, as it ran aground on the return trip. The mechanism and associated parts were put together by Ephraim Southworth; little thought was put into the construction and it was poorly built. The maiden voyage was in June 1809 with a group of gentlemen from the Grand Lodge of the State. The first attempt of the \"Screw Boat\" began at Jackson's Wharf on Eddy's Point near Providence, Rhode Island, with a destination of Pawtuxet Village. The eight horses for the \"horse power\" were owned by Marvin Morris; they were connected to a poorly designed contraption to make the boat move. It obtained a top speed of four knots with the help of a tide going in her direction and the wind on her back. It managed to get to Pawtuxet Village, where there was much celebration over its success. The return trip, however, resulted in humiliation when a gust of wind drove \"Experiment\" onto mud flats, causing its demise.\n\nThe \"Experiment\" venture had sold shares of stock from a prospectus to raise money to build it. There was so much confidence in the venture that tickets were engraved by William Hamlin for its anticipated voyages to \"New-Port\" and Providence. Ultimately, the horse boat and all the associated items were seized by the Sheriff at the behest of Grieve's creditors and sold for lack of payments on the loans, since it was not a successful venture. Wilkinson later said that \"after the frolic\" it was \"hauled up\" and allowed to go to waste and ruin. Nevertheless, the ship was carefully studied by Daniel French, who did the drawings for Robert Fulton's \"North River Steamboat\" (known as \"Clermont\"), and may have benefited that enterprise.\n\n\"Experiment\" is important as a precursor of public transportation on rivers, and it was the forerunner of a number of horse-powered boats, chiefly ferries used for more than a half-century along the eastern seaboard of the United States. Most commonly, those were paddle wheel boats, not screw-type propellers. Inclined treadmills were often used.\n\n\n"}
{"id": "4184889", "url": "https://en.wikipedia.org/wiki?curid=4184889", "title": "Fast Breeder Test Reactor", "text": "Fast Breeder Test Reactor\n\nThe Fast Breeder Test Reactor (FBTR) is a breeder reactor located at Kalpakkam, India. The Indira Gandhi Center for Atomic Research (IGCAR) and Bhabha Atomic Research Centre (BARC) jointly designed, constructed, and operate the reactor.\n\nIt first reached criticality in October 1985, making India the seventh nation to have the technology to build and operate a breeder reactor after United States, UK, France, Japan, Germany, and Russia. The reactor was designed to produce 40 MW of thermal power and 13.2 MW of electrical power. The FBTR has rarely operated at its designed capacity and had to be shut down between 1987 and 1989 due to technical problems. From 1989 to 1992, the reactor operated at 1 MW. In 1993, the reactor's power level was raised to 10.5 MW. The initial nuclear fuel core used in the FBTR consisted of approximately 50 kg of weapons-grade plutonium. In September 2002, fuel burn-up in the FBTR for the first time reached the 100,000 megawatt-days per metric ton uranium (MWd/MTU) mark. This is considered an important milestone in breeder reactor technology. Using the experience gained from the operation of the FBTR, a 500 MWe Prototype Fast Breeder Reactor (PFBR) is in advanced stage of construction at Kalpakkam.\n\nThe reactor uses a plutonium-uranium mixed carbide fuel and liquid sodium as a coolant. The fuel is an indigenous mix of 70 percent plutonium carbide and 30 percent uranium carbide. Plutonium for the fuel is extracted from irradiated fuel in the Madras power reactors and reprocessed in Tarapur.\n\nSome of the uranium is created from the transmutation of thorium bundles that are also placed in the core.\n"}
{"id": "5663113", "url": "https://en.wikipedia.org/wiki?curid=5663113", "title": "Flue-gas stack", "text": "Flue-gas stack\n\nA flue-gas stack is a type of chimney, a vertical pipe, channel or similar structure through which combustion product gases called flue gases are exhausted to the outside air. Flue gases are produced when coal, oil, natural gas, wood or any other fuel is combusted in an industrial furnace, a power plant's steam-generating boiler, or other large combustion device. Flue gas is usually composed of carbon dioxide (CO) and water vapor as well as nitrogen and excess oxygen remaining from the intake combustion air. It also contains a small percentage of pollutants such as particulate matter, carbon monoxide, nitrogen oxides and sulfur oxides. The flue gas stacks are often quite tall, up to 400 metres (1300 feet) or more, so as to disperse the exhaust pollutants over a greater area and thereby reduce the concentration of the pollutants to the levels required by governmental environmental policy and environmental regulation.\n\nWhen the flue gases are exhausted from stoves, ovens, fireplaces, or other small sources within residential abodes, restaurants, hotels, or other public buildings and small commercial enterprises, their flue gas stacks are referred to as chimneys.\n\nThe first industrial chimneys were built in the mid-17th century when it was first understood how they could improve the combustion of a furnace by increasing the draft of air into the combustion zone. As such, they played an important part in the development of reverberatory furnaces and a coal-based metallurgical industry, one of the key sectors of the early Industrial Revolution. Most 18th-century industrial chimneys (now commonly referred to as flue gas stacks) were built into the walls of the furnace much like a domestic chimney. The first free-standing industrial chimneys were probably those erected at the end of the long condensing flues associated with smelting lead.\n\nThe powerful association between industrial chimneys and the characteristic smoke-filled landscapes of the industrial revolution was due to the universal application of the steam engine for most manufacturing processes. The chimney is part of a steam-generating boiler, and its evolution is closely linked to increases in the power of the steam engine. The chimneys of Thomas Newcomen’s steam engine were incorporated into the walls of the engine house. The taller, free-standing industrial chimneys that appeared in the early 19th century were related to the changes in boiler design associated with James Watt’s \"double-powered\" engines, and they continued to grow in stature throughout the Victorian period. Decorative embellishments are a feature of many industrial chimneys from the 1860s, with over-sailing caps and patterned brickwork.\n\nThe invention of fan-assisted forced draft in the early 20th century removed the industrial chimney's original function, that of drawing air into the steam-generating boilers or other furnaces. With the replacement of the steam engine as a prime mover, first by diesel engines and then by electric motors, the early industrial chimneys began to disappear from the industrial landscape. Building materials changed from stone and brick to steel and later reinforced concrete, and the height of the industrial chimney was determined by the need to disperse combustion flue gases to comply with governmental air pollution control regulations.\n\nThe combustion flue gases inside the flue gas stacks are much hotter than the ambient outside air and therefore less dense than the ambient air. That causes the bottom of the vertical column of hot flue gas to have a lower pressure than the pressure at the bottom of a corresponding column of outside air. That higher pressure outside the chimney is the driving force that moves the required combustion air into the combustion zone and also moves the flue gas up and out of the chimney. That movement or flow of combustion air and flue gas is called \"natural draft\", \"natural ventilation\", \"chimney effect\", or \"stack effect\". The taller the stack, the more draft is created.\n\nThe equation below provides an approximation of the pressure difference, Δ\"P\", (between the bottom and the top of the flue gas stack) that is created by the draft:\n\nwhere:\n\nThe above equation is an approximation because it assumes that the molar mass of the flue gas and the outside air are equal and that the pressure drop through the flue gas stack is quite small. Both assumptions are fairly good but not exactly accurate.\n\nAs a \"first guess\" approximation, the following equation can be used to estimate the flue-gas flow-rate induced by the draft of a flue-gas stack. The equation assumes that the molar mass of the flue gas and the outside air are equal and that the frictional resistance and heat losses are negligible:.\n\nwhere:\n\nAlso, this equation is only valid when the resistance to the draft flow is caused by a single orifice characterized by the discharge coefficient C. In many, if not most situations, the resistance is primarily imposed by the flue stack itself. In these cases, the resistance is proportional to the stack height H. This causes a cancellation of the H in the above equation predicting Q to be invariant with respect to the flue height.\n\nDesigning chimneys and stacks to provide the correct amount of natural draft involves a great many factors such as:\n\n\nThe calculation of many of the above design factors requires trial-and-error reiterative methods.\n\nGovernment agencies in most countries have specific codes which govern how such design calculations must be performed. Many non-governmental organizations also have codes governing the design of chimneys and stacks (notably, the ASME codes).\n\nThe design of large stacks poses considerable engineering challenges. Vortex shedding in high winds can cause dangerous oscillations in the stack, and may lead to its collapse. The use of helical faring is common to prevent this process occurring at or close to the resonant frequency of the stack.\n\nSome fuel-burning industrial equipment does not rely upon natural draft. Many such equipment items use large fans or blowers to accomplish the same objectives, namely: the flow of combustion air into the combustion chamber and the flow of the hot flue gas out of the chimney or stack.\n\nA great many power plants are equipped with facilities for the removal of sulfur dioxide (i.e., flue-gas desulfurization), nitrogen oxides (i.e., selective catalytic reduction, exhaust gas recirculation, thermal deNOx, or low NOx burners) and particulate matter (i.e., electrostatic precipitator)s. At such power plants, it is possible to use a cooling tower as a flue gas stack. Examples can be seen in Germany at the Power Station Staudinger Grosskrotzenburg and at the Rostock Power Station. Power plants without flue gas purification, would experience serious corrosion in such stacks.\n\nIn the United States and a number of other countries, atmospheric dispersion modeling studies are required to determine the flue gas stack height needed to comply with the local air pollution regulations. The United States also limits the maximum height of a flue gas stack to what is known as the \"Good Engineering Practice (GEP)\" stack height. In the case of existing flue gas stacks that exceed the GEP stack height, any air pollution dispersion modelling studies for such stacks must use the GEP stack height rather than the actual stack height.\n\n\n"}
{"id": "42562531", "url": "https://en.wikipedia.org/wiki?curid=42562531", "title": "Heptatriacontanoic acid", "text": "Heptatriacontanoic acid\n\nHeptatriacontanoic acid is a 37-carbon saturated fatty acid.\n\nHeptatriacontanoic acid is present in \"Abelmoschus manihot\" and \"Alpinia nigra\". Heptatriacontanoic acid was also measured in zooplankton.\n\nThe compound 4,21-dimethyl-5,19-di-(trans)-enoyl-heptatriacontanoic acid is the \"structure of the major homolog of free mycobacteric acids\" of \"Mycobacterium brumae\".\n\nU.S. patent 5502226 covers a method of ω-hydroxy acid preparation that includes heptatriacontanoic acid.\n\n"}
{"id": "2473450", "url": "https://en.wikipedia.org/wiki?curid=2473450", "title": "Hydrospring", "text": "Hydrospring\n\nA hydrospring is a mechanical device that performs a similar function to a spring. The advantage of a hydrospring over a normal spring is the increased damping that it performs. The damping is achieved by hydraulic fluid (typically oil) being driven through holes in a piston, as that piston moves in response to a force.\n\nHydrosprings are used in a variety of applications: shock absorbers, vibration dampers on train suspension, bulldozer blade shock absorbers and as recoil absorbers for artillery.\n\n"}
{"id": "9730967", "url": "https://en.wikipedia.org/wiki?curid=9730967", "title": "Infrared heater", "text": "Infrared heater\n\nAn infrared heater or heat lamp is a body with a higher temperature which transfers energy to a body with a lower temperature through electromagnetic radiation. Depending on the temperature of the emitting body, the wavelength of the peak of the infrared radiation ranges from to 1 mm. No contact or medium between the two bodies is needed for the energy transfer. Infrared heaters can be operated in vacuum or atmosphere.\n\nOne classification of infrared heaters is by the wavelength bands of infrared emission.\n\n\nGerman-British astronomer Sir William Herschel is credited with the discovery of infrared in 1800. He made an instrument called a spectrometer to measure the magnitude of radiant power at different wavelengths. This instrument was made from three pieces. The first was a prism to catch the sunlight and direct and disperse the colors down onto a table, the second was a small panel of cardboard with a slit wide enough for only a single color to pass through it and finally, three mercury-in-glass thermometers. Through his experiment Herschel found that red light had the highest degree of temperature change in the light spectrum, however, infrared heating was not commonly used until World War II. During World War II infrared heating became more widely used and recognized. The main applications were in the metal finishing fields, particularly in the curing and drying of paints and lacquers on military equipment. Banks of lamp bulbs were used very successfully but by today's standards, the power intensities were very low. The technique offered much faster drying times than the fuel convection ovens of the time. Production bottlenecks were mitigated and military supplies to the armed forces were maintained. After World War II the adoption of infrared heating techniques continued but on a much slower basis. In the mid 1950s the motor vehicle industry began to show interest in the capabilities of infrared for paint curing and a number of production line infrared tunnels came into use.\nThe most common filament material used for electrical infrared heaters is tungsten wire, which is coiled to provide more surface area. Low temperature alternatives for tungsten are carbon, or alloys of iron, chromium, and aluminum (trademark and brand name \"Kanthal\"). While carbon filaments are more fickle to produce, they heat up much more quickly than a comparable medium-wave heater based on a FeCrAl filament.\n\nWhen light is undesirable or not necessary in a heater, ceramic infrared radiant heaters are the preferred choice. Containing 8 meters of coiled alloy resistance wire, they emit a uniform heat across the entire surface of the heater and the ceramic is 90% absorbent of the radiation. As absorption and emission are based on the same physical causes in each body, ceramic is ideally suited as a material for infrared heaters.\n\nIndustrial infrared heaters sometimes use a gold coating on the quartz tube that reflects the infrared radiation and directs it towards the product to be heated. Consequently, the infrared radiation impinging on the product is virtually doubled. Gold is used because of its oxidation resistance and very high infrared reflectivity of approximately 95%.\n\nInfrared heaters are commonly used in infrared modules (or emitter banks) combining several heaters to achieve larger heated areas.\n\nInfrared heaters are usually classified by the wavelength they emit:\n\nNear infrared (NIR) or short-wave infrared heaters operate at high filament temperatures above and when arranged in a field reach high power densities of some hundreds of kW/m. Their peak wavelength is well below the absorption spectrum for water, making them unsuitable for many drying applications. They are well suited for heating of silica where a deep penetration is needed.\n\nMedium-wave and carbon (CIR) infrared heaters operate at filament temperatures of around . They reach maximum power densities of up to (medium-wave) and (CIR).\n\nFar infrared emitters (FIR) are typically used in the so-called low-temperature far infrared saunas. These constitute only the higher and more expensive range of the market of infrared sauna. Instead of using carbon, quartz or high watt ceramic emitters, which emit near and medium infrared radiation, heat and light, far infrared emitters use low watt ceramic plates that remain cold, while still emitting far infrared radiation.\n\nThe relationship between temperature and peak wavelength is expressed by Wien's displacement law.\n\nMetal wire heating elements first appeared in the 1920s. These elements consist of wire made from chromel. Chromel is made from nickel and chrome and it is also known as nichrome. This wire was then coiled into a spiral and wrapped around a ceramic body. When heated to high temperatures it forms a protective layer of chromium-oxide which protects the wire from burning and corrosion, this also causes the element to glow.\n\nA heat lamp is an incandescent light bulb that is used for the principal purpose of creating heat. The spectrum of black body radiation emitted by the lamp is shifted to produce more infrared light. Many heat lamps include a red filter to minimize the amount of visible light emitted. Heat lamps often include an internal reflector.\n\nHeat lamps are commonly used in shower and bathrooms to warm bathers and in food-preparation areas of restaurants to keep food warm before serving. They are also commonly used for animal husbandry. Lights used for poultry are often called brooding lamps. Aside from young birds, other types of animals which can benefit from heat lamps include reptiles, amphibians, insects, arachnids, and the young of some mammals.\n\nThe sockets used for heat lamps are usually ceramic because plastic sockets can melt or burn when exposed to the large amount of waste heat produced by the lamps, especially when operated in the \"base up\" position. The shroud or hood of the lamp is generally metal. There may be a wire guard over the front of the shroud, to prevent touching the hot surface of the bulb.\n\nOrdinary household white incandescent bulbs can also be used as heat lamps, but red and blue bulbs are sold for use in brood lamps and reptile lamps. 250-watt heat lamps are commonly packaged in the \"R40\" (5\" reflector lamp) form factor with an intermediate screw base.\n\nHeat lamps can be used as a medical treatment to provide dry heat when other treatments are ineffective or impractical.\n\nCeramic infrared heating elements are used in a diverse range of industrial processes where long wave infrared radiation is required. Their useful wavelength range is 2–10 µm. They are often used in the area of animal/pet healthcare too. The ceramic infrared heaters (emitters) are manufactured with three basic emitter faces: trough (concave), flat, and bulb or Edison screw element for normal installation via an E27 ceramic lamp holder.\n\nThis heating technology is used in some expensive infrared saunas. It is also found in space heaters. These heaters use low watt density ceramic emitters (usually fairly big panels) which emit long wave infrared radiation. Because the heating elements are at a relatively low temperature, far-infrared heaters do not give emissions and smell from dust, dirt, formaldehyde, toxic fumes from paint-coating, etc. This has made this type of space heating very popular among people with severe allergies and multiple chemical sensitivity in Europe. Because far infrared technology does not heat the air of the room directly, it is important to maximize the exposure of available surfaces which then re-emit the warmth to provide an even all round ambient warmth.\n\nHalogen lamps are incandescent lamps filled with highly pressurized inert gas. This gas is combined with a small amount of halogen gas (bromine or iodine) which causes tungsten atoms to regenerate by lessening the evaporation of the filament. This leads to a much longer life of halogen lamps than other incandescent lamps. Due to the high pressure and temperature halogen lamps produce, they are relatively small and made out of quartz glass because it has a higher melting point than standard glass. Common uses for halogen lamps are table top heaters.\n\nQuartz infrared heating elements emit medium wave infrared energy and are particularly effective in systems where rapid heater response is required. Tubular infrared lamps in quartz bulbs produce infrared radiation in wavelengths of 1.5–8 µm. The enclosed filament operates at around , producing more shorter-wavelength radiation than open wire-coil sources. Developed in the 1950s at General Electric, these lamps produce about () and can be combined to radiate 500 watts per square foot (). To achieve even higher power densities, halogen lamps were used. Quartz infrared lamps are used in highly polished reflectors to direct radiation in a uniform and concentrated pattern.\n\nQuartz heat lamps are used in food processing, chemical processing, paint drying, and thawing of frozen materials. They can also be used for comfort heating in cold areas, in incubators, and in other applications for heating, drying, and baking. During development of space re-entry vehicles, banks of quartz infrared lamps were used to test heat shield materials at power densities as high as 28 kilowatts/square foot (300 kW/m).\n\nMost common designs consist of either a satin milky-white quartz glass tube or clear quartz with an electrically resistant element, usually a tungsten wire, or a thin coil of iron-chromium-aluminum alloy. The atmospheric air is removed and filled with inert gases such as nitrogen and argon then sealed. In quartz halogen lamps, a small amount of halogen gas is added to prolong the heater's operational life.\n\nMuch of the infrared and visible energy released is caused by the direct heating of the quartz material, 97% of the near infrared is absorbed by the silica quartz glass tube causing the temperature of the tube wall to increase, this causes the silicon-oxygen bond to radiate far infrared rays. Quartz glass heating elements were originally designed for lighting applications, but when a lamp is at full power less than 5% of the emitted energy is in the visible spectrum.\n\nQuartz tungsten infrared heaters emit medium wave energy reaching operating temperatures of up to (medium wave) and (short wave). They reach operating temperature within seconds. Peak wavelength emissions of approximately 1.6 µm (medium wave infrared) and 1 µm (short wave infrared).\n\nCarbon heaters use a carbon fiber heating element capable of producing long, medium and short wave far infrared heat. They need to be accurately specified for the spaces to be heated.\n\nThere are two basic types of infrared radiant heaters.\nRadiant tube gas-fired heaters used for industrial and commercial building space heating burn natural gas or propane to heat a steel emitter tube. Gas passing through a control valve flows through a cup burner or a venturi. The combustion product gases heat the emitter tube. As the tube heats, radiant energy from the tube strikes floors and other objects in the area, warming them. This form of heating maintains warmth even when a large volume of cold air is suddenly introduced, such as in maintenance garages. They cannot however, combat a cold draught.\n\nThe efficiency of an infrared heater is a rating of the total energy consumed by the heater compared to the amount of infrared energy generated. While there will always be some amount of convective heat generated through the process, any introduction of air motion across the heater will reduce its infrared conversion efficiency. With new untarnished reflectors, radiant tubes have a downward radiant efficiency of about 60%. (The other 40% comprises unrecoverable upwards radiant and convective losses, and flue losses.)\n\nIn addition to the dangers of touching the hot bulb or element, high-intensity short-wave infrared radiation may cause indirect thermal burns when the skin is exposed for too long or the heater is positioned too close to the subject. Individuals exposed to large amounts of infrared radiation (like glass blowers and arc welders) over an extended period of time may develop depigmentation of the iris and opacity of the aqueous humor, so exposure should be moderated.\n\nElectrically-heated infrared heaters radiate up to 86% of their input as radiant energy. Nearly all the electrical energy input is converted into infrared radiant heat in the filament and directed onto the target by reflectors. Some heat energy is removed from the heating element by conduction or convection, which may be no loss at all for some designs where all of the electrical energy is desired in the heated space, or may be considered a loss, in situations where only the radiative heat transfer is desired or productive. \n\nFor practical applications, the efficiency of the infrared heater depends on matching the emitted wavelength and the absorption spectrum of the material to be heated. For example, the absorption spectrum for water has its peak at around . This means that emission from medium-wave or carbon infrared heaters is much better absorbed by water and water-based coatings than NIR or short-wave infrared radiation. The same is true for many plastics like PVC or polyethylene. Their peak absorption is around . On the other hand, some metals absorb only in the short-wave range and show a strong reflectivity in the medium and far infrared. This makes a careful selection of the right infrared heater type important for energy efficiency in the heating process.\n\nCeramic elements operate in the temperature of producing infrared wavelengths in the 2000 to range. Most plastics and many other materials absorb infrared best in this range, which makes the ceramic heater most suited for this task.\n\nIR heaters can satisfy a variety of heating requirements, including:\n\n\nThus, IR heaters are applied for many purposes including:\n\n\n"}
{"id": "1392633", "url": "https://en.wikipedia.org/wiki?curid=1392633", "title": "Kudurru", "text": "Kudurru\n\nKudurru was a type of stone document used as boundary stones and as records of land grants to vassals by the Kassites in ancient Babylonia between the 16th and 12th centuries BCE. The word is Akkadian for \"frontier\" or \"boundary\" (cf. \"gader\", fence, boundary; \"jadr\", \"jidar\" 'wall'; pl. \"judūr\"). The kudurrus are the only surviving artworks for the period of Kassite rule in Babylonia with examples kept in the Louvre, the British Museum, and the National Museum of Iraq.\n\nThe kudurrus recorded the land granted by the king to his vassals as a record of his decision. The original kudurru would be stored in a temple while the person granted the land would be given a clay copy to use as a boundary stone to confirm legal ownership.\n\nThe kudurrus would contain symbolic images of the deities who were protecting the contract, the contract, and the divine curse that would be placed on a person who broke the contract. Some kudurrus also contained an image of the king who granted the land. As they contained a great deal of images as well as a contract, kudurrus were engraved on large slabs of stone.\n\nKassite-era kudurrus, in approximate chronological order:\n\n\nPost-Kassite kudurrus:\n\n\n\n"}
{"id": "45370555", "url": "https://en.wikipedia.org/wiki?curid=45370555", "title": "LUTZ Pathfinder", "text": "LUTZ Pathfinder\n\nThe two-seater prototype pod has been built by Coventry-based RDM Group, and was first shown to the public in February 2015.\n\nThe LUTZ (Low-carbon Urban Transport Zone) Pathfinder pod is part of the UK Government's Transport Systems Catapult Autodrive project, a 20 million project.\n\nThree pods were tested initially in Milton Keynes during 2015 to ensure that they can comply with the Highway Code. A further trial, with a four seat vehicle developed from the LUTZ, commenced along a guided busway near Cambridge for possible use in an after hours service.\n\nThe pod is a two-seater electric car with space for luggage. It has a limited top speed of and has a range of or can run for eight hours. The self-driving equipment includes 19 sensors, cameras, radar and Lidar. Users can hail them by using a smartphone app.\n\nThe autonomous control software is developed by Mobile Robotics Group from University of Oxford.\n\nThe Lutz Pathfinder pod has been developed by the UK Automotive Council, Department for Business, Innovation and Skills and RDM Group.\n\nThe first trial of autonomous operation on a public road, with pedestrians, cycles and other vehicles, was conducted in Milton Keynes on 11 October 2016. The vehicles \"operated as expected.\"\n\nIn October 2017 a trial service, using a four-seat vehicle, between Trumpington Park and Ride and Cambridge railway station along the guided busway, for possible use in an after hours service. The £250,000 project was part funded by Innovate UK and delivered in partnership with Connecting Cambridgeshire and the Smart Cambridge Programme. If the trial is successful a ten seat vehicle will be used in routine service.\n\n"}
{"id": "26865330", "url": "https://en.wikipedia.org/wiki?curid=26865330", "title": "Lifeline Energy", "text": "Lifeline Energy\n\nLifeline Energy (formerly Freeplay Foundation) is a non-profit social enterprise that provides technology solutions for off-grid learning. The organization designs, manufactures and distributes solar and wind-up media players and radios for classroom and group listening and was behind the first solar and wind-up radio for humanitarian use which launched in 2003. Since it began operations in 1999 more than 550,000 wind-up and solar powered radios and media players have been distributed, mainly in sub-Saharan Africa. Millions of listeners have been reached, as classrooms of up to 60 children or listening groups can use each radio or media player.\n\nLifeline Energy is a 501 (c) (3) US charity and a Section 18A and 21 South African public benefit organisation, and relies on contributions from individuals, family foundations, corporate funding and government-sponsored programmes. It collaborates with governments, international relief organizations and in-country NGO partners to implement education and information projects. In 2009, it established Lifeline Technologies Trading Ltd, a for-profit new product development and trading arm that designs and develops the products that Lifeline Energy uses in its projects. This is a relatively unique hybrid business structure with Lifeline Technology’s profits accruing to the charity.\n\n\"The Times\", a UK newspaper, selected Lifeline Energy (then known as Freeplay Foundation) as a beneficiary for its 2005/2006 Christmas Charity Appeal. The organization was also awarded the first annual Tech Museum of Innovation Award in the education category in 2001 and won the Energy Project award in the World Bank’s Development Marketplace in 2006.\n\nThe Lifeplayer MP3 was awarded a top prize in the inaugural SAB Foundation Innovation awards in 2011 and was also chosen as a finalist in the in the same year.\n\nLifeline Energy’s current on-going projects include a radio distance education programme in Zambia and providing information and education access to Somali women refugees in Kenya’s Dadaab camps.\n\nThe Freeplay Foundation was legally established in late 1998 by Rory Stear, the co-founder of Freeplay Energy Group. Freeplay Energy, which by then was a public company, was sold in July 2008 to Delhi-based businessman Devin Narang. Rory Stear – along with all other directors - resigned from Freeplay Energy Group in 2008.\nLifeline Energy runs under the direction of Chief Executive Officer, Kristine Pearson. She has held this position since January 1999 when the charity became operational.\n\nOn 8 April 2010 the Freeplay Foundation changed its name to Lifeline Energy. The name change reflects the organisation’s desire to not only develop and distribute radios and other communication tools, but also a range of other practical, fit-for-purpose products for the poor.\n\nOver the years Lifeline Energy has implemented information and education projects using solar and wind-up radios in more than a dozen countries in sub-Saharan Africa. These include Rwanda, South Africa, Kenya, Tanzania, Niger, Nigeria, Ethiopia, Mozambique, Burundi, Zambia, Zimbabwe, South Sudan and Sudan.\n\nDonors have included Anglo American PLC, the Body Shop, Vodafone Group Foundation, Asda, NASDAQ, CARE, Vodacom, the Founders and Supporters of International Tom Hanks Day and the World Bank. Gordon Roddick, co-founder and former chairman of The Body Shop International plc said in an interview with The Times, London \"Radio is the perfect way of getting quality education to a wide area incredibly cheaply.\"\n\nLifeline Energy is technology agnostic and focused on finding appropriate ways to improve the lives of those it aims to assist. It seeks technology solutions to meet these challenges – whether high or low tech. As with its Lifeplayer MP3, Lifeline Energy encapsulates known technologies (solar, battery, audio, radio, MP3) in products that are developed to meet the challenges of the environment and social contexts of their use. Products must be robust, intuitive in their function and sustainable, even for the least sophisticated users.\n\nThe Lifeplayer MP3 was launched to the media in September 2010. It was the world’s first MP3, 5-band radio and recorder designed specifically for humanitarian use and group listening. Solar-powered and wind-up, it was designed to give large groups of up to 50 people sustainable listening access. Listening content is pre-loaded on a microSD card, and the Lifeplayer can record live voice or radio programmes for playback later.\n\nIn 2011, the Lifeplayer MP3 was launched in a project for farmers in Rwanda, in partnership with the multinational giant SC Johnson and a US-based NGO, Radio Lifeline. The initiative disseminated information on modern farming techniques, market information and health news to Rwandan pyrethrum farmers via MP3 podcasts and a monthly nationwide FM radio programme. Since then, it has been used in a number of initiatives, including school lessons for children, health information for women's listening groups, and conservation and farming skills for farmers.\n\nThe Lifeplayer was created and developed by Lifeline Energy and Lifeline Technologies. Its approach to new product development is end-user focused and based on field feedback. The development funding was raised through private donations. \nSupporters of the Lifeplayer include James Kimonyo - Rwanda's Ambassador to the U.S. - as well as Academy Award winner Tom Hanks.\n\nThe Prime radio replaced the Lifeline radio - the first solar and wind-up radio made exclusively for the humanitarian sector and which Lifeline Energy and Kristine Pearson innovated. Prime radios can reach up to 60 listeners and has five bands – AM, FM and three short-wave bandwidths. It also features an LCD screen, a solar panel, a hand crank, and a DC input that can be plugged into either a wall socket or a car battery. The solar panels can charge the radio directly through a cable or can charge wirelessly.\n\nThe Prime radio improves on its predecessor, the Lifeline. In 2001, the Lifeline radio concept won the first Tech Museum of Innovation Award for Technology Benefiting Humanity for Lifeline Energy, in the education category. During the development the organisation arranged focus groups of orphaned children in South Africa, Kenya and Rwanda. The focus group findings determined that the radio needed to be a bright colour, easy to carry and robust.\n\nSince its inception, Lifeline Energy has undertaken in-depth research into the effects of energy poverty, specifically how it relates to lighting and energy access, in remote areas throughout sub-Saharan Africa. \nResearch was conducted in rural and peri-urban slum areas of South Africa to determine how much child and grandmother-headed families spent on candles and kerosene for lighting per month. The research involved looking at the types of tasks and activities that were used for lighting and the consequences people experienced as a result of their not having any access to electricity.\n\nSince 2000 Lifeline Energy has been helping to provide education access to vulnerable children taking part in the Learning at Taonga Market radio distance education initiative in Zambia. The programme offers a high quality primary school education to children who are unable to access government-run schools. Since the Zambian Ministry of Education launched it, close to 800,000 children have obtained a basic primary education.\n\nLearning at Taonga Market is entirely free and has had a profound effect on orphaned children, who are often left out of Zambia’s education system. The informal classrooms are run by community volunteers who use Lifeline Energy’s solar and wind-up radios and Lifeplayers to access the daily radio broadcasts. Children taking part in Taonga Market performed better than conventional schools in reading, numeracy and life skills. Taonga Market students are also more likely to continue to secondary school than children in government-run schools.\n\nSince 2007 Lifeline Energy has distributed more than 1,100 solar and wind-up radios in the Dadaab refugee camps and nearby host communities, benefitting more than 15,000 Somali women directly. The radios provide access to programming on women’s rights, health and controversial issues such as female genital cutting. The radios also provide access to daily broadcasts on where refugees can attain food aid, how new refugees can register and where to locate lost relatives who have also travelled to the camps from neighboring countries.\n\nThe radios have also helped educate children. Using Lifeline Energy’s radios, children are able to access radio-based primary education lessons in Somali.\n\nTom Hanks\n\nSince 2003, Academy Award winner Tom Hanks has served as Lifeline Energy's American ambassador. He supports a range of projects including Lifeline radios for distance education in Tanzania. and the organisation's Child Headed Household Lifeline radio project in Rwanda. Hanks also is the primary funder behind the research and development of Lifelight.\n\nHanks participated in an eBay charity auction in which ten Lifeline radios autographed by Hanks were available. In addition to the radio, the highest bidder also received a personal letter and a signed photo from Hanks. During the campaign, Hanks stated that \"The Lifeline radio can change the world – one person, one house, one village at a time.\"\n\nTerry Waite\n\nWorld-renowned British humanitarian Terry Waite (CBE) has served as the organisations European Ambassador since 2000. Waite recently stepped down as chairman of Lifeline Energy's UK Board of Trustees after nine years of service, but remains a patron.\n\nSibusiso Vilane\n\nTwo time Everest conqueror, South African Sibusiso Vilane, is Lifeline Energy's African Ambassador. Vilane fundraised for Lifeline radios by walking 1,113 kilometers to the South Pole, being the first black African to trek there. He stated \"I thought if I can get a radio for every kilometre I walk that would be awesome.\"\n\n"}
{"id": "6157276", "url": "https://en.wikipedia.org/wiki?curid=6157276", "title": "List of heat waves", "text": "List of heat waves\n\nThis is a partial list of temperature phenomena that have been labeled as heat waves, listed in order of occurrence.\n\n\n\n\n\n\n\nThe long persistence of this heat wave (22 days), made the event an exceptional one, breaking several brands in regard to more consecutive days with minimum and maximum temperatures above the average in several meteorological stations of the affected zone.\nThe National Meteorological Service communicated, through its daily reports, reports on the development of the heat wave.\nThe strongest point of heat was registered in the city of Chamical, province of La Rioja with 45.5 °C (113,9 °F) in the city of Santiago del Estero (provincial capital) was 45 °C (113 °F) and in Buenos Aires (national capital) was 39 °C (102,2 °F).\nThe extensive heat wave severely affected the health of thousands of people who needed medical assistance during those days, it is believed that the historical heat wave caused hundreds of victims in different points of the center and north of the country.\n\n\n\n\n"}
{"id": "29991054", "url": "https://en.wikipedia.org/wiki?curid=29991054", "title": "List of hydroelectric power stations in Guatemala", "text": "List of hydroelectric power stations in Guatemala\n\nThis is a list of hydroelectric power stations in Guatemala: The list is incomplete. The Guatemalan Electricity regulating Authority CNEE also has a listing on their website.\n"}
{"id": "44740197", "url": "https://en.wikipedia.org/wiki?curid=44740197", "title": "List of wind farms in Morocco", "text": "List of wind farms in Morocco\n\nAs of 2013, there was an installed capacity of 947 MW and 500 MW are under construction.\n\n\n\n"}
{"id": "51503229", "url": "https://en.wikipedia.org/wiki?curid=51503229", "title": "Montana flume", "text": "Montana flume\n\nA Montana flume, is a popular modification of the standard Parshall flume. The Montana flume removes the throat and discharge sections of the Parshall flume, resulting a flume that is lighter in weight, shorter in length, and less costly to manufacture. Montana flumes are used to measure surface waters, irrigations flows, industrial discharges, and wastewater treatment plant flows.\n\nAs a short-throated flume, the Montana flume has a single, specified point of measurement in the contracting section at which the level is measured. The Montana flume is described in US Bureau of Reclamation's Water Measurement Manual and two technical standards MT199127AG and MT199128AG by Montana State University.\n\nAs a modification of the Parshall flume, the design of the Montana flume is standardized under ASTM D1941, ISO 9826:1992, and JIS B7553-1993. The flumes are not patented and the discharge tables are not copyright protected.\n\nA total of 22 standard sizes of Montana flumes have been developed, covering flow ranges from 0.005 cfs [0.1416 l/s] to 3,280 cfs [92,890 l/s].\n\nLacking the extended throat and discharge sections of the Parshall flume, Montana flumes are not intended for use under submerged conditions. Where submergence is possible, a full length Parshall flume should be used. Should submergence occur, investigations have been made into correcting the flow.\n\nUnder laboratory conditions the Parshall flume - upon which the Montana is based - can be expected to exhibit accuracies to within +/-2%, although field conditions make accuracies better than 5% doubtful.\n\nThe Montana Flume is a restriction with free-spilling discharge that accelerates flow from a sub-critical state (Fr~0.5) to a supercritical one (Fr>1).\n\nThe free-flow discharge can be summarized as\n\nWhere\n\nMontana flume discharge table for free flow conditions:\n\nFree-Flow – when there is no “back water” to restrict flow through a flume. Only the single depth (primary point of measurement -Ha) needs to be measured to calculate the flow rate. A free flow also induces a hydraulic jump downstream of the flume.\n\nSubmerged Flow – when the water surface downstream of the flume is high enough to restrict flow through a flume, the flume is deemed to be submerged. Lacking the extended throat and discharge sections of the Parshall flume, the Montana flume has little resistance to the effects of submergence and as such it should be avoided. Where submerged flow is or may become present, there are several methods of correcting the situation: the flume may be raised above the channel floor, the downstream channel may be modified, or a different flume type may be used (typically a Parshall flume). Although commonly thought of as occurring at higher flow rates, It should be noted that submerged flow can exist at any flow level as it is a function of downstream conditions. In natural stream applications, submerged flow is frequently the result of vegetative growth on the downstream channel banks, sedimentation, or subsidence of the flume.\n\nMontana flumes can be constructed from a variety of materials:\n\n\nSmaller Montana flumes tend to be fabricated from fiberglass and galvanized steel (depending upon the application), while larger Montana flumes can be fabricated from fiberglass (sizes up to 160\") or concrete (160\"-600\").\n\nIn practice, is it usual to see Montana flumes larger than 48-inches as the need for free-spilling discharge can not usually be met, downstream scour would be excessive, or other flume types better handle the flow.\n\n\n"}
{"id": "9069542", "url": "https://en.wikipedia.org/wiki?curid=9069542", "title": "Napakivi", "text": "Napakivi\n\nNapakivi (pole/navel stone) or tonttukivi (elf stone) is a traditional Finnish name for a standing stone in the middle of a field or another central spot.\n\nGenerally speaking napakivi are unhewn stones that people have set upright. Some of them may have been erected by withdrawal of the receding ice-masses after the ice-age, in which case they will not be napakivi proper. Napakivi are usually longish and erect, and frequently have a round head. This has been interpreted by some to perhaps indicate an omphalos/penile reference symbolically. Napakivi can be located in the middle of a field, or the heart of an adjacent pile of stones which will be compiled of stones which had to be removed from the field to make it cultivatable by a plough. It can also be the central stone of a burial mound.\n\nNapakivi may have been considered facilitators of fertility or protectors of domain, or they may have been legal indidcators of ownership. It is plausible they may have been considered some kind of magical centres of force or energy accumulators; perhaps the seat of a tutelary spirits power. The name tonttukivi refers to the elfs known as tonttu and also to the Finnish language word for a plot of land \"tontti\". Some stones equivalent to napakivi have been referred to with the term Juminkeko or Jumin kurikka, in which case they will have been connected to the mysterious spirit known as Jumi, who served as the basis for the Finnish word for god.\n\nNapakivi may have some cultural connection with saami seids or central European and great British megaliths, although it has not been demonstrated with any scientific rigour. Megaliths too are erected by ancient folk, giant, usually over man high stones which are sole or in groups. Most megaliths as well are considered to have a connexion to the penis and fertility.\n\nA stone in the center of a graveyard set up at the end of battle to interr the combatants is often called a napakivi, in which case the attendant mythology described above will not be attached to it.\n"}
{"id": "365092", "url": "https://en.wikipedia.org/wiki?curid=365092", "title": "Neodymium magnet", "text": "Neodymium magnet\n\nA neodymium magnet (also known as NdFeB, NIB or Neo magnet), the most widely used type of rare-earth magnet, is a permanent magnet made from an alloy of neodymium, iron and boron to form the NdFeB tetragonal crystalline structure. Developed independently in 1982 by General Motors and Sumitomo Special Metals, neodymium magnets are the strongest type of permanent magnet commercially available. They have replaced other types of magnets in many applications in modern products that require strong permanent magnets, such as motors in cordless tools, hard disk drives and magnetic fasteners.\n\nNeodymium is a metal which is ferromagnetic (more specifically it shows antiferromagnetic properties), meaning that like iron it can be magnetized to become a magnet, but its Curie temperature (the temperature above which its ferromagnetism disappears) is 19 K (−254 °C), so in pure form its magnetism only appears at extremely low temperatures. However, compounds of neodymium with transition metals such as iron can have Curie temperatures well above room temperature, and these are used to make neodymium magnets.\n\nThe strength of neodymium magnets is due to several factors. The most important is that the tetragonal NdFeB crystal structure has exceptionally high uniaxial magnetocrystalline anisotropy (\"H\" ~7 T –\nmagnetic field strength H in units of \"A/m\" versus magnetic moment in \"A·m\"). This means a crystal of the material preferentially magnetizes along a specific crystal axis, but is very difficult to magnetize in other directions. Like other magnets, the neodymium magnet alloy is composed of microcrystalline grains which are aligned in a powerful magnetic field during manufacture so their magnetic axes all point in the same direction. The resistance of the crystal lattice to turning its direction of magnetization gives the compound a very high coercivity, or resistance to being demagnetized.\n\nThe neodymium atom also can have a large magnetic dipole moment because it has 4 unpaired electrons in its electron structure as opposed to (on average) 3 in iron. In a magnet it is the unpaired electrons, aligned so they spin in the same direction, which generate the magnetic field. This gives the NdFeB compound a high saturation magnetization (\"J\" ~1.6 T or 16 kG) and a remnant magnetization of typically 1.3 teslas. Therefore, as the maximum energy density is proportional to \"J\", this magnetic phase has the potential for storing large amounts of magnetic energy (\"BH\" ~ 512 kJ/m or 64 MG·Oe). This magnetic energy value is about 18 times greater than \"ordinary\" ferrite magnets by volume, and 12 times by mass. This magnetic energy property is higher in NdFeB alloys than in samarium cobalt (SmCo) magnets, which were the first type of rare-earth magnet to be commercialized. In practice, the magnetic properties of neodymium magnets depend on the alloy composition, microstructure, and manufacturing technique employed.\n\nThe NdFeB crystal structure can be described as alternating layers of iron atoms and a neodymium-boron compound. The diamagnetic boron atoms do not contribute directly to the magnetism, but improve cohesion by strong covalent bonding. The relatively low rare earth content (12% by volume) and the relative abundance of neodymium and iron compared with samarium and cobalt makes neodymium magnets lower in price than samarium-cobalt magnets.\n\nGeneral Motors (GM) and Sumitomo Special Metals independently discovered the NdFeB compound almost simultaneously in 1984. The research was initially driven by the high raw materials cost of SmCo permanent magnets, which had been developed earlier. GM focused on the development of melt-spun nanocrystalline NdFeB magnets, while Sumitomo developed full-density sintered NdFeB magnets.\n\nGM commercialized its inventions of isotropic Neo powder, bonded Neo magnets, and the related production processes by founding Magnequench in 1986 (Magnequench has since become part of Neo Materials Technology, Inc., which later merged into Molycorp). The company supplied melt-spun NdFeB powder to bonded magnet manufacturers.\n\nThe Sumitomo facility became part of the Hitachi Corporation, and currently manufactures and licenses other companies to produce sintered NdFeB magnets. Hitachi holds more than 600 patents covering neodymium magnets.\n\nChinese manufacturers have become a dominant force in neodymium magnet production, based on their control of much of the world's sources of rare earth mines.\n\nThe United States Department of Energy has identified a need to find substitutes for rare earth metals in permanent magnet technology, and has begun funding such research. The Advanced Research Projects Agency-Energy has sponsored a Rare Earth Alternatives in Critical Technologies (REACT) program, to develop alternative materials. In 2011, ARPA-E awarded 31.6 million dollars to fund Rare-Earth Substitute projects.\n\nThere are two principal neodymium magnet manufacturing methods:\n\nSintered Nd-magnets are prepared by the raw materials being melted in a furnace, cast into a mold and cooled to form ingots. The ingots are pulverized and milled; the powder is then sintered into dense blocks. The blocks are then heat-treated, cut to shape, surface treated and magnetized.\n\nIn 2015, Nitto Denko Corporation of Japan announced their development of a new method of sintering neodymium magnet material. The method exploits an \"organic/inorganic hybrid technology\" to form a clay-like mixture that can be fashioned into various shapes for sintering. Most importantly, it is said to be possible to control a non-uniform orientation of the magnetic field in the sintered material to locally concentrate the field to, e.g., improve the performance of electric motors. Mass production is planned for 2017.\n\nAs of 2012, 50,000 tons of neodymium magnets are produced officially each year in China, and 80,000 tons in a \"company-by-company\" build-up done in 2013. China produces more than 95% of rare earth elements, and produces about 76% of the world's total rare-earth magnets.\n\nBonded Nd-magnets are prepared by melt spinning a thin ribbon of the NdFeB alloy. The ribbon contains randomly oriented NdFeB nano-scale grains. This ribbon is then pulverized into particles, mixed with a polymer, and either compression- or injection-molded into bonded magnets. Bonded magnets offer less flux intensity than sintered magnets, but can be net-shape formed into intricately shaped parts, as is typical with Halbach arrays or arcs, trapezoids and other shapes and assemblies (e.g. Pot Magnets, Separator Grids, etc.). There are approximately 5,500 tons of Neo bonded magnets produced each year. In addition, it is possible to hot-press the melt spun nanocrystalline particles into fully dense isotropic magnets, and then upset-forge or back-extrude these into high-energy anisotropic magnets.\n\nNeodymium magnets are graded according to their maximum energy product, which relates to the magnetic flux output per unit volume. Higher values indicate stronger magnets and range from N35 up to N52. Letters following the grade indicate maximum operating temperatures (often the Curie temperature), which range from M (up to 100 °C) to EH (200 °C).\n\nGrades of Neodymium magnets:\n\nSome important properties used to compare permanent magnets are:\n\nNeodymium magnets have higher remanence, much higher coercivity and energy product, but often lower Curie temperature than other types. Special neodymium magnet alloys that include terbium and dysprosium have been developed that have higher Curie temperature, allowing them to tolerate higher temperatures. The table below compares the magnetic performance of neodymium magnets with other types of permanent magnets.\n\nSintered NdFeB tends to be vulnerable to corrosion, especially along grain boundaries of a sintered magnet. This type of corrosion can cause serious deterioration, including crumbling of a magnet into a powder of small magnetic particles, or spalling of a surface layer.\n\nThis vulnerability is addressed in many commercial products by adding a protective coating to prevent exposure to the atmosphere. Nickel plating or two-layered copper-nickel plating are the standard methods, although plating with other metals, or polymer and lacquer protective coatings are also in use.\n\nThe greater forces exerted by rare-earth magnets create hazards that may not occur with other types of magnet. Neodymium magnets larger than a few cubic centimeters are strong enough to cause injuries to body parts pinched between two magnets, or a magnet and a ferrous metal surface, even causing broken bones.\n\nMagnets that get too near each other can strike each other with enough force to chip and shatter the brittle material, and the flying chips can cause various injuries, especially eye injuries. There have even been cases where young children who have swallowed several magnets have had sections of the digestive tract pinched between two magnets, causing injury or death. The stronger magnetic fields can be hazardous to mechanical and electronic devices, as they can erase magnetic media such as floppy disks and credit cards, and magnetize watches and the shadow masks of CRT type monitors at a greater distance than other types of magnet.\n\nNeodymium magnets have replaced alnico and ferrite magnets in many of the myriad applications in modern technology where strong permanent magnets are required, because their greater strength allows the use of smaller, lighter magnets for a given application. Some examples are:\n\n\nNeodymium content is estimated to be 31% of magnet weight\n\nIn addition, the greater strength of neodymium magnets has inspired new applications in areas where magnets were not used before, such as magnetic jewelry clasps, children's magnetic building sets (and other neodymium magnet toys) and as part of the closing mechanism of modern sport parachute equipment. They also are the main metal in the formerly popular desk-toy magnets, \"Buckyballs\" and \"Buckycubes\", though some US retailers have chosen not to sell them due to child-safety concerns, and they have been banned in Canada for the same reason. You can still buy neodymium magnets in Canada just not as toys.\n\nThe strength and magnetic field homogeneity on neodymium magnets has also opened new applications in the medical field with the introduction of open magnetic resonance imaging (MRI) scanners used to image the body in radiology departments as an alternative to superconducting magnets that use a coil of superconducting wire to produce the magnetic field.\n\nNeodymium magnets are used as a surgically placed anti-reflux system which is a band of magnets surgically implanted around the lower esophageal sphincter to treat gastroesophageal reflux disease (GERD).\n\n\n\n"}
{"id": "37528199", "url": "https://en.wikipedia.org/wiki?curid=37528199", "title": "New energy vehicles in China", "text": "New energy vehicles in China\n\nThe stock of new energy vehicles in China is the world's largest, with cumulative sales of more than 1.7 million units through December 2017. These figures include passenger cars and heavy-duty commercial vehicles such buses and sanitation trucks, and only accounts for vehicles manufactured in the country. The Chinese government uses the term new energy vehicles (NEVs) to designate plug-in electric vehicles eligible for public subsidies, and includes only battery electric vehicles (BEVs) and plug-in hybrid electric vehicles (PHEVs). \n\nSales of new energy vehicles since 2011 passed the 500,000 unit milestone in March 2016, and the 1 million mark in early 2017, both, excluding imports. Cumulative sales of new energy passenger cars achieved the 500,000 unit milestone in September 2016, and 1 million by the end of 2017. Domestically produced passenger cars account for 96% of new energy car sales in China.\n\n, the Chinese stock of domestically built plug-in electric vehicles consisted of 1,385,088 all-electric vehicles (80.1%) and 343,359 plug-in hybrid vehicles (19.9%) sold since 2011. Most of the stock of new energy vehicles was sold during the last three years. Deliveries between 2015 and 2017 account for 93.4% of all domestically built new energy vehicle sales since 2011, of which, 45.0% were sold in 2017, 29.3% in 2016, and 19.2% in 2015. \n\n, China had the world's largest fleet of light-duty plug-in electric vehicles with over 1.2 million units, after having overtook during 2016 both the U.S. and Europe in terms of cumulative sales. China has been the world's best-selling plug-in electric car market for three years running since 2015, with annual sales of more than 207,000 plug-in passenger cars in 2015; 320,000 in 2016; and record sales of about 600,000 passenger plug-in car sales in 2017, which accounted for about half of global plug-in car sales that year. A particular feature of the Chinese passenger plug-in market is the dominance of small entry level vehicles, in 2015 representing 87% of total pure electric car sales, while 96% of total plug-in hybrid car sales were in the compact segment.\n\nChina is also the world's largest electric bus market. The global stock of plug-in electric buses in 2015 was estimated to be about 173,000 units, almost entirely deployed in China. The Chinese electric bus stock grew nearly sixfold between 2014 and 2015. The plug-in buses stock reached about 385,000 units in 2017, representing over 99% of the global stock. In addition, China is also the global leader in the electrification of other transport modes, with more than 200 million electric two-wheelers, and 3 to 4 million low-speed electric vehicles (LSEVs).\n\nBYD Auto ended 2015 as the world's best selling manufacturer of highway legal light-duty plug-in electric vehicles, and for a second year running was the world's top selling plug-in car manufacturer with over 100,000 units delivered in 2016. During 2016 BYD became the world's all-time second largest plug-in electric passenger car manufacturer after the Renault-Nissan Alliance. The BYD Qin was the top selling new energy passenger car for two years in a row, 2014 and 2015. The BYD Tang was the best selling plug-in passenger car in 2016. Until December 2016, the Qin ranked as the all-time top selling plug-in electric car in the country with 68,655 units sold since its inception. The BAIC EC-Series all-electric city car was the top selling plug-in car in 2017, and with 78,079 units sold, it also listed as world's top selling plug-in car in 2017.\n\nThe government's political support for the adoption of electric vehicles has four goals, to create a world-leading industry that would produce jobs and exports; energy security to reduce its oil dependence which comes from the Middle East; to reduce urban air pollution; and to reduce its carbon emissions. In June 2012 the State Council of China published a plan to develop the domestic energy-saving and new energy vehicle industry. The plan set a sales target of 500,000 new energy vehicles by 2015 and 5 million by 2020. As sales of new energy vehicles were slower than expected, in September 2013, the central government introduced a subsidy scheme providing a maximum of toward the purchase of an all-electric passenger vehicle and up to for an electric bus. The subsidies are part of the government's efforts to address China's problematic air pollution.\n\nThe Chinese government adopted in 2009 a plan to leapfrog current automotive technology, and seize the growing new energy vehicle (NEV) market to become of the world leaders in manufacturing of all-electric and hybrid vehicles. The government's political support for the adoption of electric vehicles has four goals, to create a world-leading industry that would produce jobs and exports; energy security to reduce its oil dependence which comes from the Middle East; to reduce urban air pollution; and to reduce its carbon emissions. However, a study by McKinsey & Company found that even though local air pollution would be reduced by replacing a gasoline car with a similar-size electric car, it would reduce greenhouse gas emissions by only 19%, as China uses coal for 75% of its electricity production. The Chinese government uses the term new energy vehicles (NEVs) to designate plug-in electric vehicles, and only pure electric vehicles and plug-in hybrid electric vehicles are subject to purchase incentives. Initially, conventional hybrids were also included.\n\nOn June 1, 2010, the Chinese government announced a trial program to provide incentives for new energy vehicles of up to 60,000 yuan (~ in June 2011) for private purchase of new battery electric vehicles and 50,000 yuan (~ in June 2011) for plug-in hybrids in five cities. The cities participating in the pilot program are Shanghai, Shenzhen, Hangzhou, Hefei and Changchun. The subsidies are paid directly to automakers rather than consumers, but the government expects that vehicle prices will be reduced accordingly. The amount of the subsidy will be reduced once 50,000 units are sold. Electricity utilities have been ordered to set up electric car charging stations in Beijing, Shanghai and Tianjin. The government set the goal to raise the country's annual production capacity to 500,000 plug-in hybrid or all-electric cars and buses by the end of 2011, up from 2,100 in 2008.\n\nIn June 2012 the State Council of China published a plan to develop the domestic energy-saving and new energy vehicle industry. The plan set a sales target of 500,000 new energy vehicles by 2015 and 5 million by 2020. According to a report by McKinsey & Company, electric vehicle sales between January 2009 and June 2012 represented less than 0.01% of new car sales in China. A mid-September 2013 joint announcement by the National Development and Reform Commission and finance, science, and industry ministries confirmed that the central government would provide a maximum of toward the purchase of an all-electric passenger vehicle and up to US$81,600 for an electric bus. The subsidies are part of the government's efforts to address China's problematic air pollution.\n\nThe China Association of Automobile Manufacturers (CAAM) expected that sales of electric and hybrid electric vehicles in China would reach 60,000 to 80,000 units in 2014. As sales have been much lower than initially expected, and most of the deployed NEV stock has been purchased by the government for public fleets, new monetary incentives were issued in 2014, and the national government set a sales target of 160,000 units for 2014. Although the goal was not achieved, new energy vehicles sales in 2014 totaled 74,763 units, up 324% from 2013. The China Industrial Association of Power Sources expected new energy vehicle sales to reach between 200,000 and 220,000 NEVs in 2015, and 400,000 units in 2016. The surge in demand continued in 2015, with a total of 331,092 NEVs sold in 2015, rising 343% year-on-year.\n\nInitially, CAAM expected new energy vehicle sales to more than double 2015 sales and reach 700,000 NEVs in 2016. After the government imposed penalties to several carmakers for defrauding the subsidy program out of almost 10 billion yuan, CAAM revised downward in September 2016 its 2016 sales target to 400,000 new energy vehicle orders. Only 289,000 new energy vehicles had been sold during the first nine months of 2016.\n\nAs intercity driving is rare in China, electric cars provide several practical advantages because commutes are fairly short and at low speeds due to traffic congestion. These particular local conditions make the range limitation of all-electric cars less of a problem, especially as the latest Chinese models have a top speed of and a range of between charges. As of May 2010, Chinese automakers have developed at least 10 models of high-speed, all-electric cars with plans for volume production.\n\nThe Chinese government reaffirmed their priority to promote new energy vehicles in its 13th Five-Year Plan (2016-2020). The Central Committee of the Communist Party of China approved the document that emphasizes boosting technological innovations in the manufacturing of new energy vehicles and promoting the use of electric cars, plug-in hybrids and fuel cell vehicles, included in its latest Five-Year Plan. The consulting firm PwC estimates the sales of new-energy vehicles in China will climb to 1.4 million units by 2020, and about 3.75 million units by 2025.\n\nAs part of its commitment to promote electric vehicles, the Chinese government announced plans in September 2015 to build a nationwide charging-station network to fulfil the power demand of 5 million electric vehicles by 2020. This network will cover residential areas, business districts, public space and inter-city highways, according to a guideline released by the State Council. Also, the plan mandates that new residential complexes should build charging points or assign space for them, while public parking lots should have no less than 10% of parking spaces with charging facilities. According to the guideline, there should be at least one public charging station for every 2,000 NEVs. Also the State Council ordered local governments not to restrict the sales or use of new energy cars.\n\nIn October 2015, Tesla Motor announced the company is negotiating with the Chinese government on producing its electric cars domestically. Local production has the potential to reduce the sales prices of Tesla models by a third, and so improving the weak sales of the Model S. A Model S starts at about in the U.S., while in China pricing starts at , about , after duties and other taxes. Foreign automakers are generally required to establish a joint venture with a Chinese company to produce cars domestically.\n\nIn April 2016 the Traffic Management Bureau under the Ministry of Public Security announced the introduction of new green license plates to identify new energy vehicles, as opposed to the country's standard blue plates. The NEV plates include a Chinese character short for the provincial region where they are issued, and seven numbers and letters, compared to six on standard plates. The objective of the special plates is to facilitate police enforcement of the preferential policies that some local authorities apply to cleaner cars to help cut emissions and ease traffic. For example, central Beijing has in place a road space rationing scheme, a driving restriction regulation that bans conventional vehicles from entering the city for one day a week, but new energy vehicles are exempted from the restriction. Beijing also introduced a vehicle quota system in 2011, awarding new car licenses through a lottery, with a ceiling of 6 million units for 2017. New energy vehicles were placed in a special category where the odds of winning a license plate are much higher than conventional autos.\n\nCumulative domestically built new energy vehicle sales in China totaled 1,728,447 units between January 2011 and December 2017. These figures include heavy-duty commercial vehicles such buses and sanitation trucks, and only accounts for vehicles manufactured in the country because imports are not subject to government subsidies. , the Chinese stock of plug-in electric vehicles consisted of 1,385,088 all-electric vehicles (80.1%) and 343,359 plug-in hybrid vehicles (19.9%) sold since 2011. Most of the stock of new energy vehicles was sold during the last three years. Deliveries between 2015 and 2017 account for 93.4% of all domestically built new energy vehicle sales since 2011, of which, 45.0% were sold in 2017, 29.3% in 2016, and 19.2% in 2015.\n\nAccording to the Minister of Science and Technology, by mid-2013 more than 80% of the country's plug-in stock was on duty in public fleet vehicles, used mainly in public transport, for both bus and taxi services, and also in solid waste recollection services (sanitation trucks). , a total of 83,198 plug-in electric passenger cars and 36,500 pure electric buses had been registered in the country since 2008. A particular feature of the Chinese passenger plug-in market is the dominance of small entry level vehicles. In 2015, all-electric car sales in the mini and small segments (A-segment) represented 87% of total pure electric car sales, while 96% of total plug-in hybrid car sales were in the compact segment (C-segment). Among the electric drive segments, mid-size car (D-segment) sales were significant only in the conventional hybrid segment, representing about 50% of hybrid sales.\nThe country achieved record sales of 207,380 new energy passenger cars in 2015, making China the world's top selling plug-in passenger car country market in calendar year 2015, surpassing the European market and also the United States, the leading market in 2014. A total of 320,081 new energy passenger cars were sold in 2016, ahead of both Europe (212,000) and the U.S. (157,181), allowing the country to remain as the world's top selling plug-in car market in 2016. The domestic plug-in segment market share totaled 1.3% of new car sales in 2016. These sales figures exclude imports, such as the Tesla Model S. Domestically produced cars account for 96% of new energy car sales in China. \n\nChina, together with the U.S., had the world's largest country stock of plug-in electric passenger cars until September 2016, with the Chinese plug-in stock representing 29.2% of the global stock of highway legal plug-in electric passenger cars. In October 2016, with about 31,000 plug-in passenger cars sold in China, while U.S. sales totaled over 11,000 units, China became the country with the world's largest stock of plug-in passenger cars, totaling about 553,000 units versus almost 533,000 in the American market. The gap between the two countries widened in November 2016, as 41,795 new energy passenger cars were sold in China, while only 14,124 were sold in the U.S. By November 2016, China’s cumulative total plug-in passenger vehicles sales also overtook Europe, making the country the global leader in the light-duty plug-in vehicle segment. , sales of domestically produced new energy passenger cars since 2010 totaled 632,371 units. IHS Automotive predicted Chinese annual plug-in car sales will reach 1 million in 2019, four years before the United States.\n\n, China listed as the world's leader in the plug-in heavy-duty segment, including electric buses and plug-in trucks, the latter, particularly sanitation/garbage trucks. Over 160,000 heavy-duty new energy vehicles have been sold between 2011 and 2015, of which, 123,710 (77.2%) were sold in 2015. Sales of commercial new energy vehicles in 2015 consisted of 100,763 all-electric vehicles (81.5%) and 22,947 plug-in hybrid vehicles (18.5%).\n\nThe share of all-electric bus sales in the Chinese bus market climbed from 2% in 2010 to 9.9% in 2012, and was expected to be closed to 20% for 2013. The global stock of plug-in electric buses in 2015 was estimated to be about 173,000 units, but almost entirely deployed in China, the world's largest electric bus market. Of these, almost 150,000 are all-electric buses. The Chinese electric bus stock grew nearly sixfold between 2014 and 2015. The production of all-electric buses totaled 115,664 units in 2016, up 31% from 88,248 electric buses produced in 2015. \n\nThe Chinese stock of plug-in buses reached 343,500 units in 2016, doubling the 2015 stock, with 300,000 units being all-electric vehicles. The global stock of electric buses was about 345,000 vehicles in 2016, with only 1,273 deployed in Europe and 200 in the U.S. The city of Shenzhen is leading the modernization and electrification effort in China with hundreds of electric buses already in operation in 2016. Shenzhen set the goal of having a 100% electric bus fleet in 2017. China had about 385,000 electric buses by the end of 2017, more than 99% of the global stock.\n\nSales of low-speed electric vehicles (LSEVs) have experienced considerable growth in China between 2012 and 2016 due to their affordability and flexibility because they can be driven without a driver license. Most of these low-speed electric cars are used in small cities, but they are expanding to larger cities. These small vehicles are not accounted by the government as new energy vehicles due to safety and environmental concerns. LSEVs generally have a maximum speed of between , have short ranges and, in some cases, use lead-acid batteries and basic motor technology.\n\nAbout 200,000 low-speed small electric cars were sold in 2013, and sales totaled more than 600,000 units in 2015. LSEV sales in 2016 were estimated between 1.2 million and 1.5 million, with a year-on-year growth rate since 2014 close to 50% for the third year running. , the stock low-speed small electric car is estimated to be between 3 million and 4 million units.\n\nThe lack of regulations for LSEV manufacturers has led to poor safety performance. Traffic safety is also at stake. LSEVs struggle in large cities due to their poor acceleration and low top speeds. LSEVs could jeopardise the market for electric cars, one of China’s priorities for industrial policy development. For these reasons, legislation to regulate and standardize low-speed electric vehicles began to be discussed by the Chinese government in 2016, including battery types (lead-acid versus lithium-ion batteries) and mandatory safety tests and vehicle dimensions.\n\nChina continued to dominate both new registrations and the global stock of electric two-wheelers in 2016, with about 26 million units sold. The stock of electric two-wheelers is estimated in the 200-230 million range by the end of 2016, making China the global leader in this segment.\n\nThe high growth rate in electric two-wheelers is partially due to the country’s policies to limit air pollution hazards, such as its ban on gasoline-powered motorcycles, limits on the issuing of licences, and the division of lanes. In addition, two-wheelers have reached cost parity with internal combustion engine models, making them affordable and attractive to consumers.\n\nA total of 8,159 new energy vehicles were sold in China during 2011, including passenger cars (61%) and buses (28%). Of these, 5,579 units were all-electric vehicles and 2,580 plug-in hybrids. Electric vehicle sales represented 0.04% of total new car sales in 2011. Sales of new energy vehicles in 2012 reached 12,791 units, which includes 11,375 all-electric vehicles and 1,416 plug-in hybrids. New energy vehicle sales in 2012 represented 0.07% of the country's total new car sales. During 2013 new energy vehicle sales totaled 17,642 units, up 37.9% from 2012 and representing 0.08% of the nearly 22 million new car sold in the country in 2013. Deliveries included 14,604 pure electric vehicles and 3,038 plug-in hybrids. \n\nThe top selling new energy car in China between 2011 and 2013 was the Chery QQ3 EV city car, with 2,167 units sold in 2011, 3,129 in 2012, and 5,727 in 2013. The JAC J3 EV ranked second in 2012 with 2,485 units sold, followed by the BYD e6 with 1,690 cars. During 2013, the BYD e6 ranked second with 1,544 units sold, followed by the BAIC E150 EV with 1,466 units. The BYD Qin plug-in hybrid was launched in the country in December 2013. The Qin replaced the BYD F3DM, the world's first mass-produced plug-in hybrid automobile, launched in China in December 2008.\n\nIn April 2014 Dongfeng Nissan announced that retail sales of the Chinese manufactured version of the Nissan Leaf, the Venucia e30, were scheduled to begin in September 2014. The Venucia e30 sold 582 units in 2014.\n\nThe first Tesla Model S retail deliveries took place in Beijing on 22 April 2014. About 2,800 Model S sedans have been imported by mid September 2014, but only 432 had received the license plates. According to a Tesla spokesman, the major reasons for the discrepancy could be that registration rules were holding deliveries in Shanghai, and Tesla only recently was able to start delivering the electric cars to customers who bought them in Shanghai. Secondly, many Chinese customers have delayed taking possession of their Model S car while waiting for the government to add the Tesla to the list of electric vehicles exempt from its 8% to 10% purchase tax. , a total of 2,968 Model S cars have been registered in China.\n\nNew energy vehicle sales in China during 2014 totaled 74,763 units, consisting of 45,048 all-electric vehicles, and 29,715 plug-in hybrids. Of these, 71% were passenger cars, 27% buses, and 1% trucks. Pure electric vehicle sales increased 210% from 2013 while plug-in hybrid sales grew 880% from the previous year. Production of new energy vehicles in the country in 2014 reached 78,499 units, up 350% from 2013. The plug-in electric segment market share reached 0.32% of the 23.5 million new car sales sold in 2014. The BYD Qin ranked as the top selling plug-in electric car in China in 2014, with 14,747 units sold during the year, and became the country's top selling plug-in passenger car ever. The Qin was followed by the all-electrics Kandi EV with 14,398, Zotye Zhidou E20, with 7,341 units, and BAIC E150 EV with 5,234.\n\nDomestically produced new energy vehicle sales in 2015 totaled a record 331,092 units, consisting of 247,482 all-electric vehicles and 83,610 plug-in hybrid vehicles, up 449% and 191% from 2014, respectively. Sales of plug-in passenger cars, excluding imports, totaled 207,380 units in 2015, consisting of 146,720 all-electrics and 60,660 plug-in hybrids. This record level of sales allowed China to rank as the world's best-selling plug-in electric car country market in 2015, ahead of the U.S., which was the top selling country in 2014. The plug-in electric passenger car segment market share rose to 0.84% in 2015, up from 0.25% in 2014. The top selling plug-in passenger models in 2015 were the BYD Qin plug-in hybrid with 31,898 units sold, followed by the BYD Tang (18,375), and the all-electrics Kandi EV (16,736), BAIC E150/160/200 EV (16,488), and the Zotye Z100 EV (15,467).\n\nSeptember 2015 achieved the best monthly NEV sales volume on record, with 20,892 units sold. BYD Auto also achieved record monthly sales volume, with 5,749 of its plug-in cars delivered in September 2015, consisting of 3,044 Tangs, 2,115 Qins, 465 e6s and 125 units of the new all-electric e5. Sales of new energy vehicles in October 2015 totaled 34,316 units, a new sales record and five times higher year-on-year. Cumulative sales of NEVs reached 171,145 units during the first ten months of 2015. Sales of new energy passenger cars also reached a record sales volume, with 21,375 plug-in cars sold in October 2015, up from 18,047 the previous month, and totaling 115,058 new energy cars sold during the first ten months of 2015.\n\n, with 31,898 units sold in 2015, the BYD Qin continued to rank as the all-time top selling plug-in passenger car in the country, with cumulative sales of 46,787 units since its introduction. The BYD Qin was the world's second best selling plug-in hybrid car in 2015, and also ranked fifth among the world's top selling plug-in electric cars in 2015. BYD Auto ended 2015 as the world's best selling manufacturer of highway legal light-duty plug-in electric vehicles, with 61,772 units sold in China, followed by Tesla Motors, with global sales of 50,580 units in 2015. Accounting for heavy-duty vehicles, BYD total sales rises to 69,222 units. BYD Auto net profits jumped 552.6% in 2015 to a total of (~ ). Sales of new energy vehicles were the main driver for BYD’s huge profit growth, with alternative energy vehicles accounting for half of the company's profits while the same percentage in 2014 was just 27%.\n\nThe stock of new energy vehicles sold in China since 2011 passed the 500,000 unit milestone in March 2016, including heavy-duty commercial vehicles such buses and sanitation trucks, and making the country the world's leader in the plug-in heavy-duty segment. This figure only includes vehicles manufactured in the country as imports are not subject to government subsidies.\n\nA total of about 507,000 new energy vehicles were sold in 2016, up 51.3% year-on-year, consisting of 409,000 pure electric vehicles, up 65.2% year-on-year, and 98,000 plug-in hybrid vehicles, up 17.2% from the same period the previous year. Sales growth through September was lower than expected due to the government's inquiry about extensive fraud cases regarding subsidies granted to manufacturers in 2015. As a result of this inquiry, the government withheld the release of the electric bus subsidy scheme. CAAM considered that without this subsidy, the goal of 500,000 new energy vehicle sales for 2016 would not be met. \n\nCumulative sales of plug-in passenger cars achieved the 500,000 unit milestone in September 2016. Imported plug-in cars, such as Tesla Model S or BMW i3s are not included. A total of 320,081 new energy passenger cars were sold in 2016, ahead of both Europe (212,000) and the U.S. (157,181). The domestic plug-in segment market share totaled 1.3% of new car sales in 2016. Sales of BMW plug-in hybrid and i3 electric cars in China totaled 1,796 units during the first nine months of 2016. Tesla Inc. sales totaled 10,399 vehicles in 2016, consisting of 6,334 Model S cars and 4,065 Model X SUVs. In November 2016, with cumulative sales of about 600,000 plug-in electric passenger cars, China had overtook both Europe and the U.S., and became the market with the world's largest stock of light-duty plug-in vehicles.\n\nThree BYD Auto models topped the Chinese ranking of best-selling new energy passenger cars in 2016. The BYD Tang plug-in hybrid SUV was the top selling plug-in car with 31,405 units delivered, followed by the BYD Qin (21,868), BYD e6 (20,605), BAIC E-Series EV (18,814), and the SAIC Roewe e550 (18,805). , the BYD Qin, with 68,655 units sold since its inception, remained the all-time top selling plug-in electric car in the country.\n\nIn September 2016, BYD Auto surpassed Mitsubishi Motors as the third largest global plug-in car manufacturer with cumulative sales of 161,000 plug-in cars delivered in China since 2008, ranking behind Tesla Motors (164,000) and the Renault-Nissan Alliance (almost 369,000). In October 2016, BYD passed Tesla Motors to become the world's all-time second largest plug-in electric passenger car manufacturer with more than 171,000 units delivered in China since 2008. BYD Auto was the world's top selling plug-in car manufacturer for a second year in a row with more than 100,000 units delivered in China in 2016, up 64% from 2015, and ahead of Tesla by about 30,000 units. However, in terms of sales revenue, Tesla ranked ahead with from its electric car sales in 2016, while BYD sales totaled from its electric car division. Cumulative sales of domestically built new energy vehicles in China totaled 951,447 units between January 2011 and December 2016.\n\nChinese sales of domestically-built new energy vehicles in 2017 totaled 777,000 units, up 53% from 2016 consisting of 652,000 all-electric vehicles (up 59.4%) and 125,000 plug-in hybrid vehicles (up 27.6%). Sales of domestically-produced new energy passenger vehicles totaled 579,000 units, consisting of 468,000 all-electric cars and 111,000 plug-in hybrids. Accounting for foreign brands, plug-in car sales rise to about 600,000 in 2017, representing about half of global plug-in car sales in 2017. The plug-in segment achieved a record market share of 2.1% of new car sales. Cumulative sales of domestically built new energy passenger cars totaled over 1.2 million units between 2011 and 2017. Cumulative sales of domestically built new energy vehicles in China totaled 1,728,447 units between 2011 and 2017.\n\nThe BAIC EC-Series all-electric city car was the Chinese top selling plug-in car in 2017 with 78,079 units sold, making the city car the world's top selling plug-in car in 2017. The top selling plug-in hybrid was the BYD Song PHEV with 30,920 units. BYD Auto was the top selling Chinese car manufacturer in 2017. In 2017, General Motors sold about 11,000 Baojun E100s, 1,600 Buick Velite 5's and about 2,000 Cadillac CT6 plug-ins.\n\nThe following table presents annual sales of new energy passenger cars by model between January 2011 and December 2015.\n\n\n"}
{"id": "47735209", "url": "https://en.wikipedia.org/wiki?curid=47735209", "title": "Okkupert", "text": "Okkupert\n\nWith a budget of 90 million kr (US$11 million), the series is the most expensive Norwegian production to date and has been sold to the UK, Germany, France, Sweden, Denmark, Finland, Iceland, Serbia, Estonia, Poland, Czech Republic, Belgium, the Netherlands, Luxembourg, Spain, and Portugal. It is also streamed by Netflix in the United States, Australia, New Zealand, Ireland, the United Kingdom, India, Canada, Belgium, Sweden, Italy and the Netherlands.\n\n\"\" depicts a fictional near future in which Russia, with support from the European Union, occupies Norway to restore its oil and gas production, in response to a Europe-wide energy crisis caused by the coming to power of Norway's Green Party, which stopped the country's oil and gas production.\n\nThe series has been renewed for a third season.\n\nIn the near future, Middle East turmoil compromises oil production. The United States achieves energy independence and subsequently withdraws from NATO, causing an energy crisis. A catastrophic hurricane fueled by climate change, Hurricane Maria, devastates Norway, killing thousands. The resulting chaos leads to the unprecedented rise of the Norwegian Green Party to power in response. The new Prime Minister of Norway, Green Party leader Jesper Berg, is an idealistic politician with bold plans for thorium-based nuclear power as a viable solution to replace oil, which could help avoid further emission of carbon dioxide into the atmosphere causing global warming and climate change. To this end, he cuts off all fossil fuel production, intensifying the energy crisis in the continent. The European Union, in desperation, acquiesces to a Russia velvet glove invasion of Norway. \n\nAt the inauguration of a thorium plant, Russian special forces kidnap Berg in front of his Norwegian Police Security Service (PST) bodyguard Hans Martin Djupvik and newspaper reporter Thomas Eriksen, and is flown by helicopter to a forest. There, via video chat, EU Commissioner Pierre Anselme demands that he submit to the demands of the EU or face a full-scale Russian invasion. Berg at first refuses, but after his kidnappers execute a random civilian who approached the helicopter, Berg submits, reasoning as a pacifist that nobody deserves to die. Berg is released and is picked up by Djupvik. To conceal the nature of the occupation, Berg spins the occupation as an \"energy partnership\", and promises the Norwegian people in an interview with Eriksen that the occupation would be a temporary measure until Norway's oil and gas production is restored by Russian working crews. This intention unravels as a series of events complicates Norwegian–Russian interactions over the ensuing months.\n\nThe series of escalating complications begins when Stefan Christensen, a member of the Royal Guard, unsuccessfully attempts to assassinate the Russian ambassador, Irina Sidorova, only to be thwarted by Djupvik and the PST. The Norwegian military unit seizes two Russian officials who had been urging the Commander of the Royal Guard, Harald Vold, to surrender himself to the Russian authorities on suspicion of hatching the plot to assassinate Sidorova, inciting a hostage situation with the demands being the release of the attempted assassin from the Russian embassy, located just across the street from a restaurant frequented by Russians since the occupation began, run by Eriksen's wife Bente Norum. Peter Eriksen, Eriksen's son and Norum's stepson, joins protests against the Russians, and soon becomes involved with anti-Russian radicals.\n\nWhen a Russian agent is the victim of a hit-and-run in front of the embassy and restaurant, the Russian government demands that Norway extrajudicially extradite the driver, a suspected Chechen terrorist named Elbek Musajev. The hit-and-run was an accident caused by his son Iljas rather than a deliberate attack, but Elbek commits suicide to avoid being deported to Russia. An insurgent group, Free Norway (\"\"), emerges, led by Christensen, Vold, nationalist academic Eivind Birkeland and Iljas Musajev, and attacks police headquarters, injuring a teenager who had been detained for posting anti-Russian messages inciting opposition to the occupation. The Russian government takes this as an excuse to prolong their occupation of Norway. A gas production facility is attacked, temporarily delaying the EU production target, and killing many Russian workers, further delaying the pullout. Tensions further increase as suspicious events occur at the Norwegian–Russian border, where Russian troops infiltrate Lapland (Finnmark County), Eriksen is murdered while investigating the Russians, and a Russian naval fleet exercises off the coast of northern Norway. Berg asks the EU to protect Norway's sovereignty, but the EU fails to act decisively.\n\nFree Norway receives assistance from inside the PST by its chief Wenche Arnesen, who chose to defect after discovering that she was terminally ill. The group begins a recruitment drive of retired military personnel and prisoners. Berg has in the meantime set up an interim government under the orders of the King of Norway after losing a no-confidence motion by the Green Party, and had known Russian sleeper agents sent to internment on Svalbard; however, it is pursued by Russian jets, forcing it to turn back. In light of this internment campaign, Russian \"terrorists\" seemingly armed with suicide vests storm Berg's office and hold him hostage. Berg is rescued and evacuated to the residence of the U.S. ambassador, setting up a government in internal exile. It emerges that the oil refinery attack was actually a false-flag attack staged by the Russians. When Norum's daughter, Maja Norum, discovers that the Russians killed Eriksen, Norum expels Russian staff members from her restaurant, and provides information to Free Norway.\n\nBerg, hiding in the U.S. ambassador's residence, takes every opportunity to attempt engaging the United States in a series of Norwegian moves to try to dislodge the Russians, but the U.S. refuses direct involvement in any conflict they have a risk of losing. Events spiral further when after calling the public to resist the Russians, Free Norway kidnaps Sidorova and one of her bodyguards, executing the latter live over the internet; in response, Russian special forces seize Oslo Airport. Berg's pressure on the U.S. prompts the ambassador to poison his food and Berg is admitted to a hospital where he is captured yet again, this time by Free Norway, who also assassinate a high-ranking Russian general along with several Russian soldiers in front of the Russian embassy, using Norum's information (incurring a rebuke from her lover, a Russian agent named Nikolai). After being rescued, Sidorova calls Djupvik and tells him that Russia is now at war with Norway. Arnesen records and releases a video revealing her defection. She later commits suicide at a church to avoid capture, and its priest cremates her body. Berg is taken to a Free Norway resistance camp, and is asked by Vold if he is ready to fight for his country.\n\nAfter being kidnapped, Berg is proclaimed as the new leader of Free Norway, and the group carries out a guerrilla campaign against the occupation, attacking both Russian and Norwegian forces and staging terror attacks throughout the country, nearly plummeting the country into a civil war. Six months later, however, the insurgents have lost much of their strength due to Russian military and Norwegian police action. Berg is thus sent into exile in neighboring Sweden, which had supported the initial Russian invasion as a member of the EU.\n\nBerg's replacement as prime minister, fellow Green Party member Anders Knudsen, proves unable to handle the pressure and resigns. Following his exile to Sweden, Berg attempts to form a parallel government-in-exile. While acting as a go-between Berg and the leaders of Parliament, his political adviser and lover Anita Rygh sees her political role marginalized. Rygh instead recommends to the majority party that they form a new government. The president refuses, but offers her the prime ministerial position. She accepts, severing her ties with Berg.\n\nA Norwegian Coast Guard officer approaches Berg with news that the Russians are installing cruise missile launchers on the oil installation of Melkøya in northern Norway. From his position in Sweden, Berg contacts the insurgents and orders them to get photographic evidence. A platoon of guardsmen independently confronts members of a Russian private military company stationed there under the guise of an inspection. One of the guardsmen, a naturalized Somalian immigrant named Faisal Abdi, transmits a video of the missiles, but a gunfight breaks out and the guardsmen are captured and brought to a Russian prison. The leader of the expedition dies after being hit by gunfire. The return of the soldiers becomes a source of tension between the nations and Rygh's first true test.\n\nDjupvik, now head of the PST, continues to walk a line between allegiances. Unable to track down Arnesen, his old boss and reported leader of Free Norway, he strong-arms the priest at the church where she died into admitting she has been dead the whole time. The Norwegian police capture Vold in an operation. The Security Service discovers Berg is using a video game to communicate with the rebels, and when they find proof that he ordered the illegal mission to Melkøya, he is ordered to be arrested. Tipped off, Berg flees in the night, and emerges in Poland, where he meets a reporter from Germany's \"Stern\" magazine and leaks government documents, before fleeing to Belgium and then France, where he is arrested. He pursues a case in the European Court of Human Rights, demanding the right to return to Norway, which he succeeds in doing.\n\nAs the Russians are unwilling to free the guardsmen captured, Djupvik's wife Hilde, drafts a plan to have them tried in Norway but with a Russian lay judge presiding over the trial. The Russians agree and all of the soldiers are found guilty. A group of civilian government opponents, Free Our Soldiers (FOS), is created by Faisal's girlfriend, Frida Engø, and her hacker friend Leon Tangen to peacefully force the government to pardon them. Christensen, one of the leaders of Free Norway who had incited the crisis with his attack on Ambassador Sidorova, becomes a straggler, kidnaps Djupvik's daughter and demands the soldiers be set free, prompting condemnation from FOS. Christensen is tracked down by the Russians and Djupvik's daughter is saved. The Russians allow him a chance to talk to Christensen. When Christensen says he regrets not killing the child, he is killed by Djupvik.\n\nRygh offers the rebels amnesty in return of their turning in their weapons, which is endorsed by Vold and the Russians. The government then releases the guardsmen. Vold then forms a new political party to succeed Free Norway, the Liberation Party. Rygh's mother, once an ardent supporter of her daughter, becomes despondent at how much Norway has lost. Meanwhile, EU Commissioner Anselme is succeeded by a Polish politician who is more sympathetic to Berg and his cause.\n\nThroughout the season, Bente Norum has opened a hotel with a Russian business partner named Zoya. Similar to her previous restaurant, this venue proves to be a real hit among Russians and it is a profitable venture. A powerful Russian, Konstantin Minnikov, stays at the hotel. Norum is approached by a Free Norway agent, Anette Kleven, to use a device to mirror Minnikov's phone. She declines, until she discovers that Minnikov is secretly buying out her partner. Minnikov's daughter Nadia replaces Zoya as Norum's business partner, and befriends Norum's daughter Maja. Minnikov finds out Norum has been spying on her, and he confronts her. She accidentally kills him during the confrontation. Afraid, she calls Nikolai who disposes of Minnikov's body. The rebels use this to blackmail her to spy on other Russians who are guests at her hotel. She keeps this a secret, and Zoya is blamed for the murder. When Norum discovers this, she goes to the police to turn herself in, only to find out that resistance fighters won’t let her confess. She meets Djupvik, and gives him information on the activities of the rebels.\n\nAfter being cleared, Berg returns to Norway with a group of European politicians to support him and serve as protection. However, the European politicians are forced to leave after the Norwegian government threatens gas supplies to Europe. In response, Berg recruits Tangen to hack a Russian missile system and shoot down a Finnish fighter jet in a false-flag attack, causing the EU to rebuke Russia. Tangen is killed by a Russian hit squad in retaliation for his role, but is able to pass proof of Berg's involvement to Engø beforehand. When Rygh orders the Norwegian Navy to detain Berg, they refuse, igniting a military coup led by Vold. Djupvik's wife leaves him over his involvement with the Russians, and Norum sells her hotel to Nadia, allowing her and Maja to leave Norway and immigrate to Russia. \n\nTo contain the situation, Rygh and Djupvik spin the coup as a restoration of order (which marginalizes Vold), and decide to accompany Berg to Norwegian shores, where he is met by an adoring crowd. Berg proclaims Rygh as his official successor, and she announces the end of the Russian occupation. However, she is shot by Abdi, who had become radicalized by Tangen's death (egged on by Vold, who watched from the sidelines). Later that night, Berg returns to the office of the prime minister and sits in the prime minister's seat, facing an uncertain future.\n\nJo Nesbø wrote the first episodes in 2008, and the series, planned to be produced for Norwegian Broadcasting Corporation, received a million production grant from the Norwegian Film Institute in April 2013. After four years of planning, disagreements over the progress led NRK to withdraw from the project; TV2 took over in October 2013.\n\nThe series premiered in Norway on TV2, on 5 October 2015, and in the United Kingdom and Ireland on 13 January 2016 on Sky Arts in HD. The series was added to the Netflix streaming service, in multiple countries, as of 20 January 2016. The series premiered on Pivot TV in the United States on 5 May 2016 and on the Canadian public television station TVOntario on 11 September 2016. In Poland, the series broadcasts on Ale Kino+.\n\n\"The Daily Telegraph\"s cultural reviewer Gerard O'Donovan wrote of \"Occupied\" that the series' innovation more than made up for any lack of plausibility, citing the interesting historical, geopolitical interplay between Norway and Russia as fascinating. O'Donovan went on to praise the first episode, saying, \"the tense plotting and a pace sufficiently frenetic to carry all but the most curmudgeonly along.\"\n\n, the Russian ambassador to Norway, told Russian News Agency TASS,\n\nIt is certainly a shame that, in the year of the 70th anniversary of the victory in World War II, the authors have seemingly forgotten the Soviet Army's heroic contribution to the liberation of northern Norway from Nazi occupiers, decided, in the worst traditions of the Cold War, to scare Norwegian spectators with the nonexistent threat from the east. The Russian embassy had been informed in an early stage of the work on the series.\n\n"}
{"id": "19631078", "url": "https://en.wikipedia.org/wiki?curid=19631078", "title": "Ornithological Society of the Middle East", "text": "Ornithological Society of the Middle East\n\nThe Ornithological Society of the Middle East (OSME) is a British-based ornithological and birdwatching club for people interested in the birds of the Middle East, the Caucasus and Central Asia. It was established in April 1978 as a successor organisation to the Ornithological Society of Turkey, and in 2001 was expanded to cover central Asia and the Caucasus.\n\nIts aims are, with regard to its area of interest, to collect, collate and publish ornithological data, encourage an interest in and conservation of the birds of the region, and to assist regional environmental and conservation organisations and natural history societies with ornithological studies and activities.\n\nOSME publishes the journal \"Sandgrouse\".\n\nThe following territories of the Middle East are where OSME are active:\n\n"}
{"id": "15409174", "url": "https://en.wikipedia.org/wiki?curid=15409174", "title": "Piezoelectric accelerometer", "text": "Piezoelectric accelerometer\n\nA piezoelectric accelerometer is an accelerometer that employs the piezoelectric effect of certain materials to measure dynamic changes in mechanical variables (e.g., acceleration, vibration, and mechanical shock).\n\nAs with all transducers, piezoelectric convert one form of energy into another and provide an electrical signal in response to a quantity, property, or condition that is being measured. Using the general sensing method upon which all accelerometers are based, acceleration acts upon a seismic mass that is restrained by a spring or suspended on a cantilever beam, and converts a physical force into an electrical signal. Before the acceleration can be converted into an electrical quantity it must first be converted into either a force or displacement. This conversion is done via the mass spring system shown in the figure to the right.\n\nThe word piezoelectric finds its roots in the Greek word \"piezein\", which means to squeeze or press. When a physical force is exerted on the accelerometer, the seismic mass loads the piezoelectric element according to Newton's second law of motion (formula_1). The force exerted on the piezoelectric material can be observed in the change in the electrostatic force or voltage generated by the piezoelectric material. This differs from a piezoresistive effect in that piezoresistive materials experience a change in the resistance of the material rather than a change in charge or voltage. Physical force exerted on the piezoelectric can be classified as one of two types; bending or compression. Stress of the compression type can be understood as a force exerted to one side of the piezoelectric while the opposing side rests against a fixed surface, while bending involves a force being exerted on the piezoelectric from both sides. \n\nPiezoelectric materials used for the purpose of accelerometers fall into two categories: single crystal and ceramic materials. The first and more widely used are single-crystal materials (usually quartz). Though these materials do offer a long life span in terms of sensitivity, their disadvantage is that they are generally less sensitive than some piezoelectric ceramics. The other category, ceramic materials, have a higher piezoelectric constant (sensitivity) than single-crystal materials, and are less expensive to produce. Ceramics use barium titanate, lead-zirconate-lead-titanate, lead metaniobate, and other materials whose composition is considered proprietary by the company responsible for their development. The disadvantage of piezoelectric ceramics, however, is that their sensitivity degrades with time making the longevity of the device less than that of single-crystal materials.\n\nIn applications when low sensitivity piezoelectrics are used, two or more crystals can be connected together for output multiplication. The proper material can be chosen for particular applications based on the sensitivity, frequency response, bulk-resistivity, and thermal response. Due to the low output signal and high output impedance that piezoelectric accelerometers possess, there is a need for amplification and impedance conversion of the signal produced. In the past this problem has been solved using a separate (external) amplifier/impedance converter. This method, however, is generally impractical due to the noise that is introduced as well as the physical and environmental constraints posed on the system as a result. Today IC amplifiers/impedance converters are commercially available and are generally packaged within the case of the accelerometer itself.\n\nBehind the mystery of the operation of the piezoelectric accelerometer lie some very fundamental concepts governing the behavior of crystallographic structures. In 1880, Pierre and Jacques Curie published an experimental demonstration connecting mechanical stress and surface charge on a crystal. This phenomenon became known as the piezoelectric effect. Closely related to this phenomenon is the Curie point, named for the physicist Pierre Curie, which is the temperature above which piezoelectric material loses spontaneous polarization of its atoms.\n\nThe development of the commercial piezoelectric accelerometer came about through a number of attempts to find the most effective method to measure the vibration on large structures such as bridges and on vehicles in motion such as aircraft. One attempt involved using the resistance strain gage as a device to build an accelerometer. Incidentally, it was Hans J. Meier who, through his work at MIT, is given credit as the first to construct a commercial strain gage accelerometer (circa 1938). However, the strain gage accelerometers were fragile and could only produce low resonant frequencies and they also exhibited a low frequency response. These limitations in dynamic range made it unsuitable for testing naval aircraft structures. On the other hand, the piezoelectric sensor was proven to be a much better choice over the strain gage in designing an accelerometer. The high modulus of elasticity of piezoelectric materials makes the piezoelectric sensor a more viable solution to the problems identified with the strain gage accelerometer.\n\nSimply stated, the inherent properties of the piezoelectric accelerometers made it a much better alternative to the strain gage types because of its high frequency response, and its ability to generate high resonant frequencies. The piezoelectric accelerometer allowed for a reduction in its physical size at the manufacturing level and it also provided for a higher g (standard gravity) capability relative to the strain gage type. By comparison, the strain gage type exhibited a flat frequency response above 200 Hz while the piezoelectric type provided a flat response up to 10,000 Hz. These improvements made it possible for measuring the high frequency vibrations associated with the quick movements and short duration shocks of aircraft which before was not possible with the strain gage types. Before long, the technological benefits of the piezoelectric accelerometer became apparent and in the late 1940s, large scale production of piezoelectric accelerometers began. Today, piezoelectric accelerometers are used for instrumentation in the fields of engineering, health and medicine, aeronautics and many other different industries.\n\nThere are two common methods used to manufacture accelerometers. One is based upon the principles of piezoresistance and the other is based on the principles of piezoelectricity. Both methods ensure that unwanted orthogonal acceleration vectors are excluded from detection.\n\nManufacturing an accelerometer that uses piezoresistance first starts with a semiconductor layer that is attached to a handle wafer by a thick oxide layer. The semiconductor layer is then patterned to the accelerometer's geometry. This semiconductor layer has one or more apertures so that the underlying mass will have the corresponding apertures. Next the semiconductor layer is used as a mask to etch out a cavity in the underlying thick oxide. A mass in the cavity is supported in cantilever fashion by the piezoresistant arms of the semiconductor layer. Directly below the accelerometer's geometry is a flex cavity that allows the mass in the cavity to flex or move in direction that is orthogonal to the surface of the accelerometer.\n\nAccelerometers based upon piezoelectricity are constructed with two piezoelectric transducers. The unit consists of a hollow tube that is sealed by a piezoelectric transducer on each end. The transducers are oppositely polarized and are selected to have a specific series capacitance. The tube is then partially filled with a heavy liquid and the accelerometer is excited. While excited the total output voltage is continuously measured and the volume of the heavy liquid is microadjusted until the desired output voltage is obtained. Finally the outputs of the individual transducers are measured, the residual voltage difference is tabulated, and the dominate transducer is identified.\n\nIn 1943 the Danish company Brüel & Kjær launched Type 4301 - the world's first charge accelerometer.\n\nPiezoelectric accelerometers are used in many different industries, environments and applications. Piezoelectric measuring devices are widely used today in the laboratory, on the production floor, and as original equipment for measuring and recording dynamic changes in mechanical variables including shock and vibration.\n\nSome accelerometers have built-in electronics to amplify the signal before transmitting it to the recording device. These devices usually comply with the IEPE standard or its proprietary equivalent, ICP (see integrated circuit piezoelectric sensor).\n\n\n"}
{"id": "25970", "url": "https://en.wikipedia.org/wiki?curid=25970", "title": "RCA", "text": "RCA\n\nThe RCA Corporation was a major American electronics company, which was founded as the Radio Corporation of America in 1919. It was initially a wholly owned subsidiary of General Electric (GE); however, in 1932, GE was required to divest its control as part of the settlement of a government antitrust suit.\n\nAt its height as an independent company, RCA was the dominant communications firm in the United States for over five decades. Beginning in the early 1920s, RCA was a major manufacturer of radio receivers, and also created the first national radio network, the National Broadcasting Company (NBC). RCA was also at the forefront in the introduction and development of television, both black-and-white and color. During this period, the company was closely identified with the leadership of David Sarnoff. He was general manager at RCA's founding, became company president in 1930, and remained active, as chairman of the board, until the end of 1969.\n\nBeginning in the mid 1970s, RCA's stature began to weaken, as the company attempted to diversify and suffered major financial losses in the mainframe computer industry and other failed projects such as the CED videodisc. In 1986, RCA was reacquired by General Electric, which over the next few years liquidated most of the corporation's assets. The RCA trademarks are currently owned by Sony Music Entertainment and Technicolor, which in turn license the brand name to other companies including Voxx International, Curtis International, AVC Multimedia, TCL Corporation, and Express LUCK International, Ltd. for their various products.\n\nRCA originated as a reorganization of the Marconi Wireless Telegraph Company of America (commonly called \"American Marconi\"). In 1897, the Wireless Telegraph and Signal Company, Limited, was founded in London to promote the radio (then known as \"wireless telegraphy\") inventions of Guglielmo Marconi. As part of a worldwide expansion, in 1899 American Marconi was organized as a subsidiary company, holding the rights to use the Marconi patents in the United States and Cuba. In 1912 it took over the assets of the bankrupt United Wireless Telegraph Company, and from that point forward it had been the dominant radio communications company in the United States.\n\nWith the entry of the United States into World War One in April 1917, the government took over most civilian radio stations, to use them for the war effort. Although the overall U.S. government plan was to restore civilian ownership of the seized radio stations once the war ended, many Navy officials hoped to retain a monopoly on radio communication even after the war. Defying instructions to the contrary, the Navy began purchasing large numbers of stations outright. With the conclusion of the conflict, Congress turned down the Navy's efforts to have peacetime control of the radio industry, and instructed the Navy to make plans to return the commercial stations it controlled, including the ones it had improperly purchased, to the original owners.\n\nDue to national security considerations, the Navy was particularly concerned about returning the high-powered international stations to American Marconi, since a majority of its stock was in foreign hands, and the British already largely controlled the international undersea cables. This concern was increased by the announcement in late 1918 of the formation of the Pan-American Wireless Telegraph and Telephone Company, a joint venture between American Marconi and the Federal Telegraph Company, with plans to set up service between the United States and South America.\nThe Navy had installed a high-powered Alexanderson alternator, built by General Electric (GE), at the American Marconi transmitter site in New Brunswick, New Jersey. It proved to be superior for transatlantic transmissions to the spark transmitters that had been traditionally used by the Marconi companies. Marconi officials were so impressed by the capabilities of the Alexanderson alternators that they began making preparations to adopt them as their standard transmitters for international communication. A tentative plan made with General Electric proposed that over a two-year period the Marconi companies would purchase most of GE's alternator production. However, this proposal was met with disapproval, on national security grounds, by the U.S. Navy, which was concerned that this would guarantee British domination of international radio communication.\n\nThe Navy, claiming it was acting with the support of President Wilson, looked for an alternative that would result in an \"all-American\" company taking over the American Marconi assets. In April 1919 two naval officers, Admiral H. G. Bullard and Commander S. C. Hooper, met with GE's president, Owen D. Young, asking that he suspend the pending alternator sales to the Marconi companies. This move would leave General Electric without a buyer for its transmitters, so the officers proposed that GE purchase American Marconi, and use the assets to form its own radio communications subsidiary. Young consented to this proposal, which, effective November 20, 1919, transformed American Marconi into the Radio Corporation of America. The new company was promoted as being a patriotic gesture. RCA's incorporation papers required that its officers needed to be U.S. citizens, with a majority of its stock held by Americans.\n\nRCA retained most of the American Marconi staff, although Owen Young became the new company's head as the chairman of the board. Former American Marconi vice president and general manager E. J. Nally become RCA's first president. Nally's term ended on December 31, 1922, and he was succeeded the next day by Major General James G. Harbord. Harbord in turn resigned the presidency on January 3, 1930, replacing Owen D. Young as the company's chairman of the board. He was succeeded, as RCA's third president, by David Sarnoff, who had been the company's general manager at its founding. RCA worked closely with the federal government, and felt it deserved to maintain its predominant role in U.S. radio communications. At the company's recommendation, President Woodrow Wilson appointed Rear Admiral Bullard \"to attend the stockholders' and director's meetings... in order that he may present and discuss informally the Government's views and interests\".\n\nAs of its founding RCA was the largest radio communications firm in the United States. American Marconi had been falling behind industry advances, particularly in vacuum tube technology, and GE needed access to additional patents before its new subsidiary could be fully competitive. The result was a series of negotiations and a complicated set of cross-licensing agreements between various companies. On July 1, 1920, an agreement was made with the American Telephone & Telegraph Company (AT&T), which purchased 500,000 shares of RCA, although it would divest these shares in early 1923. The United Fruit Company held a small portfolio of radio patents, and signed two agreements in 1921. GE's traditional electric company rival, the Westinghouse Electric & Manufacturing Corporation, had also purchased rights to some critical patents, including one for heterodyne receiving originally issued to Reginald Fessenden, plus regenerative circuit and superheterodyne receiver patents issued to Edwin Armstrong. Westinghouse used this position to negotiate a cross-licensing agreement, effective July 1, 1921, that included a concession that 40% of RCA's equipment purchases would be from Westinghouse. Following these transactions, GE owned 30.1% of RCA's stock, Westinghouse 20.6%, AT&T 10.3%, and United Fruit 4.1%, with the remaining 34.9% owned by individual shareholders.\n\nIn 1930, RCA agreed to occupy the yet-to-be-constructed landmark building of the Rockefeller Center complex, 30 Rockefeller Plaza, which in 1933 became known as the RCA building (later renamed the GE Building, now the Comcast Building). This lease was critical for enabling the massive project to proceed as a commercially viable venture—David Rockefeller cited RCA's action as being responsible for \"the salvation of the project\".\n\nRCA's primary business objectives at its founding were to provide equipment and services for seagoing vessels, and \"worldwide wireless\" communication in competition with the undersea cables. To provide the international service, the company soon undertook a massive project to build a \"Radio Central\" communications hub at Rocky Point, Long Island, New York, designed to achieve \"the realization of the vision of communication engineers to transmit messages to all points of the world from a single centrally located source\". The circular Radio Central site encompassed 10 square miles (25 square kilometers), with a transmission building, located at the hub, projected to ultimately house ten Alexanderson alternator transmitters. The plan called for 12 \"antenna spokes\" to be built, stretching out in all directions from the center. Each spoke was nearly three miles (4.8 kilometers) long, and consisted of sixteen wires supported by a line of six 410-foot-tall (125 meter) towers topped with 150-foot-wide (45 meter) crossbars. Construction began in July 1920, and the site was dedicated on November 5, 1921, after two of the antenna spokes had been completed, and two of the 200-kilowatt alternators installed. The debut transmissions received replies from stations in 17 countries.\n\nAlthough the initial installation would remain in operation, the additional antenna spokes and alternator installations would not be completed, due to a major discovery about radio signal propagation. While investigating transmitter \"harmonics\" – unwanted additional radio signals produced at higher frequencies than a station's normal transmission frequency – Westinghouse's Frank Conrad unexpectedly found that in some cases the harmonics could be heard farther than the primary signal, something previously thought impossible, as high-frequency shortwave signals, which had poor groundwave coverage, were thought to have a very limited transmission range. In 1924, Conrad demonstrated to Sarnoff that a low-powered shortwave station in East Pittsburgh, Pennsylvania could be readily received in London by a simple receiver using a curtain rod as an antenna, matching, at a small fraction of the cost, the performance of the massive alternator transmitters. In 1926 Dr. Harold H. Beverage further reported that a shortwave signal, transmitted on a 15-meter wavelength (approximately 20 MHz), was received in South America more readily during the daytime than the 200 kilowatt alternator transmissions.\n\nThe Alexanderson alternators, control of which had led to RCA's formation, were now considered obsolete, and international communication would be primarily conducted using vacuum tube transmitters operating on shortwave bands. RCA would continue to operate international telecommunications services for the remainder of its existence, through its subsidiary RCA Communications, Inc., and later the RCA Global Communications Company.\n\nInternational shortwave was in turn largely supplanted by communications satellites, especially for distributing network radio and television programming. In 1975, the company formed RCA American Communications, which operated its Satcom series of geostationary communications satellites.\n\nThe introduction of organized radio broadcasting in the early 1920s resulted in a dramatic reorientation and expansion of RCA's business activities. The development of vacuum tube radio transmitters made audio transmissions practical, in contrast with the earlier transmitters which were limited to sending the dits-and-dahs of Morse code. Since at least 1916, when he was still at American Marconi, David Sarnoff had proposed establishing broadcasting stations, but his memos to management promoting the idea for sales of a \"Radio Music Box\" had not been followed up at the time.\n\nStarting around 1920 a small number of broadcasting stations began operating, and soon interest in the innovation was spreading nationwide. In the summer of 1921, a Madison Square Garden employee, Julius Hopp, devised a plan to raise charitable funds by broadcasting, from ringside, the July 2, 1921 Dempsey-Carpentier heavyweight championship fight to be held in Jersey City, New Jersey. Hopp recruited theaters and halls as listening locations that would charge admission fees to be used as charitable donations. He also contacted RCA's J. Andrew White, the acting president of the National Amateur Wireless Association (NAWA), an organization originally formed by American Marconi which had been inherited by RCA. White agreed to recruit the NAWA membership for volunteers to provide assistance at the listening sites, and also enlisted David Sarnoff for financial and technical support. RCA was authorized to set up a temporary longwave radio station, located in Hoboken a short distance from the match site, and operating under the call letters WJY. For the broadcast White and Sarnoff telephoned commentary from ringside, which was typed up and then read over the air by J. Owen Smith. The demonstration was a technical success, with a claimed audience of 300,000 listeners throughout the northeast.\n\nRCA quickly moved to expand its broadcasting activities. In the fall of 1921 it set up its first fulltme broadcasting station, WDY, at the Roselle Park, New Jersey company plant. By 1923 RCA was operating three stations—WJZ (now WABC) and WJY in New York City, and WRC (now WTEM) in Washington, D.C. A restriction imposed by AT&T's interpretation of the patent cross-licensing agreements required that the RCA stations remain commercial free, and they were financed by profits from radio equipment sales.\n\nBeginning in 1922, AT&T became heavily involved in radio broadcasting, and soon became the new industry's most important participant. From the beginning AT&T's policy was to finance stations by commercial sponsorship of the programs. The company also created the first radio network, centered on its New York City station WEAF (now WFAN), using its long distance telephone lines to interconnect stations. This allowed them to economize by having multiple stations carry the same program.\n\nRCA and its partners soon faced an economic crisis, as the costs of providing programming threatened to exceed the funds available from equipment profits. The problem was resolved in 1926 when AT&T unexpectedly decided to exit the radio broadcasting field. RCA purchased, for $1,000,000, AT&Ts two radio stations, WEAF and WCAP in Washington, D.C., as well as its network operations. These assets formed the core for the creation of the National Broadcasting Company (NBC), with ownership divided between RCA (50%), General Electric (30%), and Westinghouse (20%) until 1930, when RCA assumed 100% ownership. This purchase also included the right to begin commercial operations. NBC formed two radio networks that eventually expanded nationwide: the NBC-Red Network, with flagship station WEAF, and NBC-Blue, centered on WJZ. Although NBC was originally promoted as expecting to just break even economically, it soon became very profitable, which would be an important factor in helping RCA survive the economic pressures of the Great Depression that began in 1929.\n\nConcerned that NBC's control of two national radio networks gave it too much power over the industry, in 1941 the Federal Communications Commission (FCC) promulgated a rule designed to force NBC to divest one of them. This order was upheld by the U.S Supreme Court, and on October 12, 1943, the NBC-Blue network was sold to candy magnate Edward J. Noble for $8,000,000, and renamed \"The Blue Network, Inc.\" In 1946 the name was changed to the American Broadcasting Company. The \"Red\" network retained the NBC name, and remained under RCA ownership.\n\nFor two decades the NBC radio network's roster of stars provided ratings consistently surpassing those of its main competitor, the Columbia Broadcasting System (CBS). But in 1948, as the transition from radio to television was beginning, NBC's leadership came under attack due to what became known as the \"Paley raids\", named after the president of CBS, William S. Paley. After World War II the tax rate for annual incomes above $70,000 was 77%, while capital gains were taxed at 25%. Paley worked out an accounting technique whereby individual performers could set up corporations that allowed their earnings to be taxed at the significantly lower rate. Instead of NBC responding with a similar package, Sarnoff decided that this accounting method was legally and ethically wrong. NBC's performers did not agree, and most of the top stars, including Amos and Andy, Jack Benny, Red Skelton, Edgar Bergen, Burns and Allen, Ed Wynn, Fred Waring, Al Jolson, Groucho Marx and Frank Sinatra moved from NBC to CBS. As a result, in 1949 CBS now boasted of having sixteen of the twenty top rated programs. The consequences would carry over to television, where CBS maintained its newfound dominance for decades. Paley had personally worked to woo the performers, while Sarnoff professed his indifference to the defections, stating at an annual meeting that \"Leadership built over the years on a foundation of solid service cannot be snatched overnight by buying a few high-priced comedians. Leadership is not a laughing matter.\"\n\nRCA acted as the sales agent for a small line of Westinghouse and GE branded receivers and parts used by home constructors, originally for a limited market of amateur radio enthusiasts. By 1922, the rise of broadcasting had dramatically increased the demand for radio equipment by the general public, and this development was reflected in the title of RCA's June 1, 1922 catalog, \"Radio Enters the Home\". RCA began selling receivers under the \"Radiola\" name, marketing equipment produced by GE and Westinghouse under the production agreement that allocated a 60%–40% ratio in output between the two companies. Although the patent cross-licensing agreements had been intended to give the participants domination of equipment sales, the tremendous growth of the market led to fierce competition, and in 1925 RCA fell behind Atwater Kent as the leader in receiver sales. RCA was particularly hamstrung by the need to coordinate its sales within the limits of the GE/Westinghouse production quotas, and often had difficulty keeping up with industry trends. However, it made a key advance in early 1924 when it began to sell the first superheterodyne receivers, whose high level of performance increased the brand's reputation and popularity. Until late 1927 all the RCA receivers ran on batteries, but at that point plug-in AC sets were introduced, which provided another boost in sales.\n\nRCA inherited American Marconi's status as a major producer of vacuum tubes, which were branded Radiotron in the United States. Especially after the rise of broadcasting, they were a major profit source for the company. RCA's strong patent position meant that the company effectively set the selling prices for vacuum tubes in the U.S., which were significantly higher than in Europe, where Lee de Forest had allowed a key patent issued to him to lapse. RCA was responsible for creating a series of innovative products, ranging from octal base metal tubes co-developed with General Electric before World War II, to miniaturized Nuvistor tubes used in the tuners of the New Vista series of TV sets. The Nuvistor tubes were a last major vacuum tube innovation, and were meant to compete with the newly introduced transistor. By 1975, RCA had completely switched from tubes to solid-state devices in their television sets, except for the cathode ray tube (CRT) picture tube.\n\nThe rise of radio broadcasting during the early 1920s, which provided unlimited free home entertainment, caused significant financial problems throughout the established phonograph record industry. In 1929, RCA purchased the Victor Talking Machine Company, then the world's largest manufacturer of both records and phonographs, including its popular showcase \"Victrola\" line. This acquisition was organized as a new subsidiary called RCA Victor, and included majority ownership of the Victor Company of Japan (JVC).\n\nWith this purchase, RCA acquired the western hemisphere rights to the famous Nipper/\"His Master's Voice\" trademark. RCA Victor popularized combined radio receiver-phonographs, and also created RCA Photophone, a movie sound-on-film system that competed with William Fox's sound-on-film Movietone and Warner Bros.' sound-on-disc Vitaphone. The acquisition of the Victor company also gave RCA superior distribution and manufacturing capability through Victor's established and extensive network of authorized dealers and newly acquired factories in Camden, New Jersey, which began manufacturing radio sets and components, in addition to Victrolas and records.\n\nRCA Victor began selling the first all electric phonograph in 1930. In 1931, RCA Victor introduced 33⅓ revolutions-per-minute (rpm) records, which were a commercial failure during the Great Depression, partly because the records and playback equipment were very expensive, and also because the audio performance was generally poor; the new format used the same groove size as existing 78 rpm records, and it would require the smaller-radius stylus of the later microgroove systems to achieve acceptable slower-speed performance.\n\nIn 1932, during the nadir of the record business in America, RCA Victor introduced the inexpensive Duo Jr., a small, basic turntable designed to be plugged into radio sets. Also during the 1930s, RCA sold the modernistic RCA Victor M Special, a polished aluminum portable record player designed by John Vassos that has become an icon of Thirties American industrial design. In 1949, RCA Victor released the first 45 rpm \"single\" records, as a response to CBS/Columbia's successful introduction of its microgroove 33⅓ rpm \"LP\" format. RCA Victor began selling 33⅓ rpm LP records in 1950, and in 1951 CBS/Columbia began selling 45 rpm records.\n\nRCA also made investments in the movie industry, but they performed poorly. In April 1928 RCA Photophone, Inc., was organized by a group of companies including RCA to develop sound-movie technology. In the fall of 1927, RCA had purchased stock in Film Booking Office (FBO), and on October 25, 1928, with the help of Joseph P. Kennedy, the Radio-Keith-Orpheum Corporation (RKO) studio was formed by merging FBO with Keith-Albee-Orpheum Corporation (KAO), a company whose holdings included motion picture theaters. The theaters in which RKO had an interest provided a potential market for the RCA Photophone sound systems. RCA ownership of RKO stock expanded from approximately 25% in 1930 to approximately 61% in 1932. However, the RKO studio encountered severe financial problems, going into receivership from early 1933 to 1940. RCA sold its holdings in order to raise funds for its basic operations.\n\nFollowing years of industry complaints that the cross-licensing agreements between RCA, GE and Westinghouse had in effect created spheres-of-influence for the participating companies, resulting in illegal monopolies, in May 1930 the U.S. Department of Justice brought antitrust charges against the three companies. After a long period of negotiation, in 1932 the Justice Department accepted a consent agreement which removed the restrictions established by the cross-licensing agreements, and also provided that RCA would become a fully independent company. As a result, GE and Westinghouse gave up their ownership interests in RCA, while RCA was allowed to keep its factories. In order to give RCA a chance to establish itself, GE and Westinghouse were required to refrain from competing in the radio business for the next two and one-half years.\n\nRCA began TV development in early 1929, after an overly optimistic Vladimir K. Zworykin convinced Sarnoff that a commercial version of his prototype system could be produced in a relatively short time for $100,000. Following what would actually be many years of additional research and millions of dollars, RCA demonstrated an all-electronic black-and-white television system at the 1939 New York World's Fair. RCA began regular experimental television broadcasting from the NBC studios to the New York metropolitan area on April 30, 1939 via station W2XBS, channel 1 (which evolved into WNBC channel 4) from the new Empire State Building transmitter on top of the structure. Around this time, RCA began selling its first television set models, including the TRK-5 and TRK-9, in various New York stores. However, the FCC had not approved the start of commercial television operations, because technical standards had not been yet been finalized. Concerned that RCA's broadcasts were an attempt to flood the market with sets that would force it to adopt RCA's current technology, the FCC stepped in to limit its broadcasts.\n\nFollowing the adoption of National Television System Committee (NTSC) recommended standards, the FCC authorized the start of commercial television broadcasts on July 1, 1941. The entry of the United States into World War II a few months later greatly slowed its deployment, but RCA resumed selling television receivers almost immediately after the war ended in 1945. (\"See also:\" History of television)\n\nIn 1950, the FCC adopted a standard for color television that had been promoted by CBS, but the effort soon failed, primarily because the color broadcasts could not be received by existing black-and-white sets. As the result of a major research push, RCA engineers developed a method of \"compatible\" color transmissions that, through the use of interlacing, simultaneously broadcast color and black-and-white images, which could be picked up by both color and existing black-and-white sets. In 1953, RCA's all-electronic color TV technology was adopted as the standard for American television. At that time, Sarnoff predicted annual color TV sales would reach 1.78 million in 1956, but the sets were expensive and difficult to adjust, and there was initially a lack of color programming, so sales lagged badly and the actual 1956 total would only be 120,000. RCA's ownership of NBC proved to be a major benefit, as that network was instructed to promote its color programming offerings; even so, it was only in 1968 that color TV sales in the U.S. surpassed black-and-white.\n\nWhile lauding the technical prowess of his engineers who had developed color TV, David Sarnoff, in marked contrast to William Paley of CBS, did not disguise his dislike for popular TV programs. His authorized biography even boasted that \"no one has yet caught him in communion with one of the upper dozen or so top-rated programs\" and \"The popular programs, to put the matter bluntly, have very little appeal for him.\"\n\nRCA professional video cameras and studio gear, particularly of the TK-40/41 series, became standard equipment at many American television network affiliates, as RCA CT-100 (\"RCA Merrill\" to dealers) television sets introduced color television to the public.\n\nIn 1941, a few months before the United States entered World War II, the cornerstone was laid for a research and development facility in Princeton, New Jersey called RCA Laboratories. Led for many years by Elmer Engstrom, it was used to develop many innovations, including color television, the electron microscope, CMOS-based technology, heterojunction physics, optoelectronic emitting devices, liquid crystal displays (LCDs), videocassette recorders, direct broadcast television, direct broadcast satellite systems and high-definition television.\n\nDuring World War II, RCA was involved in radar and radio development in support of the war effort, and ranked 43rd among United States corporations in the value of wartime military production contracts. During and after the war, RCA set up several new divisions for defense, space exploration and other activities. The RCA Service Corporation provided large numbers of staff for the Distant Early Warning (DEW) Line. RCA units won five Army–Navy \"E\" Awards for Excellence in production. Also during the war, ties between RCA and JVC were severed.\n\nIn 1955, RCA sold its Estate large appliance operations to Whirlpool Corporation. As part of the transaction, Whirlpool was given the right to market \"RCA Whirlpool\" appliances through the mid-1960s.\n\nRCA was one of a number of companies in the 1960s that entered the mainframe computer field in order to challenge the market leader International Business Machines (IBM) (see also: Computing). Although at this time computers were almost universally used for routine data processing and scientific research, in 1964 Sarnoff, who prided himself as a visionary, predicted that \"The computer will become the hub of a vast network of remote data stations and information banks feeding into the machine at a transmission rate of a billion or more bits of information a second... Eventually, a global communications network handling voice, data and facsimile will instantly link man to machine—or machine to machine—by land, air, underwater, and space circuits. [The computer] will affect man's ways of thinking, his means of education, his relationship to his physical and social environment, and it will alter his ways of living. ... [Before the end of this century, these forces] will coalesce into what unquestionably will become the greatest adventure of the human mind.\"\n\nRCA marketed a Spectra 70 computer line that was hardware, but not software, compatible with IBM's System/360 series. It also produced the RCA Series, which competed against the IBM System/370. This technology was leased to the English Electric company, which used it for their System 4 series, which were essentially RCA Spectra 70 clones. RCA's TSOS operating system was the first mainframe, demand paging, virtual memory operating system on the market. Despite significant investment, in 1971 RCA only had a 4% market share, and it was estimated that it would cost $500 million over the next five years to remain competitive with the IBM/370 series. On September 17, 1971 the RCA Board of Directors announced its decision to close its computer systems division (RCA-CSD), which would be written off as a $490 million company loss. Sperry Rand's UNIVAC division took over the RCA base in January 1972.\n\nRCA Graphic Systems Division (GSD) was an early supplier of electronics designed for the printing and publishing industries. It contracted with German company Rudolf Hell to market adaptations of the Digiset photocomposition system as the Videocomp, and a Laser Color Scanner. The Videocomp was supported by a Spectra computer that ran the Page-1 and, later the Page-II and FileComp composition systems. RCA later sold the Videocomp rights to Information International Inc.\n\nRCA became a major proponent of the eight-track tape cartridge, which it launched in 1965. The eight-track cartridge initially had a huge and profitable impact on the consumer marketplace. Sales of the 8-track tape format declined when consumers increasingly favored the 4-track compact cassette tape format developed by Philips.\n\nOn January 1, 1965, Robert Sarnoff succeeded his father as RCA's president, although the elder Sarnoff remained in control as chairman of the board. In 1969, the company name was changed from \"Radio Corporation of America\" to the \"RCA Corporation\", to reflect its broader range of corporate activities and expansion into other countries. At the end of that same year David Sarnoff, after being incapacitated by a long-term illness, was removed as the company's chairman of the board. He died two years later.\n\nRCA's exit from the mainframe computer market in 1971 marked a milestone in its transition from technology toward diversification as a business conglomerate. During the late 1960s and 1970s, the company made a wide-ranging series of acquisitions, including Hertz (rental cars), Banquet (frozen foods and TV dinners), Coronet (carpeting), Random House (publishing) and Gibson (greeting cards). However, the company was slipping into financial disarray, with wags calling it \"Rugs Chickens & Automobiles\" (RCA), to poke fun at its new direction.\n\nRobert Sarnoff's time as president was unsuccessful, marked by falling profits. He was ousted in a 1975 \"boardroom coup\" led by Anthony Conrad, who became the new company president. Conrad resigned less than a year later after he admitted failing to file income tax returns for six years. His successor, Edgar H. Griffiths, proved to be unpopular and retired in early 1981. Thornton Bradshaw would be the next, and last, RCA president.\n\nRCA maintained its high standards of engineering excellence in broadcast engineering and satellite communications equipment, but ventures such as the NBC radio and television networks declined. Around 1980, RCA corporate strategy reported on moving manufacture of its television sets to Mexico. RCA was still profitable in 1983, when it switched manufacturing of its VHS VCRs from Panasonic to Hitachi.\n\nProjects attempting to establish new consumer electronics products during this era, lost money. An RCA Studio II home video game console, introduced in 1977, was canceled just under two years later due to poor sales. A capacitance electronic (CED) videodisc, marketed under the SelectaVision name, was launched in 1981 after several years of delays. The system was practically obsolete when it finally did appear, and never developed the manufacturing volumes needed to substantially bring down its price. The CED system was unable to compete against the newer cheaper, recordable videotape technology, and was abandoned in 1985, after a write-off of several hundred million dollars.\n\nIn 1981, Columbia sold its share in the home video division to RCA and outside of North America this division was renamed to \"RCA/Columbia Pictures International Video\". The following year, within North America, it was renamed to \"RCA/Columbia Pictures Home Video\". In 1983, Arista Records owner Bertelsmann sold 50% of Arista to RCA. In 1985, Bertelsmann and RCA formed a joint venture called RCA/Ariola International, which took over management of RCA Records.\n\nIn 1984, RCA Broadcast Systems Division moved from Camden, New Jersey, to the site of the RCA antenna engineering facility in Gibbsboro, New Jersey. On October 3, 1985, RCA announced it was closing the Broadcast Systems Division. In the years that followed, the broadcast product lines developed in Camden were terminated or sold off, and most of the buildings and factories in Camden were demolished, except for a few of the old, original Victor buildings that had been declared national historic buildings. For several years, RCA spinoff L-3 Communications Systems East was headquartered in the famous Nipper Building, but has since moved to an adjacent building built by the city for them. The Nipper Building now houses shops and luxury loft apartments.\n\nIn December 1985, it was announced that General Electric would reacquire its former subsidiary for $6.28 billion in cash, or $66.50 per share of stock. The sale was completed the next year, and GE proceeded to immediately sell off most of the RCA assets. (The only RCA unit which GE ultimately retained was Government Services.) GE disposed of its 50% interest in RCA Records to its partner Bertelsmann, and the company was renamed BMG Music, for Bertelsmann Music Group. In 1987, RCA Global Communications Inc., a division with roots dating back to RCA's founding, was sold to the MCI Communications Corporation. The rights to make RCA and GE-branded televisions and other consumer electronics products were purchased in 1988 by the French company Thomson Consumer Electronics, in exchange for some of Thomson's medical businesses. (For information on the RCA brand after 1986, see RCA (trademark).) That same year, its semiconductor business (including the former RCA Solid State unit and Intersil) was bought by Harris Corporation. In 1991, GE sold its share in RCA/Columbia to Sony Pictures which renamed the unit to \"Columbia TriStar Home Video\" (later further renamed to Columbia TriStar Home Entertainment, now Sony Pictures Home Entertainment). This merger surpassed the Capital Cities/ABC merger that happened earlier in 1985 as the largest non-oil merger in business history.\n\nSarnoff Labs was put on a five-year plan whereby GE would fund all the labs' activities for the first year, then reduce its support to near zero after the fifth year. This required Sarnoff Labs to change its business model to become an industrial contract research facility. In 1988 it was transferred to SRI International (SRI) as the David Sarnoff Research Center, and subsequently renamed the Sarnoff Corporation. In January 2011 it was fully integrated into SRI.\n\nGE sold all of its radio station holdings to various owners, and the NBC Radio Network to Westwood One. In 2011, a controlling interest in the National Broadcasting Company, by this time part of the multimedia NBC Universal venture that included TV and cable, was sold by GE to Comcast, and in 2013, Comcast acquired the remaining interest.\n\nRCA antique radios, and early color television receivers such as the RCA Merrill/CT-100, are among the more sought-after collectible radios and televisions, due to their popularity during the golden age of radio and the historic significance of the RCA name, as well as their styling, manufacturing quality and engineering innovations. Most collectable are the pre-war television sets manufactured by RCA beginning in 1939, including the TRK-5, TRK-9 and TRK-12 models.\n\nThe historic RCA Victor Building 17, the \"Nipper Building\", in Camden, New Jersey, was converted to luxury apartments in 2003.\n\nA type of plug/jack combination used in audio and video cables is still called the RCA connector.\n\nTo this day, a variety of consumer electronics including 2-in-1 tablets, televisions and telephones, home appliances and more are sold under the RCA brand name.\n\nNumerous former RCA manufacturing sites have been reported to be polluted with industrial waste.\n\n\n\n"}
{"id": "10330140", "url": "https://en.wikipedia.org/wiki?curid=10330140", "title": "Rough (facility)", "text": "Rough (facility)\n\nRough is a former natural gas storage facility situated off the east coast of England. In June 2017, Centrica Storage Ltd announced that gas injection and storage would cease.\n\nLicences for the Rough Field were given in 1964. Gas was brought ashore to Easington gas processing terminal in 1975. In 1980 BG Corporation (which became British Gas plc in 1986 following privatisation) purchased the Rough field with one third of reserves depleted with the idea of converting the field into a gas storage facility to manage seasonal trends in the supply and demand of gas in the UK. In 1983 BG Corporation made the final decision to convert Rough into a natural gas storage facility. This would be the largest gas storage facility built in the UK continental shelf. In 1985 the Rough storage facility became operational.\n\nThe break up of BG Corporation in 1997 into BG Group and Centrica meant that BG Storage was created as a standalone business due to competition reasons. In 2001, BG Storage sold the Rough Facility to Dynegy. In 2002, Centrica bought the plant back from Dynergy for £304m during its period of near-bankruptcy. The purchase of Rough led to the Competition Commission requesting certain undertakings being put in place due to Centrica's control of the Morecambe Bay gas fields which at the time were providing 10–15% of the UK's gas supply. In 2003 Centrica provided DECC with a set of undertakings and Centrica Storage Ltd (a wholly owned subsidiary of Centrica Plc) was formed.\n\nCentrica Storage Ltd still operates the Rough facility in accordance with the undertakings.\n\nIn June 2017, Centrica announced the closure of the Rough gas storage site due to safety concerns. The rundown of the plant will take up to four years to complete.\n\nThe facility consists of a partially depleted gas field (the Rough field) in the Southern North Sea, approximately 18 miles off the east coast of Yorkshire, together with an onshore gas processing terminal at Easington, approximately 27 miles south east of Hull.\n\nThe Rough processing terminal forms a part of the larger Easington Gas Terminal which can inject upwards of 125 million cubic metres per day (approximately 40% of the daily UK supply).\n\nThe Rough facility is operated by Centrica Storage Ltd, a wholly owned subsidiary of Centrica. Nearly 200 staff and contractors are employed within the operation, both onshore and offshore. It has a storage capacity of 3.31 billion cubic metres which is approximately 70% of the UK's gas storage capacity (approximately nine days' supply). Rough can supply 10% of the UK's peak gas demand and as such is an important part of the UK's gas infrastructure. Operational problems lead to a partial shutdown in 2016, requiring increased imports during winter.\n\nAt the time of the announcement of the rundown of the facility in June 2017, Rough facility was the only depleted UK offshore gas field reservoir that is used for gas storage and retrieval. Several projects have been developed to use other depleted offshore fields but none have proved to be economically viable. Two examples are the Baird and the Deborah Gas Storage projects.\n\n\n"}
{"id": "10640587", "url": "https://en.wikipedia.org/wiki?curid=10640587", "title": "Rāzna National Park", "text": "Rāzna National Park\n\nRāzna National Park () is a national park in the Latgale region of Latvia. It was established in 2007 and covers an area of . The initiative to create the Rāzna National Park out of an already existing nature park came from the Daugavpils University. \n\nThis national park was created to protect Lake Rāzna, the second largest lake in Latvia, and the surrounding areas. Because of this, 14% of the surface area of the national park consists of water surfaces. Valuable ecosystems - natural deciduous forests with many rare species of plants are found on several of the 26 islands in Ežezers lake. Of high value are also the semi-natural grasslands.\n\n"}
{"id": "38007788", "url": "https://en.wikipedia.org/wiki?curid=38007788", "title": "Scrap metal shredder", "text": "Scrap metal shredder\n\nA scrap metal shredder, also sometimes referred to as a metal scrap shredder, is a machine used for reducing the size of scrap metal. Scrap metal shredders come in many different variations and sizes.\n\nSome examples of scrap metal materials that are commonly shredded are: \nAuto shredders are large machines that turn a car into a large bucket of scrap steel and the rest of the car into non-ferrous materials, plastics and waste. The glass, fabric, plastic, and all other non ferrous materials are separated by eddy current magnets in place of heavy media separation. The non-ferrous materials may be referred to as \"zorba.\" Often the profit from the non ferrous materials covers the operating cost for the auto-shredder.\n\nMetal scrap recycling, also called secondary metal processing, is a large industry that processes, in the U.S. alone, 56 million tons of scrap iron and steel (including 10 million tons of scrap automobiles), 1.5 million tons of scrap copper, 2.5 million tons of scrap aluminum, 1.3 million tons of scrap lead, 300,000 tons of scrap zinc and 800,000 tons of scrap stainless steel, and smaller quantities of other metals, on a yearly basis.\n\nScrap metal shredders can be equipped with different types of cutting systems: horizontal shaft, vertical shaft, single-shaft, two-shaft, three-shaft and four-shaft cutting systems. These shredder designs can be high speed, medium speed and sometimes slow-speed systems, they always include hammermills of a vertical and horizontal shaft design, and can also include in contrast to hammer mills slow speed technology which are also used to process or shred metal and plastic and other waste materials encountered in the scrap metal industry.\nThe largest scrap metal shredders in the world often have 10,000 hp and are made by a wide range of companies which all originated their designs from the 1966 patent applications of the Newell Group and the Williams Group as the first patented vertical and horizontal designs for auto shredding or scrap metal shredding. Often cited is the 9,200 hp shredder from the Lynxs group at the Sims plant at the mouth of the River Usk in Newport Wales with access by road, rail and sea. This Lynxs shredder can eat 350 cars per hour. However, the Lynxs shredders are not unique in this high hp range design. Historically the Schnitzer Steel group installed a 10,000 hp unit that they made on their own in 1980, and there are many Newell Shredders that have these high hp designs. A 9,000 horsepower Mega Shredder at Sims Metal Management's Claremont Terminal in Jersey City, New Jersey processes 4,000 tons of metal a day.\n\nAn engineered enclosure may be used to contain noise and dust. In 2018, the California Department of Toxic Substances Control requested public comment on plans to implement enforceable operating requirements for scrap metal shredders.\n\nIn Houston, the carcinogenic substance hexavalent chromium was identified as an emission from a scrap recycling facility.\n\n\n"}
{"id": "35295552", "url": "https://en.wikipedia.org/wiki?curid=35295552", "title": "Solar power in the Czech Republic", "text": "Solar power in the Czech Republic\n\nThe Czech Republic had almost two gigawatts (GW) of photovoltaic capacity at the end of 2010, but installed less than 10 megawatts (MW) in 2011 due to the feed-in tariff being reduced by 25%, after installing almost 1,500 MW the year before. Installations increased to 109 MW in 2012. In 2014, no new installations were reported.\n\nIn 2003 a Czech-Austrian information and training center for solar power was founded in the village of Věžovatá Pláně in South Bohemia. That same year major Josef Mach claimed that the electricity from the Temelín nuclear power plant in the Czech Republic would be abandoned. He is known as one of the biggest Temelín opponents in the Czech Republic.\n\n"}
{"id": "8065381", "url": "https://en.wikipedia.org/wiki?curid=8065381", "title": "Spar (tree)", "text": "Spar (tree)\n\nA spar tree is the tree used as the highest anchor point in a high lead cable logging setup. The spar tree is selected based on height, location and especially strength and lack of rot in order to withstand the weight and pressure required. Once a spar tree is selected, a climber would remove the tree's limbs and top the tree (a logging term for cutting off the top of the tree). Block and tackle is then affixed to the tree and cabling is run.\n\nA \"high climber\" is the member of the logging crew who scales the tree, limbs it, and tops it.\n\nSelecting a tree as a spar is a particularly important task, so the strength and importance of the spar came to hold symbolic meaning for early loggers of the West.\n\nThe use of spar trees in logging is now rare, having been replaced since the 1970s by portable towers, called yarders, which can be erected on logging sites and moved as needed.\n\n"}
{"id": "8753589", "url": "https://en.wikipedia.org/wiki?curid=8753589", "title": "Steam digester", "text": "Steam digester\n\nThe steam digester (or bone digester, and also known as Papin’s digester) is a high-pressure cooker invented by French physicist Denis Papin in 1679. It is a device for extracting fats from bones in a high-pressure steam environment, which also renders them brittle enough to be easily ground into bone meal. It is the forerunner of the autoclave and the domestic pressure cooker.\n\nThe steam-release valve, which was invented for Papin's digester following various explosions of the earlier models, inspired the development of the piston-and-cylinder steam engine.\n\nArtificial vacuum was first produced in 1643 by Italian scientist Evangelista Torricelli and further developed by German scientist Otto von Guericke with his Magdeburg hemispheres. Guerike's demonstration was documented by Gaspar Schott, in a book that was read by Robert Boyle. Boyle and his assistant Robert Hooke improved Guericke's air pump design and built their own. From this, through various experiments, they formulated what is called Boyle's law, which states that the volume of a body of an ideal gas is inversely proportional to its pressure. Soon the ideal gas law was formulated.\n\nBased on these concepts in 1679 a Boyle's associate, named Denis Papin, built a \"bone digester\", which is a closed vessel with a tightly fitting lid that confines steam until a high pressure is generated. Later designs implemented a steam release valve to keep the machine from exploding. By watching the valve rhythmically moving up and down, Papin conceived the idea of a piston and cylinder engine. He did not, however, follow through with his design. In 1697, independent of Papin's designs, engineer Thomas Savery built the world's first steam engine. By 1712 an improved design based on Papin's ideas was developed by Thomas Newcomen.\n\nBoyle speaks of Papin as having gone to England in the hope of finding a place in which he could satisfactorily pursue his favorite studies. Boyle himself had already been long engaged in the study of pneumatics, and had been especially interested in the investigations which had been original with Guericke. He admitted young Papin into his laboratory, and the two philosophers worked together at these attractive problems.\nHe probably invented his \"Digester\" while in England, and it was first described in a brochure written in English, under the title, \"The New Digester.\" It was subsequently published in Paris.\n\nThis was a vessel, B, capable of being tightly closed by a screw, D, and a lid, C, in which food could be cooked in water raised by a furnace, A, to the temperature due to any desired safe pressure of steam. The pressure was determined and limited by a weight, W; on the safety valve lever, G. It is probable that this essential attachment to the steam boiler had previously been used for other purposes; but Papin is given the credit of having first made use of it to control the pressure of steam.\n\nIn 1787, Antoine Lavoisier, in his \"Elements of Chemistry\", refers to \"Papin's digester\" and discusses how the strong compression of water and other liquids in it, which is able to sustain a red heat, creates artificial atmospheres, which may potentially be able to soften or liquefy stones, salts, and the various parts of the earth.\n\n\n"}
{"id": "9917171", "url": "https://en.wikipedia.org/wiki?curid=9917171", "title": "Sustainable South Bronx", "text": "Sustainable South Bronx\n\nSustainable South Bronx (SSBx) is a non-profit workforce development and environmental justice solutions organization in New York City's South Bronx neighborhood, founded by Majora Carter in 2001. Today, it is a division of the Brooklyn-based workforce development organization,The HOPE Program.\n\nThe organization spearheaded the creation of Hunts Point Riverside Park, the first piece of the projected South Bronx Greenway. This organization also pioneered a \"green roof\" project in the South Bronx with its own for-profit installation company SmartRoofs, LLC, and started a \"green-collar\" job training program called Bronx Environmental Stewardship Program (B.E.S.T.) which prepares urban residents in areas such as ecological restoration, hazardous waste cleanup, green roof installation and maintenance, urban forestry, and landscaping; the program has a 90% placement rate after four years of operation. They are also proposing a Bronx Eco-Industrial Complex as an alternative use for a piece of land where the city government currently is planning to construct a prison, and are engaged in developing \"a collection of businesses in which the waste and byproducts of one business are the raw materials for another one.\"\n\nIn 2005, SSBx built the \"Cool and Greenroof Demonstration Project\" above their offices in the historic American Banknote Building — the first such roof in the City of New York. In 2007, SSBx launched the for-profit SmartRoofs, LLC green roof installation business.\n\nToday, SSBx still runs a robust 12 week green jobs training program (formerly called the Bronx Environmental Stewardship Training (BEST), now called Sustainable South Bronx. Training topics range from landscaping, energy auditing, green infrastructure, OSHA and BPI certifications and more. This program aims to provide economic empowerment to the community through training, employment and career services, while contributing to the environmental sustainability of New York City. SSBx also runs two transitional employment programs, which provide participants with on the job training, valuable experience, and a fair wage for their work. NYC °CoolRoofs hires New Yorkers to coat NYC rooftops with a reflective material, easing buildings' energy consumption and mitigating the urban heat island effect. Intervine, SSBx's social enterprise, trains and employs low-income community members to create and maintain green infrastructure. \n\n\n"}
{"id": "24901202", "url": "https://en.wikipedia.org/wiki?curid=24901202", "title": "Symplectite", "text": "Symplectite\n\nA symplectite (or symplektite) is a material texture: a micrometre-scale or submicrometre-scale intergrowth of two or more crystals. Symplectites form from the breakdown of unstable phases, and may be composed of minerals, ceramics, or metals. Fundamentally, their formation is the result of slow grain-boundary diffusion relative to interface propagation rate.\n\nIf a material undergoes a change in temperature, pressure or other physical conditions (e.g., fluid composition or activity), one or more phases may be rendered unstable and recrystallize to more stable constituents. If the recrystallized minerals are fine grained and intergrown, this may be termed a symplectite. A cellular precipitation reaction, in which a reactant phase decomposes to a product phase with the same structure as the parent phase and a second phase with a different structure, can form a symplectite. Eutectoid reactions, involving the breakdown of a single phase to two or more phases, neither of which is structurally or compositionally identical to the parent phase, can also form symplectites.\n\nSymplectites may be formed by reaction between adjacent phases or to decomposition of a single phase. The intergrown phases may be planar or rodlike, depending on the volume proportions of the phases, their interfacial free energies, the rate of reaction, the Gibbs free energy change, and the degree of recrystallization. Lamellar symplectites are common in retrogressed eclogite. Kelyphite is a symplectite formed from the decomposition of garnet. Myrmekite is a globular or bulbous symplectite of quartz in plagioclase.\n\nExamples of symplectites formed in Earth materials include \ndolomite + calcite, aragonite + calcite, and magnetite + clinopyroxene. \nSymplectite formation is important in metallurgy: bainite or pearlite formation from the decomposition of austenite, for example.\n\n"}
{"id": "57480338", "url": "https://en.wikipedia.org/wiki?curid=57480338", "title": "Terufumi Sasaki", "text": "Terufumi Sasaki\n\nAfter the detonation occurred, he was one of the first to observe, document and attempt to treat \"atomic bomb sickness\", now known as acute radiation syndrome. Terufumi Sasaki led intensive research into the syndrome in the weeks and months post-detonation, leading to the establishment of three recorded stages of the syndrome. Within 25–30 days of the explosion, Sasaki noticed a sharp drop in white blood cell count and established this drop, along with symptoms of fever, as prognostic standards for Acute Radiation Syndrome. In the years afterward he would become one of the leading surgeons continuing to document and treat the \"Hibakusha\" (explosion-affected) community, serving as an important source of knowledge for the Atomic Bomb Casualty Commission, and later Radiation Effects Research Foundation, who began and continue the Life Span Study of atomic bomb survivors, respectively.\n\nThe gamma ray dose received by a set of photographic film, and therefore those within the reinforced concrete Red Cross building at Hiroshima, was approximately 15 rad on the third floor.\n\n"}
{"id": "41782052", "url": "https://en.wikipedia.org/wiki?curid=41782052", "title": "The Call of the Man Eater", "text": "The Call of the Man Eater\n\nThe Call of the Man Eater is the fourth book of jungle tales and man-eaters by Kenneth Anderson, first published in 1961 by George Allen and Unwin.\n\nKenneth Anderson gratefully dedicates the book to the memory of his father, the late Douglas Stuart Anderson, Superintendent of Military Accounts in the former British Government of India, \"who instilled in me, when very young and n the hard way, a love for the wild places and living creatures of the land, the beauties of the countryside, and a deep appreciation of the marvelous ways of Nature and of God - an example which I have imperfectly striven to emulate by passing on these precepts to my son, Donald\".\n\nIntroduction\n\nAnderson introduces his fourth book by explaining he hopes to do something different, aiming to also capture and portray to the reader's mind the background and setting to his stories as well as the exciting events themselves.\n\nThe Call of the Man-Eater\n\nAnderson heads to a Gunjur bungalow on the trail of a man-eating tiger when the daughter of a hard working caretaker is killed. Anderson sets out to hunt the man-eater hampered by a lack of supplies, a torch with a failing battery and his caretaker friend whose nerves keep getting the better of him. It soon becomes apparent that the tiger is being led to his kills by a lone jackal, which upon seeing the tiger's next victim lets out a cry so that the tiger can locate its next kill. With limited supplies, Anderson and the caretaker improvise a bait to sit over, by making a scarecrow from the caretaker's pillows and clothes.\n\nThe Evil One of Umbalmeru\n\nLocal workers start to go missing in the Chamala Valley, plucked from the ground without any drag marks, blood or foot prints during the middle of the afternoon. Locals soon spread word that the Chamala Valley is haunted by an 'Evil One'. Anderson arrives in the valley to unravel this mystery and through some investigations he discovers a very interesting story about a local circus - which leads him to the reason why the man-eater kills during the day, and where it may be found.\n\nA Night by the Camp Fire\n\nAnderson takes us with him to his solitary jungle retreat, The Secret River at Kundukottai. Whilst describing his night in a jungle camp, he also tells the story of how his family acquired certain jungle pets; Bruno the sloth bear, Jackie the hyena and Ella the jackal.\n\nThe Black Rogue of the Moyar Valley\n\nAnderson takes an American tourist to the Mysore area to photograph Indian wildlife, and whilst there they encounter a rogue black elephant which had reportedly killed locals. Anderson and the tourist have a narrow escape with the elephant, and later heads back out to track and kill it. The story ends with the black elephant coming to an unexpected and sad end, which leaves Anderson feeling sympathy for the rogue despite its actions.\n\nJungle Days and Nights\n\nAnderson recounts a collection of events from the Indian jungles, some tragic, some humorous. Including; guiding a stubborn senior factory officer out to shoot a crocodile, a friend who refers to a local rogue elephant as his watchdog and problems faced with the local police when Anderson reports a body of murdered woman in the jungle - resulting in the police accusing him of the murder, which later on influenced a much regretted decision in his life.\n\nThe Creatures of the Jungle\n\nAnderson shares his knowledge and experiences about various creatures of the Indian jungle; wild dog, wild boar, sloth bear, pangolin, hyena, tiger, panther and elephant.\n\nThe Sulekunta Panther\n\nAnderson's shikaris friend, Muniappa (previously in the story of The Maneater of Jowlagiri - Nine Maneaters And One Rogue) sends many requests for him to come and shoot a cattle lifting tiger, as he will get out of a large local debt if he can provide the skin as settlement. Eventually agreeing to shoot the tiger, Anderson ends up spending the worst night of his life on a badly made machan during a terrible storm. The exposure brings on a bought of malaria, and Anderson does not return to shoot the tiger for another month or so. Upon his return it transpires that in fact the cattle lifter is a panther and not a tiger. Anderson sits up and awaits the panther, and when it arrives and he has a clean shot - he decides at the last minute to let it go.\n\nFrom Mauler to Man-Eater\n\nThis story follows on from two previously published stories: The Mauler of Rajnagara (Man Eaters and Jungle Killers) and The Maneater of Pegepalyam (The Black Panther of Sivanipalli and Other Adventures of the Indian Jungle). Previously a tiger in the area of Rajnagara had turned from a mauler to a man-eater, but had a curious habit of never using its teeth and only killing with its claws. Following the previous failed attempts to shoot the tiger, Anderson and his son, Donald kept watch on the newspapers for any further news. When two human kills are reported in Bejahahai, Donald travels there alone to dispatch the killer. At this point Donald takes over the writing of the story and describes his hunt for the man-eater in detail. Donald eventually kills the maneater at its cave up a steep hill. Examining the body he finds that the whole of the tiger's nose had been blown away by a shot gun, disabling it from using its jaws as a weapon.\n"}
{"id": "2628682", "url": "https://en.wikipedia.org/wiki?curid=2628682", "title": "Tornado outbreak", "text": "Tornado outbreak\n\nA tornado outbreak is the occurrence of multiple tornadoes spawned by the same synoptic scale weather system. The number of tornadoes required to qualify as an outbreak typically are at least six to ten.\n\nThe tornadoes usually occur within the same day, or continue into the early morning hours of the succeeding day, and within the same region. Most definitions allow for a break in tornado activity (time elapsed from the end of last tornado to the beginning of next tornado) of six hours. If tornado activity indeed resumes after such a lull, many definitions consider the event to be a new outbreak. A series of continuous or nearly continuous tornado outbreak days is a tornado outbreak sequence. Tornado outbreaks usually occur from March through June in the Great Plains of the United States and Canada, the Midwestern United States, and the Southeastern United States in an area colloquially referred to as Tornado Alley. Tornado outbreaks do occur during other times of the year and in other parts of the world, however. A secondary less active and annually inconsistent tornado \"season\" in the U.S. occurs in late autumn.\n\nThe largest tornado outbreak on record was the 2011 Super Outbreak, with 362 tornadoes and about $10 billion in direct damages. It surpasses the 1974 Super Outbreak, in which 148 tornadoes were counted. Both occurred within the United States and Canada. The total number of tornadoes is a problematic method of comparing outbreaks from different periods, however, as many more smaller tornadoes, but not stronger tornadoes, are reported in the US in recent decades than in previous ones.\n\n\n\n"}
{"id": "43850177", "url": "https://en.wikipedia.org/wiki?curid=43850177", "title": "Trailer skirt", "text": "Trailer skirt\n\nA trailer skirt or side skirt is a device affixed to the underside of a semi-trailer, for the purpose of reducing aerodynamic drag caused by air turbulence. Trailer skirts have been recognized by the U.S. Environmental Protection Agency's SmartWay Transport Partnership as a verified aerodynamic technology, eligible for funding under the Diesel Emissions Reduction Act.\n\nTrailer skirts comprise a pair of panels affixed to the lower side edges of a trailer, running most of the length of the trailer and filling the gap between the forward and rear axles. Trailer skirts are typically constructed of aluminum, plastic, or fiberglass, with plastic the most resistant to damage from side or bottom impacts. Skirts may have a modular design, allowing installation on a variety of trailer lengths. Skirts may weigh between . Installation typically requires three to five .\n\n, a set of trailer skirts cost between C$1500 and C$3000 (US$1300 to $2700). Standard trailer skirts have an estimated payback period of ten to eighteen months, while \"advanced\" skirts (those that improve fuel efficiency by over 7%) are estimated to pay for themselves in seven to fourteen months.\n\nA 2012 investigation by SAE International of nine trailer skirt designs found that three provided fuel savings greater than 5%, and four provided savings between 4% and 5%, compared with an unmodified trailer. Skirts with reduced ground clearance offer greater fuel savings; in one instance, reducing ground clearance from to resulted in an improvement in fuel savings from 4% to 7%. One 2008 Delft University of Technology study found fuel savings of up to 15% for the particular design studied. Sean Graham, president of a major supplier of trailer skirts, estimates that in typical use, drivers see fuel savings of 5% to 6%.\n\nTrailers with skirts fitted have also demonstrated reduced tire spray, and drivers have reported improved stability in crosswinds.\n\nAs of 2018 over 60% of new trailers produced in Australia are equipped with skirts, whereas the figure is over 50% for the North American market.\n\nA 2014 study by the North American Council for Freight Efficiency on adoption of fuel efficient technologies and practices found trailer skirts to be the most widely adopted technology of those studied, having been adopted by seven of the ten major shipping fleets in the study.\n\nSince the wide adoption of trailer skirts, the incidences of them becoming detached and hurting other drivers on the road have increased. On October 18th, 2018 in one particular case, a trailer skirt came off of an east bound tractor trailer and ended up in the west bound lanes where it hit a car and killed it's driver Mark Elliot, 71, of St. Louis, Missouri while he was traveling through Knoxville, Tennessee.\n\n"}
{"id": "55621437", "url": "https://en.wikipedia.org/wiki?curid=55621437", "title": "Yakisugi", "text": "Yakisugi\n\nYakisugi (焼杉) is a traditional Japanese method of wood preservation. \"Yaki\" means to heat with fire, and \"sugi\" is cypress. It is also referred to in the West as \"shō sugi ban\" (焼杉板), which uses the same \"kanji\" characters but a different pronunciation. The \"ban\" character means \"plank\".\n\nBy slightly charring the surface of the wood without combusting the whole piece, the wood becomes water-proof through the carbonisation and is thus more durable. It also protects against insects.\n\nContemporary architect Terunobu Fujimori works with \"yakisugi\".\n\n"}
{"id": "10018708", "url": "https://en.wikipedia.org/wiki?curid=10018708", "title": "Zoology of the Voyage of H.M.S. Beagle", "text": "Zoology of the Voyage of H.M.S. Beagle\n\nThe Zoology of the Voyage of H.M.S. Beagle Under the Command of Captain Fitzroy, R.N., during the Years 1832 to 1836 is a 5-part book published unbound in nineteen numbers as they were ready, between February 1838 and October 1843. It was written by various authors, and edited and superintended by Charles Darwin, publishing expert descriptions of the collections he had made during the \"Beagle\" voyage.\n\n\nDarwin also contributed notices of habits and ranges throughout the text of Mammalia and Birds, and the text of the Fish and the Reptiles included numerous notes by him that were mostly taken from his labels. The authors of these parts were as follows:\n\n\nFor a small additional fee the publishers sold the completed work bound, in five volumes, and later bound in three volumes, the first incorporating Parts 1 & 2, the second Part 3 and the third Parts 4 & 5. An example of this arrangement can be seen in the catalogue entry for the copies held at the State Library of New South Wales. The copies published by Elder Smith, 1840-1843 has the five volumes bound into three with some plates folded.\n\n"}
