{"id": "25418317", "url": "https://en.wikipedia.org/wiki?curid=25418317", "title": "5AT Advanced Technology Steam Locomotive", "text": "5AT Advanced Technology Steam Locomotive\n\nThe 5AT Advanced Technology steam locomotive is a conceptual design conceived by the British engineer David Wardale, and first described in his definitive work on modern steam, \"The Red Devil and Other Tales from the Age of Steam.\"\n\nWardale's purpose in putting forward the \"Super Class 5 4-6-0\" design concept (as he then called it) was to offer a future for steam hauled trains on the main lines in the UK on which the use of heritage traction is likely to be gradually phased out as the speed and density of commercial rail traffic increase.\n\nWork on the project was suspended in March 2012 following completion of a project Feasibility Study and subsequent failure to raise the finance needed to complete the detail design and construction of the locomotive.\n\nWardale's proposal was for a locomotive that would:\n\n\nThe April 1998 edition of the Steam Railway magazine contained an article by Wardale titled \"Whither Steam Now?\" which included reference to a \"locomotive of Class 5 4-6-0 format - calling it a 5GT - that would outperform any British Pacific.\"\n\nStill described as the 5GT in a Feb 2001 Railway Magazine article titled \"Wardale reveals £1.7 million 'new steam' proposal for the heritage market\", the name 5AT first emerged in a letter from Alan Fozard to the editor of Steam Railway Magazine issue 259 in June–July 2001, which coincided with the formation of the 5AT Project.\n\nThe 5AT in its final conceptual form remained almost identical in size and weight to the BR 5MT, sharing the axle spacings and being just 4 tonnes heavier, with axle loads of 20 tonnes on each driving axle and 10 tonnes on each bogie axle. Where it differed in size from the 5MT was in its tender which was massively enlarged to carry large quantities of fuel and water to extend its operating range. Such a large tender would also have provided space for the locomotive-carried parts of advanced signalling systems such as ERTMS. The 5AT's four-axle tender would have had a gross weight of 80 tonnes with a fuel (light-oil) capacity of 7 tonnes and water capacity of 46 tonnes (12,000 US gallons).\n\nThe 5AT Project was established in 2001 with the aim of commercializing Wardale's 5AT locomotive concept and bringing it to reality. The project was spearheaded by a small team of professional engineers, scientists and businessmen and women whose common purpose was to see steam traction continue in main-line operation into the foreseeable future.\n\nThe first task undertaken by the project team was to commission David Wardale to undertake the Fundamental Design Calculations for the locomotive. The purpose of these calculations was to verify through detailed calculations that the conceptual design was viable in engineering terms and that it would meet the performance predictions that Wardale had made for it (see below). At the same time, a project website was established.\n\nWardale completed the Fundamental Design Calculations for the locomotive in late 2004 after 2½ years of almost continuous labour. The work comprises 6100 lines of calculation divided into 26 sections as follows:\n\n\nSince the completion of the Fundamental Design Calculations, the 5AT Project team focussed on the development of a business plan to attract investment in the project. This proved to be a difficult task because of the high development costs that were anticipated for designing and gaining railway approvals for a prototype locomotive, and the limited returns that might be expected from the locomotive's use on tourist and \"cruise\" trains. It was concluded that the cost of building the locomotive could only be justified if its initial development costs were \"written off\".\n\nThe project team also undertook extensive research into the possibilities of developing alternative designs based around the high efficiency and low maintenance concepts of the 5AT. Particular focus was placed on the possibilities for (coal-fired) steam traction for coal transportation in developing countries such as Indonesia, for which the operating costs were estimated to have been substantially lower than for diesel and electric traction.\n\nThe performance predicted for the 5AT locomotive are summarized as follows:\n\n\nThe above performance predictions were predicated on several technical advances, mostly developed by Ing. L.D. Porta, all of which have been proven in practice, in most cases on Wardale's SAR Class 26 \"The Red Devil\" and described in Wardale's book on the subject. These advances are summarized as follows:\n\n\nIn addition, the locomotive would have incorporated the following features that would have minimized its maintenance requirements and increased its reliability:\n\n\nIn March 2012, after eleven years of development, the decision was made to suspend the project due to lack of financial support. In the project Feasibility Study, it was estimated that the funding required to complete the detail design, manufacture, assembly, testing and main-line approvals for a prototype locomotive was slightly over £10 million ($16M) at 2010 prices. However subsequent \"production\" locomotives were estimated to cost in the order of £2.5M ($4M), a cost that could have been justified by the anticipated returns from the haulage of luxury high-speed cruise and tour trains for which the concept was targeted. Following suspension of the project the design group re-formed as the \"Advanced Steam Traction Trust\" to provide engineering resources for alternative projects either in the form of new designs or in improvements to existing designs.\n\n\n"}
{"id": "11176707", "url": "https://en.wikipedia.org/wiki?curid=11176707", "title": "Acción de Lucha Anti-Petrola", "text": "Acción de Lucha Anti-Petrola\n\nFormed in December 1999, Acción de Lucha Anti-Petrola (ADELA) is a Costa Rican grassroots environmental group created to oppose offshore oil exploration and drilling. Translated into English as \"Anti-Petroleum Action,\" ADELA is named after a local indigenous woman who was \"struggling to defend her culture and local environment against outside forces.\" ADELA initially consisted of 30 local citizen's groups ranging from farmers' organizations and the fisherman's union, to religious groups, small business owners, and marine biologists. ADELA now comprises over 100 local citizens' organizations. The various groups came together to form ADELA after becoming aware of the negative environmental impact of seismic reflection explorations being conducted by Harken Costa Rica Holdings in November 1999.\n\nThe primary goal of ADELA is to effectuate a complete national moratorium on oil exploration. In carrying out its mission, ADLEA informs and educates both the Caribbean community and political actors about the environmental risks the oil industry poses for Costa Rica, as well as for the Caribbean generally. ADELA also works with members of local communities to promote sustainable development, and to develop more environmentally friendly energy practices and policies.\n\nWith little material resources, ADELA largely relied on the grassroots efforts of its own members until 2001. ADLEA was largely successful in its efforts, and managed to stage protests, voice their opinions over the radio, and even bring lawsuits in the Costa Rican national court. After recognizing that their message was not reaching the United States, home of the oil companies seeking to drill off the Costa Rican shoreline, ADELA contacted the Natural Resources Defense Council (NRDC) in early 2001. ADELA's cause received international exposure when the NDRC included the Talamanca tropical forest region in southern Costa Rica, in its BioGems initiative. ADELA also began to receive financial support, research, and legal resources from the following groups: NRDC, Environmental Law Alliance Worldwide, Global Response, Caribbean Conservation Corporation, Interamerican Association for Environmental Defense, International Fund for Animal Welfare, World Conservation Union, Forest Conservation Action Alerts, Environmental News Network, Project Underground, and Radio International Feminista.\n\nMuch of the oil exploration to date has occurred in the region of Costa Rica known as Talamanca. Talamanca is recognized as being one of the richest marine ecosystems on the planet, and its mountainous terrain and tropical vegetation are considered to be equally rich. Due to its biological significance, Talamanca is protected as a UNESCO World Heritage Site. Talamanca is also home to three indigenous community reserves, the Cahuita National Park, and a UN-designated wetlands site at the Gandoca-Manzanillo Wildlife Refuge. In total, 88% of the Talamanca region has some level of environmental protection.\n\nTalamanca's marine resources include coral reefs, mangroves, rare manatees, over 100 species of fish, and sea turtle nesting beaches. In fact, Costa Rica's Caribbean coast is a migratory pathway for at least three endangered species of sea turtles, and sea turtles are recognized as being especially susceptible to oil pollution.\n\nProtection of the sea turtles is a primary concern for conservationists, but it is likely that mangroves and wetlands, generally, would not be able to recover from an oil spill. Oil pollution would also significantly impact the coastal communities and local economic infrastructure, as it would likely result in hundreds of fishing jobs being lost and a decrease in eco-tourism. Eco-tourism remains Costa Rica's primary source of revenue from other countries.\n\nIn 1994, the Costa Rican legislature passed a Hydrocarbons Law which divided the country into 27 blocks for oil and natural gas exploration, and it allowed firms of foreign countries to bid for oil concessions. In July 1998, Costa Rican President Miguel Angel Rodriguex (1998–2002) granted one concession of four exploration blocks to MKJ Xploration Inc. (MKJ), a small Louisiana-based company. Located in Talamanca, the four blocks were to be used for the exploration and extraction of hydrocarbons.\n\nIn November 1998, Harken Energy Corporation (Harkan) of Dallas, Texas, purchased 40% of the concession rights from MKJ, and the two companies combined to create Harken Costa Rica Holdings (HCRH). In March 1999, Costa Rica's National Technical Environmental Secretariat (SETENA) approved an environmental impact study (EIA) submitted by HCRH to conduct marine seismic reflection blasting.\n\nBetween October and December 1999, HCRH began using seismic reflection blasting to map possible oil reserves. Such blasting was in derogation of a condition in the EIA that restricted blasting to the months of January and July, so as to minimize the impact on marine life. The blasting significantly disrupted the local Costa Rican fishing industry. ADELA formed in December 1999 to oppose the actions of HCRH.\n\nIn January 2000, ADELA was a party in a lawsuit seeking an injunction and annulment of the HCRH exploration concessions. In September of that year, the Costa Rican Constitutional Court ruled in ADELA's favor, and granted an annulment of the four-block HCRH concession. The Court's ruling was based on International Labor Organization Convention 169, \"Concerning Indigenous and Tribal Peoples in Independent Countries,\" to which Costa Rica is bound. The Court later modified the ruling in November 2000, however, and nullified only those blocks affecting indigenous territory.\n\nIn September 2001, SETENA convened a public hearing in Limón to debate issues concerning oil development. HCHR bused in approximately 100 people who they paid to attend the hearing, yet they were outnumbered by ADELA members. More than 300 people are reported to have supported ADELA's campaign with banners and posters. In addition, marine biologists and environmental organizations who support ADELA gave presentations about various social, economic, and environmental adverse impacts that could result from oil exploration and drilling.\n\nOn February 28, 2002, SETENA rejected HCRH's EIA as not being environmentally or socially viable, and SETENA listed 50 reasons for its decision. HCRH's appeal was rejected by Elizabeth Odio, then-Costa Rican Minister of the Environment.\n\nIn 2005, the Costa Rican President notified HCHR that their contract had been terminated, due to HCHR's failure to obtain an approved EIA pursuant to Costa Rican environmental law.\n"}
{"id": "15818654", "url": "https://en.wikipedia.org/wiki?curid=15818654", "title": "American Association of Woodturners", "text": "American Association of Woodturners\n\nThe American Association of Woodturners (AAW) is the principal organization in the United States supporting the art and craft of woodturning. It is sometimes stylized as American Association of Wood Turners (AAW). Established in 1986 and headquartered in Saint Paul, Minnesota, the organization encompasses more than 15,000 members in the United States and many foreign nations. As of 2013, the AAW was affiliated with nearly 350 local chapters worldwide. In addition to sponsoring an annual national symposium, the AAW provides support to local clubs for outreach and education. The 25th anniversary of the AAW was celebrated in 2011 at the annual symposium held in Saint Paul. Phil McDonald is executive director of the organization.\n\nThe AAW states: \"\"Our purpose is to foster a wider understanding and appreciation of lathe-turning as a traditional and contemporary craft and a form of art among the general public and amateur, part-time, and professional woodturners. This will be accomplished by providing education, information, organization, technical assistance, and publications related to woodturning\".\" \n\nWoodturning, which has experienced exceptional growth and interest since AAW's founding, is a pursuit that goes back 4,000 years in human history – using craft’s most organic material, wood, as its primary medium. Woodturners create utilitarian, artistic, and sculptural wooden objects on mechanical lathes. The craft differs from most other forms of woodworking in that the wood stock rotates rapidly while sharpened cutting skews, gouges, and other tools are maneuvered by hand to shape the material.\n\nMembership in AAW brings a variety of benefits, including a bimonthly magazine, insurance coverage, scholarships, national symposiums, educational opportunities for newcomers and youth, and a website where turners can display and market their woodturnings.\n\nThe AAW was founded at the Arrowmont School of Arts and Crafts in Gatlinburg, Tennessee, and remains closely affiliated with that institution, as well as with the John C. Campbell Folk School in Brasstown, North Carolina, and the Anderson Ranch in Snowmass Village, Colorado.\n\nThe AAW, a 501(c)(3) tax-exempt organization, is administered by a nine-member Board of Directors, all volunteers who serve three-year terms. Three board members are elected by the full membership each year and begin their terms in January. The Board employs an executive director and an editor, and appoints a Board of Advisors along with various committee members serving specific purposes, such as organizing the national symposium and overseeing AAW financial grants to clubs and individuals.\n\nThe AAW magazine, \"American Woodturner\", is published in print and online editions six times a year. Subscriptions to the journal are included with AAW membership, and individual copies are available on newsstands. Members may view all issues online. The publication contains articles aimed at both novice woodturners as well as intermediate craftspersons and professionals.\n\nThe AAW is affiliated with more than 350 local chapters, or clubs, and three \"virtual\" chapters. Local chapters are primarily in the United States but encompass groups in many other nations, including Canada, England, New Zealand, Taiwan, Australia and Japan. Virtual chapters are not geographically based but are organized around specific woodturning techniques. The three virtual chapters include the Ornamental Turners International, Segmented Woodturners, and Pen Turners www.principallypens.com. Ornamental woodturners specialize in the use of an ornamental lathe such as the Rose engine lathe. Segmented woodturning involves joining individual pieces of wood together prior to turning to create intricate patterns and dramatic visual effects. Pen turners specialize in making pens and other writing instruments out of both wood and synthetic materials.\n\nAnnual symposiums have been sponsored and organized by the AAW each year since 1987, alternating in cities around the United States. The first symposium was held in Lexington, Kentucky in 1987. The symposium will be in Kansas City in 2017 and Portland in 2018. The gathering typically includes live presentations, an auction of selected woodturnings, an instant gallery that showcases current woodturning craft, a rotation of how-to and hands-on demonstrations, the world's largest trade show of commercial woodturning vendors, and a youth training center.\n\nThe AAW operates the AAW Gallery of Wood Art www.galleryofwoodart.org in Saint Paul's historic Landmark Center. The gallery features changing exhibits of art of all kinds made from wood, and there also is a gift shop. Admission is free to the gallery, which is open daily except Monday and Saturday.\nThe AAW works in partnership with Collectors of Wood Art to promote the development and appreciation of studio wood art among collectors, artists, educators, art critics, galleries, museums, and the general public.\n\n"}
{"id": "518397", "url": "https://en.wikipedia.org/wiki?curid=518397", "title": "Angle of repose", "text": "Angle of repose\n\nThe angle of repose, or critical angle of repose, of a granular material is the steepest angle of descent or dip relative to the horizontal plane to which a material can be piled without slumping. At this angle, the material on the slope face is on the verge of sliding. The angle of repose can range from 0° to 90°. The morphology of the material affects the angle of repose; smooth, rounded sand grains cannot be piled as steeply as can rough, interlocking sands. The angle of repose can also be affected by additions of solvents. If a small amount of water is able to bridge the gaps between particles, electrostatic attraction of the water to mineral surfaces will increase the angle of repose, and related quantities such as the soil strength.\n\nWhen bulk granular materials are poured onto a horizontal surface, a conical pile will form. The internal angle between the surface of the pile and the horizontal surface is known as the angle of repose and is related to the density, surface area and shapes of the particles, and the coefficient of friction of the material. Material with a low angle of repose forms flatter piles than material with a high angle of repose.\n\nThe term has a related usage in mechanics, where it refers to the maximum angle at which an object can rest on an inclined plane without sliding down. This angle is equal to the arctangent of the coefficient of static friction \"μ\" between the surfaces.\n\nThe angle of repose is sometimes used in the design of equipment for the processing of particulate solids. For example, it may be used to design an appropriate hopper or silo to store the material, or to size a conveyor belt for transporting the material. It can also be used in determining whether or not a slope (of a stockpile, or uncompacted gravel bank, for example) will likely collapse; the talus slope is derived from angle of repose and represents the steepest slope a pile of granular material will take. This angle of repose is also crucial in correctly calculating stability in vessels.\n\nIt is also commonly used by mountaineers as a factor in analysing avalanche danger in mountainous areas.\n\nThere are numerous methods for measuring angle of repose and each produces slightly different results. Results are also sensitive to the exact methodology of the experimenter. As a result, data from different labs are not always comparable. One method is the triaxial shear test, another is the direct shear test.\n\nIf the coefficient of static friction is known of a material, then a good approximation of the angle of repose can be made with the following function. This function is somewhat accurate for piles where individual objects in the pile are minuscule and piled in random order.\n\nwhere, \"μ\" is the coefficient of static friction, and \"θ\" is the angle of repose.\n\nThe measured angle of repose may vary with the method used.\n\nThis method is appropriate for fine-grained, non-cohesive materials with individual particle size less than 10 mm. The material is placed within a box with a transparent side to observe the granular test material. It should initially be level and parallel to the base of the box. The box is slowly tilted until the material begins to slide in bulk, and the angle of the tilt is measured.\n\nThe material is poured through a funnel to form a cone. The tip of the funnel should be held close to the growing cone and slowly raised as the pile grows, to minimize the impact of falling particles. Stop pouring the material when the pile reaches a predetermined height or the base a predetermined width. Rather than attempt to measure the angle of the resulting cone directly, divide the height by half the width of the base of the cone. The inverse tangent of this ratio is the angle of repose.\n\nThe material is placed within a cylinder with at least one transparent end. The cylinder is rotated at a fixed speed and the observer watches the material moving within the rotating cylinder. The effect is similar to watching clothes tumble over one another in a slowly rotating clothes dryer. The granular material will assume a certain angle as it flows within the rotating cylinder. This method is recommended for obtaining the dynamic angle of repose, and may vary from the static angle of repose measured by other methods.\n\nHere is a list of various materials and their angle of repose. All measurements are approximated.\n\nDifferent supports will modify the shape of the pile, in the illustrations below sand piles, though angles of repose remain the same.\n\nThe larvae of the antlions and the unrelated wormlions Vermileonidae trap small insects such as ants by digging conical pits in loose sand, such that the slope of the walls is effectively at the critical angle of repose for the sand. They achieve this by flinging the loose sand out of the pit and permitting the sand to settle at its critical angle of repose as it falls back. Thus, when a small insect, commonly an ant, blunders into the pit, its weight causes the sand to collapse below it, drawing the victim toward the center where the predator that dug the pit lies in wait under a thin layer of loose sand. The larva assists this process by vigorously flicking sand out from the center of the pit when it detects a disturbance. This undermines the pit walls and causes them to collapse toward the center. The sand that the larva flings also pelts the prey with so much loose, rolling material as to prevent it from getting any foothold on the easier slopes that the initial collapse of the slope has presented. The combined effect is to bring the prey down to within grasp of the larva, which then can inject venom and digestive fluids.\n\nThe angle of repose plays a part in several topics of technology and science, including:\n"}
{"id": "4495", "url": "https://en.wikipedia.org/wiki?curid=4495", "title": "British thermal unit", "text": "British thermal unit\n\nThe British thermal unit (Btu or BTU) is a traditional unit of heat; it is defined as the amount of heat required to raise the temperature of one pound of water by one degree Fahrenheit. It is part of the United States customary units. Its counterpart in the metric system is the calorie, which is defined as the amount of heat required to raise the temperature of one gram of water by one degree Celsius. Heat is now known to be equivalent to energy, for which the SI unit is the joule; one BTU is about 1055 joules. While units of heat are often supplanted by energy units in scientific work, they are still important in many fields. As examples, in the United States the price of natural gas is quoted in dollars per million BTUs.\n\nA BTU was originally defined as the amount of heat required to raise the temperature of 1 avoirdupois pound of liquid water by 1 degree Fahrenheit at a constant pressure of one atmosphere. There are several different definitions of the BTU that are now known to differ slightly. This reflects the fact that the temperature change of a mass of water due to the addition of a specific amount of heat (calculated in energy units, usually joules) depends slightly upon the water's initial temperature. As seen in the table below, definitions of the BTU based on different water temperatures vary by up to 0.5%. In the table, thermochemical and steam table (IT) values, which are now defined in terms of exact values in joules, have been rounded to four decimal places.\n\nUnits kBtu are used in building energy use tracking and heating system sizing. Energy Use Index (EUI) represents kBtu per square foot of conditioned floor area. \"k\" stands for 1,000.\n\nThe units MBtu and MMBtu are used in the natural gas and other industries to indicate 1,000 and 1,000,000 BTUs, respectively. There is an ambiguity in that the metric system uses the prefix \"M\" to indicate one million (1,000,000), and \"MBtu\" is also used to indicate one million BTUs. Because of this ambiguity, some authors have deprecated the use of MBtu.\n\nEnergy analysts accustomed to the metric \"k\" for 1,000 are more likely to use MBtu to represent one million, especially in documents where M represents one million in other energy or cost units, such as MW, MWh and $.\n\nThe unit \"therm\" is used to represent 100,000 (or 10) BTUs. A decatherm is 10 therms or one MBtu. The unit \"quad\" is commonly used to represent one quadrillion (10) BTUs.\n\nOne Btu is approximately:\n\nA Btu can be approximated as the heat produced by burning a single wooden kitchen match or as the amount of energy it takes to lift a weight .\n\n\nWhen used as a unit of power for heating and cooling systems, Btu \"per hour\" (Btu/h) is the correct unit, though this is often abbreviated to just \"Btu\". \"MBH\"—thousands of Btus per hour—is also common.\n\n\n\nThe Btu should not be confused with the Board of Trade Unit (B.O.T.U.), which is a much larger quantity of energy ().\n\nThe Btu is often used to express the conversion-efficiency of heat into electrical energy in power plants. Figures are quoted in terms of the quantity of heat in Btu required to generate 1 kW⋅h of electrical energy. A typical coal-fired power plant works at  Btu/kW⋅h, an efficiency of 32–33%.\n\nThe centigrade heat unit (CHU) is the amount of heat required to raise the temperature of one pound of water by one degree Celsius. It is equal to 1.8 BTU or 1899 joules. This unit was sometimes used in the United Kingdom as an alternative to BTU but is now obsolete.\n\n\n"}
{"id": "19335893", "url": "https://en.wikipedia.org/wiki?curid=19335893", "title": "CAIFI", "text": "CAIFI\n\nThe Customer Average Interruption Frequency Index (CAIFI) is a popular index used in electrical reliability analysis. It is designed to show trends in customers interrupted and helps to show the number of customers affected out of the whole customer base.\n\nformula_1\n\n"}
{"id": "6356725", "url": "https://en.wikipedia.org/wiki?curid=6356725", "title": "Chesapeake Bay Foundation", "text": "Chesapeake Bay Foundation\n\nThe Chesapeake Bay Foundation (CBF) is a non-profit organization devoted to the restoration and protection of the Chesapeake Bay in the United States. It was founded in 1967 and has headquarters offices in Annapolis, Maryland. The foundation has field offices in Salisbury, Maryland; Harrisburg, Pennsylvania; Richmond, Virginia; Norfolk, Virginia and Washington, D.C.\n\nCBF offers an outdoor education program that has introduced several generations of school children to the Chesapeake Bay through several idyllic outposts along the Bay's shores, such as Fox Island, Smith Island, Bishop's Head and others. Children learn the fragile nature of the Bay's ecosystem, and the extent of its watershed, much of which includes their own homes in suburbia. CBF also lobbies state and local governments on regulations intended to protect the health of the Bay.\n\nAlong with education and advocacy, the CBF also moves to make the Bay cleaner through restoration and litigation. Their mission is to restore the Bay to balance in environmental programs such as planting trees and other greenery, along with restoring oyster populations. In litigation, the CBF makes it their mission to hold environmentally negligent companies and organizations accountable for their actions.\n\nIn 2001, CBF moved from a walkable downtown location in Annapolis to a new headquarters building, the Philip Merrill Environmental Center, about 5 miles (8 km) outside of town. The new building, at the former site of the Bay Ridge Inn on the western shore of the Chesapeake Bay, is a green building that demonstrates a number of energy-saving and other sustainable features. It was the first building to receive the Leadership in Energy and Environmental Design (LEED) \"Platinum\" rating from the United States Green Building Council.\n\nThe new headquarters is not accessible by public transportation. The foundation's choice for a new headquarters site symbolizes a dilemma of the modern environmental movement: how to be connected to the environment without despoiling it. In this case, the enlarged footprint of employees and visitors forced to drive to the building was offset by its reduced imperviousness compared to the former inn, use of recycled materials, re-use of wastewater on-site, and use of composting toilets. The building was an early adopter of green building principles, but apart from automobiles and bicycles, remains inaccessible by other modes of transportation.\n\n\n"}
{"id": "56073390", "url": "https://en.wikipedia.org/wiki?curid=56073390", "title": "Combustion instability", "text": "Combustion instability\n\nCombustion instabilities are physical phenomena occurring in a reacting flow (e.g., a flame) in which some perturbations, even very small ones, grow and then become large enough to alter the features of the flow in some particular way .\n\nIn many practical cases, the appearance of combustion instabilities is undesirable. For instance, thermoacoustic instabilities are a major hazard to gas turbines and rocket engines. Moreover, flame blowoff of an aero-gas-turbine engine in mid-flight is clearly dangerous (see flameout). \n\nBecause of these hazards, the engineering design process of engines involves the determination of a stability map (see figure). This process identifies a combustion-instability region and attempts to either eliminate this region or moved the operating region away from it. This is a very costly iterative process. For example, the numerous tests required to develop rocket engines are largely in part due to the need to eliminate or reduce the impact of thermoacoustic combustion instabilities.\n\nIn applications directed towards engines, combustion instability has been classified into three categories, not entirely distinct. This classification was first introduced by Marcel Barrère and Forman A. Williams in 1969. The three categories are\n\n\nIn this type of instabilities the perturbations that grow and alter the features of the flow are of an acoustics nature. Their associated pressure oscillations can have well defined frequencies with amplitudes high enough to pose a serious hazard to combustion systems. For example, in rocket engines, such as the Rocketdyne F-1 rocket engine in the Saturn V program, instabilities can lead to massive damage of the combustion chamber and surrounding components (see rocket engines). Furthermore, instabilities are known to destroy gas-turbine-engine components during testing. They represent a hazard to any type of combustion system.\n\nThermoacoustic combustion instabilities can be explained by distinguishing the following physical processes:\n\nthe feedback between heat-release fluctuations (or flame fluctuations) with the combustor or combustion chamber acoustics; the coupling of these two processes in space-time; the strength of this coupling in comparison with acoustic losses; and, the physical mechanisms behind the heat-release fluctuations.\n\nThe simplest example of a thermoacoustic combustion instability is perhaps that happening in a horizontal Rijke tube (see also thermoacoustics): Consider the flow through a horizontal tube open at both ends, in which a flat flame sits at a distance of one-quarter the tube length from the leftmost end. In a similar way to an organ pipe, acoustic waves travel up and down the tube producing a particular pattern of standing waves. Such a pattern also forms in actual combustors, but takes a more complex form. The acoustic waves perturb the flame. In turn, the flame affects the acoustics. This feedback between the acoustic waves in the combustor and the heat-release fluctuations from the flame is a hallmark of thermoacoustic combustion instabilities. It is typically represented with a block diagram (see figure). Under some conditions, the perturbations will grow and then saturate, producing a particular noise. In fact, it is said that the flame of a Rijke tube sings.\n\nThe conditions under which perturbations will grow are given by Rayleigh's (John William Strutt, 3rd Baron Rayleigh) criterion: Thermoacoustic combustion instabilities will occur if the volume integral of the correlation of pressure and heat-release fluctuations over the whole tube is larger than zero (see also thermoacoustics). In other words, instabilities will happen if heat-release fluctuations are coupled with acoustical pressure fluctuations in space-time (see figure). However, this condition is not sufficient for the instability to occur.\n\nAnother necessary condition for the establishment of a combustion instability is that the driving of the instability from the above coupling must be larger than the sum of the acoustic losses. These losses happen through the tube's boundaries, or are due to viscous dissipation. \n\nCombining the above two conditions, and for simplicity assuming here small fluctuations and an inviscid flow, leads to the extended Rayleigh's criterion. Mathematically, this criterion is given by the next inequality:\nHere p' represents pressure fluctuations, q' heat release fluctuations, formula_1 velocity fluctuations, T is a long enough time interval, V denotes volume, S surface, and formula_2 is a normal to the surface boundaries. The left hand side denotes the coupling between heat-release fluctuations and acoustic pressure fluctuations, and the right hand side represents the loss of acoustic energy at the tube boundaries. \n\nGraphically, for a particular combustor, the extended Rayleigh's criterion is represented in the figure on the right as a function of frequency. The left hand side of the above inequality is called gains, and the right hand side losses. Notice that there is a region where the gains exceeds the losses. In other words, the above inequality is satisfied. Furthermore, note that in this region the response of the combustor to acoustic fluctuations peaks. Thus, the likelihood of a combustion instability in this region is high, making it a region to avoid in the operation of the combustor. This graphical representation of a hypothetical combustor allows to group three methods to prevent combustion instabilities : increase the losses; reduce the gains; or move the combustor's peak response away from the region where gains exceed losses.\n\nTo clarify further the role of the coupling between heat-release fluctuations and pressure fluctuations in producing and driving an instability, it is useful to make a comparison with the operation of an internal combustion engine (ICE). In a ICE, a higher thermal efficiency is achieved by releasing the heat via combustion at a higher pressure. Likewise, a stronger driving of a combustion instability happens when the heat is released at a higher pressure. But while high heat release and high pressure coincide (roughly) throughout the combustion chamber in an ICE, they coincide at a particular region or regions during a combustion instability. Furthermore, whereas in an ICE the high pressure is achieved through mechanical compression with a piston or a compressor, in a combustion instability high pressure regions form when a standing acoustic wave is formed.\n\nThe physical mechanisms producing the above heat-release fluctuations are numerous. Nonetheless, they can be roughly divided into three groups: heat-release fluctuations due to mixture inhomogeneities; those due to hydrodynamic instabilities; and, those due to static combustion instabilities.\nTo picture heat-release fluctuations due to mixture inhomogeneities, consider a pulsating stream of gaseous fuel upstream of a flame-holder.\n\nSuch a pulsating stream may well be produced by acoustic oscillations in the combustion chamber that are coupled with the fuel-feed system. Many other causes are possible. The fuel mixes with the ambient air in a way that an inhomogeneous mixture reaches the flame, e.g., the blobs of fuel-and-air that reach the flame could alternate between rich and lean. As a result, heat-release fluctuations occur. \nHeat-release fluctuations produced by hydrodynamic instabilities happen, for example, in bluff-body-stabilized combustors when vortices interact with the flame (see previous figure) .\n\nLastly, heat-release fluctuations due to static instabilities are related to the mechanisms explained in the next section.\n\nStatic instability or flame blow-off refer to phenomena involving the interaction between the chemical composition of the fuel-oxidizer mixture and the flow environment of the flame . To explain these phenomena, consider a flame that is stabilized with swirl, as in a gas-turbine combustor, or with a bluff-body. Moreover, say that the chemical composition and flow conditions are such that the flame is burning vigorously, and that the former is set by the fuel-oxidizer ratio (see air-fuel ratio) and the latter by the oncoming velocity. For a fixed oncoming velocity, decreasing the fuel-oxidizer ratio makes the flame change its shape, and by decreasing it further the flame oscillates or moves intermittently. In practice, these are undesirable conditions. Further decreasing the fuel-oxidizer ratio blows-off the flame. This is clearly an operational failure. For a fixed fuel-oxidizer ratio, increasing the oncoming velocity makes the flame behave in a similar way to the one just described.\n\nEven though the processes just described are studied with experiments or with Computational Fluid Dynamics, it is instructive to explain them with a simpler analysis. In this analysis, the interaction of the flame with the flow environment is modeled as a perfectly-mixed chemical reactor . With this model, the governing parameter is the ratio between a flow time-scale (or residence time in the reactor) and a chemical-time scale, and the key observable is the reactor's maximum temperature. The relationship between parameter and observable is given by the so-called S-shape curve (see figure). This curve results from the solution of the governing equations of the reactor model. It has three branches: an upper branch in which the flame is burning vigorously, i.e., it is \"stable\"; a middle branch in which the flame is \"unstable\" (the probability for solutions of the reactor-model equations to be in this unstable branch is small); and a lower branch in which there is no flame but a cold fuel-oxidizer mixture. The decrease of the fuel-oxidizer ratio or increase of oncoming velocity mentioned above correspond to a decrease of the ratio of the flow and chemical time scales. This in turn corresponds to a movement towards the left in the S-shape curve. In this way, a flame that is burning vigorously is represented by the upper branch, and its blow-off is the movement towards the left along this branch towards the quenching point Q. Once this point is passed, the flame enters the middle branch, becoming thus \"unstable\", or blows off. This is how this simple model captures qualitatively the more complex behavior explained in the above example of a swirl or bluff-body-stabilized flame.\n\nIn contrast with thermoacoustic combustion instabilities, where the role of acoustics is dominant, intrinsic flame instabilities refer to instabilities produced by differential and preferential diffusion, thermal expansion, buoyancy, and heat losses. Examples of these instabilities include the Darrieus–Landau instability, the Rayleigh-Taylor instability, and thermal-diffusive instabilities (see Double diffusive convection).\n"}
{"id": "19344840", "url": "https://en.wikipedia.org/wiki?curid=19344840", "title": "Differential tariff", "text": "Differential tariff\n\nDifferential tariff is an example of demand side management where the price per unit of energy varies with the consumption. If a power utility uses differential tariff, it may change the rate per kWH of energy used during different times, such as raising the price during times of high energy consumption and lowering the price during times of low energy consumption. This helps balance the rate at which power is used and the rate at which power is created.\n\nA differential tariff creates a balance in production and consumption of power by utilizing customer demand. For instance, if there is time where energy is being used faster than can be supplied, a differential tariff can be used: raising the price at that specific time to balance load on the system.\n\nTo implement the above-mentioned method of flattening the load curve, this technique is employed. As the load decreases and increases, the supplier must install his equipment in a way which will be capable of supplying the peak consumer load. During low periods, the equipment will be underutilized, thereby decreasing the energy efficiency of the equipment. Therefore, the supplier will try to ensure that the equipment is used at its rated capacity so as not to waste resources, which cost money. With this type of tariff, the consumer will try to consume more energy during the low periods and avoid energy consumption during peak hours because they are more expensive.\n\nSince the 1970s, tariffs such as Economy 7 and 'White Meter' have offered domestic consumers cheaper electricity in off-peak hours, usually for a fixed, continuous period of between 7 and 8.5 hours overnight. Typically such tariffs are used in conjunction with storage heaters, which charge up cheaply overnight and release heat slowly during the following day.\n\nWhile many such installations use a simple timeswitch, households may also be fitted with a radio teleswitch which allows more dynamic switching. For example, the supplier EDF advertises its Economy 7 service in the London area as offering 7 hours of off-peak electricity between midnight and 8.00am; the precise start and finish times can be varied to smooth out peak demand.\n\nBespoke local off-peak tariffs have also been created to help manage demand. For example, the electricity company MANWEB found its supply network in Mid Wales overstretched in the early 1990s due to high peak demand for electric heating overnight. In response, it created a bespoke tariff and used radio teleswitches to stagger off-peak heating times and better distribute loads across the day. Now known as TwinHeat, the tariff saved the company over a million pounds in network reinforcement costs.\n\n"}
{"id": "25868820", "url": "https://en.wikipedia.org/wiki?curid=25868820", "title": "Dimer acid", "text": "Dimer acid\n\nDimer acids, or dimerized fatty acids, are dicarboxylic acids prepared by dimerizing unsaturated fatty acids obtained from tall oil, usually on clay catalysts. The CAS number of the material is [61788-89-4]. Dimer acids are used primarily for synthesis of polyamide resins and polyamide hot melt adhesives. They are also used in alkyd resins, adhesives, surfactants, as fuel oil additives, lubricants, etc. It is a light yellow or yellow viscous transparent liquid. It is non-toxic.\n\nDimer acid usually contains predominantly a dimer of stearic acid. It is also called C36 dimer acid.\n\nTrimer acid is a corresponding material where the resulting molecule consists of three fatty acid molecules. Its CAS number is [68937-90-6].\n\nDimer acids can be converted to dimer amines by reaction with ammonia and subsequent reduction.\n"}
{"id": "1894108", "url": "https://en.wikipedia.org/wiki?curid=1894108", "title": "Dunnite", "text": "Dunnite\n\nDunnite, also known as Explosive D or systematically as ammonium picrate, is an explosive developed in 1906 by US Army Major Beverly W. Dunn, who later served as the chief inspector of the Bureau of Transportation Explosives. Ammonium picrate is a salt formed by reacting picric acid and ammonia. It is chemically related to the more stable explosive trinitrotoluene (TNT).\n\nIt was the first explosive used in an aerial bombing operation in military history, performed by Italian pilots in Libya in 1911.\nIt was used extensively by the United States Navy during World War I.\n\nThough Dunnite was generally considered an insensitive substance, by 1911 the United States Army had abandoned its use in favor of other alternatives. The Navy, however, used it in armor-piercing artillery shells and projectiles, and in coastal defense.\n\nDunnite typically did not detonate on striking heavy armor. Rather, the shell encasing it would penetrate the armor, after which the charge would be triggered by a base fuze.\n\nIn 2008 caches of discarded Dunnite in remote locations were mistaken for rusty rocks at Cape Porcupine, Newfoundland and Labrador, Canada.\n\nDunnite can be used as a precursor to the highly stable explosive TATB (1,3,5-triamino-2,4,6-trinitrobenzene), by first dehydrating it to form picramide (attaching the ammonia as an amine group instead of an ion) and then further aminating it.\n"}
{"id": "31033582", "url": "https://en.wikipedia.org/wiki?curid=31033582", "title": "Edelfosine", "text": "Edelfosine\n\nEdelfosine (ET-18-O-CH3; 1-octadecyl-2-\"O\"-methyl-glycero-3-phosphocholine) is a synthetic alkyl-lysophospholipid (ALP). It has antineoplastic (anti-cancer) effects.\n\nLike all ALPs, it incorporates into the cell membrane and does not target the DNA. In many tumor cells, it causes selective apoptosis, sparing healthy cells. Edelfosine can activate the Fas/CD95 cell death receptor, can inhibit the MAPK/ERK mitogenic pathway and the Akt/protein kinase B (PKB) survival pathway. Aside from these plasma-level effects, edelfosine also affects gene expression by modulating the expression and activity of transcription factors.\n\nIt has immune modulating properties. \nThese characteristics cause edelfosine also to affect HIV, parasitic, and autoimmune diseases. \nIt can complement classic anti-cancer drugs such as cisplatin.\n\nIt can be administered orally, intraperitoneally (IP) and intravenously (IV).\n\nEdelfosine and other ALPs can be used for purging residual leukemic cells from bone marrow transplants.\n\nIt is an analog of miltefosine and perifosine.\n\nEdelfosine apoptosis-inducing abilities were studied with several types of cancer, among them multiple myeloma and non-small and small cell lung carcinoma cell lines. \"In vivo\" activity against human solid tumors in mice was shown against malignant gynecological tumor cells, like ovarian cancer, and against breast cancer. \"In vivo\" biodistribution studies demonstrated a “considerably higher” accumulation of Edelfosine in tumor cells than in other analyzed organs. It remained undergraded for a long time.\n\nSeveral clinical trials were conducted. Among them a phase I trials with solid tumors or leukemias and phase II with non-small-cell lung carcinomas (NSCLC). \nIn a Phase II clinical trial for use of Edelfosine in treating leukemia with bone marrow transplants, it was found to be safe and 'possibly effective'.\nA phase II trial for the treatment of brain cancers was also reported. It showed encouraging results in stopping the growth of the tumor and a considerable improvement in the “quality of life” of the patients.\nA phase II trial on the effect of Edelfosine on advanced non-small-cell bronchogenic carcinoma had a “remarkable” “high proportion of patients with stationary tumor status” as result, stable disease after initial progression in 50% of the patients.\n\nIn animal tests the main toxic effect was gastrointestinal irritation. There were no significant negative systemic side effects observed. It showed that edelfosine can be given over a long period safely. Most important, in contrast to many DNA-directed anti-cancer drugs, no bone marrow toxicity was \"in vivo\" observed. Those findings in animals were confirmed in clinical trials. No mutagenic or cytogenetic effects were observed.\n\nIn the 1960s Herbert Fischer and Paul Gerhard in Freiburg, Germany, found that lysolecitin (2-lysophosphatidylcholine, LPC) increases the phagocytotic activity of macrophages.\nSince LPC had a short half-life, synthetic LPC-analogues were tested by Fischer, Otto Westphal, Hans Ulrich Weltzien and Paul Gerhard Munder. Unexpectedly, some of the substances showed strong anti-tumor activity and among them Edelfosine was the most effective. It is therefore considered to be the prototype of synthetic anti-cancer lipids.\n\n"}
{"id": "37081517", "url": "https://en.wikipedia.org/wiki?curid=37081517", "title": "Energiewende in Germany", "text": "Energiewende in Germany\n\nThe Energiewende (German for energy transition) is the planned transition by Germany to a low carbon, environmentally sound, reliable, and affordable energy supply. \nThe term \"Energiewende\" is regularly used in English language publications without being translated (a loanword).\nThe new system will rely heavily on renewable energy (particularly wind, photovoltaics, and hydroelectricity), energy efficiency, and energy demand management. \nMost if not all existing coal-fired generation will need to be retired. \nThe phase-out of Germany's fleet of nuclear reactors, to be complete by 2022, is a key part of the program.\n\nLegislative support for the \"Energiewende\" was passed in late 2010 and includes greenhouse gas (GHG) reductions of 80–95% by 2050 (relative to 1990) and a renewable energy target of 60% by 2050. \nThese targets are ambitious.\nThe Berlin-based policy institute Agora Energiewende noted that \"while the German approach is not unique worldwide, the speed and scope of the \"Energiewende\" are exceptional\".\nThe \"Energiewende\" also seeks a greater transparency in relation to national energy policy formation.\n\nGermany has made significant progress on its GHG emissions reduction target, achieving a 27% decrease between 1990 and 2014. \nHowever the country will need to maintain an average GHG emissions abatement rate of 3.5% per year to reach its \"Energiewende\" goal, equal to the maximum historical value thus far.\n\nAs of 2013, Germany spends €1.5billion per year on energy research in an effort to solve the technical and social issues raised by the transition.\nThis includes a number of computer studies that have confirmed the feasibility and a similar cost (relative to business-as-usual and given that carbon is adequately priced) of the \"Energiewende\".\n\nThe term \"Energiewende\" was first contained in the title of a 1980 publication by the German Öko-Institut, calling for the complete abandonment of nuclear and petroleum energy.\nThe most groundbreaking claim was that economic growth was possible without increased energy consumption. On 16February 1980, the German Federal Ministry of the Environment also hosted a symposium in Berlin, called \"Energiewende – Atomausstieg und Klimaschutz\" (Energy Transition: Nuclear Phase-Out and Climate Protection). The Institute for Applied Ecology was funded by both environmental and religious organizations, and the importance of religious and conservative figures like Wolf von Fabeck and Peter Ahmels was crucial. In the following decades, the term \"Energiewende\" expanded in scope – in its present form it dates back to at least 2002.\n\n\"Energiewende\" designates a significant change in energy policy. The term encompasses a reorientation of policy from demand to supply and a shift from centralized to distributed generation (for example, producing heat and power in small cogeneration units), which should replace overproduction and avoidable energy consumption with energy-saving measures and increased efficiency.\n\nIn a broader sense, this transition also entails a democratization of energy.\nIn the traditional energy industry, a few large companies with large centralized power stations dominate the market as an oligopoly and consequently amass a worrisome level of both economic and political power. Renewable energies, in contrast, can, as a rule, be established in a decentralized manner. Public wind farms and solar parks can involve many citizens directly in energy production.\nPhotovoltaic systems can even be set up by individuals. Municipal utilities can also benefit citizens financially, while the conventional energy industry profits a relatively small number of shareholders. Also significant, the decentralized structure of renewable energies enables creation of value locally and minimizes capital outflows from a region. Renewable energy sources therefore play an increasingly important role in municipal energy policy, and local governments often promote them.\n\nThe key policy document outlining the \"Energiewende\" was published by the German government in September 2010, some six months before the Fukushima nuclear accident. Legislative support was passed in September 2010. On 6 June 2011, following Fukushima, the government removed the use of nuclear power as a bridging technology as part of their policy.\nAfter the 2013 federal elections, the new CDU/CSU and SPD coalition government continued the \"Energiewende\", with only minor modification of its goals in the coalition agreement. An intermediate target was introduced of a 55–60% share of renewable energy in gross electricity consumption in 2035. Germany imports more than half of its energy. Important aspects include ():\n\nIn addition, there will be an associated research and development drive. A chart showing German energy legislation in 2016 is available.\n\nThese targets go well beyond European Union legislation and the national policies of other European states. The policy objectives have been embraced by the German federal government and has resulted in a huge expansion of renewables, particularly wind power. Germany's share of renewables has increased from around 5% in 1999 to 22.9% in 2012, surpassing the OECD average of 18% usage of renewables.\nProducers have been guaranteed a fixed feed-in tariff for 20 years, guaranteeing a fixed income. Energy co-operatives have been created, and efforts were made to decentralize control and profits. The large energy companies have a disproportionately small share of the renewables market. However, in some cases poor investment designs have caused bankruptcies and low returns, and unrealistic promises have been shown to be far from reality.\nNuclear power plants were closed, and the existing nine plants will close earlier than planned, in 2022.\n\nOne factor that has inhibited efficient employment of new renewable energy has been the lack of an accompanying investment in power infrastructure to bring the power to market. It is believed 8,300 km of power lines must be built or upgraded. The different German States have varying attitudes to the construction of new power lines. Industry has had their rates frozen and so the increased costs of the \"Energiewende\" have been passed on to consumers, who have had rising electricity bills. Germans in 2013 had some of the highest electricity prices (including taxes) in Europe. In comparison, its neighbors (Poland, Sweden, Denmark and nuclear-reliant France) have some of the lowest costs (excluding taxes) in the EU.\n\nAccording to a 2014 survey conducted by TNS Emnid for the German Renewable Energies Agency among 1015 respondents, 94 per cent of the Germans support the enforced expansion of Renewable Energies. More than two-thirds of the interviewees agree to renewable power plants close to their homes.\nThe share of total final energy from renewables was 11% in 2014.\n\nOn 1 August 2014, a revised Renewable Energy Sources Act entered into force. Specific deployment corridors now stipulate the extent to which renewable energy is to be expanded in the future and the funding rates (feed-in tariffs) will no longer be fixed by the government, but will be determined by auction.\n\nMarket redesign is a key part of the \"Energiewende\". The German electricity market needs to be reworked to suit.\nAmong other things, wind and PV cannot be principally refinanced under the current marginal cost based market. Carbon pricing is also central to the \"Energiewende\" and the European Union Emissions Trading Scheme (EU ETS) needs to be reformed to create a genuine scarcity of certificates.\nThe German federal government is calling for such reform.\nMost of the computer scenarios used to analyse the \"Energiewende\" rely on a substantial carbon price to drive the transition to low-carbon technologies.\n\nCoal-fired generation needs to be retired as part of the \"Energiewende\". Some argue for an explicit negotiated phase-out of coal plants, along the lines of the well-publicized nuclear phase-out.\nCoal comprised 42% of electricity generation in 2015. If Germany is to limit its contribution to a global temperature increase to 1.5°C above pre-industrial levels, as declared in the 2015 Paris Agreement, a complete phase-out of fossil fuels together with a shift to 100% renewable energy is required by about 2040.\n\nThe \"Energiewende\" is made up of various technical building blocks. Electricity storage, while too expensive at present, may become a useful technology in the future.\nEnergy efficiency has a key but currently under-recognised role to play.\nImproved energy efficiency is one of Germany's official targets. Greater integration with adjoining national electricity networks can offer mutual benefits — indeed, systems with high shares of renewables can utilize geographical diversity to offset intermittency.\n\nGermany invested €1.5billion in energy research in 2013.\nOf that the German federal government spent €820million supporting projects ranging from basic research to applications. The federal government also foresees an export role for German expertise in the area.\n\nThe social and political dimensions of the \"Energiewende\" have been subject to study. Strunz argues that the underlying technological, political and economic structures will need to change radically — a process he calls regime shift.\nSchmid, Knopf, and Pechan analyse the actors and institutions that will be decisive in the \"Energiewende\" and how latency in the national electricity infrastructure may restrict progress.\n\nOn 3 December 2014, the German federal government released its National Action Plan on Energy Efficiency (NAPE) in order to improve the uptake of energy efficiency.\nThe areas covered are the energy efficiency of buildings, energy conservation for companies, consumer energy efficiency, and transport energy efficiency. German industry is expected to make a sizeable contribution.\n\nAn official federal government report on progress under the \"Energiewende\", updated for 2014, notes that:\n\n\nA commentary on the progress report expands on many of the issues raised.\n\nSlow progress on transmission network reinforcement has led to a deferment of new windfarms in northern Germany. The German cabinet earlier approved costly underground cabling in October 2015 in a bid to dispel local resistance against above-ground pylons and to speed up the expansion process.\n\nAnalysis by Agora Energiewende in late-2016 suggests that Germany will probably miss several of its key \"Energiewende\" targets, despite recent reforms to the Renewable Energy Sources Act and the wholesale electricity market. The goal to cut emissions by 40% by 2020 \"will most likely be missed... if no further measures are taken\" and the 55–60% share of renewable energy in gross electricity consumption by 2035 is \"unachievable\" with the current plans for renewables expansion. In November 2016, Agora Energiewende reported on the impact of the new and several other related new laws. It concludes that this new legislation will bring \"fundamental changes\" for large sections of the energy industry, but have limited effect on the economy and on consumers.\n\nThe 2016 Climate Action Plan for Germany, adopted on 14November 2016, introduced sector targets for GHG emissions. The goal for the energy sector is shown in the table. The plan states that the energy supply must be \"almost completely decarbonised\" by 2050, with renewables as its main source. For the electricity sector, \"in the long-term, electricity generation must be based almost entirely on renewable energies\" and \"the share of wind and solar power in total electricity production will rise significantly\". Notwithstanding, during the transition, \"less carbon-intensive natural gas power plants and the existing most modern coal power plants play an important role as interim technologies\".\n\nThe fifth monitoring report on the \"Energiewende\" for 2015 was published in December 2016. The expert commission which wrote the report warns that Germany will probably miss its 2020 climate targets and believes that this could threaten the credibility of the entire endeavor. The commission puts forward a number of measures to address the slowdown, including a flat national price imposed across all sectors, a greater focus on transport, and full market exposure for renewable generation. Regarding the carbon price, the commission thinks that a reformed EUETS would be better, but that achieving agreement across Europe is unlikely.\n\n, citizen support for the \"Energiewende\" remains high, with recent surveys indicating that about 80–90% of the public are in favor. \nOne reason for the high acceptance is the substantial participation of German citizens in the \"Energiewende\", as private households, land owners, or members of energy cooperatives (\"Genossenschaft\"). \nA 2016 survey showed that roughly one in two Germans would consider investing in community renewable energy projects.\nManfred Fischedick, Director of the Wuppertal Institute for Climate, Environment and Energy has commented that \"if people participate with their own money, for example in a wind or solar power plant in their area, they will also support [the \"Energiewende\"].\" \nA 2010 study shows the benefits to municipalities of community ownership of renewable generation in their locality.\nEstimates for 2012 suggested that almost half the renewable energy capacity in Germany was owned by citizens through energy cooperatives and private initiatives.\nMore specifically, citizens accounted for nearly half of all installed biogas and solar capacity and half of the installed onshore wind capacity. However, changes in energy policy, starting with the Renewable Energy Sources Act in 2014, have jeopardized the efforts of citizens to participate. The share of citizen-owned renewable energy has since dropped to 42.5% as of 2016.\n\nMuch of the policy development for the \"Energiewende\" is underpinned by computer models, run mostly by universities and research institutes. The models are usually based on scenario analysis and are used to investigate different assumptions regarding the stability, sustainability, cost, efficiency, and public acceptability of various sets of technologies. Some models cover the entire energy sector, while others are confined to electricity generation and consumption. A 2016 book investigates the usefulness and limitations of energy scenarios and energy models within the context of the \"Energiewende\".\n\nA number of computer studies confirm the feasibility of the German electricity system being 100% renewable in 2050. Some investigate the prospect of the entire energy system (all energy carriers) being fully renewable too.\n\nIn 2009 WWF Germany published a quantitative study prepared by the Öko-Institut, Prognos, and Hans-Joachim Ziesing. The study presumes a 95% reduction in greenhouse gases by the year 2050 and covers all sectors. The study shows that the transformation from a high-carbon to a low-carbon economy is possible and affordable. It notes that by committing to this transformation path, Germany could become a model for other countries.\n\nA 2011 report from the (SRU) concludes that Germany can attain 100% renewable electricity generation by 2050. The German Aerospace Center (DLR) REMix high-resolution energy model was used for the analysis. A range of scenarios were investigated and a cost-competitive transition with good security of supply is possible.\n\nThe authors presume that the transmission network will continue to be reinforced and that cooperation with Norway and Sweden would allow their hydro generation to be utilized for storage. The transition does not require Germany's nuclear phase-out (\"\") to be extended nor the construction of coal-fired plants with carbon capture and storage (CCS). Conventional generation assets need not be stranded and an orderly transition should prevail. Stringent energy efficiency and energy saving programs can bring down the future costs of electricity.\n\nThe Deep Decarbonization Pathways Project (DDPP) aims to demonstrate how countries can transform their energy systems by 2050 in order to achieve a low-carbon economy.\nThe 2015 German country report, produced in association with the Wuppertal Institute, examines the official target of reducing domestic GHG emissions by 80% to 95% by 2050 (compared with 1990). Decarbonization pathways for Germany are illustrated by means of three ambitious scenarios with energy-related emission reductions between 1990 and 2050 varying between 80% and more than 90%. Three strategies strongly contribute to GHG emission reduction: \nIn addition, some scenarios use controversially:\nPotential co-benefits for Germany include increased energy security, higher competitiveness of and global business opportunities for companies, job creation, stronger GDP growth, smaller energy bills for households, and less air pollution.\n\nUsing the model REMod-D (Renewable Energy Model – Germany), this 2015 Fraunhofer ISE study investigates several system transformation scenarios and their related costs. The guiding question of the study is: how can a cost-optimised transformation of the German energy system — with consideration of all energy carriers and consumer sectors — be achieved while meeting the declared climate protection targets and ensuring a secure energy supply at all times. Carbon capture and storage (CCS) is explicitly excluded from the scenarios. A future energy scenario emitting 85% less CO emissions than 1990 levels is compared with a reference scenario, which assumes that the German energy system operates in 2050 the same way as it does today. Under this comparison, primary energy supply drops 42%. The total cumulative costs depend on the future prices for carbon and oil. If the penalty for CO emissions increases to €100/tonne by 2030 and thereafter remains constant and fossil fuel prices increase annually by 2%, then the total cumulative costs of today's energy system are 8% higher than the costs required for the minus 85% scenario up to 2050. The report also notes:\n\nA 2015 study uses DIETER or Dispatch and Investment Evaluation Tool with Endogenous Renewables, developed by the German Institute for Economic Research (DIW), Berlin, Germany. The study examines the power storage requirements for renewables uptake ranging from 60% to 100%. Under the baseline scenario of 80% (the German government target for 2050), grid storage requirements remain moderate and other options on both the supply side and demand side offer flexibility at low cost. Nonetheless storage plays an important role in the provision of reserves. Storage becomes more pronounced under higher shares of renewables, but strongly depends on the costs and availability of other flexibility options, particularly on biomass availability. The model is fully described in the study report.\n\nA 2016 acatech-lead study focused on so-called flexibility technologies used to balance the fluctuations inherent in power generation from wind and photovoltaics. Set in 2050, several scenarios use gas power plants to stabilise the backbone of energy system, ensuring supply security during several weeks of low wind and solar radiation. Other scenarios investigate a 100% renewable system and show these to be possible but more costly. Flexible consumption and storage control (demand-side management) in households and the industrial sector is the most cost-efficient means of balancing short-term power fluctuations. Long-term storage systems, based on power-to-X, are only viable if carbon emissions are to be reduced by more than 80%. On the question of costs, the study notes:\n\nThe Atmosphere/Energy Program at Stanford University has developed roadmaps for 139 countries to achieve energy systems powered only by wind, water, and sunlight (WWS) by 2050. In the case of Germany, total end-use energy drops from 375.8 GW for business-as-usual to 260.9 GW under a fully renewable transition. Load shares in 2050 would be: on-shore wind 35%, off-shore wind 17%, wave 0.08%, geothermal 0.01%, hydro-electric 0.87%, tidal 0%, residential PV 6.75%, commercial PV 6.48%, utility PV 33.8%, and concentrating solar power 0%. The study also assess avoided air pollution, eliminated global climate change costs, and net job creation. These co-benefits are substantial.\n\nUsing biomass as a fuel produces air pollution in the form of carbon monoxide, carbon dioxide, NOx (nitrogen oxides), VOCs (volatile organic compounds), particulates and other pollutants. However, since the beginning of the Energiewende the balance of biomass vs other forms of renewable energy has shifted; biomass made up 7.0% of Germany's power generation mix in 2017. Biomass has the potential to be a carbon-neutral fuel because growing biomass absorbs carbon dioxide from the atmosphere and a portion of the carbon absorbed remains in the ground after harvest. Biomass contains less sulfur than coal and so produces much less sulfur dioxide than coal. \n\nAfter introduction of the original Renewable Energy Sources Act in 2000 there was a focus on long term costs, while in later years this has shifted to a focus on short term costs and the \"financial burden\" of the \"Energiewende\" while ignoring environmental externalities of fossil fuels.\nNonetheless, for the first time in more than ten years, electricity prices for household customers fell at the beginning of 2015.\n\nThe renewable energy levy to finance green power investment is added to Germans' electricity unit price. The surcharge (22.1% in 2016 ) pays the state-guaranteed price for renewable energy to producers and is 6.35 cents per kWh in 2016.\n\nA comprehensive study, published in \"Energy Policy\" in 2013, said that the phase-out of Germany's fleet of nuclear reactors, to be complete by 2022, is contradictory to the goal of the climate portion of the program. The Intergovernmental Panel on Climate Change (IPCC) recognizes nuclear as one of the lowest lifecycle emissions energy sources available, lower than even solar, and only bested (slightly) by wind. The US National Renewable Energy Lab (NREL) also cites nuclear as a very low lifecycle emissions source \n\nGerman Economy and Energy Minister Sigmar Gabriel admitted \"For a country like Germany with a strong industrial base, exiting nuclear and coal-fired power generation at the same time would not be possible.\"\nGermany's emissions were escalating in 2012 and 2013 and it is planned to reopen some of the dirtiest brown coal mines that had previously been closed. Coal generated electricity increased to 45% in 2013, the highest level since 2007.\nNonetheless, in 2014 carbon emissions had declined again. More renewable energy had been generated and a greater energy efficiency had been achieved. From 1999 to 2014 renewable energy production rose from 29 TWh to 161 TWh, while nuclear power fell from 180 to 97 TWh and coal power production fell from 291 to 265 TWh.\n\n\n"}
{"id": "18518247", "url": "https://en.wikipedia.org/wiki?curid=18518247", "title": "Evaristo Nugkuag", "text": "Evaristo Nugkuag\n\nEvaristo Nugkuag (born 1950) is an activist for environmental and indigenous people causes. He is a member of the Aguaruna. He organized the \"Alliance of the Indian Peoples of the Peruvian Amazon\" (AIDESEP) and \"Coordinadora de las Organizaciones Indígenas de la Cuenca Amazónica\" \"COICA\" to serve indigenous people.\n\n"}
{"id": "1777422", "url": "https://en.wikipedia.org/wiki?curid=1777422", "title": "Faraday cup", "text": "Faraday cup\n\nA Faraday cup is a metal (conductive) cup designed to catch charged particles in vacuum. The resulting current can be measured and used to determine the number of ions or electrons hitting the cup. The Faraday cup is named after Michael Faraday who first theorized ions around 1830.\n\nWhen a beam or packet of ions hits the metal it gains a small net charge while the ions are neutralized. The metal can then be discharged to measure a small current proportional to the number of impinging ions. Essentially the Faraday cup is part of a circuit where ions are the charge carriers in vacuum and the Faraday cup is the interface to the solid metal where electrons act as the charge carriers (as in most circuits). By measuring the electric current (the number of electrons flowing through the circuit per second) in the metal part of the circuit the number of charges being carried by the ions in the vacuum part of the circuit can be determined. For a continuous beam of ions (each with a single charge)\n\nwhere N is the number of ions observed in a time t (in seconds), I is the measured current (in amperes) and e is the elementary charge (about 1.60 × 10 C). Thus, a measured current of one nanoamp (10 A) corresponds to about 6 billion ions striking the faraday cup each second.\n\nSimilarly, a Faraday cup can act as a collector for electrons in a vacuum (for instance from an electron beam). In this case electrons simply hit the metal plate/cup and a current is produced. Faraday cups are not as sensitive as electron multiplier detectors, but are highly regarded for accuracy because of the direct relation between the measured current and number of ions.This device is considered a universal charge detector because of its independence from the energy, mass, chemistry, etc. of the analyte.\n\nThe Faraday cup utilizes a physical principle according to which the electrical charges delivered to the inner surface of a hollow conductor are redistributed around its outer surface due to mutual self-repelling of charges of the same sign – a phenomenon discovered by Faraday.\n\nThe conventional Faraday cup is applied for measurements of ion (or electron) flows from plasma boundaries and comprises a metallic cylindrical receiver-cup – 1 (Fig. 1) closed with, and insulated from, a washer-type metallic electron-suppressor lid – 2 provided with the round axial through enter-hollow of an aperture with a surface area formula_2. Both the receiver cup and the electron-suppressor lid are enveloped in, and insulated from, a grounded cylindrical shield – 3 having an axial round hole coinciding with the hole in the electron-suppressor lid – 2. The electron-suppressor lid is connected by 50 Ω RF cable with the source formula_3 of variable DC voltage formula_4. The receiver-cup is connected by 50 Ω RF cable through the load resistor formula_5 with a sweep generator producing saw-type pulses formula_6. Electric capacity formula_7 is formed of the capacity of the receiver-cup – 1 to the grounded shield – 3 and the capacity of the RF cable. The signal from formula_5 enables an observer to acquire an I-V characteristic of the Faraday cup by oscilloscope. Proper operating conditions: formula_9 (due to possible potential sag) and formula_10, where formula_11 is the ion free path. Signal from formula_5 is the Faraday cup I-V characteristic which can be observed and memorized by oscilloscope\n\nformula_13. (1)\n\nIn Fig. 1: 1 – cup-receiver, metal (stainless steel). 2 – electron-suppressor lid, metal (stainless steel). 3 – grounded shield, metal (stainless steel). 4 – insulator (teflon, ceramic). formula_7 – capacity of Faraday cup. formula_5 – load resistor.\n\nThus we measure the sum formula_16 of the electric currents through the load resistor formula_5: formula_18 (Faraday cup current) plus the current formula_19 induced through the capacitor formula_7 by the saw-type voltage formula_21of the sweep-generator:\nThe current component formula_22 can be measured at the absence of the ion flow and can be subtracted further from the total current formula_23 measured with plasma to obtain the actual Faraday cup I-V characteristic formula_24 for processing. \nAll of the Faraday cup elements and their assembly that interact with plasma are fabricated usually of temperature-resistant materials (often these are stainless steel and teflon or ceramic for insulators). \nFor processing of the Faraday cup I-V characteristic, we are going to assume that the Faraday cup is installed far enough away from an investigated plasma source where the flow of ions could be considered as the flow of particles with parallel velocities directed exactly along the Faraday cup axis. In this case, the elementary particle current formula_25 corresponding to the ion density differential formula_26 in the range of velocities between formula_27 and formula_28 of ions flowing in through operating aperture formula_29 of the electron-suppressor can be written in the form\n\nformula_30, (2)\n\nwhere\n\nformula_31, (3)\n\nformula_32 is elementary electric charge, formula_33 is the ion charge state, and formula_34 is the one-dimensional distribution function of ions over velocity formula_27. Therefore, the ion current at the ion-decelerating voltage formula_21 of the Faraday cup can be calculated by integrating Eq. (2) after substituting in it Eq. (3)\n\nformula_37, (4)\n\nwhere the lower integration limit is defined from the obvious equation formula_38 where formula_39 is the velocity of the ion stopped by the decelerating potential formula_21, and formula_41 is the ion mass. Thus the expression (4) represents the I-V characteristic of the Faraday cup. \nDifferentiating Eq. (4) with respect to formula_21, one can obtain the relation\n\nformula_43, (5)\n\nwhere the value formula_44 is an invariable constant for each measurement. Therefore, the average velocity formula_45 of ions arriving into the Faraday cup and their average energy formula_46 can be calculated (under the assumption that we operate with a single type of ion) by the expressions\n\nformula_47 [cm/s], (6)\n\nformula_48 [eV], (7)\n\nwhere formula_49 is the ion mass in atomic units. The ion concentration formula_50 in the ion flow at the Faraday cup vicinity can be calculated by the formula\n\nformula_51 (8)\n\nwhich follows from Eq. (4) at formula_52,\n\nformula_53, (9)\n\nand from the conventional condition for distribution function normalizing \nformula_54. (10)\n\nFig. 2 illustrates the I-V characteristic formula_55 and its first derivative formula_56 of the Faraday cup with formula_57 installed at output of the Inductively coupled plasma source powered with RF 13.56 MHz and operating at 6 mTorr of H2. The value of the electron-suppressor voltage (accelerating the ions) was set experimentally at formula_58, near the point of suppression of the secondary electron emission from the inner surface of the Faraday cup.\n\nThe counting of charges collected per unit time is impacted by two error sources: 1) the emission of low-energy secondary electrons from the surface struck by the incident charge and 2) backscattering (~180 degree scattering) of the incident particle, which causes it to leave the collecting surface, at least temporarily. Especially with electrons, it is fundamentally impossible to distinguish between a fresh new incident electron and one that has been backscattered or even a fast secondary electron.\n\n\n"}
{"id": "662267", "url": "https://en.wikipedia.org/wiki?curid=662267", "title": "Foundry model", "text": "Foundry model\n\nIn microelectronics, the foundry model refers to the separation of a semiconductor fabrication plant operation (foundry) from an integrated circuit design operation, into separate companies or business units.\n\nAlthough many companies continue to both design and manufacture integrated circuits (achieving efficiency through vertical integration), these integrated device manufacturers (IDMs) are not alone in the marketplace. Economic forces have led to the existence of many companies that only design devices, known as fabless semiconductor companies, as well as merchant foundries that only manufacture devices under contract by other companies, without designing them.\n\nIntegrated circuit production facilities are expensive to build and maintain. Unless they can be kept at nearly full utilization, they will become a drain on the finances of the company that owns them. The foundry model uses two methods to avoid these costs: fabless companies avoid costs by not owning such facilities. Merchant foundries, on the other hand, find work from the worldwide pool of fabless companies, and by careful scheduling, pricing, and contracting keep their plants at full utilization.\n\nOriginally, microelectronic devices were manufactured by companies that both designed and produced the devices. This was necessary because manufacturing involved tweaking parameters, precise understanding of the manufacturing processes being used, and the occasional need to redesign. These manufacturers were involved in both the research and development of manufacturing processes and the research and development of microcircuit design.\n\nHowever, as manufacturing techniques developed, microelectronic devices became more standardised allowing them to be used by more than a single manufacturer. This standardization allowed design to be split from manufacture. A design that obeyed the appropriate design rules could be manufactured by different companies that had compatible manufacturing methods. An important development that allowed this was the introduction of advances in electronic design automation (EDA), which allowed circuit designers to exchange design data with other designers using different foundries.\n\nBecause of the separation of manufacture and design, new types of companies were founded. One type of company is called a \"fabless\" semiconductor company. These companies do not have any semiconductor manufacturing capability but rather contract production from a manufacturer. This manufacturer is called a \"merchant foundry\". The fabless company concentrates on the research and development of an IC-product; the foundry concentrates on fabricating and testing the physical product. If the foundry does not have any semiconductor design capability, it is called a \"pure-play semiconductor foundry\".\n\nAn absolute separation into fabless and foundry companies is not necessary. Some companies continue to exist which perform both operations and benefit from the close coupling of their skills. Some companies manufacture some of their own designs and contract out to have others manufactured or designed, in cases where they see value or seek special skills. The foundry model is a business vision that seeks to optimize productivity.\n\nThe very first merchant foundries were part of the MOSIS service. The MOSIS service gave limited production-access to designers with limited means, such as students, researchers at universities, and engineers at small startups. The designer submitted designs and these submissions were manufactured with the commercial company's extra capacity. Manufacturers could insert some wafers for a MOSIS design into a collection of their own wafers when a processing step was compatible with both operations. The commercial company (serving as foundry) was already running the process, so they were effectively being paid by MOSIS for something they were already doing. A factory with excess capacity during slow periods could also run MOSIS designs in order to avoid having expensive capital equipment standing idle.\n\nUnder-utilization of an expensive manufacturing plant could lead to the financial ruin of the owner, so selling surplus wafer capacity was a way to maximize the fab's utilization. Hence, economic factors created a climate where fab operators wanted to sell surplus wafer-manufacturing capacity, and designers wanted to purchase manufacturing capacity rather than try to build it.\n\nAlthough MOSIS opened the doors to some fabless customers, earning additional revenue for the foundry and providing inexpensive service to the customer, running a business around MOSIS production was difficult. The merchant foundries sold wafer capacity on a surplus basis, as a secondary business activity. Services to the customers were secondary to the commercial business, with little guarantee of support. The choice of merchant dictated the design, development flow, and available techniques to the fabless customer. Merchant foundries might require proprietary and non-portable preparation steps. Foundries concerned with protecting what they considered trade secrets of their methodologies might only be willing to release data to designers after an onerous nondisclosure procedure.\n\nIn 1987, the world's first dedicated merchant foundry opened its doors: \"Taiwan Semiconductor Manufacturing Company (TSMC)\". The distinction of 'dedicated' is in reference to the typical merchant foundry of the era, whose primary business activity was building and selling of its own IC-products. The dedicated foundry offers several key advantages to its customers: first, it does not sell finished IC-products into the supply channel; thus a dedicated foundry will never compete directly with its fabless customers (obviating a common concern of fabless companies). Second, the dedicated foundry can scale production capacity to a customer's needs, offering low-quantity shuttle services in addition to full-scale production lines. Finally, the dedicated foundry offers a \"COT-flow\" (customer owned tooling) based on industry-standard EDA systems, whereas many IDM merchants required its customers to use proprietary (non-portable) development tools. The COT advantage gave the customer complete control over the design process, from concept to final design.\n\n\nAs of 2009, the top 17 semiconductor foundries were:\n\nAs of 2004, the top 10 pure-play semiconductor foundries were: \n\n<div align=\"left\">\nLike all industries, the semiconductor industry faces upcoming challenges and obstacles.\n\nThe cost to stay on the leading edge has steadily increased with each generation of chips. The financial strain is being felt by both large merchant foundries and their fabless customers. The cost of a new foundry exceeds $1 billion. These costs must be passed on to customers. Many merchant foundries have entered into joint ventures with their competitors in an effort to split research and design expenditures and fab-maintenance expenses.\n\nChip design companies sometimes avoid other companies' patents simply by purchasing the products from a licensed foundry with broad cross-license agreements with the patent owner.\n\nStolen design data is also a concern; data is rarely directly copied, because blatant copies are easily identified by distinctive features in the chip,\nplaced there either for this purpose or as a byproduct of the design process. However, the data including any procedure, process system, method of operation or concept may be sold to a competitor, who may save months or years of tedious reverse engineering.\n\n"}
{"id": "2831867", "url": "https://en.wikipedia.org/wiki?curid=2831867", "title": "Helium mass spectrometer", "text": "Helium mass spectrometer\n\nA helium mass spectrometer is an instrument commonly used to detect and locate small leaks. It was initially developed in the Manhattan Project during World War II to find extremely small leaks in the gas diffusion process of uranium enrichment plants. It typically uses a vacuum chamber in which a sealed container filled with helium is placed. Helium leaks out of the container, and the rate of the leak is detected by a mass spectrometer.\n\nHelium is used as a tracer because it penetrates small leaks rapidly. Helium has also the property of being non-toxic, chemically inert and present in the atmosphere only in minute quantities (5 ppm). Typically a helium leak detector will be used to measure leaks in the range of 10 to 10 Pa·m·s.\n\nA flow of 10 Pa·m·s is slightly less than 1 ml per minute at standard conditions for temperature and pressure (STP).\n\nA flow of 10 Pa·m·s is slightly less than 3 ml per century at STP.\n\nTypically there are two types of leaks in the detection of helium as a \"tracer\" for leak detection: residual leak and virtual leak. A residual leak is a real leak due to an imperfect seal, a puncture, or some other hole in the system. A virtual leak is the semblance of a leak in a vacuum system caused by outgassing of chemicals trapped or adhered to the interior of a system that is actually sealed. As the gases are released into the chamber, they can create a false positive indication of a residual leak in the system.\n\nHelium mass spectrometer leak detectors are used in production line industries such as refrigeration and air conditioning, automotive parts, carbonated beverage containers food packages and aerosol packaging, as well as in the manufacture of steam products, gas bottles, fire extinguishers, tire valves, and numerous other products including all vacuum systems.\n\nThis method requires the part to be tested to be connected to a helium leak detector. The outer surface of the part to be tested will be located in some kind of a tent in which the helium concentration will be raised to 100% helium.\n\nIf the part is small the vacuum system included in the leak testing instrument will be able to reach low enough pressure to allow for mass spectrometer operation.\n\nIf the size of the part is too large, an additional vacuum pumping system may be required to reach low enough pressure in a reasonable length of time. Once operating pressure has been reached, the mass spectrometer can start its measuring operation.\n\nIf leakage is encountered the small and \"agile\" molecules of helium will migrate through the cracks into the part. The vacuum system will carry any tracer gas molecule into the analyzer cell of the magnetic sector mass spectrometer. A signal will inform the operator of the value of the leakage encountered.\n\nThis method is a small variation from the one above.\nIt still requires the part to be tested to be connected to a helium leak detector. The outer surface of the part to be tested is sprayed with a localized stream of helium tracer gas.\n\nIf the part is small the vacuum system included in the instrument will be able to reach low enough pressure to allow for mass spectrometer operation.\n\nIf the size of the part is too large, an additional pumping system may be required to reach low enough pressure in a reasonable length of time. Once operating pressure has been reached, the mass spectrometer can start its measuring operation.\n\nIf leakage is encountered the small and \"agile\" molecules of helium will migrate through the cracks into the part. The vacuum system will carry any tracer gas molecule into the analyzer cell of the magnetic sector mass spectrometer. A signal will inform the operator of the value of the leakage encountered. Thus correlation between maximum leakage signal and location of helium spray head will allow the operator to pinpoint the leaky area.\n\nIn this case the part is pressurized (sometime this test is combined with a burst test, i.e. at 40 bar) with helium while sitting in a vacuum chamber. The vacuum chamber is connected to a vacuum pumping system and a leak detector. Once the vacuum has reached the mass spectrometer operating pressure, any helium leakage will be measured.\nThis test method applies to a lot of components that will operate under pressure: airbag canisters, evaporators, condensers, high-voltage SF filled switchgear.\n\nIn contrast to the Helium charged sniffer test, the partial vacuum method, the ultra sniffer test gas method (UST-method) uses the partial vacuum effect, so that the gas tightness of test sample can be detected at normal pressure with the same sensitivity as the helium charged vacuum test with helium gas helium. The method has a sensitivity of 10 Pa·m·s.\nSimilar to the classical Helium charged sniffer test the test sample is enclosed in a bag, but in contrast to the classic method, the bag is exposed with a helium-free gas, so that the helium concentration inside the bag can reduced from 5·10 to 10 Pa·m·s. This sensitivity corresponds to a theoretical gas loss of 1 cm in 3000 years.\n\nThe UST method can be used very economically for the ad hoc testing of test samples. The test system can be set up easily, with normal pneumatic items, such as valves and plastic hoses. For the embedding of the test samples, a simple plastic bag is sufficient. The UST method was also used for the leak testing of component of the fusion experiment Wendelstein 7-X in Germany.\n\nThis method applies to objects that are supposedly sealed.\n\nFirst the device under test will be exposed for an extended length of time to a high helium pressure in a \"bombing\" chamber.\n\nIf the part is leaky, helium will be able to penetrate the device.\n\nLater the device will be placed in a vacuum chamber, connected to a vacuum pump and a mass spectrometer. The tiny amount of gas that entered the device under pressure will be released in the vacuum chamber and sent to the mass spectrometer where the leak rate will be measured.\n\nThis test method applies to implantable medical devices, crystal oscillator, saw filter devices.\n\nThis method is not able to detect a massive leak as the tracer gas will be quickly pumped out when test chamber is pumped down.\n\nIn this last case the part is pressurized with helium. The mass spectrometer is fitted with a special device, a sniffer probe, that allows it to sample air (and tracer gas when confronted with a leak) at atmospheric pressure and to bring it into the mass spectrometer.\n\nThis mode of operation is frequently used to locate a leak that has been detected by other methods, in order to allow for parts repair. Modern machines can digitally remove the helium two decades below the background level and thus it is now possible detect leaks as small as 5·10 Pa·m·s in sniffing mode.\n\n\n"}
{"id": "7010729", "url": "https://en.wikipedia.org/wiki?curid=7010729", "title": "Home Energy Station", "text": "Home Energy Station\n\nHome Energy Station is the name of Honda's heat and electricity generator for the home as well as a fuel provider for hydrogen-powered fuel cell vehicles. HES IV is able to supply a sufficient amount of hydrogen to power a fuel cell vehicle, such as the Honda FCX, for daily operation while providing electricity for an average-sized household on an average days usage of energy.\n\nThe system reforms natural gas to extract up to 3 normal cubic meters per hour (Nm3/hr) of hydrogen, which is stored in an internal tank. This hydrogen is stored for later use by the vehicle, and can also be supplied to hydrogen appliances or fuel cells within the home. The heat generated by the reforming process can also provide hot water to the home.\n\nIn addition to providing as much as 5 kilowatts of electrical power to the home, the Home Energy Station is also able to function as a backup power generation system during power outages.\n\n2005 - The Home Energy Station III underwent testing at Honda R&D Americas Torrance, California.\n\n2007 - The Home Energy Station IV underwent testing for use with the Honda FCX Clarity.\n\n\n"}
{"id": "34940901", "url": "https://en.wikipedia.org/wiki?curid=34940901", "title": "Indian Airlines Flight 440", "text": "Indian Airlines Flight 440\n\nIndian Airlines Flight 440 was a flight on 31 May 1973 that crashed while on approach to Palam Airport killing 48 of the 65 passengers and crew on board.\n\nFlight 440 was a scheduled domestic passenger flight from Madras (now Chennai), Tamil Nadu to New Delhi. A Boeing 737 named \"Saranga\" was used for the flight. As Flight 440 approached Palam International Airport in driving dust and a rainstorm, the aircraft struck high tension wires during a NDB approach with visibility below minimal. The aircraft crashed and caught fire. 48 of the 65 passengers and crew on board Flight 440 perished in the accident. Rescue officials said the survivors were in the front of the aircraft, although one survivor reported sitting in the back row.\n\nThe survivors included three Americans and two Japanese. The dead included four Americans, three people from the United Kingdom, and one woman from Yemen. Among the dead was Indian Minister of Iron and Steel Mines, Mohan Kumaramangalam.\n\nInvestigators determined the Indian Airlines Flight 440 crash was caused by the crew descending below the minimum decision height.\n\n\n"}
{"id": "45273533", "url": "https://en.wikipedia.org/wiki?curid=45273533", "title": "Integral Molten Salt Reactor", "text": "Integral Molten Salt Reactor\n\nThe Integral Molten Salt Reactor (IMSR) is a design for a small modular reactor (SMR) that employs molten salt reactor technology being developed by the Canadian company Terrestrial Energy.\nIt is based closely on the denatured molten salt reactor (DMSR), a reactor design from Oak Ridge National Laboratory, and also incorporates elements found in the SmAHTR, a later design from the same laboratory. \nThe IMSR belongs to the DMSR class of molten salt reactors (MSR) and hence is a \"burner\" reactor that employs a liquid fuel rather than a conventional solid fuel; this liquid contains the nuclear fuel and also serves as primary coolant.\n\nIn 2016, Terrestrial Energy engaged in a pre-licensing design review for the IMSR with the Canadian Nuclear Safety Commission and entered the second phase of this process in October 2018 after successfully completing the first stage in late 2017.\nThe company claims it will have its first commercial IMSRs licensed and operating in the 2020s.\n\nThe Integral Molten Salt Reactor is so called because it integrates into a compact, sealed and replaceable nuclear reactor unit, called the IMSR Core-unit. \nThe unit include all the primary components of the nuclear reactor that operate on the liquid molten fluoride salt fuel: moderator, primary heat exchangers, pumps and shutdown rods.\nThe Core-unit forms the heart of the IMSR system. In the Core-unit, the fuel salt is circulated between the graphite core and heat exchangers. \nThe Core-unit itself is placed inside a surrounding vessel called the guard vessel. The entire Core-unit module can be lifted out for replacement. The guard vessel that surrounds the Core-unit acts as a containment vessel. In turn, a shielded silo surrounds the guard vessel.\n\nThe IMSR belongs to the denatured molten salt reactor (DMSR) class of molten salt reactors (MSR). \nIt is designed to have all the safety features associated with the Molten Salt class of reactors including low pressure operation (the reactor and primary coolant is operated near normal atmospheric pressure), the inability to loose primary coolant (the fuel is the coolant), the inability to suffer a melt down accident (the fuel operates in an already molten state) and the robust chemical binding of the fission products within the primary coolant salt (reduced pathway for accidental release of fission products).\n\nThe design uses standard assay low-enriched uranium fuel, with less than 5% U with a simple converter (also known as a \"burner\") fuel cycle objective (as do most operating power reactors today). The proposed fuel is in the form of uranium tetrafluoride (UF) blended with carrier salts. These salts are also fluorides, such as lithium fluoride (LiF), sodium fluoride (NaF) and/or beryllium fluoride (BeF). These carrier salts increase the heat capacity of the fuel and lower the fuel's melting point. \nThe fuel salt blend also acts as the primary coolant for the reactor.\n\nThe IMSR is a thermal-neutron reactor moderated by vertical graphite tubular elements. \nThe molten salt fuel-coolant mixture flows upward through these tubular elements where it goes critical. \nAfter heating up in this moderated core the liquid fuel flows upward through a central common chimney and is then pulled downward by pumps through heat exchanges positioned inside the reactor vessel. \nThe liquid fuel then flow down the outer edge of the reactor core to repeat the cycle. \nAll the primary components, heat exchangers, pumps etc. are positioned inside the reactor vessel. \nThe reactor’s integrated architecture avoids the use of external piping for the fuel that could leak or break. \n\nThe piping external to the reactor vessel contain two additional salt loops in series: a secondary, nonradioactive coolant salt, followed by another (third) coolant salt. \nThese salt loops act as additional barriers to any radionuclides and improve the system's heat capacity. It also allows easier integration with the heat sink end of the plant; either process heat or power applications using standard industrial grade steam turbine plants are envisioned by Terrestrial Energy.\n\nThe IMSR Core-unit is designed to be completely replaced after a 7-year period of operation. This ensures that a sufficient operational lifetime of materials used in the IMSR reactor core can be achieved. During operation, small fresh fuel/salt batches are periodically added to the reactor system. This online refueling process does not require the mechanical refueling machinery required for solid fuel reactor systems.\n\nMany of these design features are based on two previous molten salt designs from Oak Ridge National Laboratory (ORNL) – the ORNL denatured molten salt reactor (DMSR) from 1980 and the solid fuel/liquid salt cooled, small modular advanced high temperature reactor (SmAHTR), a 2010 design. The DMSR, as carried into the IMSR design, proposed to use molten salt fuel and graphite moderator in a simplified converter design using LEU , with periodic additions of LEU fuel. Most previous proposals for molten salt reactors all bred more fuel than needed to operate, so were called breeders. Converter or \"burner\" reactors like the IMSR and DMSR can also utilize plutonium from existing spent fuel as their makeup fuel source. The more recent SmAHTR proposal was for a small, modular, molten salt cooled but solid TRISO fuelled reactor.\n\nThe design uses a replaceable Core-unit. When the graphite moderator's lifetime exposure to neutron flux causes it to start distorting beyond acceptable limits, rather than remove and replace the graphite moderator, the entire IMSR Core-unit is replaced as a unit. This includes the pumps, pump motors, shutdown rods, heat exchangers and graphite moderator, all of which are either inside the vessel or directly attached to it. To facilitate a replacement, the design employs two reactor silos in the reactor building, one operating and one idle or with a previous, empty, spent Core-unit in cool-down. After 7 years of operation, the core-unit is shut down and cools in place to allow short lived radionuclides to decay. After that cool-down period, the spent core-unit is lifted out and eventually replaced.\n\nSimultaneously, a new Core-unit is installed and activated in the second silo. This entails connection to the secondary (coolant) salt piping, placement of the containment head and biological shield and loading with fresh fuel salt. The containment head provides double containment (the first being the sealed reactor vessel itself). The new Core-unit can now start its 7 years of power operations.\n\nThe IMSR vendor accumulates sealed, spent IMSR Core-units and spent fuel salt tanks in onsite, below grade silos. This operational mode reduces uncertainties with respect to long service life of materials and equipment, replacing them by design rather than allowing age-related issues such as creep or corrosion to accumulate.\n\nThe IMSR employs online fueling. While operating, small fresh fuel salt batches are periodically added to the reactor system. As the reactor uses circulating liquid fuel this process does not require complex mechanical refueling machinery. The reactor vessel is never opened, thereby ensuring a clean operating environment. During the 7 years, no fuel is removed from the reactor; this differs from solid fuel reactors which must remove fuel to make room for any new fuel assemblies, limiting fuel utilization.\n\nNuclear power reactors have three fundamental safety requirements: control, cooling, and containment.\n\nNuclear reactors require control over the critical nuclear chain reaction. As such, the design must provide for exact control over the reaction rate of the core, and must enable reliable shut-down when needed. Under routine operations, the IMSR relies on intrinsic stability for reactivity control; there are no control rods. This behavior is known as negative power feedback - the reactor is self-stabilizing in power output and temperature, and is characterized as a load-following reactor. Reactor power is controlled by the amount of heat removed from the reactor: increased heat removal results in a drop in fuel salt temperature, resulting in increased reactivity and in turn increased power. Conversely, reducing heat removal will increase reactor temperature at first, lowering reactivity and subsequently reducing reactor power. If all heat removal is lost, the reactor power will drop to a very low power level.\n\nAs backup (and shutdown method for maintenance), the IMSR employs shutdown rods filled with neutron absorber. As with other molten salt reactors, the reactor can also be shut down by draining the fuel salt from the Core-unit into storage tanks.\n\nA nuclear reactor is a thermal power system—it generates heat, transports it and eventually converts it to mechanical energy in a heat engine, in this case a steam turbine. \nSuch systems require that the heat is removed, transported and converted at the same rate it is generated.\n\nA fundamental issue for nuclear reactors is that even when the nuclear fission process is halted, heat continues to be generated at significant levels by the radioactive decay of the fission products for days and even months. \nThis is known as decay heat and is the major safety driver behind the cooling of nuclear reactors, because this decay heat must be removed. \nFor conventional light water reactors the flow of cooling water must continue in all foreseeable circumstances, otherwise damage and melting of the (solid) fuel can result. \nLight water reactors operate with a volatile coolant, requiring high pressure operation and depressurization in an emergency.\n\nThe IMSR instead uses liquid fuel at low pressure. \nIMSR does not rely on bringing coolant to the reactor or depressurizing the reactor, using instead passive cooling. \nHeat continuously dissipates from the Core-unit. \nDuring normal operation, heat loss is reduced by the moderate temperature of the reactor vessel in normal operation, combined with the stagnant air between the Core-unit and guard vessel, which only allows radiant heat transfer. Radiant heat transfer is a strong function of temperature; any increase in the temperature of the Core-unit will rapidly increase heat loss. \nUpon shutdown of the primary salt pumps, the reactor passively drops power to a very small level. \nIt can still heat up slowly by the small but constant decay heat as previously described. Due to the large heat capacity of the graphite and the salts, this increase in temperature is slow. \nThe higher temperatures slowly increase thermal radiant heat loss, and subsequent heat loss from the guard vessel itself to the outside air. \nLow pressure nitrogen flows by natural convection over the outside of the guard vessel, transporting heat to the metal reactor building roof. \nThis roof provides the passive heat loss required, acting as a giant radiator to the outside air. \nAs a result, heat loss is increased while decay heat naturally drops; an equilibrium is reached where temperatures peak and then drop. \nThe thermal dynamics and inertia of the entire system of the Core-unit in its containment silo is sufficient to absorb and disperse decay heat. \nIn the long term, as decay heat dissipates almost completely, and the plant is still not recovered, the reactor would increase power to the level of the heat loss to IRVACS, and stay at that low power level (and normal temperature) indefinitely.\n\nThe molten salts are excellent heat transfer fluids, with volumetric heat capacities close to water, and good thermal conductivity.\n\nAll molten salt reactors have features that contribute to containment safety. These mostly have to do with the properties of the salt itself. The salts are chemically inert. They do not burn and are not combustible. The salts have low volatility (high boiling point around 1400 °C), allowing a low operating pressure of the core and cooling loops. This provides a large margin above the normal operating temperature of some 600 to 700 °C. This makes it possible to operate at low pressures without risk of coolant/fuel boiling.\n\nThe high chemical stability of the salt precludes energetic chemical reactions such as hydrogen gas generation/detonation and sodium combustion, that can challenge the design and operations of other reactor types. The fluoride salt reacts with many fission products to produce chemically stable, non-volatile fluorides, such as cesium fluoride. Similarly, the majority of other high risk fission products such as iodine, dissolve into the fuel salt, bound up as iodide salts. However, for the MSRE \"of the order of\none-fourth to one-third of the iodine has not been\nadequately accounted for.\". There is some uncertainty as to whether this is a measurement error, as the concentrations are small and other fission products also had similar accounting problems. See liquid fluoride thorium reactor and molten salt reactor for more information.\n\nThe IMSR also has multiple physical containment barriers. It uses a sealed, integral reactor unit, the Core-unit. The Core-unit is surrounded by the guard vessel on its side and bottom, itself surrounded by a gas-tight structural steel and concrete silo. The Core-unit is covered up from the top by a steel containment head which is itself covered by thick round steel and concrete plates. The plates serve as radiation shield and provide protection against external hazards such as explosions or aircraft crash penetration. The reactor building provides an additional layer of protection against such external hazards, as well as a controlled, filtered-air confinement area.\n\nMost molten salt reactors use a gravity drain tank as an emergency storage reservoir for the molten fuel salt. The IMSR deliberately avoids this drain tank. The IMSR design is simpler and eliminates the bottom drain line and accompanying risks from low level vessel penetrations. The result is a more compact, robust design with fewer parts and few failure scenarios.\n\nRelative to light water reactors the scale of the containment building is significantly reduced as there is no need to deal with the phase change risk associated with a water based coolant.\n\nThe economics of nuclear reactors are dominated by the capital cost—the cost to build and finance the construction of the facility. Fuel and operating cost are relatively low.\n\nDue to the dominance of capital cost, most nuclear power reactors have sought to reduce cost per Watt by increasing the total power output of the reactor system. However, this often leads to very large projects that are difficult to finance, manage and to standardize.\n\nTerrestrial Energy is arguing for a different approach: to produce a more compact, more efficient reactor system.\n\nA large part of the cost of nuclear power reactors is related to safety and the resulting quality and regulatory requirements that can drive costs up. \nThe IMSR approach is to rely on inherent and passive safety features rather than complex active systems, potentially reducing costs in this important area, while increasing the safety profile.\n\n\nTraditional nuclear reactors, such as pressurized and boiling water reactors, feature relatively low efficiency, typically around 32-34%. \nThis is due to the relatively low temperature heat produced. \nThe higher salt temperature of the IMSR provides heat at higher temperatures that results in an efficiency in the 45-48% range.\nThe IMSR produces around 1.4 times more electricity per unit reactor heat output compared to traditional commercial reactors.\n\nNuclear efficiency is less important to economics, as fuel costs in a nuclear facility are low.\nHowever, the higher temperature of the IMSR allows for the use of lower-cost turbine systems, already in common use with coal fired power stations, as opposed to traditional nuclear power plant that usually need specialized low-temperature turbines. \nThis helps to further lower the capital cost.\n\nA key cost driver is in the nature of the equipment used. Standardized, manufactured components are lower cost than specialized, or even custom components. The compact Core-unit forms the basic modularity of the IMSR system. Core-units are identical and small enough to be fabricated in a controlled in-door environment.\n\nHigh pressure is a cost driver for any component, as it increases both quality requirements and required materials (thickness). Large, high pressure components require heavy weldings and forgings that have limited availability. A typical operating pressure for a PWR is over 150 atmospheres. Due to the low vapor pressure and high boiling point of the salt, the Core-unit operates at or near atmospheric pressure (other than a few atmospheres of pressure from the hydrostatic weight of the salt), despite the higher operating temperature. This results in lighter, thinner components that are easier to manufacture and modularize.\n\nVarious non-electric applications exist that have a large market demand for energy: steam reforming, paper and pulp production, chemicals and plastics, etc. Light water reactors are less suitable to most of these markets due to the low operating temperature of around 300 °C, and the large size of the reactors. The IMSR's smaller size and higher operating temperature (around 700 °C) could potentially open up new markets in these process heat applications. In addition, cogeneration, the production of both heat and electricity, are also potentially attractive.\n\nTerrestrial Energy was founded in Canada in 2013 with the objective of commercialising the IMSR, and is currently working to license (in both Canada and the USA) an IMSR design with a thermal power capacity of 400 MW (equivalent to 190 MW electrical). \nAs standard industrial grade steam turbines are proposed, cogeneration, or combined heat and power, is also possible. \n\nIn 2016, Terrestrial Energy engaged in a pre-licensing design review for the IMSR with the Canadian Nuclear Safety Commission. \nIt successfully completed the first stage of this process in late 2017, and entered the second phase of the design review in October 2018.\nTerrestrial Energy claims it will have its first commercial IMSRs licensed and operating in the 2020s.\n\nTerrestrial Energy had previously proposed designs in 3 other sizes, generating 80 MW, 300 MW and 600 MW thermal power, and 33, 141, and 291 MW of electricity respectively, using standard industrial grade steam turbines. However it has not sought to take these alternate designs through the licensing process.\n\n"}
{"id": "34626554", "url": "https://en.wikipedia.org/wiki?curid=34626554", "title": "Jonathan of the Bears", "text": "Jonathan of the Bears\n\nJonathan degli orsi (internationally released as \"Jonathan of the Bears\") is a 1993 Italian spaghetti western film directed by Enzo G. Castellari. It was coproduced and filmed in Russia, where it was released as \"Месть - белого индейца\" (Revenge of the White Indian).\n\nYoung Jonathan Kowalski hides in a cave when a bunch of criminals kills his family. Hereby he meets a little bear. The two orphans live together until Jonathan gets picked up by a friendly tribe of Indians. Being raised as the chief's stepson he's respected and takes a squaw named Shaya. When he thinks he's got it all, the tycoon Fred Goodwin finds oil in the soil of his tribe. Still haunted by not having been able to rescue his original family Jonathan decides to protect his clan against the Fred Goodwin and his mercenaries. His enemies finally capture and crucify him. Jonathan is soon rescued by the son of African slaves.\n\n\n"}
{"id": "42737621", "url": "https://en.wikipedia.org/wiki?curid=42737621", "title": "Lakhwar Dam", "text": "Lakhwar Dam\n\nLakhwar-Vyasi Dam project on Yamuna River, includes under-construction Lakhwar Dam and Power Station, Vyasi Dam, Hathiari Power Station and Katapathar Barrage, near the Lakhwar town in Kalsi block of Dehradun district of Uttarakhand in India, for the purpose of irrigation of 40,000 hectare land and total 927 MW hydroelectric power generation. \"Lakhwar Dam\" is a gravity dam near the Lohari village with 300MW power generation capacity. \"Vyasi Dam\" will be built 5 km downstream along with 120 MW \"Hathiari Power Station\" further 0.5 km downstream. \"Katapathar Barrage\", with the maximum ponding water level at 514.5m elevation, will be built further 2.75 km downstream of the \"Hathiari Power Station\" to supply the water to stakeholder states. Project will hold 580 million cubic metres water during monsoon and release into Yamuna during dry months. \n\nConstruction of Lakhwar dam comenced in early 2018 with the 4 year target completion date of December 2022. Vyasi dam has already been under construction since 2016.\n\nIn 1976, Planning Commission granted approval for the project. In 1986 environmental clearance was granted and construction on the tall dam began in 1987 by Jaypee Group under the supervision of the Uttar Pradesh Irrigation Department, when the area belonged to Uttar Pradesh. In 1992, the construction was halted at 35 percent progress after Jaypee Group pulled out due to the lack of funding. In 2008, it was notified as a national project by the union govt and the union govt will bear the 90% cost. In November 2013, construction restarted on the Vyasi Dam and it once again received environmental clearance from the Ministry of Environment and Forests in February 2014. Vyasi Dam was expected to be completed in 2016, revised to 2018. Project was stalled for almost 30 years, which was expedited with enhanced compensation payment in 2016 to the displaced villagers.\n\nDuring 2016, Vyasi Dam and Hathiari Power Station were being constructed simultaneously, and Lakhwar Dam was planned to be constructed after the completion of Vyasi dam, due to the lack of funds. All approvals are in place for the whole project including the Lakhwar Dam, except for the funding from the union government (c. 2017).\n\nIn April 2016, things were expedited when Government of India, under Prime Minister Narendra Modi, granted clearance for the release of funding for the whole project at an estimated cost of INR39.7 billion (3966.51 crore), including INR25 billion (2578.23 crore) for the irrigation component to build dam and canals and INR13,9 billion (1388.28 crore) for the power generation component. Subsequently, in January 2018, bid for the construction of Lakhwar dam and 300MW power station was called by the Uttrakhand Jal Vidyut Nigam Limited. Soon after in early 2018, Prime Minister Modi laid the foundation stone, and construction commenced after contracts were awarded with a target completion deadline of 4 years by December 2022.\n\nThe 927MW electricity and 580 million cubic metres water capacity project will irrigate 40,000 hectare land, and submerge 50 villages and 868.08 ha of forest land.\n\nThe project will water to the states of Haryana, Uttrakhand, Uttar Pradesh, Himachal Pradesh, Delhi and Rajasthan. Haryana's share in this project is 1.22 lakh acre feet additional water.\n\nLakhwar Dam is 192 meter high concrete gravity dam with 165.9 meter long head and 20 to 50 km long backwater reservoir on Yamuna River near Lohari village close to Lakhwar town, with 900 MW (3 X 300 MW) Underground power station on right/east side of Yamuna river. It will have storage capacity of 333.04 million cubic meter.\n\nVyasi Dam, also Byasi Dam, is 86 meter high concrete gravity dam with 113.92 meter long head that will be built 5 km downstream from Lakhwar Dam, with 120 MW power generation capacity at Hathiari. It is close to Juddo village which will be flooded once the dam is complete. Expected to be completed in May 2018.\n\nHathiari Power Station, 0.5 km downstream from Vyasi dam on the right/east side of the Yamuna, will have 120 MW of electricity generation capacity. The power station will have a 2.7 km long Run-of-the-river hydroelectricity power tunnel 7 meters in diameter with 70 Cumecs discharge, a 63.50 meter high Surge shaft 18 meters in diameter, two Pressure shafts each 209 meters long and 4 meters in diameter, and a Surface Power House (72 meters long x 24 meters wide x 40.2 meters high) with two Semi Umbrella Generators of 60 MW each composed of two Vertical Francis Turbines.\n\nKatapathar Barrage, with the maximum ponding water level at 514.5m elevation, will be built further 2.75 km downstream of the \"Hathiari Power Station\". It will be used to store and uniformly distribute water to the stakeholder states.\n\n\n"}
{"id": "39994387", "url": "https://en.wikipedia.org/wiki?curid=39994387", "title": "Limnoforming", "text": "Limnoforming\n\nLimnoforming (from Greek: limnee, \"lake\"; Latin: formo, \"to shape\", as in shaping, fashioning, molding, modeling) is the process of manipulating the physical or chemical properties of a body of water by introducing organisms which facilitate higher level biological activity, thus impacting the overall ecology of a given body of water, and eventually adjacent ecosystems. \nLimnoforming is a process using living organisms to enhance a habitat's abiotic component, ultimately rendering it more conducive to a higher ecological quality. This could be accomplished by introducing a population of organisms, e.g., invertebrates or microbes, en masse to the substrate of a body of water. These organisms would then physically and/or chemically alter the underwater environment to furnish a more suitable substrate for a wider range of biological activity; the result being an increased ecological function (e.g., in trophic dynamics), and thus a higher quality ecological state. Ultimately, limnoforming aims to accelerate the rate of ecological succession in distressed aquatic systems (e.g., lower Green Bay, Lake Michigan), so as to produce a biologically complex climax community in a comparatively short amount of time.\nThe concept of limnoforming originated from the benthic ecology laboratory of Dr. Jerry L. Kaster, School of Freshwater Sciences, University of Wisconsin – Milwaukee. Limnoforming was partially inspired by, and is similar in several aspects to, the concept of terraforming. The two concepts' main similarity is that both aim to accelerate the rate of change occurring in a given environment, in terms of its habitability for a given species or for a number of species, and furthermore, the overall function of its ecology. Instead of creating a habitable ecosystem or biosphere from scratch, limnoforming simply aims to amend degradated Earthly aqueous environments less apt to harboring a high quality ecological community into an environment which does support an ecologically flourishing system. Limnoforming differs from traditional habitat rehabilitation or restoration in that limnoforming is driven by an early sere biological succession process that modifies the physical substrate making it better suited for later seres, whereas rehabilitation/restoration is generally driven by targeting a terminal sere that is poorly adapted at re-forming habitat upon which it depends.\nThe initial limnoforming study, in Green Bay, Lake Michigan, uses freshwater oligochaetes to re-consolidate highly fluid gyttja substrate (organic black ooze) found extensively in lower Green Bay. The goal is to modify substrate suitability for the mayfly \"Hexagenia\". Historically, this mayfly was found in abundance but the eutrophication of the bay led to their demise in first half of the 20th century.\n\n"}
{"id": "3852732", "url": "https://en.wikipedia.org/wiki?curid=3852732", "title": "Linenizing", "text": "Linenizing\n\nLinenizing is the process of impressing a linen-like pattern onto the surface of paper, cloth or other malleable substance.\n\nLinenizing is most frequently done on paper products and its use on paper products goes back to the early part of the 20th century.\n\nA paper roll is threaded between two hard rollers, usually made from steel. One or both of the steel rollers has a linen pattern engraved on it. As the nip pressure between the two hard rollers increases, the pattern from the engraved roller(s) is pressed into the paper. The end result is a pattern that looks like a linen table cloth or linen dress.\n\nVarious patterns and depths of those patterns have been developed throughout the years. Generally, the pattern is a series of vertical and horizontal lines with distances varying between those lines.\n"}
{"id": "2553282", "url": "https://en.wikipedia.org/wiki?curid=2553282", "title": "Lion Air Flight 583", "text": "Lion Air Flight 583\n\nLion Air Flight 583 (JT 583) was a scheduled passenger flight from Soekarno–Hatta International Airport, Jakarta to Juanda International Airport in Surabaya with a stopover at Adi Sumarmo Airport, Surakarta, Indonesia. On 30 November 2004, the McDonnell Douglas MD-82 overran the runway of Adi Sumarmo Airport and crashed onto a cemetery on landing. The Captain, a flight attendant and 23 passengers were killed in the crash.\nInvestigation conducted by the Indonesian National Transportation Safety Committee concluded that the crash was caused by hydroplaning, which was aggravated by wind shear.\n\nThe aircraft, a McDonnell Douglas MD-82, with serial number 49189, made its first flight in November 1984 and was operated by another airline before being acquired by Lion Air in 2002 and registered as PK-LMN. Lion Air had sold the aircraft to another airline for delivery in January 2005.\n\nFlight 583 took off from Jakarta at 17:00 local time (West Indonesian Time) carrying 146 passengers and 7 crew members. Most of the passengers were members of the Nahdlatul Ulama, who were attending a national meeting held after the victorious result of the 2004 Indonesian Presidential Election. The flight was piloted by Captain Dwi Mawastoro, who was a former military pilot, and First Officer Stephen Lesdek under instrument flight regulations. The flight was uneventful until its landing.\n\nThe flight arrived at the airport during dusk, at around 18:00 local time in heavy rain. A thunderstorm was reportedly present during the landing.\n\nFlight 583 was configured appropriately for landing, touched down \"smoothly\", according to most passengers, and the thrust reversers were deployed. \nThe aircraft, however, failed to slow adequately, overran the runway and slammed into an embankment. The impact caused the floor of the front portion of the plane to collapse, reportedly killing many of the passengers. The aircraft split into two sections, coming to rest in the end of the Runway and fuel began to leak. Passengers had difficulty locating emergency exits in the waning light. Some of the passengers self-evacuated through the opening in the fuselage.\n\nThe airport was closed and emergency services were notified. Injured passengers were transported by police vehicles and ambulances to numerous hospitals across Solo. At least 14 of the dead were transported to the Pabelan Hospital. 6 people, 2 dead and 4 injured, were transported to Panti Waluyo Hospital. Others were transported to Oen Kandangsapi, Brayat Minulya, Kasih Ibu, Oen Solo Baru and PKU Muhammadiyah, as well as facilities in Boyolali and Karanganyar. Survivors with minor injuries were treated inside the airport VIP terminal.\n\nUltimately 25 persons were listed by authorities as killed and 59 injured.\n\nMost of the passengers were Indonesian, while airport officials confirmed that one Singaporean woman was among the injured. The pilot at the controls of the plane during the landing and subsequent crash was Captain Dwi Mawastoro; his co-pilot was First Officer Stephen Lesdek. Captain Dwi died in the crash while First Officer Lesdek survived with serious injuries.\n\nThe newly elected Indonesian President Susilo Bambang Yudhoyono ordered an immediate investigation onto the cause of the crash of Flight 583 and stated that the investigation should be open to the public to prevent unwanted rumors in the aftermath of the crash. Minister of Transportation Hatta Radjasa stated that the Transportation Department would evaluate the Indonesian airline operations in response to the crash of Flight 583 in addition to two other similar incidents which occurred on the same day.\n\nThe black box was subsequently found on 1 December 2004 and was transported to the Adi Sumarmo Emergency Operations Center.\n\nA witness to the crash claimed that lightning struck the plane during its landing phase. According to him, the landing light and the interior lighting were extinguished after the strike.\n\nLion Air \"claimed responsibility\" for the crash and stated that they would pay the hospital bills of the survivors. However, they denied that the crash was caused by the airline's misconduct and stated that weather was the main factor. According to them, Flight 583 experienced a tailwind during its landing, which explained why the plane didn't stop. Others claimed that the brakes or the thrust reversers malfunctioned.\n\nThe preliminary report was published in 2005. Investigators stated that the plane's braking system was not at its optimum level. This condition was aggravated by weather conditions during the accident. Investigators also identified a faulty thrust reverser as one of the cause of the crash; they subsequently issued several recommendations to Lion Air.\n\n\n"}
{"id": "11912100", "url": "https://en.wikipedia.org/wiki?curid=11912100", "title": "Low emission vehicle", "text": "Low emission vehicle\n\nA low-emission vehicle is a motor vehicle that emits relatively low levels of motor vehicle emissions. The term may be used in a general sense, but in some countries it is defined in air quality statutes.\nDifferent groups of people (\"go greens\", \"go with the flow\" and \"no greens\") show different interest in low emission vehicles\n\n"}
{"id": "7956487", "url": "https://en.wikipedia.org/wiki?curid=7956487", "title": "Lunnasting stone", "text": "Lunnasting stone\n\nThe Lunnasting stone is a stone bearing an ogham inscription, found at Lunnasting, Shetland and donated to the National Museum of Antiquities of Scotland in 1876.\n\nIt was found by Rev. J.C. Roger in a cottage, who stated that the stone had been unearthed from a \"moss\" (i.e. a peat bog) in April 1876, having been originally discovered five feet (1.5 m) below the surface.\n\nThe stone is made of slate and is long, by about in breadth and thick with the inscription on the flat surface. In addition to the ogham letters which are arranged down a centre line, there is a small cruciform mark near the top of the stone, which may be a runic letter or a Christian cross. It is unknown whether this mark and the ogham are contemporary, or whether the former was later added to a pre-existing standing stone.\n\nThe Pictish inscription has been read as:\n\nThe script probably contains the personal name \"Nechton\", and Diack (1925) took the view that the last two words mean “the vassal of Nehtonn“\nbut it is otherwise without certain interpretation. Forsyth suggests \"Ahehhttannn\" is also a personal name.\n\nOther recent attempts include:\n\nThe word-dividing dots suggest Norse influence, but this could pre-date the Viking occupation of Shetland, and an eighth- or ninth-century origin is likely for the ogham work.\n\nThe difficulties of providing a clear interpretation of the script has led to a number of other suggestions.\n\nVincent (1896) suggests that the stone may have been erected by \"Irish missionary monks not earlier than A.D. 580\" and quotes an unnamed expert's transcription of the ogham as:\n\nLockwood (1975) writes that \"the last word is clearly the commonly occurring name Nechton, but the rest, even allowing for the perhaps arbitrary doubling of consonants in Ogam, appears so exotic that philologists conclude that Pictish was a non-Indo-European language of unknown affinities\". This view was also taken of the ogham inscribed on the Orcadian Buckquoy spindle-whorl until its 1995 interpretation as Old Irish.\n\nA language of Basque origin has also been suggested as providing a solution:\n\nalthough the original speculations in 1968 by Henri Guiter do not appear convincing and were not well received academically. The eminent Vasconist Larry Trask says about Guiter's attempts that \"like the majority of such dramatic announcements, this one has been universally rejected. Pictish specialists dismiss it out of hand, and vasconists have been no more impressed\". The criticisms focus on random readings being assigned to Ogam letters, alleged complete decipherment of inscriptions too weathered to be read with certainty, the use of 20th century Basque rather than reconstructed Proto-Basque forms, disregarding syntax and highly fanciful translations.\n\n\n\n"}
{"id": "255898", "url": "https://en.wikipedia.org/wiki?curid=255898", "title": "Mad Max", "text": "Mad Max\n\nMad Max is a 1979 Australian dystopian action film directed by George Miller, produced by Byron Kennedy, and starring Mel Gibson as \"Mad\" Max Rockatansky, Joanne Samuel, Hugh Keays-Byrne, Steve Bisley, Tim Burns, and Roger Ward. James McCausland and Miller wrote the screenplay from a story by Miller and Kennedy. The film presents a tale of societal collapse, murder, and revenge set in a future Australia, in which an unhinged policeman becomes embroiled in a violent feud with a savage motorcycle gang. Principal photography took place in and around Melbourne, Australia, and lasted six weeks.\n\nThe film initially received a polarized reception upon its release in April 1979, although it won three AACTA Awards and attracted a cult following, while its critical reputation has grown since. Filmed on a budget of A$0.4 million, it earned more than US$100 million worldwide in gross revenue, and held the \"Guinness\" record for most profitable film. The success of \"Mad Max\" has been credited for further opening up the global market to Australian New Wave films. The film became the first in the \"Mad Max\" series, giving rise to three sequels, \"Mad Max 2\" (1981), \"Beyond Thunderdome\" (1985), and \"\" (2015).\n\n\"A few years from now\", when society is teetering upon the brink of collapse, berserk motorbike gang member Crawford \"Nightrider\" Montazano steals a Pursuit Special, which he uses to escape from police custody after killing a rookie officer of an Australian highway patrol called the Main Force Patrol (MFP). Even though he manages to elude other MFP officers, the MFP's top pursuit man Max Rockatansky then engages the less-skilled Nightrider in a high-speed chase. Crawford breaks off first, but then is unable to recover his concentration before he and his girlfriend are killed in a fiery crash.\n\nAt the MFP garage, Max is shown what one of the MFP mechanics has been working on: a supercharged black Pursuit Special, the last of the V8 Interceptors.\n\nMeanwhile, Nightrider's motorbike gang, led by Toecutter and Bubba Zanetti, run roughshod over a town, vandalising property, stealing fuel, and terrorising the population. They trap a young couple in a car, destroy the car, and it is implied that they rape the couple. Max and fellow officer Jim Goose arrest Toecutter's young protégé Johnny the Boy at the scene. When neither the rape victims nor any of the townspeople show for Johnny's trial, the federal courts close the case, Johnny's attorneys releasing him into Bubba's custody over Goose's furious objections.\n\nWhile Goose visits a nightclub in the city the next day, Johnny sabotages his police motorbike in the parking lot. After being thrown into a field at high speed uninjured during a ride, Goose borrows a ute to haul his damaged bike back to the MFP. However, Johnny ambushes Goose off the road by throwing a brake drum through his windshield, and, at Toecutter's insistence, throws a match into the wreck of the ute, igniting the gasoline and burning Goose alive. After seeing Goose's charred body in a hospital intensive-care unit, Max becomes disillusioned with the MFP, and informs his superior Fifi Macaffee that he will resign to maintain what sanity he has left. Fifi convinces Max to take a vacation first before he submits his final letter of resignation.\n\nMax takes his wife Jessie and their infant son Sprog on vacation in a panel van. When they stop to fix the spare tyre, Jessie takes Sprog to buy ice cream. They encounter Toecutter and his gang, who attempt to molest Jessie, but Jessie kicks Toecutter in the crotch, picking up Sprog and escaping into the car. They flee to a remote farm owned by an elderly friend named May Swaisey. Toecutter's gang follows them there and ambushes Jessie in the woods. With May's help, Jessie and Sprog escape, but when they try to drive away, the van overheats and they are run over by the gang while trying to escape on foot. Max arrives too late as Sprog is instantly killed while Jessie eventually dies while in a hospital ICU.\n\nThe loss of his family ultimately drives Max into a rage as he dons his police uniform and takes the black Pursuit Special from the MFP garage to pursue and eliminate the gang. He rams several gang members off a bridge at high speed, kills Bubba during an ambush, and forces Toecutter into the path of a speeding semi-trailer truck. Finally, Max locates Johnny at a car wreck stealing the boots of its dead driver. Max handcuffs Johnny's ankle to the wrecked vehicle, and sets a crude time-delay fuse involving a slow petrol leak and Johnny's lighter. Max throws Johnny a hacksaw, leaving him the choice of sawing through either the handcuffs or his ankle in order to escape. The vehicle explodes as Max drives away.\n\nGeorge Miller was a medical doctor in Sydney, working in a hospital emergency room where he saw many injuries and deaths of the types depicted in the film. He also witnessed many car accidents growing up in rural Queensland and lost at least three friends to accidents as a teenager.\n\nWhile in residency at a Sydney hospital, Miller met amateur filmmaker Byron Kennedy at a summer film school in 1971. The duo produced a short film, \"Violence in the Cinema, Part 1\", which was screened at a number of film festivals and won several awards. Eight years later, the duo produced \"Mad Max\", working with first-time screenwriter James McCausland (who appears in the film as the bearded man in an apron in front of the diner).\n\nAccording to Miller, his interest while writing \"Mad Max\" was \"a silent movie with sound\", employing highly kinetic images reminiscent of Buster Keaton and Harold Lloyd while the narrative itself was basic and simple. Miller believed that audiences would find his violent story more believable if set in a bleak dystopian future. Screenwriter McCausland drew heavily from his observations of the 1973 oil crisis' effects on Australian motorists:\n\nKennedy and Miller first took the film to Graham Burke of Roadshow, who was enthusiastic. The producers felt they would be unable to raise money from the government bodies \"because Australian producers were making art films, and the corporations and commissions seemed to endorse them whole-heartedly\", according to Kennedy.\n\nThey designed a 40-page presentation, circulated it widely, and eventually raised the money. Kennedy and Miller also contributed funds themselves by doing three months of emergency medical calls, with Kennedy driving the car while Miller did the doctoring. Miller claimed the final budget was between $350,000 and $400,000. His brother Bill Miller was an associate producer on the film.\n\nGeorge Miller had considered an American actor to \"get the film seen as widely as possible\" and even travelled to Los Angeles, but eventually opted to not do so as \"the whole budget would be taken up by a so-called American name.\" So instead the cast would deliberately feature lesser known actors so they did not carry past associations with them. Miller's first choice for the role of Max was the Irish-born James Healey, who at the time worked at a Melbourne abattoir and was seeking a new acting job. Upon reading the script Healey declined, finding the meager, terse dialogue too unappealing.\n\nCasting director Mitch Mathews invited for \"Mad Max\" a class of recent National Institute of Dramatic Art graduates, specifically asking a NIDA teacher for \"spunky young guys\". Among these actors was Mel Gibson, whose audition impressed Miller and Matthews and earned him the role of Max. An apocryphal tale stated that Gibson went to auditions in poor shape following a fight, but this has been denied by both Matthews and Miller. Gibson's friend and classmate Steve Bisley, who worked with him in his only screen role, 1976's \"Summer City\", became Max's partner Jim Goose. A classmate of both, Judy Davis, was said to have auditioned and passed over, but Miller has declared she was only in Matthews' studio to accompany Gibson and Bisley.\n\nMost of the biker gang extras were members of actual Australian outlaw motorcycle clubs and rode their own motorcycles in the film. They were even forced to ride the motorcycles from their residence in Sydney to the shooting locations in Melbourne because the budget did not allow for aerial transport. Three of the main cast members (Hugh Keays-Byrne, Roger Ward and Vincent Gil) had previously appeared in \"Stone\", a 1974 film about biker gangs that is said to have inspired Miller.\n\nMax's yellow \"Interceptor\" was a 1974 Ford Falcon XB sedan (previously a Victoria police car) with a 351 c.i.d. Cleveland V8 engine.\nThe \"Big Bopper\", driven by Roop and Charlie, was also a 1974 Ford Falcon XB sedan and a former Victoria police car, but was powered by a 302 c.i.d. V8. The \"March Hare\", driven by Sarse and Scuttle, was an in-line-six-powered 1972 Ford Falcon XA sedan (this car was formerly a Melbourne taxi cab).\n\nThe most memorable car, Max's black \"Pursuit Special\" was a 1973 Ford XB Falcon GT351, a limited edition hardtop (sold in Australia from December 1973 to August 1976), which was primarily modified by Murray Smith, Peter Arcadipane, and Ray Beckerley. The main modifications are the Concorde front end and the supercharger protruding through the bonnet (for looks only; it was not functional). The Concorde front was a fairly new accessory at the time, designed by Peter Arcadipane at Ford Australia as a showpiece, and later became available to the general public because of its popularity. After filming of the first movie was completed, the car went up for sale, but no buyers were found; eventually it was given to Smith.\n\nWhen production of \"Mad Max 2\" began, Miller brought the car back for use in the sequel. Once filming was over the car was left at a wrecking yard in Adelaide since it again found no buyers, and was bought and restored by Bob Forsenko. Eventually it was sold again and was put on display in the Cars of the Stars Motor Museum in Cumbria, England. When the museum closed, the car went to a collection in the Dezer Museum in Miami, Florida.\n\nThe Nightrider's vehicle, another Pursuit Special, was a 1972 Holden HQ Monaro LS coupe, also tuned but deliberately damaged to look like it had been involved in crashes.\n\nThe car driven by the young couple that is vandalised and then finally destroyed by the bikers is a 1959 Chevrolet Bel Air Sedan, also modified to look like a hot-rod car with fake fuel injection stacks, fat tires, and a flame red paint job.\n\nOf the motorcycles that appear in the film, 14 were Kawasaki Kz1000 donated by a local Kawasaki dealer. All were modified in appearance by Melbourne business La Parisienne — one as the MFP bike ridden by 'The Goose' and the balance for members of the Toecutter's gang, played in the film by members of a local Victorian motorcycle club, the Vigilantes.\n\nBy the end of filming, fourteen vehicles had been destroyed in the chase and crash scenes, including the director's personal Mazda Bongo (the small, blue van that spins uncontrollably after being struck by the \"Big Bopper\" in the film's opening chase).\n\nOriginally, filming was scheduled to take ten weeks—six weeks of first unit, and four weeks on stunt and chase sequences. However, four days into shooting, Rosie Bailey, who was originally cast as Max's wife, was injured in a bike accident. Production was halted, and Bailey was replaced by Joanne Samuel, causing a two-week delay.\n\nIn the end, the shoot took six weeks over November and December 1977, with a further six-week second unit. The unit reconvened two months later, in May 1978, and spent another two weeks doing second unit shots and re-staging some stunts. Miller described the whole experience as \"guerrilla filmmaking\", where the crew would close roads without filming permits, not use walkie-talkies because their frequency coincided with the police radio, and after filming was done Miller and Kennedy would even sweep down the roads. Still, as filming progressed the Victoria Police became interested in the production, helping the crew by closing down roads and escorting the vehicles. Because of the film's low budget, all but one of the police uniforms in the film were made of vinyl leather, with only one genuine leather uniform made for stunt sequences involving Bisley and Gibson.\n\nShooting took place in and around Melbourne. Many of the car chase scenes for \"Mad Max\" were filmed near the town of Little River, northeast of Geelong. The early town scenes with the Toe Cutter Gang were filmed in the main street of Clunes, north of Ballarat. Much of the streetscape remains unchanged. Some scenes were filmed at Tin City at Stockton Beach. The \"execution of the mannequin\" scene was filmed at Seaford Beach in Seaford, Victoria.\n\n\"Mad Max\" was one of the first Australian films to be shot with a widescreen anamorphic lens, although Peter Weir's \"The Cars That Ate Paris\" (1974) was shot in anamorphic four years earlier. Miller's desire to shoot in anamorphic made him seek a set of Todd-AO wide angle lenses used by Sam Peckinpah to film \"The Getaway\" (1972), which were damaged enough in that shoot to get discarded in Australia. The only one which worked properly was a 35mm lens which was employed in the whole of \"Mad Max\".\n\nThe film's post-production was done at a friend's apartment in North Melbourne, with Wilson and Kennedy editing the film in the small lounge room on a home-built editing machine that Kennedy's father, an engineer, had designed for them. Wilson and Kennedy also performed sound editing there.\n\nTony Patterson edited the film for four months, then had to leave because he was contracted to make \"Dimboola\" (1979). George Miller took over editing with Cliff Hayes, and they worked on it for three months. Kennedy and Miller did the final cut, in a process Miller described as \"he would cut sound in the lounge room and I’d cut picture in the kitchen.\" Professional sound engineer Roger Savage would perform the sound mixing in the studio he worked after finishing his work with Little River Band, and employed timecoding techniques that were unseen in Australian cinema.\n\nThe musical score for \"Mad Max\" was composed and conducted by Australian composer Brian May (not to be confused with the guitarist of the English rock band Queen). Miller wanted a Gothic, Bernard Herrmann–type score and hired May after hearing his work for \"Patrick\" (1978). \"With the little budget that we had we went ahead and did it, and spent a lot of time on it,\" said May. \"George was marvelous to work with; he had a lot of ideas about what he wanted although he wasn’t a musician.\" A soundtrack album was released in 1980 by Varèse Sarabande.\n\n\"Mad Max\" was first released in Australia through Roadshow Film Distributors (now Village Roadshow Pictures) in 1979. The movie was sold overseas for $1.8 million, with American International Pictures releasing in the United States and Warner Bros. handling the rest of the world. The film was banned in New Zealand and Sweden, in the former because of the scene where Goose is burned alive inside his vehicle: it unintentionally mirrored an incident with a real gang shortly before the film's release. It was later shown in New Zealand in 1983 after the success of the sequel, with an 18 certificate. The ban in Sweden was removed in 2005, and it has since been shown on television and sold on home media there.\n\nWhen shown in the United States during 1980, the original Australian dialogue was redubbed by an American crew. American International Pictures distributed this dub after it underwent a management re-organisation. Much of the Australian slang and terminology was also replaced with American usages (examples: \"Oi!\" became \"Hey!\", \"See looks!\" became \"See what I see?\", \"windscreen\" became \"windshield\", \"very toey\" became \"super hot\", and \"proby\"—probationary officer—became \"rookie\"). AIP also altered the operator's duty call on Jim Goose's bike in the beginning of the film (it ended with \"Come on, Goose, where are you?\"). The only dubbing exceptions were the voice of the singer in the Sugartown Cabaret (played by Robina Chaffey), the voice of Charlie (played by John Ley) through the mechanical voice box, and Officer Jim Goose (Steve Bisley), singing as he drives a truck before being ambushed. Since Mel Gibson was not well known to American audiences at the time, trailers and television spots in the United States emphasised the film's action content. The original Australian dialogue track was finally released in North America in 2000 in a limited theatrical reissue by MGM, the film's current rights holders. It has since been released in the US on DVD with the US and Australian soundtracks on separate tracks.\n\nUpon its release, the film polarized critics. In a 1979 review, the Australian social commentator and film producer Phillip Adams condemned \"Mad Max\", saying that it had \"all the emotional uplift of \"Mein Kampf\" and would be \"a special favourite of rapists, sadists, child murderers and incipient [Charles] Mansons\". After its United States release, Tom Buckley of \"The New York Times\" called the film \"ugly and incoherent\". Stephen King, writing in \"Danse Macabre,\" called the film a \"turkey.\" However, \"Variety\" magazine praised the directorial debut by Miller.\n\n\"Mad Max\" grossed A$5,355,490 at the box office in Australia and over US$100 million worldwide. It was the most profitable film ever made at the time, holding the Guinness World Record for the highest box office to budget ratio of any motion picture. The film was awarded three Australian Film Institute Awards in 1979 (for editing, sound, and musical score). It was also nominated for Best Film, Best Director, Best Original Screenplay, and Best Supporting Actor (Hugh Keays-Byrne) by the Australian Film Institute. The film also won the Special Jury Award at the Avoriaz Fantastic Film Festival.\n\n\"Mad Max\" holds a 90% \"Fresh\" rating on review aggregator site Rotten Tomatoes, based on 61 positive critic reviews, with consensus being \"Staging the improbable car stunts and crashes to perfection, director George Miller succeeds completely in bringing the violent, post-apocalyptic world of \"Mad Max\" to visceral life.\" The film has been included in \"best films of all time\" lists by \"The New York Times\" and \"The Guardian\".\n\n"}
{"id": "628198", "url": "https://en.wikipedia.org/wiki?curid=628198", "title": "Majoron", "text": "Majoron\n\nIn particle physics, majorons (named after Ettore Majorana) are a hypothetical type of Goldstone boson that are theorized to mediate the neutrino mass violation of lepton number or \"B\" − \"L\" in certain high energy collisions such as\n\nWhere two electrons collide to form two W bosons and the majoron J. The U(1) symmetry is assumed to be global so that the majoron is not \"eaten up\" by the gauge boson and spontaneously broken. Majorons were originally formulated in four dimensions by Y. Chikashige, R. N. Mohapatra and R. D. Peccei to understand neutrino masses by the seesaw mechanism and are being searched for in the neutrino-less double beta decay process. There are theoretical extensions of this idea into supersymmetric theories and theories involving extra compactified dimensions. By propagating through the extra spatial dimensions the detectable number of majoron creation events vary accordingly. Mathematically, majorons may be modeled by allowing them to propagate through a material while all other Standard Model forces are fixed to an orbifold point.\n\nExperiments studying double beta decay have set limits on decay modes that emit majorons.\n\nNEMO has observed a variety of elements. EXO and Kamland-Zen have set half-life limits for majoron decays in xenon.\n\n"}
{"id": "39927946", "url": "https://en.wikipedia.org/wiki?curid=39927946", "title": "Naval Weapons Industrial Reserve Plant, Dallas", "text": "Naval Weapons Industrial Reserve Plant, Dallas\n\nNaval Weapons Industrial Reserve Plant, Dallas (NWIRP) was a government-owned, contractor-operated (GOCO) facility which had the mission of designing, fabricating, and testing prototype weapons and equipment, and is located in Dallas, Texas. In 2012, the plant was sold to a private corporation for $357,500. It was located for most of its time next to Naval Air Station Dallas, now known as the Grand Prairie Armed Forces Reserve Complex.\n\nThe Naval Weapons Industrial Reserve Plant, Dallas was a property where the U.S. military crafted fighter planes during both World War II and the Cold War. NASA also used this site for the space exploration program.\n\n"}
{"id": "316824", "url": "https://en.wikipedia.org/wiki?curid=316824", "title": "Nozzle", "text": "Nozzle\n\nA nozzle is a device designed to control the direction or characteristics of a fluid flow (especially to increase velocity) as it exits (or enters) an enclosed chamber or pipe.\n\nA nozzle is often a pipe or tube of varying cross sectional area, and it can be used to direct or modify the flow of a fluid (liquid or gas). Nozzles are frequently used to control the rate of flow, speed, direction, mass, shape, and/or the pressure of the stream that emerges from them. In a nozzle, the velocity of fluid increases at the expense of its pressure energy.\n\nA gas jet, fluid jet, or hydro jet is a nozzle intended to eject gas or fluid in a coherent stream into a surrounding medium. Gas jets are commonly found in gas stoves, ovens, or barbecues. Gas jets were commonly used for light before the development of electric light. Other types of fluid jets are found in carburetors, where smooth calibrated orifices are used to regulate the flow of fuel into an engine, and in jacuzzis or spas.\n\nAnother specialized jet is the laminar jet. This is a water jet that contains devices to smooth out the pressure and flow, and gives laminar flow, as its name suggests. This gives better results for fountains.\n\nThe foam jet is another type of jet which uses foam instead of a gas or fluid.\n\nNozzles used for feeding hot blast into a blast furnace or forge are called tuyeres.\n\nJet nozzles are also used in large rooms where the distribution of air via ceiling diffusers is not possible or not practical. Diffusers that uses jet nozzles are called jet diffuser where it will be arranged in the side wall areas in order to distribute air. When the temperature difference between the supply air and the room air changes, the supply air stream is deflected upwards, to supply warm air, or downwards, to supply cold air.\n\nFrequently, the goal of a nozzle is to increase the kinetic energy of the flowing medium at the expense of its pressure and internal energy.\n\nNozzles can be described as \"convergent\" (narrowing down from a wide diameter to a smaller diameter in the direction of the flow) or \"divergent\" (expanding from a smaller diameter to a larger one). A de Laval nozzle has a convergent section followed by a divergent section and is often called a convergent-divergent nozzle (\"con-di nozzle\").\n\nConvergent nozzles accelerate subsonic fluids. If the nozzle pressure ratio is high enough, then the flow will reach sonic velocity at the narrowest point (i.e. the \"nozzle throat\"). In this situation, the nozzle is said to be \"choked\".\n\nIncreasing the nozzle pressure ratio further will not increase the throat Mach number above one. Downstream (i.e. external to the nozzle) the flow is free to expand to supersonic velocities; however, Mach 1 can be a very high speed for a hot gas because the speed of sound varies as the square root of absolute temperature. This fact is used extensively in rocketry where hypersonic flows are required and where propellant mixtures are deliberately chosen to further increase the sonic speed.\n\nDivergent nozzles slow fluids if the flow is subsonic, but they accelerate sonic or supersonic fluids.\n\nConvergent-divergent nozzles can therefore accelerate fluids that have choked in the convergent section to supersonic speeds. This CD process is more efficient than allowing a convergent nozzle to expand supersonically externally.\nThe shape of the divergent section also ensures that the direction of the escaping gases is directly backwards, as any\nsideways component would not contribute to thrust.\n\nA jet exhaust produces a net thrust from the energy obtained from combusting fuel which is added to the inducted air. This hot air passes through a high speed nozzle, a \"propelling nozzle\", which enormously increases its kinetic energy.\n\nIncreasing exhaust velocity increases thrust for a given mass flow, but matching the exhaust velocity to the air speed provides the best energy efficiency. However, momentum considerations prevent jet aircraft from maintaining velocity while exceeding their exhaust jet speed. The engines of supersonic jet aircraft, such as those of fighters and SST aircraft (e.g. Concorde) almost always achieve the high exhaust speeds necessary for supersonic flight by using a CD nozzle despite weight and cost penalties; conversely, subsonic jet engines employ relatively low, subsonic, exhaust velocities and therefore employ simple convergent nozzle, or even bypass nozzles at even lower speeds.\n\nRocket motors maximise thrust and exhaust velocity by using convergent-divergent nozzles with very large area ratios and therefore extremely high pressure ratios. Mass flow is at a premium because all the propulsive mass is carried with vehicle, and very high exhaust speeds are desirable.\n\nMagnetic nozzles have also been proposed for some types of propulsion, such as VASIMR, in which the flow of plasma is directed by magnetic fields instead of walls made of solid matter.\n\nMany nozzles produce a very fine spray of liquids.\n\nVacuum cleaner nozzles come in several different shapes. Vacuum Nozzles are used in vacuum cleaners.\n\nSome nozzles are shaped to produce a stream that is of a particular shape. For example, extrusion molding is a way of producing lengths of metals or plastics or other materials with a particular cross-section. This nozzle is typically referred to as a die.\n\n\n"}
{"id": "39022683", "url": "https://en.wikipedia.org/wiki?curid=39022683", "title": "Oystering", "text": "Oystering\n\nOystering or oyster veneer is a decorative form of veneering, a type of parquetry. This technique is using thin slices of wood branches or roots cut in cross-section, usually from small branches of walnut, olive, kingwood and less commonly laburnum, yew and cocus. The resulting circular or oval pieces of veneer are laid side by side in furniture to produce various decorative patterns. Because the shape formed resembles an oyster shell the technique acquired the name of oyster veneering.\n\nThis technique is likely to have been developed by English cabinet-makers in the 1660s, immediately after the Restoration of the monarchy, first being used on furniture such as the cocuswood cabinet on stand which bears the cipher of Queen Henrietta Maria, constructed in c.1661-65, and now at Windsor Castle. Early oyster veneered cabinets were invariably in cocus or kingwood. Contemporary longcase clock cases were similarly veneered.\n\nBy around the early 1670s softer and more cheaply available woods such as olive and walnut began to be used for oyster veneering, the fashion for such furniture becoming widespread and also spreading to Holland by around the mid-1670s. Oyster veneering fell out of fashion from c.1710. The chest illustrated here, if it is not a later reproduction, therefore appears to be an unusually late example.\n\n"}
{"id": "10065851", "url": "https://en.wikipedia.org/wiki?curid=10065851", "title": "Parbati Barua", "text": "Parbati Barua\n\nParbati Barua () is a citizen of India. Born to Late Prakritish Chandra Barua of the Royal Family Of Gauripur . She came to the limelight after the BBC created the documentary \"Queen of the Elephants\" based on her life, along with the companion book by Mark Shand. She resides in Guwahati and is also a member, Asian Elephant Specialist Group, IUCN. She is the sister of Pratima Barua Pandey and niece of filmmaker Pramathesh Barua\nof \"Devdas\" fame.\n\n"}
{"id": "3318097", "url": "https://en.wikipedia.org/wiki?curid=3318097", "title": "Pele's hair", "text": "Pele's hair\n\nPele's hair is a form of lava. It is named after Pele, the Hawaiian goddess of volcanoes. It can be defined as volcanic glass fibers or thin strands of volcanic glass. The strands are formed through the stretching of molten basaltic glass from lava, usually from lava fountains, lava cascades, and vigorous lava flows.\n\nWind often carries the light fibers high into the air and to places several kilometers away from the vent. It is common to find strands of Pele's hair on high places like treetops, radio antennas, and electric poles.\n\nPele's hair does not only occur in Hawaii. It can be found near other volcanoes around the world, for example in Nicaragua (Masaya), Italy (Etna), Ethiopia (Erta’ Ale), and Iceland, where it is known as \"nornahár\" (\"witches' hair\"). It is usually found in gaps in the ground, mostly near vents, skylights, ocean entry, or in corners where Pele's hair can accumulate.\n\nIt is not recommended to touch Pele's hair, because it is very brittle and very sharp, and small broken pieces can enter the skin. Gloves should be worn while examining it.\n\nPele's tears might occur with Pele's hair. They can tell volcanologists a lot of information about the eruption, such as the temperatures and the magma's path to the surface. Plagioclase starts to crystallize from the magma of Pele's hair at around 1,160 °C. Also, the shape of the tears can provide an indication of the velocity of the eruption, and the bubbles of gas and particles trapped within the tears can provide information about the composition of the magma chamber.\n\nThe strands are created when molten lava is ejected into the air and form tiny droplets, which elongate perfectly straight. It usually forms in lava fountains, lava cascades, and vigorous lava flows.\n\nPele's hair has a golden yellow color and looks like human hair or dry straw. In sunlight, it has a shimmering gold color. Length varies considerably, but is typically 5 to 15 cm, and can be up to 2 m. Hair diameter ranges from about 1 to 300 µm (0.001 to 0.3 mm), and therefore weight is accordingly low.\n\nA manufactured version of Pele's hair made from basalt rock and recycled slag from steel manufacturing called mineral wool or stone wool is commonly used as a non-combustible, durable, dimensionally stable, UV stable, hydrophobic, vapour-permeable building insulation for residential, commercial, and high rise buildings.\n\nA hydrophilic version is used as a low water usage, high yield, soil substitute for hydroponic agriculture.\n\nPele is known as the goddess of fire, lightning, wind, dance and volcanoes. The legends that tell how Pele first came to the Hawaiian Islands have a lot of versions, but it is believed that Pele's spirit lives in the crater of the Kilauea volcano, on the island of Hawaii. Pele appears as a spirit in many forms, and she is considered a negative harbinger. Most native Hawaiians state that they have had at least one encounter with her.\n\nHawaiians traditionally believe that they must live in harmony with all natural things, and that Pele will curse with bad luck people who take lava rock, sand, seashells, or other natural parts of the islands away with them, until they return these items to their rightful place. Additionally, federal law prohibits taking anything out of a national park. Each year large quantities of such objects are mailed back to Hawaii by people who believe they have received such bad luck.\n\n\n\n\n"}
{"id": "18920883", "url": "https://en.wikipedia.org/wiki?curid=18920883", "title": "Peter Meisen", "text": "Peter Meisen\n\nPeter Meisen was born in San Diego, California, and is a graduate (1976) of the University of California, San Diego with an Applied Mechanics and Engineering Sciences degree. In 1983, he co-founded SHARE (Self Help and Resource Exchange), North America's largest private food distribution program. Meisen founded Global Energy Network Institute, GENI, geni.org in 1989, to conduct research and to educate world leaders to the strategy of linking renewable energy resources around the world. He is an internationally recognized speaker on the global issues of renewable energy, transmission and distribution of electricity, quality of life and its relationship to electricity, the environment and sustainable development.\n\nMeisen is an active member of the IEEE and in the 90s chaired panels on the feasibility of high voltage, long distance transmission and also tapping remote renewables, both of which were featured in the IEEE Power Engineering Review. He also participates regularly either with an exhibit or as a presenter at ACORE, the World Energy Conference, ISDE, CERES, and Rotary International.\n\nIn 2005, Meisen was a co-creator in partnership with KLD kld.com, of the Global Climate 100 (GC100) - a sustainable index for socially responsible investing.\n\n\n"}
{"id": "10904347", "url": "https://en.wikipedia.org/wiki?curid=10904347", "title": "Power Engineering (magazine)", "text": "Power Engineering (magazine)\n\nPower Engineering is a monthly magazine dedicated to professionals in the field of power engineering and power generation. Articles are focused on new developments in power plant design, construction and operation in North America.\n\n\"Power Engineering\" is published by PennWell Corporation, the largest U.S. publisher of electric power industry books, directories, maps and conferences.\n\n\"Power Engineering International\", also published by PennWell, covers Europe, Asia-Pacific, the Middle East and the rest of the world.\n\n"}
{"id": "6936536", "url": "https://en.wikipedia.org/wiki?curid=6936536", "title": "Prandtl–Meyer expansion fan", "text": "Prandtl–Meyer expansion fan\n\nA supersonic expansion fan, technically known as Prandtl–Meyer expansion fan, is a centred expansion process that occurs when a supersonic flow turns around a convex corner. The fan consists of an infinite number of Mach waves, diverging from a sharp corner. When a flow turns around a smooth and circular corner, these waves can be extended backwards to meet at a point. \n\nEach wave in the expansion fan turns the flow gradually (in small steps). It is physically impossible for the flow to turn through a single \"shock\" wave because this would violate the second law of thermodynamics. \n\nAcross the expansion fan, the flow accelerates (velocity increases) and the Mach number increases, while the static pressure, temperature and density decrease. Since the process is isentropic, the stagnation properties (e.g. the total pressure and total temperature) remain constant across the fan.\n\nThe theory was described by Theodor Meyer on his thesis dissertation in 1908, along with his advisor Ludwig Prandtl, who had already discussed the problem a year before. \n\nThe expansion fan consists of an infinite number of expansion waves or Mach lines. The first Mach line is at an angle formula_1 with respect to the flow direction, and the last Mach line is at an angle formula_2 with respect to final flow direction. Since the flow turns in small angles and the changes across each expansion wave are small, the whole process is isentropic. This simplifies the calculations of the flow properties significantly. Since the flow is isentropic, the stagnation properties like stagnation pressure (formula_3), stagnation temperature (formula_4) and stagnation density (formula_5) remain constant. The final static properties are a function of the final flow Mach number (formula_6) and can be related to the initial flow conditions as follows,\n\nThe Mach number after the turn (formula_6) is related to the initial Mach number (formula_9) and the turn angle (formula_10) by,\n\nwhere, formula_12 is the Prandtl–Meyer function. This function determines the angle through which a sonic flow (M = 1) must turn to reach a particular Mach number (M). Mathematically,\n\nBy convention, formula_14\nThus, given the initial Mach number (formula_15), one can calculate formula_16 and using the turn angle find formula_17. From the value of formula_17 one can obtain the final Mach number (formula_19) and the other flow properties.\n\nAs Mach number varies from 1 to formula_20, formula_21 takes values from 0 to formula_22, where\n\nThis places a limit on how much a supersonic flow can turn through, with the maximum turn angle given by,\n\nOne can also look at it as follows. A flow has to turn so that it can satisfy the boundary conditions. In an ideal flow, there are two kinds of boundary condition that the flow has to satisfy,\n\nIf the flow turns enough so that it becomes parallel to the wall, we do not need to worry about pressure boundary condition. However, as the flow turns, its static pressure decreases (as described earlier). If there is not enough pressure to start with, the flow won't be able to complete the turn and will not be parallel to the wall. This shows up as the maximum angle through which a flow can turn. The lower the Mach number is to start with (i.e. small formula_9), the greater the maximum angle through which the flow can turn.\n\nThe streamline which separates the final flow direction and the wall is known as a slipstream (shown as the dashed line in the figure). Across this line there is a jump in the temperature, density and tangential component of the velocity (normal component being zero). Beyond the slipstream the flow is stagnant (which automatically satisfies the velocity boundary condition at the wall). In case of real flow, a shear layer is observed instead of a slipstream, because of the additional no-slip boundary condition.\n\n\n\n"}
{"id": "57265022", "url": "https://en.wikipedia.org/wiki?curid=57265022", "title": "Quweira Solar Power Plant", "text": "Quweira Solar Power Plant\n\nQuweira Solar Power Plant is a 103 MW photovoltaic power station in Quweira, Jordan. As of 2018, it is the largest solar power plant in the region. It was inaugurated on 26 April 2018, as part of Jordan's long-term plan to diversify its energy resources.\n\n"}
{"id": "8904525", "url": "https://en.wikipedia.org/wiki?curid=8904525", "title": "Riverside Methodist Hospital", "text": "Riverside Methodist Hospital\n\nRiverside Methodist Hospital is the largest member hospital of OhioHealth, a not-for-profit, faith-based healthcare system located in Columbus, Ohio.\n\nAs a regional tertiary care hospital, Riverside Methodist is host to a number of specialty centers and services, including Neuroscience and Stroke, Heart and Vascular, Maternity and Women’s Health, Cancer Care, Trauma Center II, Hand and Microvascular, Surgery and Minimally Invasive Surgeries, Orthopedics, Imaging, and Bariatric Surgery. \"U.S. News & World Report\" regionally ranked Riverside Methodist Hospital the number 9 best performing among hospitals in Ohio, number 2 in Columbus metro area, rated high performing in four specialties and procedures and a nationally ranked hospital, number 49, in Neurology & Neurosurgery.\n\nThe hospital was founded on June 2, 1892 as the Protestant Hospital. Located in a 15-room house on Dennison Avenue, Columbus, the Hospital could accommodate up to 40 patients. In 1898 the hospital relocated to a larger premises at 700 North Park Street. In 1922, the hospital joined the Ohio Methodist Episcopal Conference and was renamed the White Cross Hospital.\n\nRiverside Methodist Hospital has been named to Truven Health Analytics Top 100 Hospitals, and is one of six Hospitals to receive this award on at least ten occasions. \n\nThe neurological and spine treatment programs at Riverside Methodist Hospital are among the largest in the United States. The programs feature innovative treatments for traumatic brain injury and stroke. The facility holds Primary Stroke Center certification from The Joint Commission. Through the use of minimally invasive treatments, such as MERCI Retriever and Penumbra, the critical time window for treating stroke patients is extended, creating opportunities to improve patient outcomes. \n\nIn addition, Riverside Methodist Hospital hosts the largest dedicated Neurointensive care unit in central Ohio.\n\nRiverside Methodist's Critical Limb Care program is the first of its kind in the United States, providing a complete range of vascular care. The Structural Heart Disease Patient Evaluation and Research Center provides evaluations and treatment for patients with valvular heart disease and other structural heart defects. Facilitated by the OhioHealth Research and Innovation Institute, the Center has participated in clinical trials of Medtronic's Core Valve System.\n\nRiverside Methodist provides advanced cancer care and treatment, including Tomotherapy, radiation therapy, and the minimally invasive Da Vinci robotic surgical system (used to treat prostate issues, fibroids, gynecological cancers, and lung resections).\n\nThe Riverside Cancer Care team completes more than 100,000 outpatient, imaging, and surgical procedures for cancer patients each year. Riverside Cancer Care has received Disease-Specific Certification from The Joint Commission on Accreditation of Healthcare Organizations (JCAHO), and the American College of Surgeons Commission on Cancer.\n\nRiverside Methodist's total joint program includes specialized treatment and care for patients suffering from a range of bone, muscle, and joint disorders or injuries. The Joint Commission has certified Riverside Methodist in joint replacement in the areas of hip, knee, and total shoulder.\n\nRiverside Methodist Hospital has one of Ohio's largest maternity programs, which is recognized by the Ohio Department of Health as a Level III Maternity Center. Board-certified obstetricians and neonatologists in this department specialize in the care of patients with high-risk pregnancies, and their babies.\n\nBetween 1974 and 1976, 426 patients received \"significant\" overdoses of radiation from a cobalt-60 external beam radiotherapy unit while receiving treatment for cancers.\n"}
{"id": "32813994", "url": "https://en.wikipedia.org/wiki?curid=32813994", "title": "Stanislav Petrov", "text": "Stanislav Petrov\n\nStanislav Yevgrafovich Petrov (; 7 September 1939 – 19 May 2017) was a lieutenant colonel of the Soviet Air Defence Forces who became known as \"the man who single-handedly saved the world from nuclear war\" for his role in the 1983 Soviet nuclear false alarm incident. \n\nOn 26 September 1983, three weeks after the Soviet military had shot down Korean Air Lines Flight 007, Petrov was the duty officer at the command center for the Oko nuclear early-warning system when the system reported that a missile had been launched from the United States, followed by up to five more. Petrov judged the reports to be a false alarm, and his decision to disobey orders, against Soviet military protocol, is credited with having prevented an erroneous retaliatory nuclear attack on the United States and its NATO allies that could have resulted in large-scale nuclear war. Investigation later confirmed that the Soviet satellite warning system had indeed malfunctioned.\n\nPetrov was born on 7 September 1939 near Vladivostok. His father, Yevgraf, flew fighter aircraft during World War II. His mother was a nurse.\n\nPetrov enrolled at the Kiev Higher Engineering Radio-Technical College of the Soviet Air Forces, and after graduating in 1972 he joined the Soviet Air Defence Forces. In the early 1970s, he was assigned to the organization that oversaw the new early warning system intended to detect ballistic missile attacks from NATO countries.\n\nPetrov was married to Raisa, and had a son, Dmitri, and a daughter, Yelena. His wife died of cancer in 1997.\n\nAccording to the Permanent Mission of the Russian Federation to the UNon 19 January 2006, over 22 years after the incidentnuclear retaliation requires that multiple sources confirm an attack. In any case, the incident exposed a serious flaw in the Soviet early warning system. Petrov has said that he was neither rewarded nor punished for his actions.\n\nHad Petrov reported incoming American missiles, his superiors might have launched an assault against the United States, precipitating a corresponding nuclear response from the United States. Petrov declared the system's indication a false alarm. Later, it was apparent that he was right: no missiles were approaching and the computer detection system was malfunctioning. It was subsequently determined that the false alarm had been created by a rare alignment of sunlight on high-altitude clouds above North Dakota and the Molniya orbits of the satellites, an error later corrected by cross-referencing a geostationary satellite.\n\nPetrov later indicated that the influences on his decision included that he was informed a U.S. strike would be all-out, so five missiles seemed an illogical start; that the launch detection system was new and, in his view, not yet wholly trustworthy; that the message passed through 30 layers of verification too quickly; and that ground radar failed to pick up corroborative evidence, even after minutes of delay. However, in a 2013 interview, Petrov said at the time he was never sure that the alarm was erroneous. He felt that his civilian training helped him make the right decision. He said that his colleagues were all professional soldiers with purely military training and, following instructions, would have reported a missile launch if they had been on his shift.\n\nPetrov underwent intense questioning by his superiors about his judgment. Initially, he was praised for his decision. General Yury Votintsev, then commander of the Soviet Air Defense's Missile Defense Units, who was the first to hear Petrov's report of the incident (and the first to reveal it to the public in the 1990s), states that Petrov's \"correct actions\" were \"duly noted\". Petrov himself states he was initially praised by Votintsev and promised a reward, but recalls that he was also reprimanded for improper filing of paperwork because he had not described the incident in the war diary.\n\nHe received no reward. According to Petrov, this was because the incident and other bugs found in the missile detection system embarrassed his superiors and the influential scientists who were responsible for it, so that if he had been officially rewarded, they would have had to be punished. He was reassigned to a less sensitive post, took early retirement (although he emphasized that he was not \"forced out\" of the army, as is sometimes claimed by Western sources), and suffered a nervous breakdown.\n\nIn a later interview, Petrov stated that the famous red button has never worked, as military psychologists did not want to put the decision about a nuclear war into the hands of one single person.\n\nThe incident became known publicly in the 1990s upon the publication of Votintsev's memoirs. Widespread media reports since then have increased public awareness of Petrov's actions.\n\nThere is some confusion as to precisely what Petrov's military role was in this incident. Petrov, as an individual, was not in a position where he could single-handedly have launched any of the Soviet missile arsenal. His sole duty was to monitor satellite surveillance equipment and report missile attack warnings up the chain of command; top Soviet leadership would have decided whether to launch a retaliatory attack against the West. But Petrov's role was crucial in providing information to make that decision. According to Bruce Blair, a Cold War nuclear strategies expert and nuclear disarmament advocate, formerly with the Center for Defense Information, \"The top leadership, given only a couple of minutes to decide, told that an attack had been launched, would make a decision to retaliate.\"\n\nPetrov later said \"I had obviously never imagined that I would ever face that situation. It was the first and, as far as I know, also the last time that such a thing had happened, except for simulated practice scenarios.\"\n\nIn the aftermath of the incident, the Soviet government investigated the incident and determined that Petrov had insufficiently documented his actions during the crisis. He explained it as \"Because I had a phone in one hand and the intercom in the other, and I don’t have a third hand\"; nevertheless, Petrov received a reprimand.\n\nIn 1984, Petrov left the military and got a job at the research institute that had developed the Soviet Union's early warning system. He later retired after his wife was diagnosed with cancer so he could care for her. A BBC report in 1998 stated Petrov had suffered a mental breakdown and quoted Petrov as saying, \"I was made a scapegoat.\"\n\nPetrov died on 19 May 2017 from hypostatic pneumonia, though it was not widely reported until September.\n\nOn 21 May 2004, the San Francisco-based Association of World Citizens gave Petrov its World Citizen Award along with a trophy and $1,000 \"in recognition of the part he played in averting a catastrophe.\"\n\nIn January 2006, Petrov travelled to the United States where he was honored in a meeting at the United Nations in New York City. There the Association of World Citizens presented Petrov with a second special World Citizen Award. The next day, Petrov met American journalist Walter Cronkite at his CBS office in New York City.\n\nThat interview, in addition to other highlights of Petrov's trip to the United States, was filmed for \"The Man Who Saved the World\", a narrative feature and documentary film, directed by Peter Anthony of Denmark. It premiered in October 2014 at the Woodstock Film Festival in Woodstock, New York, winning \"Honorable Mention: Audience Award Winner for Best Narrative Feature\" and \"Honorable Mention: James Lyons Award for Best Editing of a Narrative Feature.\"\n\nFor his actions in averting a potential nuclear war in 1983, Petrov was awarded the Dresden Peace Prize in Dresden, Germany, on 17 February 2013. The award included €25,000. On 24 February 2012, he was given the 2011 German Media Award, presented to him at a ceremony in Baden-Baden, Germany.\n\nOn the same day that Petrov was honored at the United Nations in New York City, the Permanent Mission of the Russian Federation to the United Nations issued a press release contending that a single person could not have started or prevented a nuclear war, stating in part, \"Under no circumstances a decision to use nuclear weapons could be made or even considered in the Soviet Union or in the United States on the basis of data from a single source or a system. For this to happen, a confirmation is necessary from several systems: ground-based radars, early warning satellites, intelligence reports, etc.\" But nuclear security expert Bruce Blair has said that at that time, the U.S.–Soviet relationship \"had deteriorated to the point where the Soviet Union as a system—not just the Kremlin, not just Andropov, not just the KGB—but as a system, was geared to expect an attack and to retaliate very quickly to it. It was on hair-trigger alert. It was very nervous and prone to mistakes and accidents... The false alarm that happened on Petrov's watch could not have come at a more dangerous, intense phase in U.S.–Soviet relations.\" At that time, according to Oleg Kalugin, a former KGB chief of foreign counterintelligence, “The danger was in the Soviet leadership thinking, ‘The Americans may attack, so we better attack first.’ ”\n\nPetrov said he did not know whether he should have regarded himself as a hero for what he did that day. In an interview for the film \"The Man Who Saved the World\", Petrov says, \"All that happened didn't matter to me—it was my job. I was simply doing my job, and I was the right person at the right time, that's all. My late wife for 10 years knew nothing about it. 'So what did you do?' she asked me. 'Nothing. I did nothing.\n\n\n"}
{"id": "8635577", "url": "https://en.wikipedia.org/wiki?curid=8635577", "title": "Tetrahydroxyborate", "text": "Tetrahydroxyborate\n\nTetrahydroxyborate (systematically named tetrahydroxyboranuide and tetrahydroxidoborate(1−)) is an inorganic anion with the chemical formula (also written as or ). It contributes no colour to tetrahydroxyborate salts. It is found in the mineral hexahydroborite, Ca(B(OH)) (originally formulated CaBO). It is classified as a weak base.\n\nTetrahydroxyborate can assimilate a proton into the anion by recombination:\n\nBecause of this capture of a proton (), tetrahydroxyborate has Arrhenius-basic character. Its protonation product is boric acid. In aqueous solution, most tetrahydroxyborate ions are undissociated.\n\nIt is a boron oxoanion with a tetrahedral geometry. It is isoelectronic with the hypothetical compound orthocarbonic acid.\n\nTetrahydroxyborate undergoes the typical chemical reactions of a hydroxyborate. Upon treatment with a standard acid, it converts to boric acid and a metal salt. Oxidation of tetrahydroxyborate gives perborate. When heated to a high temperature, tetrahydroxyborate salts decompose to produce metaborate salts and water, or to produce boric acid and a metal hydroxide:\n\nTetrahydroxyborate is produced by boric acid alkalinisation. In this process boric acid and hydroxide anions react to produce tetrahydroxyborate according to the following reaction:\nThis process also involves aquatrihydroxyboron as an intermediate, and occurs in two steps. No catalyst is needed for alkalinisation (step 2).\nCatalytic amounts of water are used for the process. By altering the process conditions, other borate anions may also be produced on the same production site.\n\nTetrahydroxyborate can be used as a cross-link in polymers.\n\nThe tetrahydroxyborate anion is found in Na[B(OH)], Na[B(OH)]Cl and Cu[B(OH)]Cl.\n\n"}
{"id": "11317221", "url": "https://en.wikipedia.org/wiki?curid=11317221", "title": "Trillo Nuclear Power Plant", "text": "Trillo Nuclear Power Plant\n\nTrillo Nuclear Power Plant is a nuclear power station in Spain.\n\nIt consists of one pressurized water reactor (PWR) of 1066 MWe. Construction of unit one began in 1979, and first criticality was on 14 May 1988.\n\nA planned second identical unit was cancelled soon after construction began following a change of government in 1983.\n\n\n"}
{"id": "24305066", "url": "https://en.wikipedia.org/wiki?curid=24305066", "title": "V curve", "text": "V curve\n\nIn power engineering & electrical engineering, V curve is the graph showing the relation of armature current as a function of field current in synchronous machines keeping the load constant. The purpose of the curve is to show the variation in the magnitude of the armature current as the excitation voltage of the machine is varied.\n\nThe Inverted V Curve is a graph showing the relation of power factor as a function of field current.\n\n\nSaadat, Hadi. 2004. \"Power Systems Analysis\". 2nd Ed. McGraw Hill. International Edition.\n"}
{"id": "24151649", "url": "https://en.wikipedia.org/wiki?curid=24151649", "title": "Waste framework directive", "text": "Waste framework directive\n\nThe Waste Framework Directive (WFD) is a European Union Directive of 17 June 2008. The first Waste Framework Directive dates back to 1975 and was substantially amended in 1991.\n\nThe aim of the WFD was to lay the basis to turn the EU into a recycling society.\n\nOne of the features of the WFD is the European Waste Hierarchy.\n"}
{"id": "597674", "url": "https://en.wikipedia.org/wiki?curid=597674", "title": "Woodie (car body style)", "text": "Woodie (car body style)\n\nA woodie is a type of station wagon where the rear car bodywork is constructed of wood or is styled to resemble wood elements.\n\nOriginally, wood framework augmented the car's structure. Over time manufacturers supplanted wood construction with a variety of materials and methods evoking wood construction — including infill metal panels, metal framework, or simulated wood-grain sheet vinyl bordered with three-dimensional, simulated framework. In 2008, wood construction was evoked abstractly on the Ford Flex with a series of side and rear horizontal grooves.\n\nAs a variant of body-on-frame construction, the woodie as a utility vehicle or station wagon originated from the early practice of manufacturing the passenger compartment portion of a vehicle in hardwood. Woodies were popular in the United States and were produced as variants of sedans and convertibles as well as station wagons, from basic to luxury. They were typically manufactured as third-party conversions of regular vehicles—some by large, reputable coachbuilding firms and others by local carpenters and craftsmen for individual customers. They could be austere vehicles, with side curtains in lieu of roll-up windows (e.g., the 1932 Ford)—and sold in limited numbers (e.g., Ford sold 1654 woodie wagons). Eventually, bodies constructed entirely in steel supplanted wood construction—for reasons of strength, cost, safety, and durability.\n\nIn 1950, Plymouth discontinued their woodie station wagon. Buick's 1953 Super Estate Wagon and 1953 Roadmaster Estate Wagon were the last production American station wagons to retain real wood construction. Other marques by then were touting the advantages of \"all-steel\" construction to the buying public. By 1955, only Ford, Mercury, joined in 1965 by Chrysler offered a \"woodie\" appearance, evoking real wood with other materials including steel, plastics and DI-NOC (a vinyl product). As the appearance became popular, Ford, GM, and Chrysler offered multiple models with the woodgrain appearance until the early 1990s.\n\nThe British Motor Corporation (BMC) offered the Morris Minor Traveller (1953–71) with wood structural components and painted aluminum infill panels—the last true mass-produced woodie. Morris' subsequent Mini Traveller (1961–9) employed steel infill panels and faux wood structural members.\n\nAfter the demise of models using actual wood construction, manufacturers continued to evoke wood construction with sheet-vinyl appliques of simulated wood grain, sometimes augmented with three-dimensional, simulated framework, and later by a simple series of indented grooves in the bodywork.\n\nThe 1966 Chevrolet Caprice in its second season, added to the four-door hardtop body style a full line of models including a vinyl-wood trimmed station wagon, the Caprice Estate. Dodge also reintroduced simulated wood the same year.\n\nFord marketed the Ford Pinto Squire with vinyl simulated wood trim in the early 1970s. When Chevrolet proposed a simulated woodgrain option for the Chevy Vega Kammback wagon for the 1973 model year, after a gap of four years of applying woodgrain film on the Caprice, the Vega's production schedule made smooth application of the applique difficult without wrinkles and heavy scrappage — requiring retraining by the film supplier, 3M. Subsequent rebadged variants of the Vega (marketed as \"Woody\"), including the Pontiac Astre Safari, Chevrolet Monza Estate and Pontiac Sunbird Safari, also offered simulated wood trim. Chevrolet offered a simulated woodie version of the Chevette in 1976, and AMC offered the Pacer wagon with optional simulated wood trim in 1977.\n\nFord also marketed version of their Ranchero model, a coupe utility produced between 1957 and 1979 with an open bed like a pickup truck but from a station wagon platform, with simulated woodgrain siding. In 1973, Ford produced a minivan prototype that offered a woodgrain appearance the preceded the Chrysler minivan, called the Ford Carousel, but it was not put into production.\n\nIntroduced in 1981, the Ford Escort and Mercury Lynx four-door wagons offered optional simulated wood trim. GM offered its full-size wagons in wood trim versions until their final year in 1996. From 1982 to 1988, Chrysler used the Town & Country name on a station wagon version of the K-based, front wheel drive LeBaron, featuring plastic woodgrain exterior trim with three dimensional simulated framework. As the station wagon declined in North America, manufacturers offered faux wood trim on SUVs and minivans (e.g., the Jeep Cherokee and Chrysler minivans). Chrysler offered simulated wood as an option for the Chrysler PT Cruiser, introduced in 2000—and aftermarket firms offered kits as well.\n\nJapanese carmakers shied away from the appearance, although Mazda equipped the 1972-1977 Mazda Luce/RX-4 optionally, Honda briefly offered the 1980 Honda Civic station wagon, and Nissan offered the appearance on the 1983-1987 Nissan Cedric V20E SGL and Nissan Gloria V20E SGL top trim package station wagons to Japanese customers only.\n\nIn 2010, George Barris created a woodie version of the Smart Fortwo, an aftermarket firm offered a simulated wood kit for the same car, and GM displayed a prototype woodie version of the forthcoming Chevrolet Spark for the 2010 Paris Motor Show.\n\nIntroduced in 2008, the Ford Flex featured a series of side and rear horizontal grooves intended to evoke a woodie look—without either wood or simulated wood. Car Design News said the styling references \"a previous era without resorting to obvious retro styling cues.\"\n\nColumbia Pictures' top-grossing film for the 1940s--director John Stahl's 1945 \"Leave Her to Heaven\" starring Gene Tierney and Cornel Wilde--features a \"woodie\" station wagon early in the film. Many other American movies from the 1940s also feature woodies.\n\nThe woodie was also closely associated with Surf-rock, e.g., “I bought a ’34 wagon and we call it a woodie\" from the classic Surf City by Jan and Dean or the 1963 instrumental \"Boogie Woodie\" by The Beach Boys.\n\nIn 1995, the U.S. Postal Service issued a 15 cent stamp commemorating the woodie wagon .\n\n\n"}
{"id": "5571261", "url": "https://en.wikipedia.org/wiki?curid=5571261", "title": "World Future Council", "text": "World Future Council\n\nThe World Future Council (WFC) is an independent body formally founded in Hamburg, Germany on 10 May 2007. \"Formed to speak on behalf of policy solutions that serve the interests of future generations\", it includes members active in governmental bodies, civil society, business, science and the arts. The WFC's primary focus has been climate security, promoting laws such as the renewable energy Feed-in tariff. The World Future Council has special consultative status with the Economic and Social Council.\n\nThe World Future Council was founded by the Swedish writer and activist Jakob von Uexkull. The idea for a global council was first aired on German radio in 1998. In October 2004 the organisation began in London with funding from private donors in Germany, Switzerland, USA and the UK. Since 2006, the organisation headquarters is based in Hamburg, where the World Future Council is politically independent and operates and is registered as a charitable foundation. Further offices are located in London, Geneva, Windhoek and Beijing. The Council meets once a year at the Annual General Meeting.\n\nThe Hamburg Call to Action was unanimously agreed upon by all Councillors present at the Founding Ceremony of the World Future Council, 9–13 May 2007. It calls for the preservation of the environment and the health of communities, the promotion of \"systems and institutions based on equity and justice\", safeguarding traditional indigenous tribal rights, the protection of present and future generations from war crimes and crimes against humanity, a sustainable production, trade, financial and monetary system, the revival of local democracies and economies, and a universal ban on nuclear and depleted uranium weapons, cluster ammunition and landmines. It aims to generate governmental support for renewable energy technologies, the protection of forests and oceans, secure healthy food and water supplies, environmental security, healthcare, education and shelter, and a strengthened United Nations.\n\nhttps://www.futurepolicy.org/\n\nThe Future Policy Award celebrates policies that create better living conditions for current and future generations. The aim of the award is to raise global awareness for these exemplary policies and speed up policy action towards just, sustainable and peaceful societies. The Future Policy Award is the first award that celebrates policies rather than people on an international level. Each year the World Future Council chooses one topic on which policy progress is particularly urgent. In 2009, the Future Policy Award highlighted exemplary policies for food security. In the International Year on Biodiversity, the Future Policy Award 2010 celebrated the world's best biodiversity policies. In the International Year of Forests, the Future Policy Award 2011 celebrated successful policies that protect, enhance and sustainably utilize forests for people, and thus contribute to a better world. In 2012, the Future Policy Award celebrated the world’s most inspiring, innovative and influential policies on the protection of oceans and coasts. In 2013 the question was which existing disbarment policies contribute most effectively to the achievement of peace, sustainable development, and security? In 2014the Future Policy Award was dedicated to policies that address one of the most pervasive human rights abuses that humanitz is facing: violence against women and girls. The Future Policy Award in 2015 committed to policies that contributed to protecting and strengthening the rights of boys and girls. In the most recent year, 2017 the Future Policy Award was dedicated to policies that effectively addressed land and soil degradation, and the related risks to food security and livelihoods, and help secure a sustainable and just future for people living in the world's drylands. \n\nFrom 28-30 November 2017, the WFC hosted an international child rights conference in Zanzibar to explore the positive impacts of Zanzibar's Children's Act and share success stories on child protection, children friendly justice and participation from around the world. Across three days, over 100 participants took part in a varied schedule of workshops, presentations and field visits looking at how to translate child rights laws onto paper into national and location programmes that improve the experiences of children and young people on the ground and effectively tackle child abuse, neglect, and exploitation. The conference was convened with the support of Janina Özen-Otto, the JUA Foundation, and the Michael Otto Foundation. \n\nThe Global Policy Action Plan (GPACT) is a set of 22 interlinked, proven policy reforms that together, build sustainable, peaceful, and just societies and help to realise international commitments, including the Sustainable Development Goals (SDG). The \"best\" policies identified by the World Future Council are those that meet the seven Principles for Future-Just Law-Making. A coherent best policy guide that brings together working innovative policy solutions and forward-thinking practical tools. \n\nTogether, the WFC and Hamburg HafenCity University, founded the international Expert Commission on Cities and Climate Change in 2008. The commission comprises 20 experts in the field of regenerative city development including architects, city planners and representatives of UN Habitat, the United Nations Human Settlements Programme. Holding direct advisory mandates for UN Habitat’s World Urban Campaign, the WFC provided the Campaign’s network with a catalogue of criteria for future just policies. \n\nFeed-In Tariff (FIT) laws to speed up renewable energy production have been introduced in several countries e.g. the UK, Australia, several US states, among them California, as well as in Ontario (Canada), with the support of the World Future Council. In establishing the Alliance for Renewable Energy, the World Future Council has created a coalition to spread renewable energies and contributed to the implementation of Feed-in Tariffs in the United States. \n\nWith the support of the UN High Representative for Disarmament Affairs, Sergio Duarte, the World Future Council launched in 2011 the Nuclear Abolition Forum – a joint-project of 8 leading organizations in the disarmament field, hosted by the WFC. The Forum aims to examine critique and debate key nuclear disarmament issues, thereby paving the way for building the framework for achieving and sustaining a nuclear weapon-free world. To this end, it offers a website for posting articles and discussing key nuclear abolition aspects and initiatives, and a periodical which will focus on specific nuclear disarmament themes or elements. \n\nThe World Future Council has undertaken research on exemplary laws and policies implementing the rights of persons with disabilities. Together with the Essl Foundation, the WFC gathered more than 240 decision-makers from 34 countries at the first International Conference on Good Policies for Persons with Disabilities in January 2012 in Vienna, closed with the keynote speech of Dr Christian Strohal, Vice-President of the UN Human Rights Council. The WFC and the Essl Foundation currently agreed a five-year cooperation, under the name Zero Project. \n\nThe World Future Council founded the African Renewable Energy Alliance (AREA) in October 2009. It is a network comprising about 760 members from 70 countries (in February 2012). In order to promote renewable energies in Africa, AREA has co-organized international conferences and meetings on \"Renewable Energy for Sustainable African Development\" in South Africa, Ghana, Cape Verde, Nigeria and Morocco. \n\nThe WFC has embarked on a Campaign for Ombudspersons for Future Generations on all governance levels. For the United Nations Conference on Sustainable Development, or ‘Rio+20’ in Rio de Janeiro, Brazil, in June 2012 the WFC was calling for the establishment of Ombudspersons for Future Generations, as a concrete solution under the second theme of the Summit ‘Institutional Framework for Sustainable Development’. \n\nThe internet platform https://www.futurepolicy.org/ presents political solutions and assists decision-makers in developing and implementing future just policies. It now contains policies, for example on renewable energies, energy efficiency, sustainable cities and food production in the era of climate change, that have been promoted in WFC publications, films and hearings.\n\nThe World Future Council (WFC) consists of 50 eminent global change-makers from governments, parliaments, civil societies, academia, the arts, and the business world. Together they form a voice for the rights of future generations on all five continents.\n\n\n"}
{"id": "34139", "url": "https://en.wikipedia.org/wiki?curid=34139", "title": "Xenon", "text": "Xenon\n\nXenon is a chemical element with symbol Xe and atomic number 54. It is a colorless, dense, odorless noble gas found in the Earth's atmosphere in trace amounts. Although generally unreactive, xenon can undergo a few chemical reactions such as the formation of xenon hexafluoroplatinate, the first noble gas compound to be synthesized.\n\nXenon is used in flash lamps and arc lamps, and as a general anesthetic. The first excimer laser design used a xenon dimer molecule (Xe) as the lasing medium, and the earliest laser designs used xenon flash lamps as pumps. Xenon is used to search for hypothetical weakly interacting massive particles and as the propellant for ion thrusters in spacecraft.\n\nNaturally occurring xenon consists of eight stable isotopes. More than 40 unstable xenon isotopes undergo radioactive decay, and the isotope ratios of xenon are an important tool for studying the early history of the Solar System. Radioactive xenon-135 is produced by beta decay from iodine-135 (a product of nuclear fission), and is the most significant (and unwanted) neutron absorber in nuclear reactors.\n\nXenon was discovered in England by the Scottish chemist William Ramsay and English chemist Morris Travers in September 1898, shortly after their discovery of the elements krypton and neon. They found xenon in the residue left over from evaporating components of liquid air. Ramsay suggested the name \"xenon\" for this gas from the Greek word \"ξένον\" [xenon], neuter singular form of \"ξένος\" [xenos], meaning 'foreign(er)', 'strange(r)', or 'guest'. In 1902, Ramsay estimated the proportion of xenon in the Earth's atmosphere to be one part in 20 million.\n\nDuring the 1930s, American engineer Harold Edgerton began exploring strobe light technology for high speed photography. This led him to the invention of the xenon flash lamp in which light is generated by passing brief electric current through a tube filled with xenon gas. In 1934, Edgerton was able to generate flashes as brief as one microsecond with this method.\n\nIn 1939, American physician Albert R. Behnke Jr. began exploring the causes of \"drunkenness\" in deep-sea divers. He tested the effects of varying the breathing mixtures on his subjects, and discovered that this caused the divers to perceive a change in depth. From his results, he deduced that xenon gas could serve as an anesthetic. Although Russian toxicologist Nikolay V. Lazarev apparently studied xenon anesthesia in 1941, the first published report confirming xenon anesthesia was in 1946 by American medical researcher John H. Lawrence, who experimented on mice. Xenon was first used as a surgical anesthetic in 1951 by American anesthesiologist Stuart C. Cullen, who successfully used it with two patients.\n\nXenon and the other noble gases were for a long time considered to be completely chemically inert and not able to form compounds. However, while teaching at the University of British Columbia, Neil Bartlett discovered that the gas platinum hexafluoride (PtF) was a powerful oxidizing agent that could oxidize oxygen gas (O) to form dioxygenyl hexafluoroplatinate (). Since O and xenon have almost the same first ionization potential, Bartlett realized that platinum hexafluoride might also be able to oxidize xenon. On March 23, 1962, he mixed the two gases and produced the first known compound of a noble gas, xenon hexafluoroplatinate. Bartlett thought its composition to be Xe[PtF], but later work revealed that it was probably a mixture of various xenon-containing salts. Since then, many other xenon compounds have been discovered, in addition to some compounds of the noble gases argon, krypton, and radon, including argon fluorohydride (HArF), krypton difluoride (KrF), and radon fluoride. By 1971, more than 80 xenon compounds were known.\n\nIn November 1989, IBM scientists demonstrated a technology capable of manipulating individual atoms. The program, called IBM in atoms, used a scanning tunneling microscope to arrange 35 individual xenon atoms on a substrate of chilled crystal of nickel to spell out the three letter company initialism. It was the first time atoms had been precisely positioned on a flat surface.\n\nXenon has atomic number 54; that is, its nucleus contains 54 protons. At standard temperature and pressure, pure xenon gas has a density of 5.761 kg/m, about 4.5 times the density of the Earth's atmosphere at sea level, 1.217 kg/m. As a liquid, xenon has a density of up to 3.100 g/mL, with the density maximum occurring at the triple point. Liquid xenon has a high polarizability due to its large atomic volume, and thus is an excellent solvent. It can dissolve hydrocarbons, biological molecules, and even water. Under the same conditions, the density of solid xenon, 3.640 g/cm, is greater than the average density of granite, 2.75 g/cm. Under gigapascals of pressure, xenon forms a metallic phase.\n\nSolid xenon changes from face-centered cubic (fcc) to hexagonal close packed (hcp) crystal phase under pressure and begins to turn metallic at about 140 GPa, with no noticeable volume change in the hcp phase. It is completely metallic at 155 GPa. When metallized, xenon appears sky blue because it absorbs red light and transmits other visible frequencies. Such behavior is unusual for a metal and is explained by the relatively small width of the electron bands in that state.\nLiquid or solid xenon nanoparticles can be formed at room temperature by implanting Xe ions into a solid matrix. Many solids have lattice constants smaller than solid Xe. This results in compression of the implanted Xe to pressures that may be sufficient for its liquefaction or solidification.\n\nXenon is a member of the zero-valence elements that are called noble or inert gases. It is inert to most common chemical reactions (such as combustion, for example) because the outer valence shell contains eight electrons. This produces a stable, minimum energy configuration in which the outer electrons are tightly bound.\n\nIn a gas-filled tube, xenon emits a blue or lavenderish glow when excited by electrical discharge. Xenon emits a band of emission lines that span the visual spectrum, but the most intense lines occur in the region of blue light, producing the coloration.\n\nXenon is a trace gas in Earth's atmosphere, occurring at (parts per billion), or approximately 1 part per 11.5 million. It is also found as a component of gases emitted from some mineral springs.\n\nXenon is obtained commercially as a by-product of the separation of air into oxygen and nitrogen. After this separation, generally performed by fractional distillation in a double-column plant, the liquid oxygen produced will contain small quantities of krypton and xenon. By additional fractional distillation, the liquid oxygen may be enriched to contain 0.1–0.2% of a krypton/xenon mixture, which is extracted either by absorption onto silica gel or by distillation. Finally, the krypton/xenon mixture may be separated into krypton and xenon by further distillation. Worldwide production of xenon in 1998 was estimated at 5,000–7,000 m. Because of its scarcity, xenon is much more expensive than the lighter noble gases—approximate prices for the purchase of small quantities in Europe in 1999 were 10 €/L for xenon, 1 €/L for krypton, and 0.20 €/L for neon, while the much more plentiful argon costs less than a cent per liter.\n\nWithin the Solar System, the nucleon fraction of xenon is , for an abundance of approximately one part in 630 thousand of the total mass. Xenon is relatively rare in the Sun's atmosphere, on Earth, and in asteroids and comets. The abundance of xenon in the atmosphere of planet Jupiter is unusually high, about 2.6 times that of the Sun. This abundance remains unexplained, but may have been caused by an early and rapid buildup of planetesimals—small, subplanetary bodies—before the heating of the presolar disk. (Otherwise, xenon would not have been trapped in the planetesimal ices.) The problem of the low terrestrial xenon may be explained by covalent bonding of xenon to oxygen within quartz, reducing the outgassing of xenon into the atmosphere.\n\nUnlike the lower-mass noble gases, the normal stellar nucleosynthesis process inside a star does not form xenon. Elements more massive than iron-56 consume energy through fusion, and the synthesis of xenon represents no energy gain for a star. Instead, xenon is formed during supernova explosions, in classical nova explosions, by the slow neutron-capture process (s-process) in red giant stars that have exhausted their core hydrogen and entered the asymptotic giant branch, and from radioactive decay, for example by beta decay of extinct iodine-129 and spontaneous fission of thorium, uranium, and plutonium.\n\nNaturally occurring xenon is composed of eight stable isotopes. This is more than any other element except tin, which has ten. The isotopes Xe and Xe are predicted by theory to undergo double beta decay, but this has never been observed so they are considered stable. In addition, more than 40 unstable isotopes that have been studied. The longest lived of these isotopes is Xe, which undergoes double beta decay with a half-life of . Xe is produced by beta decay of I, which has a half-life of 16 million years. Xe, Xe, Xe, and Xe are some of the fission products of U and Pu, and are used to detect and monitor nuclear explosions.\n\nNuclei of two of the stable isotopes of xenon, Xe and Xe, have non-zero intrinsic angular momenta (nuclear spins, suitable for nuclear magnetic resonance). The nuclear spins can be aligned beyond ordinary polarization levels by means of circularly polarized light and rubidium vapor. The resulting spin polarization of xenon nuclei can surpass 50% of its maximum possible value, greatly exceeding the thermal equilibrium value dictated by paramagnetic statistics (typically 0.001% of the maximum value at room temperature, even in the strongest magnets). Such non-equilibrium alignment of spins is a temporary condition, and is called \"hyperpolarization\". The process of hyperpolarizing the xenon is called \"optical pumping\" (although the process is different from pumping a laser).\n\nBecause a Xe nucleus has a spin of 1/2, and therefore a zero electric quadrupole moment, the Xe nucleus does not experience any quadrupolar interactions during collisions with other atoms, and the hyperpolarization persists for long periods even after the engendering light and vapor have been removed. Spin polarization of Xe can persist from several seconds for xenon atoms dissolved in blood to several hours in the gas phase and several days in deeply frozen solid xenon. In contrast, Xe has a nuclear spin value of and a nonzero quadrupole moment, and has t relaxation times in the millisecond and second ranges.\n\nSome radioactive isotopes of xenon (for example, Xe and Xe) are produced by neutron irradiation of fissionable material within nuclear reactors. Xe is of considerable significance in the operation of nuclear fission reactors. Xe has a huge cross section for thermal neutrons, 2.6×10 barns, and operates as a neutron absorber or \"poison\" that can slow or stop the chain reaction after a period of operation. This was discovered in the earliest nuclear reactors built by the American Manhattan Project for plutonium production. However, the designers had made provisions in the design to increase the reactor's reactivity (the number of neutrons per fission that go on to fission other atoms of nuclear fuel).\nXe reactor poisoning was a major factor in the Chernobyl disaster. A shutdown or decrease of power of a reactor can result in buildup of Xe, with reactor operation going into a condition known as the iodine pit.\n\nUnder adverse conditions, relatively high concentrations of radioactive xenon isotopes may emanate from cracked fuel rods, or fissioning of uranium in cooling water.\n\nBecause xenon is a tracer for two parent isotopes, xenon isotope ratios in meteorites are a powerful tool for studying the formation of the Solar System. The iodine–xenon method of dating gives the time elapsed between nucleosynthesis and the condensation of a solid object from the solar nebula. In 1960, physicist John H. Reynolds discovered that certain meteorites contained an isotopic anomaly in the form of an overabundance of xenon-129. He inferred that this was a decay product of radioactive iodine-129. This isotope is produced slowly by cosmic ray spallation and nuclear fission, but is produced in quantity only in supernova explosions. Because the half-life of I is comparatively short on a cosmological time scale (16 million years), this demonstrated that only a short time had passed between the supernova and the time the meteorites had solidified and trapped the I. These two events (supernova and solidification of gas cloud) were inferred to have happened during the early history of the Solar System, because the I isotope was likely generated shortly before the Solar System was formed, seeding the solar gas cloud with isotopes from a second source. This supernova source may also have caused collapse of the solar gas cloud.\n\nIn a similar way, xenon isotopic ratios such as Xe/Xe and Xe/Xe are a powerful tool for understanding planetary differentiation and early outgassing. For example, the atmosphere of Mars shows a xenon abundance similar to that of Earth (0.08 parts per million) but Mars shows a greater abundance of Xe than the Earth or the Sun. Since this isotope is generated by radioactive decay, the result may indicate that Mars lost most of its primordial atmosphere, possibly within the first 100 million years after the planet was formed. In another example, excess Xe found in carbon dioxide well gases from New Mexico is believed to be from the decay of mantle-derived gases from soon after Earth's formation.\n\nAfter Neil Bartlett's discovery in 1962 that xenon can form chemical compounds, a large number of xenon compounds have been discovered and described. Almost all known xenon compounds contain the electronegative atoms fluorine or oxygen. The chemistry of xenon in each oxidation state is analogous to that of the neighboring element iodine in the immediately lower oxidation state.\n\nThree fluorides are known: , , and . XeF is theorized to be unstable. These are the starting points for the synthesis of almost all xenon compounds.\n\nThe solid, crystalline difluoride is formed when a mixture of fluorine and xenon gases is exposed to ultraviolet light. The ultraviolet component of ordinary daylight is sufficient. Long-term heating of at high temperatures under an catalyst yields . Pyrolysis of in the presence of NaF yields high-purity .\n\nThe xenon fluorides behave as both fluoride acceptors and fluoride donors, forming salts that contain such cations as and , and anions such as , , and . The green, paramagnetic is formed by the reduction of by xenon gas.\n\nWhereas the xenon fluorides are well characterized, with the exception of dichloride XeCl, the other halides are not known. Xenon dichloride, formed by the high-frequency irradiation of a mixture of xenon, fluorine, and silicon or carbon tetrachloride, is reported to be an endothermic, colorless, crystalline compound that decomposes into the elements at 80 °C. However, may be merely a van der Waals molecule of weakly bound Xe atoms and molecules and not a real compound. Theoretical calculations indicate that the linear molecule is less stable than the van der Waals complex.\n\nThree oxides of xenon are known: xenon trioxide () and xenon tetroxide (), both of which are dangerously explosive and powerful oxidizing agents, and xenon dioxide (XeO), which was reported in 2011 with a coordination number of four. XeO forms when xenon tetrafluoride is poured over ice. Its crystal structure may allow it to replace silicon in silicate minerals. The XeOO cation has been identified by infrared spectroscopy in solid argon.\n\nXenon does not react with oxygen directly; the trioxide is formed by the hydrolysis of :\n\nBarium perxenate, when treated with concentrated sulfuric acid, yields gaseous xenon tetroxide:\n\nTo prevent decomposition, the xenon tetroxide thus formed is quickly cooled to form a pale-yellow solid. It explodes above −35.9 °C into xenon and oxygen gas.\n\nA number of xenon oxyfluorides are known, including , , , and . is formed by reacting with xenon gas at low temperatures. It may also be obtained by partial hydrolysis of . It disproportionates at −20 °C into and . is formed by the partial hydrolysis of , or the reaction of with sodium perxenate, . The latter reaction also produces a small amount of . reacts with CsF to form the anion, while XeOF reacts with the alkali metal fluorides KF, RbF and CsF to form the anion.\n\nRecently, there has been an interest in xenon compounds where xenon is directly bonded to a less electronegative element than fluorine or oxygen, particularly carbon. Electron-withdrawing groups, such as groups with fluorine substitution, are necessary to stabilize these compounds. Numerous such compounds have been characterized, including:\n\nOther compounds containing xenon bonded to a less electronegative element include and . The latter is synthesized from dioxygenyl tetrafluoroborate, , at −100 °C.\n\nAn unusual ion containing xenon is the tetraxenonogold(II) cation, , which contains Xe–Au bonds. This ion occurs in the compound , and is remarkable in having direct chemical bonds between two notoriously unreactive atoms, xenon and gold, with xenon acting as a transition metal ligand.\n\nThe compound contains a Xe–Xe bond, the longest element-element bond known (308.71 pm = 3.0871 Å).\n\nIn 1995, M. Räsänen and co-workers, scientists at the University of Helsinki in Finland, announced the preparation of xenon dihydride (HXeH), and later xenon hydride-hydroxide (HXeOH), hydroxenoacetylene (HXeCCH), and other Xe-containing molecules. In 2008, Khriachtchev \"et al.\" reported the preparation of HXeOXeH by the photolysis of water within a cryogenic xenon matrix. Deuterated molecules, HXeOD and DXeOH, have also been produced.\n\nIn addition to compounds where xenon forms a chemical bond, xenon can form clathrates—substances where xenon atoms or pairs are trapped by the crystalline lattice of another compound. One example is xenon hydrate (Xe•5.75 HO), where xenon atoms occupy vacancies in a lattice of water molecules. This clathrate has a melting point of 24 °C. The deuterated version of this hydrate has also been produced. Another example is xenon hydride (Xe(H)), in which xenon pairs (dimers) are trapped inside solid hydrogen. Such clathrate hydrates can occur naturally under conditions of high pressure,\nsuch as in Lake Vostok underneath the Antarctic ice sheet. Clathrate formation can be used to fractionally distill xenon, argon and krypton.\n\nXenon can also form endohedral fullerene compounds, where a xenon atom is trapped inside a fullerene molecule. The xenon atom trapped in the fullerene can be observed by Xe nuclear magnetic resonance (NMR) spectroscopy. Through the sensitive chemical shift of the xenon atom to its environment, chemical reactions on the fullerene molecule can be analyzed. These observations are not without caveat, however, because the xenon atom has an electronic influence on the reactivity of the fullerene.\n\nWhen xenon atoms are in the ground energy state, they repel each other and will not form a bond. When xenon atoms becomes energized, however, they can form an excimer (excited dimer) until the electrons return to the ground state. This entity is formed because the xenon atom tends to complete the outermost electronic shell by adding an electron from a neighboring xenon atom. The typical lifetime of a xenon excimer is 1–5 ns, and the decay releases photons with wavelengths of about 150 and 173 nm. Xenon can also form excimers with other elements, such as the halogens bromine, chlorine, and fluorine.\n\nAlthough xenon is rare and relatively expensive to extract from the Earth's atmosphere, it has a number of applications.\n\nXenon is used in light-emitting devices called xenon flash lamps, used in photographic flashes and stroboscopic lamps; to excite the active medium in lasers which then generate coherent light; and, occasionally, in bactericidal lamps. The first solid-state laser, invented in 1960, was pumped by a xenon flash lamp, and lasers used to power inertial confinement fusion are also pumped by xenon flash lamps.\n\nContinuous, short-arc, high pressure xenon arc lamps have a color temperature closely approximating noon sunlight and are used in solar simulators. That is, the chromaticity of these lamps closely approximates a heated black body radiator at the temperature of the Sun. First introduced in the 1940s, these lamps replaced the shorter-lived carbon arc lamps in movie projectors. They are also employed in typical 35mm, IMAX, and digital film projection systems. They are an excellent source of short wavelength ultraviolet radiation and have intense emissions in the near infrared used in some night vision systems. Xenon is used as a starter gas in HID automotive headlights, and high-end \"tactical\" flashlights.\n\nThe individual cells in a plasma display contain a mixture of xenon and neon ionized with electrodes. The interaction of this plasma with the electrodes generates ultraviolet photons, which then excite the phosphor coating on the front of the display.\n\nXenon is used as a \"starter gas\" in high pressure sodium lamps. It has the lowest thermal conductivity and lowest ionization potential of all the non-radioactive noble gases. As a noble gas, it does not interfere with the chemical reactions occurring in the operating lamp. The low thermal conductivity minimizes thermal losses in the lamp while in the operating state, and the low ionization potential causes the breakdown voltage of the gas to be relatively low in the cold state, which allows the lamp to be more easily started.\n\nIn 1962, a group of researchers at Bell Laboratories discovered laser action in xenon, and later found that the laser gain was improved by adding helium to the lasing medium. The first excimer laser used a xenon dimer (Xe) energized by a beam of electrons to produce stimulated emission at an ultraviolet wavelength of 176 nm.\nXenon chloride and xenon fluoride have also been used in excimer (or, more accurately, exciplex) lasers.\n\nXenon has been used as a general anesthetic. Although it is expensive, anesthesia machines that can deliver xenon are expected to appear on the European market because advances in recovery and recycling of xenon have made it economically viable.\n\nXenon interacts with many different receptors and ion channels, and like many theoretically multi-modal inhalation anesthetics, these interactions are likely complementary. Xenon is a high-affinity glycine-site NMDA receptor antagonist. However, xenon is different from certain other NMDA receptor antagonists in that it is not neurotoxic and it inhibits the neurotoxicity of ketamine and nitrous oxide, while actually producing neuroprotective effects. Unlike ketamine and nitrous oxide, xenon does not stimulate a dopamine efflux in the nucleus accumbens. Like nitrous oxide and cyclopropane, xenon activates the two-pore domain potassium channel TREK-1. A related channel TASK-3 also implicated in the actions of inhalation anesthetics is insensitive to xenon. Xenon inhibits nicotinic acetylcholine αβ receptors which contribute to spinally mediated analgesia. Xenon is an effective inhibitor of plasma membrane Ca ATPase. Xenon inhibits Ca ATPase by binding to a hydrophobic pore within the enzyme and preventing the enzyme from assuming active conformations.\n\nXenon is a competitive inhibitor of the serotonin 5-HT receptor. While neither anesthetic nor antinociceptive, this reduces anesthesia-emergent nausea and vomiting.\n\nXenon has a minimum alveolar concentration (MAC) of 72% at age 40, making it 44% more potent than NO as an anesthetic. Thus, it can be used with oxygen in concentrations that have a lower risk of hypoxia. Unlike nitrous oxide (NO), xenon is not a greenhouse gas and is viewed as environmentally friendly. Though recycled in modern systems, xenon vented to the atmosphere is only returning to its original source, without environmental impact.\n\nXenon induces robust cardioprotection and neuroprotection through a variety of mechanisms. Through its influence on Ca, K, KATP\\HIF, and NMDA antagonism, xenon is neuroprotective when administered before, during and after ischemic insults. Xenon is a high affinity antagonist at the NMDA receptor glycine site. Xenon is cardioprotective in ischemia-reperfusion conditions by inducing pharmacologic non-ischemic preconditioning. Xenon is cardioprotective by activating PKC-epsilon and downstream p38-MAPK. Xenon mimics neuronal ischemic preconditioning by activating ATP sensitive potassium channels. Xenon allosterically reduces ATP mediated channel activation inhibition independently of the sulfonylurea receptor1 subunit, increasing KATP open-channel time and frequency.\n\nInhaling a xenon/oxygen mixture activates production of the transcription factor HIF-1-alpha, which may lead to increased production of erythropoietin. The latter hormone is known to increase red blood cell production and athletic performance. Reportedly, doping with xenon inhalation has been used in Russia since 2004 and perhaps earlier. On August 31, 2014, the World Anti Doping Agency (WADA) added xenon (and argon) to the list of prohibited substances and methods, although no reliable doping tests for these gases have yet been developed. In addition, effects of xenon on erythropoietin production in humans have not been demonstrated, so far.\n\nGamma emission from the radioisotope Xe of xenon can be used to image the heart, lungs, and brain, for example, by means of single photon emission computed tomography. Xe has also been used to measure blood flow.\n\nXenon, particularly hyperpolarized Xe, is a useful contrast agent for magnetic resonance imaging (MRI). In the gas phase, it can image cavities in a porous sample, alveoli in lungs, or the flow of gases within the lungs. Because xenon is soluble both in water and in hydrophobic solvents, it can image various soft living tissues.\n\nThe xenon chloride excimer laser has certain dermatological uses.\n\nBecause of the xenon atom's large, flexible outer electron shell, the NMR spectrum changes in response to surrounding conditions and can be used to monitor the surrounding chemical circumstances. For instance, xenon dissolved in water, xenon dissolved in hydrophobic solvent, and xenon associated with certain proteins can be distinguished by NMR.\n\nHyperpolarized xenon can be used by surface chemists. Normally, it is difficult to characterize surfaces with NMR because signals from a surface are overwhelmed by signals from the atomic nuclei in the bulk of the sample, which are much more numerous than surface nuclei. However, nuclear spins on solid surfaces can be selectively polarized by transferring spin polarization to them from hyperpolarized xenon gas. This makes the surface signals strong enough to measure and distinguish from bulk signals.\n\nIn nuclear energy studies, xenon is used in bubble chambers, probes, and in other areas where a high molecular weight and inert chemistry is desirable. A by-product of nuclear weapon testing is the release of radioactive xenon-133 and xenon-135. These isotopes are monitored to ensure compliance with nuclear test ban treaties, and to confirm nuclear tests by states such as North Korea.\nLiquid xenon is used in calorimeters to measure gamma rays, and as a detector of hypothetical weakly interacting massive particles, or WIMPs. When a WIMP collides with a xenon nucleus, theory predicts it will impart enough energy to cause ionization and scintillation. Liquid xenon is useful for these experiments because its density makes dark matter interaction more likely and it permits a quiet detector through self-shielding.\n\nXenon is the preferred propellant for ion propulsion of spacecraft because it has low ionization potential per atomic weight and can be stored as a liquid at near room temperature (under high pressure), yet easily evaporated to feed the engine. Xenon is inert, environmentally friendly, and less corrosive to an ion engine than other fuels such as mercury or caesium. Xenon was first used for satellite ion engines during the 1970s. It was later employed as a propellant for JPL's Deep Space 1 probe, Europe's SMART-1 spacecraft and for the three ion propulsion engines on NASA's Dawn Spacecraft.\n\nChemically, the perxenate compounds are used as oxidizing agents in analytical chemistry. Xenon difluoride is used as an etchant for silicon, particularly in the production of microelectromechanical systems (MEMS). The anticancer drug 5-fluorouracil can be produced by reacting xenon difluoride with uracil. Xenon is also used in protein crystallography. Applied at pressures from 0.5 to 5 MPa (5 to 50 atm) to a protein crystal, xenon atoms bind in predominantly hydrophobic cavities, often creating a high-quality, isomorphous, heavy-atom derivative that can be used for solving the phase problem.\n\nBecause they are strongly oxidative, many oxygen-xenon compounds are toxic; they are also explosive (highly exothermic), breaking down to elemental xenon and diatomic oxygen (O) with much stronger chemical bonds than the xenon compounds.\n\nXenon gas can be safely kept in normal sealed glass or metal containers at standard temperature and pressure. However, it readily dissolves in most plastics and rubber, and will gradually escape from a container sealed with such materials. Xenon is non-toxic, although it does dissolve in blood and belongs to a select group of substances that penetrate the blood–brain barrier, causing mild to full surgical anesthesia when inhaled in high concentrations with oxygen.\n\nThe speed of sound in xenon gas (169 m/s) is less than that in air because the average velocity of the heavy xenon atoms is less than that of nitrogen and oxygen molecules in air. Hence, xenon vibrates more slowly in the vocal cords when exhaled and produces lowered voice tones, an effect opposite to the high-toned voice produced in helium. Like helium, xenon does not satisfy the body's need for oxygen, and it is both a simple asphyxiant and an anesthetic more powerful than nitrous oxide; consequently, and because xenon is expensive, many universities have prohibited the voice stunt as a general chemistry demonstration. The gas sulfur hexafluoride is similar to xenon in molecular weight (146 versus 131), less expensive, and though an asphyxiant, not toxic or anesthetic; it is often substituted in these demonstrations.\n\nDense gases such as xenon and sulfur hexafluoride can be breathed safely when mixed with at least 20% oxygen. Xenon at 80% concentration along with 20% oxygen rapidly produces the unconsciousness of general anesthesia (and has been used for this, as discussed above). Breathing mixes gases of different densities very effectively and rapidly so that heavier gases are purged along with the oxygen, and do not accumulate at the bottom of the lungs. There is, however, a danger associated with any heavy gas in large quantities: it may sit invisibly in a container, and a person who enters an area filled with an odorless, colorless gas may be asphyxiated without warning. Xenon is rarely used in large enough quantities for this to be a concern, though the potential for danger exists any time a tank or container of xenon is kept in an unventilated space.\n\n"}
