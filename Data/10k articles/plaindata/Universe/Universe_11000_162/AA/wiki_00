{"id": "17733182", "url": "https://en.wikipedia.org/wiki?curid=17733182", "title": "1958 BOAC Bristol Britannia crash", "text": "1958 BOAC Bristol Britannia crash\n\nG-AOVD was a Bristol Britannia 312 operated by BOAC which crashed near Christchurch, Dorset in the south of England on Christmas Eve 1958, killing two of the five crew and all seven passengers.\n\nOn 24 December 1958 much of the south of England was covered in thick fog making travel by any means hazardous. Many aircraft had to be diverted as visibility was below the minimum permissible distance at most of the airports on the south coast. To a pilot who was less than aware of the conditions on the ground and the altitude at which they were flying, this fog would have an appearance very similar to normal cloud cover. For the pilots of G-AOVD this may have added to the illusion that they were at a much higher altitude and that they were reading the instruments correctly.\n\nThe aircraft departed from London Heathrow Airport at 10:10 am on a test flight to renew its certificate of airworthiness with 12 persons aboard including five crew. After completing the test, at approximately 11:55 am, the crew requested clearance to descend from 12,000 feet to 3,000 feet for approach to Hurn Airport, possibly as an alternate destination due to poor weather at Heathrow. Approximately 3 minutes later, at 11:58 am, Hurn Airport lost contact with the aircraft as it struck the ground, crossing a road into a ploughed field, bringing down telephone lines and trees and alerting residents in the nearby villages. Upon realising they had lost contact with the aircraft, the controller at Hurn contacted the emergency services giving the last known position before contact was lost. Likewise the residents of Winkton, Sopley, and people living near Bransgore contacted emergency services saying they believed that they had heard the sound of a low flying aircraft and the sound of a crash.\n\nAround midday, the members of the volunteer fire service in Burley and Christchurch were alerted by an air raid siren calling them to the station to respond to the report of the crash. The initial report from Hurn Airport stated that they were unaware of the type of aircraft involved or how many passengers were being carried, and that they believed the aircraft was to the north of the airport when it crashed. However, on receiving updated information on the reports from Winkton and Sopley the fire crews decided to start the search for the aircraft in that area.\n\nThe fire service searching in Winkton discovered the location of the wreckage after travelling a short distance along the Burley Road and finding telephone poles and cables which had been broken and dragged into a field off the road. A foot search was mounted and eventually the crew spotted some broken trees along with aircraft debris and a fire. The crew chief sent a message to fire control to confirm the location of the crash and set up a rendezvous at a local public house to give emergency services a positive location. Another appliance which had been sent to Sopley to search there could not be contacted as it was not fitted with a radio; fortunately, however, its crew encountered other appliances heading towards the incident, and were then informed of the location.\n\nAnother hindrance to the emergency effort was the lack of a four-wheel drive appliance which meant that the firemen had to attempt to drive an eight-ton vehicle over a ploughed field, which delayed the rescue. While this was going on, the crew chief and some of the crew from the first appliance on the scene continued to search on foot and eventually found the remains of the cockpit with the injured co-pilot trapped inside. They began to cut him free and as further emergency services arrived on the scene, a coordinated search and rescue effort was mounted over the site, fanning out and finding a further two survivors. The fire station was eventually able to confirm what aircraft had been involved and the number of people on board at the time. Having received this information the emergency services were able to account for all the people involved and to continue putting out the fires.\n\nThe crash was attributed to a failure on the part of the captain and first officer to correctly establish the altitude of the aircraft before and during the descent. The Britannia aircraft was fitted with a three-handed altimeter which required a higher degree of concentration to read correctly than was desirable. The crew misread the instrument believing that they were at 11,500 feet when they began descending, when in fact they were at only 1,500 feet. As a result, they flew the aircraft into the ground which was obscured by fog at the time. The type of flight in which the aircraft was engaged was also thought to be a contributing factor.\n\nIt was concluded that this crash was a controlled flight into terrain (CFIT) and that there were no defects with the aircraft or its systems which contributed to the crash. For this the failure to read the instruments correctly rests with the captain. This was not the first crash involving a crew misreading this type of altimeter in this long distance, high altitude aircraft. As a direct result of this and other similar incidents, altimeters would now be required to display a cross-hatch or chequered flag when indicating an altitude below 1500 feet. Furthermore, all fire appliances in Christchurch would now be fitted with radios for improved communication, and when four-wheel drive appliances became available, Christchurch was one of the first rural stations to be allocated one.\n\nAs a result of the accident the Ministry of Transport issued a directive to replace all three-pointer altimeters in British registered aircraft which operated at over 20,000 feet before September 1959. This followed an investigation of the problems of interpretation of the display. An interim flight safety warning was also issued pending altimeter replacement which described the risk of misreading these altimeters as \"most likely when the routine monitoring of the instrument panel has been interrupted. If this happened during climb or descent the height when the instruments are rescanned may be very different from the anticipated.\"\n\n\n"}
{"id": "25697878", "url": "https://en.wikipedia.org/wiki?curid=25697878", "title": "Airborne Launch Control System", "text": "Airborne Launch Control System\n\nThe Airborne Launch Control System (ALCS) provides a survivable launch capability for the United States Air Force's LGM-30G Minuteman III intercontinental ballistic missile (ICBM) force. The ALCS is operated by airborne missileers from Air Force Global Strike Command's (AFGSC) 625th Strategic Operations Squadron (STOS) and United States Strategic Command (USSTRATCOM). The weapon system is located onboard the United States Navy's E-6B Mercury, which serves as USSTRATCOM's \"Looking Glass\" Airborne Command Post (ABNCP). The ALCS crew is integrated into the ABNCP battle staff and is on alert around-the-clock.\n\nIn 1962, when the Minuteman ICBM was first placed on alert, the Soviet Union did not have the number of weapons, accuracy, nor significant nuclear yield to completely destroy the Minuteman ICBM force during an attack. However, Strategic Air Command (SAC) planners knew it was only a matter of time before the Soviets could have such capability. Early on, the Air Force experimented with using trains to make the Minuteman ICBMs mobile, and therefore more survivable. However, the Air Force decided to scrap the mobile Minuteman ICBM concept and emplace Minuteman in 1000 missile silos along with their 100 associated LCCs. Each facility was spread out several miles apart from each other so that the Soviets could not destroy multiple sites with just one nuclear warhead.\n\nStarting in the mid-1960s, the Soviets began to gain parity with the US and now had the potential capability to target and successfully attack the Minuteman force with an increased number of ICBMs that had greater yields and accuracy than were previously available. Studying the problem even more, SAC realized that in order to prevent the US from launching all 1000 Minuteman ICBMs, the Soviets did not have to target all 1000 Minuteman missile silos. The Soviets only needed to launch a disarming decapitation strike against the 100 Minuteman LCCs - the command and control sites - in order to prevent the launch of all Minuteman ICBMs. Even though the Minuteman ICBMs would have been left unscathed in their missile silos following an LCC decapitation strike, the Minuteman missiles could not be launched without a command and control capability. In other words, the Soviets only needed 100 warheads to fully eliminate command and control of the Minuteman ICBMs. Even if the Soviets chose to expend two to three warheads per LCC for assured damage expectancy, the Soviets would only have had to expend up to 300 warheads to disable the Minuteman ICBM force - far less than the total number of Minuteman silos. The Soviets could have then used the remaining warheads to strike other targets they chose.\n\nFaced with only a few Minuteman LCC targets, the Soviets could have concluded that the odds of being successful in a Minuteman LCC decapitation strike were higher with less risk than it would have been having to face the almost insurmountable task of successfully attacking and destroying 1000 Minuteman silos and 100 Minuteman LCCs to ensure Minuteman was disabled. This theory motivated SAC to design a survivable means to launch Minuteman, even if all the ground-based command and control sites were destroyed.\n\nBorn out of an original concept to have an airborne launch capability from SAC’s Looking Glass aircraft to launch the Emergency Rocket Communication System (ERCS), SAC modified this plan to launch the entire Minuteman force from the air as well. The ERCS was an Ultra High Frequency (UHF) communications package placed on top of a Minuteman II ICBM in place of its nuclear warhead. Missile crews could record an Emergency Action Message (EAM) into the ERCS and launch the missile on a lofted trajectory broadcasting the recorded EAM below to any available strategic forces. Placed on a modified EC-135 command post aircraft and thoroughly tested, the ALCS demonstrated its capability on 17 April 1967 by launching an ERCS configured Minuteman II out of Vandenberg AFB, CA. This first test launch using the ALCS and ERCS shows the original roots of ALCS. However, all Minuteman ICBM sites were modified and built to have the capability to receive commands from ALCS.\n\nAfter successfully demonstrating the ALCS could launch a Minuteman ICBM from the air, ALCS achieved Initial Operational Capability (IOC) on 31 May 1967. From that point on, airborne missileers stood alert with ALCS-capable EC-135 aircraft for several decades. Over the years, ALCS operations were adapted in order to ensure the most effective use of this survivable ICBM launch capability. With ALCS now standing alert around-the-clock, the Soviets could no longer successfully launch a Minuteman LCC decapitation strike. Even if the Soviets attempted to do so, EC-135s equipped with the ALCS could fly overhead and launch the remaining Minuteman ICBMs in retaliation.\n\nNow that ALCS was on alert, this complicated Soviet war planning by forcing the Soviets to not only target the 100 LCCs, but also the 1000 silos with more than one warhead in order to guarantee destruction. This would have required upwards of 3000 warheads to complete such an attack. The odds of being successful in such an attack on the Minuteman ICBM force would have been extremely low. What is more, the Soviets would have been faced with attacking the remainder of the US nuclear triad, which would have made the odds even lower. Therefore, the mission of ALCS and the nuclear triad was to deter the Soviets from launching any attack in the first place. This deterrence mission continues to this day.\n\nFrom 1967 to 1970, ALCS missile crews belonged to the 68th Strategic Missile Squadron (SMS) at Ellsworth AFB, SD and 91st Strategic Missile Wing (SMW) at Minot AFB, ND. ALCS equipment was installed on various EC-135 variants to include the EC-135A, EC-135C, EC-135G, and for a short while on the EC-135L. From 1967 to 1970 these aircraft belonged to the 28th Air Refueling Squadron (AREFS) at Ellsworth AFB, SD, the 906th AREFS at Minot AFB, ND, and the 38th Strategic Reconnaissance Squadron (SRS) at [Offutt AFB], NE.\n\nStarting in 1970, there were only two SAC squadrons that operated ALCS-capable aircraft. This included the 2nd Airborne Command and Control Squadron (ACCS) operating EC-135C aircraft out of Offutt AFB, NE and the 4th ACCS operating EC-135A, EC-135C, and EC-135G aircraft out of Ellsworth AFB, SD. All three variants of these EC-135A/C/G aircraft had ALCS equipment installed onboard.\n\nThe 4th ACCS was the workhorse of ALCS operations. Three dedicated Airborne Launch Control Centers (ALCC) (pronounced “Al-see”), designated ALCC No. 1, ALCC No. 2, and ALCC No. 3 were on ground alert around-the-clock providing ALCS coverage for five of the six Minuteman ICBM Wings. These dedicated ALCCs were mostly EC-135A aircraft but sometimes were EC-135C or EC-135G aircraft, depending on availability. ALCC No. 1 was on ground alert at Ellsworth AFB, SD and during a wartime scenario, its role would have been to take off and orbit between the Minuteman Wings at Ellsworth AFB, SD and F.E. Warren AFB, WY, providing ALCS assistance if needed. ALCCs No. 2 and No. 3 were routinely on forward deployed ground alert at Minot AFB, ND. During a wartime scenario, ALCC No. 3’s role would have been to take off and orbit between the Minuteman ICBM Wings at Minot AFB, ND and Grand Forks AFB, ND, providing ALCS assistance if needed. ALCC No. 2’s dedicated role was to take off and orbit near the Minuteman ICBM Wing at Malmstrom AFB, MT, providing ALCS assistance if needed. The 4th ACCS also maintained an EC-135C or EC-135G on ground alert at Ellsworth as the West Auxiliary Airborne Command Post (WESTAUXCP), which was a backup to SAC’s Looking Glass Airborne Command Post (ABNCP), as well as a radio relay link between the Looking Glass and ALCCs when airborne. Although equipped with ALCS, the WESTAUXCP did not have a dedicated Minuteman ICBM wing to provide ALCS assistance to.\n\nThe 2nd ACCS was another major player in ALCS operations. The primary mission of the 2nd ACCS was to fly the SAC ABNCP Looking Glass aircraft in continuous airborne operations. However, due to its close proximity in orbiting over the central US, the airborne Looking Glass provided ALCS coverage for the Minuteman Wing located at Whiteman AFB, MO. Not only did Whiteman AFB have Minuteman II ICBMs, but it also had ERCS configured Minuteman missiles on alert. The 2 ACCS also had an additional EC-135C on ground alert at Offutt AFB, NE as the EASTAUXCP, providing backup to the airborne Looking Glass, radio relay capability, and a means for the Commander in Chief of SAC to escape an enemy nuclear attack. Although the EASTAUXCP was ALCS capable, it did not have a dedicated ALCS mission.\n\nFor a short time in the early 1980s, one E-4B National Emergency Airborne Command Post (NEACP) also had ALCS equipment installed onboard during a test proof-of-concept program. Originally, SAC planned for a whole fleet of E-4Bs to replace all of the existing EC-135 Looking Glass aircraft in a program known as the Advanced Airborne Command Post. During the test trial, an E-4B, with a full SAC battlestaff and ALCS crew onboard, periodically flew Looking Glass missions out of Offutt AFB, NE in order to ascertain the feasibility of replacing the EC-135 fleet. In the end however, too many resources were needed and it was deemed too expensive to have the E-4B replace the EC-135. The ALCS equipment was subsequently removed from the E-4B and it continued the NEACP mission while the various EC-135s continued to perform the ABNCP and ALCC missions.\n\nNeedless to say, this was the heyday of ALCS operations. Around-the-clock, there were three dedicated ALCCs on ground alert, one ALCS capable Looking Glass SAC ABNCP airborne at all times, and at least two ALCS-capable Auxiliary ABNCPs on ground alert. Airborne missileers, along with all other Cold Warriors kept the peace for several decades.\n\nWith the collapse of the Warsaw Pact and Soviet Union and subsequent end of the Cold War, several unfortunate events for ALCS unfolded. First, the 4th ACCS was deactivated along with the retirement of most of the EC-135 aircraft in the Air Force’s inventory. Looking Glass continuous airborne operations ceased, the Strategic Air Command was disbanded, and the 2nd ACCS was redesignated as the 7th ACCS. Even though times had changed and many were eager to cash in on the so-called “Peace Dividend” of the post-Cold War era, there were others that argued against these changes. In the end, it was impossible to stop the momentum of world events and all these changes. However, through all the turmoil, airborne missileers operating ALCS remained on alert and continued to remain vigilant, just like they always had. Although ALCS operations were no longer as wide spread as they once were with dedicated ALCCs on alert, ALCS remained on alert with the SAC and the US Strategic Command (USSTRATCOM) EC-135C ABNCPs. The Looking Glass alert posture was now a mix of both airborne and ground alert operations.\n\nAnother big change to ALCS operations occurred on 1 October 1998. On this day, the Air Force’s EC-135Cs ceased to perform USSTRATCOM Looking Glass operations and was subsequently retired. The Navy’s E-6B Mercury took over USSTRATCOM’s Looking Glass mission and associated ALCS mission.\n\nOriginally, the E-6A was built to perform the TAke Charge And Move Out (TACAMO) mission of relaying Emergency Action Messages to Navy Ballistic Missile Submarines in the Atlantic and Pacific oceans. However, when it was decided to retire the remainder of the Air Force’s EC-135C fleet, the E-6A was extensively modified. A battlestaff compartment was added, additional communications equipment was installed, and ALCS equipment was installed. Due to these extensive modifications, this new variant of the Mercury was redesignated from the E-6A to the E-6B. Now, whenever a USSTRATCOM battlestaff and ALCS crew are onboard, the E-6B is known as the USSTRATCOM ABNCP.\n\nToday, at least one USSTRATCOM ABNCP is on alert around-the-clock. It is postured with a full USSTRATCOM battlestaff and ALCS crew onboard to perform the Looking Glass mission in the event the USSTRATCOM Global Operations Center (GOC) is incapacitated. The aircraft can takeoff quickly to avoid any threat. The ALCS crew onboard still provides a survivable launch capability for the Air Force’s Minuteman III ICBMs located at the three remaining Missile Wings located at Malmstrom AFB, MT; Minot AFB, ND; and F.E. Warren AFB, WY. Just like its original inception, ALCS on alert today provides an adversary with an insurmountable task of trying to destroy the Minuteman ICBM force. Even if the ground Launch Control Centers are destroyed, the USSTRATCOM ABNCP can fly overhead and the airborne missileers onboard can launch the remaining Minuteman III ICBMs.\n\nThe ALCS mission has been held by multiple aircraft during the last 50 years:\n\n\n\n\nAlthough the ALCS now resides on a Navy aircraft, the ALCS equipment is owned and operated by Air Force Global Strike Command (AFGSC). The Missile Combat Crew Commander-Airborne (MCCC-A) is always a second or third tour assignment missileer and is from the 625th Strategic Operations Squadron (STOS) located at Offutt AFB, NE. The Deputy Missile Combat Crew Commander-Airborne (DMCCC-A) is a joint USSTRATCOM position. Both ALCS crewmembers go through initial and monthly recurring ALCS training provided by the 625th STOS.\n\n\n"}
{"id": "2541207", "url": "https://en.wikipedia.org/wiki?curid=2541207", "title": "Alpha Magnetic Spectrometer", "text": "Alpha Magnetic Spectrometer\n\nThe Alpha Magnetic Spectrometer, also designated AMS-02, is a particle physics experiment module that is mounted on the International Space Station (ISS). The module is a detector that measures antimatter in cosmic rays, this information is needed to understand the formation of the Universe and search for evidence of dark matter.\n\nThe principal investigator is Nobel laureate particle physicist Samuel Ting. The launch of flight STS-134 carrying AMS-02 took place on 16 May 2011, and the spectrometer was installed on 19 May 2011. By April 15, 2015, AMS-02 had recorded over 60 billion cosmic ray events and 90 billion after five years of operation since its installation in May 2011.\n\nIn March 2013, at a seminar at CERN, Professor Samuel Ting reported that AMS had observed over 400,000 positrons, with the positron to electron fraction increasing from 10 GeV to 250 GeV. (Later results have shown a decrease in positron fraction at energies over about 275 GeV). There was \"no significant variation over time, or any preferred incoming direction. These results are consistent with the positrons originating from the annihilation of dark matter particles in space, but not yet sufficiently conclusive to rule out other explanations.\" The results have been published in \"Physical Review Letters\". Additional data are still being collected.\n\nThe alpha magnetic spectrometer was proposed in 1995 by MIT particle physicist Samuel Ting, not long after the cancellation of the Superconducting Super Collider. The proposal was accepted and Ting became the principal investigator.\n\nAn AMS prototype designated AMS-01, a simplified version of the detector, was built by the international consortium under Ting's direction and flown into space aboard the on STS-91 in June 1998. By not detecting any antihelium the \"AMS-01\" established an upper limit of 1.1×10 for the antihelium to helium flux ratio and proved that the detector concept worked in space. This shuttle mission was the last shuttle flight to the \"Mir\" Space Station.\n\nAfter the flight of the prototype, Ting began the development of a full research system designated AMS-02. This development effort involved the work of 500 scientists from 56 institutions and 16 countries organized under United States Department of Energy (DOE) sponsorship.\n\nThe instrument which eventually resulted from a long evolutionary process has been called \"the most sophisticated particle detector ever sent into space\", rivaling very large detectors used at major particle accelerators, and has cost four times as much as any of its ground-based counterparts. Its goals have also evolved and been refined over time. As built it is a more comprehensive detector which has a better chance of discovering evidence of dark matter along other goals.\n\nThe power requirements for AMS-02 were thought to be too great for a practical independent spacecraft. So AMS-02 was designed to be installed as an external module on the International Space Station and use power from the ISS. The post- plan was to deliver AMS-02 to the ISS by space shuttle in 2005 on station assembly mission UF4.1, but technical difficulties and shuttle scheduling issues added more delays.\n\nAMS-02 successfully completed final integration and operational testing at CERN in Geneva, Switzerland which included exposure to energetic proton beams generated by the CERN SPS particle accelerator. AMS-02 was then shipped by specialist haulier to ESA's European Space Research and Technology Centre (ESTEC) facility in the Netherlands where it arrived 16 February 2010. Here it underwent thermal vacuum, electromagnetic compatibility and electromagnetic interference testing. AMS-02 was scheduled for delivery to the Kennedy Space Center in Florida, United States. in late May 2010. This was however postponed to August 26, as AMS-02 underwent final alignment beam testing at CERN.\n\nA cryogenic, superconducting magnet system was developed for the AMS-02. With Obama administration plans to extend International Space Station operations beyond 2015, the decision was made by AMS management to exchange the AMS-02 superconducting magnet for the non-superconducting magnet previously flown on AMS-01. Although the non-superconducting magnet has a weaker field strength, its on-orbit operational time at ISS is expected to be 10 to 18 years versus only three years for the superconducting version. In January 2014 it was announced that funding for the ISS had been extended until 2024.\n\nIn 1999, after the successful flight of AMS-01, the total cost of the AMS program was estimated to be $33 million, with AMS-02 planned for flight to the ISS in 2003. After the Space Shuttle \"Columbia\" disaster in 2003, and after a number of technical difficulties with the construction of AMS-02, the cost of the program ballooned to an estimated $2 billion.\n\nFor several years it was uncertain if AMS-02 would ever be launched because it was not manifested to fly on any of the remaining Space Shuttle flights. After the 2003 \"Columbia\" disaster NASA decided to reduce shuttle flights and retire the remaining shuttles by 2010. A number of flights were removed from the remaining manifest including the flight for AMS-02. In 2006 NASA studied alternative ways of delivering AMS-02 to the space station, but they all proved to be too expensive.\n\nIn May 2008 a bill was proposed to launch AMS-02 to ISS on an additional shuttle flight in 2010 or 2011. The bill was passed by the full House of Representatives on 11 June 2008. The bill then went before the Senate Commerce, Science and Transportation Committee where it also passed. It was then amended and passed by the full Senate on 25 September 2008, and was passed again by the House on 27 September 2008. It was signed by President George W. Bush on 15 October 2008. The bill authorized NASA to add another space shuttle flight to the schedule before the space shuttle program was discontinued. In January 2009 NASA restored AMS-02 to the shuttle manifest. On 26 August 2010, AMS-02 was delivered from CERN to the Kennedy Space Center by a Lockheed C-5 Galaxy.\n\nIt was delivered to the International Space Station on May 19, 2011 as part of station assembly flight ULF6 on shuttle flight STS-134, commanded by Mark Kelly. It was removed from the shuttle cargo bay using the shuttle's robotic arm and handed off to the station's robotic arm for installation. AMS-02 is mounted on top of the Integrated Truss Structure, on USS-02, the zenith side of the S3-element of the truss.\n\nAbout 1,000 cosmic rays are recorded by the instrument per second, generating about one GB/sec of data. This data is filtered and compressed to about 300 kB/sec for download to the operation center POCC at CERN.\n\nThe detector module consists of a series of detectors that are used to determine various characteristics of the radiation and particles as they pass through. Characteristics are determined only for particles that pass through from top to bottom. Particles that enter the detector at any other angles are rejected. From top to bottom the subsystems are identified as:\n\n\nThe AMS-02 will use the unique environment of space to advance knowledge of the Universe and lead to the understanding of its origin by searching for antimatter, dark matter and measuring cosmic rays.\n\nExperimental evidence indicates that our galaxy is made of matter; however, scientists believe there are about 100–200 billion galaxies in the Universe and some versions of the Big Bang theory of the origin of the Universe require equal amounts of matter and antimatter. Theories that explain this apparent asymmetry violate other measurements. Whether or not there is significant antimatter is one of the fundamental questions of the origin and nature of the Universe. Any observations of an antihelium nucleus would provide evidence for the existence of antimatter in space. In 1999, \"AMS-01\" established a new upper limit of 10 for the antihelium/helium flux ratio in the Universe. AMS-02 will search with a sensitivity of 10, an improvement of three orders of magnitude over \"AMS-01\", sufficient to reach the edge of the expanding Universe and resolve the issue definitively.\n\nThe visible matter in the Universe, such as stars, adds up to less than 5 percent of the total mass that is known to exist from many other observations. The other 95 percent is dark, either dark matter, which is estimated at 20 percent of the Universe by weight, or dark energy, which makes up the balance. The exact nature of both still is unknown. One of the leading candidates for dark matter is the neutralino. If neutralinos exist, they should be colliding with each other and giving off an excess of charged particles that can be detected by AMS-02. Any peaks in the background positron, antiproton, or gamma ray flux could signal the presence of neutralinos or other dark matter candidates, but would need to be distinguished from poorly known confounding astrophysical signals.\n\nSix types of quarks (up, down, strange, charm, bottom and top) have been found experimentally; however, the majority of matter on Earth is made up of only up and down quarks. It is a fundamental question whether there exists stable matter made up of strange quarks in combination with up and down quarks. Particles of such matter are known as strangelets. Strangelets might have extremely large mass and very small charge-to-mass ratios. It would be a totally new form of matter. AMS-02 may determine whether this extraordinary matter exists in our local environment.\n\nCosmic radiation during transit is a significant obstacle to sending humans to Mars. Accurate measurements of the cosmic ray environment are needed to plan appropriate countermeasures. Most cosmic ray studies are done by balloon-borne instruments with flight times that are measured in days; these studies have shown significant variations. AMS-02 is operative on the ISS, gathering a large amount of accurate data and allowing measurements of the long term variation of the cosmic ray flux over a wide energy range, for nuclei from protons to iron. In addition to the understanding the radiation protection required for astronauts during interplanetary flight, this data will allow the interstellar propagation and origins of cosmic rays to be identified.\n\nIn July 2012, it was reported that AMS-02 had observed over 18 billion cosmic rays.\n\nIn February 2013, Samuel Ting acknowledged that he would be publishing the first scholarly paper in a few weeks, and that in its first 18 months of operation AMS had recorded 25 billion particle events including nearly eight billion fast electrons and positrons. The AMS paper reported the positron-electron ratio in the mass range of 0.5 to 350 GeV, providing evidence about the weakly interacting massive particle (WIMP) model of dark matter.\n\nOn 30 March 2013, the first results from the AMS experiment were announced by the CERN press office. The first physics results were published in \"Physical Review Letters\" on 3 April 2013. A total of 6.8×10 positron and electron events were collected in the energy range from 0.5 to 350 GeV. The positron fraction (of the total electron plus positron events) steadily increased from energies of 10 to 250  GeV, but the slope decreased by an order of magnitude above 20 GeV, even though the fraction of positrons still increased. There was no fine structure in the positron fraction spectrum, and no anisotropies were observed. The accompanying \"Physics\" Viewpoint said that \"The first results from the space-borne Alpha Magnetic Spectrometer confirm an unexplained excess of high-energy positrons in Earth-bound cosmic rays.\" These results are consistent with the positrons originating from the annihilation of dark matter particles in space, but not yet sufficiently conclusive to rule out other explanations. Samuel Ting said \"Over the coming months, AMS will be able to tell us conclusively whether these positrons are a signal for dark matter, or whether they have some other origin.\"\n\nOn September 18, 2014, new results with almost twice as much data were presented in a talk at CERN and published in \"Physical Review Letters\". A new measurement of positron fraction up to 500 GeV was reported, showing that positron fraction peaks at a maximum of about 16% of total electron+positron events, around an energy of 275 ± 32 GeV. At higher energies, up to 500 GeV, the ratio of positrons to electrons begins to fall again.\n\nAMS presented for 3 days at CERN in April 2015, covering new data on 300 million proton events and helium flux. It revealed in December 2016 that it had discovered a few signals consistent with antihelium nuclei amidst several billion helium nuclei. The result remains to be verified, and the team is currently trying to rule out contamination.\n\n\n\n\n"}
{"id": "22565770", "url": "https://en.wikipedia.org/wiki?curid=22565770", "title": "American Council for an Energy-Efficient Economy", "text": "American Council for an Energy-Efficient Economy\n\nThe American Council for an Energy-Efficient Economy, or ACEEE, is a nonprofit, 501(c)(3) organization. Founded in 1980, ACEEE's mission is to act as a catalyst to advance energy efficiency policies, programs, technologies, investments, and behaviors in order to help achieve greater economic prosperity, energy security, and environmental protection.\n\nACEEE promotes energy efficiency by conducting technical and policy analyses; advising policymakers and program managers; and working collaboratively with businesses, government officials, public interest groups, and other organizations. It convenes conferences and workshops, primarily for energy efficiency professionals, and produces reports, books, conference proceedings, and media outreach.\n\nACEEE employs more than 35 Washington, D.C.-based employees, and holds field offices in Delaware, Michigan, Washington, and Wisconsin. The organization's primary focuses are on end-use efficiency in industry, buildings, utilities, and transportation; economic analysis and human behavior; and state and national policy.\n\nACEEE has worked on federal energy policy since the 1980s. The organization played central roles in the development of the National Appliance Energy Conservation Act of 1987, energy efficiency provisions in the Energy Policy Act of 1992 and the Energy Policy Act of 2005, and the Energy Independence and Security Act of 2007. It also played a role in the development of energy efficiency sections in recent farm bills. Many of these provisions were developed in cooperation with interested business and received bipartisan support.\n\nACEEE staff testify before Congress and work closely with Congressional staff in both parties to help shape new initiatives and analyze the impacts of energy and climate policy proposals. They weigh in on the federal budget process, promoting and increasing funding for what they deem the most effective energy efficiency programs.\n\nThe organization also works with federal agencies such as the Environmental Protection Agency and the Department of Energy on programs and policies, and participates in formal rulemakings on energy efficiency issues. Most of ACEEE's federal policy work involves research and education; however, they do a limited amount of lobbying using unrestricted funds.\n\nThey supported the EPS Service Parts Act of 2014 (H.R. 5057; 113th Congress), a bill that would exempt certain external power supplies from complying with standards set forth in a final rule published by the United States Department of Energy in February 2014. The United States House Committee on Energy and Commerce describes the bill as a bill that \"provides regulatory relief by making a simple technical correction to the 2007 Energy Independence and Security Act to exempt certain power supply (EPS) service and spare parts from federal efficiency standards.\"\n\nAt the state level, ACEEE works closely with public officials and local energy efficiency advocates, providing advice, analysis, and technical support. In order to maximize its impact, the organization concentrates its efforts on large states that are poised to make major decisions on energy efficiency policy issues. For example, in 2007 and 2008, it emphasized work in Florida, Maryland, Michigan, Ohio, Pennsylvania, Texas, and Virginia. All of these states passed significant energy efficiency legislation, with additional legislation and regulations likely in some of them.\n\nACEEE also works on state policies that can potentially be adopted at the national level. For example, staff have worked with several states to develop state appliance and equipment efficiency standards, which have subsequently been adopted by Congress.\n\nACEEE devotes much of its resources to research and analysis in order to discern the best technical, program, and policy practices that promote efficient energy use, and to define the magnitude of benefits of energy efficiency. Research staff survey market trends and activities, analyze technical and economic potential for energy efficiency, seek to understand consumer energy decisions, and assess the potential for regulations, policies, and programs to achieve energy savings.\nACEEE's research programs include:\n\nACEEE attempts to reach out to and inform diverse audiences in a variety of ways:\n\nACEEE holds several conferences each year that aim to bring together disparate stakeholders to focus on multiple aspects of energy efficiency and the role it plays in addressing critical issues such as climate change, energy resources, utility structure and regulation, and energy use in buildings, industry, and agriculture. Recent conferences include:\n\nACEEE provides research and technical analysis of current energy efficiency policies and practices, as well as forecasting future trends. These publications include research reports, consumer books, white papers, legislative testimony, and conference proceedings. Major recent ACEEE reports include:\n\nACEEE shares its work online by providing publications and conference presentations on the internet. While not a consumer-focused organization, ACEEE has several web resources devoted to educating consumers about making wise energy efficiency choices relating to their homes and their vehicles, through:\n\nACEEE convenes primary stakeholders in discussions and networking opportunities through policy briefings, webinars covering recent published research and analysis, and one-to-one interaction with staff and Board members\n\nACEEE's sources of funding from 2006 to 2007 were broken down as follows:\n\n\n\n"}
{"id": "18933196", "url": "https://en.wikipedia.org/wiki?curid=18933196", "title": "Bismuth", "text": "Bismuth\n\nBismuth is a chemical element with symbol Bi and atomic number 83. It is a pentavalent post-transition metal and one of the pnictogens with chemical properties resembling its lighter homologs arsenic and antimony. Elemental bismuth may occur naturally, although its sulfide and oxide form important commercial ores. The free element is 86% as dense as lead. It is a brittle metal with a silvery white color when freshly produced, but surface oxidation can give it a pink tinge. Bismuth is marginally radioactive, and the most naturally diamagnetic element, and has one of the lowest values of thermal conductivity among metals.\n\nBismuth was long considered the element with the highest atomic mass that is stable, but in 2003 it was discovered to be extremely weakly radioactive: its only primordial isotope, bismuth-209, decays via alpha decay with a half-life more than a billion times the estimated age of the universe. Because of its tremendously long half-life, bismuth may still be considered stable for almost all purposes.\n\nBismuth metal has been known since ancient times, although it was often confused with lead and tin, which share some physical properties. The etymology is uncertain, but possibly comes from Arabic \"bi ismid\", meaning having the properties of antimony or the German words \"weiße Masse\" or \"Wismuth\" (\"white mass\"), translated in the mid-sixteenth century to New Latin \"bisemutum\".\n\nBismuth compounds account for about half the production of bismuth. They are used in cosmetics, pigments, and a few pharmaceuticals, notably bismuth subsalicylate, used to treat diarrhea. Bismuth's unusual propensity to expand upon freezing is responsible for some of its uses, such as in casting of printing type. Bismuth has unusually low toxicity for a heavy metal. As the toxicity of lead has become more apparent in recent years, there is an increasing use of bismuth alloys (presently about a third of bismuth production) as a replacement for lead.\n\nThe name \"bismuth\" dates from around the 1660s, and is of uncertain etymology. It is one of the first 10 metals to have been discovered. Bismuth appears in the 1660s, from obsolete German ', ', ' (early 16th century); perhaps related to Old High German ' (\"white\"). The New Latin ' (due to Georgius Agricola, who Latinized many German mining and technical words) is from the German ', perhaps from \"\", \"white mass\". The element was confused in early times with tin and lead because of its resemblance to those elements. Bismuth has been known since ancient times, so no one person is credited with its discovery. Agricola, in De Natura Fossilium (c. 1546) states that bismuth is a distinct metal in a family of metals including tin and lead. This was based on observation of the metals and their physical properties. Miners in the age of alchemy also gave bismuth the name \",\" or \"silver being made,\" in the sense of silver still in the process of being formed within the Earth.\n\nBeginning with Johann Heinrich Pott in 1738, Carl Wilhelm Scheele and Torbern Olof Bergman, the distinctness of lead and bismuth became clear, and Claude François Geoffroy demonstrated in 1753 that this metal is distinct from lead and tin.\n\nBismuth was also known to the Incas and used (along with the usual copper and tin) in a special bronze alloy for knives.\n\nBismuth is a brittle metal with a white, silver-pink hue, often with an iridescent oxide tarnish showing many colors from yellow to blue. The spiral, stair-stepped structure of bismuth crystals is the result of a higher growth rate around the outside edges than on the inside edges. The variations in the thickness of the oxide layer that forms on the surface of the crystal cause different wavelengths of light to interfere upon reflection, thus displaying a rainbow of colors. When burned in oxygen, bismuth burns with a blue flame and its oxide forms yellow fumes. Its toxicity is much lower than that of its neighbors in the periodic table, such as lead, antimony, and polonium.\n\nNo other metal is verified to be more naturally diamagnetic than bismuth. (Superdiamagnetism is a different physical phenomenon.) Of any metal, it has one of the lowest values of thermal conductivity (after manganese, and maybe neptunium and plutonium) and the highest Hall coefficient. It has a high electrical resistivity. When deposited in sufficiently thin layers on a substrate, bismuth is a semiconductor, despite being a post-transition metal.Elemental bismuth is denser in the liquid phase than the solid, a characteristic it shares with germanium, silicon, gallium and water. Bismuth expands 3.32% on solidification; therefore, it was long a component of low-melting typesetting alloys, where it compensated for the contraction of the other alloying components to form almost isostatic bismuth-lead eutectic alloys.\n\nThough virtually unseen in nature, high-purity bismuth can form distinctive, colorful hopper crystals. It is relatively nontoxic and has a low melting point just above 271 °C, so crystals may be grown using a household stove, although the resulting crystals will tend to be lower quality than lab-grown crystals.\n\nAt ambient conditions bismuth shares the same layered structure as the metallic forms of arsenic and antimony, crystallizing in the rhombohedral lattice (Pearson symbol hR6, space group Rm No. 166), which is often classed into trigonal or hexagonal crystal systems. When compressed at room temperature, this Bi-I structure changes first to the monoclinic Bi-II at 2.55 GPa, then to the tetragonal Bi-III at 2.7 GPa, and finally to the body-centered cubic Bi-IV at 7.7 GPa. The corresponding transitions can be monitored via changes in electrical conductivity; they are rather reproducible and abrupt, and are therefore used for calibration of high-pressure equipment.\n\nBismuth is stable to both dry and moist air at ordinary temperatures. When red-hot, it reacts with water to make bismuth(III) oxide.\n\nIt reacts with fluorine to make bismuth(V) fluoride at 500 °C or bismuth(III) fluoride at lower temperatures (typically from Bi melts); with other halogens it yields only bismuth(III) halides. The trihalides are corrosive and easily react with moisture, forming oxyhalides with the formula BiOX.\n\nBismuth dissolves in concentrated sulfuric acid to make bismuth(III) sulfate and sulfur dioxide.\n\nIt reacts with nitric acid to make bismuth(III) nitrate.\n\nIt also dissolves in hydrochloric acid, but only with oxygen present.\n\nIt is used as a transmetalating agent in the synthesis of alkaline-earth metal complexes:\n\nThe only primordial isotope of bismuth, bismuth-209, was traditionally regarded as the heaviest stable isotope, but it had long been suspected to be unstable on theoretical grounds. This was finally demonstrated in 2003, when researchers at the Institut d'Astrophysique Spatiale in Orsay, France, measured the alpha emission half-life of to be , over a billion times longer than the current estimated age of the universe. Owing to its extraordinarily long half-life, for all presently known medical and industrial applications, bismuth can be treated as if it is stable and nonradioactive. The radioactivity is of academic interest because bismuth is one of a few elements whose radioactivity was suspected and theoretically predicted before being detected in the laboratory. Bismuth has the longest known alpha decay half-life, although tellurium-128 has a double beta decay half-life of over . Bismuth's extremely long half life means that less than one billionth of the bismuth present at the formation of the planet Earth would have decayed into thallium since then.\n\nSeveral isotopes of bismuth with short half-lives occur within the radioactive disintegration chains of actinium, radium, and thorium, and more have been synthesized experimentally. Bismuth-213 is also found on the decay chain of uranium-233.\n\nCommercially, the radioactive isotope bismuth-213 can be produced by bombarding radium with bremsstrahlung photons from a linear particle accelerator. In 1997, an antibody conjugate with bismuth-213, which has a 45-minute half-life and decays with the emission of an alpha particle, was used to treat patients with leukemia. This isotope has also been tried in cancer treatment, for example, in the targeted alpha therapy (TAT) program.\n\nBismuth forms trivalent and pentavalent compounds, the trivalent ones being more common. Many of its chemical properties are similar to those of arsenic and antimony, although they are less toxic than derivatives of those lighter elements.\n\nAt elevated temperatures, the vapors of the metal combine rapidly with oxygen, forming the yellow trioxide, . When molten, at temperatures above 710 °C, this oxide corrodes any metal oxide, and even platinum. On reaction with base, it forms two series of oxyanions: , which is polymeric and forms linear chains, and . The anion in is actually a cubic octameric anion, , whereas the anion in is tetrameric.\n\nThe dark red bismuth(V) oxide, , is unstable, liberating gas upon heating. The compound NaBiO is a strong oxidising agent.\n\nBismuth sulfide, , occurs naturally in bismuth ores. It is also produced by the combination of molten bismuth and sulfur.\nBismuth oxychloride (BiOCl, see figure at right) and bismuth oxynitrate (BiONO) stoichiometrically appear as simple anionic salts of the bismuthyl(III) cation (BiO) which commonly occurs in aqueous bismuth compounds. However, in the case of BiOCl, the salt crystal forms in a structure of alternating plates of Bi, O, and Cl atoms, with each oxygen coordinating with four bismuth atoms in the adjacent plane. This mineral compound is used as a pigment and cosmetic (see below).\n\nUnlike the lighter pnictogens nitrogen, phosphorus, and arsenic, but similar to antimony, bismuth does not form a stable hydride. Bismuth hydride, bismuthine (), is an endothermic compound that spontaneously decomposes at room temperature. It is stable only below −60 °C. Bismuthides are intermetallic compounds between bismuth and other metals.\n\nIn 2014 researchers discovered that sodium bismuthide can exist as a form of matter called a “three-dimensional topological Dirac semi-metal” (3DTDS) that possess 3D Dirac fermions in bulk. It is a natural, three-dimensional counterpart to graphene with similar electron mobility and velocity. Graphene and topological insulators (such as those in 3DTDS) are both crystalline materials that are electrically insulating inside but conducting on the surface, allowing them to function as transistors and other electronic devices. While sodium bismuthide () is too unstable to be used in devices without packaging, it can demonstrate potential applications of 3DTDS systems, which offer distinct efficiency and fabrication advantages over planar graphene in semiconductor and spintronics applications.\n\nThe halides of bismuth in low oxidation states have been shown to adopt unusual structures. What was originally thought to be bismuth(I) chloride, BiCl, turns out to be a complex compound consisting of Bi cations and BiCl and BiCl anions. The Bi cation has a distorted tricapped trigonal prismatic molecular geometry, and is also found in , which is prepared by reducing a mixture of hafnium(IV) chloride and bismuth chloride with elemental bismuth, having the structure . Other polyatomic bismuth cations are also known, such as Bi, found in . Bismuth also forms a low-valence bromide with the same structure as \"BiCl\". There is a \"true\" monoiodide, BiI, which contains chains of units. BiI decomposes upon heating to the triiodide, , and elemental bismuth. A monobromide of the same structure also exists.\nIn oxidation state +3, bismuth forms trihalides with all of the halogens: , , , and . All of these except are hydrolyzed by water.\n\nBismuth(III) chloride reacts with hydrogen chloride in ether solution to produce the acid .\n\nThe oxidation state +5 is less frequently encountered. One such compound is , a powerful oxidizing and fluorinating agent. It is also a strong fluoride acceptor, reacting with xenon tetrafluoride to form the cation:\n\nIn aqueous solution, the Bi ion is solvated to form the aqua ion in strongly acidic conditions. At pH > 0 polynuclear species exist, the most important of which is believed to be the octahedral complex [].\n\nIn the Earth's crust, bismuth is about twice as abundant as gold. The most important ores of bismuth are bismuthinite and bismite. Native bismuth is known from Australia, Bolivia, and China.\n\nAccording to the United States Geological Survey, the world mining production of bismuth in 2014 was 13,600 tonnes, with the major contributions from China (7,600 tonnes), Vietnam (4,950 tonnes) and Mexico (948 tonnes). The refinery production in 2010 was 16,000 tonnes, of which China produced 13,000, Mexico 850 and Belgium 800 tonnes. The difference reflects bismuth's status as a byproduct of extraction of other metals such as lead, copper, tin, molybdenum and tungsten. World bismuth production from refineries is a more complete and reliable statistic.\n\nBismuth travels in crude lead bullion (which can contain up to 10% bismuth) through several stages of refining, until it is removed by the Kroll-Betterton process which separates the impurities as slag, or the electrolytic Betts process. Bismuth will behave similarly with another of its major metals, copper. The raw bismuth metal from both processes contains still considerable amounts of other metals, foremost lead. By reacting the molten mixture with chlorine gas the metals are converted to their chlorides while bismuth remains unchanged. Impurities can also be removed by various other methods for example with fluxes and treatments yielding high-purity bismuth metal (over 99% Bi).\n\nThe price for pure bismuth metal has been relatively stable through most of the 20th century, except for a spike in the 1970s. Bismuth has always been produced mainly as a byproduct of lead refining, and thus the price usually reflected the cost of recovery and the balance between production and demand.\n\nDemand for bismuth was small prior to World War II and was pharmaceutical – bismuth compounds were used to treat such conditions as digestive disorders, sexually transmitted infections and burns. Minor amounts of bismuth metal were consumed in fusible alloys for fire sprinkler systems and fuse wire. During World War II bismuth was considered a strategic material, used for solders, fusible alloys, medications and atomic research. To stabilize the market, the producers set the price at $1.25 per pound (2.75 $/kg) during the war and at $2.25 per pound (4.96 $/kg) from 1950 until 1964.\n\nIn the early 1970s, the price rose rapidly as a result of increasing demand for bismuth as a metallurgical additive to aluminium, iron and steel. This was followed by a decline owing to increased world production, stabilized consumption, and the recessions of 1980 and 1981–82. In 1984, the price began to climb as consumption increased worldwide, especially in the United States and Japan. In the early 1990s, research began on the evaluation of bismuth as a nontoxic replacement for lead in ceramic glazes, fishing sinkers, food-processing equipment, free-machining brasses for plumbing applications, lubricating greases, and shot for waterfowl hunting. Growth in these areas remained slow during the middle 1990s, in spite of the backing of lead replacement by the US Government, but intensified around 2005. This resulted in a rapid and continuing increase in price.\n\nMost bismuth is produced as a byproduct of other metal-extraction processes including the smelting of lead, and also of tungsten and copper. Its sustainability is dependent on increased recycling, which is problematic.\n\nIt was once believed that bismuth could be practically recycled from the soldered joints in electronic equipment. Recent efficiencies in solder application in electronics mean there is substantially less solder deposited, and thus less to recycle. While recovering the silver from silver-bearing solder may remain economic, recovering bismuth is substantially less so.\n\nNext in recycling feasibility would be sizeable catalysts with a fair bismuth content, such as bismuth phosphomolybdate., bismuth used in galvanizing, and as a free-machining metallurgical additive.\n\nBismuth in uses where it is dispersed most widely include certain stomach medicines (bismuth subsalicylate), paints (bismuth vanadate), pearlescent cosmetics (bismuth oxychloride), and bismuth-containing bullets. Recycling bismuth from these uses is impractical.\n\nBismuth has few commercial applications, and those applications that use it generally require small quantities relative to other raw materials. In the United States, for example, 884 tonnes of bismuth were consumed in 2010, of which 63% went into chemicals (including pharmaceuticals, pigments, and cosmetics); 26% into metallurgical additives for casting and galvanizing; 7% into bismuth alloys, solders and ammunition; and 4% into research and other uses.\n\nSome manufacturers use bismuth as a substitute in equipment for potable water systems such as valves to meet \"lead-free\" mandates in the U.S. (began in 2014). This is a fairly large application since it covers all residential and commercial building construction.\n\nIn the early 1990s, researchers began to evaluate bismuth as a nontoxic replacement for lead in various applications.\n\nBismuth is an ingredient in some pharmaceuticals, although the use of some of these substances is declining.\n\nBismuth oxychloride (BiOCl) is sometimes used in cosmetics, as a pigment in paint for eye shadows, hair sprays and nail polishes. This compound is found as the mineral bismoclite and in crystal form contains layers of atoms (see figure above) that refract light chromatically, resulting in an iridescent appearance similar to nacre of pearl. It was used as a cosmetic in ancient Egypt and in many places since. \"Bismuth white\" (also \"Spanish white\") can refer to either bismuth oxychloride or bismuth oxynitrate (BiONO), when used as a white pigment. Bismuth vanadate is used as a light-stable non-reactive paint pigment (particularly for artists' paints), often as a replacement for the more toxic cadmium sulfide yellow and orange-yellow pigments. The most common variety in artists' paints is a lemon yellow, visually indistinguishable from its cadmium-containing alternative.\n\nBismuth is used in metal alloys with other metals such as iron, to create alloys to go into automatic sprinkler systems for fires. It was also used to make bismuth bronze which was used in the Bronze Age.\n\nThe density difference between lead (11.32 g/cm) and bismuth (9.78 g/cm) is small enough that for many ballistics and weighting applications, bismuth can substitute for lead. For example, it can replace lead as a dense material in fishing sinkers. It has been used as a replacement for lead in shot, bullets and less-lethal riot gun ammunition. The Netherlands, Denmark, England, Wales, the US, and many other countries now prohibit the use of lead shot for the hunting of wetland birds, as many birds are prone to lead poisoning owing to mistaken ingestion of lead (instead of small stones and grit) to aid digestion, or even prohibit the use of lead for all hunting, such as in the Netherlands. Bismuth-tin alloy shot is one alternative that provides similar ballistic performance to lead. (Another less expensive but also more poorly performing alternative is \"steel\" shot, which is actually soft iron.) Bismuth's lack of malleability does, however, make it unsuitable for use in expanding hunting bullets.\n\nBismuth, as a dense element of high atomic weight, is used in bismuth-impregnated latex shields to shield from X-ray in medical examinations, such as CTs, mostly as it is considered non-toxic.\n\nThe European Union's Restriction of Hazardous Substances Directive (RoHS) for reduction of lead has broadened bismuth's use in electronics as a component of low-melting point solders, as a replacement for traditional tin-lead solders. Its low toxicity will be especially important for solders to be used in food processing equipment and copper water pipes, although it can also be used in other applications including those in the automobile industry, in the EU for example.\n\nBismuth has been evaluated as a replacement for lead in free-machining brasses for plumbing applications, although it does not equal the performance of leaded steels.\n\nMany bismuth alloys have low melting points and are found in specialty applications such as solders. Many automatic sprinklers, electric fuses, and safety devices in fire detection and suppression systems contain the eutectic In19.1-Cd5.3-Pb22.6-Sn8.3-Bi44.7 alloy that melts at This is a convenient temperature since it is unlikely to be exceeded in normal living conditions. Low-melting alloys, such as Bi-Cd-Pb-Sn alloy which melts at 70 °C, are also used in automotive and aviation industries. Before deforming a thin-walled metal part, it is filled with a melt or covered with a thin layer of the alloy to reduce the chance of breaking. Then the alloy is removed by submerging the part in boiling water.\n\nBismuth is used to make free-machining steels and free-machining aluminium alloys for precision machining properties. It has similar effect to lead and improves the chip breaking during machining. The shrinking on solidification in lead and the expansion of bismuth compensate each other and therefore lead and bismuth are often used in similar quantities. Similarly, alloys containing comparable parts of bismuth and lead exhibit a very small change (on the order 0.01%) upon melting, solidification or aging. Such alloys are used in high-precision casting, e.g. in dentistry, to create models and molds. Bismuth is also used as an alloying agent in production of malleable irons and as a thermocouple material.\n\nBismuth is also used in aluminium-silicon cast alloys in order to refine silicon morphology. However, it indicated a poisoning effect on modification of strontium (Sr). Some bismuth alloys, such as Bi35-Pb37-Sn25, are combined with non-sticking materials such as mica, glass and enamels because they easily wet them allowing to make joints to other parts. Addition of bismuth to caesium enhances the quantum yield of caesium cathodes. Sintering of bismuth and manganese powders at 300 °C produces a permanent magnet and magnetostrictive material, which is used in ultrasonic generators and receivers working in the 10–100 kHz range and in magnetic memory devices.\n\n\nScientific literature indicates that some of the compounds of bismuth are less toxic to humans via ingestion compared to other heavy metals (lead, arsenic, antimony, etc.) presumably due to the comparatively low solubility of bismuth salts. Its biological half-life for whole-body retention is reported to be 5 days but it can remain in the kidney for years in people treated with bismuth compounds.\n\nBismuth poisoning can occur and has according to some reports been common in relatively recent times. As with lead, bismuth poisoning can result in the formation of a black deposit on the gingiva, known as a bismuth line. Poisoning may be treated with dimercaprol; however, evidence for benefit is unclear.\n\nBismuth's environmental impacts are not well known; it may be less likely to bioaccumulate than some other heavy metals, and this is an area of active research.\n\nThe fungus \"Marasmius oreades\" can be used for the biological remediation of bismuth in polluted soils.\n\n\n\n"}
{"id": "28580469", "url": "https://en.wikipedia.org/wiki?curid=28580469", "title": "Brazi Power Station", "text": "Brazi Power Station\n\nThe Brazi Power Station is a large thermal power plant located in Brazi, having 9 generation groups, 5 of 50 MW each, 2 of 150 MW and 2 of 200 MW resulting a total electricity generation capacity of 950 MW.\n\n\n"}
{"id": "33811263", "url": "https://en.wikipedia.org/wiki?curid=33811263", "title": "Caribbean pipefish", "text": "Caribbean pipefish\n\nCaribbean pipefish (\"Syngnathus caribbaeus\") is a species of the pipefish. Widespread in the Western Atlantic near the coasts of South America from Belize to Suriname, also from the Greater and Lesser Antilles. Marine tropical reef-associated fish, up to 22.5 cm length.\n\n"}
{"id": "15374320", "url": "https://en.wikipedia.org/wiki?curid=15374320", "title": "Compounds of oxygen", "text": "Compounds of oxygen\n\nThe oxidation state of oxygen is −2 in almost all known compounds of oxygen. The oxidation state −1 is found in a few compounds such as peroxides. Compounds containing oxygen in other oxidation states are very uncommon: − (superoxides), − (ozonides), 0 (elemental, hypofluorous acid), + (dioxygenyl), +1 (dioxygen difluoride), and +2 (oxygen difluoride). \n\nOxygen is reactive and will form oxides with all other elements except the noble gases helium, neon, argon, and krypton.\n\nWater () is the oxide of hydrogen and most familiar oxygen compound. Its bulk properties partly result from the interaction of its component atoms, oxygen and hydrogen, with atoms of nearby water molecules. Hydrogen atoms are covalently bonded to oxygen in a water molecule but also have an additional attraction (about 23.3 kJ·mol per hydrogen atom) to an adjacent oxygen atom in a separate molecule. These hydrogen bonds between water molecules hold them approximately 15% closer than what would be expected in a simple liquid with just Van der Waals forces.\nDue to its electronegativity, oxygen forms chemical bonds with almost all other free elements at elevated temperatures to give corresponding oxides. However, some elements, such as iron which oxidises to iron oxide, or rust, , readily oxidise at standard conditions for temperature and pressure (STP). The surface of metals like aluminium and titanium are oxidized in the presence of air and become coated with a thin film of oxide that passivates the metal and slows further corrosion. So-called noble metals, such as gold and platinum, resist direct chemical combination with oxygen, and substances like gold(III) oxide () must be formed by an indirect route.\n\nThe alkali metals and alkali earth metals all react spontaneously with oxygen when exposed to dry air to form oxides, and form hydroxides in the presence of oxygen and water. As a result, none of these elements is found in nature as a free metal. Caesium is so reactive with oxygen that it is used as a getter in vacuum tubes. Although solid magnesium reacts slowly with oxygen at STP, it is capable of burning in air, generating very high temperatures, and its metal powder may form explosive mixtures with air.\n\nOxygen is present as compounds in the atmosphere in trace quantities in the form of carbon dioxide () and oxides of nitrogen (NO). The earth's crustal rock is composed in large part of oxides of silicon (silica , found in granite and sand), aluminium (aluminium oxide , in bauxite and corundum), iron (iron (III) oxide , in hematite and rust) and other oxides of metals.\n\nThe rest of the Earth's crust is formed also of oxygen compounds, most importantly calcium carbonate (in limestone) and silicates (in feldspars). Water-soluble silicates in the form of , , and are used as detergents and adhesives.\n\nPeroxides retain some of oxygen's original molecular structure (<(O-O). White or light yellow sodium peroxide () is formed when metallic sodium is burned in oxygen. Each oxygen atom in its peroxide ion may have a full octet of 4 pairs of electrons. Superoxides are a class of compounds that are very similar to peroxides, but with just one unpaired electron for each pair of oxygen atoms (). These compounds form by oxidation of alkali metals with larger ionic radii (K, Rb, Cs). For example, potassium superoxide () is an orange-yellow solid formed when potassium reacts with oxygen. \n\nHydrogen peroxide () can be produced by passing a volume of 96% to 98% hydrogen and 2 to 4% oxygen through an electric discharge. A more commercially-viable method is to allow autoxidation of an organic intermediate, 2-ethylanthrahydroquinone dissolved in an organic solvent, to oxidize to and 2-ethylanthraquinone. The 2-ethylanthraquinone is then reduced and recycled back into the process.\n\nWhen dissolved in water, many metallic oxide form alkaline solutions, while many oxides of nonmetals form acidic solutions. For example, sodium oxide in solution forms the strong base sodium hydroxide, while phosphorus pentoxide in solution forms phosphoric acid.\n\nOxygenated anions such as chlorates (), perchlorates (), chromates (), dichromates (), permanganates (), and nitrates () are strong oxidizing agents. Oxygen forms heteropoly acids and polyoxometalate ions with tungsten, molybdenum and some other transition metals, such as phosphotungstic acid () and octadecamolybdophosphoric acid ().\n\nOne unexpected oxygen compound is dioxygenyl hexafluoroplatinate, , discovered in studying the properties of platinum hexafluoride (). A change in color when this compound was exposed to atmospheric air suggested that dioxygen was being oxidized (in turn the difficulty of oxidizing oxygen led to the hypothesis that xenon might be oxidized by , resulting in discovery of the first xenon compound xenon hexafluoroplatinate ). The cations of oxygen are formed only in the presence of stronger oxidants than oxygen, which limits them to the action of fluorine and certain fluorine compounds. Simple oxygen fluorides are known.\n\nAmong the most important classes of organic compounds that contain oxygen are (where \"R\" is an organic group): alcohol (R-OH); ethers (R-O-R); ketones (R-CO-R); aldehydes (R-CO-H); carboxylic acids (R-COOH); esters (R-COO-R); acid anhydrides (R-CO-O-CO-R); amides (R-C(O)-NR). There are many important organic solvents that contain oxygen, among which: acetone, methanol, ethanol, isopropanol, furan, THF, diethyl ether, dioxane, ethylacetate, DMF, DMSO, acetic acid, formic acid. Acetone () and phenol () are used as feeder materials in the synthesis of many different substances. Other important organic compounds that contain oxygen are: glycerol, formaldehyde, glutaraldehyde, citric acid, acetic anhydride, acetamide, etc. Epoxides are ethers in which the oxygen\natom is part of a ring of three atoms. \n\nOxygen reacts spontaneously with many organic compounds at or below room temperature in a process called autoxidation. Alkaline solutions of pyrogallol, benzene-1,2,3-triol absorb oxygen from the air, and are used in the determination of the atmospheric concentration of oxygen. Most of the organic compounds that contain oxygen are not made by direct action of oxygen. Organic compounds important in industry and commerce are made by direct oxidation of a precursor include:\n\n\n\nThe element is found in almost all biomolecules that are important to, or generated by, life. Only a few common complex biomolecules, such as squalene and the carotenes, contain no oxygen. Of the organic compounds with biological relevance, carbohydrates contain the largest proportion by mass of oxygen (about 50%). All fats, fatty acids, amino acids, and proteins contain oxygen (due to the presence of carbonyl groups in these acids and their ester residues). Furthermore, seven of the amino acids which are incorporated into proteins, have oxygen incorporated into their side-chains, as well. Oxygen also occurs in phosphate (PO) groups in the biologically important energy-carrying molecules ATP and ADP, in the backbone and the purines (except adenine) and pyrimidines of RNA and DNA, and in bones as calcium phosphate and hydroxylapatite.\n"}
{"id": "17163378", "url": "https://en.wikipedia.org/wiki?curid=17163378", "title": "Consuta", "text": "Consuta\n\nConsuta was a revolutionary form of construction of watertight hulls for boats and marine aircraft, comprising four veneers of mahogany planking interleaved with waterproofed calico and stitched together with copper wire.\n\nThe technique was patented by Sam Saunders of Goring-on-Thames and was first used on the 1898 umpire's steam launch of the same name. Having been restored, the steam launch Consuta was returned to service on the River Thames on 15 October 2001.\n\nAfter opening the S. E. Saunders boatyard at East Cowes on the Isle of Wight, the technique was further used to build the crew and engine gondolas for HMA1 Mayfly, Britain's first airship. Later, the same technique was used to construct the hull of the Sopwith Bat Boat, the early flying boat that won the Mortimer Singer prize.\n\nThe technique remained in use until waterproof glues became available in the 1950s.\n\n"}
{"id": "5675", "url": "https://en.wikipedia.org/wiki?curid=5675", "title": "Curium", "text": "Curium\n\nCurium is a transuranic radioactive chemical element with symbol Cm and atomic number 96. This element of the actinide series was named after Marie and Pierre Curie – both were known for their research on radioactivity. Curium was first intentionally produced and identified in July 1944 by the group of Glenn T. Seaborg at the University of California, Berkeley. The discovery was kept secret and only released to the public in November 1947. Most curium is produced by bombarding uranium or plutonium with neutrons in nuclear reactors – one tonne of spent nuclear fuel contains about 20 grams of curium.\n\nCurium is a hard, dense, silvery metal with a relatively high melting point and boiling point for an actinide. Whereas it is paramagnetic at ambient conditions, it becomes antiferromagnetic upon cooling, and other magnetic transitions are also observed for many curium compounds. In compounds, curium usually exhibits valence +3 and sometimes +4, and the +3 valence is predominant in solutions. Curium readily oxidizes, and its oxides are a dominant form of this element. It forms strongly fluorescent complexes with various organic compounds, but there is no evidence of its incorporation into bacteria and archaea. When introduced into the human body, curium accumulates in the bones, lungs and liver, where it promotes cancer.\n\nAll known isotopes of curium are radioactive and have a small critical mass for a sustained nuclear chain reaction. They predominantly emit α-particles, and the heat released in this process can serve as a heat source in radioisotope thermoelectric generators, but this application is hindered by the scarcity and high cost of curium isotopes. Curium is used in production of heavier actinides and of the Pu radionuclide for power sources in artificial pacemakers. It served as the α-source in the alpha particle X-ray spectrometers installed on several space probes, including the Sojourner, Spirit, Opportunity and Curiosity Mars rovers and the Philae lander on comet 67P/Churyumov–Gerasimenko, to analyze the composition and structure of the surface.\n\nAlthough curium had likely been produced in previous nuclear experiments, it was first intentionally synthesized, isolated and identified in 1944, at the University of California, Berkeley, by Glenn T. Seaborg, Ralph A. James, and Albert Ghiorso. In their experiments, they used a cyclotron.\n\nCurium was chemically identified at the Metallurgical Laboratory (now Argonne National Laboratory) at the University of Chicago. It was the third transuranium element to be discovered even though it is the fourth in the series – the lighter element americium was unknown at the time.\n\nThe sample was prepared as follows: first plutonium nitrate solution was coated on a platinum foil of about 0.5 cm area, the solution was evaporated and the residue was converted into plutonium(IV) oxide (PuO) by annealing. Following cyclotron irradiation of the oxide, the coating was dissolved with nitric acid and then precipitated as the hydroxide using concentrated aqueous ammonia solution. The residue was dissolved in perchloric acid, and further separation was carried out by ion exchange to yield a certain isotope of curium. The separation of curium and americium was so painstaking that the Berkeley group initially called those elements \"pandemonium\" (from Greek for \"all demons\" or \"hell\") and \"delirium\" (from Latin for \"madness\").\n\nThe curium-242 isotope was produced in July–August 1944 by bombarding Pu with α-particles to produce curium with the release of a neutron:\n\nCurium-242 was unambiguously identified by the characteristic energy of the α-particles emitted during the decay:\nThe half-life of this alpha decay was first measured as 150 days and then corrected to 162.8 days.\n\nAnother isotope Cm was produced in a similar reaction in March 1945:\nThe half-life of the Cm α-decay was correctly determined as 26.7 days.\n\nThe discovery of curium, as well as americium, in 1944 was closely related to the Manhattan Project, so the results were confidential and declassified only in 1945. Seaborg leaked the synthesis of the elements 95 and 96 on the U.S. radio show for children, the Quiz Kids, five days before the official presentation at an American Chemical Society meeting on November 11, 1945, when one of the listeners asked whether any new transuranium element beside plutonium and neptunium had been discovered during the war. The discovery of curium (Cm and Cm), their production and compounds were later patented listing only Seaborg as the inventor.\nThe new element was named after Marie Skłodowska-Curie and her husband Pierre Curie who are noted for discovering radium and for their work in radioactivity. It followed the example of gadolinium, a lanthanide element above curium in the periodic table, which was named after the explorer of the rare earth elements Johan Gadolin:\n\nThe first curium samples were barely visible, and were identified by their radioactivity. Louis Werner and Isadore Perlman created the first substantial sample of 30 µg curium-242 hydroxide at the University of California in 1947 by bombarding americium-241 with neutrons. Macroscopic amounts of curium(III) fluoride were obtained in 1950 by W. W. T. Crane, J. C. Wallmann and B. B. Cunningham. Its magnetic susceptibility was very close to that of GdF providing the first experimental evidence for the +3 valence of curium in its compounds. Curium metal was produced only in 1951 by reduction of CmF with barium.\n\nA synthetic, radioactive element, curium is a hard, dense metal with a silvery-white appearance and physical and chemical properties resembling those of gadolinium. Its melting point of 1340 °C is significantly higher than that of the previous transuranic elements neptunium (637 °C), plutonium (639 °C) and americium (1173 °C). In comparison, gadolinium melts at 1312 °C. The boiling point of curium is 3110 °C. With a density of 13.52 g/cm, curium is significantly lighter than neptunium (20.45 g/cm) and plutonium (19.8 g/cm), but is heavier than most other metals. Between two crystalline forms of curium, the α-Cm is more stable at ambient conditions. It has a hexagonal symmetry, space group P6/mmc, lattice parameters \"a\" = 365 pm and \"c\" = 1182 pm, and four formula units per unit cell. The crystal consists of a double-hexagonal close packing with the layer sequence ABAC and so is isotypic with α-lanthanum. At pressures above 23 GPa, at room temperature, α-Cm transforms into β-Cm, which has a face-centered cubic symmetry, space group Fmm and the lattice constant \"a\" = 493 pm. Upon further compression to 43 GPa, curium transforms to an orthorhombic γ-Cm structure similar to that of α-uranium, with no further transitions observed up to 52 GPa. These three curium phases are also referred to as Cm I, II and III.\n\nCurium has peculiar magnetic properties. Whereas its neighbor element americium shows no deviation from Curie-Weiss paramagnetism in the entire temperature range, α-Cm transforms to an antiferromagnetic state upon cooling to 65–52 K, and β-Cm exhibits a ferrimagnetic transition at about 205 K. Meanwhile, curium pnictides show ferromagnetic transitions upon cooling: CmN and CmAs at 109 K, CmP at 73 K and CmSb at 162 K. The lanthanide analogue of curium, gadolinium, as well as its pnictides, also show magnetic transitions upon cooling, but the transition character is somewhat different: Gd and GdN become ferromagnetic, and GdP, GdAs and GdSb show antiferromagnetic ordering.\n\nIn accordance with magnetic data, electrical resistivity of curium increases with temperature – about twice between 4 and 60 K – and then remains nearly constant up to room temperature. There is a significant increase in resistivity over time (about 10 µΩ·cm/h) due to self-damage of the crystal lattice by alpha radiation. This makes uncertain the absolute resistivity value for curium (about 125 µΩ·cm). The resistivity of curium is similar to that of gadolinium and of the actinides plutonium and neptunium, but is significantly higher than that of americium, uranium, polonium and thorium.\n\nUnder ultraviolet illumination, curium(III) ions exhibit strong and stable yellow-orange fluorescence with a maximum in the range about 590–640 nm depending on their environment. The fluorescence originates from the transitions from the first excited state D and the ground state S. Analysis of this fluorescence allows monitoring interactions between Cm(III) ions in organic and inorganic complexes.\n\nCurium ions in solution almost exclusively assume the oxidation state of +3, which is the most stable oxidation state for curium. The +4 oxidation state is observed mainly in a few solid phases, such as CmO and CmF. Aqueous curium(IV) is only known in the presence of strong oxidizers such as potassium persulfate, and is easily reduced to curium(III) by radiolysis and even by water itself. The chemical behavior of curium is different from the actinides thorium and uranium, and is similar to that of americium and many lanthanides. In aqueous solution, the Cm ion is colorless to pale green, and Cm ion is pale yellow. The optical absorption of Cm ions contains three sharp peaks at 375.4, 381.2 and 396.5 nanometers and their strength can be directly converted into the concentration of the ions. The +6 oxidation state has only been reported once in solution in 1978, as the curyl ion (): this was prepared from the beta decay of americium-242 in the americium(V) ion . Failure to obtain Cm(VI) from oxidation of Cm(III) and Cm(IV) may be due to the high Cm/Cm ionization potential and the instability of Cm(V).\n\nCurium ions are hard Lewis acids and thus form most stable complexes with hard bases. The bonding is mostly ionic, with a small covalent component. Curium in its complexes commonly exhibits a 9-fold coordination environment, within a tricapped trigonal prismatic geometry.\n\nAbout 20 radioisotopes and 7 nuclear isomers between Cm and Cm are known for curium, and no stable isotopes. The longest half-lives have been reported for Cm (15.6 million years) and Cm (348,000 years). Other long-lived isotopes are Cm (half-life 8500 years), Cm (8,300 years) and Cm (4,760 years). Curium-250 is unusual in that it predominantly (about 86%) decays via spontaneous fission. The most commonly used curium isotopes are Cm and Cm with the half-lives of 162.8 days and 18.1 years, respectively.\nAll isotopes between Cm and Cm, as well as Cm, undergo a self-sustaining nuclear chain reaction and thus in principle can act as a nuclear fuel in a reactor. As in most transuranic elements, the nuclear fission cross section is especially high for the odd-mass curium isotopes Cm, Cm and Cm. These can be used in thermal-neutron reactors, whereas a mixture of curium isotopes is only suitable for fast breeder reactors since the even-mass isotopes are not fissile in a thermal reactor and accumulate as burn-up increases. The mixed-oxide (MOX) fuel, which is to be used in power reactors, should contain little or no curium because the neutron activation of Cm will create californium. Californium is a strong neutron emitter, and would pollute the back end of the fuel cycle and increase the dose to reactor personnel. Hence, if the minor actinides are to be used as fuel in a thermal neutron reactor, the curium should be excluded from the fuel or placed in special fuel rods where it is the only actinide present.\n\nThe adjacent table lists the critical masses for curium isotopes for a sphere, without a moderator and reflector. With a metal reflector (30 cm of steel), the critical masses of the odd isotopes are about 3–4 kg. When using water (thickness ~20–30 cm) as the reflector, the critical mass can be as small as 59 gram for Cm, 155 gram for Cm and 1550 gram for Cm. There is a significant uncertainty in these critical mass values. Whereas it is usually on the order of 20%, the values for Cm and Cm were listed as large as 371 kg and 70.1 kg, respectively, by some research groups.\n\nCurrently, curium is not used as a nuclear fuel owing to its low availability and high price. Cm and Cm have a very small critical mass and therefore could be used in portable nuclear weapons, but none have been reported thus far. Curium-243 is not suitable for this purpose because of its short half-life and strong α emission which would result in excessive heat. Curium-247 would be highly suitable, having a half-life 647 times that of plutonium-239.\n\nThe longest-lived isotope of curium, Cm, has a half-life of 15.6 million years. Therefore, any primordial curium, that is curium present on the Earth during its formation, should have decayed by now, although some of it would be detectable as an extinct radionuclide as an excess of its nearly stable daughter U. Curium is produced artificially, in small quantities for research purposes. Furthermore, it occurs in spent nuclear fuel. Curium is present in nature in certain areas used for the atmospheric nuclear weapons tests, which were conducted between 1945 and 1980. So the analysis of the debris at the testing site of the first U.S. hydrogen bomb, Ivy Mike, (1 November 1952, Enewetak Atoll), beside einsteinium, fermium, plutonium and americium also revealed isotopes of berkelium, californium and curium, in particular Cm, Cm and smaller quantities of Cm, Cm and Cm. For reasons of military secrecy, this result was published only in 1956.\n\nAtmospheric curium compounds are poorly soluble in common solvents and mostly adhere to soil particles. Soil analysis revealed about 4,000 times higher concentration of curium at the sandy soil particles than in water present in the soil pores. An even higher ratio of about 18,000 was measured in loam soils.\n\nThe transuranic elements from americium to fermium, including curium, occurred naturally in the natural nuclear fission reactor at Oklo, but no longer do so.\n\nCurium is produced in small quantities in nuclear reactors, and by now only kilograms of it have been accumulated for the Cm and Cm and grams or even milligrams for heavier isotopes. This explains the high price of curium, which has been quoted at 160–185 USD per milligram, with a more recent estimate at 2,000 USD/g for Cm and 170 USD/g for Cm. In nuclear reactors, curium is formed from U in a series of nuclear reactions. In the first chain, U captures a neutron and converts into U, which via β decay transforms into Np and Pu.\n\nFurther neutron capture followed by β-decay produces the Am isotope of americium which further converts into Cm:\nFor research purposes, curium is obtained by irradiating not uranium but plutonium, which is available in large amounts from spent nuclear fuel. A much higher neutron flux is used for the irradiation that results in a different reaction chain and formation of Cm:\nCurium-244 decays into Pu by emission of alpha particle, but it also absorbs neutrons resulting in a small amount of heavier curium isotopes. Among those, Cm and Cm are popular in scientific research because of their long half-lives. However, the production rate of Cm in thermal neutron reactors is relatively low because of it is prone to undergo fission induced by thermal neutrons. Synthesis of Cm via neutron absorption is also rather unlikely because of the short half-life of the intermediate product Cm (64 min), which converts by β decay to the berkelium isotope Bk.\n\nThe above cascade of (\\ce n,γ) reactions produces a mixture of different curium isotopes. Their post-synthesis separation is cumbersome, and therefore a selective synthesis is desired. Curium-248 is favored for research purposes because of its long half-life. The most efficient preparation method of this isotope is via α-decay of the californium isotope Cf, which is available in relatively large quantities due to its long half-life (2.65 years). About 35–50 mg of Cm is being produced by this method every year. The associated reaction produces Cm with isotopic purity of 97%.\n\nAnother interesting for research isotope Cm can be obtained from the α-decay of Cf, and the latter isotope is produced in minute quantities from the β-decay of the berkelium isotope Bk.\n\nMost synthesis routines yield a mixture of different actinide isotopes as oxides, from which a certain isotope of curium needs to be separated. An example procedure could be to dissolve spent reactor fuel (e.g. MOX fuel) in nitric acid, and remove the bulk of the uranium and plutonium using a PUREX (Plutonium – URanium EXtraction) type extraction with tributyl phosphate in a hydrocarbon. The lanthanides and the remaining actinides are then separated from the aqueous residue (raffinate) by a diamide-based extraction to give, after stripping, a mixture of trivalent actinides and lanthanides. A curium compound is then selectively extracted using multi-step chromatographic and centrifugation techniques with an appropriate reagent. \"Bis\"-triazinyl bipyridine complex has been recently proposed as such reagent which is highly selective to curium. Separation of curium from a very similar americium can also be achieved by treating a slurry of their hydroxides in aqueous sodium bicarbonate with ozone at elevated temperature. Both americium and curium are present in solutions mostly in the +3 valence state; whereas americium oxidizes to soluble Am(IV) complexes, curium remains unchanged and can thus be isolated by repeated centrifugation.\n\nMetallic curium is obtained by reduction of its compounds. Initially, curium(III) fluoride was used for this purpose. The reaction was conducted in the environment free from water and oxygen, in the apparatus made of tantalum and tungsten, using elemental barium or lithium as reducing agents.\n\nAnother possibility is the reduction of curium(IV) oxide using a magnesium-zinc alloy in a melt of magnesium chloride and magnesium fluoride.\n\nCurium readily reacts with oxygen forming mostly CmO and CmO oxides, but the divalent oxide CmO is also known. Black CmO can be obtained by burning curium oxalate (Cm(CO)), nitrate (Cm(NO)) or hydroxide in pure oxygen. Upon heating to 600–650 °C in vacuum (about 0.01 Pa), it transforms into the whitish CmO:\n\nAlternatively, CmO can be obtained by reducing CmO with molecular hydrogen:\n\nFurthermore, a number of ternary oxides of the type M(II)CmO are known, where M stands for a divalent metal, such as barium.\n\nThermal oxidation of trace quantities of curium hydride (CmH) has been reported to produce a volatile form of CmO and the volatile trioxide CmO, one of the two known examples of the very rare +6 state for curium. Another observed species was reported to behave similarly to a supposed plutonium tetroxide and was tentatively characterized as CmO, with curium in the extremely rare +8 state; however, new experiments seem to indicate that CmO does not exist, and have cast doubt on the existence of PuO as well.\n\nThe colorless curium(III) fluoride (CmF) can be produced by introducing fluoride ions into curium(III)-containing solutions. The brown tetravalent curium(IV) fluoride (CmF) on the other hand is only obtained by reacting curium(III) fluoride with molecular fluorine:\n\nA series of ternary fluorides are known of the form ACmF, where A stands for alkali metal.\n\nThe colorless curium(III) chloride (CmCl) is produced in the reaction of curium(III) hydroxide (Cm(OH)) with anhydrous hydrogen chloride gas. It can further be converted into other halides, such as curium(III) bromide (colorless to light green) and curium(III) iodide (colorless), by reacting it with the ammonia salt of the corresponding halide at elevated temperature of about 400–450 °C:\n\nAn alternative procedure is heating curium oxide to about 600 °C with the corresponding acid (such as hydrobromic for curium bromide). Vapor phase hydrolysis of curium(III) chloride results in curium oxychloride:\n\nSulfides, selenides and tellurides of curium have been obtained by treating curium with gaseous sulfur, selenium or tellurium in vacuum at elevated temperature. The pnictides of curium of the type CmX are known for the elements nitrogen, phosphorus, arsenic and antimony. They can be prepared by reacting either curium(III) hydride (CmH) or metallic curium with these elements at elevated temperatures.\n\nOrganometallic complexes analogous to uranocene are known also for other actinides, such as thorium, protactinium, neptunium, plutonium and americium. Molecular orbital theory predicts a stable \"curocene\" complex (η-CH)Cm, but it has not been reported experimentally yet.\n\nFormation of the complexes of the type Cm(n-CH-BTP), where BTP stands for 2,6-di(1,2,4-triazin-3-yl)pyridine, in solutions containing n-CH-BTP and Cm ions has been confirmed by EXAFS. Some of these BTP-type complexes selectively interact with curium and therefore are useful in its selective separation from lanthanides and another actinides. Dissolved Cm ions bind with many organic compounds, such as hydroxamic acid, urea, fluorescein and adenosine triphosphate. Many of these compounds are related to biological activity of various microorganisms. The resulting complexes exhibit strong yellow-orange emission under UV light excitation, which is convenient not only for their detection, but also for studying the interactions between the Cm ion and the ligands via changes in the half-life (of the order ~0.1 ms) and spectrum of the fluorescence.\n\nCurium has no biological significance. There are a few reports on biosorption of Cm by bacteria and archaea, however no evidence for incorporation of curium into them.\n\nCurium is one of the most radioactive isolable elements. Its two most common isotopes Cm and Cm are strong alpha emitters (energy 6 MeV); they have relatively short half-lives of 162.8 days and 18.1 years, and produce as much as 120 W/g and 3 W/g of thermal energy, respectively. Therefore, curium can be used in its common oxide form in radioisotope thermoelectric generators like those in spacecraft. This application has been studied for the Cm isotope, while Cm was abandoned due to its prohibitive price of around 2000 USD/g. Cm with a ~30 year half-life and good energy yield of ~1.6 W/g could make a suitable fuel, but it produces significant amounts of harmful gamma and beta radiation from radioactive decay products. Though as an α-emitter, Cm requires a much thinner radiation protection shielding, it has a high spontaneous fission rate, and thus the neutron and gamma radiation rate are relatively strong. As compared to a competing thermoelectric generator isotope such as Pu, Cm emits a 500-fold greater fluence of neutrons, and its higher gamma emission requires a shield that is 20 times thicker — about 2 inches of lead for a 1 kW source, as compared to 0.1 in for Pu. Therefore, this application of curium is currently considered impractical.\n\nA more promising application of Cm is to produce Pu, a more suitable radioisotope for thermoelectric generators such as in cardiac pacemakers. The alternative routes to Pu use the (n,γ) reaction of Np, or the deuteron bombardment of uranium, which both always produce Pu as an undesired by-product — since the latter decays to U with strong gamma emission. Curium is also a common starting material for the production of higher transuranic elements and transactinides. Thus, bombardment of Cm with neon (Ne), magnesium (Mg), or calcium (Ca) yielded certain isotopes of seaborgium (Sg), hassium (Hs and Hs), and livermorium (Lv, Lv, and possibly Lv). Californium was discovered when a microgram-sized target of curium-242 was irradiated with 35 MeV alpha particles using the cyclotron at Berkeley:\nOnly about 5,000 atoms of californium were produced in this experiment.\n\nThe most practical application of Cm — though rather limited in total volume — is as α-particle source in the alpha particle X-ray spectrometers (APXS). These instruments were installed on the Sojourner, Mars, Mars 96, Mars Exploration Rovers and Philae comet lander, as well as the Mars Science Laboratory to analyze the composition and structure of the rocks on the surface of planet Mars. APXS was also used in the Surveyor 5–7 moon probes but with a Cm source.\n\nAn elaborated APXS setup is equipped with a sensor head containing six curium sources having the total radioactive decay rate of several tens of millicuries (roughly a gigabecquerel). The sources are collimated on the sample, and the energy spectra of the alpha particles and protons scattered from the sample are analyzed (the proton analysis is implemented only in some spectrometers). These spectra contain quantitative information on all major elements in the samples except for hydrogen, helium and lithium.\n\nOwing to its high radioactivity, curium and its compounds must be handled in appropriate laboratories under special arrangements. Whereas curium itself mostly emits α-particles which are absorbed by thin layers of common materials, some of its decay products emit significant fractions of beta and gamma radiation, which require a more elaborate protection. If consumed, curium is excreted within a few days and only 0.05% is absorbed in the blood. From there, about 45% goes to the liver, 45% to the bones, and the remaining 10% is excreted. In the bone, curium accumulates on the inside of the interfaces to the bone marrow and does not significantly redistribute with time; its radiation destroys bone marrow and thus stops red blood cell creation. The biological half-life of curium is about 20 years in the liver and 50 years in the bones. Curium is absorbed in the body much more strongly via inhalation, and the allowed total dose of Cm in soluble form is 0.3 μC. Intravenous injection of Cm and Cm containing solutions to rats increased the incidence of bone tumor, and inhalation promoted pulmonary and liver cancer.\n\nCurium isotopes are inevitably present in spent nuclear fuel with a concentration of about 20 g/tonne. Among them, the Cm–Cm isotopes have decay times of thousands of years and need to be removed to neutralize the fuel for disposal. The associated procedure involves several steps, where curium is first separated and then converted by neutron bombardment in special reactors to short-lived nuclides. This procedure, nuclear transmutation, while well documented for other elements, is still being developed for curium.\n\n\n"}
{"id": "44562388", "url": "https://en.wikipedia.org/wiki?curid=44562388", "title": "Dahlander pole changing motor", "text": "Dahlander pole changing motor\n\nA Dahlander motor (also known as a pole changing motor, dual- or two speed-motor) is a type of multispeed induction motor, in which the speed of the motor is varied by altering the number of poles; this is achieved by altering the wiring connections inside the motor. The motor may have fixed or variable torque depending on the stator winding. It is named after its inventor Robert Dahlander (1870–1935).\n\nRobert Dahlander, a Swedish engineer working for ASEA, discovered that switching the poles in a motor led to a reduction in the speed of the motor. In 1897 he invented an electrical configuration to switch between poles in a motor for which he was granted a patent along with his co-worker Karl Arvid Lindstroem. The new connection was named the \"Dahlander connection\" and a motor having such a configuration is commonly referred to as a \"pole changing motor\" or a \"Dahlander motor\".\n\nThe Dahlander motor is based on a 'consequent pole' connection. The primary factor in determining the speed of an induction motor is the number of poles, given by the formula\nwhere\nA regular induction motor has an equal number of opposite poles; that is, at any instant, there are an equal number of North and South magnetic poles. Some smaller induction motors are connected so that all the poles are identical, causing the motor to act as though there is an equal number of opposite poles in between.\n\nA Dahlander motor achieves different speeds by switching the configuration of the electrical windings, indirectly adding or removing poles and thus varying the rotor speed. The poles can be varied at a ratio of 1:2 and thus the speed can be varied at 2:1. Normally, the electrical configuration of windings is varied from a Delta connection (Δ) to a double star connection (YY) configuration in order to change the speed of the motor for constant torque applications, such as the hoists in cranes. Star connections (Y) varied to double star connections (YY) are used for quadratic torque applications, such as pumps.\n\nDahlander motors have advantages compared to other speed control systems like variable frequency drives, as there is less power loss. This is because most of the power is used to drive the motor and no electrical pulse switching is done. The system is much simpler and easier to use compared to other speed control methods available. However, the Dahlander motor has the disadvantage of fast mechanical wear and tear due to changing speeds in such a drastic ratio; this type of connection also produces high harmonic distortion during the shifting of poles as the angular distance between the power generated increases as the poles are decreased in the motor.\n\nPole changing motors are normally used in applications where two speed controls are necessary. Some typical applications are:\n\n"}
{"id": "48870431", "url": "https://en.wikipedia.org/wiki?curid=48870431", "title": "Disordered hyperuniformity", "text": "Disordered hyperuniformity\n\nDisordered hyperuniformity is a type of liquid which has crystal properties. It greatly suppresses variations in the density of particles, like a crystal, and the particles have the same physical properties in all directions at shorter distances, like a liquid. It was discovered in the eyes of chickens. This is thought to be the case because chicken eyes cannot support the ordered, complex system best for eyesight. This may eventually be used for self-organizing colloids or optics with the ability to transmit light with crystal efficiency while still retaining liquid flexibility. \n\nUnique optical properties have been uncovered in dense hyperuniform materials, wherein light of a wavelength specific to the material is able to propagate forward despite high particle density due to microscopic order. The uniformly spaced particles scatter light as it propagates through the material, but most of the scattering self-interferes except that in the direction of propagation. In conditions where light is propagated through an uncorrelated disordered material of the same density, the material would appear opaque due to multiple scattering. Such materials can be theoretically designed for light of any wavelength, and the applications of the concept cover a wide variety of fields of wave physics and materials engineering.\n\nThe term \"hyperuniformity\" was coined by chemist and packing expert Salvatore Torquato, co-author of a pioneering 2003 paper on the topic. Torquato says that another example of this ordering is that found in a shaken box of marbles, which fall into an arrangement, called \"maximally random jammed packing\". Two classes of hyperuniformity include equilibrium systems - such as quasicrystals - and non-equilibrium systems, which include shaken marbles, emulsions, colloids and ensembles of cold atoms. It's also thought to emerge on the mysterious biological patterns known as fairy circles - circle and patterns of circles that emerge in arid places.\n\n\n"}
{"id": "36408563", "url": "https://en.wikipedia.org/wiki?curid=36408563", "title": "Double sunset", "text": "Double sunset\n\nA double sunset is a rare astro-geographical phenomenon, in which the sun appears to set twice in the same evening from a specific viewing-point. Such phenomena may have been regarded as significant in prehistoric times, and double sunsets have been discussed in the context of archaeoastronomy by researchers such as Alexander Thom.\n\nA well-documented example of a double or occulted sunset is associated with Leek, Staffordshire, England. The phenomenon is viewable from the town on and around the summer solstice in good weather.\n\nThe first published mention of the Leek double sunset was made in 1686 by Dr Robert Plot in his book \"The Natural History Of Stafford-Shire\". The phenomenon would have been visible well before the seventeenth century. However, the alignment of sun and landscape is subject to change over the centuries as it is affected by the Earth's axial precession. This was realised by Plot who suggested that the sunset could be used to measure the obliquity of the ecliptic.\n\nThe traditional location for observing the phenomenon, as described by Plot, is the churchyard belonging to the parish church of St Edward the Confessor. The church is a medieval building, and it has been conjectured that the churchyard is an example of an ancient sacred site having been Christianised. Because of the chronology of the changing alignment, it seems that the site could not have been a viewing-point for the double sunset before the Iron Age. The first people to view the phenomenon may have been the area's Iron Age inhabitants.\nIt has been argued by Jeff Kent that the first people to witness the spectacle may have been Danish settlers from the Great Heathen Army, which invaded England in the ninth century. \n\nFrom a particular point in the churchyard, the whole of the sun set on the summit of Bosley Cloud, a millstone grit hill six miles to the northwest. The sun partially reappeared from The Cloud's steep northern slope and soon afterwards set for a second and final time on the horizon. The spectacle was last reliably witnessed, and filmed, from the churchyard in 1977, but is no longer visible from the location because of tree interference. It is, however, still observable from Leek on and around the summer solstice from the road to Pickwood Hall, off Milltown Way, and from Lowe Hill on the outskirts of the town. Better viewing points, though, are from the A 523, above Rudyard Lake, and Woodhouse Green, both of which are nearer to The Cloud and therefore enable a larger proportion of the sun to reappear.\n\nFurther double sunsets were identified by Jeff Kent in 1997 from three places in west Derbyshire, observed against Chrome Hill, Parkhouse Hill and Thorpe Cloud (three limestone reef knolls):\n\nIn the 1950s Alexander Thom surveyed a megalithic site at Kintraw, a locality on the mainland of Scotland, in the context of a double sunset on the island of Jura (both the island and the mainland site are in Argyll and Bute). The proposed alignment is to a notch at a distance of 28 miles between the mountains of Beinn Shiantaidh and Beinn a' Chaolais which are visible from Kintraw. \n\nThom described the site as a type of midwinter observatory, but his interpretation has been the subject of controversy, one point at issue being the visibility of the midwinter notch: a higher observation point is needed to see the midwinter notch on Jura over a nearby ridge.\nEuan MacKie, recognising that Thom's theories needed to be tested, excavated at the site in 1970 and 1971, and found evidence for an observation platform.\n\nNotes\n\nReferences\nBibliography\n"}
{"id": "29857100", "url": "https://en.wikipedia.org/wiki?curid=29857100", "title": "Fereydoon Abbasi", "text": "Fereydoon Abbasi\n\nFereydoon Abbasi-Davani (; born 11 July 1958) is an Iranian nuclear scientist who was head of Atomic Energy Organization from 2011 to 2013. He survived an assassination attempt in 2010, but was seriously wounded.\n\nAbbasi was born in Kazerun, Iran, on 11 July 1958. According to Mashregh News, an Iranian news website, he holds a PhD in nuclear physics.\n\nAbbasi was a professor of nuclear physics at Shahid Beheshti University and has reportedly been a member of Iran's Islamic Revolutionary Guard Corps (IRGC) since the 1979 Islamic Revolution. He reportedly did nuclear research at the Atomic Energy Organization of Iran (AEOI). Prior to his appointment as head of the AEOI he chaired the physics department at Tehran's Imam Hossein University.\n\nAbbasi was appointed head of AEOI by then President Mahmoud Ahmadinejad on 13 February 2011 to succeed Ali Akbar Salehi. In his tenure the AEOI was unwilling to cooperate with the International Atomic Energy Agency (IAEA). In fact in May 2011 Fereydoon Abbasi was the recipient of a letter from the IAEA, where its Director General Yukiya Amano reiterated the agency's concerns about the \"existence of a possible military dimension\" to Iran's nuclear programme and stressed the importance of clarifying these issues, requesting prompt access to relevant locations, equipment, documentation and persons. In June 2011 Abbasi replayed evasively, so much so that the IAEA Director could only repeat his request for credible assurances.\n\nAbbasi was removed from office on 16 August 2013 and Salehi replaced him in the post.\n\nAbbasi has regularly been linked to Iran's alleged efforts to make the nuclear weapon, a process called weaponization. According to an Institute for Science and International Security report citing an expert close to the International Atomic Energy Agency (IAEA), Abbasi was a key scientist in the alleged Iranian covert nuclear weapons program headed by Mohsen Fakhrizadeh-Mahabadi, a strong supporter of Iran's alleged nuclear weapons program. Abbasi personally directed work to calculate the yield of a nuclear weapon as well as work on high energy neutron sources, this expert added.\n\nAccording to the same report, the IAEA had information that Abbasi was the head of the Institute of Applied Physics (IAP), which was a follow-on organization to the Physics Research Center. Both of the organizations acted as fronts for scientific work on a possible Iranian nuclear weapons program.\n\nAbbasi is \"listed in an annex to U.N. Security Council Resolution 1747 of 24 March 2007, as a person involved in Iran's nuclear or ballistic missile activities\". This resolution imposes an asset freeze and travel notification requirements. Abbasi is described as a \"Senior Ministry of Defence and Armed Forces Logistics (MODAFL) scientist with links to the Institute of Applied Physics, working closely with Mahabadi (also designated by the UN).\n\nOn 29 November 2010, Abbasi was seriously wounded and narrowly survived an assassination attempt on a Tehran street, as a man on a motorbike attached a bomb to his car as he drove to work. A separate similar bomb attack the same day killed another scientist, Dr. Majid Shahriari, who also taught at Shahid Beheshti University.\n\n"}
{"id": "43270387", "url": "https://en.wikipedia.org/wiki?curid=43270387", "title": "Formvar", "text": "Formvar\n\nFormvar refers to any of several thermoplastic resins that are polyvinyl formals, which are polymers formed from polyvinyl alcohol and formaldehyde as copolymers with polyvinyl acetate. They are typically used as coatings, adhesives, and molding materials. \"Formvar\" used to be the registered trade name of the polyvinyl formal resin produced by Monsanto Chemical Company in St. Louis, Missouri. That manufacturing unit was sold and formvar is now distributed under the name \"Vinylec\".\n\nFormvar is used in many different applications, such as wire insulation, coatings for musical instruments, magnetic tape backing, and support films for electron microscopy. Formvar is also used as a main ingredient for special adhesives in structural applications such as the aircraft industry.\n\nThe major application of formvar resins is as electrical insulation for magnet wire. It is combined with other \"wire enamels\" which are then coated onto copper wire and cured in an oven to create a crosslinked film coating.\n\nMost specimens used in transmission electron microscopy (TEM) need to be supported by a thin electron-transparent film to hold the sample in place. Formvar films are a common choice of film grid for TEM. Formvar is favored because it allows users to utilize grids with lower mesh rating.\n\nFormvar resin has a high softening point and strong electric insulation properties. It is also very flexible, water-insoluble, and resistant to abrasion. Formvar is also halogen free. Formvar resins are combustible and can cause dust explosions. For this reason exposure to heat, sparks, and flame should be avoided. Formvar is most commonly dissolved in ethylene dichloride, chloroform, and dioxane.\n"}
{"id": "3514745", "url": "https://en.wikipedia.org/wiki?curid=3514745", "title": "Gallium phosphide", "text": "Gallium phosphide\n\nGallium phosphide (), a phosphide of gallium, is a compound semiconductor material with an indirect band gap of 2.26 eV(300K). The polycrystalline material has the appearance of pale orange pieces. Undoped single crystal wafers appear clear orange, but strongly doped wafers appear darker due to free-carrier absorption. It is odorless and insoluble in water.\n\nSulfur or tellurium are used as dopants to produce n-type semiconductors. Zinc is used as a dopant for the p-type semiconductor.\n\nGallium phosphide has applications in optical systems. Its refractive index is between 4.30 at 262 nm (UV), 3.45 at 550 nm (green) and 3.19 at 840 nm (IR), which is higher than in most known materials, including diamond (2.4).\n\nGallium phosphide has been used in the manufacture of low-cost red, orange, and green light-emitting diodes (LEDs) with low to medium brightness since the 1960s. It has a relatively short life at higher current and its lifetime is sensitive to temperature. It is used standalone or together with gallium arsenide phosphide.\n\nPure GaP LEDs emit green light at a wavelength of 555 nm. Nitrogen-doped GaP emits yellow-green (565 nm) light, zinc oxide doped GaP emits red (700 nm).\n\nGallium phosphide is transparent for yellow and red light, therefore GaAsP-on-GaP LEDs are more efficient than GaAsP-on-GaAs.\n\nAt temperatures above ~900 °C, gallium phosphide dissociates and the phosphorus escapes as a gas. In crystal growth from a 1500 °C melt (for LED wafers), this must be prevented by holding the phosphorus in with a blanket of molten boric oxide in inert gas pressure of 10-100 atmospheres. The process is called Liquid Encapsulated Czochralski (LEC) growth, an elaboration of the Czochralski process used for silicon wafers.\n\n\n\n"}
{"id": "92582", "url": "https://en.wikipedia.org/wiki?curid=92582", "title": "Giant (1956 film)", "text": "Giant (1956 film)\n\nGiant is a 1956 American epic Western drama film, directed by George Stevens from a screenplay adapted by Fred Guiol and Ivan Moffat from Edna Ferber's 1952 novel. The film stars Elizabeth Taylor, Rock Hudson and James Dean and features Carroll Baker, Jane Withers, Chill Wills, Mercedes McCambridge, Dennis Hopper, Sal Mineo, Rod Taylor, Elsa Cardenas and Earl Holliman. \"Giant \" was the last of James Dean's three films as a leading actor, and earned him his second and last Academy Award nomination – he was killed in a car accident before the film was released. Nick Adams was called in to do some voice dubbing for Dean's role.\n\nIn 2005, the film was selected for preservation in the United States National Film Registry by the Library of Congress as being \"culturally, historically, or aesthetically significant\".\n\nWealthy Texas rancher Jordan \"Bick\" Benedict Jr. travels to Maryland to buy a horse. There he meets and courts socialite Leslie Lynnton, who ends a budding relationship with British diplomat Sir David Karfrey and marries Bick after a whirlwind romance. They return to Texas to start their life together on the family ranch, Reata, where Bick's older sister Luz runs the household. Leslie meets Jett Rink, a local handyman, and he becomes infatuated with her. On a ride with Jett, Leslie discovers the local Mexican workers' living conditions are terrible. After tending to one of the Mexican children, Angel Obregon II, she presses Bick to take steps to improve their condition.\n\nWhen riding Leslie's beloved horse, War Winds, Luz expresses her hostility for Leslie by cruelly digging in her spurs. War Winds bucks her off, killing her. She leaves Jett a small piece of land on the Benedict ranch. Bick, who despises Jett, tries to buy back the land, but Jett refuses to sell. Jett makes the land his home and names it Little Reata. Over the next ten years, Leslie and Bick have twins, Jordan III (\"Jordy\") and Judy, and later have a daughter, Luz II.\n\nAfter discovering oil within a footprint left by Leslie, Jett begins digging and strikes oil on his land. He then drives to the Benedict house, covered in crude, to proclaim to the Benedicts that he will be richer than them. Jett makes a pass at Leslie, and this leads to a brief fistfight with Bick before he drives off. Jett's oil drilling company prospers over the years, and he tries to persuade Bick to let him drill for oil on Reata. Bick is determined to preserve his family legacy, however, and refuses.\n\nMeanwhile, tensions arise regarding the now-grown children. Bick insists that Jordy succeed him and run the ranch, but Jordy wants to become a doctor. Leslie wants Judy to attend finishing school in Switzerland, but Judy wants to study animal husbandry at Texas Tech. Both children succeed in pursuing their own vocations, each asking one parent to convince the other to let them have their way.\n\nAt the family Christmas party, Bick tries to interest Judy's new husband, Bob Dace, in working on the ranch after he returns from the recently declared war, but Dace refuses. Jett arrives and persuades Bick to allow oil production on his land. Realizing that his children will not take over the ranch when he retires, Bick agrees. Luz II, now in her teens, starts flirting with Jett. Once oil production starts on the ranch, the Benedict family becomes even wealthier and more powerful. Meanwhile, the now-grown Angel is killed in the war, and his body is sent home for burial.\n\nThe Benedict–Rink rivalry comes to a head when the Benedicts discover that Luz II and the much older Jett have been dating. At a huge party given by Jett in his own honor at his hotel in Austin, he orders his staff not to serve Jordy's Mexican wife, Juana. Enraged, Jordy tries to start a fight with Jett, who beats him and has him thrown out. Fed up, Bick challenges Jett to a fight. Drunk and almost incoherent, Jett leads the way to a wine storage room. Seeing that Jett is in no state to defend himself, Bick lowers his fists, and instead topples Jett's wine cellar shelves. The Benedict family leaves the party. Jett staggers into the banquet hall, takes his seat of honor, and passes out on the table. Later, Luz II sees Jett drunkenly bemoaning his unrequited love for Leslie. Luz II leaves, heartbroken, as Jett falls over onto the floor.\n\nThe next day, on their way home, the Benedicts stop at a diner with a sign at the counter saying, \"We reserve the right to refuse service to anyone,\" which during the mid-twentieth century in the southern United States was often taken as meaning that ethnic minorities were not welcome. The racist owner, Sarge, insults Juana and her and Jordy's son Jordan IV. When the owner goes on to eject a Mexican family from the diner, Bick tells Sarge to leave them alone. Bick fights Sarge, who beats him, but then takes down the sign and tosses it on top of Bick. Back at the ranch, Bick laments that he has failed to preserve the Benedict family legacy. Leslie replies that, after the fight in the diner, he was her hero for the first time, and that she considers their own family legacy a success. They look at their two grandchildren, one white and one Hispanic.\n\nNick Adams recorded some of James Dean's dialogue after Dean's death.\n\nFerber's character of Jordan Benedict II and her description of the Reata Ranch were based on Robert \"Bob\" J. Kleberg Jr. (1896–1974) and the King Ranch in Kingsville, Texas. Like the over half-million-acre Reata, King Ranch comprises 825,000 acres (3,340 km; 1,289 sq mi) and includes portions of six Texas counties, including most of Kleberg County and much of Kenedy County, and was largely a livestock ranch before the discovery of oil. The fictional character Jett Rink was inspired partly by the extraordinary rags-to-riches life story of the wildcatter oil tycoon Glenn Herbert McCarthy (1907–1988). Author Edna Ferber met McCarthy when she was a guest at his Houston, Texas, Shamrock Hotel (known as the Shamrock Hilton after 1955), the fictional Emperador Hotel in both the book and the film.\n\nThe Australian actor Rod Taylor was cast in one of his earliest Hollywood roles after being seen in an episode of \"Studio 57\", \"The Black Sheep's Daughter\".\n\nStevens gave Hudson a choice between Elizabeth Taylor and Grace Kelly to play his leading lady, Leslie. Hudson chose Taylor.\n\nAfter James Dean's death late in production, Nick Adams overdubbed some of Dean's lines, which were nearly inaudible, as Rink's voice. George Stevens had a reputation as a meticulous film editor, and the film spent an entire year in the editing room.\n\nThe film begins with Jordan \"Bick\" Benedict, played by Hudson, arriving at Ardmore, Maryland, to purchase a stallion from the Lynnton family. The first part of the picture was actually shot in Albemarle County, Virginia, and used the Keswick, Virginia, railroad station as the Ardmore railway depot. \nMuch of the subsequent film, depicting \"Reata\", the Benedict ranch, was shot in and around the town of Marfa, Texas, and the remote, dry plains found nearby, with interiors filmed at the Warner Brothers studios in Burbank, California. The \"Jett Rink Day\" parade and airport festivities were filmed at the Burbank Airport.\n\nDuring the restaurant fight scene at the film's end, the jukebox plays \"The Yellow Rose of Texas\" by Mitch Miller. That version of the song happened to be the #1 pop single on the \"Billboard Magazine\" Best Sellers chart at the time of James Dean's death.\n\nThe movie is an epic portrayal of a powerful Texas ranching family challenged by changing times and the coming of big oil. A major subplot concerns the racism of many Anglo-European Americans in Texas during the mid twentieth century and the discriminatory social segregation enforced against Mexican Americans. In the first third of the film, Bick and Luz treat the Mexicans who work on their ranch condescendingly, which upsets the more socially-conscious Leslie. Bick eventually comes to realize his moral shortcomings – in a climactic scene at a roadside diner he loses a fistfight to the racist owner, but earns Leslie's respect for defending the human rights of his brown-skinned daughter-in-law and grandson. Another subplot involves Leslie's own striving for women's equal rights as she defies the patriarchal social order, asserting herself and expressing her own opinions when the men talk. She protests being expected to suppress her beliefs in deference to Bick's; this conflict leads to their temporary separation.\n\n\"Giant\" is Edna Ferber's third novel dealing with racism; the first was \"Show Boat\" (1926), which was adapted into the legendary Broadway musical \"Show Boat\" (1927); her second was \"Cimarron\" (1929), which was adapted to film twice, in 1931 and 1960.\n\n\"Giant\" premiered in New York City on October 10, 1956, with the local DuMont station, WABD, televising the arrival of cast and crew, as well as other celebrities and studio chief Jack L. Warner. The picture was released to nationwide distribution on November 24, 1956.\n\nCapitol Records, which had issued some of Dimitri Tiomkin's music from the soundtrack (with the composer conducting the Warner Brothers studio orchestra) on an LP, later digitally remastered the tracks and issued them on CD, including two tracks conducted by Ray Heindorf. Both versions used a monaural blend of the multi-channel soundtrack recording.\n\n\"Giant\" won praise from both critics and the public, and according to the Texan author, Larry McMurtry, was especially popular with Texans, even though it was sharply critical of Texan society. Bosley Crowther of the \"New York Times\" wrote that \"George Stevens takes three hours and seventeen minutes to put his story across. That's a heap of time to go on about Texas, but Mr. Stevens has made a heap of film.\" He continued to write that \"\"Giant\", for all its complexity, is a strong contender for the year's top-film award.\"\n\n\"Variety\"'s \"Hift\" claimed that \"Giant\" was \"for the most part, an excellent film which registers strongly on all levels, whether it's in its breathtaking panoramic shots of the dusty Texas plains; the personal, dramatic impact of the story itself, or the resounding message it has to impart.\"\n\n\"TV Guide\" gave the film four stars out of five, saying of James Dean's performance \"This was the last role in Dean's all-too-brief career – he was dead when the film was released – and his presence ran away with the film. He performs his role in the overwrought method manner of the era, and the rest of the cast seems to be split between awe of his talent and disgust over his indulgence.\"\n\nThe film received a 95% approval rating on the review aggregator site Rotten Tomatoes, based on 39 reviews, with an average rating of 7.6/10.\n\nLess praising was director and critic Francois Truffaut, who in an early review called \"Giant\" a ”silly, solemn, sly, parternalistic, demagogic movie without any boldness, rich in all sorts of concessions, pettiness, and contemptible actions.”\n\nThe movie earned $12 million in rentals in North America during its initial release. It was one of the biggest hits of the year in France, with admissions of 3,723,209.\n\n\"Giant\" won the Academy Award for Best Director and was nominated nine other times, twice for Best Actor in a Leading Role (James Dean and Rock Hudson). The other nominations came in the categories of Best Actress in a Supporting Role (Mercedes McCambridge); Best Art Direction–Set Decoration, Color (Boris Leven, Ralph S. Hurst); Best Costume Design, Color; Best Film Editing; Best Music, Scoring of a Dramatic or Comedy Picture; Best Picture; and Best Writing, Best Screenplay – Adapted.\n\n\n"}
{"id": "870857", "url": "https://en.wikipedia.org/wiki?curid=870857", "title": "Glow discharge", "text": "Glow discharge\n\nA glow discharge is a plasma formed by the passage of electric current through a gas. It is often created by applying a voltage between two electrodes in a glass tube containing a low-pressure gas. When the voltage exceeds a value called the striking voltage, the gas ionization becomes self-sustaining, and the tube glows with a colored light. The color depends on the gas used.\n\nGlow discharges are used as a source of light in devices such as neon lights, fluorescent lamps, and plasma-screen televisions. Analyzing the light produced with spectroscopy can reveal information about the atomic interactions in the gas, so glow discharges are used in plasma physics and analytical chemistry. They are also used in the surface treatment technique called sputtering.\n\nConduction in a gas requires charge carriers, which can be either electrons or ions. Charge carriers come from ionizing some of the gas molecules. In terms of current flow, glow discharge falls between dark discharge and arc discharge.\n\n\nBelow the breakdown voltage there is no or little glow and the electric field is uniform. When the electric field increases enough to cause ionization, the Townsend discharge starts. When a glow discharge develops, the electric field is considerably modified by the presence of positive ions; the field is concentrated near the cathode. The glow discharge starts as a normal glow. As the current is increased, more of the cathode surface is involved in the glow. When the current is increased above the level where the entire cathode surface is involved, the discharge is known as an abnormal glow. If the current is increased still further, other factors come into play and an arc discharge begins.\n\nThe simplest type of glow discharge is a direct-current glow discharge. In its simplest form, it consists of two electrodes in a cell held at low pressure (0.1–10 torr; about 1/10000th to 1/100th of atmospheric pressure). A low pressure is used to increase the mean free path; for a fixed electric field, a longer mean free path allows a charged particle to gain more energy before colliding with another particle. The cell is typically filled with neon, but other gases can also be used. An electric potential of several hundred volts is applied between the two electrodes. A small fraction of the population of atoms within the cell is initially ionized through random processes, such as thermal collisions between atoms or by gamma rays. The positive ions are driven towards the cathode by the electric potential, and the electrons are driven towards the anode by the same potential. The initial population of ions and electrons collides with other atoms, exciting or ionizing them. As long as the potential is maintained, a population of ions and electrons remains.\n\nSome of the ions' kinetic energy is transferred to the cathode. This happens partially through the ions striking the cathode directly. The primary mechanism, however, is less direct. Ions strike the more numerous neutral gas atoms, transferring a portion of their energy to them. These neutral atoms then strike the cathode. Whichever species (ions or atoms) strike the cathode, collisions within the cathode redistribute this energy resulting in electrons ejected from the cathode. This process is known as secondary electron emission. Once free of the cathode, the electric field accelerates electrons into the bulk of the glow discharge. Atoms can then be excited by collisions with ions, electrons, or other atoms that have been previously excited by collisions.\n\nOnce excited, atoms will lose their energy fairly quickly. Of the various ways that this energy can be lost, the most important is radiatively, meaning that a photon is released to carry the energy away. In optical atomic spectroscopy, the wavelength of this photon can be used to determine the identity of the atom (that is, which chemical element it is) and the number of photons is directly proportional to the concentration of that element in the sample. Some collisions (those of high enough energy) will cause ionization. In atomic mass spectrometry, these ions are detected. Their mass identifies the type of atoms and their quantity reveals the amount of that element in the sample.\n\nThe illustrations to the right shows the main regions that may be present in a glow discharge. Regions described as \"glows\" emit significant light; regions labeled as \"dark spaces\" do not. As the discharge becomes more extended (i.e., stretched horizontally in the geometry of the illustrations), the positive column may become striated. That is, alternating dark and bright regions may form. Compressing the discharge horizontally will result in fewer regions. The positive column will be compressed while the negative glow will remain the same size, and, with small enough gaps, the positive column will disappear altogether. In an analytical glow discharge, the discharge is primarily a negative glow with dark region above and below it.\n\nThe cathode layer begins with the Aston dark space, and ends with the negative glow region. The cathode layer shortens with increased gas pressure. The cathode layer has a positive space charge and a strong electric field.\n\nElectrons leave the cathode with an energy of about 1 eV, which is not enough to ionize or excite atoms, leaving a thin dark layer next to the cathode.\n\nElectrons from the cathode eventually attain enough energy to excite atoms. These excited atoms quickly fall back to the ground state, emitting light at a wavelength corresponding to the difference between the energy bands of the atoms. This glow is seen very near the cathode.\n\nAs electrons from the cathode gain more energy, they tend to ionize, rather than excite atoms. Excited atoms quickly fall back to ground level emitting light, however, when atoms are ionized, the opposite charges are separated, and do not immediately recombine. This results in more ions and electrons, but no light. This region is sometimes called Crookes dark space, and sometimes referred to as the \"cathode fall\", because the largest voltage drop in the tube occurs in this region.\n\nThe ionization in the cathode dark space result in a high electron density, but slower electrons, making it easier for the electrons to recombine with positive ions, leading to intense light, through a process called bremsstrahlung radiation.\n\nAs the electrons keep losing energy, less light is emitted, resulting in another dark space.\n\nThe anode layer begin with the positive column, and ends at the anode. The anode layer has a negative space charge and a moderate electric field.\n\nWith fewer ions, the electric field increases, resulting in electrons with energy of about 2 eV, which is enough to excite atoms and produce light. With longer glow discharge tubes, the longer space is occupied by a longer positive column, while the cathode layer remains the same. For example, with a neon sign, the positive column occupies almost the entire length of the tube.\n\nAn electric field increase results in the anode glow.\n\nFewer electrons result in another dark space.\n\nBands of alternating light and dark in the positive column are called striations. Striations occur because only discrete amounts of energy can be absorbed or released by atoms, when electrons move from one quantum level to another. The effect was explained by Franck and Hertz in 1914.\n\nIn addition to causing secondary emission, positive ions can strike the cathode with sufficient force to eject particles of the material from which the cathode is made. This process is called sputtering and it gradually ablates the cathode. Sputtering is useful when using spectroscopy to analyze the composition of the cathode, as is done in Glow-discharge optical emission spectroscopy.\n\nHowever, sputtering is not desirable when glow discharge is used for lighting, because it shortens the life of the lamp. For example, neon signs have hollow cathodes designed to minimize sputtering, and contain charcoal to continuously remove undesired ions and atoms.\n\nIn the context of sputtering, the gas in the tube is called \"carrier gas,\" because it carries the particles from the cathode.\n\nBecause of sputtering occurring at the cathode, the colors emitted from regions near the cathode are quite different than the anode. Particles sputtered from the cathode are excited and emit radiation from the metals and oxides that make up the cathode. The radiation from these particles combines with radiation from excited carrier gas, giving the cathode region a white or blue color, while in the rest of the tube, radiation is only from the carrier gas and tends to be more monochromatic.\n\nElectrons near the cathode are less energetic than the rest of the tube. Surrounding the cathode is a negative field, which slows electrons as they are ejected from the surface. Only those electrons with the highest velocity are able to escape this field, and those without enough kinetic energy are pulled back into the cathode. Once outside the negative field, the attraction from the positive field begins to accelerate these electrons toward the anode. During this acceleration electrons are deflected and slowed down by positive ions speeding toward the cathode, which, in turn, produces bright blue-white bremsstrahlung radiation in the negative glow region.\n\nGlow discharges can be used to analyze the elemental, and sometimes molecular, composition of solids, liquids, and gases, but elemental analysis of solids is the most common. In this arrangement, the sample is used as the cathode. As mentioned earlier, gas ions and atoms striking the sample surface knock atoms off of it, a process known as sputtering.\n\nThe sputtered atoms, now in the gas phase, can be detected by atomic absorption, but this is a comparatively rare strategy. Instead, atomic emission and mass spectrometry are usually used. Collisions between the gas-phase sample atoms and the plasma gas pass energy to the sample atoms. This energy can excite the atoms, after which they can lose their energy through atomic emission.\n\nBy observing the wavelength of the emitted light, the atom's identity can be determined. By observing the intensity of the emission, the concentration of atoms of that type can be determined. Energy gained through collisions can also ionize the sample atoms. The ions can then be detected by mass spectrometry. In this case, it is the mass of the ions that identify the element and the number of ions that reflect the concentration. This method is referred to as glow discharge mass spectrometry (GDMS) and it has detection limits down to the sub-ppb range for most elements that are nearly matrix-independent.\n\nBoth bulk and depth analysis of solids may be performed with glow discharge. Bulk analysis assumes that the sample is fairly homogeneous and averages the emission or mass spectrometric signal over time. Depth analysis relies on tracking the signal in time, therefore, is the same as tracking the elemental composition in depth.\n\nDepth analysis requires greater control over operational parameters. For example, conditions (current, potential, pressure) need to be adjusted so that the crater produced by sputtering is flat bottom (that is, so that the depth analyzed over the crater area is uniform). In bulk measurement, a rough or rounded crater bottom would not adversely impact analysis. Under the best conditions, depth resolution in the single nanometer range has been achieved (in fact, within-molecule resolution has been demonstrated).\n\nThe chemistry of ions and neutrals in vacuum is called gas phase ion chemistry and is part of the analytical study that includes glow discharge.\n\nIn analytical chemistry, glow discharges are usually operated in direct-current mode. For direct-current, the cathode (which is the sample in solids analysis) must be conductive. In contrast, analysis of a non conductive cathode requires the use of a high frequency alternating current.\n\nThe potential, pressure, and current are interrelated. Only two can be directly controlled at once, while the third must be allowed to vary. The pressure is most typically held constant, but other schemes may be used. The pressure and current may be held constant, while potential is allowed to vary. The pressure and voltage may be held constant while the current is allowed to vary. The power (product of voltage and current) may be held constant while the pressure is allowed to vary.\n\nGlow discharges may also be operated in radio-frequency. The use of this frequency will establish a negative DC-bias voltage on the sample surface. The DC-bias is the result of an alternating current waveform that is centered about negative potential; as such it more or less represent the average potential residing on the sample surface. Radio-frequency has ability to appear to flow through insulators (non-conductive materials).\n\nBoth radio-frequency and direct-current glow discharges can be operated in pulsed mode, where the potential is turned on and off. This allows higher instantaneous powers to be applied without excessively heating the cathode. These higher instantaneous powers produce higher instantaneous signals, aiding detection. Combining time-resolved detection with pulsed powering results in additional benefits. In atomic emission, analyte atoms emit during different portions of the pulse than background atoms, allowing the two to be discriminated. Analogously, in mass spectrometry, sample and background ions are created at different times.\n\nAn interesting application for using glow discharge was described in a 2002 scientific paper by Ryes, Ghanem \"et al.\"\nAccording to a Nature news article describing the work, researchers at Imperial College London demonstrated how they built a mini-map that glows along the shortest route between two points. The Nature news article describes the system as follows:\n\nThe approach itself provides a novel visible analog computing approach for solving a wide class of maze searching problems based on the properties of lighting up of a glow discharge in a microfluidic chip.\n\nIn the mid-20th century, prior to the development of solid state components such as Zener diodes, voltage regulation in circuits was often accomplished with voltage-regulator tubes, which used glow discharge.\n\n\n"}
{"id": "1671129", "url": "https://en.wikipedia.org/wiki?curid=1671129", "title": "HVDC Haenam–Cheju", "text": "HVDC Haenam–Cheju\n\nThe HVDC Haenam–Cheju is a 101 kilometer long HVDC submarine cable connection between the Korean Peninsula and the island of Jeju (spelled Cheju until 2000) in South Korea, which went into service in 1996. The connection is bipolar, consisting of two 180kV cables with a maximum transmission power of 300 megawatts.\n\nThe Haenam–Cheju HVDC system is owned and operated by Korea Electric Power Corporation (KEPCO).\n\nThe line runs from the Haenam converter station situated at to Jeju converter station at and has no overhead line sections. The first 5 kilometres on Korean Mainland are underground, the rest of the cable is laid partially buried in the sea. As the converter station on Jeju is close to the sea, all equipment (even the AC harmonic filters) is installed indoors in order to prevent corrosion and pollution-related flashovers from sea salt.\n\nThe system is designed to take over the full power supply of Jeju and is therefore equipped with synchronous compensators delivering the required reactive power and commutation voltage when there is no generation unit at Jeju in operation. These synchronous compensators can be started with gas turbines, which allows starting and subsequently running the HVDC transmission scheme when there is no generation available on the island.\n\nThe HVDC scheme includes several unusual design features arising from the design objective of using the HVDC link as the sole source of power to Jeju, in which mode it must define the frequency of the island system. For this reason the inverter on Jeju island must be able to control the received power rapidly without reliance on telecommunications to the mainland. As a result, the control characteristics are reversed from the conventional practice, the inverter (Jeju) station controlling current and the rectifier (Haenam) station controlling DC voltage. The thyristor valves are also designed to operate with a minimum power down to zero (compared with the usual minimum of 5-10%).\n\n\n"}
{"id": "6520784", "url": "https://en.wikipedia.org/wiki?curid=6520784", "title": "Habermaaß", "text": "Habermaaß\n\nHabermaaß GmbH is a German toy manufacturing company (commonly referred to as HABA) based in Bad Rodach, Bavaria. The company defines itself as an “Inventor for inquisitive minds”, offering a broad selection of products such as wooden and textile toys, home furnishings and accessories for babies and children. Today the company is still a family run company, under the leadership of the managing partner Klaus Habermaaß and the managing director Harald Grosch.\n\nIn 1938, Eugen Habermaaß and Anton Engel founded “Habermaaß & Co.” as a “factory for fine wooden products”, and a couple of months later “Wehrfritz & Co.\" in cooperation with Karl Wehrfritz.\nIn 1940, only two years after its foundation, when Anton Engel withdrew from the company, Eugen Habermaaß became its sole managing director. When he died, in 1955, his wife Luise Habermaaß took over the management of the company. At the same time, their son, Klaus Habermaaß, later to become the managing director of the company, decided to start an apprenticeship in cabinetmaking. On finishing his engineering studies in 1961, he became actively engaged in the running of the company, which grew and expanded in the following years.\n\nThe small manufacturer of wooden toys sets out to become a market leader in the sector, facing up to the challenge of globalization. The toy manufacturer based in Upper Frankonia expands first in the United States where in 1980 the American company “Skaneateles Handicrafters” becomes part of the Habermaaß Inc., first as a production and later as a sales and distribution site. Renamed in the following years as “T.C. Timber”, in 2002 the company receives the definitive company name of “Habermaaß Corp. Inc.\". In the meantime, in 1993, a subsidiary is founded in the French location of Evry near Paris under the name of “HABA S.A.R.L\" and in 2005 the “HABA UK” site is established in the United Kingdom.\n\nIn order to expand their product range even further the corporate family of HABA and Wehrfritz decided back in 1987 to found Jako-o GmbH, a joint mail-order company for children’s fashion, toys and accessories which in 2004 was enlarged with the addition of the sales branches Fit-z and Qiéro.\n\nSince the company was founded in 1938, wooden toys have been a trademark of HABA. The company became known in particular for its construction blocks, which up until now have been an integral part of the product range. Over the decades the product range has expanded: first incorporating other wooden toys such as pulling figures, vehicles, clutching toys and a complex upgradeable ball track system, followed later by the first society games. Which is how in 1986 the most famous HABA game “The Orchard” was created. The connections with the closely related Wehrfritz company were also taken advantage of in the upcoming development and production of children’s furniture –all made in a child friendly way using only the finest woods.\n\nEven today the main focus of the company still lies in manufacturing out of wood. However, progressively different materials have been incorporated: with dolls and fabric animals, rugs, furniture for children’s rooms and illumination articles finding their way into the product range which is further complemented by gift articles, accessories and children’s jewelry. All products made by the Habermaaß company, however, have one shared aim, fostering the healthy development of children.\n\nSince it was founded in 1938, the company’s production site has been located in the Upper Frankonian town of Bad Rodach, just a few kilometers away from the federal state boundary with Thuringia. It is from here that all business and the daily activities of the different branch offices are monitored and directed. The company holds distribution branches in the USA, France and the United Kingdom. Today the company is firmly established in both the European and world markets – with distribution areas stretching from Japan, Russia, the Near East and Europe to North and South America.\n\nAmongst the most prestigious awards that have been conferred on HABA are the Best Children's Game of the Year, the German Children;s Games award, the Toy Innovation Award and the German Educational Game Award.\n\n\nMany HABA playthings have also been designated with the \"Spiel Gut\" ('Good Toy') stamp.\n\n\"Karuba\", one of HABA's strategy board games, was nominated for the 2016 Spiel des Jahres award.\n\n"}
{"id": "302691", "url": "https://en.wikipedia.org/wiki?curid=302691", "title": "Henri Victor Regnault", "text": "Henri Victor Regnault\n\nProf Henri Victor Regnault FRS HFRSE (21 July 1810 – 19 January 1878) was a French chemist and physicist best known for his careful measurements of the thermal properties of gases. He was an early thermodynamicist and was mentor to William Thomson in the late 1840s.\n\nBorn in Aachen in 1810, he moved to Paris at the age of eight, following the death of his parents. There, he worked for an upholstery firm until he was eighteen. In 1830, he was admitted to the École Polytechnique, and in 1832 he graduated from the École des mines.\n\nWorking under Justus von Liebig at Gießen, Regnault distinguished himself in the nascent field of organic chemistry by synthesizing several chlorinated hydrocarbons (e.g. vinyl chloride, polyvinylidene chloride, dichloromethane), and he was appointed professor of chemistry at the University of Lyon. In 1840, he was appointed the chair of chemistry of the École Polytechnique, and in 1841, he became a professor of Physics in the Collège de France.\n\nBeginning in 1843, he began compiling extensive numerical tables on the properties of steam. These were published in 1847, and led to his receiving the Rumford Medal of the Royal Society of London and appointment as Chief Engineer of Mines. In 1851 he was elected a foreign member of the Royal Swedish Academy of Sciences. In 1854 he was appointed director of the porcelain works at Sèvres, the \"Manufacture nationale de Sèvres\".\n\nAt Sèvres, he continued work on the thermal properties of matter. He designed sensitive thermometers, hygrometers, hypsometers and calorimeters, and measured the specific heats of many substances and the coefficient of thermal expansion of gases. In the course of this work, he discovered that not all gases expand equally when heated and that Boyle's Law is only an approximation, especially at temperatures near a substance's boiling point.\n\nRegnault was also an avid amateur photographer. He introduced the use of pyrogallic acid as a developing agent, and was one of the first photographers to use paper negatives. In 1854, he became the founding president of the Société française de photographie.\n\nIn 1871, his laboratory at Sèvres was destroyed and his son Alex-Georges-Henri Regnault killed, both as a result of the Franco-Prussian War. He retired from science the next year, never recovering from these losses.\n\nThe crater Regnault on the Moon is named after Regnault, and his name is one of the 72 names inscribed on the Eiffel Tower. Some have suggested that the symbol \"R\" for the ideal gas constant is also named after him.\n\nHe was the first president of Société française de photographie.\n\n"}
{"id": "2286702", "url": "https://en.wikipedia.org/wiki?curid=2286702", "title": "Kaonic hydrogen", "text": "Kaonic hydrogen\n\nKaonic hydrogen is an exotic atom consisting of a negatively charged kaon orbiting a proton.\n\nSuch particles were first identified, through their X-ray spectrum, at the KEK proton synchrotron in Tsukuba, Japan in 1997.\nMore detailed studies have been performed at DAFNE in Frascati, Italy.\n\nKaonic hydrogen has been created in very low energy collisions of kaons with the protons in a gaseous hydrogen target. At DAFNE, kaons are produced by the decay of φ mesons which are in turn created in collisions between electrons and positrons. The experiments analyzed X-rays from several electronic transitions in kaonic hydrogen.\n\nUnlike in the hydrogen atom, where the binding between electron and proton is dominated by the electromagnetic interaction, kaons and protons interact also to a large extent by the strong interaction.\nIn kaonic hydrogen this strong contribution was found to be repulsive, shifting the ground state energy by 283 ± 36 (statistical) ± 6 (systematic) eV, thus making the system unstable with a resonance width of 541 ± 89 (stat) ± 22 (syst) eV (decay into Λπ and Σπ).\n\nKaonic hydrogen is studied mainly because of its importance for the understanding of kaon-nucleon interactions and for testing quantum chromodynamics.\n\n\n"}
{"id": "25853619", "url": "https://en.wikipedia.org/wiki?curid=25853619", "title": "Luttinger's theorem", "text": "Luttinger's theorem\n\nIn condensed matter physics, Luttinger's theorem is a result derived by J. M. Luttinger and J. C. Ward in 1960 that has broad implications in the field of electron transport. It arises frequently in theoretical models of correlated electrons, such as the high-temperature superconductors, and in photoemission, where a metal's Fermi surface can be directly observed.\n\nLuttinger's theorem states that the volume enclosed by a material's Fermi surface is directly proportional to the particle density.\n\nWhile the theorem is an immediate result of the Pauli exclusion principle in the case of noninteracting particles, it remains true even as interactions between particles are taken into consideration provided that the appropriate definitions of Fermi surface and particle density are adopted. Specifically, in the interacting case the Fermi surface must be defined according to the criteria that\nwhere formula_3 is the single-particle Green function in terms of frequency and momentum. Then Luttinger's theorem can be recast into the form\nwhere formula_3 is as above and formula_6 is the differential volume of formula_7-space in formula_8 dimensions.\n\n\n"}
{"id": "17369631", "url": "https://en.wikipedia.org/wiki?curid=17369631", "title": "MacCready Gossamer Penguin", "text": "MacCready Gossamer Penguin\n\nThe Gossamer Penguin was a solar-powered experimental aircraft created by Paul MacCready's AeroVironment.\n\nThe Penguin was a 3/4 scale version of the Gossamer Albatross II, and had a 71 ft.(21.64 meter) wingspan and a weight, without pilot, of . The powerplant was an AstroFlight Astro-40 electric motor, driven by a 541 watt solar panel consisting of 3920 solar cells.\n\nInitial test flights were performed using a 28 cell NiCad battery pack instead of a panel. The test pilot for these flights was MacCready's 13-year-old son Marshall, who weighed .\n\nThe official pilot for the project was Janice Brown, a charter pilot with commercial, instrument, and glider ratings who weighed slightly less than . She flew the Penguin approximately 40 times before a public demonstration at NASA's Dryden Flight Research Center on August 7, 1980.\n"}
{"id": "38611109", "url": "https://en.wikipedia.org/wiki?curid=38611109", "title": "Mengkofen Solar Park", "text": "Mengkofen Solar Park\n\nThe Mengkofen Solar Park is a photovoltaic power station in Bavaria, Germany. It has an installed capacity of 21.78 megawatt (MW).\n\n"}
{"id": "13476223", "url": "https://en.wikipedia.org/wiki?curid=13476223", "title": "Natural hazards in Colombia", "text": "Natural hazards in Colombia\n\nNatural disasters in Colombia are the result of several different natural hazards that affect the country according to its particular geographic and geologic features. Human vulnerability, exacerbated by the lack of planning or lack of appropriate emergency management, and the fragility of the economy and infrastructure contribute to a high rate of financial, structural, and human losses.\n\nSome of the natural hazards present in Colombia are:\n\nColombia is part of the Pacific Ring of Fire and Andean Volcanic Belt due to the collision of the South American Plate and the Nazca Plate. This produces an increased risk of earthquakes and volcanic eruptions. Some natural disasters of this type are:\n\nRainfall is heaviest in the Pacific lowlands and in parts of eastern Colombia, where rain is almost a daily occurrence and rain forests predominate. Precipitation exceeds 760 centimeters annually in most of the Pacific lowlands, making this one of the wettest regions in the world. The highest average annual precipitation in the world is estimated to be in Lloro, Colombia, with 13,299 mm (523.9 inches). In eastern Colombia, it decreases from 635 centimeters in portions of the Andean piedmont to 254 centimeters eastward. Extensive areas of the Caribbean interior are permanently flooded, more because of poor drainage than because of the moderately heavy precipitation during the rainy season.\nThe Caribbean Region of Colombia, valleys of Magdalena river and Cauca river and the eastern savannahs are prone to floods during the two main monsoon seasons (April and November). The opposite phenomenon of drought is also frequent. January through March and July through September are the dry seasons, when abnormally dry periods cause shortage in the water supply to crops and urban centers.\n\nThe presence of coastal regions both in the Atlantic and the Pacific oceans increases the risk of hurricanes and tropical storms. Waves in the trade winds in the Atlantic Ocean—areas of converging winds that move along the same track as the prevailing wind—create instabilities in the atmosphere that may lead to the formation of hurricanes. Some of the events of this type that have affected the country are:\n\nSome of the main public health issues in Colombia are: malnutrition, pregnancy-related deaths, neonatal deaths, acute respiratory disease-related deaths in children under 5 years, diarrhea-related deaths in children under 5 years, lack of vaccinations, tropical diseases such as malaria, dengue fever, hemorrhagic dengue fever, yellow fever, Chagas disease and leishmaniasis, poly-parasitism, snakebites and violence related causes of mortality.\n"}
{"id": "24008985", "url": "https://en.wikipedia.org/wiki?curid=24008985", "title": "Nuclear Power School", "text": "Nuclear Power School\n\nNuclear Power School is a technical school operated by the U.S. Navy in Goose Creek, South Carolina to train enlisted sailors, officers, KAPL civilians and Bettis civilians for shipboard nuclear power plant operation and maintenance of surface ships and submarines in the U.S. nuclear navy. \nThe United States Navy currently operates 95 total nuclear power plants including 71 submarines (each with one reactor), 10 aircraft carriers (each with two reactors), and 4 training/research prototype plants.\n\nProspective enlisted enrollees in the Nuclear Power Program must have qualifying line scores on the ASVAB exam, may need to pass the NAPT (nuclear aptitude test), and must undergo a NACLC investigation for attaining a \"Secret\" security clearance. Additionally, each applicant must pass an interview with the Advanced Programs Coordinator in the associated recruiting district.\n\nAll officer students have had college-level courses in calculus and calculus-based physics. Acceptance to the officer program requires successful completion of interviews at Naval Reactors in Washington, D.C., and a final approval via a direct interview with the Director, Naval Nuclear Propulsion, a unique eight-year, four-star admiral position which was originally held by the program's founder, Admiral Hyman G. Rickover.\n\nWomen were allowed into the Naval Nuclear Field from 1978 until 1980, when the Navy began only allowing men again. With the repeal of the Combat Exclusion Law in the 1994 Defense Authorization Act, and the decision to open combatant ships to women, the Navy once again began accepting women into NNPS for duty aboard nuclear-powered surface combatant ships. Female graduates of NNPS may serve at shore commands and on Nimitz Class aircraft carriers. Female officers may also serve aboard SSBN and SSGN submarines. The first female officers bound for submarines began training at NNPTC in late August 2010.\n\nEnlisted personnel graduate from Nuclear Field \"A\" School for rating as Machinist's Mate (MMN), Electrician's Mate (EMN), or Electronics Technician (ETN) and are advanced to the rank of a Third Class Petty Officer. They then continue to Nuclear Power School. Graduates of the Nuclear Power School continue training with twenty four weeks of instruction at a Nuclear Power Training Unit. This training involves the operation and simulated maintenance of nuclear reactor plants and steam plants. Graduates of NPTU are qualified nuclear operators and continue on to serve in the fleet, unless they are selected as a Junior Staff Instructor (JSI). JSIs go through training to be instructors at a NPTU where they will directly assist in qualifying future students. The enlisted school has a very high academic attrition rate.\n\nSailors in the nuclear ratings account for 3% of the enlisted Navy.\n\nTraining for Fleet operators was initially conducted by civilian engineers at Idaho Falls, Idaho (1955-1958) and West Milton, New York (1955-1956). The first formal Nuclear Power School was established in New London, Connecticut in January 1956 with a pilot course offered for six officers and fourteen enlisted men.\nSubsequent locations include Naval Training Center Bainbridge, Maryland (1956-1976); Naval Shipyard Mare Island, California (1958-1976); Naval Training Center Orlando, Florida (1976-1998) and its current location, Goose Creek, South Carolina. In 1986, Nuclear Field A School was established in Orlando to provide nuclear in-rate training to Sailors prior to attending Nuclear Power School.\n\nIn 1993, in response to the BRAC-directed closure of NTC Orlando by the end of Fiscal Year 1999, the Nuclear Field A School and Nuclear Power School were joined to create Naval Nuclear Power Training Command. A move from Orlando, Florida to Goose Creek, South Carolina began in May 1998 and was completed in January 1999. Construction of the new command allowed Nuclear Field A School and Nuclear Power School to be located in the same building.\n\nMany improvements were added to the command to improve each sailor's quality of life and the effectiveness of training. The Bachelor Enlisted Quarters include microwaves and refrigerators along with semiprivate rooms joined by a common bath. The complex also includes a galley, recreation building, and recreation fields conveniently located for the sailors' use. The NNPTC complex is fully manned with over 3,600 students and 480 staff members. Naval Health Clinic Charleston is located across NNPTC Circle from the NNPTC site and is a short walk from the main Rickover Center building.\n\nThe following topics are learned in the curriculum for all program attendees:\n\n\nEven more intensive than the enlisted course, the officer course involves extensive post-calculus mathematical examination of reactor dynamics. Officers cover all topics in equal depth, whereas enlisted training is specialized for each student's job rating (with significant cross-training in the remaining \"nuke\" specialties). The officer course also assumes students have undergraduate engineering or science degrees.\n\nThe nuclear program is widely acknowledged as having the most demanding academic program in the U.S. military. The school operates at a fast pace, with stringent academic standards in all subjects. Students typically spend 45 hours per week in the classroom, and are required to study an additional 10 to 35 hours per week outside lecture hours, five days per week. Because the classified materials are restricted from leaving the training building, students cannot study outside the classroom.\n\nStudents who fail tests and otherwise struggle academically are required to review their performance with instructors. The student may be given remedial homework or other study requirements. Failing scores due to personal negligence, rather than a lack of ability, can result in charges of \"dereliction of duty\" under the Uniform Code of Military Justice. Failing students may be held back to repeat the coursework with a new group of classmates, but such students are typically released from the Nuclear Power Program and are re-designated or discharged.\n\nThe American Council of Education recommends an average of 60-80 semester-hours of college credit, in the lower-division baccalaureate/associate degree category, for completion of the entire curriculum including both Nuclear Field \"A\" School and Naval Nuclear Power School. The variation in total amount depends on the specific pipeline completed — MM, EM, or ET. Further, under the Servicemembers Opportunity Colleges degree program for the Navy (SOCNAV), the residency requirements at these civilian institutions are reduced to only 10-25%, allowing a student to take as little as 9 units of coursework (typically 3 courses) through the degree-granting institution to complete their Associate in Applied Science degree in nuclear engineering technology or as much as 40 units to complete a Bachelor in Nuclear Engineering Technology degree.\n\nThe following select colleges offer college credit and degree programs to graduates of the U.S. Naval Nuclear Power School (NUPOC).\n\n\nThe American Council on Education has evaluated the course of instruction at NNPTC and recommended the following credits be given for completion of the enlisted curriculum:\n\n\nAdditionally, for Machinist's Mates\n\nFor Electronics Technicians and Electrician's Mates\n\nSeveral universities offer graduate level credit for completion of the officer training course.\n\nNuclear Power Training Unit (NPTU), one of which is also located at the former Naval Weapons Station Charleston, has two decommissioned submarines, ex-\"Daniel Webster\" (MTS-626) and ex-\"Sam Rayburn\" (MTS-635). These moored training ships have their missile compartments removed, but have fully operational S5W reactor power plants. Both of these training ships are equipped with a diesel-powered Supplemental Water Injection System (SWIS) to provide emergency cooling water in the event of an accident.\n\n\"USS La Jolla (SSN-701)\" was placed in commissioned (Reserve, Stand down) status in February 2015 for conversion to a Moored Training Ship (MTS). The conversion is expected to take 32 months according to the Commanding Officer. During that time, the submarine will be cut into three pieces, and a portion of the hull will be taken out. Three new hull sections from General Dynamics Electric Boat will be added to accommodate the sub’s new mission. A newly fabricated hull section will be welded in place, and the new space will contain training spaces, office spaces, and a Supplemental Water Injection System (SWIS) to provide emergency cooling water in the event of an accident. The future MTS-701 will be permanently moored at Nuclear Power Training Unit (NTPU) at Naval Support Activity Charleston in South Carolina.\n\n\"La Jolla\" is the first Los Angeles-class boat to undergo the conversion to a training ship and will be followed by the \"USS San Francisco (SSN-711)\" about two years later, according to the Navy’s long-range ship decommissioning plans.\n\nTwo land-based reactor prototypes are based at the Knolls Atomic Power Laboratory, Kenneth A. Kesselring Site Operation, in Ballston Spa, New York. These are the MARF/S7G and the S8G Trident prototypes. (The S8G core has now been replaced with the S6W reactor core). At one time, two additional prototypes were operational: D1G and S3G.\n\nKnolls Atomic Power Laboratory in New York has the longest operational history of NPTUs. However, two other sites also provided operational training during the Cold War.\n\nFrom the early 1950s to the mid-1990s, Naval Reactors Facility (NRF) in Idaho trained nearly 40,000 Navy personnel in surface and submarine nuclear power plant operations with three nuclear propulsion prototypes — A1W, S1W, and S5G.\n\nFrom 1959 until 1993, over 14,000 Naval operators were trained at the S1C prototype at Windsor, Connecticut including President Carter.\n"}
{"id": "32233277", "url": "https://en.wikipedia.org/wiki?curid=32233277", "title": "Paper and Fibre Research Institute", "text": "Paper and Fibre Research Institute\n\nThe Paper and Fibre Research Institute (PFI) () is a centre of expertise for wood fibres, pulp and paper, new biobased materials and sustainable biorefining.\n\nPFI was established in Kristiania in 1923 as the Norwegian Pulp and Paper Research Institute. PFI moved to Trondheim in 1997/98, and established a close cooperation with the Department of Chemical Engineering at the Norwegian University of Science and Technology. In 2004 PFI changed name to the Paper and Fibre Research Institute and became a subsidiary of Innventia in Stockholm, Sweden.\n\n"}
{"id": "21949890", "url": "https://en.wikipedia.org/wiki?curid=21949890", "title": "Peter Droege", "text": "Peter Droege\n\nPeter Droege Prof. Peter Droege\n\nDI (TUM) MAAS (MIT) ISDE\nOverview: policy, design, planning and training background\n\nPeter Droege is an international urban sustainability expert and experienced mind on advanced urban policy, management, design, planning and renewable infrastructure development – and the director of the Liechtenstein Institutes for Strategic Development (http://www.eurisd.org and http://www.eurisd.de). He is particularly knowledgeable in International Building Exhibitions (IBA), especially from an urban sustainability point of view. He holds a Conjoint Professorship at the University of Newcastle’s School of Architecture and Built Environment and has developed a Chair for Sustainable Spatial Development at the University of Liechtenstein. Peter Droege’s academic career stretches from the School of Architecture and Planning at the Massachusetts Institute of Technology (MIT) and his position at the University of Tokyo as Urban Development Engineering Endowed Chair to his position as Lend Lease Chair and Professor of Urban Design at the University of Sydney, to the University of Liechtenstein.\nAward winning author and editor of seminal books on city futures, resilience, regenerative design, energy and climate\n\nA recipient of the European Solar Prize, Peter Droege not only produced the major textbook on space and IT, Intelligent Environments (Elsevier 1998), but also has become an influential author and editor of seminal books on transforming the urban system: “The Renewable City” (Wiley 2006), “Urban Energy Transition” (Elsevier 2008), “100 Percent Renewable” (Routledge 2012, Earthscan 2009), “Climate Design” (ORO Editions 2010), “Regenerative Region” (Oekom 2014) as well as chapters in the new “Handbook of Architectural Theory” (Sage 2011), “Encyclopedia of Energy” (Elsevier 2004), “The New Blackwell Guide to the City” (Blackwell 2011), “Sustainable Urbanism and Beyond” (Rizzoli 2012), and “Superlux” (Thames and Hudson 2015). These findings and ideas combined with his projects led Peter Droege to serve as sustainability innovation advisor to the City of Zurich’s Urban Design Department, Principal Advisor to Beijing’s Municipal Institute for City Planning and Design, be elected as President of Eurosolar, appointed as General Chairman of the World Council for Renewable Energy, and be invited as inaugural Selection Committee member of Masdar’s Zayed Future Energy Prize. In June 2016 Peter Droege was commissioned by the world’ largest science publisher, Elsevier, to develop a second edition of his 2008 bestseller, Urban Energy Transition: From Fossil Fuel to Renewable Power, to be published in 2018 as Urban Energy Trantion, 2nd Edition - Handbook for Cities and Regions.\nInternational ecological and renewable energy based urban management, planning and design expertise\n\nPeter Droege initiated in 2007 the ‘Renewable Wilhelmsburg’ theme of the International Building Exhibition IBA-Hamburg, and advised the organisation on energy and climate design and planning issues. Peter Droege has long pursued the innovative IBA Bodensee / International Building Exhibition Lake Constance development initiative, the first IBA based on universal sustainable lifestyle and metro-regional best practice across all scales and types of intervention. Peter Droege is also a highly valued urban sustainabiliy policy and governance advisor for organisiations as varied as UNDP, UNCHS, UNECE, Australian, German, Vietnamese and Indonesian governments, and has also long been active in Japan and China, winning a number of important competitions.\nPeter Droege has been successfully engaged by US state government agencies, while working and teaching in America, where he also emerged as sustainable waterfront development expert. In Australia, Professor Droege has been instrumental in the master plan guidance for Melbourne Docklands, and oversaw the large-scale development of the Ultimo-Pyrmont peninsula. He was also the urban design/synergy advisor for the Australian Technology Park (ATP), one of the first clean-tech themed industrial re-development sites in the world, to enhance the opportunities for scientific exchange and innovation. In Europe Peter Droege served as in-house principal urban sustainability and design expert at the Urban Planning Department of Amsterdam, and shaped the energy and climate agenda of the multi-billion-Euro city development process of the Internationale Bausaustellung IBA-Hamburg 2007-2013.\nRecent and current applied research on climate and urban development\n\nAs Professor and Chairholder at the University of Liechtenstein Peter Droege initiated, directed and/or participated in the IBH / internationally and locally funded projects ‘Lake Constance 2030 Foresight Study’, Peter Droege initiated and led a EU-funded five-university, four-country urban and regional energy autonomy study ‘Energy Region Lake Constance-Rhine Valley’ running from 2010 through 2014, focused on a transition to renewable energy and mobility based on local resources. The final report has been released by the German Oekom publishers, entitled ‘Regenerative Region: Energy and Climate Atlas’. He initiated and successfully completed in 2012 the government co-funded model development and scenario study ‘Renewable Liechtenstein’. In 2013 he secured a 30-month follow-up study on regional implementation strategies, and expanding the scope to global ecological footprint measurement and reductions.\nHe is engaged in the ‘Common European Sustainable Built Environment Assessment – Alpine Countries’ (CESBA-Alps) and innovated for it ATLAS, the GIS-based Alpine Territorial Levelised Assessment System. Peter Droege served as advisor and jury member at the international university based, applied R&D oriented 2014 Solar Decathlon in Versailles, in urban design. He has been a juror and jury chair on a wide range of international urban and urban sustainability projects for many years.\n\nSource: \n\n"}
{"id": "5096257", "url": "https://en.wikipedia.org/wiki?curid=5096257", "title": "Pitch-up", "text": "Pitch-up\n\nIn aerodynamics, pitch-up is a severe form of stall in an aircraft. It is directly related to inherent properties of all swept wings, and seen primarily on those platforms. Unlike conventional low-speed stalls, pitch-up can occur at any speed, and are especially dangerous when they take place in the transonic; at these speeds the aerodynamic loads can become so high as to break up the aircraft, as occurred in 1964 when a F-105 Thunderchief of the USAF Thunderbirds broke up in mid-air. It can also occur at low speeds, in which case it has been called a Sabre dance, a particularly dangerous behaviour of swept wings that became apparent during the development of the USAF F-100 Super Sabre.\n\nPitch-up problems were first noticed on high-speed test aircraft with swept wings. It was a common problem on the Douglas Skyrocket, which was used extensively to test the problem.\n\nBefore the pitch-up phenomenon was well understood, it plagued all early swept-wing aircraft. In the F-100 Super Sabre it even got its own name, the Sabre dance. In aircraft with high-mounted tailplanes, like the F-101 Voodoo, recovery was especially difficult because the tailplane was placed directly in the wing wake during the pitch-up, causing deep stall (although the T-tail was meant to prevent pitch-up from starting in the first place). Deployment of the braking parachute and a considerable height above the ground were essential for a chance at recovery.\n\nWings generate a relatively complex pattern of forces at different points on their planform. These are usually described as lift and drag components, using vector decomposition. If these vectors are added up for the entire wing, the result is a single force acting at some point on the wing. This point is known as the \"center of pressure\", or CoP, and is normally located somewhere between ⅓ and ½ of the way back from the leading edge. The exact location changes with changes in the angle of attack, which leads to the requirement to trim aircraft as they change their speed or power settings.\n\nAnother major consideration for aircraft design is a similar vector addition of all of the weight terms of the parts of the aircraft, including the wing. This too can be reduced to a single weight term acting at some point along the longitudinal axis of the aircraft, the \"center of gravity\", or CoG. If the wing is positioned so its CoP lies near CoG for the aircraft, in level flight the wing will lift the aircraft straight up. This reduces any net forces pitching the aircraft up or down, but for a number of reasons the two points are normally slightly separated and a small amount of force from the flight control surfaces is used to balance this out.\n\nThe same basic layout is desirable for an aircraft with a swept wing as well. On a conventional rectangular wing, the CoP meets the aircraft at the point on the chord running directly out from the root. While the same analysis will reveal a center of pressure point for a swept wing, its location may be considerably behind the leading edge \"measured at the root of the wing\". For highly swept planforms, the CoP may lie behind the trailing edge of the wing root, requiring the wing to meet the aircraft at a seemingly far-forward location.\n\nIn this case of a swept wing, changes to the CoP with angle of attack may be magnified.\n\nThe introduction of swept wings took place during a move to more highly tapered designs as well. Although it had long been known that an elliptical planform is \"perfect\" from an induced drag standpoint, it was also noticed that a linear taper of the wing had much the same effect, while being lighter. Research during the war led to widespread use of taper, especially in the post-war era. However, it had been noticed early on that such designs had unfavourable stall characteristics; as the tips were more highly loaded in high angles of attack, they operated closer to their stall point.\n\nAlthough this effect was unfavourable in a conventional straight wing aircraft, on a swept-wing design it had unexpected and dangerous results. When the tips stall on a swept wing, the center of pressure, the average lift point for the wing as a whole, moves forward. This is because the section still generating considerable lift is further forward. This causes further nose-up force, increasing the angle of attack and causing more of the tip area to stall. This may lead to a chain reaction that causes violent nose-up pitching of the aircraft.\n\nThis effect first noticed in the Douglas D-558-2 Skyrocket in August 1949, when a 0.6 G turn suddenly increased out of control to 6 G. This was not entirely surprising; the effect had been seen earlier in wind tunnel simulations. These effects can be seen at any speed; in the Skyrocket they occurred primarily in the transonic (the Weil-Gray criteria) but with more highly swept and tapered planforms, like on the North American F-100 Super Sabre, the effect was common at low speeds as well (the Furlong-McHugh boundary), when the aircraft flew at higher angles of attack in order to maintain lift at low speeds.\n\nIn addition, swept wings tend to generate span wise flow of the boundary layer, causing some of the airflow to move \"sideways\" along the wing. This occurs all along the wing, but as one moves towards the tip the sideways flow increases, as it includes both the contribution of the wing at that point, as well as span wise flow from points closer to the root. This effect takes time to build up, at higher speeds the span wise flow tends to be blown off the back of the wing before it has time to become serious. At lower speeds, however, this can lead to a considerable buildup of the boundary layer at the wing tip, adding to the problems noted above.\n\nFinally, while not directly related to the effects above, it was common during the early jet age to use T-tail designs in order to keep the aerodynamic surfaces well clear of the jet engine area. In this case it is possible for a pitch-up event to cause the turbulent air behind the wing to flow across the horizontal stabilizer, making it difficult or impossible to apply nose-down pressure to counteract the pitch-up. Aircraft with low-mounted tail surfaces did not suffer from this effect, and in fact improved their control authority as the wing's wake cleared the controls surfaces, flowing above it. This was not always enough to correct for the problem, however; the F-86 continued to suffer from pitch-up in spite of increasing nose-down pressure from the tail surfaces.\n\nAs the primary causes of the pitch-up problem are due to spanwise flow and more loading at the tips, measures to address these issues can eliminate the problem. In early designs these were typically \"add-ons\" to an otherwise conventional wing planform, but in modern designs this is part of the overall wing design and normally controlled via the existing high-lift devices.\n\nThe first known attempt to address these problems took place on the platform where they were first noticed, the Douglas Skyrocket. This took the form of a series of vortex generators added to the outboard portions of the wing, breaking up the boundary layer. However, this was found to have almost no effect in practice. Nevertheless, a similar solution was attempted on the Boeing B-47 Stratojet where it proved considerably more effective. This may have been helped by the presence of the podded engines, whose vertical mountings acted as barriers to span wise flow.\n\nMore common solutions to the problem of spanwise flow is the use of a wing fence or the related dogtooth notch on the leading edge of the wing. This disrupts the flow and re-directs it rearward, while also causing the buildup of stagnant air inboard to lower the stall point. This does have an effect on overall airflow on the wing, and is generally not used where the sweep is mild.\n\nTo address the problems with spanwise loading, a wider variety of techniques have been used, including dedicated slats or flaps, the use of washout or automated control of the ailerons. An unusual solution tried on the XF-91 Thunderceptor prototype fighter was to give the wingtips a wider chord than the wing roots. The idea was to increase wingtip efficiency and cause the wing roots to stall first.\n\nAngle of attack sensors on the aircraft can also detect when the angle of attack approaches the attitude known to result in pitch-up and activate devices like the stick shaker to warn the pilot, and the stick pusher which overpowers the pilot and forces the nose of the aircraft down to a safer angle of attack. Twist or washout built into the wingtips can also alleviate pitch-up. In effect, the angle of attack at the wingtip becomes smaller than elsewhere on the wing, meaning that the inboard portions of the wing will stall first.\n\nA commonly used solution to pitch-up in modern combat aircraft is to use a control-canard. Another modern solution to pitch-up is the use of slats. When slats are extended they increase wing camber and increase maximum lift coefficient.\n\nPitch-up is also possible in aircraft with forward-swept wings as used on the Grumman X-29. With forward-swept wings the span wise flow is inboard, causing the wing root to stall before the wingtip. Although at first glance it would appear that this would cause pitch-\"down\" problems, the extreme rear mounting of the wing means that when the root stalls the lift moves forward, towards the tips.\n\nWhen a swept wing starts to stall, the outermost portions tend to stall first. Since these portions are behind the center of lift, the overall lift force moves forward, pitching the nose of the aircraft upwards. This leads to a higher angle of attack and causes more of the wing to stall, which exacerbates the problem. The pilot often loses control, with fatal results at low altitude because there was insufficient time for the pilot to regain control or eject before hitting the ground. A large number of aircraft were lost to this phenomenon during landing, which left aircraft tumbling onto the runway, often in flames.\n\nOne of the most notorious incidents was the loss of F-100C-20-NA Super Sabre 54-1907 and its pilot during an attempted emergency landing at Edwards AFB, California on January 10, 1956. By chance, this particular incident was recorded in detail on 16 mm film by cameras set up to cover an unrelated test. The pilot fought desperately to regain control due to faulty landing technique, finally rolling and yawing to the right before striking the ground with the fuselage turned approximately 90 degrees to the line of flight. Anderson,1993 states the F-100 was noticeably underpowered for its day and had very pronounced \"backside\" tendencies if airspeed was allowed to decay too much.\n\nThe brand new F-100C was flown by Lt. Barty R. Brooks, a native of Martha, Oklahoma and a Texas A&M graduate, of the 1708th Ferrying Wing, Detachment 12, Kelly AFB, Texas. The aircraft was one of three being delivered from North American's Palmdale plant to George AFB, California, but the nose gear pivot pin worked loose, allowing the wheel to swivel at random, so he diverted to Edwards, which had a longer runway. On approach, at a high angle of attack, the fighter exceeded its flight envelope, and, too far into stall condition, lost directional control with fatal results. These scenes were inserted in the movie \"The Hunters\", starring Robert Mitchum and Robert Wagner, in the movie \"X-15\" with actor Charles Bronson playing the pilot, and in the Made for TV film \"Red Flag: The Ultimate Game\", although in \"The Hunters\" and in \"Red Flag: The Ultimate Game\", the aircraft supposedly represented were respectively an F-86 and an F-5E. The incident was also commemorated in the fighter pilot song \"Give Me Operations\" (set to the tune of the California Gold Rush song \"What Was Your Name in the States?\"): \n\n\n"}
{"id": "173283", "url": "https://en.wikipedia.org/wiki?curid=173283", "title": "Poly(methyl methacrylate)", "text": "Poly(methyl methacrylate)\n\nPoly(methyl methacrylate) (PMMA), also known as acrylic or acrylic glass as well as by the trade names Crylux, Plexiglas, Acrylite, Lucite, and Perspex among several others (see below), is a transparent thermoplastic often used in sheet form as a lightweight or shatter-resistant alternative to glass. The same material can be used as a casting resin, in inks and coatings, and has many other uses.\n\nAlthough not a type of familiar silica-based glass, the substance, like many thermoplastics, is often technically classified as a type of glass (in that it is a non-crystalline vitreous substance) hence its occasional historical designation as \"acrylic glass\". Chemically, it is the synthetic polymer of methyl methacrylate. The material was developed in 1928 in several different laboratories by many chemists, such as William Chalmers, Otto Röhm, and Walter Bauer, and was first brought to market in 1933 by the Rohm and Haas Company under the trademark Plexiglas.\n\nPMMA is an economical alternative to polycarbonate (PC) when tensile strength, flexural strength, transparency, polishability, and UV tolerance are more important than impact strength, chemical resistance and heat resistance. Additionally, PMMA does not contain the potentially harmful bisphenol-A subunits found in polycarbonate. It is often preferred because of its moderate properties, easy handling and processing, and low cost. Non-modified PMMA behaves in a brittle manner when under load, especially under an impact force, and is more prone to scratching than conventional inorganic glass, but modified PMMA is sometimes able to achieve high scratch and impact resistance.\n\nThe first acrylic acid was created in 1843. Methacrylic acid, derived from acrylic acid, was formulated in 1865. The reaction between methacrylic acid and methanol results in the ester methyl methacrylate. Polymethyl methacrylate was discovered in the early 1930s by British chemists Rowland Hill and John Crawford at Imperial Chemical Industries (ICI) in England. ICI registered the product under the trademark Perspex. About the same time, chemist and industrialist Otto Röhm of Rohm and Haas AG in Germany attempted to produce safety glass by polymerizing methyl methacrylate between two layers of glass. The polymer separated from the glass as a clear plastic sheet, which Röhm gave the trademarked name Plexiglas in 1933. Both Perspex and Plexiglas were commercialized in the late 1930s. In the United States, E.I. du Pont de Nemours & Company (now DuPont Company) subsequently introduced its own product under the trademark Lucite. In 1936 ICI Acrylics (now Lucite International) began the first commercially viable production of acrylic safety glass. During World War II both Allied and Axis forces used acrylic glass for submarine periscopes and aircraft windshields, canopies, and gun turrets. Airplane pilots whose eyes were damaged by flying shards of PMMA fared much better than those injured by standard glass, demonstrating better compatibility between human tissue and PMMA than glass. Civilian applications followed after the war.\n\nCommon orthographic stylings include \"polymethyl methacrylate\" and \"polymethylmethacrylate\". The full chemical name is poly(methyl 2-methylpropenoate). (It is a common mistake to use \"an\" instead of \"en\".)\n\nAlthough PMMA is often called simply \"acrylic\", \"acrylic\" can also refer to other polymers or copolymers containing polyacrylonitrile. Notable trade names include Acrylite, Lucite, R-Cast, Plexiglas, Optix, Perspex, Oroglas, Altuglas, Cyrolite, and Sumipex.\n\nPMMA is routinely produced by emulsion polymerization, solution polymerization, and bulk polymerization. Generally, radical initiation is used (including living polymerization methods), but anionic polymerization of PMMA can also be performed. To produce of PMMA, about of petroleum is needed. PMMA produced by radical polymerization (all commercial PMMA) is atactic and completely amorphous.\n\nThe glass transition temperature (\"T\") of atactic PMMA is . The \"T\" values of commercial grades of PMMA range from ; the range is so wide because of the vast number of commercial compositions which are copolymers with co-monomers other than methyl methacrylate. PMMA is thus an organic glass at room temperature; i.e., it is below its \"T\". The forming temperature starts at the glass transition temperature and goes up from there. All common molding processes may be used, including injection molding, compression molding, and extrusion. The highest quality PMMA sheets are produced by cell casting, but in this case, the polymerization and molding steps occur concurrently. The strength of the material is higher than molding grades owing to its extremely high molecular mass. Rubber toughening has been used to increase the toughness of PMMA to overcome its brittle behavior in response to applied loads.\n\nPMMA can be joined using cyanoacrylate cement (commonly known as superglue), with heat (welding), or by using solvents such as di- or trichloromethane to dissolve the plastic at the joint, which then fuses and sets, forming an almost invisible weld. Scratches may easily be removed by polishing or by heating the surface of the material.\n\nLaser cutting may be used to form intricate designs from PMMA sheets. PMMA vaporizes to gaseous compounds (including its monomers) upon laser cutting, so a very clean cut is made, and cutting is performed very easily. However, the pulsed lasercutting introduces high internal stresses along the cut edge, which on exposure to solvents produce undesirable \"stress-crazing\" at the cut edge and several millimetres deep. Even ammonium-based glass-cleaner and almost everything short of soap-and-water produces similar undesirable crazing, sometimes over the entire surface of the cut parts, at great distances from the stressed edge. Annealing the PMMA sheet/parts is therefore an obligatory post-processing step when intending to chemically bond lasercut parts together.\n\nIn the majority of applications, it will not shatter. Rather, it breaks into large dull pieces. Since PMMA is softer and more easily scratched than glass, scratch-resistant coatings are often added to PMMA sheets to protect it (as well as possible other functions).\n\nMethyl methacrylate \"synthetic resin\" for casting (simply the bulk liquid chemical) may be used in conjunction with a polymerization catalyst such as MEKP, to produce hardened transparent PMMA in any shape, from a mold. Objects like insects or coins, or even dangerous chemicals in breakable quartz ampules, may be embedded in such \"cast\" blocks, for display and safe handling.\n\nPMMA is a strong, tough, and lightweight material. It has a density of 1.17–1.20 g/cm, which is less than half that of glass. It also has good impact strength, higher than both glass and polystyrene; however, PMMA's impact strength is still significantly lower than polycarbonate and some engineered polymers. PMMA ignites at and burns, forming carbon dioxide, water, carbon monoxide and low-molecular-weight compounds, including formaldehyde.\n\nPMMA transmits up to 92% of visible light (3 mm thickness), and gives a reflection of about 4% from each of its surfaces due to its refractive index (1.4905 at 589.3 nm). It filters ultraviolet (UV) light at wavelengths below about 300 nm (similar to ordinary window glass). Some manufacturers add coatings or additives to PMMA to improve absorption in the 300–400 nm range. PMMA passes infrared light of up to 2,800 nm and blocks IR of longer wavelengths up to 25,000 nm. Colored PMMA varieties allow specific IR wavelengths to pass while blocking visible light (for remote control or heat sensor applications, for example).\n\nPMMA swells and dissolves in many organic solvents; it also has poor resistance to many other chemicals due to its easily hydrolyzed ester groups. Nevertheless, its environmental stability is superior to most other plastics such as polystyrene and polyethylene, and PMMA is therefore often the material of choice for outdoor applications.\n\nPMMA has a maximum water absorption ratio of 0.3–0.4% by weight. Tensile strength decreases with increased water absorption. Its coefficient of thermal expansion is relatively high at (5–10)×10 °C.\n\nPure poly(methyl methacrylate) homopolymer is rarely sold as an end product, since it is not optimized for most applications. Rather, modified formulations with varying amounts of other comonomers, additives, and fillers are created for uses where specific properties are required. For example,\n\nThe polymer of methyl acrylate, PMA or poly(methyl acrylate), is similar to poly(methyl methacrylate), except for the lack of methyl groups on the backbone carbon chain. PMA is a soft white rubbery material that is softer than PMMA because its long polymer chains are thinner and smoother and can more easily slide past each other.\n\nBeing transparent and durable, PMMA is a versatile material and has been used in a wide range of fields and applications such as rear-lights and instrument clusters for vehicles, appliances, and lenses for glasses. PMMA in the form of sheets affords to shatter resistant panels for building windows, skylights, bulletproof security barriers, signs & displays, sanitary ware (bathtubs), LCD screens, furniture and many other applications. It is also used for coating polymers based on MMA provides outstanding stability against environmental conditions with reduced emission of VOC. Methacrylate polymers are used extensively in medical and dental applications where purity and stability are critical to performance.\n\n\n\nIn particular, acrylic-type contact lenses are useful for cataract surgery in patients that have recurrent ocular inflammation (uveitis), as acrylic material induces less inflammation. \n\nDue to its aforementioned biocompatibility, Poly(methyl methacrylate) is a commonly used material in modern dentistry, particularly in the fabrication of dental prosthetics, artificial teeth, and orthodontic appliances.\n\n\n\n\nThe Futuro house was made of fibreglass-reinforced polyester plastic, polyester-polyurethane, and poly(methylmethacrylate); one of them was found to be degrading by cyanobacteria and Archaea.\n\nPerspex Technical Properties https://www.theplasticshop.co.uk/plastic_technical_data_sheets/perspex_technical_properties_data_sheet.pdf\n"}
{"id": "3338404", "url": "https://en.wikipedia.org/wiki?curid=3338404", "title": "Provan Gas Works", "text": "Provan Gas Works\n\nProvan Gas Works is an industrial gas holding plant in the city of Glasgow, Scotland. The plant is in the Provanmill area of the city, and was built by Glasgow Corporation between 1900 and 1904. It later became part of British Gas, and subsequently Transco and most recently Scotia Gas Networks (a subsidiary of Scottish and Southern Energy) who operate it today.\n\nOriginally the plant was a gasworks, manufacturing town gas via the coking of coal. The plant was expanded after 1919. Following nationalisation of the gas supply in 1948, the plant passed to the Scottish Gas Board, and then to British Gas in 1973. In 1972, supplies of inexpensive natural gas from North Sea oilfields became available. The gasworks was downsized significantly in the 1980s in response to changing economic conditions arising as the British gas industry was privatised under Prime Minister Margaret Thatcher. Today the plant is largely unmanned, used solely for gas storage and distribution.\n\nThe plant has become significant for its two massive column-guided gasometers and an additional spiral-guided gasometer, which have become an iconic industrial landmark in Glasgow's East End. Among the largest of their kind in the UK, each of the towers can hold of gas, and is in diameter. Their combined storage capacity is – each enough to supply a city the size of York for an entire day.\n\nSince the construction of the M8 and M80 motorways in the 1970s & 1980s, which run directly next to the plant, the twin gasholders have become an unofficial portal into the city's central area for drivers from Edinburgh and the north. The towers have also gained an affectionate following among locals since they are often used to display huge placards showcasing the various promotional slogans for the city, the most memorable being the \"Glasgow's Miles Better\" and \"Everyone's Glasgowing On\" campaigns.\n\nThe future of the plant is under discussion: Glasgow City Council have proposed plans for decontaminating the unused land on the brownfield site and redeveloping it for commercial use. In 2012, Scotia Gas Networks announced that the twin gasometers were to be decommissioned, with no decision made on their future.\n\n\n"}
{"id": "24526762", "url": "https://en.wikipedia.org/wiki?curid=24526762", "title": "Reticulated foam", "text": "Reticulated foam\n\nReticulated foam is a very porous, low density solid foam. 'Reticulated' means like a net. Reticulated foams are extremely open foams i.e. there are few, if any, intact bubbles or cell windows. In contrast, the foam formed by soap bubbles is composed solely of intact (fully enclosed) bubbles. In a reticulated foam only the lineal boundaries where the bubbles meet (Plateau borders) remain. \n\nThe solid component of a reticulated foam may be an organic polymer like polyurethane, a ceramic or a metal. These materials are used in a wide range of applications where the high porosity and large surface area are needed, including filters, catalyst supports, fuel tank inserts, and loudspeaker covers.\n\nA description of the structure of reticulated foams is still being developed. While Plateau's laws, the rules governing the shape of soap films in foams were developed in the 19th century, a mathematical description of the structure is still debated. The computer-generated Weaire–Phelan structure is the most recent. In a reticulated foam only the edges of the polyhedra remain; the faces are missing. In commercial reticulated foam, up to 98% of the faces are removed. The dodecahedron is sometimes given as the basic unit for these foams, but the most representative shape is a polyhedron with 13 faces. Cell size and cell size distribution are critical parameters for most applications. Porosity is typically 95%, but can be as high as 98%. Reticulation affects many of the physical properties of a foam. Typically resistance to compression is decreased while tensile properties like elongation and resistance to tearing are increased.\n\nRobert A. Volz is credited with discovering the first process for making reticulated polyurethane foam in 1956 while working for the Scott Paper Company. Producing reticulated polyurethane foam is a two step procedure: a conventional closed-cell polyurethane foam is produced, then the faces (or \"windows\") of the cells are removed. The high surface area and lower mass of the cells' faces compared with the cells' struts (or edges) makes them much more susceptible to both combustion and chemical degradation; either filling the closed-cell foam with a combustible gas like hydrogen and igniting it under controlled conditions, or exposing the foam to a sodium hydroxide solution will remove the faces and leave the edges. \n\nReticulated ceramic foams are made by coating a reticulated polyurethane foam with an aqueous suspension of a ceramic powder then heating the material to first evaporate the water then fuse the ceramic particles and finally to burn off the organic polymer.\n\nReticulated metal foam can also be made using polyurethane foam as a template similar to its use in ceramic foams. Metals can be vapor deposited onto the polyurethane foam and then the organic polymer burned off.\n\nReticulated foams are used where porosity, surface area, low density are important.\n\n"}
{"id": "518790", "url": "https://en.wikipedia.org/wiki?curid=518790", "title": "Rock flour", "text": "Rock flour\n\nRock flour, or glacial flour, consists of fine-grained, silt-sized particles of rock, generated by mechanical grinding of bedrock by glacial erosion or by artificial grinding to a similar size. Because the material is very small, it becomes suspended in meltwater making the water appear cloudy, which is sometimes known as glacial milk. \n\nWhen the sediments enter a river, they turn the river's colour grey, light brown, iridescent blue-green, or milky white. If the river flows into a glacial lake, the lake may appear turquoise in colour as a result. When flows of the flour are extensive, a distinct layer of a different colour flows into the lake and begins to dissipate and settle as the flow extends from the increase in water flow from the glacier during snow melts and heavy rain periods. Examples of this phenomenon may be seen at Lake Pukaki and Lake Tekapo in New Zealand, Lake Louise, Moraine Lake, Emerald Lake, and Peyto Lake in Canada, Gjende lake in Norway, and several lakes (among others, Nordenskjöld and Pehoé) in Chile's Torres del Paine National Park.\n\nTypically, natural rock flour is formed during glacial migration, where the glacier grinds against the sides and bottom of the rock beneath it, but also is produced by freeze-and-thaw action, where the act of water freezing and expanding in cracks helps break up rock formations. Multiple cycles create a greater amount. \n\nAlthough clay-sized, the flour particles are not clay minerals but typically ground up quartz and feldspar. Rock flour is carried out from the system via meltwater streams, where the particles travel in suspension. Rock flour particles may travel great distances either suspended in water or carried by the wind, in the latter case forming deposits called loess.\n\nSome agronomists believe that rock flour has a powerful effect in restoring trace minerals to soil. An early experimenter was the German miller, Julius Hensel, author of \"Bread from Stones\", who reported successful results with \"steinmehl\" (stonemeal) in the 1890s. His ideas were not taken up due to technical limitations and, according to proponents of his method, because of opposition from the champions of conventional fertilisers.\n\nJohn D. Hamaker argued that widespread remineralization of soils with rock dust will be necessary to reverse soil depletion by current agriculture and forestry practice.\n\nWhile this originally was an alternative concept, increasing mainstream research has been devoted to soil amendment and other benefits of rock flour application: for instance, a pilot project on the use of glacial rock, granite and basaltic fines by the U.S. Department of Agriculture exists at the Henry A. Wallace Beltsville Agricultural Research Center. The SEER Centre in Scotland is a leading source of information on the use of rock dusts and mineral fines. The Soil Remineralization Forum was established with sponsorship from the Scottish Environment Protection Agency and has commissioned a portfolio of research into the benefits of using mineral fines. The Forum provides an interface among research, environmentalists, and industry.\n\n\n"}
{"id": "5608037", "url": "https://en.wikipedia.org/wiki?curid=5608037", "title": "Sandwich-structured composite", "text": "Sandwich-structured composite\n\nA sandwich-structured composite is a special class of composite materials that is fabricated by attaching two thin but stiff skins to a lightweight but thick core. The core material is normally low strength material, but its higher thickness provides the sandwich composite with high bending stiffness with overall low density.\n\nOpen- and closed-cell-structured foams like polyethersulfone polyvinylchloride, polyurethane, polyethylene or polystyrene foams, balsa wood, syntactic foams, and honeycombs are commonly used core materials. Sometimes, the honeycomb structure is filled with other foams for added strength. Open- and closed-cell metal foam can also be used as core materials.\n\nLaminates of glass or carbon fiber-reinforced thermoplastics or mainly thermoset polymers (unsaturated polyesters, epoxies...) are widely used as skin materials. Sheet metal is also used as skin material in some cases.\n\nThe core is bonded to the skins with an adhesive or with metal components by brazing together.\n\nA summary of the important developments in sandwich structures is given below.\n\n\nMetal composite material (MCM) is a type of sandwich formed from two thin skins of metal bonded to a plastic core in a continuous process under controlled pressure, heat, and tension.\n\nRecycled paper is also now being used over a closed-cell recycled kraft honeycomb core, creating a lightweight, strong, and fully repulpable composite board. This material is being used for applications including point-of-purchase displays, bulkheads, recyclable office furniture, exhibition stands, and wall dividers.\n\nTo fix different panels, among other solutions, a transition zone is normally used, which is a gradual reduction of the core height, until the two fiber skins are in touch. In this place, the fixation can be made by means of bolts, rivets, or adhesive.\n\nWith respect to the core type and the way the core supports the skins, sandwich structures can be divided into the following groups: homogeneously supported, locally supported, regionally supported, unidirectionally supported, bidirectionally supported. The latter group is represented by honeycomb structure which, due to an optimal performance-to-weight ratio, is typically used in most demanding applications including aerospace.\n\nThe strength of the composite material is dependent largely on two factors:\n\nSandwich structures can be widely used in sandwich panels, this kinds of panels can be in different types such as FRP sandwich panel, aluminium composite panel etc. FRP polyester reinforced composite honeycomb panel (sandwich panel) is made of polyester reinforced plastic, multi-axial high-strength glass fiber and PP honeycomb panel in special antiskid tread pattern mold through the process of constant temperature vacuum adsorption & agglutination and solidification.\n\nSandwich theory describes the behaviour of a beam, plate, or shell which consists of three layers - two face sheets and one core. The most commonly used sandwich theory is linear and is an extension of first order beam theory. Linear local buckling sandwich theory is of importance for the design and analysis of Sandwich plates or sandwich panels, which are of use in building construction, vehicle construction, airplane construction and refrigeration engineering.\n\n\n"}
{"id": "24427816", "url": "https://en.wikipedia.org/wiki?curid=24427816", "title": "Sapienic acid", "text": "Sapienic acid\n\nSapienic acid (16:1, n-10, cis-6 hexadecenoic, or sapienate) is a fatty acid that is a major component of human sebum. Sapienic acid is a sebum fatty acid that is unique to humans (from whose scientific name it takes the root \"sapiens\"). The equivalent fatty acid in mouse sebum is palmitoleic acid. Sapienic acid has been implicated in the development of acne, and it may have potent antibacterial activity.\n\nDelta-6-desaturation of palmitic acid leads to the biosynthesis of sapienic acid. In other tissues linoleic acid is the target for delta 6 desaturase, but linoleic acid is degraded in sebaceous cells, allowing the enzyme to desaturate palmitic to sapienic acid. A two-carbon extension product of sapienic acid, sebaleic acid, is also present in sebum.\n\n"}
{"id": "14703713", "url": "https://en.wikipedia.org/wiki?curid=14703713", "title": "Shock-capturing method", "text": "Shock-capturing method\n\nIn computational fluid dynamics, shock-capturing methods are a class of techniques for computing inviscid flows with shock waves. The computation of flow containing shock waves is an extremely difficult task because such flows result in sharp, discontinuous changes in flow variables such as pressure, temperature, density, and velocity across the shock.\n\nIn shock-capturing methods, the governing equations of inviscid flows (i.e. Euler equations) are cast in conservation form and any shock waves or discontinuities are computed as part of the solution. Here, no special treatment is employed to take care of the shocks themselves, which is in contrast to the shock-fitting method, where shock waves are explicitly introduced in the solution using appropriate shock relations (Rankine–Hugoniot relations). The shock waves predicted by shock-capturing methods are generally not sharp and may be smeared over several grid elements. Also, classical shock-capturing methods have the disadvantage that unphysical oscillations (Gibbs phenomenon) may develop near strong shocks.\n\nThe Euler equations are the governing equations for inviscid flow. To implement shock-capturing methods, the conservation form of the Euler equations are used. For a flow without external heat transfer and work transfer (isoenergetic flow), the conservation form of the Euler equation in Cartesian coordinate system can be written as\n\nwhere the vectors U, F, G, and H are given by\n\nwhere formula_3 is the total energy (internal energy + kinetic energy + potential energy) per unit mass. That is\n\nThe Euler equations may be integrated with any of the shock-capturing methods available to obtain the solution.\n\nFrom a historical point of view, shock-capturing methods can be classified into two general categories: classical methods and modern shock capturing methods (also called high-resolution schemes). Modern shock-capturing methods are generally upwind biased in contrast to classical symmetric or central discretizations. Upwind-biased differencing schemes attempt to discretize hyperbolic partial differential equations by using differencing based on the direction of the flow. On the other hand, symmetric or central schemes do not consider any information about the direction of wave propagation.\n\nRegardless of the shock-capturing scheme used, a stable calculation in the presence of shock waves requires a certain amount of numerical dissipation, in order to avoid the formation of unphysical numerical oscillations. In the case of classical shock-capturing methods, numerical dissipation terms are usually linear and the same amount is uniformly applied at all grid points. Classical shock-capturing methods only exhibit accurate results in the case of smooth and weak shock solutions, but when strong shock waves are present in the solution, non-linear instabilities and oscillations may arise across discontinuities. Modern shock-capturing methods usually employ nonlinear numerical dissipation, where a feedback mechanism adjusts the amount of artificial dissipation added in accord with the features in the solution. Ideally, artificial numerical dissipation needs to be added only in the vicinity of shocks or other sharp features, and regions of smooth flow must be left unmodified. These schemes have proven to be stable and accurate even for problems containing strong shock waves.\n\nSome of the well-known classical shock-capturing methods include the MacCormack method (uses a discretization scheme for the numerical solution of hyperbolic partial differential equations), Lax–Wendroff method (based on finite differences, uses a numerical method for the solution of hyperbolic partial differential equations), and Beam–Warming method. Examples of modern shock-capturing schemes include higher-order total variation diminishing (TVD) schemes first proposed by Harten, flux-corrected transport scheme introduced by Boris and Book, Monotonic Upstream-centered Schemes for Conservation Laws (MUSCL) based on Godunov approach and introduced by van Leer, various essentially non-oscillatory schemes (ENO) proposed by Harten et al., and the piecewise parabolic method (PPM) proposed by Colella and Woodward. Another important class of high-resolution schemes belongs to the approximate Riemann solvers proposed by Roe and by Osher. The schemes proposed by Jameson and Baker, where linear numerical dissipation terms depend on nonlinear switch functions, fall in between the classical and modern shock-capturing methods.\n\n\n"}
{"id": "31587146", "url": "https://en.wikipedia.org/wiki?curid=31587146", "title": "Soviet laser pistol", "text": "Soviet laser pistol\n\nThe Soviet laser pistol was a prototype energy weapon designed for cosmonauts. The weapon was magazine fed and used pyrotechnic flashbulb technology to project its beam. Another example was a Laser revolver. The weapons were meant to disable optical sensors on enemy spacecraft.\n\n\n"}
{"id": "57911638", "url": "https://en.wikipedia.org/wiki?curid=57911638", "title": "Sulfur vulcanization", "text": "Sulfur vulcanization\n\nSulfur vulcanization or sulfur vulcanisation is a chemical process for converting natural rubber or related polymers into more durable materials by heating them with sulfur or other equivalent curatives or accelerators. Sulfur forms cross-links (bridges) between sections of polymer chain which results in increased rigidity and durability, as well as other changes in the mechanical and electronic properties of the material. A vast array of products are made with vulcanized rubber, including tires, shoe soles, hoses, and conveyor belts. The term is derived from Vulcan, the Roman god of fire. \n\nThe main polymers subjected to sulfur vulcanization are polyisoprene (natural rubber, NR), polybutadiene rubber (BR) and styrene-butadiene rubber (SBR), all of which are rich in unsaturated groups. Together these have a wide range of applications of which vehicle tires are perhaps the main example. Several other specialty rubbers may also be vulcanised, such as nitrile rubber (NBR), butyl rubber (IIR) and EPDM rubber. \n\nVulcanization, in common with the curing of other thermosetting polymers, is generally irreversible; however there has been significant effort to try and develop 'de-vulcanisation' processes in order to improve the recycling of rubber wastes.\n\nThe chemistry of vulcanisation is complex, and there has long been uncertainly as to whether it proceeds in a radical or ionic manner.\nThe reactive sites, often referred to as 'cure sites', are unsaturated groups such as alkenes and allyls. During vulcanization sulfur bridges are formed between these sites, crosslinking the polymer. These bridges may consist one or several sulfur atoms. \nBoth the extent of crosslinking and the number of sulfur atoms in the crosslinks strongly influences the physical properties of the rubber produced:\n\n\nSulfur, by itself, is a slow vulcanizing agent and does not vulcanize synthetic polyolefins. Even with natural rubber large amounts of sulfur as well as high temperatures and long heating periods are necessary, with the end products often being of an unsatisfactory quality.\n\nOver the last 200 years various chemicals have been developed to improve the speed and efficiency of vulcanisation, as well as to control the nature of the cross-linking, in order to produce rubber articles with the desired properties. When used together to give a rubber with particular properties vulcanization reagents are generally referred to as a cure package.\n\nThe cure package consists of various reagents that modify the kinetics and chemistry of crosslinking. These include accelerators, activators, retarders and inhibitors. \nNote that these are merely the additives used for vulcanisation and that other compounds may also be added to the rubber such as fillers or polymer stabilizers.\n\nOrdinary sulfur (octasulfur) is by far the most commonly used reagent due to its low cost and low toxicity, however it is possible replace sulfur with other sulfur-donating compounds, for example accelerators bearing disulphide groups, in what is often termed 'efficient vulcanization' (EV). Disulfur dichloride may also be used for 'cold vulcanization'. Under normal circumstances sulfur is not miscible in the polymer prior to vulcanization and attention is paid to prevent sulfur bloom, where it migrates to the surface of the article.\n\nAccelerators act much like catalysts allowing vulcanisation to be performed cooler yet faster and with a more efficient use of sulfur.\nThey achieve this by reacting with and breaking the sulfur ring to form a reactive intermediate, referred to as a sulfurating agent. This in turn reacts with cure site in the rubber to bring about vulcanization.\n\nThere are two major classes of vulcanization accelerators: primary accelerators and secondary accelerators (also known as ultra accelerators). Primary activators date back to the use of ammonia in 1881, with secondary accelerators have been developed from around 1920.\n\nPrimary accelerators perform the bulk of the accelerating and mostly consist of thiazoles, often derivatised with sulfenamide groups. The principle compound is 2-mercaptobenzothiazole (MBT) which has been in use since the 1920s, it remains a moderately fast curing agent giving sulfur chains of a medium length but it's relatively short induction period can be a disadvantage. Other primary accelerators are essentially 'masked' forms of MBT which take time to decompose into MBT during vulcanisation and thus have longer inductions periods.\n\nOxidative dimerization of MBT gives mercaptobenzthiazole disulfide (MBTS), sulfenamide derivatives are produced by reacting this with primary amines like cyclohexylamine or tert-butylamine. Secondary amines like dicyclohexylamine can be used and result in even slower accelerators. Such a slow accelerator is required in applications in which the rubber is being cured onto a metal component to which is it required to adhere, such as the steel cords in vehicle tires.\n\nSecondary or ultra-accelerators are used in small amounts to augment the behaviour of primary accelerators, they act to boost the cure speed and increase cross-link density but also shorten the induction time which can lead to premature vulcanization. Chemically, they consist mainly of thio-carbonyl species such as thiurams, dithiocarbamates, xanthates and organic thioureas. Many of these compounds need to be combined with activators, typically zinc ions, in order to be fully active. Aromatic guanidines are also used.\n\nSecondary accelerators have very fast vulcanization speeds with minimal induction time, making them unsuitable as primary accelerators in highly unsaturated rubbers such as NR or SBR, however they can be use as primary accelerators in compounds with fewer curing site such as EPDM. Xanthates (principally, zinc isopropyl xanthate) are important in the vulcanization of latex, which is cured at relatively low temperature (100-120°C) and therefore need an inherently rapid accelerator.\nThe major thiurams used are TMTD (tetramethylthiuram disulfide) and TETD (tetraethylthiuram disulfide). The major dithiocarbamates are the zinc salts ZDMC (zinc dimethyldithiocarbamate), ZDEC (zinc diethyldithiocarbamate) and ZDBC (zinc dibutyldithiocarbamate).\n\nActivators consist of various metal salts, fatty acids as well as nitrogen-containing bases, the most important these being zinc oxide. Zinc actives many accelerators by coordination, for example causing thiuram to convert into ziram. Zinc also coordinates to the sulfur-chains of sulfurating agents, changing the most likely bond to break during cross-link formation, untilmately they promote efficient use of sulfur to give a high density of cross-links. Due to the low solubility of ZnO it is often combined with fatty acids such as stearic acid to form more soluble metallic soap (i.e. zinc stearate).\n\nIn order for good quality vulcanisation to take place the rubber, sulfur, accelerators, activators and other compounds must be fully mixed to give a homogeneous liquid. In practise this can involve melting the sulfur (mpt. 115°C) and at these temperatures vulcanisation can begin prematurely. This is often undesirable as the mixture may still need to be pumped and moulded into its final form before it sets solid. \nPremature vulcanisation is often referred to as 'scorch' and can be prevented by the addition of retarders or inhibitors which increase the induction period before vulcanisation commences and thus provide scorch resistance.\nA retarder slows down both the onset and rate of vulcanisation, whereas inhibitors only delay the start of vulcanisation and do not effect the rate to any great extent. In general inhibitors are preferred, with cyclohexylthiophthalimide (often termed PVI - pre vulcanisation inhibitor) being the most common example.\n\nThe market for new raw rubber or equivalent is large. The auto industry consumes a substantial fraction of natural and synthetic rubber. Reclaimed rubber has altered properties and is unsuitable for use in many products, including tires. Tires and other vulcanized products are potentially amenable to devulcanization, but this technology has not produced material that can supplant unvulcanized materials. The main problem is that the carbon-sulfur linkages are not readily broken, without the input of costly reagents and heat. Thus, more than half of scrap rubber is simply burned for fuel.\n\nAlthough polymeric sulfur is unstable and decomposes back to its monomer, it is possible to create stable polymers consisting mostly of sulfur via a reaction with low levels of unsaturated organic linkers (e.g. 1,3‐diisopropenylbenzene). This process is called inverse vulcanisation and produces polymers where sulfur is the main component. It is not currently of commercial importance but had been investigated as a means of producing polymers for lithium–sulfur batterys and for creating water filters with a high affinity for mercury.\n\nThe curing of rubber has been carried out since prehistoric times. The name of the first major civilization in Guatemala and Mexico, the Olmec, means 'rubber people' in the Aztec language. Ancient Mesoamericans, spanning from ancient Olmecs to Aztecs, extracted latex from \"Castilla elastica\", a type of rubber tree in the area. The juice of a local vine, \"Ipomoea alba\", was then mixed with this latex to create processed rubber as early as 1600 BC. In the Western world, rubber remained a curiosity, although it was eventually used to produce waterproofed products, such as Mackintosh rainwear, beginning in the early 1800s.\n\nIn 1832–1834 Nathaniel Hayward and Friedrich Ludersdorf discovered that rubber treated with sulfur lost its stickiness. It is likely Hayward shared his discovery with Charles Goodyear, possibly inspiring him to make the discovery of vulcanization.\n\nThomas Hancock (1786–1865), a scientist and engineer, was the first to patent vulcanization of rubber. He was awarded a British patent on May 21, 1845. Three weeks later, on June 15, 1845, Charles Goodyear was awarded a patent in the United States. It was Hancock's friend William Brockedon who coined term 'vulcanization'.\n\nGoodyear claimed that he had discovered vulcanization earlier, in 1839. He wrote the story of the discovery in 1853 in his autobiographical book \"Gum-Elastica\". Here is Goodyear's account of the invention, taken from \"Gum-Elastica\". Although the book is an autobiography, Goodyear chose to write it in the third person so that and referred to in the text are the author. He describes the scene in a rubber factory where his brother worked:\n\nThe inventor made experiments to ascertain the effect of heat on the same compound that had decomposed in the mail-bags and other articles. He was surprised to find that the specimen, being carelessly brought into contact with a hot stove, charred like leather.\n\nGoodyear goes on to describe how his discovery was not readily accepted.\n\nHe directly inferred that if the process of charring could be stopped at the right point, it might divest the gum of its native adhesiveness throughout, which would make it better than the native gum. Upon further trial with heat, he was further convinced of the correctness of this inference, by finding that the India rubber could not be melted in boiling sulfur at any heat, but always charred.\n\nHe made another trial of heating a similar fabric before an open fire. The same effect, that of charring the gum, followed. There were further indications of success in producing the desired result, as upon the edge of the charred portion appeared a line or border, that was not charred, but perfectly cured.\n\nGoodyear then goes on to describe how he moved to Woburn, Massachusetts and carried out a series of systematic experiments to optimize the curing of rubber, collaborating with Nathaniel Hayward.\n\nOn ascertaining to a certainty that he had found the object of his search and much more, and that the new substance was proof against cold and the solvent of the native gum, he felt himself amply repaid for the past, and quite indifferent to the trials of the future.\n\nThe discovery of the rubber-sulfur reaction revolutionized the use and applications of rubber, changing the face of the industrial world. Formerly, the only way to seal a small gap between moving machine parts was to use leather soaked in oil. This practice was acceptable only at moderate pressures, but above a certain point, machine designers were forced to compromise between the extra friction generated by tighter packing and greater leakage of steam. Vulcanized rubber solved this problem. It could be formed to precise shapes and dimensions, it accepted moderate to large deformations under load and recovered quickly to its original dimensions once the load is removed. These exceptional qualities, combined with good durability and lack of stickiness, were critical for an effective sealing material. Further experiments in the processing and compounding of rubber by Hancock and his colleagues led to a more reliable process.\n\nAround 1900, disulfiram was introduced as a vulcanizing agent, and became widely used. \n\nIn 1905 George Oenslager discovered that a derivative of aniline called thiocarbanilide accelerated the reaction of sulfur with rubber, leading to shorter cure times and reducing energy consumption. This breakthrough was almost as fundamental to the rubber industry as Goodyear's sulfur cure. Accelerators made the cure process faster, improved the reliability of the process and enabled vulcanization to be applied to synthetic polymers. One year after his discovery, Oenslager had found hundreds of applications for his additive. Thus, the science of accelerators and retarders was born. An accelerator speeds up the cure reaction, while a retarder delays it. A typical retarder is cyclohexylthiophthalimide. In the subsequent century chemists developed other accelerators and ultra-accelerators, that are used in the manufacture of most modern rubber goods.\n"}
{"id": "1267525", "url": "https://en.wikipedia.org/wiki?curid=1267525", "title": "Thermate", "text": "Thermate\n\nThermate is a variation of thermite and is an incendiary pyrotechnic composition that can generate short bursts of very high temperatures focused on a small area for a short period of time. It is used primarily in incendiary grenades. \n\nThe main chemical reaction in thermate is the same as in thermite: an aluminothermic reaction between powdered aluminium and a metal oxide. In addition to thermite, thermate also contains sulfur and sometimes barium nitrate, both of which increase its thermal effect, create flame in burning, and significantly reduce the ignition temperature. Various mixtures of these compounds can be called thermate, but to avoid confusion with thermate-TH3, one can refer to them as thermite variants or analogs. The composition by weight of Thermate-TH3 (in military use) is 68.7% thermite, 29.0% barium nitrate, 2.0% sulfur and 0.3% binder (such as PBAN). As both thermite and thermate are notoriously difficult to ignite, initiating the reaction normally requires supervision and sometimes persistent effort.\n\nBecause thermate burns at higher temperatures than ordinary thermite, it has military applications in cutting through tank armor or other hardened military vehicles or bunkers. As with thermite, thermate's ability to burn without an external supply of oxygen renders it useful for underwater incendiary devices.\n\n\n"}
{"id": "30924691", "url": "https://en.wikipedia.org/wiki?curid=30924691", "title": "Timor-Leste Petroleum Fund", "text": "Timor-Leste Petroleum Fund\n\nThe Timor-Leste Petroleum Fund () is a sovereign wealth fund into which the surplus wealth produced by East Timor petroleum and gas income is deposited by the East Timorese government.\n\nThe fund was established in 2005 with an opening balance of $205 million. As of 31 December 2010, the capital of the fund was US$6.9 billion.\n\nEarnings of the fund were around 4% for the last 5 years, as of July 2011. A key milestone was achieved in June 2014 when the equity allocation in the fund reached 40%. The Petroleum Fund returned 3.3% in 2014, or 2.5% after inflation. The fund's end-of-year balance in 2014 was $16.5 billion.\n\nThe fund signed up to the Santiago Principles on best practices for managing Sovereign Wealth Funds and joined the International Forum of Sovereign Wealth Funds. As a member it publishes how it adopts and implements the principles within its governance procedures.\n"}
{"id": "6587903", "url": "https://en.wikipedia.org/wiki?curid=6587903", "title": "Tonewood", "text": "Tonewood\n\nTonewood refers to specific wood varieties that possess tonal properties that make them good choices for use in woodwind or acoustic stringed instruments.\n\nAs a rough generalization it can be said that stiff-but-light softwoods are favored for the soundboards or soundboard-like surface that transmits the vibrations of the strings to the ambient air. Hardwoods are favored for the body or framing element of an instrument. Woods used for woodwind instruments include African Blackwood, (Dalbergia melanoxylon), also known as Grenadilla wood, used in modern clarinets and oboes. Bassoons are usually made of maple, especially Acer platanoides. Wooden flutes, recorders, and baroque and classical period instruments may be made of various hardwoods, such as pear wood (Pyrus species), boxwood (Buxus species), or ebony (Diospyros species).\n\n\n\nIn addition to perceived differences in acoustic properties, a luthier may use a tonewood because of:\n\n\nMany tonewoods come from sustainable sources through specialist dealers. Spruce, for example, is very common, but large pieces with even grain represent a small proportion of total supply and can be expensive. Some tonewoods are particularly hard to find on the open market, and small-scale instrument makers often turn to reclamation, for instance from disused salmon traps in Alaska, various old construction in the U.S Pacific Northwest, from trees that have blown down, or from specially permitted removals in conservation areas where logging is not generally permitted. Mass market instrument manufacturers have started using Asian and African woods, such as Bubinga (\"Guibourtia\" species) and Wenge (\"Millettia laurentii\"), as inexpensive alternatives to traditional tonewoods.\n\nThe Fiemme Valley, in the Alps of Northern Italy, has long served as a source of high-quality spruce for musical instruments, dating from the violins of Antonio Stradivari to the piano soundboards of the contemporary maker Fazioli.\n\nTonewood choices vary greatly among different instrument types. Guitar makers generally favor quartersawn wood because it provides added stiffness and dimensional stability. Soft woods, like spruce, may be split rather than sawn into boards so the board surface follows the grain as much as possible, thus limiting run-out.\n\nFor most applications, wood must be dried before use, either in air or kilns. Some luthiers prefer further seasoning for several years. Some guitar manufacturers subject the wood to rarefaction, which mimics the natural aging process of tonewoods. Torrefaction is also used for this purpose, but it often changes the cosmetic properties of the wood.\n\n"}
{"id": "15518944", "url": "https://en.wikipedia.org/wiki?curid=15518944", "title": "Trestle (mill)", "text": "Trestle (mill)\n\nThe Trestle of a Post mill is the arrangement of the \"Main post\", \"crosstrees\" and \"quarterbars\" that form the substructure of this type of windmill. It may or may not be surrounded by a \"roundhouse\". Post mills without a roundhouse are known as \"Open Trestle Post Mills\".\nA Trestle Mill is a variety of Smock mill, usually without weatherboards, formerly used for drainage in the Norfolk Broads.\n"}
{"id": "169942", "url": "https://en.wikipedia.org/wiki?curid=169942", "title": "Troy weight", "text": "Troy weight\n\nTroy weight is a system of units of mass customarily used for precious metals and gemstones. One troy ounce (abbreviated \"t oz\" or \"oz t\") is equal to , (or about 1.0971 oz. avoirdupois, the \"avoirdupois\" ounce being the most common definition of an \"ounce\" in the US). There are only 12 troy ounces per troy pound, rather than the 16 ounces per pound found in the more common avoirdupois system. The avoirdupois pound has 7000 grains whereas the troy pound has only 5760 grains (i.e. 12 × 480 grains). Both systems use the same grain defined by the international yard and pound agreement of 1959 as . Therefore, the troy ounce is , compared with the avoirdupois ounce, which is . The troy ounce, then, is about 10% heavier (ratio 192/175) than the avoirdupois ounce. Although troy ounces are still used to weigh gold, silver, and gemstones, troy weight is no longer used in most other applications. One troy ounce of gold is denoted with the ISO 4217 currency code XAU, while one troy ounce of silver is denoted as XAG.\n\nTroy weight probably takes its name from the French market town of Troyes in France where English merchants traded at least as early as the early 9th century. The name \"troy\" is first attested in 1390, describing the weight of a platter, in an account of the travels in Europe of the Earl of Derby.\n\nCharles Moore Watson (1844–1916) proposes an alternative etymology: \"The Assize of Weights and Measures\" (also known as \"Tractatus de Ponderibus et Mensuris\"), one of the statutes of uncertain date from the reign of either Henry III or Edward I, thus before 1307, specifies \"troni ponderacionem\"—which the Public Record Commissioners translate as \"troy weight\". The word \"troni\" refers to markets. Watson finds the dialect word \"troi\", meaning a balance in Wright's Dialect Dictionary. Troy weight referred to the tower system; the earliest reference to the modern troy weights is in 1414.\n\nMany aspects of the troy weight system were indirectly derived from the Roman monetary system. The Romans used bronze bars of varying weights as currency. An (\"heavy bronze\") weighed one pound. One twelfth of an was called an , or in English, an \"ounce\". Before the adoption of the metric system, many systems of troy weights were in use in various parts of Europe, among them Holland troy, Paris troy, etc. Their values varied from one another by up to several percentage points. Troy weights were first used in England in the 15th century, and were made official for gold and silver in 1527. The British Imperial system of weights and measures (also known as Imperial units) was established in 1824, prior to which the troy weight system was a subset of pre-Imperial English units.\n\nThe troy ounce in use today is essentially the same as the British Imperial troy ounce (1824–1971), adopted as an official weight standard for United States coinage by Act of Congress on May 19, 1828.\nThe British Imperial troy ounce (known more commonly simply as the imperial troy ounce) was based on, and virtually identical with, the pre-1824 British troy ounce and the pre-1707 English troy ounce. (1824 was the year the British Imperial system of weights and measures was adopted, 1707 was the year of the Act of Union which created the Kingdom of Great Britain.) Troy ounces have been used in England since about 1400 and the English troy ounce was officially adopted for coinage in 1527. Before that time, various sorts of troy ounces were in use on the continent.\n\nThe troy ounce and grain were also part of the apothecaries' system. This was long used in medicine, but has now been largely replaced by the metric system (milligrams).\n\nThe only troy weight in widespread use today is the British Imperial troy ounce and its American counterpart. Both are currently based on a grain of 0.06479891 gram (exact, by definition), with 480 grains to a troy ounce (compared with grains for an ounce avoirdupois).\n\nThe British Empire abolished the 12-ounce troy pound in the 19th century, though it has been retained (although rarely used) in the American system.\n\nThe origin of the troy weight system is unknown. Although the name probably comes from the Champagne fairs at Troyes, in northeastern France, the units themselves may be of more northern origin. English troy weights were nearly identical to the troy weight system of Bremen. (The Bremen troy ounce had a mass of 480.8 British Imperial grains.)\n\nAn alternative suggestion is that the weights come from the Muslim domains by way of the Gold Dirhem (47.966 British Imperial grains), in the manner that King Offa's weights were derived from the silver Dirhem (about 45.0 British grains).\n\nAccording to Watson, troy relates to a dialect word troi (balance). Then troy weight is a style of weighing, like auncel or bismar weights, or other kindred methods. The troy weight then refers to weighing of small precious or potent goods, such as bullion and medicines.\n\nTroy ounces are still often used in precious metal markets in countries that otherwise use International System of Units (SI), except in East Asia. The People's Bank of China, in particular, which has never historically used troy measurements, has begun issuing Gold Pandas minted according to SI weights.\n\nThe troy pound is 5 760 grains (≈ 373.24 g, 12 oz t), while an avoirdupois pound is approximately 21.53% heavier at 7 000 grains (≈ 453.59 g).\n\nOne troy ounce (oz t) is equal to grams.\nAlso equal to avoirdupois ounces, exactly , or about 10% larger.\n\nThe pennyweight symbol is \"dwt\". There are 24 grains in 1 dwt, and 20 dwt in one troy ounce. Because there were 12 troy ounces in the old troy pound, there would have been 240 pennyweights to the pound—the basis of the fact that the old British pound sterling of currency contained 240 pence. (However, prior to 1526, English pound sterling was based on the tower pound, which is of a troy pound.) The \"d\" in \"dwt\" stands for \"denarius\", the ancient Roman coin that equates loosely to a penny. The symbol \"d\" for penny can be recognized in the notation for British pre-decimal pennies, in which pounds, shillings, and pence were indicated using the symbols \"£\", \"s\", and \"d\", respectively. For example, \"£6 11s 8d\" indicated six pounds, eleven shillings, and eight pence.\n\nMint weights, also known as \"moneyers' weights\" were legalised by Act of Parliament dated 17 July 1649 entitled \"An Act touching the monies and coins of England\". A grain is 20 mites, a mite is 24 droits, a droit is 20 perits, a perit is 24 blanks.\n\nIn Scotland, the Incorporation of Goldsmiths of the City of Edinburgh used a system in multiples of sixteen. (\"See Assay-Master's Accounts, 1681–1702, on loan from the Incorporation to the National Archives of Scotland\".) Thus, there were 16 drops to the troy ounce, 16 ounces to the troy pound, and 16 pounds to the troy stone. The Scots had several other ways of measuring precious metals and gems, but this was the common usage for gold and silver.\n\nThe Pound was 7716 BI grains, but after the union, rounded to 7680 BI grains. This divides to 16 ounces, each of 16 drops, each of 30 grains. The rounding makes the ounce and grain equal to the English standard.\n\nThe Dutch troy system is based on a Mark, of 8 Ounces, the ounce of 20 Engels (pennyweight), the Engel of 32 As. The mark was rated as 3798 Grains, English troy, or 246.084 metric grams. The divisions are identical to the tower system.\n\nThe troy system was used in the apothecaries' system, but with different further subdivisions.\n\nKing Offa's currency reform replaced the sceat with the silver penny. This coin was derived from half of a silver dirhem. The weights were then derived by a count of coins, by a mix of Charlemagne and Roman systems. A shilling was set to twelve pence, an ounce to twenty pence, and a pound to twelve ounces or twenty shillings. The penny was quite a lot of money, so weight by coins was not a general practice.\n\nLater kings debased the coin, both in weight and fineness. The original pound divided was the tower pound of 5400 grains, but a later pound of 5760 grains displaced it. Where once 240 pence made a tower pound (and 256 make a troy pound), by the time of the United Kingdom Weights and Measures Act of 1824, a troy pound gives 792 silver pence, still minted as such as Maundy Money.\n\nSterling originally referred to the Norman silver penny of the late 11th century. The coin was minted to a fineness of 11 oz, 2 dwt (in the pound), or 925 Millesimal fineness.\n\n"}
{"id": "5134517", "url": "https://en.wikipedia.org/wiki?curid=5134517", "title": "Tulsa (film)", "text": "Tulsa (film)\n\nTulsa is a 1949 American Technicolor Western action film directed by Stuart Heisler and starring Susan Hayward and Robert Preston, and featured Lloyd Gough, Chill Wills (as the narrator), and Ed Begley in one of his earliest film roles, billed as Edward Begley.\n\nThe film's plot revolved around greed, conservation, and romance. It was nominated for an Oscar for its special effects in 1950.\n\nThe plot revolved around the Tulsa, Oklahoma oil boom of the 1920s and detailed how obsession with accumulating wealth and power can tend to corrupt moral character. The story begins with the death of rancher Nelse Lansing, who is killed by an oil well blowout while visiting a well operated by Tanner Petroleum to report that pollution from the oil production has killed some of his cattle. The plot thickens as Lansing's daughter, Cherokee, acquires drilling rights and meets Brad Brady, a geologist who wants the oil drillers to limit their drilling in order to minimize oil field depletion and to preserve the area's grasslands.\n\nA fire in a derrick tailing pool started by Jim Redbird, a Cherokee who had been made a rich owner of oil land through crooked dealings of oilmen, and who later renounces his holdings, results in an extravagant fire scene for which the movie got its Oscar nomination. In its aftermath, in recognition of the destruction caused by improper oil drilling, and how money and power can corrupt even those who love the land, the oil drillers and the geologist learn to work together.\n\n\nThe film earned an estimated $1.6 million in the US. It recorded a loss of $746,099.\n\n\n"}
{"id": "12348147", "url": "https://en.wikipedia.org/wiki?curid=12348147", "title": "Ultimate Tornado", "text": "Ultimate Tornado\n\nUltimate Tornado is a documentary that first aired on the National Geographic Channel on April 12, 2006. It focuses on several unusually violent tornado events that have occurred in the United States, which include the 2004 Attica, Kansas tornado outbreak (F2-F4), the 1995 Pampa, Texas tornado (F4), the Jarrell tornado outbreak (F5), and the 1999 Oklahoma tornado outbreak (F5). \n\nIt last examined the possible effects of a theoretical F6 tornado hitting downtown Dallas, Texas, postulating that this would be the worst tornado in history in terms of cost, damage, destruction and loss of life. \n"}
{"id": "16844676", "url": "https://en.wikipedia.org/wiki?curid=16844676", "title": "W &amp; T Avery Ltd.", "text": "W &amp; T Avery Ltd.\n\nW & T Avery Ltd. is a British manufacturer of weighing machines. The company was founded in the early 18th century and took the name W & T Avery in 1818. Having been taken over by GEC in 1979 the company was later renamed into GEC-Avery. The company became Avery Berkel in 1993 when GEC acquired the Dutch company Berkel. After the take over by Weigh-Tronix in 2000 the company was again renamed to be called Avery Weigh-Tronix. The company is based in Smethwick, West Midlands, United Kingdom.\n\nThe undocumented origin of the company goes back to 1730 when James Ford established the business in Digbeth. On Joseph Balden the then owner's death in 1813 William and Thomas Avery took over his scalemaking business and in 1818 renamed it W & T Avery. The business rapidly expanded and in 1885 they owned three factories: the Atlas Works in West Bromwich, the Mill Lane Works in Birmingham and the Moat Lane Works in Digbeth. In 1891 the business became a limited company with a board of directors and in 1894 the shares were quoted on the London Stock Exchange. In 1895 the company bought the legendary Soho Foundry in Smethwick, a former steam engine factory owned by James Watt & Co. In 1897 the move was complete and the steam engine business was gradually converted to pure manufacture of weighing machines. The turn of the century was marked by managing director William Hipkins' determined efforts in broadening the renown of the Avery brand and transforming the business into a specialist manufacturer of weighing machines. By 1914 the company occupied an area of 32,000m² and had some 3000 employees.\n\nIn the inter-war period the growth continued with the addition of specialised shops for cast parts, enamel paints and weighbridge assembly and the product range diversified into counting machines, testing machines, automatic packing machines and petrol pumps. During the second world war the company also produced various types of heavy guns. At that time the site underwent severe damage from parachute mines and incendiary bombs. \n\nFrom 1931 to 1973 the company occupied the 18th-century Middlesex Sessions House in Clerkenwell as its headquarters.\n\nChanges in weighing machine technology after World War II led to the closure of the foundry, the introduction of load cells and electronic weighing with the simultaneous gradual disappearance of purely mechanical devices.\n\nThe continued expansion was partly achieved through a series of acquisitions of other companies. The most important are:\n\n\nAfter almost a century of national and international expansion the company was taken over by GEC in 1979. Keith Hodgkinson, managing director at the time, completed the turn-around from mechanical to electronic weighing with a complete overhaul of the product range of retail scales and industrial platform scales. In 1993 GEC took over the Dutch-based company Berkel and the Avery-Berkel name was introduced. In 2000 the business was in turn acquired by the US-American company Weigh-Tronix, who already owned Salter, and is today operating as Avery Weigh-Tronix.\n\nIn 2008 Illinois Tool Works Inc. purchased Avery Weigh-Tronix from its then owners, European Capital.\n\n\n\n\n"}
{"id": "11253931", "url": "https://en.wikipedia.org/wiki?curid=11253931", "title": "Zerodur", "text": "Zerodur\n\nZerodur (notation of the manufacturer: ZERODUR®), a registered trademark of Schott AG, is a lithium-aluminosilicate glass-ceramic produced by Schott AG since 1968. It has been used for a number of very large telescope mirrors including Keck I, Keck II, and SOFIA, as well as some smaller telescopes (such as the GREGOR Solar Telescope). With its very low coefficient of thermal expansion it can be used to produce mirrors that retain acceptable figures in extremely cold environments such as deep space. Although it has advantages for applications requiring a coefficient of thermal expansion less than that of borosilicate glass, it remains very expensive as compared to borosilicate. The tight tolerance on CTE, ±0.007 K, allows highly accurate applications that require high-precision.\n\n\nZerodur has both an amorphous (vitreous) component and a crystalline component. Its most important properties are:\n\n"}
