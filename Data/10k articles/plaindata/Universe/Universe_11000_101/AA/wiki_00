{"id": "2769817", "url": "https://en.wikipedia.org/wiki?curid=2769817", "title": "Adjustable-speed drive", "text": "Adjustable-speed drive\n\nAdjustable speed drive (ASD) or variable-speed drive (VSD) describes equipment used to control the speed of machinery. Many industrial processes such as assembly lines must operate at different speeds for different products. Where process conditions demand adjustment of flow from a pump or fan, varying the speed of the drive may save energy compared with other techniques for flow control.\n\nWhere speeds may be selected from several different pre-set ranges, usually the drive is said to be adjustable speed. If the output speed can be changed without steps over a range, the drive is usually referred to as \"variable speed\".\n\nAdjustable and variable speed drives may be purely mechanical (termed \"variators\"), electromechanical, hydraulic, or electronic.\n\nAC electric motors can be run in fixed-speed operation determined by the number of stator pole pairs in the motor and the frequency of the alternating current supply. AC motors can be made for \"pole changing\" operation, reconnecting the stator winding to vary the number of poles so that two, sometimes three, speeds are obtained. For example a machine with 8 physical pairs of poles, could be connected to allow running with either 4 or 8 pole pairs, giving two speeds - at 60 Hz, these would be 1800 RPM and 900 RPM. If speed changes are rare, the motor may be initially connected for one speed then re-wired for the other speed as process conditions change, or, magnetic contactors can be used to switch between the two speeds as process needs fluctuate. Connections for more than three speeds are uneconomic.\n\nThe number of such fixed-speed-operation speeds is constrained by cost as number of pole pairs increases. If many different speeds or continuously variable speeds are required, other methods are required.\n\nDirect-current motors allow for changes of speed by adjusting the shunt field current. Another way of changing speed of a direct current motor is to change the voltage applied to the armature.\n\nAn adjustable speed drive might consist of an electric motor and controller that is used to adjust the motor's operating speed. The combination of a constant-speed motor and a continuously adjustable mechanical speed-changing device might also be called an adjustable speed drive. Power electronics based variable frequency drives are rapidly making older technology redundant.\n\nProcess control and energy conservation are the two primary reasons for using an adjustable speed drive. Historically, adjustable speed drives were developed for process control, but energy conservation has emerged as an equally important objective.\n\nAn adjustable speed drive can often provide smoother operation compared to an alternative fixed speed mode of operation. For example, in a sewage lift station sewage usually flows through sewer pipes under the force of gravity to a wet well location. From there it is pumped up to a treatment process. When fixed speed pumps are used, the pumps are set to start when the level of the liquid in the wet well reaches some high point and stop when the level has been reduced to a low point. Cycling the pumps on and off results in frequent high surges of electric current to start the motors that results in electromagnetic and thermal stresses in the motors and power control equipment, the pumps and pipes are subjected to mechanical and hydraulic stresses, and the sewage treatment process is forced to accommodate surges in the flow of sewage through the process. When adjustable speed drives are used, the pumps operate continuously at a speed that increases as the wet well level increases. This matches the outflow to the average inflow and provides a much smoother operation of the process.\n\nFans and pumps consume a large part of the energy used by industrial electrical motors. Where fans and pumps serve a varying process load, a simple way to vary the delivered quantity of fluid is with a damper or valve in the outlet of the fan or pump, which by its increased pressure drop, reduces the flow in the process. However, this additional pressure drop represents energy loss. Sometimes it is economically practical to put in some device that recovers this otherwise lost energy. With a variable speed drive on the pump or fan, the supply can be adjusted to match demand and no extra loss is introduced. \n\nFor example, when a fan is driven directly by a fixed-speed motor, the airflow is designed for the maximum demand of the system, and so will usually be higher than it needs to be. Airflow can be regulated using a damper but it is more efficient to directly regulate fan motor speed. Following the affinity laws, for 50% of the airflow, the variable-speed motor consumes about 20% of the input power (Amps). The fixed-speed motor still consumes about 85% of the input power at half the flow.\n\nSome prime movers (internal combustion engines, reciprocating or turbine steam engines, water wheels, and others) have a range of operating speeds which can be varied continuously (by adjusting fuel rate or similar means). However, efficiency may be low at extremes of the speed range, and there may be system reasons why the prime mover speed cannot be maintained at very low or very high speeds. \n\nBefore electric motors were invented, mechanical speed changers were used to control the mechanical power provided by water wheels and steam engines. When electric motors came into use, means of controlling their speed were developed almost immediately. Today, various types of mechanical drives, hydraulic drives and electric drives compete with one another in the industrial drives market.\n\nThere are two types of mechanical drives, variable pitch drives and traction drives.\n\nVariable pitch drives are pulley and belt drives in which the pitch diameter of one or both pulleys can be adjusted.\n\nTraction drives transmit power through metal rollers running against mating metal rollers. The input/output speed ratio is adjusted by moving the rollers to change the\ndiameters of the contact path. Many different roller shapes and mechanical designs have been used.\n\nThere are three types of hydraulic drives, those are: hydrostatic drives, hydrodynamic drives and hydroviscous drives.\n\nA hydrostatic drive consists of a hydraulic pump and a hydraulic motor. Since positive displacement pumps and motors are used, one revolution of the pump or motor corresponds to a set volume of fluid flow that is determined by the displacement regardless of speed or torque. Speed is regulated by regulating the fluid flow with a valve or by changing the displacement of the pump or motor. Many different design variations have been used. A swash plate drive employs an axial piston pump and/or motor in which the swash plate angle can be changed to adjust the displacement and thus adjust the speed.\n\nHydrodynamic drives or fluid couplings use oil to transmit torque between an impeller on the constant-speed input shaft and a rotor on the adjustable-speed output shaft. The torque converter in the automatic transmission of a car is a hydrodynamic drive.\n\nA hydroviscous drive consists of one or more discs connected to the input shaft pressed against a similar disc or discs connected to the output shaft. Torque is transmitted from the input shaft to the output shaft through an oil film between the discs. The transmitted torque is proportional to the pressure exerted by a hydraulic cylinder that presses the discs together. This effect may be used as a clutch, such at the Hele-Shaw clutch, or as a variable-speed drive, such as the Beier variable-ratio gear.\n\nMechanical and hydraulic adjustable speed drives are usually called transmissions or continuously variable transmissions when they are used in vehicles, farm equipment and some other types of equipment.\n\nControl can mean either manually adjustable - by means of a potentiometer or linear hall effect device, (which is more resistant to dust and grease) or it can also be automatically controlled for example by using a rotational detector such as a Gray code optical encoder.\n\nThere are three general categories of electric drives: DC motor drives, eddy current drives and AC motor drives. Each of these general types can be further divided into numerous variations. Electric drives generally include both an electric motor and a speed control unit or system. The term \"drive\" is often applied to the controller without the motor. In the early days of electric drive technology, electromechanical control systems were used. Later, electronic controllers were designed using various types of vacuum tubes. As suitable solid state electronic components became available, new controller designs incorporated the latest electronic technology.\n\nDC drives are DC motor speed control systems. Since the speed of a DC motor is directly proportional to armature voltage and inversely proportional to motor flux (which is a function of field current), either armature voltage or field current can be used to control speed. Several types of DC motors are described in the electric motor article. The electric motor article also describes electronic speed controls used with various types of DC motors.\n\nAn eddy current drive (sometimes called a Dynamatic drive, after one of the most common brand names) consists of a fixed speed motor (generally an induction motor) and an eddy current clutch. The clutch contains a fixed speed rotor and an adjustable speed rotor separated by a small air gap. A direct current in a field coil produces a magnetic field that determines the torque transmitted from the input rotor to the output rotor. The controller provides closed loop speed regulation by varying clutch current, only allowing the clutch to transmit enough torque to operate at the desired speed. Speed feedback is typically provided via an integral AC tachometer.\n\nEddy current drives are slip-controlled systems the slip energy of which is necessarily all dissipated as heat. Such drives are therefore generally less efficient than AC/DC-AC conversion based drives. The motor develops the torque required by the load and operates at full speed. The output shaft transmits the same torque to the load, but turns at a slower speed. Since power is proportional to torque multiplied by speed, the input power is proportional to motor speed times operating torque while the output power is output speed times operating torque. The difference between the motor speed and the output speed is called the \"slip speed\". Power proportional to the slip speed times operating torque is dissipated as heat in the clutch. While it has been surpassed by the variable-frequency drive in most variable-speed applications, the eddy current clutch is still often used to couple motors to high-inertia loads that are frequently stopped and started, such as stamping presses, conveyors, hoisting machinery, and some larger machine tools, allowing gradual starting, with less maintenance than a mechanical clutch or hydraulic transmission.\n\nAC drives are AC motor speed control systems.\n\nA slip-controlled wound-rotor induction motor (WRIM) drive controls speed by varying motor slip via rotor slip rings either by electronically recovering slip power fed back to the stator bus or by varying the resistance of external resistors in the rotor circuit. Along with eddy current drives, resistance-based WRIM drives have lost popularity because they are less efficient than AC/DC-AC-based WRIM drives and are used only in special situations.\n\nSlip energy recovery systems return energy to the WRIM's stator bus, converting slip energy and feeding it back to the stator supply. Such recovered energy would otherwise be wasted as heat in resistance-based WRIM drives. Slip energy recovery variable-speed drives are used in such applications as large pumps and fans, wind turbines, shipboard propulsion systems, large hydro-pumps/generators and utility energy storage flywheels. Early slip energy recovery systems using electromechanical components for AC/DC-AC conversion (i.e., consisting of rectifier, DC motor and AC generator) are termed \"Kramer drives\", more recent systems using variable-frequency drives (VFDs) being referred to as \"static Kramer drives\".\n\nIn general, a VFD in its most basic configuration controls the speed of an induction or synchronous motor by adjusting the frequency of the power supplied to the motor. \n\nWhen changing VFD frequency in standard low-performance variable-torque applications using Volt-per-Hertz (V/Hz) control, the AC motor's voltage-to-frequency ratio can be maintained constant, and its power can be varied, between the minimum and maximum operating frequencies up to a base frequency. Constant voltage operation above base frequency, and therefore with reduced V/Hz ratio, provides reduced torque and constant power capability.\n\nRegenerative AC drives are a type of AC drive which have the capacity to recover the braking energy of a load moving faster than the motor speed (an overhauling load) and return it to the power system.\n\nThe VFD article provides additional information on electronic speed controls used with various types of AC motors.\n\n\n"}
{"id": "488132", "url": "https://en.wikipedia.org/wiki?curid=488132", "title": "Anisothermal plasma", "text": "Anisothermal plasma\n\nAn Anisothermal plasma is a plasma which thermal state can be approximated by more than one temperature for the different degrees of freedom of the plasma. \nThe degrees of freedom refer to translation (kinetic energy), rotation, vibration of each particle type. \n\nExamples of anisothermal plasmas can be found among low-pressure plasmas that are excited by high frequency electric fields, see frequency classification of plasmas. They generally exhibit hot electrons that are powered by the alternating electric field, and a neutral and ion component, which is significantly colder due the low efficiency of the energy transfer between light electrons and heavy neutrals and ions.\n\nThe term nonthermal plasma includes anisothermal plasmas and means that the different degrees of freedom of the plasma are \"not\" in thermal equilibrium.\n\n"}
{"id": "316737", "url": "https://en.wikipedia.org/wiki?curid=316737", "title": "Autoignition temperature", "text": "Autoignition temperature\n\nThe autoignition temperature or kindling point of a substance is the lowest temperature at which it spontaneously ignites in normal atmosphere without an external source of ignition, such as a flame or spark. This temperature is required to supply the activation energy needed for combustion. The temperature at which a chemical ignites decreases as the pressure or oxygen concentration increases. It is usually applied to a combustible fuel mixture.\n\nAutoignition temperatures of liquid chemicals are typically measured using a flask placed in a temperature-controlled oven in accordance with the procedure described in ASTM E659.\n\nWhen measured for plastics, autoignition temperature can be also measured under elevated pressure and at 100% oxygen concentration. The resulting value is used as a predictor of viability for high-oxygen service. The main testing standard for this is ASTM G72.\n\nThe time formula_1 it takes for a material to reach its autoignition temperature formula_2 when exposed to a heat flux formula_3 is given by the following equation:\n\nwhere \"k\" = thermal conductivity, \"ρ\" = density, and \"c\" = specific heat capacity of the material of interest, formula_5 is the initial temperature of the material (or the temperature of the bulk material).\n\nTemperatures vary widely in the literature and should only be used as estimates. Factors that may cause variation include partial pressure of oxygen, altitude, humidity, and amount of time required for ignition. Generally the autoignition temperature for hydrocarbon/air mixtures decreases with increasing molecular mass and increasing chain length. The autoignition temperature is also higher for branched-chain hydrocarbons than for straight-chain hydrocarbons.\n\n\n"}
{"id": "3958005", "url": "https://en.wikipedia.org/wiki?curid=3958005", "title": "Availability factor", "text": "Availability factor\n\nThe availability factor of a power plant is the amount of time that it is able to produce electricity over a certain period, divided by the amount of the time in the period. Occasions where only partial capacity is available may or may not be deducted. Where they are deducted, the metric is titled \"equivalent availability factor\" (EAF). The availability factor should not be confused with the capacity factor. The capacity factor for a given period can never exceed the availability factor for the same period. The difference arises when the plant is run at less than full capacity, in which case the capacity factor is less than the availability factor.\n\nThe availability of a power plant varies greatly depending on the type of fuel, the design of the plant and how the plant is operated. Everything else being equal, plants that are run less frequently have higher availability factors because they require less maintenance and because more inspections and maintenance can be scheduled during idle time. Most thermal power stations, such as coal, geothermal and nuclear power plants, have availability factors between 70% and 90%. Newer plants tend to have significantly higher availability factors, but preventive maintenance is as important as improvements in design and technology. Gas turbines have relatively high availability factors, ranging from 80% to 99%. Gas turbines are commonly used for peaking power plants, co-generation plants and the first stage of combined cycle plants.\n\nOriginally the term availability factor was used only for power plants that depended on an active, controlled supply of fuel, typically fossil or later also nuclear. The emergence of renewable energy such as hydro, wind and solar power, which operate without an active, controlled supply of fuel and which come to a standstill when their natural supply of energy ceases, requires a more careful distinction between the availability factor and the capacity factor. By convention, such zero production periods are counted against the capacity factor but not against the availability factor, which thus remains defined as depending on an active, controlled supply of fuel, along with factors concerning reliability and maintenance. A wind turbine cannot operate in wind speeds above a certain limit, which counts against its availability factor. With this definition, modern wind turbines which require very little maintenance, have very high availability factors, up to about 98%. Photovoltaic power stations which have few or no moving parts and which can undergo planned inspections and maintenance during night have an availability factor approaching or equal to 100%.\n\n\n"}
{"id": "1011901", "url": "https://en.wikipedia.org/wiki?curid=1011901", "title": "Borazon", "text": "Borazon\n\nBorazon is a brand name of a cubic form of boron nitride (cBN). Its color ranges from black to brown and gold, depending on the chemical bond. It is one of the hardest known materials, along with various forms of diamond and boron nitride. Borazon is a crystal created by heating equal quantities of boron and nitrogen at temperatures greater than 1800 °C (3300 °F) at 7 GPa (1 million lbf/in). \n\nBorazon was first produced in 1957 by Robert H. Wentorf, Jr., a physical chemist working for the General Electric Company. In 1969, General Electric adopted the name Borazon as its trademark for the material.\n\nBorazon has a number of uses, such as: cutting tools, dies, punches, shears, knives, saw blades, bearing rings, needles, rollers, spacers, balls, pump, compressor parts, engine & drive train components (e.g. camshafts, crankshafts, gears, valve stems, drive shafts, CV joints, piston pins, fuel injectors, turbochargers, and aerospace and land-based gas turbine parts such as vanes, blades, nozzles, and seals), surgical knives, blades, scissors, honing, superfinishing, cylinder liners, connecting rods, grinding of steel and paper mill rolls, and gears.\n\nPrior to the production of Borazon, diamond was the preferred abrasive used for grinding very hard superalloys but it could not be used effectively on steels because carbon tends to dissolve in iron at high temperatures. Aluminium oxide was the conventional abrasive used on hardened steel tools.\n\nBorazon replaced aluminium oxide for grinding hardened steels owing to its superior abrasive properties, comparable to that of diamond. Borazon is used in industrial applications to shape tools, as it can withstand temperatures greater than 2000 °C (3632 °F), much higher than that of a pure diamond at 871 °C (1600 °F). Other uses include jewellery designing, glass cutting and laceration of diamonds.\n\nCBN-coated grinding wheels, referred to as Borazon wheels, are routinely used in the machining of hard ferrous metals, cast irons, and nickel-base and cobalt-base superalloys. They can grind more material, to a higher degree of accuracy, than any other abrasive. The limiting factor in the life of such tools is typically determined not by wear on the cutting surface but by its break-down and separation from the metal core resulting from failure of the bonding layer.\n\n\n"}
{"id": "56319121", "url": "https://en.wikipedia.org/wiki?curid=56319121", "title": "CFR-600", "text": "CFR-600\n\nThe CFR-600 is a sodium-cooled pool-type fast-neutron nuclear reactor under construction in Xiapu County, Fujian province, China, on Changbiao Island.\nIt is a generation IV demonstration project by the China National Nuclear Corporation (CNNC).\nThe project is also known as Xiapu fast reactor pilot project.\nConstruction of the reactor started in late 2017.\nThe reactor will have an output of 1500 MWth thermal power and 600 MW electric power.\n\nThe CFR-600 is part of the Chinese plan to reach a closed nuclear fuel cycle.\nFast neutron reactors are considered the main technology in the future for nuclear power in China.\n\nA larger commercial-scale reactor, the CFR-1000, is also planned.\n\nOn the same site, a second 600 MW fast reactor is planned to be built in the future, a 600 MW HTR-PM600 and four 1000 MW CAP1000 are proposed.\n\n"}
{"id": "1538943", "url": "https://en.wikipedia.org/wiki?curid=1538943", "title": "Carnobacterium pleistocenium", "text": "Carnobacterium pleistocenium\n\nCarnobacterium pleistocenium is recently discovered bacterium from the arctic part of Alaska. It was found in permafrost, seemingly frozen there for 32,000 years. Melting the ice, however, brought these extremophiles back to life. This is the first case of an organism \"coming back to life\" from ancient ice. These bacterial cells were discovered in a tunnel dug by the Army Corps of Engineers in the 1960s to allow scientists to study the permafrost in preparation for the construction of the Trans-Alaska pipeline system.\n\nThe discovery of this bacterium is of particular interest for NASA, for it may be possible for such life to exist in the permafrost of Mars or on the surface of Europa. It is also of interest for scientists investigating the potential for cryogenically freezing life forms to reduce the transportation costs (in terms of life support systems) that would be associated with long-duration space travel.\n\n"}
{"id": "17977641", "url": "https://en.wikipedia.org/wiki?curid=17977641", "title": "Community wind energy", "text": "Community wind energy\n\nCommunity wind projects are locally owned by farmers, investors, businesses, schools, utilities, or other public or private entities who utilize wind energy to support and reduce energy costs to the local community. The key feature is that local community members have a significant, direct financial stake in the project beyond land lease payments and tax revenue. Projects may be used for on-site power or to generate wholesale power for sale, usually on a commercial-scale greater than 100 kW.\n\nThe Hepburn Wind Project is a wind farm at Leonards Hill near Daylesford, Victoria, north-west of Melbourne, Victoria. It comprises two 2MW wind turbines which produce enough power for 2,300 households.\n\nThis is the first Australian community-owned wind farm. The initiative has emerged because the community felt that the state and federal governments were not doing enough to address climate change.\n\nCommunity wind power is in its infancy in Canada but there are reasons for optimism. One such reason is the launch of a new Feed-in Tariff (FIT) program in the Province of Ontario . A number of community wind projects are in development in Ontario but the first project that is likely to obtain a FIT contract and connect to the grid is the Pukwis Community Wind Park. Pukwis will be unique in that it is a joint Aboriginal/Community wind project that will be majority-owned by the Chippewas of Georgina Island First Nation, with a local renewable energy co-operative (the Pukwis Energy Co-operative) owning the remainder of the project.\n\nIn Denmark, families were offered a tax exemption for generating their own electricity within their own or an adjoining commune. By 2001 over 100,000 families belonged to wind turbine cooperatives, which had installed 86% of all the wind turbines in Denmark, a world leader in wind power. Wind power has gained very high social acceptance in Denmark, with the development of community wind farms playing a major role.\n\nIn 1997, Samsø won a government competition to become a model renewable energy community. An offshore wind farm comprising 10 turbines (making a total of 21 altogether including land-based windmills), was completed, funded by the islanders. Now 100% of its electricity comes from wind power and 75% of its heat comes from solar power and biomass energy. An Energy Academy has opened in Ballen, with a visitor education center.\n\nIn Germany, hundreds of thousands of people have invested in citizens' wind farms across the country and thousands of small and medium-sized enterprises are running successful businesses in a new sector that in 2008 employed 90,000 people and generated 8 percent of Germany's electricity. Wind power has gained very high social acceptance in Germany, with the development of community wind farms playing a major role.\n\nIn the German district of North Frisia there are more than 60 wind farms with a capacity of about 700 MW, and 90 percent are community-owned. North Frisia is seen to be a model location for community wind, leading the way for other regions, especially in southern Germany.\n\nStarting in 2006, a village panchayat (local self-governing body) in Tamil Nadu state has become completely self-sufficient in energy by using renewable sources like wind, solar and biogas.\n\nThe Odanthurai village panchayat near Coimbatore city comprises 11 villages and has a population of about 8,000. By 2009, it had set up its own 350 kW windfarm to meet its energy needs. The windmill was set up at Malwadi near Udumalpet and generates about 8 lakh (800,000) units annually. The power requirement for Odanthurai stands at about 4.5 lakh (450,000) units, and the local panchayat body is now selling the surplus power to the state grid. This gives the panchayat an annual income of 19 lakh rupees.\n\nThe village cooperative is also using other sources of renewable energy. It has 65 solar streetlights in two hamlets and a nine-KW (kilowatt) biomass gasifier to pump drinking water from the river to the overhead tanks. Doing so, Odanthurai became the first local body in India to utilize the remunerative enterprises scheme of the state government.\n\nSixty-three farmers in \"De Zuidlob\", the southern part of the municipality of Zeewolde, have entered into a cooperative agreement that aims to develop a wind farm of at least 108 MW. The project will include the installation of three phases of 12 wind turbines with capacities of 3 to 4.5 MW each. The aim is to put the wind farm into service in 2012.\n\nThe Netherlands has an active community of wind cooperatives. They build and operate wind parks in all regions of the Netherlands. This started in the 1980s with the first Lagerweij turbines. Back then, these turbines could be financed by the members of the cooperatives.\nToday, the cooperatives build larger wind parks, but not as large as commercial parties do. Some still operate self-sufficiently, others partner with larger commercial wind park developers.\n\nBecause of the very unproductive state policies for financing wind parks in the Netherlands, the cooperatives have developed a new financing model, where members of a cooperative do not have to pay taxes for the electricity they generate with their community wind park.\nIn this construction the Zelfleveringsmodel the cooperative operates the wind park, and a traditional energy company only acts as a service provider, for billing and energy balance on the public grid.\nThis is the new role for energy companies in the future, where production is largely decentralized.\n\nIn 2012 a new company launched a new business model for community energy, Windcentrale. The wind turbine is sold in physical shares to families. Every share does not give financial gains, but real power, 500 kWh per year, average. A power company, part of the model, subtracts the generated amount of power, from the yearly power bill. Owners only have to pay for the power they used in excess of the amount their share generated. The Windcentrale started with 2 existing turbines that were sold to about 3 months. 8 months later they sold a turbine in a single evening. By the end of 2016 they were a community of about 17.000 members with 10 turbines and about 15 MW rated power. Every turbine is owned by a separate cooperative, with the Windcentrale doing all organizational work in the cooperative. In three years they grew to the same size, in members, than older wind cooperatives with average age of 25 years. Too of these older wind cooperatives, DeltaWind and Zeeuwind are run as a business and are building a 100 MW wind farm Krammer\n\nAs of 2012, there are 43 communities who are in the process of or already producing renewable energy through co-operative structures in the UK. They are set up and run by everyday people, mostly local residents, who are investing their time and money and together installing large wind turbines, solar panels, or hydro-electric power for their local communities.\n\nBaywind Energy Co-operative was the first co-operative to own wind turbines in the United Kingdom. Baywind was modeled on the similar wind turbine cooperatives and other renewable energy co-operatives that are common in Scandinavia, and was founded as an industrial and provident society in 1996. It grew to exceed 1,300 members, each with one vote.\n\nA proportion of the profits is invested in local community environmental initiatives through the Baywind Energy Conservation Trust. As of 2006, Baywind owns a 2.5 megawatt five-turbine wind farm at Harlock Hill near Ulverston, Cumbria (operational since 29 January 1997), and one of the 600 kilowatt turbines at the Haverigg II wind farm near Millom, Cumbria.\n\nCommunity-owned schemes in Scotland include one on the Isle of Gigha. The Heritage Trust set up Gigha Renewable Energy to buy and operate three Vestas V27 wind turbines, known locally as \"The Dancing Ladies\" or \"Creideas, Dòchas is Carthannas\" (Gaelic for \"Faith, Hope and Charity\"). They were commissioned on 21 January 2005 and are capable of generating up to 675 kW of power. Revenue is produced by selling the electricity to the grid via an intermediary called Green Energy UK. Gigha residents control the whole project and profits are reinvested in the community.\n\nAnother community-owned wind farm, Westmill Wind Farm Cooperative, opened in May 2008 in the Oxfordshire village of Watchfield. It consists of five 1.3 megawatt turbines, and is described by its promoters as the UK's largest community-owned wind farm. It was structured as a cooperative, whose shares and loan stock were sold to the local community. Other businesses, such as Midcounties Co-operative, also invested, and the Co-operative Bank provided a loan.\n\nCommunity Energy Scotland is an independent Scottish charity established in 2008 that provides advice and financial support for renewable energy projects developed by community groups in Scotland. The stated aim of Community Energy Scotland is 'to build confidence, resilience and wealth at community level in Scotland through sustainable energy development'.\n\nFindhorn Ecovillage has four Vestas wind turbines which can generate up to 750 kW. These make the community net exporters of renewable-generated electricity. Most of the generation is used on-site with any surplus exported to the National Grid.\n\nBoyndie Wind Farm Co-operative is part of the Energy4All group, which promotes community ownership. A number of other schemes supported by Highlands and Islands Community Energy Company are in the pipeline.\n\nCommunity Renewable Energy (CoRE) have worked with Berwick Community Development Trust who agreed the installation of a 500 kW Enercon turbine near the A1. The Trust now has income of £60,000 a year (increasing) after the turbine was installed in 2014. CoRE supported Oakenshaw Community Association setting up a 500 kW wind turbine near Durham. The turbine begun operating in 2014 and the Association now receives substantial yearly income.\n\nUnity Wind Ltd is an industrial and provident society that intends to install two 2MW wind turbines at North Walsham in North Norfolk. Its key aim is community wind turbines and run by community investment and for financial benefit to the community.\n\nIn 2009, the National Renewable Energy Laboratory published a report that identified three different types of community wind projects in the United States. The first model describes a project owned by a municipal utility, such as the Hull Wind Project in Massachusetts. The second model is a wind project that is jointly owned by local community members, such as the MinWind Projects near Luverne, Minnesota. The third type is a flip-style ownership. This model allows local investors to partner with a corporation in order to take advantage of Production Tax Credit federal incentives. Flip projects have been built in Minnesota and Texas.\n\nIn a community-based model, the developer/manager of a wind farm shares ownership of the project with area landowners and other community members. Property owners whose land was used for the wind farm are generally given a choice between a monthly cash lease and ownership units in the development.\n\nA wind turbine cooperative, also known as a wind energy cooperative, is a jointly owned and democratically controlled enterprise that follows the cooperative model, investing in wind turbines or wind farms. The cooperative model was developed in Denmark. The model has also spread to Germany, the Netherlands, Australia and United Kingdom, with isolated examples elsewhere.\n\nSome places have enacted policies to encourage development of municipally owned and operated wind turbines on town land. These projects are publicly owned and tax exempt. An example is the Hull Wind One project in Massachusetts' Boston Harbor in 2001. A 660 kW wind turbine was installed, and is still a great example of small scale commercial wind.\n\nOnce a wind farm project is established in a community, jobs are needed for: manufacturing the materials needed to build the project, transportation of supplies to the project area, and construction of the project as well as building roads leading to the project. After the project is complete, jobs will be needed to maintain and operate the facility. According to a study by the New York State Energy Research and Development Authority, wind energy produces 27% more jobs per kilowatt-hour than coal plants and 66% more jobs than natural gas plants. 3. Landowners will also collect revenues for hosting turbines on their property. Given a typical wind turbine spacing requirements, a 250-acre farm could increase annual farm income by $14,000 per year with little effect on their normal farming and ranching operations. 4. Community wind energy projects increase local property tax revenue because there was very little to be taxed previously due to the sparse population and vast farm land. Once the wind turbines are in service they are taxed, creating much needed revenue for the local community.\n\nThe Midwest and the Great Plains regions in the United States are ideal areas for community wind energy projects; they are also often prone to drought. Fossil fuel plants use large amounts of water for cooling purposes which is detrimental to communities' water supply if there is a drought. Wind turbines do not use any water since there is no considerable amount of heat produced during energy generation. Wind energy adds power to the electric grid which decreases the amount of oil needed to generate a community's electricity. Local land owners, who produce the wind energy, can also control the amount of energy produced, which expands the regional energy mix. Overall community wind energy reduces the local community's dependence on oil but, because of the subsidies involved, can greatly increase their costs for electricity.[citation needed]\n\nCompared to the environmental impact of traditional energy sources, the environmental impact of wind power is relatively minor. Wind power consumes no fuel, and emits no air pollution, unlike fossil fuel power sources. The energy consumed to manufacture and transport the materials used to build a wind power plant is equal to the new energy produced by the plant within a few months. While a wind farm may cover a large area of land, many land uses such as agriculture are compatible, with only small areas of turbine foundations and infrastructure made unavailable for use.\n\nThere are reports of bird and bat mortality at wind turbines as there are around other artificial structures. The scale of the ecological impact may or may not be significant, depending on specific circumstances. Prevention and mitigation of wildlife fatalities, and protection of peat bogs, affect the siting and operation of wind turbines.\n\nThere are anecdotal reports of negative effects from noise on people who live very close to wind turbines. Peer-reviewed research has generally not supported these statements.\n\nIn 1992, the renewable energy production tax credit of 2.1 cents per kilowatt-hour was established. In February 2009, through the American Recovery and Reinvestment Act, Congress acted to provide a three-year extension of the PTC through December 31, 2012.\nWind projects that were up and running in 2009 and 2010 can choose to receive a 30% investment tax credit instead of the PTC. The investment tax credit is also an option for wind projects that are in service before 2013 if the final construction is complete before the end of 2010. Smaller wind farms (100 kW or less) can receive a credit for 30% towards the cost of installment of the system. The ITC, written into law through the Emergency Economic Stabilization Act of 2008, is available for equipment installed from October 3, 2008 through December 31, 2016. The value of the credit is now uncapped, through the American Recovery and Reinvestment Act of 2009.\n\nIn order to ensure wind energy's future in the energy market, the renewable electricity standard (RES) is a policy in which market mechanisms guarantee a growing percentage of electricity produced comes from renewable sources, like wind energy. The RES exists in 28 states (not at a national level). An example is the Obama-Biden New Energy for America plan, which sets future goals of rapid renewable energy production at 10% by 2012.\n\nA pressing issue of concern is the lack of a modern interstate transmission grid which delivers carbon free electricity to customers. Currently the US Senate and the Natural Resources Committee have reported the bill out of committee on June 17, 2009. A combined energy and climate bill is expected to be considered by the full Senate this fall. In the US House of Representatives the House Energy and Commerce Committee approved a comprehensive energy and climate bill on May 21, 2010.\nThe clean air and climate change policy is goal to switch from fossil fuel energy sources to renewable carbon-free energy sources for electricity production. Generating 20% of U.S. electricity from wind would be the climate equivalent of removing 140 million vehicles from the roadways. Currently the US Senate Committee on Environmental and Public Works has control over the legislation and will begin to complete a markup by September 25, 2009. The House of Representatives passed the American Clean Energy and Security Act on June 26, 2009, comprising a provision to reduce carbon dioxide emissions 17% below 2005 levels by 2020 and 83% below 2005 levels by 2050. It also allocates a portion of the allowances given away for free to energy efficiency and renewable energy. However, the allowances flow through state governments rather than directly to renewable generators.\n\nOverall federal funding for community wind research and development is insufficient and even more so when compared to other fuels and energy sources. In 2009 the US Department of Energy (DOE) received $118 million from the American Recovery and Reinvestment Act for wind energy research and development. In 2010 the Senate passed a bill granting the DOE $85 million for the DOE wind program. For the same purpose, the House of Representatives allowed the DOE $70 million.\n\n\n"}
{"id": "826277", "url": "https://en.wikipedia.org/wiki?curid=826277", "title": "Ericsson cycle", "text": "Ericsson cycle\n\nThe Ericsson cycle is named after inventor John Ericsson who designed and built many unique heat engines based on various thermodynamic cycles. He is credited with inventing two unique heat engine cycles and developing practical engines based on these cycles. His \"first\" cycle is now known as the closed Brayton cycle, while his second cycle is what is now called the Ericsson cycle.\nEricsson is one of the few who built open-cycle engines, but he also built closed-cycle ones.\n\nThe following is a list of the four processes that occur between the four stages of the ideal Ericsson cycle:\n\nThe ideal Otto and Diesel cycles are not totally reversible because they involve heat transfer through a finite temperature difference during the irreversible isochoric/isobaric heat-addition and isochoric heat-rejection processes. The aforementioned irreversibility renders the thermal efficiency of these cycles less than that of a Carnot engine operating within the same limits of temperature. Another cycle that features isobaric heat-addition and heat-rejection processes is the Ericsson cycle. The Ericsson cycle is an altered version of the Carnot cycle in which the two isentropic processes featured in the Carnot cycle are replaced by two constant-pressure regeneration processes.\n\nThe Ericsson cycle is often compared with the Stirling cycle, since the engine designs based on these respective cycles are both external combustion engines with regenerators. The Ericsson is perhaps most similar to the so-called \"double-acting\" type of Stirling engine, in which the displacer piston also acts as the power piston. Theoretically, both of these cycles have so called \"ideal\" efficiency, which is the highest allowed by the second law of thermodynamics. The most well-known ideal cycle is the Carnot cycle, although a useful \"Carnot engine\" is not known to have been invented.\nThe theoretical efficiencies for both, Ericsson and Stirling cycles acting in the same limits are equal to the Carnot Efficiency for same limits.\n\nThe first cycle Ericsson developed is now called the \"Brayton cycle\", commonly applied to the rotary jet engines for airplanes.\n\nThe second Ericsson cycle is the cycle most commonly referred to as simply the \"Ericsson cycle\". The (second) Ericsson cycle is also the limit of an ideal gas-turbine Brayton cycle, operating with multistage intercooled compression, and multistage expansion with reheat and regeneration. Compared to the Brayton cycle which uses adiabatic compression and expansion, the second Ericsson cycle uses isothermal compression and expansion, thus producing more net work per stroke. Also the use of regeneration in the Ericsson cycle increases efficiency by reducing the required heat input. For further comparisons of thermodynamic cycles, see heat engine.\n\nThe Ericsson engine is based on the Ericsson cycle, and is known as an \"external combustion engine\", because it is externally heated. To improve efficiency, the engine has a regenerator or recuperator between the compressor and the expander. The engine can be run open- or closed-cycle. Expansion occurs simultaneously with compression, on opposite sides of the piston.\n\nEricsson coined the term \"regenerator\" for his independent invention of the mixed-flow counter-current heat exchanger. However, Rev. Robert Stirling had invented the same device, prior to Ericsson, so the invention is credited to Stirling. Stirling called it an \"economiser\" or \"economizer\", because it increased the fuel economy of various types of heat processes. The invention was found to be useful, in many other devices and systems, where it became more widely used, since other types of engines became favored over the Stirling engine. The term \"regenerator\" is now the name given to the component in the Stirling engine.\n\nThe term \"recuperator\" refers to a separated-flow, counter-current heat exchanger. As if this weren't confusing enough, a mixed-flow regenerator is sometimes used as a quasi-separated-flow recuperator. This can be done through the use of moving valves, or by a rotating regenerates with fixed baffles, or by the use of other moving parts. When heat is recovered from exhaust gases and used to preheat combustion air, typically the term recuperator is used, because the two flows are separate.\n\nIn 1791, before Ericsson, John Barber proposed a similar engine. The Barber engine used a bellows compressor and a turbine expander, but it lacked a regenerator/recuperator. There are no records of a working Barber engine. Ericsson invented and patented his first engine using an external version of the Brayton cycle in 1833 (number 6409/1833 British). This was 18 years before Joule and 43 years before Brayton. Brayton engines were all piston engines and for the most part, internal combustion versions of the un-recuperated Ericsson engine. The \"Brayton cycle\" is now known as the gas turbine cycle, which differs from the original \"Brayton cycle\" in the use of a turbine compressor and expander. The gas turbine cycle is used for all modern gas turbine and turbojet engines, however simple cycle turbines are often recuperated to improve efficiency and these recuperated turbines more closely resemble Ericsson's work.\n\nEricsson eventually abandoned the open cycle in favor of the traditional closed Stirling cycle.\n\nEricsson's engine can easily be modified to operate in a closed-cycle mode, using a second, lower-pressure, cooled container between the original exhaust and intake. In closed cycle, the lower pressure can be significantly above ambient pressure, and He or H working gas can be used. Because of the higher pressure difference between the upward and downward movement of the work-piston, specific output can be greater than of a valveless Stirling engine. The added cost is the valve. Ericsson's engine also minimizes mechanical losses: the power necessary for compression does not go through crank-bearing frictional losses, but is applied directly from the expansion force. The piston-type Ericsson engine can potentially be the highest efficiency heat engine arrangement ever constructed. Admittedly, this has yet to be proven in practical applications.\n\nEricsson designed and built a very great number of engines running on various cycles including steam, Stirling, Brayton, externally heated diesel air fluid cycle. He ran his engines on a great variety of fuels including coal and solar heat.\n\nEricsson was also responsible for an early use of the screw propeller for ship propulsion, in the USS \"Princeton\", built in 1842–43.\n\nIn 1851 the Ericsson-cycle engine (the second of the two discussed here) was used to power a 2,000-ton ship, the caloric ship \"Ericsson\", and ran flawlessly for 73 hours. The combination engine produced about . It had a combination of four dual-piston engines; the larger expansion piston/cylinder, at in diameter, was perhaps the largest piston ever built. Rumor has it that tables were placed on top of those pistons (obviously in the cool compression chamber, not the hot power chamber) and dinner was served and eaten, while the engine was running at full power. At 6.5 RPM the pressure was limited to . According to the official report it only consumed 4200 kg coal per 24 hours (original target was 8000 kg, which is still better than contemporary steam engines). The one sea trial proved that even though the engine ran well, the ship was underpowered. Some time after the trials, the \"Ericsson\" sank. When it was raised, the Ericsson-cycle engine was removed and a steam engine took its place. The ship was wrecked when blown aground in November 1892 at the entrance to Barkley Sound, British Columbia, Canada.\n\nThe Ericsson cycle (and the similar Brayton cycle) receives renewed interest today to extract power from the exhaust heat of gas (and producer gas) engines and solar concentrators. An important advantage of the Ericsson cycle over the widely known Stirling engine is often not recognized : the volume of the heat exchanger does not adversely affect the efficiency.\n\n(...)\"despite having significant advantages over the Stirling. Amongst them, it is worth to note that the Ericsson engine heat exchangers are not dead volumes, whereas the Stirling engine heat exchangers designer has to face a difficult compromise between as large heat transfer areas as possible, but as small heat exchanger volumes as possible.\"\n\nFor medium and large engines the cost of valves can be small compared to this advantage. Turbocompressor plus turbine implementations seem favorable in the MWe range, positive displacement compressor plus turbine for Nx100 kWe power, and positive displacement compressor+expander below 100 kW. With high temperature hydraulic fluid, both the compressor and the expander can be liquid ring pump even up to 400 °C, with rotating casing for best efficiency.\n\n\n"}
{"id": "31639799", "url": "https://en.wikipedia.org/wiki?curid=31639799", "title": "Europium barium titanate", "text": "Europium barium titanate\n\nEuropium barium titanate is a chemical compound composed of barium, europium, titanium, and oxygen. It is magnetic and ferroelectric.\n\nIt is a ceramic material which was used in 2010 experiments on a new theory on baryon asymmetry.\n"}
{"id": "31333237", "url": "https://en.wikipedia.org/wiki?curid=31333237", "title": "Feed-in tariffs in the United Kingdom", "text": "Feed-in tariffs in the United Kingdom\n\nA Feed-in tariff is when payments are given by energy suppliers if a property or organisation generates their own electricity using technology such as solar panels or wind turbines.\n\nFeed-in tariffs in the United Kingdom were announced in October 2008 and took effect from April 2010. They were entered into law by the Energy Act of 2008.\n\nThe Feed-In Tariff applies to small-scale generation of electricity using eligible renewable technologies. To encourage development of these technologies, feed-in tariffs pay the generator a certain amount even for energy which the generator themselves consumes. Electricity fed into the grid receives an additional export tariff, currently (June 2013) 4.5p per KWh. Costs for the programme are borne by all British electricity consumers proportionally: all consumers will bear a slight increase in their annual bill, thus allowing electricity utilities to pay the FIT for renewable electricity generated at the rates set by the government. Payments through the mechanism are intended to replace the ROCs available through the Renewables Obligation for small-scale renewable energy generators and is based on a few key elements:\n\n\nThe government estimated that feed-in tariffs to support small-scale low-carbon generation would cost £8.6 billion up to 2030 and produce monetised carbon savings worth £0.42 billion.\n\nFeed-in-Tariff payments are Tax-free in the UK.\n\nA new study from the University of London has assessed the first year of the UK FIT scheme through interviews with both users of the scheme and government figures. The key findings are that users have had a wide variety of experiences, depending on the technology they are working with, and that the government has very limited ambitions on small-scale renewable energy generation.\n\nDomestic solar has performed well in the first year, with 28,028 of the 28,614 total solar installations (totalling nearly 78MW). Wind power has the next highest installation level with 1,348 (20.4MW). Small hydro had 206 (12.1MW), although many were not new installations, but had been transferred from the Renewable Obligation scheme. Micro-CHP had 98 installations (0.09MW), and Anaerobic Digestion (AD) had just 2 (0.66MW). AD is under scrutiny at present (April 2011) to determine why development has been so poor.\n\nThe study suggests that technologies have a variety of factors affecting their performance in terms of installation levels. The factors include cost, size, availability, standardisation of the technology, planning issues, ease of installation, perceived sensory impact (sight, sound and smell) and administrative complexity. Domestic PV scores very positively on all these factors, while small hydro and AD do far less well.\n\nThe proposed changes to the tariff levels for PV have been met with anger by many in the solar industry, but the FIT policy, along with the Green Investment Bank and now carbon reduction targets, are widely understood to be threatened by the Treasury department. This is due to the schemes being considered as liabilities on the national balance sheet.\n\nLess than a year into the scheme, in March 2011 the new coalition Government announced that support for large-scale photovoltaic installations (greater than 50 kW) would be cut. From 1 August 2011 the rate for installations over 50 kW was to range from 19p/kWh to 8.5p/kWh for the largest qualifying installations (5MW), with the Government claiming that this would prevent the scheme from becoming 'overwhelmed'.\n\nRevised tariffs for farm-scale anaerobic digestion initially of either 14p/kWh or 13p/kWh, depending on the installation size, were introduced from September 2011.\n\nOn 31 October 2011 a second review of the Feed in Tariffs for low carbon electricity generation was announced which is likely to take effect from 12 December 2011. The rates for small photovoltaic installations have been reduced from 43.3p/kWh to 21 pence/kWh. The reason for the second review is that FITs for PV were being taken up too quickly and that the DECC funding allocation for FITs was in danger of being exceeded. A further reason is that the cost of installing PV panels has reduced by around 50% and therefore the FITs had become less of an encouragement to install PV panels and more of an incitement to profit from excessive subsidies. See revised tariff tables for FITs.\n\nIn its second year, the government announced further cuts to the FIT scheme. On 3 March the tariff was cut to 21p/kWh. This cut was originally scheduled for 12 December 2011 but was delayed, following a successful joint appeal to the High Court by Friends of the Earth and two solar companies, Solar Century and HomeSun. The 1 August review of the FIT brought an additional cut to 16p/kWh. The cut was partnered with a rise in export rate (the price at which the homeowner can sell excess electricity back to the supplier) from 3.1p to 4.5p for every kWh of electricity exported to the grid. The latest cut came into effect on 1 November, the tariff dropping to 15.44p/kWh, and this rate is set to remain until 1 February 2013. In addition to this, generators with more than 25 Solar PV installations were granted a 10% increase in the amount they receive of the FIT, from 80% to 90%, this however will not be likely to affect domestic users. The cut in FITs was due to the falling installation costs, and the fact that people were applying for the feed-in tariff scheme in numbers exceeding DECC forecasts and funding allocations. The aforementioned rates would only affect new installations - existing schemes would not be affected . The new tariffs would also now be paid over 20 years instead of 25 years (It will remain linked to the Retail Price Index) with a review every three months based on solar PV uptake levels in the three different bands: domestic (size 0-10 kW), small commercial (10-50 kW) and large commercial (above 50 kW and standalone installations). Despite suggestions that the European solar market is in decline, a report by the International Energy Agency has shown that for a second year in a row solar PV was the dominant form of new electricity installation during 2012, ahead of both wind and gas power.\n\nThe Department of Business Energy and Industrial Strategy (BEIS) published a consultation on 19 July 2018. In this, they state their intention to close the FIT scheme to new applicants from 1 April 2019 and will not be replaced by a new subsidy.\n\nIn addition to the feed-in tariff, a similar incentive - the Renewable Heat Incentive for renewable heat was introduced in November 2011.\n\nThe scheme has created a number of start-up companies providing free electricity in return for installing solar panels on the homeowner's roof. If the homeowner can not afford the capital outlay, the free solar companies offer a capital-free way of getting the benefits of solar and free electricity. However, if the homeowner can afford the capital outlay it is more cost effective to purchase the solar equipment directly.\n\nAfter the December 2015 Feed-in Tariff reductions were announced, many free solar panel installers have started to cease trading, or have plans to stop installing, as the returns are no longer financially viable. The change in the feed-in tariff will equate to a 64% decrease in the generation tariff for solar arrays less than 4 kW, which is the largest decrease since the scheme was started in 2010. Unusually, the feed-in tariff changes will mean that larger systems (over 10 kW) will receive higher feed in tariff rate than smaller domestic sized systems, which could result in a strong switch of the remaining free solar panel companies to exclusively providing commercial installations.\n\nThere has been much criticism of the tariffs as being inconsistent. Those people who installed equipment before the FIT started, are given a fixed tariff of 9p per unit (9.9p from April 2012 ). Both the Conservatives and the Liberal Democrats pledged to reverse this in the 2010 General Election, but then reneged on this decision afterwards. The pioneers are now on a lower rate than that before FITs started, despite generating the same electricity.\n\nThere has also been criticism of the high rates for FITs compared to those for the Renewable Heat Incentive: the renewable heat technologies are more economic than renewable electricity generation so the government could save carbon much less expensively by increasing the RHI on ground source heat pumps (originally at 4.5 pence/kWh, but raised to 8.7 pence/kWh in 2014) compared to the tariffs for photovoltaic (originally up to 41.3 pence/kWh, but reduced to 14 by 2014) or the tariffs for wind turbines (originally up to 34.5 pence/kWh, but reduced to 18 by 2014).\n\n"}
{"id": "8148752", "url": "https://en.wikipedia.org/wiki?curid=8148752", "title": "Gorlov helical turbine", "text": "Gorlov helical turbine\n\nThe Gorlov helical turbine (GHT) is a water turbine evolved from the Darrieus turbine design by altering it to have helical blades/foils. It was patented in a series of patents from September 19, 1995 to July 3, 2001 and won 2001 ASME Thomas A. Edison Patent Award. GHT was invented by Professor Alexander M. Gorlov of Northeastern University.\nThe physical principles of the GHT work are the same as for its main prototype, the Darrieus turbine, and for the family of similar Vertical axis wind turbines which includes also Turby wind turbine, aerotecture turbine, Quietrevolution wind turbine, ... GHT, turby and quietrevolution solved pulsatory torque issues by using the helical twist of the blades.\n\nThe term \"foil\" is used to describe the shape of the blade cross-section at a given point, with no distinction for the type of fluid, (thus referring to either an \"airfoil\" or \"hydrofoil\"). In the helical design, the blades curve around the axis, which has the effect of evenly distributing the foil sections throughout the rotation cycle, so there is always a foil section at every possible angle of attack. In this way, the sum of the lift and drag forces on each blade do not change abruptly with rotation angle. The turbine generates a smoother torque curve, so there is much less vibration and noise than in the Darrieus design. It also minimizes peak stresses in the structure and materials, and facilitates self-starting of the turbine. In testing environments the GHT has been observed to have up to 35% efficiency in energy capture reported by several groups. \"Among the other vertical-axis turbine systems, the Davis Hydro turbine, the EnCurrent turbine, and the Gorlov Helical turbine have all undergone scale testing at laboratory or sea. Overall, these technologies represent the current norm of tidal current development.\"\n\nThe main difference between the Gorlov helical turbine and conventional turbines is the orientation of the axis in relation to current flow. The GHT is a vertical-axis turbine which means the axis is positioned perpendicular to current flow, whereas traditional turbines are horizontal-axis turbines which means the axis is positioned parallel to the flow of the current. Fluid flows, such as wind, will naturally change direction, however they will still remain parallel to the ground. So in all vertical-axis turbines, the flow remains perpendicular to the axis, regardless of the flow direction, and the turbines always rotate in the same direction. This is one of the main advantages of vertical-axis turbines.\n\nIf the direction of the water flow is fixed, then the Gorlov turbine axis could be vertical or horizontal, the only requirement is orthogonality to the flow.\n\nThe GHT operates under a lift-based concept (see airfoil). The foil sections on the GHT are symmetrical, both top-to-bottom and also from the leading-to-trailing edge. The GHT can actually spin equally well in either direction. The GHT works under the same principle as the Darrieus turbine; that is, it relies upon the movement of the foils in order to change the apparent direction of the flow relative to the foils, and thus change the (apparent) \"angle of attack\" of the foil.\n\nA GHT is proposed for low-head micro hydro installations, when construction of a dam is undesirable. The GHT is an example of damless hydro technology. The technology may potentially offer cost and environmental benefits over dam-based micro-hydro systems.\n\nSome advantages of damless hydro are that it eliminates the potential for failure of a dam, which improves public safety. It also eliminates the initial cost of dam engineering, construction and maintenance, reduces the environmental and ecological complications, and potentially simplifies the regulatory issues put into law specifically to mitigate the problems with dams.\n\nIn general, a major ecological issue with hydropower installations is their actual and perceived risk to aquatic life. It is claimed that a GHT spins slowly enough that fish can see it soon enough to swim around it. From preliminary tests in 2001, it was claimed that if a fish swims between the slowly moving turbine blades, the fish will not be harmed. Also it would be difficult for a fish to become lodged or stuck in the turbine, because the open spaces between the blades are larger than even the largest fish living in a small river. A fish also would not be tumbled around in a vortex, because the GHT does not create a lot of turbulence, so small objects would be harmlessly swept through with the current.\n\nIn this example the direction of the fluid flow is to the left. \nAs the turbine rotates, in this case in a clockwise direction, the motion of the foil through the fluid changes the apparent velocity and angle of attack (speed and direction) of the fluid with respect to the frame of reference of the foil. The combined effect of these two flow components (i.e. the vector sum), yields the net total \"Apparent flow velocity\" as shown in the next figure.\n\nThe action of this apparent flow on each foil section generates both a lift and drag force, the sum of which is shown in the figure above titled \"Net force vectors\". Each of these net force vectors can be split into two orthogonal vectors: a radial component and a tangential component, shown here as \"Normal force\" and \"Axial force\" respectively. The normal forces are opposed by the rigidity of the turbine structure and do not impart any rotational force or energy to the turbine. The remaining force component propels the turbine in the clockwise direction, and it is from this torque that energy can be harvested.\n\n[With regards to the figure above left \"Apparent flow velocity...\", Lucid Energy Technologies, rights holder to the patent to the Gorlov Helical Turbine, notes that this diagram, with no apparent velocity at an azimuth angle of 180 degrees (blade at its point in rotation where it is instantaneously moving in downstream direction), may be subject to misinterpretation. This is because a zero apparently flow velocity could occur only at a tip speed ratio of unity (i.e. TSR=1, where the current flow induced by rotation equals the current flow). The GHT generally operates at a TSR substantially greater than unity.]\n\n(The diagrams \"Net Force Vectors\" and \"Normal Force Vectors\" are partially incorrect. The downwind segments should show the vectors outside the circles. Otherwise there would be no net sideways loading on the turbine.) M Koester 2015.\n\nHelical turbines in water stream generate mechanical power independent on direction of the water flow. Then electric generators assembled upon the common shaft transfer the power into electricity for the commercial use.\n\nChain of Horizontal Gorlov Turbines. TideGen by Ocean Renewable Power Company – possibility for shallow waters.\nTidal Power Station with Gorlov Helical Turbines before deployment in the ocean.\nUSA, Cobscook Bay, Maine, September, 2012.\n\nGorlov Helical Turbines in South Korea, 1997-1998. Installation in shallow waters.\n\n\n"}
{"id": "38044446", "url": "https://en.wikipedia.org/wiki?curid=38044446", "title": "Great Plains Conservation", "text": "Great Plains Conservation\n\nGreat Plains Conservation is a South Africa-based conservation organization that manages several wildlife reserves in Kenya and Botswana. The group currently operates six reserves, which include luxury lodges and tent camps. It works together with local governments and community groups to promote low-density, environmentally conscious tourism, supplying economic incentives for the protection of wildlife.\n\nGreat Plains Conservation was founded in 2006 by a group of conservationists and filmmakers. Its CEO is Dereck Joubert.\n"}
{"id": "18714208", "url": "https://en.wikipedia.org/wiki?curid=18714208", "title": "Grozăvești Power Station", "text": "Grozăvești Power Station\n\nThe Grozăveşti Power Station is a large thermal power plant located in Bucharest, having 2 generation groups of 50 MW each having a total electricity generation capacity of 100 MW.\n\n"}
{"id": "45382697", "url": "https://en.wikipedia.org/wiki?curid=45382697", "title": "HVDC DolWin3", "text": "HVDC DolWin3\n\nHVDC DolWin3 is a high voltage direct current (HVDC) link under construction to transmit Offshore wind power to the power grid of the German mainland. The project differs from most HVDC systems in that one of the two converter stations is built on a platform in the sea. Voltage-Sourced Converters with DC ratings of 900 MW, ±320 kV are used and the total cable length is 160 km.\n\nThe overall project is being built by Alstom with the cables supplied by Prysmian and offshore platform by Nordic Yards. Construction commenced on the onshore converter station at Dörpen West in May 2014 and on the offshore platform in October 2014. As of summer 2015 the platform was being fitted out and the first converter transformer was installed on 17 July 2015. The project is expected to be handed over to its owner, TenneT, in 2017.\n\n\n"}
{"id": "39416534", "url": "https://en.wikipedia.org/wiki?curid=39416534", "title": "Hsieh-ho Power Plant", "text": "Hsieh-ho Power Plant\n\nThe Hsieh-ho Power Plant () is an oil-fired power plant in Zhongshan District, Keelung, Taiwan. The power plant is the only fully oil-fired power plant in Taiwan.\n\nThe power plant consists of four 500-MW generation units. The third 500-MW unit was finished on 19 December 1979 after a record-breaking construction period of 26 months. It went into operation in March 1980. The fourth 500-MW unit was completed in 1985 after 29 months construction period.\n\nThe steam generator is rated at 1,701 tonne/hour, 176 kg/cm and 542 °C at superheater outlet and reheat to 542 °C.\n\nThe steam turbine is a tandem-compound with four flow exhaust, 3,600 rpm single reheat with throttle steam conditions of 166 kg/cm, 538 °C with reheat to 538 °C.\n\nThe stacks are 200 meters high slip-form reinforced concrete stack.\n\nHsieh-ho Power Plant is accessible North from TRA Keelung Station.\n\n"}
{"id": "5652595", "url": "https://en.wikipedia.org/wiki?curid=5652595", "title": "Institute of Chartered Foresters", "text": "Institute of Chartered Foresters\n\nThe Institute of Chartered Foresters is the professional body for foresters and arboriculturists in the United Kingdom. Its Royal Charter was granted in 1982. The Institute grants Chartered status to individuals following an examination process that includes a period of management or supervisory experience. Members are recognised by their titles 'Chartered Arboriculturist' or 'Chartered Forester' and by the letters MICFor(Member of the Institute of Chartered Foresters). Fellows of the institute bear the title FICFor.\n\nMembers of the Institute of Chartered Foresters are required to undertake Continuous Professional Development (CPD) and are bound by a Code of Ethics and Professional Conduct. ICF's membership is currently over 1,400.\n\nICF is also involved in raising awareness of issues facing forestry sector professionals and providing guidance to professionals from other sectors, and the public, in relation to these issues.\n\nThere are 5 membership grades at the Institute of Chartered Foresters, as follows\n\n\nThe ICF National conference takes place annually, attracting both members and non-members. In 2011, and again in 2014 the Institute hosted the Trees, People and the Built Environment (TPBE) event as their National Conference. TPBE is a partnership event of some 20 UK forestry, arboriculture and built environment organisations, which presents urban trees research from around the world.\n\n"}
{"id": "21106847", "url": "https://en.wikipedia.org/wiki?curid=21106847", "title": "Integrated operations", "text": "Integrated operations\n\nIn the Petroleum industry, Integrated operations (IO) refers to the integration of people, disciplines, organizations, work processes and information and communication technology to make smarter decisions. In short, IO is collaboration with focus on production.\n\nThe most striking part of IO has been the use of always-on videoconference rooms between offshore platforms and land-based offices. This includes broadband connections for sharing of data and video-surveillance of the platform. This has made it possible to move some personnel onshore and use the existing human resources more efficiently. Instead of having e.g. an expert in geology on duty at every platform, the expert may be stationed on land and be available for consultation for several offshore platforms. \nIt's also possible for a team at an office in a different time zone to be consulting the night-shift of the platform, so that no land-based workers need work at night.\n\nSplitting the team between land and sea demands new work processes, which together with ICT is the two main focus points for IO. Tools like videoconferencing and 3D-visualization also creates an opportunity for new, more cross-discipline cooperations. For instance, a shared 3D-visualization may be tailored to each member of the group, so that the geologist gets a visualization of the geological structures while the drilling engineer focuses on visualizing the well. Here, real-time measurements from the well are important but the downhole bandwidth has previously been very restricted. Improvements in bandwidth, better measurement devices, better aggregation and visualization of this information and improved models that simulate the rock formations and wellbore currently all feed on each other. An important task where all these improvements play together is real-time production optimization.\n\nIn the process industry in general, the term is used to describe the increased cooperation, independent of location, between operators, maintenance personnel, electricians, production management as well as business management and suppliers to provide a more streamlined plant operation.\n\nBy deploying IO, the petroleum industry draws on lessons from the process industry. This can be seen in a larger focus on the whole production chain and management ideas imported from the production and process industry. A prominent idea in this regard is real-time optimization of the whole value chain, from long term management of the oil reservoir, through capacity allocations in pipe networks and calculations of the net present value of the produced oil. Reviews of the application of Integrated Operations can be found in papers presented in the by-annual society of petroleum engineers Intelligent Energy conferences. \n\nA focus on the whole production chain is also seen in debates about how to organize people in an IO organisation, with frequent calls for breaking down the Information silos in the oil companies. A large oil company is typically organized in functional silos corresponding to disciplines such as drilling, production and reservoir management. This is regarded as inefficient by the IO movement, pointing out that the activities in any well or field by any of the silos will involve or affect all of the others. \nWhile some companies focus on their inhouse management structure, others also emphasize the integration and coordination of outside suppliers and collaborators in offshore-operations. For instance, it is pointed out that the oil and gas industry is lagging behind other industries in terms of Operational intelligence.\n\nIdeas and theories that IO management and work processes build on will be familiar from operations research, knowledge management and continual improvement as well as information systems and business transformation. This is perhaps most evident in the repeated referral to \"people, process and technology\" in IO discussions\n. As bullet-points this mirror many of the aforementioned fields.\n\nSince 2010 major mining companies have become implementers of Integrated Operations, most notable Rio Tinto, BHP Biliton and Codelco. \nCommon to most companies is that IO leads to cost savings as fewer people are stationed offshore and an increased efficiency. Lower costs, more efficient reservoir management and fewer mistakes during well drilling will in turn raise profits and make more oil fields economically viable. IO comes at a time when the oil industry is faced with more \"brown fields\", also referred to as \"tail production\", where the cost of extracting the oil will be higher than its market value, unless major improvements in technology and work processes are made.\nIt has been estimated that deployment of IO could produce 300 billion NOK of added value to the Norwegian continental shelf alone. On a longer time-scale, working onshore control and monitoring of the oil production may become a necessity as new fields at deeper waters are based purely on unmanned sub-sea facilities.\n\nMoving jobs onshore has also been touted as a way to keep and make better use of an aging workforce, which is regarded as a challenge by western oil and gas companies. As the average age of the industry workforce is increasing with many nearing retirement, IO is being leveraged for knowledge sharing and training of younger workforce. More comfortable onshore jobs together with \"high-tech\" tools has also been fronted as a way to recruit young workers into an industry that is seen as \"unsexy\", \"lowtech\" and difficult to combine with a normal family life.\n\nThe security aspect of reducing the offshore workforce has been raised. Will on-site experience be lost and can familiarity with the platform and its processes be attained from an onshore office? The new working environment in any case demands changes to HSE routines. Some of the challenges also include clear role and responsibility definitions and clarifications between the onshore & offshore personnel. Who in a given situation has the authority to take decisions, the onsite or the offshore staff. The increased integration of the offshore facilities with the onshore office environment and outside collaborators also expose work-critical ICT-infrastructure to the internet and the hazards of everyday ICT. \nAs for the efficiency aspect, some criticize the onshore-offshore collaboration for creating a more bureaucratic working environment.\n\nBoth the exact terms and the content used to describe IO vary between companies. The oil company Shell has traditionally branded the term Smart Fields, which was an extension of Smart Wells that only referred to remote-controlled well-valves. BP uses Field of the future to refer to its innovations in oil production. Chevron has i-field, Honeywell has Digital Suites for Oil and Gas (a set of software and services), and Schlumberger terms it Digital Energy. The latter term, understood as referring to oil and gas, is adopted in the title of the digital energy journal. This term could have several meanings, as GE Digital Energy for instance, do not appear to use it in the IO sense.\n\nOther terms include e-Field, i-Field, Digital Oilfield, Intelligent Oilfield, Field of the future and Intelligent Energy. Integrated operations has been the preferred term by Statoil, the Norwegian Oil Industry Association (OLF), a professional body and employer's association for oil and supplier companies and vendors such as ABB. IO is also the preferred term for Petrobras. Intelligent Energy is the dominant term in publications revolving around the biannual SPE Intelligent Energy conference, which has been one of the major conferences for the IO movement, along with the annual IO Science and Practice conference which obviously supports the IO term.\n\n\n"}
{"id": "2710750", "url": "https://en.wikipedia.org/wiki?curid=2710750", "title": "JASO M345", "text": "JASO M345\n\nJASO M345 is a quality classification standard for two stroke engine oils for engines of Japanese origin. It was introduced by the Japanese Automotive Standards Organization (JASO) in 1994 as JASO M345-93 with the quality levels JASO FA, JASO FB and JASO FC – with FC setting the highest standard. It was revised in 2004 as JASO M345:2004 which discontinued the FA level, and introduced the new JASO FD level – the FD level supersedes JASO FC as the highest rating.\n\nThe different quality levels set requirements with regard to properties such as minimum lubrication, minimum detergency, maximum smoke and maximum deposits. A two stroke engine oil is granted the official JASO seal if it has been independently tested. The seal is a rectangle; in the upper quarter of the rectangle will be a serial number, and the lower three quarters will just have the letters FB, FC or FD.\n\nJASO FA – original spec established regulating lubricity, detergency, initial torque, exhaust smoke and exhaust system blocking.\n\nJASO FB – improved requirements over FA with regard to lubricity, detergency, exhaust smoke and exhaust system blocking.\n\nJASO FC – lubricity and initial torque requirements same as FB, however far higher requirements over FB with regard to detergency, exhaust smoke and exhaust system blocking.\n\nJASO FD - same as FC, except with far higher detergency requirement.\n\n"}
{"id": "19665065", "url": "https://en.wikipedia.org/wiki?curid=19665065", "title": "Katakado Dam", "text": "Katakado Dam\n\nKatakado Dam is a gravity dam on the Tadami River west of Aizubange in the Fukushima Prefecture of Japan. It was constructed between 1951 and 1953 for the purpose of hydroelectric power generation. It supplies a 57 MW power station with water.\n\n"}
{"id": "42108510", "url": "https://en.wikipedia.org/wiki?curid=42108510", "title": "Lifosa Power Plant", "text": "Lifosa Power Plant\n\nLifosa Power Plant is a power plant in Kėdainiai, Lithuania. Its primary use is to serve Lifosa factory.\n\nIn 2014 it had installed capacity of 37 MW.\n"}
{"id": "18991324", "url": "https://en.wikipedia.org/wiki?curid=18991324", "title": "Light electric vehicle", "text": "Light electric vehicle\n\nLight electric vehicles are electric vehicles with 2 or 4 wheels powered by a battery, fuel cell, or hybrid-powered, and generally weighing less than . Examples include electric bicycles, electric kick scooters. In Europe these are classified under the L category.\n\n\n"}
{"id": "48516470", "url": "https://en.wikipedia.org/wiki?curid=48516470", "title": "Low-voltage network", "text": "Low-voltage network\n\nA low-voltage network or secondary network is a part of electric power distribution which carries electric energy from distribution transformers to electricity meters of end customers. Secondary networks are operated at a low voltage level, which is typically equal to the mains voltage of electric appliances.\n\nMost modern secondary networks are operated at AC rated voltage of 100–127 or 220–240 volts, at the frequency of 50 or 60 hertz (see mains electricity by country). Operating voltage, required number of phases (three-phase or single-phase) and required reliability dictate topology and configuration of the network. The simplest form are radial service drop lines from the transformer to the customer premises. Low-voltage radial feeders supply multiple customers. For increased reliability, so-called \"spot networks\" and \"grid networks\" provide supply of customers from multiple distribution transformers and supply paths. Electric wiring can be realized by overhead power lines, aerial or underground power cables, or their mixture.\n\nElectric power distribution systems are designed to serve their customers with reliable and high-quality power. The most common distribution system consists of simple radial circuits (feeders) that can be overhead, underground, or a combination. From the distribution substation, feeders carry the power to the end customers, forming the \"medium-voltage\" or \"primary\" network, operated at a medium voltage level, typically 5–35 kV. Feeders range in length from a few kilometers to several tens of kilometers. As they must supply all customers in the designated distribution area, they often curve and branch along the assigned corridors. A substation typically supplies 3–30 feeders.\n\nDistribution transformers or \"secondary transformers\", placed along feeders, convert the voltage from the medium to a low voltage level, suitable for direct consumption by end customers (mains voltage). Typically, a rural primary feeder supplies up to 50 distribution transformers, spread over a wide region, but the figure significantly varies depending on configuration. They are sited on pole tops, cellars or designated small plots. From these transformers, \"low-voltage\" or \"secondary\" network branches off to the customer connections at customer premises, equipped with electricity meters.\n\nMost of differences in the layout and design of low-voltage networks are dictated by the mains voltage rating. In Europe and most of the world 220–240 V is the dominant choice, while in North America 120 V is the standard.\n\nANSI standard C84.1 recommends a +5%, −2.5% tolerance for the voltage range at a service point. North American LV networks feature much shorter secondary connections, up to , while in European design they can reach up to . North American distribution transformers must be therefore placed much closer to consumers, and are smaller (25–50 kVA), while European ones can cover larger areas and thus have higher ratings (300–1000 kVA); only the remote rural areas in European design are served by single-phase transformers.\n\nAs the low-voltage distribute the electric power to the widest class of end users, another main design concern is safety of consumers who use the electric appliances and their protection against electric shocks. An earthing system, in combination with protective devices such as fuses and residual current devices, must ultimately ensure that a person must not come into touch with a metallic object whose potential relative to the person's potential (which is, in turn, equal to the ground potential unless insulating mats are used) exceeds a \"safe\" threshold, typically set at about 50 V.\n\nRadial operation is the most widespread and most economic design of both MV and LV networks.It provides a sufficiently high degree of reliability and service continuity for most customers. In American (120 V) systems, the customers are commonly supplied directly from the distribution transformers via relatively short service drop lines, in star-like topology. In 240 V systems, the customers are served by several low-voltage feeders, realized by overhead power lines, aerial or underground power cables, or their mixture; in an overhead network, service drops are drawn from pole tops to roof connections. In a cable network, all necessary connections and protection devices are typically placed in pad-mounted cabinets or, occasionally, manholes (buried T-joint connections are prone to failures).\n\nPower-system protection in radial networks is simple to design and implement, since short-circuit currents have only one possible path that needs to be interrupted. Fuses are most commonly used for both short-circuit and overload protection, while low-voltage circuit breakers may be used in special circumstances.\n\nSpot networks are used when increased reliability of supply is required for important customers. The low-voltage network is supplied from two or more distribution transformers at a single site, each fed from a different MV feeder (which may originate from the same or different substations). The transformers are connected together with a bus or a cable on secondary side, termed \"paralleling bus\" or \"collector bus\". The paralleling bus typically does not have connecting cables (\"reaches\") to other network units, in which case such networks are termed \"isolating spot networks\"; when they have, they are referred to as \"spot networks with reach\". In some cases, fast-acting secondary bus tie breakers may be applied between bus sections to isolate faults in the secondary switchgear and limit loss of service.\n\nSpot systems are commonly applied in high load-density areas such as business districts, large hospitals, small industry and important facilities such as water supply systems. In normal operation, the energy supply is provided by both primary feeders in parallel. In case of an outage of either primary feeder, network protector device at the corresponding spot transformer secondary automatically opens; the remaining transformers continue to provide supply through their respective primary feeders. Only in cases when the short circuit is located at the paralleling bus, or a total loss of primary supply occurs, the customer will remain out of service. Faults on the low-voltage network are handled by fuses or local circuit breakers, resulting in loss of service only for the affected loads.\n\nA grid networks consist of an interconnected grid of circuits, energized from several primary feeders through distribution transformers at multiple locations. Grid networks are typically featured in downtowns of large cities, with connecting cables laid out in underground conduits along the streets. Numerous cables allow for multiple current paths from every transformer to every load within the grid.\n\nAs with spot networks, network protectors are used to protect against primary feeder faults, and prevent fault current to propagate from the grid to the primary feeder. Individual cable sections may be protected by cable limiters on both ends, special fuses providing very fast short-circuit protection. Cable limiters do not have an ampere rating, and cannot be used to provide overload protection; their sole purpose is to isolate the fault. Under high short-circuit conditions, limiters blow and cut off the faulted cable, while the unaffected cables take over its load and continue to provide service. Primary feeder outages, as well as limiters and network protectors cleared because of previous faults, cause changes in load flow that are not readily detected, so their statuses may require a periodic inspection. The inherent system redundancy generally prevents any customer from experiencing outage.\n\n\n"}
{"id": "246267", "url": "https://en.wikipedia.org/wiki?curid=246267", "title": "Magnesium sulfate", "text": "Magnesium sulfate\n\nMagnesium sulfate is an inorganic salt with the formula MgSO(HO) where 0≤x≤7. It is often encountered as the heptahydrate sulfate mineral epsomite (MgSO·7HO), commonly called Epsom salt. The overall global annual usage in the mid-1970s of the monohydrate was 2.3 million tons, of which the majority was used in agriculture.\n\nEpsom salt has been traditionally used as a component of bath salts. Epsom salt can also be used as a beauty product. Athletes use it to soothe sore muscles, while gardeners use it to improve crops. It has a variety of other uses: for example, Epsom salt is also effective in the removal of splinters.\n\nA variety of hydrates are known.\n\nThe heptahydrate (epsomite) readily loses one equivalent of water to form the hexahydrate. Epsom salt takes its name from a bitter saline spring in Epsom in Surrey, England, where the salt was produced from the springs that arise where the porous chalk of the North Downs meets non-porous London clay.\n\nThe monohydrate, MgSO·HO is found as the mineral kieserite. It can be prepared by heating the hexahydrate to approximately 150 °C. Further heating to approximately 200 °C gives anhydrous magnesium sulfate. Upon further heating, the anhydrous salt decomposes into magnesium oxide (MgO) and sulfur trioxide (SO).\n\nThe heptahydrate can be prepared by neutralizing sulfuric acid with magnesium carbonate or oxide, but it is usually obtained directly from natural sources.\n\nIt is on the WHO Model List of Essential Medicines, the most important medications needed in a basic health system.\n\nMagnesium sulfate is a common mineral pharmaceutical preparation of magnesium, commonly known as Epsom salt, used both externally and internally. Magnesium sulfate is highly water-soluble and solubility is inhibited with lipids typically used in lotions. Lotions often employ the use of emulsions or suspensions to include both oil and water-soluble ingredients. Hence, magnesium sulfate in a lotion may not be as freely available to migrate to the skin nor to be absorbed through the skin, hence both studies may properly suggest absorption or lack thereof as a function of the carrier (in a water solution vs. in an oil emulsion/suspension). Temperature and concentration gradients may also be contributing factors to absorption.\n\nExternally, magnesium sulfate paste is used to treat skin inflammations such as small boils or localised infections. Known in the UK as 'drawing paste' it is also used to remove splinters. The standard British Pharmacopoeia composition is dried Magnesium Sulfate 47.76 % w/w, Phenol 0.49 % w/w. and glycerol (E422).\n\nEpsom salt is used as bath salts and for isolation tanks. Magnesium sulfate is the main preparation of intravenous magnesium.\n\nInternal uses include:\n\nIn agriculture, magnesium sulfate is used to increase magnesium or sulfur content in soil. It is most commonly applied to potted plants, or to magnesium-hungry crops, such as potatoes, roses, tomatoes, lemon trees, carrots, and peppers. The advantage of magnesium sulfate over other magnesium soil amendments (such as dolomitic lime) is its high solubility, which also allows the option of foliar feeding. Solutions of magnesium sulfate are also nearly neutral, compared with alkaline salts of magnesium as found in limestone; therefore, the use of magnesium sulfate as a magnesium source for soil does not significantly change the soil pH.\n\nMagnesium sulfate is used as a brewing salt in beer.\n\nIt may also be used as a coagulant for making tofu.\n\nAnhydrous magnesium sulfate is commonly used as a desiccant in organic synthesis due to its affinity for water. During work-up, an organic phase is treated with anhydrous magnesium sulfate. The hydrated solid is then removed with filtration or decantation. Other inorganic sulfate salts such as sodium sulfate and calcium sulfate may be used in the same way.\n\nMagnesium sulfate heptahydrate is also used to maintain the magnesium concentration in marine aquaria which contain large amounts of stony corals, as it is slowly depleted in their calcification process. In a magnesium-deficient marine aquarium, calcium and alkalinity concentrations are very difficult to control because not enough magnesium is present to stabilize these ions in the saltwater and prevent their spontaneous precipitation into calcium carbonate.\n\nMagnesium sulfates are common minerals in geological environments. Their occurrence is mostly connected with supergene processes. Some of them are also important constituents of evaporitic potassium-magnesium (K-Mg) salts deposits.\n\nBright spots observed by the Dawn Spacecraft in Occator Crater on the dwarf planet Ceres are most consistent with reflected light from magnesium sulfate hexahydrate.\n\nAlmost all known mineralogical forms of MgSO are hydrates. Epsomite is the natural analogue of \"Epsom salt\". Another heptahydrate, the copper-containing mineral alpersite (Mg,Cu)SO·7HO, was recently recognized. Both are, however, not the highest known hydrates of MgSO, due to the recent terrestrial find of meridianiite, MgSO·11HO, which is thought to also occur on Mars. Hexahydrite is the next lower (6) hydrate. Three next lower hydrates—pentahydrite, starkeyite, and especially sanderite are rare. Kieserite is a monohydrate and is common among evaporitic deposits. Anhydrous magnesium sulfate was reported from some burning coal dumps.\n\nDouble salts containing magnesium sulfate exist, for example there are several known sodium magnesium sulfates and potassium magnesium sulfates.\n\nAn abnormally elevated plasma concentration of magnesium is called hypermagnesemia.\n\n"}
{"id": "3318048", "url": "https://en.wikipedia.org/wiki?curid=3318048", "title": "Maximum power point tracking", "text": "Maximum power point tracking\n\nMaximum power point tracking (MPPT) or sometimes just power point tracking (PPT)) is a technique used commonly with wind turbines and photovoltaic (PV) solar systems to maximize power extraction under all conditions.\n\nAlthough solar power is mainly covered, the principle applies generally to sources with variable power: for example, optical power transmission and thermophotovoltaics.\n\nPV solar systems exist in many different configurations with regard to their relationship to inverter systems, external grids, battery banks, or other electrical loads. Regardless of the ultimate destination of the solar power, though, the central problem addressed by MPPT is that the efficiency of power transfer from the solar cell depends on both the amount of sunlight falling on the solar panels and the electrical characteristics of the load. As the amount of sunlight varies, the load characteristic that gives the highest power transfer efficiency changes, so that the efficiency of the system is optimized when the load characteristic changes to keep the power transfer at highest efficiency. This load characteristic is called the \"maximum power point\" (MPP) and MPPT is the process of finding this point and keeping the load characteristic there. Electrical circuits can be designed to present arbitrary loads to the photovoltaic cells and then convert the voltage, current, or frequency to suit other devices or systems, and MPPT solves the problem of choosing the best load to be presented to the cells in order to get the most usable power out.\n\nSolar cells have a complex relationship between temperature and total resistance that produces a non-linear output efficiency which can be analyzed based on the I-V curve. It is the purpose of the MPPT system to sample the output of the PV cells and apply the proper resistance (load) to obtain maximum power for any given environmental conditions. MPPT devices are typically integrated into an electric power converter system that provides voltage or current conversion, filtering, and regulation for driving various loads, including power grids, batteries, or motors.\n\nPhotovoltaic cells have a complex relationship between their operating environment and the maximum power they can produce. The fill factor, abbreviated \"FF\", is a parameter which characterizes the non-linear electrical behavior of the solar cell. Fill factor is defined as the ratio of the maximum power from the solar cell to the product of open circuit voltage V and short-circuit current I. In tabulated data it is often used to estimate the maximum power that a cell can provide with an optimal load under given conditions, \"P=FF*V*I\". For most purposes, FF, V, and I are enough information to give a useful approximate model of the electrical behavior of a photovoltaic cell under typical conditions.\n\nFor any given set of operational conditions, cells have a single operating point where the values of the current (\"I\") and voltage (\"V\") of the cell result in a maximum power output. These values correspond to a particular load resistance, which is equal to \"V\" / I as specified by Ohm's Law. The power P is given by \"P=V*I\". A photovoltaic cell, for the majority of its useful curve, acts as a constant current source. However, at a photovoltaic cell's MPP region, its curve has an approximately inverse exponential relationship between current and voltage. From basic circuit theory, the power delivered from or to a device is optimized where the derivative (graphically, the slope) \"dI/dV\" of the I-V curve is equal and opposite the \"I/V\" ratio (where d\"P/dV\"=0). This is known as the \"maximum power point\" (MPP) and corresponds to the \"knee\" of the curve.\n\nA load with resistance \"R=V/I\" equal to the reciprocal of this value draws the maximum power from the device. This is sometimes called the 'characteristic resistance' of the cell. This is a dynamic quantity which changes depending on the level of illumination, as well as other factors such as temperature and the age of the cell. If the resistance is lower or higher than this value, the power drawn will be less than the maximum available, and thus the cell will not be used as efficiently as it could be. Maximum power point trackers utilize different types of control circuit or logic to search for this point and thus to allow the converter circuit to extract the maximum power available from a cell.\n\nWhen a load is directly connected to the solar panel, the operating point of the panel will rarely be at peak power. The impedance seen by the panel derives the operating point of the solar panel. Thus by varying the impedance seen by the panel, the operating point can be moved towards peak power point. Since panels are DC devices, DC-DC converters must be utilized to transform the impedance of one circuit (source) to the other circuit (load). Changing the duty ratio of the DC-DC converter results in an impedance change as seen by the panel. At a particular impedance (or duty ratio) the operating point will be at the peak power transfer point. The I-V curve of the panel can vary considerably with variation in atmospheric conditions such as radiance and temperature. Therefore, it is not feasible to fix the duty ratio with such dynamically changing operating conditions.\n\nMPPT implementations utilize algorithms that frequently sample panel voltages and currents, then adjust the duty ratio as needed. Microcontrollers are employed to implement the algorithms. Modern implementations often utilize larger computers for analytics and load forecasting.\n\nControllers can follow several strategies to optimize the power output of an array. Maximum power point trackers may implement different algorithms and switch between them based on the operating conditions of the array.\n\nIn this method the controller adjusts the voltage by a small amount from the array and measures power; if the power increases, further adjustments in that direction are tried until power no longer increases. This is called the perturb and observe method and is most common, although this method can result in oscillations of power output. It is referred to as a \"hill climbing\" method, because it depends on the rise of the curve of power against voltage below the maximum power point, and the fall above that point. Perturb and observe is the most commonly used MPPT method due to its ease of implementation. Perturb and observe method may result in top-level efficiency, provided that a proper predictive and adaptive hill climbing strategy is adopted.\n\nIn the incremental conductance method, the controller measures incremental changes in PV array current and voltage to predict the effect of a voltage change. This method requires more computation in the controller, but can track changing conditions more rapidly than the perturb and observe method (P&O). Like the P&O algorithm, it can produce oscillations in power output. This method utilizes the incremental conductance (dI/dV) of the photovoltaic array to compute the sign of the change in power with respect to voltage (dP/dV).\n\nThe incremental conductance method computes the maximum power point by comparison of the incremental conductance (I / V) to the array conductance (I / V). When these two are the same (I / V = I / V), the output voltage is the MPP voltage. The controller maintains this voltage until the irradiation changes and the process is repeated.\n\nThe incremental conductance method is based on the observation that at the maximum power point\ndP/dV = 0, and that P = IV. The current from the array can be expressed as a function of the voltage: P = I(V)V. Therefore, dP/dV = VdI/dV + I(V). Setting this equal to zero yields: dI/dV = -I(V)/V. Therefore, the maximum power point is achieved when the incremental conductance is equal to the negative of the instantaneous conductance.\n\nThe current sweep method uses a sweep waveform for the PV array current such that the I-V characteristic of the PV array is obtained and updated at fixed time intervals. The maximum power point voltage can then be computed from the characteristic curve at the same intervals.\n\nThe term \"constant voltage\" in MPP tracking is used to describe different techniques by different authors, one in which the output voltage is regulated to a constant value under all conditions and one in which the output voltage is regulated based on a constant ratio to the measured open circuit voltage (V). The latter technique is referred to in contrast as the \"open voltage\" method by some authors. If the output voltage is held constant, there is no attempt to track the maximum power point, so it is not a maximum power point tracking technique in a strict sense, though it does have some advantages in cases when the MPP tracking tends to fail, and thus it is sometimes used to supplement an MPPT method in those cases.\n\nIn the \"constant voltage\" MPPT method (also known as the \"open voltage method\"), the power delivered to the load is momentarily interrupted and the open-circuit voltage with zero current is measured. The controller then resumes operation with the voltage controlled at a fixed ratio, such as 0.76, of the open-circuit voltage V. This is usually a value which has been determined to be the maximum power point, either empirically or based on modelling, for expected operating conditions. The operating point of the PV array is thus kept near the MPP by regulating the array voltage and matching it to the fixed reference voltage V=kV. The value of V may be also chosen to give optimal performance relative to other factors as well as the MPP, but the central idea in this technique is that V is determined as a ratio to V.\n\nOne of the inherent approximations to the \"constant voltage\" ratio method is that the ratio of the MPP voltage to V is only approximately constant, so it leaves room for further possible optimization.\n\nThis method of MPPT estimates the MPP voltage (formula_1) by measuring the temperature of the solar module and comparing it against a reference. Since changes on irradiation levels have little effect on the maximum power point voltage, its influences may be neglect and the voltage can be assumed to only vary linearly with the temperature.\n\nThis algorithm calculates the following equation:\n\nformula_2\n\nWhere:\n\nformula_1is the voltage at the maximum power point for a given temperature;\n\nformula_4is a reference temperature;\n\nformula_5is the measured temperature;\n\nformula_6is the temperature coefficient of formula_1(available in the datasheet).\n\n\n\nBoth perturb and observe, and incremental conductance, are examples of \"hill climbing\" methods that can find the local maximum of the power curve for the operating condition of the PV array, and so provide a true maximum power point.\n\nThe perturb and observe method requires oscillating power output around the maximum power point even under steady state irradiance.\n\nThe incremental conductance method has the advantage over the perturb and observe (P&O) method that it can determine the maximum power point without oscillating around this value. It can perform maximum power point tracking under rapidly varying irradiation conditions with higher accuracy than the perturb and observe method. However, the incremental conductance method can produce oscillations (unintentionally) and can perform erratically under rapidly changing atmospheric conditions. The sampling frequency is decreased due to the higher complexity of the algorithm compared to the P&O method.\n\nIn the constant voltage ratio (or \"open voltage\") method, the current from the photovoltaic array must be set to zero momentarily to measure the open circuit voltage and then afterwards set to a predetermined percentage of the measured voltage, usually around 76%. Energy may be wasted during the time the current is set to zero. The approximation of 76% as the MPP/V ratio is not necessarily accurate. Although simple and low-cost to implement, the interruptions reduce array efficiency and do not ensure finding the actual maximum power point. However, efficiencies of some systems may reach above 95%.\n\nTraditional solar inverters perform MPPT for the entire PV array (module association) as a whole. In such systems the same current, dictated by the inverter, flows through all modules in the string (series). Because different modules have different I-V curves and different MPPs (due to manufacturing tolerance, partial shading, etc.) this architecture means some modules will be performing below their MPP, resulting in lower efficiency.\n\nSome companies (see power optimizer) are now placing maximum power point tracker into individual modules, allowing each to operate at peak efficiency despite uneven shading, soiling or electrical mismatch.\n\nData suggests having one inverter with one MPPT for a project that has east and west-facing modules presents no disadvantages when compared to having two inverters or one inverter with more than one MPPT.\n\nAt night, an off-grid PV system may use batteries to supply loads. Although the fully charged battery pack voltage may be close to the PV panel's maximum power point voltage, this is unlikely to be true at sunrise when the battery has been partially discharged. Charging may begin at a voltage considerably below the PV panel maximum power point voltage, and an MPPT can resolve this mismatch.\n\nWhen the batteries in an off-grid system are fully charged and PV production exceeds local loads, an MPPT can no longer operate the panel at its maximum power point as the excess power has no load to absorb it. The MPPT must then shift the PV panel operating point away from the peak power point until production exactly matches demand. (An alternative approach commonly used in spacecraft is to divert surplus PV power into a resistive load, allowing the panel to operate continuously at its peak power point.)\n\nIn a grid connected photovoltaic system, all delivered power from solar modules will be sent to the grid. Therefore, the MPPT in a grid connected PV system will always attempt to operate the PV modules at its maximum power point.\n\n"}
{"id": "18617142", "url": "https://en.wikipedia.org/wiki?curid=18617142", "title": "Mercury (element)", "text": "Mercury (element)\n\nMercury is a chemical element with symbol Hg and atomic number 80. It is commonly known as quicksilver and was formerly named hydrargyrum ( ). A heavy, silvery d-block element, mercury is the only metallic element that is liquid at standard conditions for temperature and pressure; the only other element that is liquid under these conditions is bromine, though metals such as caesium, gallium, and rubidium melt just above room temperature.\n\nMercury occurs in deposits throughout the world mostly as cinnabar (mercuric sulfide). The red pigment vermilion is obtained by grinding natural cinnabar or synthetic mercuric sulfide.\n\nMercury is used in thermometers, barometers, manometers, sphygmomanometers, float valves, mercury switches, mercury relays, fluorescent lamps and other devices, though concerns about the element's toxicity have led to mercury thermometers and sphygmomanometers being largely phased out in clinical environments in favor of alternatives such as alcohol- or galinstan-filled glass thermometers and thermistor- or infrared-based electronic instruments. Likewise, mechanical pressure gauges and electronic strain gauge sensors have replaced mercury sphygmomanometers.\n\nMercury remains in use in scientific research applications and in amalgam for dental restoration in some locales. It is also used in fluorescent lighting. Electricity passed through mercury vapor in a fluorescent lamp produces short-wave ultraviolet light, which then causes the phosphor in the tube to fluoresce, making visible light.\n\nMercury poisoning can result from exposure to water-soluble forms of mercury (such as mercuric chloride or methylmercury), by inhalation of mercury vapor, or by ingesting any form of mercury.\n\nMercury is a heavy, silvery-white liquid metal. Compared to other metals, it is a poor conductor of heat, but a fair conductor of electricity.\n\nIt has a freezing point of −38.83 °C and a boiling point of 356.73 °C, both the lowest of any stable metal, although preliminary experiments on copernicium and flerovium have indicated that they have even lower boiling points (copernicium being the element below mercury in the periodic table, following the trend of decreasing boiling points down group 12). Upon freezing, the volume of mercury decreases by 3.59% and its density changes from 13.69 g/cm when liquid to 14.184 g/cm when solid. The coefficient of volume expansion is 181.59 × 10 at 0 °C, 181.71 × 10 at 20 °C and 182.50 × 10 at 100 °C (per °C). Solid mercury is malleable and ductile and can be cut with a knife.\n\nA complete explanation of mercury's extreme volatility delves deep into the realm of quantum physics, but it can be summarized as follows: mercury has a unique electron configuration where electrons fill up all the available 1s, 2s, 2p, 3s, 3p, 3d, 4s, 4p, 4d, 4f, 5s, 5p, 5d, and 6s subshells. Because this configuration strongly resists removal of an electron, mercury behaves similarly to noble gases, which form weak bonds and hence melt at low temperatures.\n\nThe stability of the 6s shell is due to the presence of a filled 4f shell. An f shell poorly screens the nuclear charge that increases the attractive Coulomb interaction of the 6s shell and the nucleus (see lanthanide contraction). The absence of a filled inner f shell is the reason for the somewhat higher melting temperature of cadmium and zinc, although both these metals still melt easily and, in addition, have unusually low boiling points.\n\nMercury does not react with most acids, such as dilute sulfuric acid, although oxidizing acids such as concentrated sulfuric acid and nitric acid or aqua regia dissolve it to give sulfate, nitrate, and chloride. Like silver, mercury reacts with atmospheric hydrogen sulfide. Mercury reacts with solid sulfur flakes, which are used in mercury spill kits to absorb mercury (spill kits also use activated carbon and powdered zinc).\n\nMercury dissolves many other metals such as gold and silver to form amalgams. Iron is an exception, and iron flasks have traditionally been used to trade mercury. Several other first row transition metals with the exception of manganese, copper and zinc are reluctant to form amalgams. Other elements that do not readily form amalgams with mercury include platinum. Sodium amalgam is a common reducing agent in organic synthesis, and is also used in high-pressure sodium lamps.\n\nMercury readily combines with aluminium to form a mercury-aluminium amalgam when the two pure metals come into contact. Since the amalgam destroys the aluminium oxide layer which protects metallic aluminium from oxidizing in-depth (as in iron rusting), even small amounts of mercury can seriously corrode aluminium. For this reason, mercury is not allowed aboard an aircraft under most circumstances because of the risk of it forming an amalgam with exposed aluminium parts in the aircraft.\n\nMercury embrittlement is the most common type of liquid metal embrittlement.\n\nThere are seven stable isotopes of mercury with being the most abundant (29.86%). The longest-lived radioisotopes are with a half-life of 444 years, and with a half-life of 46.612 days. Most of the remaining radioisotopes have half-lives that are less than a day. and are the most often studied NMR-active nuclei, having spins of and respectively.\n\nHg is the modern chemical symbol for mercury. It comes from \"hydrargyrum\", a Latinized form of the Greek word ὑδράργυρος (\"hydrargyros\"), which is a compound word meaning \"water-silver\" (from ὑδρ- \"hydr-\", the root of ὕδωρ, \"water,\" and ἄργυρος \"argyros\" \"silver\") – since it is liquid like water and shiny like silver. The element was named after the Roman god Mercury, known for his speed and mobility. It is associated with the planet Mercury; the astrological symbol for the planet is also one of the alchemical symbols for the metal; the Sanskrit word for alchemy is \"Rasavātam\" which means \"the way of mercury\". Mercury is the only metal for which the alchemical planetary name became the common name.\n\nMercury was found in Egyptian tombs that date from 1500 BC.\n\nIn China and Tibet, mercury use was thought to prolong life, heal fractures, and maintain generally good health, although it is now known that exposure to mercury vapor leads to serious adverse health effects. The first emperor of China, Qín Shǐ Huáng Dì—allegedly buried in a tomb that contained rivers of flowing mercury on a model of the land he ruled, representative of the rivers of China—was killed by drinking a mercury and powdered jade mixture formulated by Qin alchemists (causing liver failure, mercury poisoning, and brain death) who intended to give him eternal life. Khumarawayh ibn Ahmad ibn Tulun, the second Tulunid ruler of Egypt (r. 884–896), known for his extravagance and profligacy, reportedly built a basin filled with mercury, on which he would lie on top of air-filled cushions and be rocked to sleep.\n\nIn November 2014 \"large quantities\" of mercury were discovered in a chamber 60 feet below the 1800-year-old pyramid known as the \"Temple of the Feathered Serpent,\" \"the third largest pyramid of Teotihuacan,\" Mexico along with \"jade statues, jaguar remains, a box filled with carved shells and rubber balls.\"\n\nThe ancient Greeks used cinnabar (mercury sulfide) in ointments; the ancient Egyptians and the Romans used it in cosmetics. In Lamanai, once a major city of the Maya civilization, a pool of mercury was found under a marker in a Mesoamerican ballcourt. By 500 BC mercury was used to make amalgams (Medieval Latin \"amalgama\", \"alloy of mercury\") with other metals.\n\nAlchemists thought of mercury as the First Matter from which all metals were formed. They believed that different metals could be produced by varying the quality and quantity of sulfur contained within the mercury. The purest of these was gold, and mercury was called for in attempts at the transmutation of base (or impure) metals into gold, which was the goal of many alchemists.\n\nThe mines in Almadén (Spain), Monte Amiata (Italy), and Idrija (now Slovenia) dominated mercury production from the opening of the mine in Almadén 2500 years ago, until new deposits were found at the end of the 19th century.\n\nMercury is an extremely rare element in Earth's crust, having an average crustal abundance by mass of only 0.08 parts per million (ppm). Because it does not blend geochemically with those elements that constitute the majority of the crustal mass, mercury ores can be extraordinarily concentrated considering the element's abundance in ordinary rock. The richest mercury ores contain up to 2.5% mercury by mass, and even the leanest concentrated deposits are at least 0.1% mercury (12,000 times average crustal abundance). It is found either as a native metal (rare) or in cinnabar, metacinnabar, corderoite, livingstonite and other minerals, with cinnabar (HgS) being the most common ore. Mercury ores often in hot springs or other volcanic regions.\n\nBeginning in 1558, with the invention of the patio process to extract silver from ore using mercury, mercury became an essential resource in the economy of Spain and its American colonies. Mercury was used to extract silver from the lucrative mines in New Spain and Peru. Initially, the Spanish Crown's mines in Almadén in Southern Spain supplied all the mercury for the colonies. Mercury deposits were discovered in the New World, and more than 100,000 tons of mercury were mined from the region of Huancavelica, Peru, over the course of three centuries following the discovery of deposits there in 1563. The patio process and later pan amalgamation process continued to create great demand for mercury to treat silver ores until the late 19th century.\nFormer mines in Italy, the United States and Mexico, which once produced a large proportion of the world supply, have now been completely mined out or, in the case of Slovenia (Idrija) and Spain (Almadén), shut down due to the fall of the price of mercury. Nevada's McDermitt Mine, the last mercury mine in the United States, closed in 1992. The price of mercury has been highly volatile over the years and in 2006 was $650 per 76-pound (34.46 kg) flask.\n\nMercury is extracted by heating cinnabar in a current of air and condensing the vapor. The equation for this extraction is\n\nIn 2005, China was the top producer of mercury with almost two-thirds global share followed by Kyrgyzstan. Several other countries are believed to have unrecorded production of mercury from copper electrowinning processes and by recovery from effluents.\n\nBecause of the high toxicity of mercury, both the mining of cinnabar and refining for mercury are hazardous and historic causes of mercury poisoning. In China, prison labor was used by a private mining company as recently as the 1950s to develop new cinnabar mines. Thousands of prisoners were used by the Luo Xi mining company to establish new tunnels. Worker health in functioning mines is at high risk.\n\nThe European Union directive calling for compact fluorescent bulbs to be made mandatory by 2012 has encouraged China to re-open cinnabar mines to obtain the mercury required for CFL bulb manufacture. Environmental dangers have been a concern, particularly in the southern cities of Foshan and Guangzhou, and in Guizhou province in the southwest.\n\nAbandoned mercury mine processing sites often contain very hazardous waste piles of roasted cinnabar calcines. Water run-off from such sites is a recognized source of ecological damage. Former mercury mines may be suited for constructive re-use. For example, in 1976 Santa Clara County, California purchased the historic Almaden Quicksilver Mine and created a county park on the site, after conducting extensive safety and environmental analysis of the property.\n\nMercury exists in two oxidation states, I and II. Despite claims otherwise, Hg(III) and Hg(IV) compounds remain unknown.\n\nUnlike its lighter neighbors, cadmium and zinc, mercury usually forms simple stable compounds with metal-metal bonds. Most mercury(I) compounds are diamagnetic and feature the dimeric cation, Hg. Stable derivatives include the chloride and nitrate. Treatment of Hg(I) compounds complexation with strong ligands such as sulfide, cyanide, etc. induces disproportionation to and elemental mercury. Mercury(I) chloride, a colorless solid also known as calomel, is really the compound with the formula HgCl, with the connectivity Cl-Hg-Hg-Cl. It is a standard in electrochemistry. It reacts with chlorine to give mercuric chloride, which resists further oxidation. Mercury(I) hydride, a colorless gas, has the formula HgH, containing no Hg-Hg bond.\n\nIndicative of its tendency to bond to itself, mercury forms mercury polycations, which consist of linear chains of mercury centers, capped with a positive charge. One example is .\n\nMercury(II) is the most common oxidation state and is the main one in nature as well. All four mercuric halides are known. They form tetrahedral complexes with other ligands but the halides adopt linear coordination geometry, somewhat like Ag does. Best known is mercury(II) chloride, an easily sublimating white solid. HgCl forms coordination complexes that are typically tetrahedral, e.g. .\n\nMercury(II) oxide, the main oxide of mercury, arises when the metal is exposed to air for long periods at elevated temperatures. It reverts to the elements upon heating near 400 °C, as was demonstrated by Joseph Priestley in an early synthesis of pure oxygen. Hydroxides of mercury are poorly characterized, as they are for its neighbors gold and silver.\n\nBeing a soft metal, mercury forms very stable derivatives with the heavier chalcogens. Preeminent is mercury(II) sulfide, HgS, which occurs in nature as the ore cinnabar and is the brilliant pigment vermillion. Like ZnS, HgS crystallizes in two forms, the reddish cubic form and the black zinc blende form. The latter sometimes occurs naturally as metacinnabar. Mercury(II) selenide (HgSe) and mercury(II) telluride (HgTe) are also known, these as well as various derivatives, e.g. mercury cadmium telluride and mercury zinc telluride being semiconductors useful as infrared detector materials.\n\nMercury(II) salts form a variety of complex derivatives with ammonia. These include Millon's base (HgN), the one-dimensional polymer (salts of )), and \"fusible white precipitate\" or [Hg(NH)]Cl. Known as Nessler's reagent, potassium tetraiodomercurate(II) () is still occasionally used to test for ammonia owing to its tendency to form the deeply colored iodide salt of Millon's base.\n\nMercury fulminate is a detonator widely used in explosives.\n\nOrganic mercury compounds are historically important but are of little industrial value in the western world. Mercury(II) salts are a rare example of simple metal complexes that react directly with aromatic rings. Organomercury compounds are always divalent and usually two-coordinate and linear geometry. Unlike organocadmium and organozinc compounds, organomercury compounds do not react with water. They usually have the formula HgR, which are often volatile, or HgRX, which are often solids, where R is aryl or alkyl and X is usually halide or acetate. Methylmercury, a generic term for compounds with the formula CHHgX, is a dangerous family of compounds that are often found in polluted water. They arise by a process known as biomethylation.\n\nMercury is used primarily for the manufacture of industrial chemicals or for electrical and electronic applications. It is used in some thermometers, especially ones which are used to measure high temperatures. A still increasing amount is used as gaseous mercury in fluorescent lamps, while most of the other applications are slowly phased out due to health and safety regulations and is in some applications replaced with less toxic but considerably more expensive Galinstan alloy.\n\nMercury and its compounds have been used in medicine, although they are much less common today than they once were, now that the toxic effects of mercury and its compounds are more widely understood. The first edition of the Merck's Manual featured many mercuric compounds such as:\n\n\nMercury is an ingredient in dental amalgams. Thiomersal (called \"Thimerosal\" in the United States) is an organic compound used as a preservative in vaccines, though this use is in decline. Thiomersal is metabolized to ethyl mercury. Although it was widely speculated that this mercury-based preservative could cause or trigger autism in children, scientific studies showed no evidence supporting any such link. Nevertheless, thiomersal has been removed from, or reduced to trace amounts in all U.S. vaccines recommended for children 6 years of age and under, with the exception of inactivated influenza vaccine.\n\nAnother mercury compound, merbromin (Mercurochrome), is a topical antiseptic used for minor cuts and scrapes that is still in use in some countries.\n\nMercury in the form of one of its common ores, cinnabar, is used in various traditional medicines, especially in traditional Chinese medicine. Review of its safety has found that cinnabar can lead to significant mercury intoxication when heated, consumed in overdose, or taken long term, and can have adverse effects at therapeutic doses, though effects from therapeutic doses are typically reversible. Although this form of mercury appears to be less toxic than other forms, its use in traditional Chinese medicine has not yet been justified, as the therapeutic basis for the use of cinnabar is not clear.\n\nToday, the use of mercury in medicine has greatly declined in all respects, especially in developed countries. Thermometers and sphygmomanometers containing mercury were invented in the early 18th and late 19th centuries, respectively. In the early 21st century, their use is declining and has been banned in some countries, states and medical institutions. In 2002, the U.S. Senate passed legislation to phase out the sale of non-prescription mercury thermometers. In 2003, Washington and Maine became the first states to ban mercury blood pressure devices. Mercury compounds are found in some over-the-counter drugs, including topical antiseptics, stimulant laxatives, diaper-rash ointment, eye drops, and nasal sprays. The FDA has \"inadequate data to establish general recognition of the safety and effectiveness\" of the mercury ingredients in these products. Mercury is still used in some diuretics although substitutes now exist for most therapeutic uses.\n\nChlorine is produced from sodium chloride (common salt, NaCl) using electrolysis to separate the metallic sodium from the chlorine gas. Usually the salt is dissolved in water to produce a brine. By-products of any such chloralkali process are hydrogen (H) and sodium hydroxide (NaOH), which is commonly called caustic soda or lye. By far the largest use of mercury in the late 20th century was in the mercury cell process (also called the Castner-Kellner process) where metallic sodium is formed as an amalgam at a cathode made from mercury; this sodium is then reacted with water to produce sodium hydroxide. Many of the industrial mercury releases of the 20th century came from this process, although modern plants claimed to be safe in this regard. After about 1985, all new chloralkali production facilities that were built in the United States used membrane cell or diaphragm cell technologies to produce chlorine.\n\nSome medical thermometers, especially those for high temperatures, are filled with mercury; they are gradually disappearing. In the United States, non-prescription sale of mercury fever thermometers has been banned since 2003.\n\nMercury is also found in liquid mirror telescopes.\n\nSome transit telescopes use a basin of mercury to form a flat and absolutely horizontal mirror, useful in determining an absolute vertical or perpendicular reference. Concave horizontal parabolic mirrors may be formed by rotating liquid mercury on a disk, the parabolic form of the liquid thus formed reflecting and focusing incident light. Such telescopes are cheaper than conventional large mirror telescopes by up to a factor of 100, but the mirror cannot be tilted and always points straight up.\n\nLiquid mercury is a part of popular secondary reference electrode (called the calomel electrode) in electrochemistry as an alternative to the standard hydrogen electrode. The calomel electrode is used to work out the electrode potential of half cells. Last, but not least, the triple point of mercury, −38.8344 °C, is a fixed point used as a temperature standard for the International Temperature Scale (ITS-90).\n\nIn polarography both the dropping mercury electrode and the hanging mercury drop electrode use elemental mercury. This use allows a new uncontaminated electrode to be available for each measurement or each new experiment.\n\nGaseous mercury is used in mercury-vapor lamps and some \"neon sign\" type advertising signs and fluorescent lamps. Those low-pressure lamps emit very spectrally narrow lines, which are traditionally used in optical spectroscopy for calibration of spectral position. Commercial calibration lamps are sold for this purpose; reflecting a fluorescent ceiling light into a spectrometer is a common calibration practice. Gaseous mercury is also found in some electron tubes, including ignitrons, thyratrons, and mercury arc rectifiers. It is also used in specialist medical care lamps for skin tanning and disinfection. Gaseous mercury is added to cold cathode argon-filled lamps to increase the ionization and electrical conductivity. An argon-filled lamp without mercury will have dull spots and will fail to light correctly. Lighting containing mercury can be bombarded/oven pumped only once. When added to neon filled tubes the light produced will be inconsistent red/blue spots until the initial burning-in process is completed; eventually it will light a consistent dull off-blue color.\n\nMercury, as thiomersal, is widely used in the manufacture of mascara. In 2008, Minnesota became the first state in the United States to ban intentionally added mercury in cosmetics, giving it a tougher standard than the federal government.\n\nA study in geometric mean urine mercury concentration identified a previously unrecognized source of exposure (skin care products) to inorganic mercury among New York City residents. Population-based biomonitoring also showed that mercury concentration levels are higher in consumers of seafood and fish meals.\n\nMercury(II) fulminate is a primary explosive which is mainly used as a primer of a cartridge in firearms.\n\nMany historic applications made use of the peculiar physical properties of mercury, especially as a dense liquid and a liquid metal:\n\nOthers applications made use of the chemical properties of mercury:\n\nMercury(I) chloride (also known as calomel or mercurous chloride) has been used in traditional medicine as a diuretic, topical disinfectant, and laxative. Mercury(II) chloride (also known as mercuric chloride or corrosive sublimate) was once used to treat syphilis (along with other mercury compounds), although it is so toxic that sometimes the symptoms of its toxicity were confused with those of the syphilis it was believed to treat. It is also used as a disinfectant. Blue mass, a pill or syrup in which mercury is the main ingredient, was prescribed throughout the 19th century for numerous conditions including constipation, depression, child-bearing and toothaches. In the early 20th century, mercury was administered to children yearly as a laxative and dewormer, and it was used in teething powders for infants. The mercury-containing organohalide merbromin (sometimes sold as Mercurochrome) is still widely used but has been banned in some countries such as the U.S.\n\nMercury and most of its compounds are extremely toxic and must be handled with care; in cases of spills involving mercury (such as from certain thermometers or fluorescent light bulbs), specific cleaning procedures are used to avoid exposure and contain the spill. Protocols call for physically merging smaller droplets on hard surfaces, combining them into a single larger pool for easier removal with an eyedropper, or for gently pushing the spill into a disposable container. Vacuum cleaners and brooms cause greater dispersal of the mercury and should not be used. Afterwards, fine sulfur, zinc, or some other powder that readily forms an amalgam (alloy) with mercury at ordinary temperatures is sprinkled over the area before itself being collected and properly disposed of. Cleaning porous surfaces and clothing is not effective at removing all traces of mercury and it is therefore advised to discard these kinds of items should they be exposed to a mercury spill.\n\nMercury can be absorbed through the skin and mucous membranes and mercury vapors can be inhaled, so containers of mercury are securely sealed to avoid spills and evaporation. Heating of mercury, or of compounds of mercury that may decompose when heated, should be carried out with adequate ventilation in order to minimize exposure to mercury vapor. The most toxic forms of mercury are its organic compounds, such as dimethylmercury and methylmercury. Mercury can cause both chronic and acute poisoning.\n\nPreindustrial deposition rates of mercury from the atmosphere may be about 4 ng /(1 L of ice deposit). Although that can be considered a natural level of exposure, regional or global sources have significant effects. Volcanic eruptions can increase the atmospheric source by 4–6 times.\n\nNatural sources, such as volcanoes, are responsible for approximately half of atmospheric mercury emissions. The human-generated half can be divided into the following estimated percentages:\nThe above percentages are estimates of the global human-caused mercury emissions in 2000, excluding biomass burning, an important source in some regions.\n\nRecent atmospheric mercury contamination in outdoor urban air was measured at 0.01–0.02 µg/m. A 2001 study measured mercury levels in 12 indoor sites chosen to represent a cross-section of building types, locations and ages in the New York area. This study found mercury concentrations significantly elevated over outdoor concentrations, at a range of 0.0065 – 0.523 μg/m. The average was 0.069 μg/m.\n\nMercury also enters into the environment through the improper disposal (e.g., land filling, incineration) of certain products. Products containing mercury include: auto parts, batteries, fluorescent bulbs, medical products, thermometers, and thermostats. Due to health concerns (see below), toxics use reduction efforts are cutting back or eliminating mercury in such products. For example, the amount of mercury sold in thermostats in the United States decreased from 14.5 tons in 2004 to 3.9 tons in 2007.\n\nMost thermometers now use pigmented alcohol instead of mercury, and galinstan alloy thermometers are also an option. Mercury thermometers are still occasionally used in the medical field because they are more accurate than alcohol thermometers, though both are commonly being replaced by electronic thermometers and less commonly by galinstan thermometers. Mercury thermometers are still widely used for certain scientific applications because of their greater accuracy and working range.\n\nHistorically, one of the largest releases was from the Colex plant, a lithium-isotope separation plant at Oak Ridge, Tennessee. The plant operated in the 1950s and 1960s. Records are incomplete and unclear, but government commissions have estimated that some two million pounds of mercury are unaccounted for.\n\nA serious industrial disaster was the dumping of mercury compounds into Minamata Bay, Japan. It is estimated that over 3,000 people suffered various deformities, severe mercury poisoning symptoms or death from what became known as Minamata disease.\n\nThe tobacco plant readily absorbs and accumulates heavy metals such as mercury from the surrounding soil into its leaves. These are subsequently inhaled during tobacco smoking. While mercury is a constituent of tobacco smoke, studies have largely failed to discover a significant correlation between smoking and Hg uptake by humans compared to sources such as occupational exposure, fish consumption, and amalgam tooth fillings.\n\nSediments within large urban-industrial estuaries act as an important sink for point source and diffuse mercury pollution within catchments. A 2015 study of foreshore sediments from the Thames estuary measured total mercury at 0.01 to 12.07 mg/kg with mean of 2.10 mg/kg and median of 0.85 mg/kg (n=351). The highest mercury concentrations were shown to occur in and around the city of London in association with fine grain muds and high total organic carbon content. The strong affinity of mercury for carbon rich sediments has also been observed in salt marsh sediments of the River Mersey mean of 2 mg/kg up to 5 mg/kg. These concentrations are far higher than those shown in salt marsh river creek sediments of New Jersey and mangroves of Southern China which exhibit low mercury concentrations of about 0.2 mg/kg.\n\nDue to the health effects of mercury exposure, industrial and commercial uses are regulated in many countries. The World Health Organization, OSHA, and NIOSH all treat mercury as an occupational hazard, and have established specific occupational exposure limits. Environmental releases and disposal of mercury are regulated in the U.S. primarily by the United States Environmental Protection Agency.\n\nToxic effects include damage to the brain, kidneys and lungs. Mercury poisoning can result in several diseases, including acrodynia (pink disease), Hunter-Russell syndrome, and Minamata disease.\n\nSymptoms typically include sensory impairment (vision, hearing, speech), disturbed sensation and a lack of coordination. The type and degree of symptoms exhibited depend upon the individual toxin, the dose, and the method and duration of exposure. Case–control studies have shown effects such as tremors, impaired cognitive skills, and sleep disturbance in workers with chronic exposure to mercury vapor even at low concentrations in the range 0.7–42 μg/m. A study has shown that acute exposure (4–8 hours) to calculated elemental mercury levels of 1.1 to 44 mg/m resulted in chest pain, dyspnea, cough, hemoptysis, impairment of pulmonary function, and evidence of interstitial pneumonitis. Acute exposure to mercury vapor has been shown to result in profound central nervous system effects, including psychotic reactions characterized by delirium, hallucinations, and suicidal tendency. Occupational exposure has resulted in broad-ranging functional disturbance, including erethism, irritability, excitability, excessive shyness, and insomnia. With continuing exposure, a fine tremor develops and may escalate to violent muscular spasms. Tremor initially involves the hands and later spreads to the eyelids, lips, and tongue. Long-term, low-level exposure has been associated with more subtle symptoms of erethism, including fatigue, irritability, loss of memory, vivid dreams and depression.\n\nResearch on the treatment of mercury poisoning is limited. Currently available drugs for acute mercurial poisoning include chelators N-acetyl-D, L-penicillamine (NAP), British Anti-Lewisite (BAL), 2,3-dimercapto-1-propanesulfonic acid (DMPS), and dimercaptosuccinic acid (DMSA). In one small study including 11 construction workers exposed to elemental mercury, patients were treated with DMSA and NAP. Chelation therapy with both drugs resulted in the mobilization of a small fraction of the total estimated body mercury. DMSA was able to increase the excretion of mercury to a greater extent than NAP.\n\nFish and shellfish have a natural tendency to concentrate mercury in their bodies, often in the form of methylmercury, a highly toxic organic compound of mercury. Species of fish that are high on the food chain, such as shark, swordfish, king mackerel, bluefin tuna, albacore tuna, and tilefish contain higher concentrations of mercury than others. As mercury and methylmercury are fat soluble, they primarily accumulate in the viscera, although they are also found throughout the muscle tissue. When this fish is consumed by a predator, the mercury level is accumulated. Since fish are less efficient at depurating than accumulating methylmercury, fish-tissue concentrations increase over time. Thus species that are high on the food chain amass body burdens of mercury that can be ten times higher than the species they consume. This process is called biomagnification. Mercury poisoning happened this way in Minamata, Japan, now called Minamata disease.\n\n140 countries agreed in the Minamata Convention on Mercury by the United Nations Environment Programme (UNEP) to prevent emissions. The convention was signed on 10 October 2013.\n\nIn the United States, the Environmental Protection Agency is charged with regulating and managing mercury contamination. Several laws give the EPA this authority, including the Clean Air Act, the Clean Water Act, the Resource Conservation and Recovery Act, and the Safe Drinking Water Act. Additionally, the Mercury-Containing and Rechargeable Battery Management Act, passed in 1996, phases out the use of mercury in batteries, and provides for the efficient and cost-effective disposal of many types of used batteries. North America contributed approximately 11% of the total global anthropogenic mercury emissions in 1995.\n\nThe United States Clean Air Act, passed in 1990, put mercury on a list of toxic pollutants that need to be controlled to the greatest possible extent. Thus, industries that release high concentrations of mercury into the environment agreed to install maximum achievable control technologies (MACT). In March 2005, the EPA promulgated a regulation that added power plants to the list of sources that should be controlled and instituted a national cap and trade system. States were given until November 2006 to impose stricter controls, but after a legal challenge from several states, the regulations were struck down by a federal appeals court on 8 February 2008. The rule was deemed not sufficient to protect the health of persons living near coal-fired power plants, given the negative effects documented in the EPA Study Report to Congress of 1998. However newer data published in 2015 showed that after introduction of the stricter controls mercury declined sharply, indicating that the Clean Air Act had its intended impact.\n\nThe EPA announced new rules for coal-fired power plants on 22 December 2011. Cement kilns that burn hazardous waste are held to a looser standard than are standard hazardous waste incinerators in the United States, and as a result are a disproportionate source of mercury pollution.\n\nIn the European Union, the directive on the Restriction of the Use of Certain Hazardous Substances in Electrical and Electronic Equipment (see RoHS) bans mercury from certain electrical and electronic products, and limits the amount of mercury in other products to less than 1000 ppm. There are restrictions for mercury concentration in packaging (the limit is 100 ppm for sum of mercury, lead, hexavalent chromium and cadmium) and batteries (the limit is 5 ppm). In July 2007, the European Union also banned mercury in non-electrical measuring devices, such as thermometers and barometers. The ban applies to new devices only, and contains exemptions for the health care sector and a two-year grace period for manufacturers of barometers.\nNorway enacted a total ban on the use of mercury in the manufacturing and import/export of mercury products, effective 1 January 2008. In 2002, several lakes in Norway were found to have a poor state of mercury pollution, with an excess of 1 µg/g of mercury in their sediment.\nIn 2008, Norway’s Minister of Environment Development Erik Solheim said: \"Mercury is among the most dangerous environmental toxins. Satisfactory alternatives to Hg in products are\navailable, and it is therefore fitting to induce a ban.\"\n\nProducts containing mercury were banned in Sweden in 2009.\n\nIn 2008, Denmark also banned dental mercury amalgam, except for molar masticating surface fillings in permanent (adult) teeth.\n\n\n\n"}
{"id": "52175691", "url": "https://en.wikipedia.org/wiki?curid=52175691", "title": "Ministry of Energy (Poland)", "text": "Ministry of Energy (Poland)\n\nMinistry of Energy (Polish: \"Ministerstwo Energii\") is the office of government in Poland responsible for energy policy and the management of mineral deposits. Krzysztof Tchórzewski is the current Minister of Energy. It was created in late 2015 from the split of the Ministry of Infrastructure and Development.\n"}
{"id": "42478068", "url": "https://en.wikipedia.org/wiki?curid=42478068", "title": "Moduli stack of formal group laws", "text": "Moduli stack of formal group laws\n\nIn algebraic geometry, the moduli stack of formal group laws is a stack classifying formal group laws and isomorphisms between them. It is denoted by formula_1. It is a \"geometric “object\" that underlies the chromatic approach to the stable homotopy theory, a branch of algebraic topology.\n\nCurrently, it is not known whether formula_1 is a derived stack or not. Hence, it is typical to work with stratifications. Let formula_3 be given so that formula_4 consists of formal group laws over \"R\" of height exactly \"n\". They form a stratification of the moduli stack formula_1. formula_6 is faithfully flat. In fact, formula_3 is of the form formula_8 where formula_9 is a profinite group called the Morava stabilizer group. The Lubin–Tate theory describes how the strata formula_3 fit together.\n"}
{"id": "22232389", "url": "https://en.wikipedia.org/wiki?curid=22232389", "title": "Nevada Renewable Energy and Energy Efficiency Authority", "text": "Nevada Renewable Energy and Energy Efficiency Authority\n\nThe Nevada Renewable Energy and Energy Efficiency Authority was a state government agency in Nevada charged with promoting energy conservation and use of renewable energy sources. Its administrator held the title of Nevada Energy Commissioner.\n\nThe Authority was created by the state legislature in 2009. The Authority was eliminated in 2011, and its responsibilities were transferred to the State Office of Energy.\n\nIn 2009 Governor Jim Gibbons appointed the first Energy Commissioner, Hatice Gecol.\n"}
{"id": "566327", "url": "https://en.wikipedia.org/wiki?curid=566327", "title": "Oymyakon", "text": "Oymyakon\n\nOymyakon (, ; , \"Öymököön\", ) is a rural locality (a \"selo\") in Oymyakonsky District of the Sakha Republic, Russia, located along the Indigirka River, northwest of Tomtor on the Kolyma Highway. It is one of the coldest permanently inhabited locales on Earth.\n\nIt is named after the Oymyakon River, whose name reportedly comes from the Even word \"kheium\", meaning \"unfrozen patch of water; place where fish spend the winter.\" However, another source states that the Even word \"heyum\" (hэjум) (\"kheium\" may be a misspelling) means \"frozen lake\".\n\nOymyakon has two main valleys beside it. These valleys trap wind inside the town and create the colder climate. However, children are still allowed to go to school if it is warmer than .\n\nDuring World War II, an airfield was built there for the Alaska-Siberian (ALSIB) air route used to ferry American Lend-Lease aircraft to the Eastern Front.\n\nOver the last few decades, the population of Oymyakon has shrunk considerably. The village had a peak population of around 2,500 inhabitants when it was a central town of the region, but that number has dwindled to less than 900 in 2018.\n\nWith an extreme subarctic climate (Köppen climate classification \"Dfd/Dwd\"), Oymyakon is known as one of the places considered the Northern Pole of Cold, the others being the rural locality of Delyankir, also located in the Oymyakonsky district of the Sakha Republic, and the town of Verkhoyansk, located 629 km (391 miles) away by air. The ground is permanently frozen (continuous permafrost).\n\nThere is a monument built around the town square commemorating a reading in the 1920s of −71.2. This was shown on the Australian programme \"60 Minutes\" in a 2012 documentary. On February 6, 1933, a temperature of was recorded at Oymyakon's weather station. This was the coldest officially recorded temperature in the Northern Hemisphere. Only Antarctica has recorded lower official temperatures (the lowest being , recorded at Vostok Station on 21 July 1983).\n\nThe weather station is in a valley between Oymyakon and Tomtor. The station is at and the surrounding mountains at , causing cold air to pool in the valley: in fact, recent studies show that winter temperatures in the area \"increase\" with altitude by as much as 10 °C (18 °F).\n\nSome years the temperature drops below in late September and may remain below freezing until mid-May. In Oymyakon sometimes the average minimum temperature for January, February and December remains below . Sometimes summer months can also be quite cold, but June and July are the only months where the temperature has never dropped below . Oymyakon and Verkhoyansk are the only two permanently inhabited places in the world that have recorded temperatures below for every day in January.\n\nOymyakon has never recorded an above-freezing temperature between October 25 and March 17.\n\nAlthough winters in Oymyakon are long and excessively cold, summers are mild to warm, sometimes hot.\nThe warmest month on record is July 2010 with an average temperature of . In June, July and August temperatures over are not rare during the day. On July 28, 2010, Oymyakon recorded a record high temperature of , yielding a temperature range of 102.3 °C (184.1 °F). Verkhoyansk,Yakutsk, and Delyankir are the only other places in the world with a temperature amplitude higher than 100 °C (180 °F).\n\nThe climate is quite dry, but as average monthly temperatures are below freezing for seven months of the year, substantial evaporation occurs only in summer months. Summers are much wetter than winters.\n\nOymyakon has been featured in a number of television series:\n\n\n"}
{"id": "26778836", "url": "https://en.wikipedia.org/wiki?curid=26778836", "title": "PACE financing", "text": "PACE financing\n\nProperty Assessed Clean Energy (PACE) is a means of financing energy efficiency upgrades or renewable energy installations for residential, commercial and industrial property owners. Depending on state legislation, PACE can be used to finance building envelope energy efficiency improvements like insulation and air sealing, cool roofs, water efficiency products, seismic retrofits, and hurricane preparedness measures. In some states, commercial PACE can also fund a portion of new construction projects, as long as the building owner agrees to build the new structure to exceed the local energy code. \n\nExamples of energy efficiency and renewable energy upgrades range from adding more attic insulation to installing rooftop solar panels for residential projects and chillers, boilers, LED lighting and roofing for commercial projects. In areas with PACE legislation in place, governments offer a specific bond to investors or in the case of the open-market model, private lenders provide financing to the building owners to put towards an energy retrofit. The loans are repaid over the selected term (over the course of somewhere between 5 and 25 years) via an annual assessment on their property tax bill. PACE bonds can be issued by municipal financing districts, state agencies or finance companies and the proceeds can be used to retrofit both commercial and residential properties. One of the most notable characteristics of PACE programs is that the loan is attached to the property rather than an individual. A PACE loan is therefore said to be nonrecourse to the borrower.\n\nPACE can also be used to finance leases and power purchase agreements (PPAs). In this structure, the PACE property tax assessment is used to collect a lease payment of services fee. The primary benefit of this approach is that project costs may be lower due to the provider retaining the tax incentives and passing the benefit on to the property owner as a lower lease or services payment.\n\nPACE programs help home and business owners pay for the upfront costs of green initiatives, such as solar panels, which the property owner then pays back by increasing property taxes by a set rate for an agreed-upon term ranging from 5–25 years. This allows property owners to begin saving on energy costs while they are paying for their solar panels. This usually means that property owners have net gains even with increased property tax.\n\nVoluntary assessments for repaying municipal bonds have been attached to property taxes since the early 1800s to fund projects for public good such as sidewalks, fire stations, and street lighting. PACE uses the same concept, but for projects that benefit the private sector, or individual building owners. PACE was originally known as a \"Special Energy Financing District\" or \"on-tax bill solar and efficiency financing.\" The concept was first conceived and proposed in the Monterey Bay Regional Energy Plan in 2005 but followed voter approval of a similar solar bonds program approved by San Francisco voters in 2001. The concept was designed to overcome one of the most significant barriers to solar and costly energy efficiency retrofits: up-front costs. A homeowner could spend tens of thousands of dollars on a solar photovoltaic system, upgrading windows to be more energy efficient or adding insulation throughout the home, yet all of these investments would not likely be recovered when the home was sold. PACE enables the homeowner to \"mortgage\" these improvements and pay only for the benefits they derive while they own the home.\n\nThe first PACE program was implemented by Berkeley, California, led by Cisco DeVries, the chief of staff to Berkeley's mayor and then-CEO of RenewableFunding. University of California, Berkeley led the development of the program via the \"Guide to Energy Efficiency & Renewable Energy Financing Districts for Local Governments\" contributed by Cisco DeVries, Ann Livingston and Matthew Brown. The guide was funded through grants to the City of Berkeley from the U.S. Environmental Protection Agency. Berkeley's PACE program was recommended as an alternative to the solar bonds authority approved by neighboring San Francisco voters in 2001 in conjunction with the City's Community Choice Aggregation program, which is being implemented in both San Francisco and Sonoma counties. DeVries saw PACE as a way to provide a viable means to help achieve the Bay Area's climate goals. California passed the first legislation for PACE financing and started the BerkeleyFIRST climate program in 2008. Since then, PACE-enabling legislation has been passed in 31 states and the District of Columbia, allowing localities to establish PACE financing programs.\n\nHowever, PACE financing for residential properties was dealt a serious blow in 2010 when Fannie Mae and Freddie Mac refused to back mortgages with PACE liens on them. In August 2015, the Department of Housing and Urban Development (HUD) announced that it intends to require liens created by energy retrofit programs to remain subordinate to loans guaranteed by the Federal Housing Administration (FHA) and that it would be issuing guidance on how to handle the transfer and sale of homes with a PACE assessment. HUD clarified its policy in 2017 by stating properties with PACE obligations were ineligible for FHA financing.\n\nFor a city, PACE can play an important role in reducing local greenhouse gas emissions, promoting energy efficiency improvements in its buildings, making the shift to renewable sources of energy more affordable, and reducing energy costs for residents and businesses. Because PACE is funded through private lending or municipal bonds, it creates no liability to the city's funds. Additionally, most PACE programs made possible through public-private partnerships rely on private capital to source financing. PACE also enables states and local governments to design and implement such programs in their communities. PACE programs also help to create jobs and thus spur local economic development when local solar installers and renewable energy companies partner with the program. It is also an opt-in program, so only those property owners who choose to participate are responsible for the costs of PACE financing.\n\nPACE enables individuals and businesses to defer the upfront costs that are the most common barrier to energy efficiency or renewables installations. The PACE loans are paid by additional assessments on the property owner's property taxes over an agreed upon term while energy costs are simultaneously lower, providing the PACE consumer with net gains. Also, because the solar panels and the PACE loan is attached to the property, the consumer can sell the property leaving the debt to be paid through the property tax assessed on the subsequent owners.\n\nFor consumers, PACE type programs have several problems. Because the financing is designed to stay with the property, eligibility is based primarily on property information rather than income and FICO scores. A homeowner’s ability to pay is currently based primarily on their mortgage and property tax payment history as well as the requirement that there are no recent bankruptcies. Most significantly, homeowners are financed for the home improvements without any assessment of whether the financing is affordable for the homeowner, though Senate Bill 242, currently being considered in California, would require PACE providers to conduct an income review. Because the PACE financing is structured as a tax assessment instead of a loan, the PACE programs historically have not had to provide to homeowners the same disclosures about the financing costs that traditional lenders must provide. In September 2016, California Governor Jerry Brown signed AB2693 into law, which required PACE programs to provide mortgage-level disclosures and some providers to conduct live recorded calls with the homeowner to confirm financing terms and obligation. In some cases, without either an assessment of affordability or these disclosures about the costs of the financing, homeowners depend on what the PACE program providers tell them when trying to figure out whether the financing is affordable. Homeowners have complained that PACE contractors are lying about the costs of financing as part of selling the program. These problems create a situation in which homeowners can suddenly owe far more in property taxes than they can afford to repay; this is especially true for retired and disabled homeowners on fixed incomes. PACE architects Cisco DeVries and Matthew Brown deny these claims as \"isolated incidents.\" \n\nInterest rates for PACE programs are usually 3-4% higher than for traditional mortgage loans, with additional administrative fees close to 5%, but compare well to many lending options such as credit cards and HELOCs, without tying up credit lines. \n\nMany buyers and sellers have had difficulty with sales of homes with PACE tax assessments. Though the PACE assessment typically appears on the title report, some buyers find out about the assessments after the sale, which may force them to pay money out-of-pocket unexpectedly. In some cases, sellers have agreed to pay off the PACE assessment or lower the sale price to compensate for the PACE tax assessment. When the remaining payments are unable to be assumed by the new buyer, this renders the PACE financing to a position more similar to that of a traditional lending product.\n\nSome homeowners have reported that there were massive differences in the \"lump\" sum payment required at the first assessment as well as \"no where near\" the reported monthly costs to increase in escrow.\n\nA problem with PACE for both residential lenders and consumers is that the tax liens for PACE financing take priority over other lien-holders, and those lien-holders may not be notified or given an opportunity to object. Commercial PACE is less problematic because priority lien-holders for those properties are notified before hand. Fannie Mae and Freddie Mac have refused to purchase or underwrite loans for properties with existing PACE-based tax assessments, but in mid-2016 the Veterans Benefits Administration (VA) announced guidance for how to manage the financing of properties with PACE obligations. Properties encumbered with PACE obligations are not eligible for Federal Housing Administration (FHA) insured financing. \n\nBonds associated with PACE assessments can be packaged and securitized. Securitization, which was developed in the mortgage industry, works by pooling a series of assets, such as mortgages, and selling notes backed by these assets to investors. Because these bonds are for property improvements which achieve a positive environmental impact, some PACE providers have had their bonds green certified. PACE bonds are unique amongst the green bond market because products rated as efficient are reducing carbon emissions as soon as they are installed.\n\nPACE is enabled in 35 states and the District of Columbia, covering more than 80% of the U.S. population.\n"}
{"id": "35005263", "url": "https://en.wikipedia.org/wiki?curid=35005263", "title": "Paullinic acid", "text": "Paullinic acid\n\nPaullinic acid is an omega-7 fatty acid find in a variety of plant sources, including guarana (\"Paullinia cupana\") from which it gets its name. It is one of a number of eicosenoic acids.\n\n"}
{"id": "19338927", "url": "https://en.wikipedia.org/wiki?curid=19338927", "title": "Phospholipid-derived fatty acids", "text": "Phospholipid-derived fatty acids\n\nPhospholipid-derived fatty acids (PLFA) are widely used in microbial ecology as chemotaxonomic markers of bacteria and other organisms. Phospholipids are the primary lipids composing cellular membranes. Phospholipids can be saponified, which releases the fatty acids contained in their diglyceride tail. Once the phospholipids of an unknown sample are saponified, the composition of the resulting PLFA can be compared to the PLFA of known organisms to determine the identity of the sample organism. PLFA analysis may be combined with other techniques, such as stable isotope probing to determine which microbes are metabolically active in a sample. PLFA analysis was pioneered by D.C. White, MD, PhD, at the University of Tennessee, in the early to mid 1980s.\n\nPhospholipid fatty acids (PLFA) are an essential structural component of all microbial cellular membranes. PLFA analysis is a technique widely used for estimation of the total biomass and to observe broad changes in the community composition of the living microbiota of soil and aqueous environments. There has been a surge of interest in PLFAs in recent years, evident from the large increase in peer-reviewed journal references on the subject. However, there is increasing concern that some researchers are assigning PLFAs to specific microbial classes when in fact those PLFAs are present in a broad range of life forms. Phospholipids can occur in many biological classes (such as in plant roots, fungi, as well as in soil bacteria), so care has to be taken in over-assigning PLFA biomarkers to the wrong class. Even though phospholipids occur in many different life forms, the fatty acid side chains between differing life forms can be quite unique. Polyunsaturated fatty acids (\"e.g.\" 18:3 ω3c) are found in plants, algae and cyanobacteria, but are often not present in bacteria. Monounsaturated fatty acids (particularly at the omega-7 position), odd-chain saturated fatty acids (\"e.g.\" 15:0), branched-chain fatty acids (mainly iso/anetiso and 10-methyl) and cyclopropane fatty acids (\"e.g.\" 19:0 cyclo ω7c) are mostly synthesized by bacteria. The monounsaturated fatty acid, 16:1 ω5c, is mostly synthesized by Arbuscular mycorrhizal fungi (AMF) and the polyunsaturated fatty acid, 18:2 ω6c (Linoleic acid), is mostly synthesized by Ectomycorrhizal fungi.\n\nThe basic premise is that as individual organisms (especially bacteria and fungi) die, phospholipids are rapidly degraded and the remaining phospholipid content of the sample is assumed to be from living organisms. As the phospholipids of different groups of bacteria and fungi contain a variety of somewhat unusual fatty acids, they can serve as useful biomarkers for such groups. PLFA profiles and composition can be determined by purifying the phospholipids and then cleaving the fatty acids for further analysis. Knowledge of the composition and metabolic activity of the microbiota in soils, water and waste materials is useful in optimizing crop production, in bioremediation and in understanding microbial ecosystems. Soil microbial community analysis by PLFA has been a widely used technique due to the sensitive, reproducible measurement of the dominant portions of the soil microbiota and the fact that PLFA does not require cultivation of the organisms. Sampling of soil populations by culturing has proven not cost effective and results in biased results due to the differing ease of culturing of some organisms. The main drawback of PLFA has been that the extraction time is very long and cumbersome. A new 96-well plate PLFA extraction procedure has been developed which represents a 4-to-5 fold increase in throughput over traditional PLFA extraction methods. This new method, coupled to new software tools for analyzing the PLFA data, will be useful to laboratories performing large numbers of PLFA analyses, or for laboratories wanting to begin PLFA research.\n\n\n\n\n\nEarly studies of the living soil microbial communities were largely based on attempts at culturing bacteria and fungi of soil. However, due to difficulty in culturing many of the organisms, the differential growth rates of the organisms, and labor involved, this proved to be not satisfactory. A 1965 article proposed using molecules produced by the organisms as biomarkers for the microbial communities. In the following two decades, rapid progress was made in development of gas chromatographs (GC) and of fused silica capillary columns for the GC instruments, enabling better analysis of biological materials, including fatty acid methyl esters (FAMEs). PLFA analysis can be used for microbial community structure and activity through the use of “signature” fatty acids. The basic concept is that the phospholipid content represents living organisms as these compounds are rapidly decomposed in aerobic mixed communities and that some of the neutral lipid components such as the lipopolysaccharides of Gram-negative bacteria do not reflect organisms alive at the time of sampling.\n\nAlthough the method of sample collection is different for soil, water samples, etc., the extraction-derivatization is generally similar to the following protocol from an article on soil microbial communities. The lipids were extracted from the dried soil sample by use of a chloroform-methanol-phosphate buffer mixture by use of a brief sonication followed by shaking for 2 hours and centrifugation to pellet the soil material. The liquid above the soil had additional chloroform and water added to cause separation of the lipid-containing chloroform from the buffer/methanol phase. The lipids were fractionated on a solid-phase extraction column and the neutral lipids, free fatty acids and other materials discarded and the phospholipid phase then dried, prior esterification to form the fatty acid methyl esters (FAMEs) to make them suitable for analysis.\n\nAnalysis by gas chromatography (GC) is the method of choice for FAME analysis. The GC is coupled with either a mass spectrometer detector (MSD) or a flame ionization detector (FID). The GC-MSD system is more expensive to purchase and to maintain as well as requiring considerable skill in operation. Identification of fatty acids using the GC-FID system is dependent on comparison of retention times of the compounds in comparison to purchased standards of fatty acid esters. A commercially available, fatty-acid based microbial identification system (using GC-FID), which reproducibly names and quantitates the FAMEs, has been widely adopted for PLFA analysis.\n\nActinomycetes are Gram-positive bacteria that are some of the most common bacteria in soil, freshwater and marine environments. Actinomycetes are active in decomposition of organic matter and give rise to the rich “earthy” smell of freshly tilled soils. This group of bacteria produce distinctive biomarker fatty acids having a methyl branch at the 10th carbon, such as 16:0 10-methyl and 18:0 10-methyl. Some common species of soil actinomycetes include \"Rhodococcus, Nocardia, Corynebacterium \"and\" Streptomyces. \"\n\nGram-positive bacteria include aerobic \"Bacillus\" species especially those related to \"B. cereus \"and to\" B. subtilis.\" These bacteria are common in the bulk soil and increase in numbers in the rhizosphere. The PLFA profiles of these Gram-positive species have high percentages of biomarker branched-chain fatty acids such as 15:0 iso and 15:0 anteiso. Thus, the sum of the iso and anteiso fatty acids in a PLFA analysis may provide an estimate of the abundance of the Gram-positive bacteria (other than actinomycetes) in the sample.\n\nGram-negative bacteria are a major component of the plant rhizosphere and improve plant growth by increasing solubility of phosphate, producing ionophore compounds that increase uptake of iron or other minerals and may produce antifungal compounds. Gram-negative bacteria produce high levels of monounsaturated fatty acids (\"e.g.\" 16:1 omega-7 and 18:1 omega-9) during active metabolism but convert much of the unsaturated fatty acid composition to cyclopropane fatty acids (\"e.g.\" 17:0 cyclopropane and 19:0 cyclopropane) when metabolism and cell division slow due to shortage of nutrition or other stress. Thus, in PLFA analysis, the sum of monounsaturated and cyclopropane fatty acids may provide an estimate of the abundance of Gram-negative bacteria. A high ratio of cyclopropane to monounsaturated fatty acid indicates stress conditions.\n\nAnaerobic bacteria in agriculture are primarily a factor in soils of low oxygen levels such as occur in greater depths or of wet conditions such as in rice paddies. Using PLFA analysis in early sampling, the bacteria- archaea consortia in rice paddy soil was about 44% aerobic bacteria, 32% facultatively anaerobic- bacteria and 24% archaea. Under longer term flooding, the levels were 27%, 36% and 37% respectively and with total biomass being significantly lower. Dimethyl acetals (DMA) formed during derivatization are considered to be biomarkers of anaerobic bacteria.\n\nArchaea are universally distributed in soils and have been shown to control nitrification in acidic conditions and to contribute to ammonia oxidation in agricultural and forest soils. However, as the phospholipids of archaea are not ester linked as in bacteria, but are ether linked, they are not significantly present in routine PLFA sample preparation which is designed to cleave ester-linked fatty acids.\n\nArbuscular mycorrhizae fungi (AMF) penetrate the walls of cortical cells of about 80% of all vascular plant families, generating a symbiotic relationship. The fungi form membrane structures adjacent to the plant cell membrane allowing exchange of phosphorus, nitrogen compounds and minerals from the fungus and the plant provides the fungus primarily with photosynthesis-derived sugars. As the AMF are obligate symbiotic fungi, they are not free-living in the soil. The AMF hyphae in the root form lipid materials which then are transported to the hyphae that extend into the soil from the root and thus may occur in a soil sample. Vesicles are lipid storage organs of AMF and these and the hyphae in the soil contain the fatty acids 18:2 w6c (often used as an indicator of fungal content of the PLFA analysis) as well as containing the fatty acid 16:1 w5c which has been recommended as a biomarker for AMF.\n\nSampling of agricultural soils for analysis of chemical composition (\"e.g.\" pH, N, P, K, Ca, Mg, \"etc\".) has long been practiced in crop production and while there has been recognition of the importance of the soil microbiota, tools for studying the microbiota have been developed relatively recently.\n\nMany high-value vegetable crops easily justify soil testing both for chemical content and the soil microbiota. Conventional, low-input and organic farming systems showed a rapid response of the soil microbial communities to wet/dry cycles and that increases in bacterial cyclopropyl fatty acids were useful to detect periods of stress. Lines of transgenic corn (maize) expressing \"Bacillus thuringiensis\" endotoxins were found to have small effect on soil microbial communities when compared by PLFA analysis to their non-transgenic isolines. Successful exotic invasive plant species can have profound effects on the microbial communities of the soil perhaps thus improving their competitiveness. Grassland restoration practices of tillage, weeding and herbicide use showed an impact on microbial communities of the upper soil but very small changes on the microbiota of lower soil layers and that after 4 years of recovery the communities were very similar to untreated plots.\n\nBioremediation has been studied using PLFA analysis of soil microbiota from sites contaminated by diesel oil, crude oil, explosives, olive mill waste, pentachlorophenol, coal tar and PCBs. There are reports of the effects on PLFAs of heavy metals on arbuscular fungi and on bacteria, of polycyclic aromatic hydrocarbons on rice paddy bacteria and of methylene chloride on bacteria.\n\nPhytoplankton (eukaryotic algae) are microscopic photosynthesizing plants that inhabit the sunlit layers of oceans and bodies of freshwater. As the primary source of elaborated carbon compounds, they are vital to the aquatic food web. Phytoplankton produce considerable amounts of polyunsaturated fatty acids (PUFA), including Eicosapentaenoic acid (EPA, 20:5 w3c), with microalgae being the origin of omega-3 fatty acids in fish oil. The diverse taxonomic groups in algae vary in abundance dependent on environmental conditions such as temperature, salinity, sunlight, and nutrient availability. The PLFA biomarker compositions were found to enable determination of the prevalence of the major groups in several marine environments. In a study of reservoir sedimentary deposits, an assumption was made that the community PUFA content constituted \"ca\". 50% of the total microeukaryotic PLFAs. It was also assumed that “The ratio of omega-3 to omega-6 fatty acids describes the relative contribution of phototrophic to heterotrophic members of the microeukaryotic community…”.\n\nIn contrast to the considerable microbial diversity in soils, free-living microbes distributed by marine currents and exposed to algal exudates exhibit global distributions for a few dominant microbial groups of relatively few species. Streambed sediments displayed a variation in microbial community structure (as measured by PLFA) related to the forest environment and geographic location of the stream, with much of the variation determined by use of the algal biomarker fatty acid 18:3 w3. By PLFA analysis, considerable spatial and seasonal variations were determined in a freshwater reservoir sedimentary microbial community.\n\nConiferous forests are dependent on available nutrients in soil rather than agricultural fertilizers and thus are routinely colonized by symbiotic mycorrhizal fungi. The mycorrhizae may be ectomycorrhizae (EMF) and/or arbuscular (AMF) in type in the forest. The amount of total PLFA in soil provides an estimate of the total soil fungi (not including AMF). The AMF can be estimated by the amount of 16:1 w5c fatty acid in the PLFA. Water stress was indicated by an increase in [PLFA ratios of saturated, monounsaturated and (cyclo 17:0 + cyclo 19:0) / (16:1 w7c + 18:1 w7c)] in a Douglas fir forest. Boreal forests with low soil pH values had elevated EM PLFAs and raising the pH of the soil increased bacterial PLFAs. The introduction of photosynthates through tree roots is a major source of carbon for soil microbiota and influences the composition of fungal and bacterial communities. Forest areas without tree roots had less fungal biomarkers and more actinobacterial biomarkers than areas with tree roots. Addition of nitrogen fertilizer to an oak forest reduced the ectomycorrhizal fungal content of the soil microbiota.\n\nComposting of organic materials is the microbial degradation of heterogeneous organic material under moist, self-heating, aerobic conditions. Initially, activity by mesophilic organisms leads to a rapid rise in temperature, followed by thermophilic organisms dominating the degradation process leading to a cooling period in which mesophilic bacteria again dominate populations. A commercial FAME extraction protocol developed for identification of bacteria, a mild alkaline methanolysis protocol and PLFA-extraction/derivatization were compared for effectiveness. The PLFA protocol gave the most detailed information about community succession, however, the other two protocols were much simpler and appeared suitable for analysis of microbial FAME profiles in compost.\n\nActivated sludge technology is the most widely used method for wastewater treatment. Complex microbial communities in activated sludge processes are needed for the stable removal efficiency of organic pollutants. PLFA analysis can be used to monitor the microbial community composition of activated sludge reactors, which microbial groups are predominant, and the efficiency of such systems.\n"}
{"id": "605697", "url": "https://en.wikipedia.org/wiki?curid=605697", "title": "Photohydrogen", "text": "Photohydrogen\n\nPhotohydrogen is hydrogen produced with the help of artificial or natural light This is how the leaf of a tree splits water molecules into protons (hydrogen ions), electrons (to make carbohydrates) and oxygen (released into the air as a waste product). Photohydrogen may also be produced by the photodissociation of water by ultraviolet light.\n\nPhotohydrogen is sometimes discussed in the context of obtaining renewable energy from sunlight, by using microscopic organisms such as bacteria or algae. These organisms create hydrogen with the help of hydrogenase enzymes which convert protons derived from the water splitting reaction into hydrogen gas which can then be collected and used as a biofuel.\n\n"}
{"id": "33752698", "url": "https://en.wikipedia.org/wiki?curid=33752698", "title": "Rebecca Bell", "text": "Rebecca Bell\n\nRebecca Bell (born 1953) is an environmental educational specialist from Maryland, United States. Bell was a leader at bringing environmental issues to Maryland public school curriculum and has been honored as a Maryland Middle School Science Teacher of the Year. She has worked for the National Oceanic and Atmospheric Administration's Teacher at Sea program which works with scientists to monitor changes in ecosystems. Bell also serves on Maryland Governor Martin O'Malley's Climate Change Commission. In 2009 she was designated Women's History Month Honoree by the National Women's History Project.\n"}
{"id": "25784", "url": "https://en.wikipedia.org/wiki?curid=25784", "title": "Renewable energy", "text": "Renewable energy\n\nRenewable energy is energy that is collected from renewable resources, which are naturally replenished on a human timescale, such as sunlight, wind, rain, tides, waves, and geothermal heat. Renewable energy often provides energy in four important areas: electricity generation, air and water heating/cooling, transportation, and rural (off-grid) energy services.\n\nBased on REN21's 2017 report, renewables contributed 19.3% to humans' global energy consumption and 24.5% to their generation of electricity in 2015 and 2016, respectively. This energy consumption is divided as 8.9% coming from traditional biomass, 4.2% as heat energy (modern biomass, geothermal and solar heat), 3.9% hydro electricity and 2.2% is electricity from wind, solar, geothermal, and biomass. Worldwide investments in renewable technologies amounted to more than US$286 billion in 2015, with countries such as China and the United States heavily investing in wind, hydro, solar and biofuels. Globally, there are an estimated 7.7 million jobs associated with the renewable energy industries, with solar photovoltaics being the largest renewable employer. As of 2015 worldwide, more than half of all new electricity capacity installed was renewable.\n\nRenewable energy resources exist over wide geographical areas, in contrast to other energy sources, which are concentrated in a limited number of countries. Rapid deployment of renewable energy and energy efficiency is resulting in significant energy security, climate change mitigation, and economic benefits. The results of a recent review of the literature concluded that as greenhouse gas (GHG) emitters begin to be held liable for damages resulting from GHG emissions resulting in climate change, a high value for liability mitigation would provide powerful incentives for deployment of renewable energy technologies. In international public opinion surveys there is strong support for promoting renewable sources such as solar power and wind power. At the national level, at least 30 nations around the world already have renewable energy contributing more than 20 percent of energy supply. National renewable energy markets are projected to continue to grow strongly in the coming decade and beyond. Some places and at least two countries, Iceland and Norway generate all their electricity using renewable energy already, and many other countries have the set a goal to reach 100% renewable energy in the future. For example, in Denmark the government decided to switch the total energy supply (electricity, mobility and heating/cooling) to 100% renewable energy by 2050.\n\nWhile many renewable energy projects are large-scale, renewable technologies are also suited to rural and remote areas and developing countries, where energy is often crucial in human development. Former United Nations Secretary-General Ban Ki-moon has said that renewable energy has the ability to lift the poorest nations to new levels of prosperity. As most of renewables provide electricity, renewable energy deployment is often applied in conjunction with further electrification, which has several benefits: Electricity can be converted to heat (where necessary generating higher temperatures than fossil fuels), can be converted into mechanical energy with high efficiency and is clean at the point of consumption. In addition to that electrification with renewable energy is much more efficient and therefore leads to a significant reduction in primary energy requirements, because most renewables don't have a steam cycle with high losses (fossil power plants usually have losses of 40 to 65%).\n\nRenewable energy systems are rapidly becoming more efficient and cheaper. Their share of total energy consumption is increasing. Growth in consumption of coal and oil could end by 2020 due to increased uptake of renewables and natural gas.\n\nRenewable energy flows involve natural phenomena such as sunlight, wind, tides, plant growth, and geothermal heat, as the International Energy Agency explains:\nRenewable energy resources and significant opportunities for energy efficiency exist over wide geographical areas, in contrast to other energy sources, which are concentrated in a limited number of countries. Rapid deployment of renewable energy and energy efficiency, and technological diversification of energy sources, would result in significant energy security and economic benefits. It would also reduce environmental pollution such as air pollution caused by burning of fossil fuels and improve public health, reduce premature mortalities due to pollution and save associated health costs that amount to several hundred billion dollars annually only in the United States. Renewable energy sources, that derive their energy from the sun, either directly or indirectly, such as hydro and wind, are expected to be capable of supplying humanity energy for almost another 1 billion years, at which point the predicted increase in heat from the sun is expected to make the surface of the earth too hot for liquid water to exist.\n\nClimate change and global warming concerns, coupled with high oil prices, peak oil, and increasing government support, are driving increasing renewable energy legislation, incentives and commercialization. New government spending, regulation and policies helped the industry weather the global financial crisis better than many other sectors. According to a 2011 projection by the International Energy Agency, solar power generators may produce most of the world's electricity within 50 years, reducing the emissions of greenhouse gases that harm the environment.\n\nAs of 2011, small solar PV systems provide electricity to a few million households, and micro-hydro configured into mini-grids serves many more. Over 44 million households use biogas made in household-scale digesters for lighting and/or cooking, and more than 166 million households rely on a new generation of more-efficient biomass cookstoves. United Nations' Secretary-General Ban Ki-moon has said that renewable energy has the ability to lift the poorest nations to new levels of prosperity. At the national level, at least 30 nations around the world already have renewable energy contributing more than 20% of energy supply. National renewable energy markets are projected to continue to grow strongly in the coming decade and beyond, and some 120 countries have various policy targets for longer-term shares of renewable energy, including a 20% target of all electricity generated for the European Union by 2020. Some countries have much higher long-term policy targets of up to 100% renewables. Outside Europe, a diverse group of 20 or more other countries target renewable energy shares in the 2020–2030 time frame that range from 10% to 50%.\n\nRenewable energy often displaces conventional fuels in four areas: electricity generation, hot water/space heating, transportation, and rural (off-grid) energy services:\n\nPrior to the development of coal in the mid 19th century, nearly all energy used was renewable. Almost without a doubt the oldest known use of renewable energy, in the form of traditional biomass to fuel fires, dates from 790,000 years ago. Use of biomass for fire did not become commonplace until many hundreds of thousands of years later, sometime between 200,000 and 400,000 years ago. Probably the second oldest usage of renewable energy is harnessing the wind in order to drive ships over water. This practice can be traced back some 7000 years, to ships in the Persian Gulf and on the Nile. Moving into the time of recorded history, the primary sources of traditional renewable energy were human labor, animal power, water power, wind, in grain crushing windmills, and firewood, a traditional biomass. A graph of energy use in the United States up until 1900 shows oil and natural gas with about the same importance in 1900 as wind and solar played in 2010.\n\nIn the 1860s and '70s there were already fears that civilization would run out of fossil fuels and the need was felt for a better source. In 1873 Professor Augustin Mouchot wrote:\n\nThe time will arrive when the industry of Europe will cease to find those natural resources, so necessary for it. Petroleum springs and coal mines are not inexhaustible but are rapidly diminishing in many places. Will man, then, return to the power of water and wind? Or will he emigrate where the most powerful source of heat sends its rays to all? History will show what will come.\n\nIn 1885, Werner von Siemens, commenting on the discovery of the photovoltaic effect in the solid state, wrote:\n\nIn conclusion, I would say that however great the scientific importance of this discovery may be, its practical value will be no less obvious when we reflect that the supply of solar energy is both without limit and without cost, and that it will continue to pour down upon us for countless ages after all the coal deposits of the earth have been exhausted and forgotten.\n\nMax Weber mentioned the end of fossil fuel in the concluding paragraphs of his Die protestantische Ethik und der Geist des Kapitalismus, published in 1905.\n\nDevelopment of solar engines continued until the outbreak of World War I. The importance of solar energy was recognized in a 1911 \"Scientific American\" article: \"in the far distant future, natural fuels having been exhausted [solar power] will remain as the only means of existence of the human race\".\n\nThe theory of peak oil was published in 1956. In the 1970s environmentalists promoted the development of renewable energy both as a replacement for the eventual depletion of oil, as well as for an escape from dependence on oil, and the first electricity generating wind turbines appeared. Solar had long been used for heating and cooling, but solar panels were too costly to build solar farms until 1980.\n\nThe IEA 2014 World Energy Outlook projects a growth of renewable energy supply from 1,700 gigawatts in 2014 to 4,550 gigawatts in 2040. Fossil fuels received about $550 billion in subsidies in 2013, compared to $120 billion for all renewable energies.\n\nAirflows can be used to run wind turbines. Modern utility-scale wind turbines range from around 600 kW to 5 MW of rated power, although turbines with rated output of 1.5–3 MW have become the most common for commercial use. The largest generator capacity of a single installed onshore wind turbine reached 7.5 MW in 2015. The power available from the wind is a function of the cube of the wind speed, so as wind speed increases, power output increases up to the maximum output for the particular turbine. Areas where winds are stronger and more constant, such as offshore and high altitude sites, are preferred locations for wind farms. Typically full load hours of wind turbines vary between 16 and 57 percent annually, but might be higher in particularly favorable offshore sites.\n\nWind-generated electricity met nearly 4% of global electricity demand in 2015, with nearly 63 GW of new wind power capacity installed. Wind energy was the leading source of new capacity in Europe, the US and Canada, and the second largest in China. In Denmark, wind energy met more than 40% of its electricity demand while Ireland, Portugal and Spain each met nearly 20%.\n\nGlobally, the long-term technical potential of wind energy is believed to be five times total current global energy production, or 40 times current electricity demand, assuming all practical barriers needed were overcome. This would require wind turbines to be installed over large areas, particularly in areas of higher wind resources, such as offshore. As offshore wind speeds average ~90% greater than that of land, so offshore resources can contribute substantially more energy than land stationed turbines. In 2014 global wind generation was 706 terawatt-hours or 3% of the worlds total electricity.\n\nIn 2015 hydropower generated 16.6% of the worlds total electricity and 70% of all renewable electricity. Since water is about 800 times denser than air, even a slow flowing stream of water, or moderate sea swell, can yield considerable amounts of energy. There are many forms of water energy:\n\nHydropower is produced in 150 countries, with the Asia-Pacific region generating 32 percent of global hydropower in 2010. For countries having the largest percentage of electricity from renewables, the top 50 are primarily hydroelectric. China is the largest hydroelectricity producer, with 721 terawatt-hours of production in 2010, representing around 17 percent of domestic electricity use. There are now three hydroelectricity stations larger than 10 GW: the Three Gorges Dam in China, Itaipu Dam across the Brazil/Paraguay border, and Guri Dam in Venezuela.\n\nWave power, which captures the energy of ocean surface waves, and tidal power, converting the energy of tides, are two forms of hydropower with future potential; however, they are not yet widely employed commercially. A demonstration project operated by the Ocean Renewable Power Company on the coast of Maine, and connected to the grid, harnesses tidal power from the Bay of Fundy, location of world's highest tidal flow. Ocean thermal energy conversion, which uses the temperature difference between cooler deep and warmer surface waters, currently has no economic feasibility.\n\nSolar energy, radiant light and heat from the sun, is harnessed using a range of ever-evolving technologies such as solar heating, photovoltaics, concentrated solar power (CSP), concentrator photovoltaics (CPV), solar architecture and artificial photosynthesis. Solar technologies are broadly characterized as either passive solar or active solar depending on the way they capture, convert and distribute solar energy. Passive solar techniques include orienting a building to the Sun, selecting materials with favorable thermal mass or light dispersing properties, and designing spaces that naturally circulate air. Active solar technologies encompass solar thermal energy, using solar collectors for heating, and solar power, converting sunlight into electricity either directly using photovoltaics (PV), or indirectly using concentrated solar power (CSP).\n\nA photovoltaic system converts light into electrical direct current (DC) by taking advantage of the photoelectric effect. Solar PV has turned into a multi-billion, fast-growing industry, continues to improve its cost-effectiveness, and has the most potential of any renewable technologies together with CSP. Concentrated solar power (CSP) systems use lenses or mirrors and tracking systems to focus a large area of sunlight into a small beam. Commercial concentrated solar power plants were first developed in the 1980s. CSP-Stirling has by far the highest efficiency among all solar energy technologies.\n\nIn 2011, the International Energy Agency said that \"the development of affordable, inexhaustible and clean solar energy technologies will have huge longer-term benefits. It will increase countries' energy security through reliance on an indigenous, inexhaustible and mostly import-independent resource, enhance sustainability, reduce pollution, lower the costs of mitigating climate change, and keep fossil fuel prices lower than otherwise. These advantages are global. Hence the additional costs of the incentives for early deployment should be considered learning investments; they must be wisely spent and need to be widely shared\". Italy has the largest proportion of solar electricity in the world, in 2015 solar supplied 7.8% of electricity demand in Italy. In 2016, after another year of rapid growth, solar generated 1.3% of global power.\n\nHigh Temperature Geothermal energy is from thermal energy generated and stored in the Earth. Thermal energy is the energy that determines the temperature of matter. Earth's geothermal energy originates from the original formation of the planet and from radioactive decay of minerals (in currently uncertain but possibly roughly equal proportions). The geothermal gradient, which is the difference in temperature between the core of the planet and its surface, drives a continuous conduction of thermal energy in the form of heat from the core to the surface. The adjective \"geothermal\" originates from the Greek roots \"geo\", meaning earth, and \"thermos\", meaning heat.\n\nThe heat that is used for geothermal energy can be from deep within the Earth, all the way down to Earth's core – down. At the core, temperatures may reach over 9,000 °F (5,000 °C). Heat conducts from the core to surrounding rock. Extremely high temperature and pressure cause some rock to melt, which is commonly known as magma. Magma convects upward since it is lighter than the solid rock. This magma then heats rock and water in the crust, sometimes up to .\n\nFrom hot springs, geothermal energy has been used for bathing since Paleolithic times and for space heating since ancient Roman times, but it is now better known for electricity generation.\n\nLow Temperature Geothermal refers to the use of the outer crust of the earth as a Thermal Battery to facilitate Renewable thermal energy for heating and cooling buildings, and other refrigeration and industrial uses. In this form of Geothermal, a Geothermal Heat Pump and Ground-coupled heat exchanger are used together to move heat energy into the earth (for cooling) and out of the earth (for heating) on a varying seasonal basis. Low temperature Geothermal (generally referred to as \"GHP\") is an increasingly important renewable technology because it both reduces total annual energy loads associated with heating and cooling, and it also flattens the electric demand curve eliminating the extreme summer and winter peak electric supply requirements. Thus Low Temperature Geothermal/GHP is becoming an increasing national priority with multiple tax credit support and focus as part of the ongoing movement toward Net Zero Energy. In 2016, New York City passed a law requiring GHP anytime is shown to be economical with 20 year financing including the Socialized Cost of Carbon.\n\nBiomass is biological material derived from living, or recently living organisms. It most often refers to plants or plant-derived materials which are specifically called lignocellulosic biomass. As an energy source, biomass can either be used directly via combustion to produce heat, or indirectly after converting it to various forms of biofuel. Conversion of biomass to biofuel can be achieved by different methods which are broadly classified into: \"thermal\", \"chemical\", and \"biochemical\" methods. Wood remains the largest biomass energy source today; examples include forest residues – such as dead trees, branches and tree stumps –, yard clippings, wood chips and even municipal solid waste. In the second sense, biomass includes plant or animal matter that can be converted into fibers or other industrial chemicals, including biofuels. Industrial biomass can be grown from numerous types of plants, including miscanthus, switchgrass, hemp, corn, poplar, willow, sorghum, sugarcane, bamboo, and a variety of tree species, ranging from eucalyptus to oil palm (palm oil).\n\nPlant energy is produced by crops specifically grown for use as fuel that offer high biomass output per hectare with low input energy. Some examples of these plants are wheat, which typically yield 7.5–8 tonnes of grain per hectare, and straw, which typically yield 3.5–5 tonnes per hectare in the UK. The grain can be used for liquid transportation fuels while the straw can be burned to produce heat or electricity. Plant biomass can also be degraded from cellulose to glucose through a series of chemical treatments, and the resulting sugar can then be used as a first generation biofuel.\n\nBiomass can be converted to other usable forms of energy such as methane gas or transportation fuels such as ethanol and biodiesel. Rotting garbage, and agricultural and human waste, all release methane gasalso called landfill gas or biogas. Crops, such as corn and sugarcane, can be fermented to produce the transportation fuel, ethanol. Biodiesel, another transportation fuel, can be produced from left-over food products such as vegetable oils and animal fats. Also, biomass to liquids (BTLs) and cellulosic ethanol are still under research. There is a great deal of research involving algal fuel or algae-derived biomass due to the fact that it's a non-food resource and can be produced at rates 5 to 10 times those of other types of land-based agriculture, such as corn and soy. Once harvested, it can be fermented to produce biofuels such as ethanol, butanol, and methane, as well as biodiesel and hydrogen. The biomass used for electricity generation varies by region. Forest by-products, such as wood residues, are common in the United States. Agricultural waste is common in Mauritius (sugar cane residue) and Southeast Asia (rice husks). Animal husbandry residues, such as poultry litter, are common in the United Kingdom.\n\nBiofuels include a wide range of fuels which are derived from biomass. The term covers solid, liquid, and gaseous fuels. Liquid biofuels include bioalcohols, such as bioethanol, and oils, such as biodiesel. Gaseous biofuels include biogas, landfill gas and synthetic gas. Bioethanol is an alcohol made by fermenting the sugar components of plant materials and it is made mostly from sugar and starch crops. These include maize, sugarcane and, more recently, sweet sorghum. The latter crop is particularly suitable for growing in dryland conditions, and is being investigated by International Crops Research Institute for the Semi-Arid Tropics for its potential to provide fuel, along with food and animal feed, in arid parts of Asia and Africa.\n\nWith advanced technology being developed, cellulosic biomass, such as trees and grasses, are also used as feedstocks for ethanol production. Ethanol can be used as a fuel for vehicles in its pure form, but it is usually used as a gasoline additive to increase octane and improve vehicle emissions. Bioethanol is widely used in the United States and in Brazil. The energy costs for producing bio-ethanol are almost equal to, the energy yields from bio-ethanol. However, according to the European Environment Agency, biofuels do not address global warming concerns. Biodiesel is made from vegetable oils, animal fats or recycled greases. It can be used as a fuel for vehicles in its pure form, or more commonly as a diesel additive to reduce levels of particulates, carbon monoxide, and hydrocarbons from diesel-powered vehicles. Biodiesel is produced from oils or fats using transesterification and is the most common biofuel in Europe. Biofuels provided 2.7% of the world's transport fuel in 2010.\n\nBiomass, biogas and biofuels are burned to produce heat/power and in doing so harm the environment. Pollutants such as sulphurous oxides (SO), nitrous oxides (NO), and particulate matter (PM) are produced from the combustion of biomass; the World Health Organisation estimates that 7 million premature deaths are caused each year by air pollution. Biomass combustion is a major contributor.\n\nEnergy storage is a collection of methods used to store electrical energy on an electrical power grid, or off it. Electrical energy is stored during times when production (especially from intermittent power plants such as renewable electricity sources such as wind power, tidal power, solar power) exceeds consumption, and returned to the grid when production falls below consumption. Pumped-storage hydroelectricity is used for more than 90% of all grid power storage. Costs of lithium ion batteries are dropping rapidly, and are increasingly being deployed as fast acting sources of grid power (i.e. operating reserve) and for domestic storage.\n\nRenewable power has been more effective in creating jobs than coal or oil in the United States.\n\nFrom the end of 2004, worldwide renewable energy capacity grew at rates of 10–60% annually for many technologies. In 2015 global investment in renewables rose 5% to $285.9 billion, breaking the previous record of $278.5 billion in 2011. 2015 was also the first year that saw renewables, excluding large hydro, account for the majority of all new power capacity (134 GW, making up 53.6% of the total). Of the renewables total, wind accounted for 72 GW and solar photovoltaics 56 GW; both record-breaking numbers and sharply up from 2014 figures (49 GW and 45 GW respectively). In financial terms, solar made up 56% of total new investment and wind accounted for 38%.\n\nProjections vary. The EIA has predicted that almost two thirds of net additions to power capacity will come from renewables by 2020 due to the combined policy benefits of local pollution, decarbonisation and energy diversification. Some studies have set out roadmaps to power 100% of the world’s energy with wind, hydroelectric and solar by the year 2030.\n\nAccording to a 2011 projection by the International Energy Agency, solar power generators may produce most of the world's electricity within 50 years, reducing the emissions of greenhouse gases that harm the environment. Cedric Philibert, senior analyst in the renewable energy division at the IEA said: \"Photovoltaic and solar-thermal plants may meet most of the world's demand for electricity by 2060and half of all energy needswith wind, hydropower and biomass plants supplying much of the remaining generation\". \"Photovoltaic and concentrated solar power together can become the major source of electricity\", Philibert said.\n\nIn 2014 global wind power capacity expanded 16% to 369,553 MW. Yearly wind energy production is also growing rapidly and has reached around 4% of worldwide electricity usage, 11.4% in the EU, and it is widely used in Asia, and the United States. In 2015, worldwide installed photovoltaics capacity increased to 227 gigawatts (GW), sufficient to supply 1 percent of global electricity demands. Solar thermal energy stations operate in the United States and Spain, and as of 2016, the largest of these is the 392 MW Ivanpah Solar Electric Generating System in California. The world's largest geothermal power installation is The Geysers in California, with a rated capacity of 750 MW. Brazil has one of the largest renewable energy programs in the world, involving production of ethanol fuel from sugar cane, and ethanol now provides 18% of the country's automotive fuel. Ethanol fuel is also widely available in the United States.\nAs of 2018, American electric utility companies are planning new or extra renewable energy investments. These investments are particularly aimed at solar energy, thanks to the Tax Cuts and Jobs Act of 2017 being signed into law. The law retained incentives for renewable energy development. Utility companies are taking advantage of the federal solar investment tax credit before it permanently goes down to 10% after 2021. According to the March 28 S&P Global Market Intelligence report summary, \"NextEra Energy Inc., Duke Energy Corp., and Dominion Energy Inc.’s utilities are among a number of companies in the sector contemplating significant solar investments in the near-term. Other companies, including Xcel Energy Inc. and Alliant Energy Corp., are undertaking large wind projects in the near-term, but are considering ramping up solar investments in the coming years.\"\n\nRenewable energy technologies are getting cheaper, through technological change and through the benefits of mass production and market competition. A 2011 IEA report said: \"A portfolio of renewable energy technologies is becoming cost-competitive in an increasingly broad range of circumstances, in some cases providing investment opportunities without the need for specific economic support,\" and added that \"cost reductions in critical technologies, such as wind and solar, are set to continue.\"\n\nHydro-electricity and geothermal electricity produced at favourable sites are now the cheapest way to generate electricity. Renewable energy costs continue to drop, and the levelised cost of electricity (LCOE) is declining for wind power, solar photovoltaic (PV), concentrated solar power (CSP) and some biomass technologies. Renewable energy is also the most economic solution for new grid-connected capacity in areas with good resources. As the cost of renewable power falls, the scope of economically viable applications increases. Renewable technologies are now often the most economic solution for new generating capacity. Where \"oil-fired generation is the predominant power generation source (e.g. on islands, off-grid and in some countries) a lower-cost renewable solution almost always exists today\". A series of studies by the US National Renewable Energy Laboratory modeled the \"grid in the Western US under a number of different scenarios where intermittent renewables accounted for 33 percent of the total power.\" In the models, inefficiencies in cycling the fossil fuel plants to compensate for the variation in solar and wind energy resulted in an additional cost of \"between $0.47 and $1.28 to each MegaWatt hour generated\"; however, the savings in the cost of the fuels saved \"adds up to $7 billion, meaning the added costs are, at most, two percent of the savings.\"\n\nOnly a quarter of the worlds estimated hydroelectric potential of 14,000 TWh/year has been developed, the regional potentials for the growth of hydropower around the world are, 71% Europe, 75% North America, 79% South America, 95% Africa, 95% Middle East, 82% Asia Pacific. However, the political realities of new reservoirs in western countries, economic limitations in the third world and the lack of a transmission system in undeveloped areas, result in the possibility of developing 25% of the remaining potential before 2050, with the bulk of that being in the Asia Pacific area. There is slow growth taking place in Western counties, but not in the conventional dam and reservoir style of the past. New projects take the form of run-of-the-river and small hydro, neither using large reservoirs. It is popular to repower old dams thereby increasing their efficiency and capacity as well as quicker responsiveness on the grid. Where circumstances permit existing dams such as the Russell Dam built in 1985 may be updated with \"pump back\" facilities for pumped-storage which is useful for peak loads or to support intermittent wind and solar power. Countries with large hydroelectric developments such as Canada and Norway are spending billions to expand their grids to trade with neighboring countries having limited hydro.\n\nWind power is widely used in Europe, China, and the United States. From 2004 to 2014, worldwide installed capacity of wind power has been growing from 47 GW to 369 GW—a more than sevenfold increase within 10 years with 2014 breaking a new record in global installations (51 GW). As of the end of 2014, China, the United States and Germany combined accounted for half of total global capacity. Several other countries have achieved relatively high levels of wind power penetration, such as 21% of stationary electricity production in Denmark, 18% in Portugal, 16% in Spain, and 14% in Ireland in 2010 and have since continued to expand their installed capacity. More than 80 countries around the world are using wind power on a commercial basis.\n\n\nThe United States conducted much early research in photovoltaics and concentrated solar power. The U.S. is among the top countries in the world in electricity generated by the Sun and several of the world's largest utility-scale installations are located in the desert Southwest.\n\nThe oldest solar thermal power plant in the world is the 354 megawatt (MW) SEGS thermal power plant, in California. The Ivanpah Solar Electric Generating System is a solar thermal power project in the California Mojave Desert, 40 miles (64 km) southwest of Las Vegas, with a gross capacity of 377 MW. The 280 MW Solana Generating Station is a solar power plant near Gila Bend, Arizona, about southwest of Phoenix, completed in 2013. When commissioned it was the largest parabolic trough plant in the world and the first U.S. solar plant with molten salt thermal energy storage.\n\nThe solar thermal power industry is growing rapidly with 1.3 GW under construction in 2012 and more planned. Spain is the epicenter of solar thermal power development with 873 MW under construction, and a further 271 MW under development. In the United States, 5,600 MW of solar thermal power projects have been announced. Several power plants have been constructed in the Mojave Desert, Southwestern United States. The Ivanpah Solar Power Facility being the most recent. In developing countries, three World Bank projects for integrated solar thermal/combined-cycle gas-turbine power plants in Egypt, Mexico, and Morocco have been approved.\n\nPhotovoltaics (PV) uses solar cells assembled into solar panels to convert sunlight into electricity. It's a fast-growing technology doubling its worldwide installed capacity every couple of years. PV systems range from small, residential and commercial rooftop or building integrated installations, to large utility-scale photovoltaic power station. The predominant PV technology is crystalline silicon, while thin-film solar cell technology accounts for about 10 percent of global photovoltaic deployment. In recent years, PV technology has improved its electricity generating efficiency, reduced the installation cost per watt as well as its energy payback time, and has reached grid parity in at least 30 different markets by 2014. Financial institutions are predicting a second solar \"gold rush\" in the near future.\n\nAt the end of 2014, worldwide PV capacity reached at least 177,000 megawatts. Photovoltaics grew fastest in China, followed by Japan and the United States, while Germany remains the world's largest overall producer of photovoltaic power, contributing about 7.0 percent to the overall electricity generation. Italy meets 7.9 percent of its electricity demands with photovoltaic power—the highest share worldwide. For 2015, global cumulative capacity is forecasted to increase by more than 50 gigawatts (GW). By 2018, worldwide capacity is projected to reach as much as 430 gigawatts. This corresponds to a tripling within five years. Solar power is forecasted to become the world's largest source of electricity by 2050, with solar photovoltaics and concentrated solar power contributing 16% and 11%, respectively. This requires an increase of installed PV capacity to 4,600 GW, of which more than half is expected to be deployed in China and India.\n\nCommercial concentrated solar power plants were first developed in the 1980s. As the cost of solar electricity has fallen, the number of grid-connected solar PV systems has grown into the millions and utility-scale solar power stations with hundreds of megawatts are being built. Solar PV is rapidly becoming an inexpensive, low-carbon technology to harness renewable energy from the Sun.\n\nMany solar photovoltaic power stations have been built, mainly in Europe, China and the United States. The 579 MW Solar Star, in the United States, is the world's largest PV power station.\n\nMany of these plants are integrated with agriculture and some use tracking systems that follow the sun's daily path across the sky to generate more electricity than fixed-mounted systems. There are no fuel costs or emissions during operation of the power stations.\n\nHowever, when it comes to renewable energy systems and PV, it is not just large systems that matter. Building-integrated photovoltaics or \"onsite\" PV systems use existing land and structures and generate power close to where it is consumed.\n\nBiofuels provided 3% of the world's transport fuel in 2010. Mandates for blending biofuels exist in 31 countries at the national level and in 29 states/provinces. According to the International Energy Agency, biofuels have the potential to meet more than a quarter of world demand for transportation fuels by 2050.\n\nSince the 1970s, Brazil has had an ethanol fuel program which has allowed the country to become the world's second largest producer of ethanol (after the United States) and the world's largest exporter. Brazil's ethanol fuel program uses modern equipment and cheap sugarcane as feedstock, and the residual cane-waste (bagasse) is used to produce heat and power. There are no longer light vehicles in Brazil running on pure gasoline. By the end of 2008 there were 35,000 filling stations throughout Brazil with at least one ethanol pump. Unfortunately, Operation Car Wash has seriously eroded public trust in oil companies and has implicated several high ranking Brazilian officials.\n\nNearly all the gasoline sold in the United States today is mixed with 10% ethanol, and motor vehicle manufacturers already produce vehicles designed to run on much higher ethanol blends. Ford, Daimler AG, and GM are among the automobile companies that sell \"flexible-fuel\" cars, trucks, and minivans that can use gasoline and ethanol blends ranging from pure gasoline up to 85% ethanol. By mid-2006, there were approximately 6 million ethanol compatible vehicles on U.S. roads.\n\nGeothermal power is cost effective, reliable, sustainable, and environmentally friendly, but has historically been limited to areas near tectonic plate boundaries. Recent technological advances have expanded the range and size of viable resources, especially for applications such as home heating, opening a potential for widespread exploitation. Geothermal wells release greenhouse gases trapped deep within the earth, but these emissions are much lower per energy unit than those of fossil fuels. As a result, geothermal power has the potential to help mitigate global warming if widely deployed in place of fossil fuels.\n\nThe International Geothermal Association (IGA) has reported that 10,715 MW of geothermal power in 24 countries is online, which is expected to generate 67,246 GWh of electricity in 2010. This represents a 20% increase in geothermal power online capacity since 2005. IGA projects this will grow to 18,500 MW by 2015, due to the large number of projects presently under consideration, often in areas previously assumed to have little exploitable resource.\n\nIn 2010, the United States led the world in geothermal electricity production with 3,086 MW of installed capacity from 77 power plants; the largest group of geothermal power plants in the world is located at The Geysers, a geothermal field in California. The Philippines follows the US as the second highest producer of geothermal power in the world, with 1,904 MW of capacity online; geothermal power makes up approximately 18% of the country's electricity generation.\n\nRenewable energy technology has sometimes been seen as a costly luxury item by critics, and affordable only in the affluent developed world. This erroneous view has persisted for many years, but 2015 was the first year when investment in non-hydro renewables, was higher in developing countries, with $156 billion invested, mainly in China, India, and Brazil.\n\nRenewable energy can be particularly suitable for developing countries. In rural and remote areas, transmission and distribution of energy generated from fossil fuels can be difficult and expensive. Producing renewable energy locally can offer a viable alternative.\nTechnology advances are opening up a huge new market for solar power: the approximately 1.3 billion people around the world who don't have access to grid electricity. Even though they are typically very poor, these people have to pay far more for lighting than people in rich countries because they use inefficient kerosene lamps. Solar power costs half as much as lighting with kerosene. As of 2010, an estimated 3 million households get power from small solar PV systems. Kenya is the world leader in the number of solar power systems installed per capita. More than 30,000 very small solar panels, each producing 12 to 30 watts, are sold in Kenya annually. Some Small Island Developing States (SIDS) are also turning to solar power to reduce their costs and increase their sustainability.\n\nMicro-hydro configured into mini-grids also provide power. Over 44 million households use biogas made in household-scale digesters for lighting and/or cooking, and more than 166 million households rely on a new generation of more-efficient biomass cookstoves. Clean liquid fuel sourced from renewable feedstocks are used for cooking and lighting in energy-poor areas of the developing world. Alcohol fuels (ethanol and methanol) can be produced sustainably from non-food sugary, starchy, and cellulostic feedstocks. Project Gaia, Inc. and CleanStar Mozambique are implementing clean cooking programs with liquid ethanol stoves in Ethiopia, Kenya, Nigeria and Mozambique.\n\nRenewable energy projects in many developing countries have demonstrated that renewable energy can directly contribute to poverty reduction by providing the energy needed for creating businesses and employment. Renewable energy technologies can also make indirect contributions to alleviating poverty by providing energy for cooking, space heating, and lighting. Renewable energy can also contribute to education, by providing electricity to schools.\n\nU.S. President Barack Obama's American Recovery and Reinvestment Act of 2009 includes more than $70 billion in direct spending and tax credits for clean energy and associated transportation programs. Leading renewable energy companies include First Solar, Gamesa, GE Energy, Hanwha Q Cells, Sharp Solar, Siemens, SunOpta, Suntech Power, and Vestas.\n\nMany national, state, and local governments have also created green banks. A green bank is a quasi-public financial institution that uses public capital to leverage private investment in clean energy technologies. Green banks use a variety of financial tools to bridge market gaps that hinder the deployment of clean energy.\n\nThe military has also focused on the use of renewable fuels for military vehicles. Unlike fossil fuels, renewable fuels can be produced in any country, creating a strategic advantage. The US military has already committed itself to have 50% of its energy consumption come from alternative sources.\n\nThe International Renewable Energy Agency (IRENA) is an intergovernmental organization for promoting the adoption of renewable energy worldwide. It aims to provide concrete policy advice and facilitate capacity building and technology transfer. IRENA was formed on 26 January 2009, by 75 countries signing the charter of IRENA. As of March 2010, IRENA has 143 member states who all are considered as founding members, of which 14 have also ratified the statute.\n\nAs of 2011, 119 countries have some form of national renewable energy policy target or renewable support policy. National targets now exist in at least 98 countries. There is also a wide range of policies at state/provincial and local levels.\n\nUnited Nations' Secretary-General Ban Ki-moon has said that renewable energy has the ability to lift the poorest nations to new levels of prosperity. In October 2011, he \"announced the creation of a high-level group to drum up support for energy access, energy efficiency and greater use of renewable energy. The group is to be co-chaired by Kandeh Yumkella, the chair of UN Energy and director general of the UN Industrial Development Organisation, and Charles Holliday, chairman of Bank of America\".\n\nThe incentive to use 100% renewable energy, for electricity, transport, or even total primary energy supply globally, has been motivated by global warming and other ecological as well as economic concerns. The Intergovernmental Panel on Climate Change has said that there are few fundamental technological limits to integrating a portfolio of renewable energy technologies to meet most of total global energy demand. Renewable energy use has grown much faster than even advocates anticipated. At the national level, at least 30 nations around the world already have renewable energy contributing more than 20% of energy supply. Also, Professors S. Pacala and Robert H. Socolow have developed a series of \"stabilization wedges\" that can allow us to maintain our quality of life while avoiding catastrophic climate change, and \"renewable energy sources,\" in aggregate, constitute the largest number of their \"wedges\".\n\nUsing 100% renewable energy was first suggested in a Science paper published in 1975 by Danish physicist Bent Sørensen. It was followed by several other proposals, until in 1998 the first detailed analysis of scenarios with very high shares of renewables were published. These were followed by the first detailed 100% scenarios. In 2006 a PhD thesis was published by Czisch in which it was shown that in a 100% renewable scenario energy supply could match demand in every hour of the year in Europe and North Africa. In the same year Danish Energy professor Henrik Lund published a first paper in which he addresses the optimal combination of renewables, which was followed by several other papers on the transition to 100% renewable energy in Denmark. Since then Lund has been publishing several papers on 100% renewable energy. After 2009 publications began to rise steeply, covering 100% scenarios for countries in Europe, America, Australia and other parts of the world.\n\nIn 2011 Mark Z. Jacobson, professor of civil and environmental engineering at Stanford University, and Mark Delucchi published a study on 100% renewable global energy supply in the journal Energy Policy. They found producing all new energy with wind power, solar power, and hydropower by 2030 is feasible and existing energy supply arrangements could be replaced by 2050. Barriers to implementing the renewable energy plan are seen to be \"primarily social and political, not technological or economic\". They also found that energy costs with a wind, solar, water system should be similar to today's energy costs.\n\nSimilarly, in the United States, the independent National Research Council has noted that \"sufficient domestic renewable resources exist to allow renewable electricity to play a significant role in future electricity generation and thus help confront issues related to climate change, energy security, and the escalation of energy costs … Renewable energy is an attractive option because renewable resources available in the United States, taken collectively, can supply significantly greater amounts of electricity than the total current or projected domestic demand.\"\n\nThe most significant barriers to the widespread implementation of large-scale renewable energy and low carbon energy strategies are primarily political and not technological. According to the 2013 \"Post Carbon Pathways\" report, which reviewed many international studies, the key roadblocks are: climate change denial, the fossil fuels lobby, political inaction, unsustainable energy consumption, outdated energy infrastructure, and financial constraints.\n\nOther renewable energy technologies are still under development, and include cellulosic ethanol, hot-dry-rock geothermal power, and marine energy. These technologies are not yet widely demonstrated or have limited commercialization. Many are on the horizon and may have potential comparable to other renewable energy technologies, but still depend on attracting sufficient attention and research, development and demonstration (RD&D) funding.\n\nThere are numerous organizations within the academic, federal, and commercial sectors conducting large scale advanced research in the field of renewable energy. This research spans several areas of focus across the renewable energy spectrum. Most of the research is targeted at improving efficiency and increasing overall energy yields.\nMultiple federally supported research organizations have focused on renewable energy in recent years. Two of the most prominent of these labs are Sandia National Laboratories and the National Renewable Energy Laboratory (NREL), both of which are funded by the United States Department of Energy and supported by various corporate partners. Sandia has a total budget of $2.4 billion while NREL has a budget of $375 million.\n\n\n\nRenewable electricity production, from sources such as wind power and solar power, is sometimes criticized for being variable or intermittent, but is not true for concentrated solar, geothermal and biofuels, that have continuity. In any case, the International Energy Agency has stated that deployment of renewable technologies usually increases the diversity of electricity sources and, through local generation, contributes to the flexibility of the system and its resistance to central shocks.\n\nThere have been \"not in my back yard\" (NIMBY) concerns relating to the visual and other impacts of some wind farms, with local residents sometimes fighting or blocking construction. In the United States, the Massachusetts Cape Wind project was delayed for years partly because of aesthetic concerns. However, residents in other areas have been more positive. According to a town councilor, the overwhelming majority of locals believe that the Ardrossan Wind Farm in Scotland has enhanced the area.\n\nA recent UK Government document states that \"projects are generally more likely to succeed if they have broad public support and the consent of local communities. This means giving communities both a say and a stake\". In countries such as Germany and Denmark many renewable projects are owned by communities, particularly through cooperative structures, and contribute significantly to overall levels of renewable energy deployment.\n\nThe market for renewable energy technologies has continued to grow. Climate change concerns and increasing in green jobs, coupled with high oil prices, peak oil, oil wars, oil spills, promotion of electric vehicles and renewable electricity, nuclear disasters and increasing government support, are driving increasing renewable energy legislation, incentives and commercialization. New government spending, regulation and policies helped the industry weather the 2009 economic crisis better than many other sectors.\n\nWhile renewables have been very successful in their ever-growing contribution to electrical power there are no countries dominated by fossil fuels who have a plan to stop and get that power from renwables. Only Scotland and Ontario have stopped burning coal, largely due to good natural gas supplies. In the area of transportation, fossil fuels are even more entrenched and solutions harder to find. It's unclear if there are failures with policy or renewable energy, but twenty years after the Kyoto Protocol fossil fuels are still our primary energy source and consumption continues to grow.\n\nThe ability of biomass and biofuels to contribute to a reduction in emissions is limited because both biomass and biofuels emit large amounts of air pollution when burned and in some cases compete with food supply. Furthermore, biomass and biofuels consume large amounts of water. Other renewable sources such as wind power, photovoltaics, and hydroelectricity have the advantage of being able to conserve water, lower pollution and reduce emissions.\n\n\n\n\n"}
{"id": "38050156", "url": "https://en.wikipedia.org/wiki?curid=38050156", "title": "RiAFP", "text": "RiAFP\n\nRiAFP refers to an antifreeze protein (AFP) produced by the \"Rhagium inquisitor\" longhorned beetle. It is a type V antifreeze protein with a molecular weight of 12.8 kDa; this type of AFP is noted for its hyperactivity. \"R. inquisitor\" is a freeze-avoidant species, meaning that, due to its AFP, \"R. inquisitor\" prevents its body fluids from freezing altogether. This contrasts with freeze-tolerant species, whose AFPs simply depress levels of ice crystal formation in low temperatures. Whereas most insect antifreeze proteins contain cysteines at least every sixth residue, as well as varying numbers of 12- or 13-mer repeats of 8.3-12.5kDa, RiAFP is notable for containing only one disulfide bridge. This property of RiAFP makes it particularly attractive for recombinant expression and biotechnological applications.\n\nAFPs work through an interaction with small ice crystals that is similar to an enzyme-ligand binding mechanism which inhibits recrystallization of ice. This explanation of the interruption of the ice crystal structure by the AFP has come to be known as the \"adsorption-inhibition hypothesis\".\n\nAccording to this hypothesis, AFPs disrupt the thermodynamically favourable growth of an ice crystal via kinetic inhibition of contact between solid ice and liquid water. In this manner, the nucleation sites of the ice crystal lattice are blocked by the AFP, inhibiting the rapid growth of the crystal that could be fatal for the organism. In physical chemistry terms, the AFPs adsorbed onto the exposed ice crystal force the growth of the ice crystal in a convex fashion as the temperature drops, which elevates the ice vapour pressure at the nucleation sites. Ice vapour pressure continues to increase until it reaches equilibrium with the surrounding solution (water), at which point the growth of the ice crystal stops.\n\nThe aforementioned effect of AFPs on ice crystal nucleation is lost at the thermal hysteresis point. At a certain low temperature, the maximum convexity of the ice nucleation site is reached. Any further cooling will actually result in a \"spreading\" of the nucleation site away from this convex region, causing rapid, uncontrollable nucleation of the ice crystal. The temperature at which this phenomenon occurs is the thermal hysteresis point.<br>\nThe \"adsorption-inhibition hypothesis\" is further supported by the observation that antifreeze activity increases with increasing AFP concentration – the more AFPs adsorb onto the forming ice crystal, the more 'crowded' these proteins become, making ice crystal nucleation less favourable.\n\nIn the \"R. inquisitor\" beetle, AFPs are found in the haemolymph, a fluid that bathes all the cells of the beetle and fills a cavity called the haemocoel. The presence of AFPs in \"R. inquisitor\" allows the tissues and fluids within the beetle to withstand freezing up to -30 °C (the thermal hysteresis point for this AFP). This strategy provides an obvious survival benefit to these beetles, who are endemic to cold climates, such as Scandinavia, Siberia, and Alaska.\n\nThe primary structure of RiAFP (the sequence may be found here) determined by Mass Spectroscopy, Edman degradation and by constructing a partial cDNA sequence and PCR have shown that a TxTxTxT internal repeat exists . Sequence logos constructed from the RiAFP internal repeats, have been particularly helpful in the determination of the consensus sequence of these repeats. The TxTxTxT domains are irregularly spaced within the protein and have been shown to be conserved from the TxT binding motif of other AFPs. The hydroxyl moiety of the T residues fits well, when spaced as they are in the internal repeats, with the hydroxyl moieties of externally facing water molecules in the forming ice lattice. This mimics the formation of the growth cone at a nucleation site in the absence of AFPs. Thus, the binding of RiAFP inhibits the growth of the crystal in the basal and prism planes of the ice.\n\nThe fact that the binding motif appears as a \"triplet\" of the conserved TxT repeat, as well as the observation that blastp queries have returned no viable matches, has led some researchers to suggest that RiAFP represents a new type of AFP – one that differs from the heavily studied \"TmAFP\" (from \"T. molitor\"), \"DcAFP\" (from \"D. canadensis\"), and \"CfAFP\" (from \"C. fumiferana\"). On the basis of these observations, it has been predicted that the need for insect AFPs came about after insect evolutionary divergence, much like the evolution of fish AFPs; thus, different AFPs most likely evolved in parallel from adaptations to cold (environmental) stress. As a result, homology modelling with \"TmAFP\", \"DcAFP\", or \"CfAFP\" would prove to be fruitless.\n\nSecondary structure modelling algorithms have determined that the internal repeats are spaced sufficiently to tend towards β-strand configuration; no helical regions include the conserved repeats; and all turn regions are located at the ends of β-strand regions. These data suggest that RiAFP is a well-folded β-helical protein, having six β-strand regions consisting of 13-amino acids (including one TxTxTxT binding motif) per strand.\n\nPrimary crystallographic studies, have been published on a RiAFP crystal (which diffracted to 1.3Å resolution) in the trigonal space group P321 (or P321), with unit-cell parameters a = b = 46.46, c = 193.21Å.\n"}
{"id": "3778786", "url": "https://en.wikipedia.org/wiki?curid=3778786", "title": "Rotor–stator interaction", "text": "Rotor–stator interaction\n\nAn important issue for the aeronautical industry is the reduction of aircraft noise. The rotor–stator interaction is a predominant part of the noise emission.\nWe will present an introduction to these interaction theory, whose applications are numerous. For example, the conception of air-conditioning ventilators requires a full understanding of this interaction.\n\nA rotor wake induces on the downstream stator blades a fluctuating vane loading, which is directly linked to the noise emission.\n\nWe consider a B blades rotor (at a rotation speed of formula_1) and a V blades stator, in a unique rotor–stator configuration. The source frequencies are multiples of formula_2, that is to say formula_3. For the moment we don’t have access to the source levels formula_4. The noise frequencies are also formula_3, not depending on the number of blades of the stator. Nevertheless, this number V has a predominant role in the noise levels (formula_6) and directivity, as it will be discussed later.\n\n\"Example\"\n\n\"For an airplane air-conditioning ventilator, reasonable data are :\"\n\n\"formula_7 and formula_8 rnd/min\"\n\n\"The blade passing frequency is 2600 Hz, so we only have to include the first two multiples (2600 Hz and 5200 Hz), because of the human ear high-sensibility limit. We have to study the frequencies m=1 and m=2.\"\n\nAs the source levels can't be easily modified, we have to focus on the interaction between those levels and the noise levels.\n\nThe transfer function formula_9 contains the following part :\n\nformula_10\n\nWhere \"M\" is the Mach number and formula_11 the Bessel function of \"mB\"–\"sV\" order. The influence of the transfer function may be minimized by reducing the value of the Bessel function. To do so, the argument must be smaller than the order of the Bessel function.\n\n\"Back to the example :\"\n\n\"For m=1, with a Mach number M=0.3, the argument of the Bessel function is about 4. We have to avoid having mB-sV less than 4. If V=10, we have 13-1x10=3, so there will be a noisy mode. If V=19, the minimum of mB-sV is 6, and the noise emission will be limited.\"\n\n\"Remark :\"\n\n\"The case that is to be strictly avoided is when mB-sV can be nul, which causes the order of the Bessel function to be 0. As a consequence, we have to take care having B and V prime numbers.\"\n\nThe minimization of the transfer function formula_9 is a great step in the process of reducing the noise emission. Nevertheless, to be highly efficient, we also have to predict the source levels formula_4. This will lead us to choose to minimize the Bessel functions for the most significant values of m. For example, if the source level for m=1 is very higher than for m=2, we will not consider the Bessel functions of order 2B-sV.\nThe determination of the source levels is given by the Sears theory,which will not be explicated here.\n\nAll this study was made for a privileged direction : the axis of the rotor–stator. All the results are acceptable when the noise reduction is ought to be in this direction. In the case where the noise to reduce is perpendicular to the axis, the results are very different, as those figures shown :\n\nFor B=13 and V=13, which is the worst case, we see that the sound level is very high on the axis (for formula_14) \n\nFor B=13 and V=19, the sound level is very low on the axis but high perpendicularly to the axis (for formula_15)\n"}
{"id": "8056137", "url": "https://en.wikipedia.org/wiki?curid=8056137", "title": "Royal Forestry Society", "text": "Royal Forestry Society\n\nThe Royal Forestry Society (RFS) is an educational charity and one of the oldest membership organisations in England, Wales and Northern Ireland for those actively involved in woodland management. HM The Queen is the patron.\n\nThe RFS has a broad membership which includes woodland owners, managers, countryside professionals (land agents, ecologists, conservationists), academics, students and others with a general interest in woodland management. Membership is open to all.\n\nThe Royal Forestry Society was established in 1882 in Northumberland, England. Originally known as the English Arboricultural Society, the organisation was founded by forester Henry Clark and nurseryman John W Robson, both from Hexham.\n\nThe Society has 20 Divisions which between them organise up to 100 woodland field meetings a year on topics that span seed to sawmill. Annually: a top-level conference is held; there is a 4 or 5 day woodland study tour in England, Wales or Northern Ireland; Excellence in Forestry Awards take place, there is a travel bursary for forestry study abroad, and workshops, seminars and knowledge transfer events. Overseas study tours are held every other year.\n\nThe RFS helps shape formal forestry qualifications and its own Certificate of Arboriculture is recognised across the sector; it encourages students with a range of awards, bursaries and internships; has launched a research programme with colleges and has partnered with a number of organisations to help share knowledge.\n\nThe RFS Teaching Trees project is expanding, introducing primary schools and their pupils to their local woodlands and their benefits to the economy, environment and society.\n\nThe Society manages three woodlands, in the Chilterns, part of the National Forest, and the largest and oldest grove of coast redwoods in Europe at Leighton Hall, Powys near Welshpool. These are managed as examples of good practice and lessons learned are shared with members.\n\nThe RFS maintains a library of forestry related books for members.\n\nThe primary publication of The Royal Forestry Society is the Quarterly Journal of Forestry (since 1907).\n\n"}
{"id": "38324147", "url": "https://en.wikipedia.org/wiki?curid=38324147", "title": "Series and parallel springs", "text": "Series and parallel springs\n\nIn mechanics, two or more springs are said to be in series when they are connected end-to-end or point to point, and in parallel when they are connected side-by-side; in both cases, so as to act as a single spring.\n\nMore generally, two or more springs are \"in series\" when any external stress applied to the ensemble gets applied to each spring without change of magnitude, and the amount strain (deformation) of the ensemble is the sum of the strains of the individual springs. Conversely, they are said to be \"in parallel\" if the strain of the ensemble is their common strain, and the stress of the ensemble is the sum of their stresses.\n\nAny combination of Hookean (linear-response) springs in series or parallel behaves like a single Hookean spring. The formulas for combining their physical attributes are analogous to those that apply to capacitors connected in series or parallel in an electrical circuit.\n\nThe following table gives formulas for the spring that is equivalent to a system of two springs, in series or in parallel, whose spring constants are formula_1 and formula_2. (The compliance formula_3 of a spring is the reciprocal formula_4 of its spring constant.)\n\n"}
{"id": "5520376", "url": "https://en.wikipedia.org/wiki?curid=5520376", "title": "Shock factor", "text": "Shock factor\n\nShock factor is a commonly used figure of merit for estimating the amount of shock experienced by a naval target from an underwater explosion as a function of explosive charge weight, slant range, and depression angle (between vessel and charge). \n\n<math>SF = {\\frac{\\sqrt W}{R}} \n"}
{"id": "29124", "url": "https://en.wikipedia.org/wiki?curid=29124", "title": "Soviet submarine K-219", "text": "Soviet submarine K-219\n\nK-219 was a Project 667A \"Navaga\"-class ballistic missile submarine (NATO reporting name Yankee I) of the Soviet Navy. It carried 16 (later 15) SS-N-6 liquid-fuel missiles powered by UDMH with IRFNA, equipped with an estimated 34 nuclear warheads.\n\n\"K-219\" was involved in what has become one of the most controversial submarine incidents during the Cold War.\n\nOn Friday 3 October 1986, while on an otherwise routine Cold War nuclear deterrence patrol in the North Atlantic northeast of Bermuda, the 15-year-old \"K-219\" suffered an explosion and fire in a missile tube. The seal in a missile hatch cover failed, allowing saltwater to leak into the missile tube and react with residue from the missile's liquid fuel. Though there was no official announcement, a published source (citing no sources) said the Soviet Union claimed that the leak was caused by a collision with the submarine . \"Augusta\" was certainly operating in proximity, but both the United States Navy and the commander of \"K-219\", Captain Second Rank Igor Britanov, deny that a collision took place. \"K-219\" had previously experienced a similar casualty; one of her missile tubes was already disabled and welded shut, having been permanently sealed after an explosion caused by reaction between seawater leaking into the silo and missile fuel residue. \nThe authors of the book \"Hostile Waters\" reconstructed the incident from descriptions by the survivors, ships' logs, the official investigations, and participants both ashore and afloat from the Soviet and the American sides. The result was a novelized version of events.\n\nShortly after 0530 Moscow time, seawater leaking into silo six of \"K-219\" reacted with missile fuel, producing chlorine and nitrogen dioxide gases and sufficient heat to explosively decompose additional fuming nitric acid to produce more nitrogen dioxide gas. \"K-219\" weapons officer Alexander Petrachkov attempted to cope with this by disengaging the hatch cover and venting the missile tube to the sea. Shortly after 0532, an explosion occurred in silo six.\n\nAn article in \"Undersea warfare\" by Captain First Rank (Ret.) Igor Kurdin, Russian Navy – \"K-219\"s previous XO (executive officer) – and Lieutenant Commander Wayne Grasdock, USN described the explosion occurrence as follows:\n\nTwo sailors were killed outright in the explosion, and a third died soon afterward from toxic gas poisoning. Through a breach in the hull, the vessel immediately started taking on sea water, quickly sinking from its original depth of to eventually reach a depth in excess of . Sealing of all of the compartments and full engagement of the sea water pumps in the stricken compartments enabled the depth to be stabilised.\n\n25 sailors were trapped in a sealed section, and it was only after a conference with his incident specialists that the Captain allowed the Chief Engineer to open the hatch and save the 25 lives. It could be seen from instruments that although the nuclear reactor should have automatically shut down, it was not. Lt. Nikolai Belikov, one of the reactor control officers, entered the reactor compartment but ran out of oxygen after turning just one of the four rod assemblies on the first reactor. Twenty-year-old enlisted seaman Sergei Preminin then volunteered to shut down the reactor, to be enabled by operating under instruction from the Chief Engineer. Working with a full-face gas mask, he successfully shut down the reactor. A large fire had developed within the compartment, raising the pressure. When Preminin tried to reach his comrades on the other side of a door, the pressure difference prevented him from opening it, and he subsequently died of asphyxiation in the reactor compartment.\n\nIn a nuclear safe condition, and with sufficient stability to allow it to surface, Captain Britanov surfaced \"K-219\" on battery power alone. He was then ordered to have the ship towed by a Soviet freighter back to her home port of Gadzhiyevo, away. Although a towline was attached, towing attempts were unsuccessful, and after subsequent poison gas leaks into the final aft compartments and against orders, Britanov ordered the crew to evacuate onto the towing ship, but remained aboard \"K-219\" himself.\n\nDispleased with Britanov's inability to repair his submarine and continue his patrol, Moscow ordered Valery Pshenichny, \"K-219\"’s security officer, to assume command, transfer the surviving crew back to the submarine, and return to duty. Before those orders could be carried out the flooding reached a point beyond recovery and on 6 October 1986 the \"K-219\" sank to the bottom of the Hatteras Abyssal Plain at a depth of about 6,000 m (18,000 ft). Britanov abandoned ship shortly before the sinking. \"K-219\"'s full complement of nuclear weapons was lost along with the vessel.\n\nIn 1988, the Soviet hydrographic research ship \"Keldysh\" positioned itself over the wreck of \"K-219\", and found the submarine sitting upright on the sandy bottom. It had broken in two, aft of the conning tower. Several missile silo hatches had been forced open, and the missiles, along with the nuclear warheads they contained, were gone.\n\nPreminin was posthumously awarded the Order of the Red Star for his bravery in securing the reactors. Britanov was charged with negligence, sabotage, and treason. He was never imprisoned, but waited for his trial in Sverdlovsk. On 30 May 1987, Defense Minister Sergey Sokolov was dismissed as a result of the Mathias Rust incident two days earlier, and replaced by Dmitry Yazov; the charges against Britanov were subsequently dismissed.\n\nIn 1997, the British BBC television film \"Hostile Waters\", co-produced with HBO and starring Rutger Hauer, Martin Sheen, and Max von Sydow, was released in the United States by Warner Bros. It was based on the book by the same name, which claimed to describe the loss of \"K-219\". In 2001, Captain Britanov filed suit, claiming Warner Bros. did not seek or get his permission to use his story or his character, and that the film did not portray the events accurately and made him look incompetent. After three years of hearing, the court ruled in Britanov's favor. Russian media reported that the filmmaker paid a settlement totaling under $100,000.\n\nAfter the release of the movie, The U.S. Navy issued the following statement regarding both the book and the movie:\n\nAn article on the U.S. Navy's website posted by Captain 1st Rank (Ret.) Igor Kurdin (former XO of \"K-219\") and Lieutenant Commander Wayne Grasdock denied any collision between \"K-219\" and \"Augusta\". Captain Britanov also denies a collision, and he has stated that he was not asked to be a guest speaker at Russian functions, because he refuses to follow the Russian government's interpretation of the \"K-219\" incident.\n\nIn a BBC interview recorded in February 2013, Admiral of the Fleet Vladimir Chernavin, the C-in-C of the Soviet Navy at the time of the \"K-219\" incident, says the accident was caused by a malfunction in a missile tube, and makes no mention of a collision with an American submarine. The interview was conducted for the BBC2 series \"The Silent War\".\n\n\n"}
{"id": "24326448", "url": "https://en.wikipedia.org/wiki?curid=24326448", "title": "Spotted Lake", "text": "Spotted Lake\n\nSpotted Lake is a saline endorheic alkali lake located northwest of Osoyoos in the eastern Similkameen Valley of British Columbia, Canada, accessed via Highway 3.\n\nSpotted Lake is richly concentrated with various minerals. It contains dense deposits of magnesium sulfate, calcium and sodium sulphates. It also contains high concentrations of eight other minerals and lower amounts of silver and titanium.\n\nMost of the water in the lake evaporates over the summer, revealing colorful mineral deposits. Large “spots” on the lake appear and are colored according to the mineral composition and seasonal amount of precipitation. Magnesium sulfate, which crystallizes in the summer, is a major contributor to spot color. In the summer, remaining minerals in the lake harden to form natural “walkways” around and between the spots.\n\nOriginally known to the First Nations of the Okanagan Valley as \"Kliluk\", Spotted Lake was for centuries and remains revered as a sacred site thought to provide therapeutic waters. During World War I, the minerals of Spotted Lake were used in manufacturing ammunition.\n\nLater, the area came under the control of the Ernest Smith Family for a term of about 40 years. In 1979, Smith attempted to create interest in a spa at the lake. The First Nations responded with an effort to buy the lake, then in October 2001, struck a deal by purchasing 22 hectares of land for a total of $720,000, and contributed about 20% of the cost. The Indian Affairs Department paid the remainder.\n\nToday, there is a roadside sign telling visitors that the lake is a cultural and ecologically sensitive area, and a traditional medicine lake for the Okanagan Syilx people. The lake can be viewed from the fence that has been erected for protection from the liabilities of public access. Many travelers stop to view the site.\n\n"}
{"id": "54277715", "url": "https://en.wikipedia.org/wiki?curid=54277715", "title": "Sputnik-1 EMC/EMI lab model", "text": "Sputnik-1 EMC/EMI lab model\n\nSputnik 1 EMC/EMI is a class of full-scale laboratory models of the Soviet Sputnik 1 satellite, made to test ground Electromagnetic Compatibility (EMC) and Electromagnetic Interference (AMI). The models, manufactured by OKB-1 and NII-885 (headed by Mikhail Ryazansky), were introduced on February 15, 1957.\n\nOn July 20, 2016, a working Sputnik-1 EMC/EMI model, serial number 003, was sold at Bonhams in New York for US$269,000. It featured a still-operational transmitter and four antennas. Of four known Sputnik-1 test articles, this was the only one known to be functional.\n\nOf four known models, two reside in private hands, one is located at the Energia Corporate Museum outside Moscow, and one, lacking internal components, is displayed at the Museum of Flight in Seattle, Washington, US.\n"}
{"id": "53884873", "url": "https://en.wikipedia.org/wiki?curid=53884873", "title": "Stanley Pearl", "text": "Stanley Pearl\n\nAustralian Stanley Keith Pearl was a sapper with the First Australian Imperial Force during World War I. His exact dates of birth and death are unclear. After enlisting at the town of Ulverstone in Tasmania on 9 November 1915 at twenty-one years of age, Pearl, in August 1916, became a sapper with the Australian 5 Field Company Engineers. He performed this role for the duration of the war. Many details on Pearl himself remain a mystery. A consideration of the phenomenal trench art collection that he produced, however, helps to piece together essential information about him.\n\nPearl was a joiner by trade whose father was Charles Pearl of Ulverstone, Tasmania. Pearl was Anglican. He embarked from Sydney to Alexandria and then the Western Front (World War I) in France aboard HMAT \"Orsova\" on 11 March 1916. He undertook a brief stint with the 8 Field Company Engineers (Reinforcement 4) before joining the 5 Field Company Engineers. Pearl’s Victory Medal (United Kingdom) and British War Medal are held in the Australian War Memorial’s collection.\n\nPearl produced a phenomenal collection of trench art between 1916 and 1919. Pearl donated his trench art objects to the Australian War Memorial but whether this reflects the full extent of that which he produced in the context of war remains a mystery. A total of fifteen of Pearl’s oeuvre are held in the Australian War Memorial’s Military Heraldry collection. Military shell cases, other parts of shells, badges and buttons were just some of the war-related materials used by Pearl to assemble everyday objects such as a clock, a map of Tasmania and a hat pin stand, as is evidenced by the accessibility and digitisation of his work on the War Memorial’s online collection (see external links for selection of images). One can become more familiar with Pearl’s work through such a collection as it displays images of the objects together with Pearl’s own field notes. The accompanying field notes that Pearl documented alongside each piece are fascinating as this was not a feature typically associated with First World War trench art. Pearl used matter-of-fact language where tales of death became mere descriptions. Pearl’s field notes and his details on the First World War embarkation roll are useful additional primary sources for historians, art historians and anthropologists.\n\nThere is no evidence to suggest that Pearl is survived by anyone, nor have any photographs of him been discovered. He is an \"enigma.\" It is therefore Pearl’s material existence in the form of trench art that survives, the power of which is subsequently elevated. The creative and artistic vision of Pearl to contribute to the material culture of World War I, of which trench art is a fascinating example, has meant that his name lives on through these well-crafted, striking objects. Pearl was unconsciously engaging with features of material culture such as bricolage and recyclia in producing trench art. The fact that he was a joiner pre-war also speaks to this refined, craftsmanship-like quality to his pieces.\n\nPearl survived World War I. During March of 1919, Pearl returned to Tasmania, although soon relocated to Canberra. He began working for the Australian War Memorial in 1941, the year that it opened, and continued to do so until he retired. He worked as a carpenter and senior tradesman at the War Memorial. The electoral roll details Pearl’s exact position at the War Memorial as an ‘installation manager.’ Such hands-on employment echoes his pre-war occupation as a joiner.\n\nA recent exhibition entitled \"Sappers & Shrapnel: Contemporary Art and the Art of the Trenches\" at the Art Gallery of South Australia showcased Pearl’s trench art. The exhibition presented Pearl’s pieces alongside other First World War trench art objects and more recent works of art that reflected on war today. Such a display, in bringing together the historical and the contemporary, was the first opportunity whereby the trench art of Pearl was exposed to a wider audience. It is evidence that Pearl and his trench art collection hold an important relevance today and will continue to do so in the future, especially in studies of history, art history and anthropology.\n\n\n\n\n"}
{"id": "57026470", "url": "https://en.wikipedia.org/wiki?curid=57026470", "title": "Stiegler's Gorge Hydroelectric Power Station", "text": "Stiegler's Gorge Hydroelectric Power Station\n\nStiegler's Gorge Hydroelectric Power Station (SGHPS), is a planned hydroelectric dam in Tanzania. The power station is expected to produce 5,920GWh of power annually.\n\nThe power station would be located across the Rufiji River, in the \"Stiegler's Gorge\", in the Selous Game Reserve, Morogoro Region, approximately , by road, southwest of Dar es Salaam, the commercial capital and largest city of Tanzania.\n\nThis power station is located in Selous Game Reserve, one of the world's largest World Heritage sites, measuring . The power station and reservoir lake are planned to occupy approximately , within the game reserve.\n\nThe government of Tanzania has been considering establishing this power station since the 1960s. When fully developed, it will be the largest power station in Tanzania. The arched, concrete dam is expected to create a reservoir lake, , in length, measuring , with of water.\n\nIn August 2017, the Tanzanian government advertised for bids to construct this dam. The selected contractor is expected to complete the dam in no more than 36 months. The power generated will be evacuated via a new 400kV high voltage power line to a substantiation where the power will be integrated into the national electricity grid. The government of Ethiopia is advising the Tanzanian government on the implementation of this project.\n\nIn April 2018, The Citizen (Tanzania) newspaper reported that construction of the dam would commence in July 2018.\n\n\n\n"}
{"id": "2295918", "url": "https://en.wikipedia.org/wiki?curid=2295918", "title": "Sulfonium", "text": "Sulfonium\n\nA sulfonium ion, also known as sulphonium ion or sulfanium ion, is a positively charged ion (a \"cation\") featuring three organic substituents attached to sulfur. These organosulfur compounds have the formula [SR]. Together with a negatively charged counterion, they give sulfonium salts. They are typically colorless solids that are soluble in organic solvent.\n\nSulfonium compounds are usually synthesized by the reaction of thioethers with alkyl halides. For example, the reaction of dimethyl sulfide with iodomethane yields trimethylsulfonium iodide:\n\nBefore the above reaction, the sulfur atom has two lone electron pairs. One of these lone pairs links to the methyl group. At the same time, as part of a concerted nucleophilic substitution mechanism (S2), the iodide leaving group departs. This leaves a positively charged trimethylsulfonium ion, whose charge is balanced by the iodide. The rate of reaction is even faster with stronger methylating agents, such as methyl trifluoromethanesulfonate.\n\nThe compounds are pyramidal at sulfur. Thus, MeS is isostructural and isoelectronic to trimethylphosphine. Sulfonium compounds wherein the three substituents differ are chiral and optically stable.\n\nThe sulfonium (more specifically methioninium) species \"S\"-adenosylmethionine occurs widely in nature, where it is used as a source of the adenosoyl radical. This radical participates in the biosynthesis of various compounds.\nAnother, sulfonium (methioninium) species found in nature is \"S\"-methylmethionine.\n\nSulfonium salts are precursor to sulfur ylides, which are useful in C-C forming reactions. In a typical application, a RSCHR′ center is deprotonated to give the ylide RSCHR.\n\nTris(dimethylamino)sulfonium difluorotrimethylsilicate [((CH)N)S][FSi(CH)] is a popular fluoridation agent.\n\nSome azo dyes are modified with sulfonium groups to give them a positive charge. The compound triphenylsulfonium triflate is a photoacid, a compound that under light converts to an acid.\n\n\n"}
{"id": "49639712", "url": "https://en.wikipedia.org/wiki?curid=49639712", "title": "TUGE Energia", "text": "TUGE Energia\n\nTUGE Energia is a wind turbine manufacturer headquartered in Estonia. It manufactures and supplies small wind turbines with capacity of 2, 10 and 50 kW. \n\nThe company was founded in 2015 in Tallinn. At the beginning of 2017, it has more than 20 small wind turbines installed in Estonia and Finland. Company also provides complementary solar and battery storage solutions.\n\nThe company is a member of a World Wind Energy Association.\n\n\n"}
{"id": "40366920", "url": "https://en.wikipedia.org/wiki?curid=40366920", "title": "Tehachapi Wind Resource Area", "text": "Tehachapi Wind Resource Area\n\nTehachapi Wind Resource Area, also known as TWRA, is considered the largest wind resource area of California. \n\nThe resource area is located in Kern County at the southern end of the San Joaquin Valley and spreads through Tehachapi Pass into the adjacent Mojave Desert. It is the home of many large scale wind farms which collectively produce more power than any other wind development in the United States. \n\nThe development of the Tehachapi Wind Resource Area has been in the planning since 2009 in conjunction with the development of the Tehachapi Renewable Transmission Project. The transmission project was required to support new wind developments in the area at the time including Alta-Oak Creek Mojave Project which was part of Alta Wind Energy Center, the largest wind farm in the world .\n\n"}
{"id": "9947735", "url": "https://en.wikipedia.org/wiki?curid=9947735", "title": "Zero-fuel weight", "text": "Zero-fuel weight\n\nThe zero-fuel weight (ZFW) of an aircraft is the total weight of the airplane and all its contents, minus the total weight of the usable fuel on board. Unusable fuel is included in ZFW.\n\nFor example, if an aircraft is flying at a weight of 5,000 kg and the weight of fuel on board is 500 kg, the ZFW is 4,500 kg. Some time later, after 100 kg of fuel has been used, the total weight of the airplane is 4,900 kg, the weight of fuel is 400 kg, and the ZFW is unchanged at 4,500 kg.\n\nAs a flight progresses and fuel is consumed, the total weight of the airplane reduces, but the ZFW remains constant (unless some part of the load, such as parachutists or stores, is jettisoned in flight).\n\nFor many types of airplane, the airworthiness limitations include a maximum zero fuel weight.\n\nThe maximum zero fuel weight (MZFW) is the maximum weight allowed before usable fuel and other specified usable agents (engine injection fluid, and other consumable propulsion agents) are loaded in defined sections of the aircraft as limited by strength and airworthiness requirements. It may include usable fuel in specified tanks when carried in lieu of payload. The addition of usable and consumable items to the zero fuel weight must be in accordance with the applicable government regulations so that airplane structure and airworthiness requirements are not exceeded.\n\nWhen an aircraft is being loaded with crew, passengers, baggage and freight it is most important to ensure that the ZFW does not exceed the MZFW. When an aircraft is being loaded with fuel it is most important to ensure that the takeoff weight will not exceed the maximum permissible takeoff weight.\n\nMZFW : The maximum weight of an aircraft prior to fuel being loaded.\n\nFor any aircraft with a defined MZFW, the maximum payload (formula_2) can be calculated as the MZFW minus the OEW (operational empty weight)\n\nThe maximum zero fuel weight is an important parameter in demonstrating compliance with gust design criteria for transport category airplanes.\n\nIn fixed-wing aircraft, fuel is usually carried in the wings. Weight in the wings does not contribute as significantly to the bending moment in the wing as does weight in the fuselage. This is because the lift on the wings and the weight of the fuselage bend the wing tips upwards and the wing roots downwards; but the weight of the wings, including the weight of fuel in the wings, bend the wing tips downwards, providing relief to the bending effect on the wing.\n\nWhen an airplane is being loaded, the capacity for extra weight in the wings is greater than the capacity for extra weight in the fuselage. Designers of airplanes can optimise the maximum takeoff weight and prevent overloading in the fuselage by specifying a MZFW. This is usually done for large airplanes with cantilever wings. (Airplanes with strut-braced wings achieve substantial wing bending relief by having the load of the fuselage applied by the strut mid-way along the wing semi-span. Extra wing bending relief cannot be achieved by particular placement of the fuel. There is usually no MZFW specified for an airplane with a strut-braced wing.)\n\nMost small airplanes do not have an MZFW specified among their limitations. For these airplanes with cantilever wings, the loading case that must be considered when determining the maximum takeoff weight is the airplane with zero fuel and all disposable load in the fuselage. With zero fuel in the wing the only wing bending relief is due to the weight of the wing.\n\n"}
