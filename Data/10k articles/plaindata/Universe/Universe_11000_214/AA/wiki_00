{"id": "2710775", "url": "https://en.wikipedia.org/wiki?curid=2710775", "title": "API-TC", "text": "API-TC\n\nAPI TC is a certification for two-stroke oils, awarded by the American Petroleum Institute. It is given after the product passes through stringent tests that determine the level of detergent performance, dispersion, and anti-oxidation. It is the only remaining, not revoked classification of the API Two-Cycle motor oil specifications (TA, TB, TC, TD). Being a very old standard itself, most currently produced 2T lubricants meet its specifications, even the lowest quality ones; current high-quality oils exceed them (often labeled \"API TC+\" although not based on actual measurements).\n\nThe more current JASO_M345 or the international ISO two-cycle oil specifications are much better indicators of oil quality, with requirements based on modern two-stroke engines and environmental policies.\n\n"}
{"id": "34985440", "url": "https://en.wikipedia.org/wiki?curid=34985440", "title": "Aquion Energy", "text": "Aquion Energy\n\nAquion Energy is a Pittsburgh-based company that manufactured sodium ion batteries (salt water batteries) and electricity storage systems.\n\nThe company claims to provide a low-cost way to store large amounts of energy (e.g. for an electricity grid) through thousands of battery cycles, and a non-toxic end product made from widely available material inputs and which operates safely and reliably across a wide range of temperatures and operating environments.\n\nThe company was founded in 2008 by Jay F. Whitacre, a professor at Carnegie Mellon University, and Ted Wiley. They set up research and development offices in Lawrenceville, where it produced pilot-stage batteries.\nWhitacre received a BA in physics from Oberlin College and a PhD in materials science from the University of Michigan. He held positions at California Institute of Technology and the Jet Propulsion Laboratory, studying energy-related topics ranging from fundamental materials function to systems engineering. In 2007 he accepted a professorship at Carnegie Mellon.\n\nThe company raised funding from Kleiner Perkins, Foundation Capital, Bill Gates, Nick and Jobey Pritzker, Bright Capital and Advanced Technology Ventures, among others.\n\nIn 2011, an individual battery stack was promoted to store 1.5 kWh, a shipping container-sized unit 180 kWh.\nThe battery cannot overheat.\nThe company expected its products to last many charge/discharge cycles, twice as long as a lead-acid battery. Costs were claimed to be about the same as with lead-acid.\n\nIn October 2014 they announced a new generation with a single stack reaching 2.4 kWh and a multi-stack module holding 25.5 kWh.\n\nIn 2015, the company announced it would supply batteries for a Hawaii microgrid to serve as backup for a 176-kilowatt solar panel array that would store 1,000 kilowatt-hours of electricity.\nIn April 2015 they announced they have been cradle-to-cradle design certified. It was also reported they were reducing headcount.\n\nIn September 2015, Whitacre won the Lemelson–MIT Prize.\n\nIn March 2017, Aquion Energy filed for voluntary bankruptcy under Chapter 11.\n\nIn June 2017, bidding starting with a stalking horse offer of $2.8 million from an Austrian battery firm, BlueSky Energy. Juline-Titans LLC, an affiliate of the China Titans Energy Technology Group, won the bidding with an offer of $9.16 million - a small fraction of the reported $190 million in venture capital and debt the company had raised from investors including Bill Gates, Gentry Venture Partners, Kleiner Perkins Caufield & Byers, Foundation Capital, Bright Capital, Advanced Technology Ventures, Trinity Capital Investment and CapX Partners, Yung’s Enterprise, and Nick and Joby Pritzker.\n\nIn August 2017, MIT Technology Review claimed the China Titans acquisition would mean that Aquion \"\"will continue operating as an independent entity, with research and development probably remaining in Pittsburgh. But manufacturing may move elsewhere, potentially somewhere in China\".\"\n\nIn September 2017 Juline-Titans closed the East Huntingdon Township facility and moved production to China.\n\nThe battery materials are non-toxic. As of early 2014, the cathode used manganese oxide and relies on intercalation reactions. The anode was a titanium phosphate (NaTi(PO)). The electrolyte was <5M NaClO. A synthetic cotton separator was reported. The electrode layers were unusually thick (>2 mm), which reduces power density. The device used Siemens power inverter technology.\n\nThe company set up manufacturing facilities at a former Sony television assembly plant in East Huntingdon, Pennsylvania initially proposing a capacity of 500 megawatt-hours per year in 2013 and 2014. In March 2014 they announced that commercial shipments of batteries would begin in mid-2014, and in May 2014 announced they had shipped 100 units.\n\n\n"}
{"id": "21921858", "url": "https://en.wikipedia.org/wiki?curid=21921858", "title": "Basic sediment and water", "text": "Basic sediment and water\n\nBasic sediment and water (BS&W) is a technical specification of certain impurities in crude oil. When extracted from an oil reservoir, the crude oil will contain some amount of water and suspended solids from the reservoir formation. The particulate matter is known as sediment or mud. The water content can vary greatly from field to field, and may be present in large quantities for older fields, or if oil extraction is enhanced using water injection technology. The bulk of the water and sediment is usually separated at the field to minimize the quantity that needs to be transported further. The residual content of these unwanted impurities is measured as BS&W. Oil refineries may either buy crude to a certain BS&W specification or may alternatively have initial crude oil dehydration and desalting process units that reduce the BS&W to acceptable limits, or a combination thereof.\n"}
{"id": "3900510", "url": "https://en.wikipedia.org/wiki?curid=3900510", "title": "Bosenova", "text": "Bosenova\n\nA bosenova or bose supernova is a very small, supernova-like explosion, which can be induced in a Bose–Einstein condensate (BEC) by changing the external magnetic field, so that the \"self-scattering\" interaction transitions from repulsive to attractive due to the Feshbach resonance, causing the BEC to \"collapse and bounce\" or \"rebound.\"\n\nAlthough the total energy of the explosion is very small, the \"collapse and bounce\" scenario qualitatively resembles a condensed matter version of a core-collapse supernova, hence the term \"bosenova\". The nomenclature is a play of words on the Brazilian music style, bossa nova.\n\nIn the particular experiment when a bosenova was first detected, transitioning the self-interaction from repulsive to attractive caused the BEC to implode and shrink to a size smaller than the optical detector's minimum resolution limit, and then suddenly \"explode.\" In this explosion, about half of the atoms in the condensate superficially seemed to have \"disappeared\" from the experiment altogether, i.e., they were not detected in either the cold particle remnants nor in the expanding gas cloud produced.\n\nUnder current BEC theory, which only very crudely accounts for the interactions between the particles composing the BEC, the bosenova phenomenon remains unexplained, because the energy available to the individual atoms of the condensate near absolute zero appears to be insufficient to cause the observed implosion. However, subsequent mean-field theories have been proposed to explain bosenovas as a collective phenomenon.\n\nThe bosenova behaviour of a BEC may provide insights into the behavior of a neutron star, as well as into the possible properties of still-hypothetical boson stars and into the quantum theory of \"collective phenomena\" in general.\n\n"}
{"id": "51515331", "url": "https://en.wikipedia.org/wiki?curid=51515331", "title": "Captive power plant", "text": "Captive power plant\n\nA captive power plant also called autoproducer or embedded generation is a power generation facility used and managed by an industrial or commercial energy user for their own energy consumption. Captive power plants can operate off-grid or they can be connected to the electric grid to exchange excess generation.\n\nCaptive power plants are generally used by power-intensive industries where continuity and quality of energy supply are crucial, such as aluminum smelters, steel plants, chemical industries etc.\n\n"}
{"id": "16432404", "url": "https://en.wikipedia.org/wiki?curid=16432404", "title": "Carbon emissions reporting", "text": "Carbon emissions reporting\n\nHuman activities — including the burning of fossil fuels, deforestation, agricultural practices (e.g., the use of fertilizer, and raising livestock), industrial processes, refrigeration, and the use of several consumer products — result in the emission of greenhouse gases. \n\nOne method to encourage and possibly regulate companies to reduce the greenhouse gas emissions resulting from their activities, is through the voluntary or mandatory reporting of companies' greenhouse gas emissions. Large power stations and manufacturing plants are often required to report their emissions to appropriate government entities, for example to the European Union as part of the Emissions Trading System or to the US EPA as part of the Greenhouse Gas Reporting Program. In the United Kingdom, Department for Environment, Food and Rural Affairs (Defra) has described climate change as the \"greatest environmental challenge facing the world today,\" and it is now a legal requirement for all quoted companies to report their annual greenhouse gas emissions.\n\nIn the past there were several attempts to institute mandatory reporting legislation but none has been implemented in the US. In the wake of BP oil spill in the Gulf of Mexico and increasing social awareness about the environment, the US Environmental Protection Agency (EPA) started the environmental Greenhouse Gas Reporting Program. The EPA’s GHG reporting program became a law on January 1, 2010. It forces 85% of the nation’s top emitters to report on how much GHG they have emitted. According to this law companies are due to report their emission for the year of 2010 on March 21 of 2011.\n\nIn the first year of this legislation only 85% of the nations leading emitters are required to report their annual reports. Plans are to slightly increase this number each to start keep tabs on the amount they emit.\n\nThis program is the initial step into countering rising emissions rate. While many believe that if companies are forced to report their emissions, they will be more inclined to lower their impact, this effect has not been thoroughly studied. The ability to attract more investment as consumers prefer environmentally friendly products may be another incentive, but again there is little evidence to support any strong claims.\n\nIn June 2012, the UK coalition government announced the introduction of mandatory carbon reporting, requiring all UK companies listed on the Main Market of the London Stock Exchange - around 1,100 of the UK’s largest listed companies - to report their greenhouse gas emissions every year. Deputy Prime Minister Nick Clegg confirmed that emission reporting rules would come into effect from April 2013 in his piece for The Guardian. However, this date has now been set to 1 October 2013.\n\nThe ISO 14064 standards for greenhouse gas accounting and verification published on 1 March 2006 by ISO (International Organization for Standardization) provide government and industry with an integrated set of tools for programmes aimed at reducing greenhouse gas emissions, as well as for emissions trading.\n\nISO 14064-1:2006 specifies principles and requirements at the organization level for quantification and reporting of greenhouse gas (GHG) emissions and removals. It includes requirements for the design, development, management, reporting and verification of an organization's GHG inventory.\n\nISO 14064-2:2006 specifies principles and requirements and provides guidance at the project level for quantification, monitoring and reporting of activities intended to cause greenhouse gas (GHG) emission reductions or removal enhancements. It includes requirements for planning a GHG project, identifying and selecting GHG sources, sinks and reservoirs relevant to the project and baseline scenario, monitoring, quantifying, documenting and reporting GHG project performance and managing data quality.\n\nISO 14064-3:2006 specifies principles and requirements and provides guidance for those conducting or managing the validation and/or verification of greenhouse gas (GHG) assertions. It can be applied to organizational or GHG project quantification, including GHG quantification, monitoring and reporting carried out in accordance with ISO 14064-1 or ISO 14064-2.\n\nMany companies have adopted the standards put forth by the Greenhouse Gas Protocol, a partnership between the World Resources Institute (WRI) and the World Business Council for Sustainable Development (WBCSD).The Greenhouse Gas Protocol (GHGP) provides accounting and reporting standards, sector guidance, calculation tools, and trainings for business and government. It establishes a comprehensive, global, standardized framework for measuring and managing emissions from private and public sector operations, value chains, products, cities, and policies. A new universal method for logistics emissions accounting has been launched in June 2016 in collaboration with the World Resources Institute (WRI) and the Greenhouse Gas protocol. It's called the GLEC framework (Global Logistics Emissions Council). The Greenhouse Gas Protocol is recognized by the UK government as an independent standard for reporting greenhouse gases. The Greenhouse Gas Protocol divides emissions into 3 Scopes.\n\nScope 1 covers all direct GHG emissions by a company. It includes fuel combustion, company vehicles and fugitive emissions.\n\nScope 2 covers indirect GHG emissions from consumption of purchased electricity, heat or steam.\n\nScope 3 covers other indirect emissions, such as the extraction and production of purchased materials and fuels, transport-related activities in vehicles not owned or controlled by the reporting entity, electricity-related activities (e.g. T&D losses) not covered in Scope 2, outsourced activities, waste disposal, etc. Scope 3 emissions (also known as value chain emissions) often represent the largest source of greenhouse gas emissions and in some cases can account for up to 90% of the total carbon impact.\n\nCradle-to-gate (sometimes referred to as \"upstream\") emission factors, which include all emissions that occur in the life cycle of a material/product up to the point of sale by the producer.\n\nEmission factors that occur in the life cycle of a material/product after the sale by the producer. This includes \"distribution and storage\", \"use\" of the product and \"end-of-life\".\n\nISO (International Organization for Standardization), the World Resources Institute (WRI) and the World Business Council for Sustainable Development (WBCSD) have signed a Memorandum of Understanding (MoU) under which they have agreed to jointly promote the ISO 14064 standards and the WRI and WBCSD GHG Protocol standards. The move is in response to concerns among businesses and GHG program designers that the two standards might not be consistent and mutually supportive. In fact, for corporate accounting, requirements and guidance contained in ISO and GHG Protocol standards are consistent and they are designed so that they can be used in a complementary manner.\n\nWhilst specific critiques of carbon reporting have emerged (see below) by and large, the actual practice of how organisations account for and report emissions remains understudied. Studies of practices of carbon accounting and reporting point to systemic externalities and raise issues about accountability.\n\nWhen two or more individuals or organizations claim ownership of specific emission reductions or carbon offsets. Double-counting occurs when the greenhouse gas emissions (GHG) resulting from a particular activity are allocated to multiple parties in a supply chain, so that the total allocated emissions exceed the total actual emissions of that activity. For investors and according to cross-asset footprint calculations, double-counting can reach about 30-40% of an institutional investor’s portfolio emissions.\n\nA recent academic study (2004) on corporate disclosure of greenhouse gas emissions found that only 15 percent of companies that disclose GHG emissions report them in a manner that the authors consider complete with respect to scope of emissions, type of emissions, and reporting boundary. Another study (2012) of a Fortune 50 multinational company details the quality of how data was sourced and discusses the politics of making judgments about data quality.\n\n\n\n\n"}
{"id": "14695652", "url": "https://en.wikipedia.org/wiki?curid=14695652", "title": "Ceramography", "text": "Ceramography\n\nCeramography is the art and science of preparation, examination and evaluation of ceramic microstructures. Ceramography can be thought of as the metallography of ceramics. The microstructure is the structure level of approximately 0.1 to 100 µm, between the minimum wavelength of visible light and the resolution limit of the naked eye. The microstructure includes most grains, secondary phases, grain boundaries, pores, micro-cracks and hardness microindentions. Most bulk mechanical, optical, thermal, electrical and magnetic properties are significantly affected by the microstructure. The fabrication method and process conditions are generally indicated by the microstructure. The root cause of many ceramic failures is evident in the microstructure. Ceramography is part of the broader field of materialography, which includes all the microscopic techniques of material analysis, such as metallography, petrography and plastography. Ceramography is usually reserved for high-performance ceramics for industrial applications, such as 85–99.9% alumina (AlO) in Fig. 1, zirconia (ZrO), silicon carbide (SiC), silicon nitride (SiN), and ceramic-matrix composites. It is seldom used on whiteware ceramics such as sanitaryware, wall tiles and dishware.\n\nCeramography evolved along with other branches of materialography and ceramic engineering. Alois de Widmanstätten of Austria etched a meteorite in 1808 to reveal proeutectoid ferrite bands that grew on prior austenite grain boundaries. Geologist Henry Clifton Sorby, the \"father of metallography,\" applied petrographic techniques to the steel industry in the 1860s in Sheffield, England. French geologist Auguste Michel-Lévy devised a chart that correlated the optical properties of minerals to their transmitted color and thickness in the 1880s. Swedish metallurgist J.A. Brinell invented the first quantitative hardness scale in 1900. Smith and Sandland developed the first microindention hardness test at Vickers Ltd. in London in 1922. Swiss-born microscopist A.I. Buehler started the first metallographic equipment manufacturer near Chicago in 1936. Frederick Knoop and colleagues at the National Bureau of Standards developed a less-penetrating (than Vickers) microindention test in 1939. Struers A/S of Copenhagen introduced the electrolytic polisher to metallography in 1943. George Kehl of Columbia University wrote a book that was considered the bible of materialography until the 1980s. Kehl co-founded a group within the Atomic Energy Commission that became the International Metallographic Society in 1967.\n\nThe preparation of ceramic specimens for microstructural analysis consists of five broad steps: sawing, embedding, grinding, polishing and etching. The tools and consumables for ceramographic preparation are available worldwide from metallography equipment vendors and laboratory supply companies.\nMost ceramics are extremely hard and must be wet-sawed with a circular blade embedded with diamond particles. A metallography or lapidary saw equipped with a low-density diamond blade is usually suitable. The blade must be cooled by a continuous liquid spray.\n\nTo facilitate further preparation, the sawed specimen is usually embedded (or mounted or encapsulated) in a plastic disc, 25, 30 or 35 mm in diameter. A thermosetting solid resin, activated by heat and compression, e.g. mineral-filled epoxy, is best for most applications. A castable (liquid) resin such as unfilled epoxy, acrylic or polyester may be used for porous refractory ceramics or microelectronic devices. The castable resins are also available with fluorescent dyes that aid in fluorescence microscopy. The left and right specimens in Fig. 3 were embedded in mineral-filled epoxy. The center refractory in Fig. 3 was embedded in castable, transparent acrylic.\n\nGrinding is abrasion of the surface of interest by abrasive particles, usually diamond, that are bonded to paper or a metal disc. Grinding erases saw marks, coarsely smooths the surface, and removes stock to a desired depth. A typical grinding sequence for ceramics is one minute on a 240-grit metal-bonded diamond wheel rotating at 240 rpm and lubricated by flowing water, followed by a similar treatment on a 400-grit wheel. The specimen is washed in an ultrasonic bath after each step.\n\nPolishing is abrasion by free abrasives that are suspended in a lubricant and can roll or slide between the specimen and paper. Polishing erases grinding marks and smooths the specimen to a mirror-like finish. Polishing on a bare metallic platen is called lapping. A typical polishing sequence for ceramics is 5–10 minutes each on 15-, 6- and 1-µm diamond paste or slurry on napless paper rotating at 240 rpm. The specimen is again washed in an ultrasonic bath after each step. The three sets of specimens in Fig. 3 have been sawed, embedded, ground and polished.\n\nEtching reveals and delineates grain boundaries and other microstructural features that are not apparent on the as-polished surface. The two most common types of etching in ceramography are selective chemical corrosion, and a thermal treatment that causes relief. As an example, alumina can be chemically etched by immersion in boiling concentrated phosphoric acid for 30–60 s, or thermally etched in a furnace for 20–40 min at in air. The plastic encapsulation must be removed before thermal etching. The alumina in Fig. 1 was thermally etched.\n\nAlternatively, non-cubic ceramics can be prepared as thin sections, also known as petrography, for examination by polarized transmitted light microscopy. In this technique, the specimen is sawed to ~1 mm thick, glued to a microscope slide, and ground or sawed (e.g., by microtome) to a thickness (\"x\") approaching 30 µm. A cover slip is glued onto the exposed surface. The adhesives, such as epoxy or Canada balsam resin, must have approximately the same refractive index (η ≈ 1.54) as glass. Most ceramics have a very small absorption coefficient (α ≈ 0.5 cm for alumina in Fig. 2) in the Beer-Lambert law below, and can be viewed in transmitted light. Cubic ceramics, e.g. yttria-stabilized zirconia and spinel, have the same refractive index in all crystallographic directions and appear, therefore, black when the microscope's polarizer is 90° out of phase with its analyzer.\n\nCeramographic specimens are electrical insulators in most cases, and must be coated with a conductive ~10-nm layer of metal or carbon for electron microscopy, after polishing and etching. Gold or Au-Pd alloy from a sputter coater or evaporative coater also improves the reflection of visible light from the polished surface under a microscope, by the Fresnel formula below. Bare alumina (η ≈ 1.77, \"k\" ≈ 10) has a negligible extinction coefficient and reflects only 8% of the incident light from the microscope, as in Fig. 1. Gold-coated (\"η\" ≈ 0.82, \"k\" ≈ 1.59 @ λ = 500 nm) alumina reflects 44% in air, 39% in immersion oil.\n\nCeramic microstructures are most often analyzed by reflected visible-light microscopy in brightfield. Darkfield is used in limited circumstances, e.g., to reveal cracks. Polarized transmitted light is used with thin sections, where the contrast between grains comes from birefringence. Very fine microstructures may require the higher magnification and resolution of a scanning electron microscope (SEM) or confocal laser scanning microscope (CLSM). The cathodoluminescence microscope (CLM) is useful for distinguishing phases of refractories. The transmission electron microscope (TEM) and scanning acoustic microscope (SAM) have specialty applications in ceramography.\n\nCeramography is often done qualitatively, for comparison of the microstructure of a component to a standard for quality control or failure analysis purposes. Three common quantitative analyses of microstructures are grain size, second-phase content and porosity. Microstructures are measured by the principles of stereology, in which three-dimensional objects are evaluated in 2-D by projections or cross-sections.\n\nGrain size can be measured by the line-fraction or area-fraction methods of ASTM E112. In the line-fraction methods, a statistical grain size is calculated from the number of grains or grain boundaries intersecting a line of known length or circle of known circumference. In the area-fraction method, the grain size is calculated from the number of grains inside a known area. In each case, the measurement is affected by secondary phases, porosity, preferred orientation, exponential distribution of sizes, and non-equiaxed grains. Image analysis can measure the shape factors of individual grains by ASTM E1382.\n\nSecond-phase content and porosity are measured the same way in a microstructure, such as ASTM E562. Procedure E562 is a point-fraction method based on the stereological principle of point fraction = volume fraction, i.e., \"P\" = \"V\". Second-phase content in ceramics, such as carbide whiskers in an oxide matrix, is usually expressed as a mass fraction. Volume fractions can be converted to mass fractions if the density of each phase is known. Image analysis can measure porosity, pore-size distribution and volume fractions of secondary phases by ASTM E1245. Porosity measurements do not require etching. Multi-phase microstructures do not require etching if the contrast between phases is adequate, as is usually the case.\n\nGrain size, porosity and second-phase content have all been correlated with ceramic properties such as mechanical strength σ by the Hall–Petch equation. Hardness, toughness, dielectric constant and many other properties are microstructure-dependent.\n\nThe hardness of a material can be measured in many ways. The Knoop hardness test, a method of microindention hardness, is the most reproducible for dense ceramics. The Vickers hardness test and superficial Rockwell scales (e.g., 45N) can also be used, but tend to cause more surface damage than Knoop. The Brinell test is suitable for ductile metals, but not ceramics. In the Knoop test, a diamond indenter in the shape of an elongated pyramid is forced into a polished (but not etched) surface under a predetermined load, typically 500 or 1000 g. The load is held for some amount of time, say 10 s, and the indenter is retracted. The indention long diagonal (\"d\", μm, in Fig. 4) is measured under a microscope, and the Knoop hardness (HK) is calculated from the load (P, g) and the square of the diagonal length in the equations below. The constants account for the projected area of the indenter and unit conversion factors. Most oxide ceramics have a Knoop hardness in the range of 1000–1500 kg/mm (10 – 15 GPa), and many carbides are over 2000 (20 GPa). The method is specified in ASTM C849, C1326 & E384. Microindention hardness is also called microindentation hardness or simply microhardness. The hardness of very small particles and thin films of ceramics, on the order of 100 nm, can be measured by nanoindentation methods that use a Berkovich indenter.\n\nThe toughness of ceramics can be determined from a Vickers test under a load of 10 – 20 kg. Toughness is the ability of a material to resist crack propagation. Several calculations have been formulated from the load (P), elastic modulus (E), microindention hardness (H), crack length (\"c\" in Fig. 5) and flexural strength (σ). Modulus of rupture (MOR) bars with a rectangular cross-section are indented in three places on a polished surface. The bars are loaded in 4-point bending with the polished, indented surface in tension, until fracture. The fracture normally originates at one of the indentions. The crack lengths are measured under a microscope. The toughness of most ceramics is 2–4 MPa, but toughened zirconia is as much as 13, and cemented carbides are often over 20. The toughness-by-indention methods have been discredited recently and are being replaced by more rigorous methods that measure crack growth in a notched beam in flexure.\n\n"}
{"id": "10393875", "url": "https://en.wikipedia.org/wiki?curid=10393875", "title": "Chechnya Spetsnaz base explosion", "text": "Chechnya Spetsnaz base explosion\n\nThe Chechnya Spetsnaz base explosion was the February 8, 2006, blast at the Russian Defense Ministry's special security unit barracks at Kurchaloi near Grozny, Chechnya, which killed at least 13 Russian special forces troops and injured 28 more, according to the official statement. \n\nThe two-story building, which collapsed, housed unit of about 200 GRU servicemen, of which some 43 were believed to have been inside at the time of the explosion. The officials said a gas leak was the most likely cause but did not rule out other theories. Chechen separatists reported on the gas explosion theory with scepticism but did not suggest outright that militants had targeted it.\n"}
{"id": "32534667", "url": "https://en.wikipedia.org/wiki?curid=32534667", "title": "Chip formation", "text": "Chip formation\n\nChip formation is part of the process of cutting materials by mechanical means, using tools such as saws, lathes and milling cutters. An understanding of the theory and engineering of this formation is an important part of the development of such machines and their cutting tools.\n\nThe formal study of chip formation was encouraged around World War II and shortly afterwards, with increases in the use of faster and more powerful cutting machines, particularly for metal cutting with the new high speed steel cutters. Pioneering work in this field was carried out by Kivima (1952) and Franz (1958).\n\nChip formation is usually described according to a three-way model developed by Franz. This model is best known within the field of machine tool design, although it is also used when an application area, such as woodworking, requires a vocabulary to describe chip formation in more detail than is usually attempted.\n\nThe first three chip types are the original characterisation, by Dr. Norman Franz. The type of chip that forms depends on many factors, of both tool and material. In general, main factors are the angle formed by the edge faces of the tool and also the angle at which this is presented to the surface.\n\nSharpness of the cutting tool does not usually define the \"type\" of chip, but rather the \"quality\" of the chip, and the clear distinctions between types. A blunt tool produces a degenerate chip that is large, torn and varies from one means of formation to another, often leaving behind a poor quality surface where this means changes.\n\nType I chips form when a material splits \"ahead\" of the cutting edge, owing to some upwards wedge action of the tool exceeding the \"tensile\" strength of the material, perpendicular to the surface. They are thus particularly important in fibrous materials, such as wood, where individual fibres are strong but they may be levered apart relatively easily. Type I chips generally form in cutting by tools with shallow cutting angles.\n\nType I chips may form long, continuous swarf, limited in size only by the length of cut.\n\nThis is the idealised chip formation for wood shavings, particularly those produced by a well-tuned plane with a finely adjusted mouth.\n\nType II chips form when a shearing force is produced by the wedge of the tool angle. The material fails along a short angled plane, from the apex of the tool edge, diagonally upwards and forwards to the surface. The material deforms along this line, forming an upward curling chip. These chips generally form from intermediate cutting angles.\n\nType II chips may form in ductile materials, such as metals.\n\nType II chips may also form long, continuous swarf.\n\nType III chips form a compression failure of the material, ahead of a relatively obtuse cutting angle, approaching 90°. In some weak or non-ductile materials this may form an acceptable chip, usually as a fine dust, but often it gives rise instead to a random \"snowplough\" effect where the waste material is bunched up ahead of the tool but not cleared decisively away as a well-formed chip.\n\nThis type of chip is formed by routers. It is also formed by woodworking scrapers, although when properly sharpened and used, these form such a thin Type III chip that it instead appears as a well-formed Type II chip. Their waste chip is thin enough that the compression failure volume is small enough to act as for the well-defined shear plane of the Type II.\nThis type was characterised later, by William McKenzie (1960).\n"}
{"id": "1067538", "url": "https://en.wikipedia.org/wiki?curid=1067538", "title": "Cryosurgery", "text": "Cryosurgery\n\nCryosurgery is the use of extreme cold in surgery to destroy abnormal or diseased tissue; thus, it is the surgical application of cryoablation. The term comes from the Greek words cryo (κρύο) (\"icy cold\") and surgery (\"cheirourgiki\" – χειρουργική) meaning \"hand work\" or \"handiwork\".\n\nCryosurgery has been historically used to treat a number of diseases and disorders, especially a variety of benign and malignant skin conditions.\n\nWarts, moles, skin tags, solar keratoses, Morton's neuroma and small skin cancers are candidates for cryosurgical treatment. Several internal disorders are also treated with cryosurgery, including liver cancer, prostate cancer, lung cancer, oral cancers, cervical disorders and, more commonly in the past, hemorrhoids. Soft tissue conditions such as plantar fasciitis (jogger's heel) and fibroma (benign excrescence of connective tissue) can be treated with cryosurgery. \nGenerally, all tumors that can be reached by the cryoprobes used during an operation are treatable. Although found to be effective, this method of treatment is only appropriate for use against localized disease, and solid tumors larger than 1 cm. Tiny, diffuse metastases that often coincide with cancers are usually not affected by cryotherapy.\n\nCryosurgery works by taking advantage of the destructive force of freezing temperatures on cells. When their temperature sinks beyond a certain level ice crystals begin forming inside the cells and, because of their lower density, eventually tear apart those cells. Further harm to malignant growth will result once the blood vessels supplying the affected tissue begin to freeze.\n\nA common method of freezing lesions is using liquid nitrogen as the cooling solution. This cold liquid may be sprayed on the diseased tissue, circulated through a tube called a cryoprobe, or simply dabbed on with a cotton or foam swab.\n\nCarbon dioxide is also available as a spray and is used to treat a variety of benign spots. Less frequently, doctors use carbon dioxide \"snow\" formed into a cylinder or mixed with acetone to form a slush that is applied directly to the treated tissue.\n\nRecent advances in technology have allowed for the use of argon gas to drive ice formation using a principle known as the Joule-Thomson effect. This gives physicians excellent control of the ice, and minimizing complications using ultra-thin 17 gauge cryoneedles.\n\nA mixture of dimethyl ether and propane is used in some \"freeze spray\" preparations such as Dr. Scholl's Freeze Away. The mixture is stored in an aerosol spray type container at room temperature and drops to when dispensed. The mixture is often dispensed into a straw with a cotton-tipped swab. Similar products may use tetrafluoroethane or other substances.\n\nA number of medical supply companies have developed cryogen delivery systems for cryosurgery. Most are based on the use of liquid nitrogen, although some employ the use of proprietary mixtures of gases that combine to form the cryogen.\n\nCryosurgery is also used to treat internal and external tumors as well as tumors in the bone. To cure internal tumors, a hollow instrument called a cryoprobe is used, which is placed in contact with the tumor. Liquid nitrogen or argon gas is passed through the cryoprobe. Ultrasound or MRI is used to guide the cryoprobe and monitor the freezing of the cells. This helps in limiting damage to adjacent healthy tissues. A ball of ice crystals forms around the probe which results in freezing of nearby cells. When it is required to deliver gas to various parts of the tumor, more than one probe is used. After cryosurgery, the frozen tissue is either naturally absorbed by the body in the case of internal tumors, or it dissolves and forms a scab for external tumors.\n\nCryosurgery is a minimally invasive procedure, and is often preferred to more traditional kinds of surgery because of its minimal pain, scarring, and cost; however, as with any medical treatment, there are risks involved, primarily that of damage to nearby healthy tissue. Damage to nerve tissue is of particular concern.\n\nPatients undergoing cryosurgery usually experience redness and minor-to-moderate localized pain, which most of the time can be alleviated sufficiently by oral administration of mild analgesics such as Ibuprofen, codeine or acetaminophen (paracetamol). Blisters may form as a result of cryosurgery, but these usually scab over and peel away within a few days.\n\n"}
{"id": "53273108", "url": "https://en.wikipedia.org/wiki?curid=53273108", "title": "Cyclopropenium ion", "text": "Cyclopropenium ion\n\nThe cyclopropenium ion is the cation with the formula . It attracted attention as the smallest example of an aromatic cation. Its salts have been isolated, and many derivatives have been characterized by X-ray crystallography.\n\nWith two π electrons, the cyclopropenium cation class obeys Hückel’s rules of aromaticity for electrons since, in this case, . Consistent with this prediction, the CH core is planar and the C–C bonds are equivalent. In the case of the cation in [C(SiMe)], the ring C–C distances range from 1.374(2) to 1.392(2) Å.\nSalts of many cyclopropenyl cations have been characterized. Their stability varies according to the steric and inductive effects of the substituents.\n\nSalts of triphenylcyclopropenium were first reported by Ronald Breslow in 1957. The salt was prepared in two steps starting with the reaction phenyldiazoacetonitrile with diphenylacetylene to yield 1,2,3-triphenyl-2-cyclopropene nitrile. Treatment of this with boron trifluoride yielded [CPh]BF.\n\nThe parent cation, [CH], was reported as its hexachloroantimonate () salt in 1970. It is indefinitely stable at −20 °C.\n\nTrichlorocyclopropenium salts are generated by chloride abstraction from tetrachlorocyclopropene: \n\nTetrachlorocyclopropene can be converted to tris(\"tert\"-butyldimethylsilyl)cyclopropene. Hydride abstraction with nitrosonium tetrafluoroborate yields the trisilyl-substituted cyclopropenium cation.\n\nAmino-substituted cyclopropenium salts are particularly stable. Calicene is an unusual derivative featuring cyclopropenium linked to a cyclopentadienide.\n\nChloride salts of cyclopropenium esters are intermediates in the use of dichlorocyclopropenes for the conversion of carboxylic acids to acid chlorides:\n\nRelated cyclopropenium cations are produced in the regeneration of the 1,1-dichlorocyclopropenes from the cyclopropenones.\n\nThe cyclopropenium chlorides have been applied to peptide bond formation. For example, in the figure below, reacting a boc-protected amino acid with an unprotected amino acid in the presence of the cyclopropenium ion allows the formation of a peptide bond via acid chloride formation followed by nucleophilic substitution with the unprotected amino acid.\n\nThis method of mildly generating acid chlorides can also be useful for linking alpha-anomeric sugars. After using the cyclopropenium ion to form the chloride at the anomeric carbon, the compound is iodinated with tetrabutylammonium iodide. This iodine can thereafter be substituted by any ROH group to quickly undergo alpha-selective linkage of sugars.\n\nAdditionally, some synthetic routes make use of cyclopropenium ring openings yielding an allylcarbene cation. The linear degradation product yields both a nucleophilic and electrophilic carbon centers.\n\nMany complexes are known with cyclopropenium ligands. Examples include [M(CPh)(PPh)] (M = Ni, Pd, Pt) and Co(CPh)(CO). Such compounds are prepared by reaction of cyclopropenium salts with low valent metal complexes.\n\nBecause many substituted derivatives are known, cyclopropenium salts have attracted attention as possible polyelectrolytes, relevant to technologies such as desalination and fuel cells. The tris(dialkylamino)cyclopropenium salts have been particularly evaluated because of their high stability.\n\n"}
{"id": "5618599", "url": "https://en.wikipedia.org/wiki?curid=5618599", "title": "Dallol, Ethiopia", "text": "Dallol, Ethiopia\n\nDallol (Amharic: ዳሎል) is a locality in the Dallol woreda of northern Ethiopia. Located in Administrative Zone 2 of the Afar Region in the Afar Depression, it has a latitude and longitude of with an elevation of about below sea level. The Central Statistical Agency has not published an estimate for this settlement's 2005 population; it has been described as a ghost town.\n\nDallol currently holds the official record for record high average temperature for an inhabited location on Earth, where an average annual temperature of 35°C (95°F) was recorded between the years 1960 and 1966. Dallol is also one of the most remote places on Earth, although paved roads to the village of Hamedela, which is close, are being built. Still, the most important mode of transport besides jeeps are the camel caravans which travel to the area to collect salt.\n\nNearby is the Dallol volcano, which last erupted in 2011.\n\nA railway from the port of Mersa Fatma in Eritrea to a point 28 km from Dallol was completed in April 1918. Built from 1917-1918, using the 600 mm gauge Decauville system (\"Decauville\" describes ready-made sections of small-gauge track which can be rapidly assembled) it transported salt from the \"Iron Point\" rail terminal near Dallol, via Kululli to the port.\n\nPotash production is said to have reached about 51,000 metric tons after the railway was constructed. Production was stopped after World War I owing to large-scale supplies from Germany, USA, and USSR. Unsuccessful attempts to reopen production were made in the period 1920-1941. Between the years 1925-29 an Italian company mined 25,000 tons of sylvite, averaging 70% KCl, which was transported by rail to Mersa Fatma. After the Second World War, the British administration dismantled the railway and removed all traces of it.\n\nThe Dallol Co. of Asmara sold a few tons of salt from this site to India in 1951-1953. In the 1960s, the Parsons Company of the USA, a mining company, conducted a series of geological surveys at Dallol. By 1965, about 10,000 holes had been drilled at 65 locations.\n\nDallol became more known in the West in 2004 when it was featured in the Channel 4/National Geographic documentary \"Going to Extremes\". , some buildings still stand in Dallol, all built with salt blocks.\n\nDallol features an extreme version of a hot desert climate (Köppen climate classification \"BWh\") typical of the Danakil Desert. Dallol is the hottest place year-round on the planet and currently holds the record high average temperature for an inhabited location on Earth, where an average annual temperature of 34.6 °C (94.3 °F) was recorded between the years 1960 and 1966. The annual average high temperature is 41 °C (105 °F) and the hottest month has an average high of 46.7 °C (116.1 °F). In addition to being extremely hot year-round, the climate of the lowlands of the Danakil Depression is also extremely dry and hyperarid in terms of annual average rainy days as only a few days record measurable precipitation. The hot desert climate of Dallol is particularly due to the extremely low elevation, it being inside the tropics and near the hot Red Sea during winters, the very low seasonality impact, the constants of the extreme heat and the lack of nighttime cooling.\n\n"}
{"id": "558379", "url": "https://en.wikipedia.org/wiki?curid=558379", "title": "Dirham", "text": "Dirham\n\nDirham, dirhem or dirhm (درهم) was and, in some cases, still is a unit of currency in several Arab states. It was formerly the related unit of mass (the Ottoman dram) in the Ottoman Empire and old Persian states.\n\nThe dirham was a unit of weight used across North Africa, the Middle East, and Persia, with varying values.\n\nIn the late Ottoman Empire (Ottoman Turkish درهم), the standard dirham was 3.207 g; 400 dirhem equal one oka. The Ottoman dirham was based on the Sassanian drachm, which was itself based on the Roman dram/drachm.\n\nIn Egypt in 1895, it was equivalent to 47.661 troy grains (3.088 g).\n\nThere is currently a movement within the Islamic world to revive the Dirham as a unit of mass for measuring silver, although the exact value is disputed (either 3 grams or 2.975 grams).\n\nThe word \"dirham\" is an Arabic word from the Ancient Greek \"drachma\" which was originally a gold coin. Near the end of the 7th century the coin became an Islamic currency bearing the name of the sovereign and a religious verse. The dirham was struck in many Mediterranean countries, including Al-Andalus (Moorish Spain) and the Byzantine Empire (\"miliaresion\"), and could be used as currency in Europe between the 10th and 12th centuries, notably in areas with Viking connections, such as Viking York and Dublin.\nThe \"dirham\" is frequently mentioned in Jewish orthodox law as a unit of weight used to measure various requirements in religious functions, such as the weight in silver specie pledged in Marriage Contracts (\"Ketubbah\"), the quantity of flour requiring the separation of the dough-portion, etc. Jewish physician and philosopher, Maimonides, uses the Egyptian \"dirham\" to approximate the quantity of flour for dough-portion, writing in Mishnah \"Eduyot\" 1:2: \"...And I found the rate of the dough-portion in that measurement to be approximately five-hundred and twenty \"dirhams\" of wheat flour, while all these dirhams are the Egyptian [\"dirham\"].\" This view is repeated by Maran's \"Shulhan Arukh\" (\"Hil. Hallah\", Yoreh Deah § 324:3) in the name of the Tur. In Maimonides' commentary of the Mishnah (\"Eduyot\" 1:2, note 18), Rabbi Yosef Qafih explains that the weight of each Egyptian \"dirham\" was approximately 3.333 grammes, or what was the equivalent to 16 carob-grains which, when taken together, the minimum weight of flour requiring the separation of the dough-portion comes to approx. 1 kilo and 733 grammes. Rabbi Ovadiah Yosef, in his \"Sefer Halikhot ʿOlam\" (vol. 1, pp. 288-291), makes use of a different standard for the Egyptian \"dirham\", saying that it weighed approx. 3.0 grammes, meaning the minimum requirement for separating the priest's portion is 1 kilo and 560 grammes. Others (e.g. Rabbi Avraham Chaim Naeh) say the Egyptian \"dirham\" weighed approx. 3.205 grammes, which total weight for the requirement of separating the dough-portion comes to 1 kilo and 666 grammes. Rabbi Shelomo Qorah (Chief Rabbi of Bnei Barak) writes that the traditional weight used in Yemen for each \"dirham\" weighed 3.36 grammes, making the total weight for the required separation of the dough-portion to be 1 kilo and 770.72 grammes.\n\nThe word \"drachmon\" (), used in some translations of Maimonides' commentary of the Mishnah, has in all places the same connotation as \"dirham\".\n\nCurrently the valid national currencies with the name \"dirham\" are :\n\n\nModern currencies with the subdivision \"dirham\" or \"diram\" are:\n\nAlso the unofficial modern gold dinar is divided into dirham.\n\n"}
{"id": "12440261", "url": "https://en.wikipedia.org/wiki?curid=12440261", "title": "East Carpathians Protected Landscape Area", "text": "East Carpathians Protected Landscape Area\n\nEast Carpathians Protected Landscape Area () is a protected landscape area in north-eastern Slovakia, in the Prešov Region, in the Carpathian Mountains. The PLA occupies a slice of land along with the border with Poland. The park was first created in 1977, when it was originally larger than today, occupying area of 968.1 km (373.8 sq mi) until 1997, when the Poloniny National Park was created in the eastern part, reducing the area into 253.07 km (97.7 sq mi).\n\nBeech forests are dominant in the park, with the corresponding plant and animal associations. Original growth forests constitute an ideal environment for protected or threatened species, such as the gray wolf, lynx, bear, otter and others.\n\n"}
{"id": "34425993", "url": "https://en.wikipedia.org/wiki?curid=34425993", "title": "Electrofuel", "text": "Electrofuel\n\nElectrofuels (synthetic fuel) are an emerging class of carbon-neutral drop-in replacement fuels that are made by storing electrical energy from renewable sources in the chemical bonds of liquid or gas fuels. The primary targets are butanol, biodiesel, and hydrogen, but include other alcohols and carbon-containing gasses such as methane and butane.\n\nA primary source of funding for research on liquid electrofuels for transportation is the Electrofuels Program of the Advanced Research Projects Agency-Energy (ARPA-E), headed by Eric Toone. ARPA-E, created in 2009 under President Obama’s Secretary of Energy Steven Chu, is the Department of Energy’s (DOE) attempt to duplicate the effectiveness of the Defense Advanced Research Projects Agency, DARPA. Examples of projects funded under this program include OPX Biotechnologies’ biodiesel effort led by Michael Lynch and Derek Lovley’s work on microbial electrosynthesis at the University of Massachusetts Amherst, which reportedly produced the first liquid electrofuel using CO2 as the feedstock. Descriptions of all ARPA-E Electrofuels Program research projects can be found at the ARPA-E Electrofuels Program website.\n\nThe first Electrofuels Conference, sponsored by the American Institute of Chemical Engineers was held in Providence, RI in November 2011. At that conference, Director Eric Toone stated that \"Eighteen months into the program, we know it works. We need to know if we can make it matter.\" Several groups are beyond proof-of-principle, and are working to scale up cost-effectively.\n\nElectrofuels have the potential to be disruptive if carbon-neutral electrofuels can be cheaper than petroleum fuels, and chemical feedstocks produced by electrosynthesis cheaper than those refined from crude oil. Electrofuels also have a great potential to alter the renewable energy landscape, as electrofuels allow renewables from all sources to be stored conveniently as a liquid fuel.\n\n, prompted by the fracking boom, ARPA-E's focus has moved from electrical feedstocks to natural-gas based feedstocks, and thus away from electrofuels.\n\n"}
{"id": "2941671", "url": "https://en.wikipedia.org/wiki?curid=2941671", "title": "Esso Longford gas explosion", "text": "Esso Longford gas explosion\n\nThe Esso Longford gas explosion was a catastrophic industrial accident which occurred at the Esso natural gas plant at Longford in the Australian state of Victoria's Gippsland region. On 25 September 1998, an explosion took place at the plant, killing two workers and injuring eight. Gas supplies to the state of Victoria were severely affected for two weeks.\n\nIn 1998, the Longford gas plant was owned by a joint partnership between Esso and BHP. Esso was responsible for the operation of the plant. Esso was a wholly owned subsidiary of US based company Exxon, which has since merged with Mobil, becoming ExxonMobil. BHP has since merged with UK based Billiton becoming BHP Billiton.\n\nBuilt in 1969, the plant at Longford is the onshore receiving point for oil and natural gas output from production platforms in Bass Strait. The Longford Gas Plant Complex consists of three gas processing plants and one crude oil stabilisation plant. It was the primary provider of natural gas to Victoria, and provided some supply to New South Wales.\n\nThe feed from the Bass Strait platforms consists of liquid and gaseous hydrocarbons, water (HO) and hydrogen sulfide (HS). The water and HS are removed before reaching the plant, leaving a hydrocarbon stream to be the feed to Gas Plant 1. This stream contained both gaseous and liquid components. The liquid component was known as \"condensate\". The LPG is further extracted by means of a shell and tube heat exchanger, in which heated \"lean oil\" and cold \"rich oil\" (oil which has absorbed LPG) are pumped into the exchanger, cooling the lean oil and heating the rich oil.\n\nDuring the morning of Friday 25 September 1998, a pump supplying heated lean oil to heat exchanger GP905 in Gas Plant No. 1 went offline for four hours, due to an increase in flow from the Marlin Gas Field which caused an overflow of condensate in the absorber. (The plant was complex and the hot oil pump was only one component involved in the accident process; why the pump shut down is complicated and important.)\n\nA heat exchanger is a vessel that allows the transfer of heat from a hot stream to a cold stream, and so does not operate at a single temperature, but experiences a range of temperatures throughout the vessel. Temperatures throughout GP905 normally ranged from 60 °C to 230 °C (140 °F to 446 °F). Investigators estimated that, due to the failure of the lean oil pump, parts of GP905 experienced temperatures as low as . Ice had formed on the unit, and it was decided to resume pumping heated lean oil in to thaw it. When the lean oil pump resumed operation, it pumped oil into the heat exchanger at —the temperature differential caused a brittle fracture in the exchanger (GP905) at 12.26pm.\n\nAbout 10 metric tonnes of hydrocarbon vapour were immediately vented from the rupture. A vapour cloud formed and drifted downwind. When it reached a set of heaters 170 metres away, it ignited. This caused a deflagration (a burning vapour cloud). The flame front burnt its way through the vapour cloud, without causing an explosion. When the flamefront reached the rupture in the heat exchanger, a fierce jet fire developed that lasted for two days.\n\nThe rupture of GP905 led to other releases and minor fires. The main fire was an intense jet fire emanating from GP905. There was no blast wave—the nearby control room was undamaged. Damage was localised to the immediate area around and above the GP905 exchanger.\n\nPeter Wilson and John Lowery were killed in the accident and eight others were injured.\n\nThe fire at the plant was not extinguished until two days later. The Longford plant was shut down immediately, and the state of Victoria was left without its primary gas supplier. Within days, VENCorp shut down the state's entire gas supply. The resulting gas supply shortage was devastating to Victoria's economy, crippling industry and the commercial sector (in particular, the hospitality industry which relied on natural gas for cooking). Loss to industry during the crisis was estimated at around A$1.3 billion.\n\nAs natural gas was also widely used in houses in Victoria for cooking, water heating and home heating, many Victorians endured 20 days without gas, hot-water or heating.\n\nGas supplies to Victoria resumed on 14 October. Many Victorians were outraged and upset to discover only minor compensation on their next gas bill, with the average compensation figure being only around $10.\n\nA Royal Commission was called into the explosion at Longford, headed by former High Court judge Daryl Dawson. The Commission sat for 53 days, commencing with a preliminary hearing on 12 November 1998 and concluding with a closing address by Counsel Assisting the Royal Commission on 15 April 1999.\n\nEsso blamed the accident on worker negligence, in particular Jim Ward, one of the panel workers on duty on the day of the explosion.\n\nThe findings of the Royal Commission, however, cleared Ward of any negligence or wrongdoing. Instead, the Commission found Esso fully responsible for the accident:\n\nOther findings of the Royal Commission included:\n\nCertain managerial shortcomings were also identified:\n\nEsso was taken to the Supreme Court of Victoria by the Victorian WorkCover Authority. The jury found the company guilty of eleven breaches of the Occupational Health and Safety Act 1985, and Justice Philip Cummins imposed a record fine of $2 million in July 2001.\n\nIn addition, a class action was taken on behalf of businesses, industries and domestic users who were financially affected by the gas crisis. The class action went to trial in the Supreme Court on 4 September 2002, and was eventually settled in December 2004 when Esso was ordered to pay $32.5 million to businesses which suffered property damage as a result of the incident.\n\nFollowing the Longford accident, Victoria introduced Major Hazard Facilities regulations to regulate safety at plants that contain major chemical hazards. These regulations impose a so-called \"non-prescriptive\" regime on facility operators, requiring them to \"demonstrate\" control of major chemical hazards via the use of a Safety Management System and a Safety Case.\n\nOther states have also implemented similar regulatory regimes.\n\n\n\n"}
{"id": "13618937", "url": "https://en.wikipedia.org/wiki?curid=13618937", "title": "Fault breccia", "text": "Fault breccia\n\nFault breccia ( or ; Italian for \"breach\"), or tectonic breccia, is a breccia (a rock type consisting of angular clasts) that was formed by tectonic forces.\n\nFault breccia is a tectonite formed by localized zone of brittle deformation (a fault zone) in a rock.\n\nFault breccias are tectonites formed primarily by tectonic movement along a localized zone of brittle deformation (a fault zone) in a rock formation or province.\n\nThe grinding and milling occurring when the two sides of the fault zone moving along each other results in a material that is made of loose fragments. Because of this fragmentation fault zones are easily infiltrated by groundwater.\n\nSecondary minerals such as calcite, epidote, quartz or talc can precipitate from the circulating groundwater filling the voids and cementing the rock. However, when the tectonic movement along the fault zone continues the cement itself can be fragmented leading to a new gouge material containing neoformed clasts.\n\nDeeper in the Earth's crust, where temperatures and pressures are higher, the rocks in the fault zone can still brecciate, but they keep their internal cohesion. The resulting type of rock is called a cataclasite.\n\nFault breccia has no cohesion; it is normally an unconsolidated rock type, unless cementation took place at a later stage. Sometimes a distinction is made between fault gouge and fault breccia, the first has a smaller grain size.\n\nZones of fault breccia and fault gouge in rocks can be a hazard for the construction of tunnels and mines, as the non-cohesive zones form weak places in the rock where a tunnel can collapse more easily.\n\n"}
{"id": "16369125", "url": "https://en.wikipedia.org/wiki?curid=16369125", "title": "Ferropericlase", "text": "Ferropericlase\n\nFerropericlase or magnesiowüstite is a magnesium/iron oxide ((Mg,Fe)O) that is interpreted to be one of the main constituents of the Earth's lower mantle together with silicate perovskite, a magnesium/iron silicate with a perovskite structure. Ferropericlase has been found as inclusions in a few natural diamonds. An unusually high iron content in one suite of diamonds has been associated with an origin from the lowermost mantle. Discrete ultralow-velocity zones in the deepest parts of the mantle, near the Earth's core, are thought to be blobs of ferropericlase, as seismic waves are significantly slowed down as they pass through them, and ferropericlase is known to have this effect at the high pressures and temperatures found deep within the Earth's mantle. In May of 2018, ferropericlase was shown to be anisotropic in specific ways in the high pressures of the lower mantle, and these anisotropies may help seismologists and geologists to confirm whether those ultrslow-velocity zones are indeed ferropericlase, by passing seismic waves through them from various different directions and observing the exact amount of change in the velocity of those waves.\n\nChanges in the spin state of electrons in iron in mantle minerals has been studied experimentally in ferropericlase. Samples are subject to the conditions of the lower mantle in a laser-heated diamond anvil cell and the spin-state is measured using synchrotron X-ray spectroscopy. Results indicate that the change from a high to low spin state in iron occurs with increasing depth over a range from 1000 km to 2200 km. The change in spin state is expected to be associated with a higher than expected increase in density with depth.\n\n"}
{"id": "57805865", "url": "https://en.wikipedia.org/wiki?curid=57805865", "title": "Floc (biofilm)", "text": "Floc (biofilm)\n\nA floc is a type of microbial aggregate that may be contrasted with biofilms and granules, or else considered a specialized type of biofilm. Flocs appear as cloudy suspensions of cells floating in water, rather than attached to and growing on a surface like most biofilms. The floc typically is held together by a matrix of extracellular polymeric substance (EPS), which may contain variable amounts of polysaccharide, protein, and other biopolymers. The formation and the properties of flocs may affect the performance of industrial water treatment bioreactors such as activated sludge systems.\n\nFloc formation may benefit the constituent microorganisms in a number of ways, including protection from pH stress, resistance to predation, manipulation of microenvironments, and facilitation of mutualistic relationships in mixed microbial communities.\n\nIn general, the mechanisms by which flocculating microbial aggregates hold together are poorly understood. However, work on the activated sludge bacterium Zoogloea resiniphila has shown that PEP-CTERM proteins must be expressed for flocs to form; in their absence, growth is planktonic, even though exopolysaccharide is produced.\n"}
{"id": "159083", "url": "https://en.wikipedia.org/wiki?curid=159083", "title": "Ford Taurus", "text": "Ford Taurus\n\nThe Ford Taurus is an automobile manufactured by Ford in the United States. Now in its sixth generation, it was originally introduced in the 1986 model year, and has remained in near-continuous production for more than three decades. It has had a Mercury-branded twin, the Sable (1986–2005; 2008–2009), as well as a performance variant, the Ford Taurus SHO (1989–1999 and 2010–); in addition, it served as the basis for the first-ever front-wheel drive Lincoln Continental (1988–2002). It was a front-wheel drive mid-size sedan until 2000 (when it was classified as full-sized), although it remained mid-sized as a station wagon until 2007; the nameplate has been registered to a \"global\" full-size car (built on the Ford D3 platform) since 2008, and available in front- or all-wheel drive since 2008.\n\nThe original Taurus was a milestone for Ford and the entire American automotive industry, being the first automobile at Ford designed and manufactured using the statistical process control ideas brought to Ford by W. Edwards Deming, a prominent statistician consulted by Ford to bring a \"culture of quality\" to the enterprise. The Taurus had an influential design that brought many new features and innovations to the marketplace. Since its launch in 1986, Ford had built 7,519,919 Tauruses through the 2007 model year, making it the fifth-best-selling North American nameplate in Ford's history; only the F-150, Escort, Model T, and Mustang have sold more units. However, between 1992 and 1996 the Taurus was the best-selling car in the United States, eventually losing the title to the Toyota Camry in 1997. The 1986–1995 Taurus was built on the DN-5 platform, and the 1996–1999 Taurus was built on the DN101 platform. The 2000–2007 Tauruses were built on the D186 which was a modified DN 101 platform. All generations of the Taurus have been built at the Chicago Assembly. They were also produced at the Atlanta assembly plant until it was closed.\n\nIn the late 1990s and early 2000s, sales of the Taurus declined as it lost market share to Japanese midsize sedans and as Ford shifted resources towards developing SUVs. It was discontinued in 2006, with production initially ending on October 27, 2006, and 2007 being the last model year. Ford had decided to replace the Taurus with the fullsize Five Hundred and midsize Fusion sedans, as well as replacing the Taurus wagon with the Freestyle crossover SUV. However, Ford revived the Taurus name during the 2007 Chicago Auto Show a few months later by renaming two new models that had been intended to be updated versions of the Five Hundred and the Freestyle, the \"2008 Taurus\" and \"2008 Taurus X\", respectively. A new model of fullsize Taurus was then released for the 2010 model year, and the 2013 mid-generational refresh (minor model update) was unveiled at the New York Auto Show with minor exterior changes and interior technology options.\n\nThe Taurus was the first car resulting from introduction of a new quality culture at Ford. Between 1979 and 1982, Ford had incurred $3 billion in losses. In the Spring of 1980, Ford Chairman Donald E. Peterson initiated a new \"team\" approach to the design and manufacture of automobiles at Ford, that eventually resulted in the creation of the Ford Taurus. Ford's newly appointed Corporate Quality Director, Larry Moore, was charged with recruiting the famous statistician, W. Edwards Deming to help jump-start a quality movement at Ford. Deming told Ford that management actions were responsible for 85% of all problems in developing better cars. Based on Deming's advice, Ford management was charged with primary responsibility for automobile quality. Ford also adopted a quality culture employing statistical process control across all aspects of automobile design and manufacture. The Ford Taurus was the first Ford model resulting from this statistical approach to manufacture. In a letter to Autoweek, Donald Petersen, then Ford chairman, said, \"We are moving toward building a quality culture at Ford and the many changes that have been taking place here have their roots directly in Deming's teachings.\" This new emphasis on quality in the manufacture of the Ford Taurus was reflected in Ford's advertising and marketing. The New York advertising firm Wells, Rich, Greene took on the Ford account in 1979 and Robert Cox was assigned to the Ford account and by the summer of 1981, “Quality is Job 1” became Ford’s calling card in marketing. This emphasis on quality was used heavily in marketing of the Ford Taurus.\n\nThe first-generation Taurus was launched in 1985 as a 1986 model to strong fanfare and sales, replacing the slow-selling mid-size Ford LTD. (The full-size Ford LTD Crown Victoria remained as part of the Ford line up.) The release of the Ford Taurus was one of the most anticipated ever, mostly because it was a first in car design and also the start of new quality standards for Ford. At the time of the Taurus's debut, Ford had been producing mainly rear-wheel drive cars, and Chrysler and General Motors were offering more front-wheel drive vehicles up to midrange including the Chrysler K platform and A-body Chevrolet Celebrity. With the introduction of the Escort and Tempo, Ford was making a transition to front-wheel drive. The Taurus displayed a rounder shape than its contemporaries, often likened to a 'jelly bean' or 'flying potato', inspired by the design of the Audi 5000 and Ford's European sedan, the Ford Sierra, an updated appearance of a styling approach used in the late 1940s to early 1960s called \"ponton\" styling. Instead of a grille, the Taurus mainstreamed the smooth grille-less 'bottom breather' nose. The aerodynamic design of the Taurus made the car more fuel efficient, allowing Ford to meet more stringent corporate average fuel economy (CAFE) standard applied by the United States government. The Taurus's success ultimately led to an American automobile design revolution; Chrysler and General Motors developed aerodynamic cars in order to capitalize on the Taurus's success. It also benefitted from sharing a similar appearance to the limited production Ford Mustang SVO introduced two years earlier in 1984.\n\nThe first generation was available with either a V6 or an inline four-cylinder engine and came with either a manual (MT-5) or automatic transmission. (The Taurus's twin, the Mercury Sable, has never offered a manual transmission in either of its incarnations.) Like its exterior, the Taurus's interior was ahead of its time, and many features originating from it are still used in most cars today. Its interior was designed to be extremely user-friendly, with all of its controls designed to be recognizable by touch, allowing drivers to operate them without taking their eyes off the road. For example, the switches to the power windows and power locks were designed with one half of the switch raised up, with the other half recessed, in order for its function to be identified by touch. To further enhance this quality, the car's dashboard has all of the controls in the central area within reach of the driver. The left side of the dash curves slightly around the driver to make controls easily accessible, as well as creating a \"cockpit\" feel.\n\nThe interior of the Taurus was customizable to fit buyers' needs, with a large number of options and three different configurations. This means that the interior of the Taurus can be spartan or luxurious, depending on the buyer's choice of options. On models with an automatic transmission, the Taurus's interior was available in three different seating configurations. The interior equipment depends on model. The most basic model, the L (see below), came standard, with just an AM radio and a front cloth bench seat, while the LX, the more luxurious model, came with a greater number of features as standard equipment.\n\nThe Taurus was well received by both the public and the press. It won many awards, most notably being named to the 1986 Car and Driver Ten Best List and becoming the 1986 Motor Trend Car of the Year Over 200,000 of the Taurus were sold during the 1986 model year and the millionth Taurus was sold during the 1989 model year. When production ended in 1991, more than 2,000,000 first-generation Tauruses had been sold.\n\nThe Ford Taurus received its first redesign in mid-1991 for the 1992 model year. Still based on the same chassis, every exterior body panel (with the exception of the doors) was restyled. In spite of the extensive changes, few modifications were made to the successful styling; in the marketplace, the redesign was largely released as a mid-cycle facelift. In terms of size, the 1992 Taurus gained several inches in length and over 200 pounds in curb weight. Following market demand, the new Taurus was available solely with V6 engines and an automatic transmissions. The Taurus SHO made its return, with an automatic transmission option joining the manual transmission.\n\nThe interior was also completely redesigned for 1992. As part of the redesign, the Ford Taurus gained a passenger-side airbag as an option, which became standard in 1994, becoming the first mid-size sedan sold in the United States with standard dual airbags.\n\nThe second generation sold just as well as the first, becoming the best-selling car in the United States, a title it would retain for as long as this generation was sold. When production ended in 1995, more than 1,400,000 second-generation cars had been sold.\n\nFor the 1996 model year, Ford debuted the third generation of the Ford Taurus. Although not completely new, the chassis was heavily upgraded, becoming the DN101 generation. Alongside the Mercury Sable, the Ford Taurus shared its underpinnings with the redesigned Lincoln Continental and all-new Ford Windstar.\n\nIn a break from the familiar styling of the previous two generations (that chief designer Jack Telnack had likened to a \"pair of slippers\"), Ford had sought to again make the Taurus stand out for buyers of mid-size sedans, giving the vehicle a much more extensive restyling than its 1992 predecessor. Moving away from straight lines, the 1996 Taurus sought to include rounded lines, moving past the cab-forward design of the Chrysler LH sedans. Alongside the Ford Blue Oval emblem itself, the Taurus repeated the shape several places in its exterior; in a controversial design element, the rear window of the Taurus was oval, as were the side windows of the Mercury Sable. To allow better differentiation between models, the Ford Taurus and Mercury Sable were given separate rooflines; Taurus/Sable station wagons were fitted with the doors of Sable sedans.\n\nThe interior saw a complete redesign. To simplify production, all versions of the Taurus were fitted with bucket seats; six-passenger versions were fitted with a flip-forward center seat cushion also meant for use as a center console; five-passenger versions were fitted with a floor shifter and center console. To improve ergonomics, radio and climate controls were centralized on an oval-shaped console on the dashboard.\n\nReaction to the third-generation Ford Taurus was mixed; Ford found that customers disliked the oval-shaped exterior. For 1996, the Ford Taurus stayed the best-selling car in the United States. At the time, 51% of all Taurus sales for 1996 went to rental fleets, in contrast to the Honda Accord and Toyota Camry, of which most sales were directly to private customers through retail outlets. In 1997, the Ford Taurus lost its best-selling status to the Toyota Camry.\n\nFor 1996, Ford Australia imported the Ford Taurus sedan as the \"Taurus Ghia\" alongside its locally produced Ford Falcon EL, but imports ceased after only one year due to poor sales. Ford New Zealand imported both Ford Taurus sedans and station wagons from 1996 to 1998 with success alongside the RWD Australian Ford Falcon/Fairmont/Fairlane.\n\nThe third-generation Taurus had a presence in NASCAR, replacing the Thunderbird for the 1998 season. The Taurus became the first four-door sedan to be approved for competition. The first Taurus driver to win the Winston Cup (the NASCAR sponsor of the time) championship was Dale Jarrett, who drove #88 Ford Quality Care/Ford Credit-sponsored cars owned by Robert Yates. The first Taurus driver to win the Busch Series (now Xfinity Series) championship was Greg Biffle, who drove #60 Grainger Industrial Supply-sponsored cars owned by Jack Roush.\n\nIn total, the Ford Taurus has won three Winston Cup championships and two Busch Series championships.\n\nThe Taurus received another redesign for 2000, which replaced many of the oval-derived design elements of the previous model with sharper creases and corners, an aspect of Ford's New Edge styling language. To reduce the car's price and keep it competitive, Ford reduced costs on the car in 1999, such as giving the Taurus sedan rear drum brakes on ABS equipped vehicles (previously, upgrading to ABS included the addition of rear disc brakes), eliminating the dual exhaust on the higher end models, and trimming many other small features.\n\nFord designed the fourth generation with much more conservative styling in hopes of increasing the car's appeal. Instead of sloping back, the car's trunk stood upright in a more traditional shape, increasing trunk space by another two cubic feet. The roof was also given a more upright stance to increase headroom, which can be evidenced by the thicker C-pillar and larger area between the tops of the doors and the top of the roof.\n\nThe interior was also completely redesigned with a more conventional shape, although some features from the previous Taurus generations were carried over. The dashboard was given a squarer design instead of curving around the driver as in the previous generation. The \"integrated control panel\" concept was carried over but redesigned, with a bigger, squarer shape, and it was placed in the center of the dash instead of being angled toward the driver. The flip-fold center console was also carried over from the previous generation, although it was revamped as well. When folded out, it now rested against the floor instead of the dashboard, and had reworked cup holders and storage areas. In another change from previous versions, the fourth generation offered rear cup holders that either slid or folded out of the front console, depending on which console the car was equipped with.\n\nTaurus sales had slumped significantly in the years prior to its short-lived demise with model year 2007, losing significant market share to Japanese sedans. Due to waning popularity and customer demand, Ford decided to gradually discontinue the Taurus. The last Chicago, Illinois Ford Taurus Sedan rolled off the assembly line on June 25, 2004. Production of the Taurus wagon was discontinued on December 8, 2004; sedan retail sales halted after a short 2006 model year, and the Taurus became sold exclusively to fleets in the United States, while still being sold to retail customers in Canada. Production ended on October 27, 2006, as Ford idled the Atlanta plant, as part of its \"The Way Forward\" restructuring plan.\nFord had decided to replace the Taurus with the full-size Five Hundred (a revival of the \"500\" nameplate suffix dating back to the 1950s), and midsize Fusion sedans and the Taurus wagon with the Freestyle crossover SUV.\n\nDiscontinuation of the Taurus was controversial. While many believed that the Taurus had been dropped because it could no longer compete in the growing sedan market, others claimed Ford could have easily have saved the nameplate had it wished to. Autoblog went so far as to call the Taurus' discontinuation \"the biggest fall from grace in automotive history\", and even blamed Ford's current financial problems on its failure to keep the Taurus competitive, focusing too unilaterally on trucks and SUVs. The Truth About Cars published a review/editorial also showing their disappointment at how Ford neglected the Taurus to the point where it became a \"rental car\".\n\nWorkers thought Ford had abandoned a car that had done so much to revitalize Ford and the US industry. An October 25, 2006, USA Today editorial, \"How Ford starved its Taurus\", noted that the Japanese stuck with their winners and make them better (such as the Toyota Corolla, which has been in continuous production since the 1960s), while Detroit automakers retire cars and even entire division nameplates in search of \"the next big thing\".\n\nBut Alan Mulally, Ford's new CEO named in late 2006, wanted to revive the Taurus, saying in an interview with the Associated Press that he was baffled by the Taurus's discontinuation and believed it a mistake, adding that the Five Hundred should have been named \"Taurus\" from the beginning. Rumors of a possible Taurus revival were confirmed in early 2007, when the revamped Five Hundred and Freestyles were unveiled as \"Taurus\" and \"Taurus X\" at the 2007 Chicago Auto Show, a decision influenced strongly by Mulally. Later, Mulally explained that the fact that the Taurus's name recognition and positive brand equity strongly influenced his decision to revive the nameplate. The Mercury division's twin also dropped the Sable name in favor of reviving the historic Montego nameplate, used in the 1960s and 1970s, but the Sable name returned in 2008.\n\nThe fifth generation (5G) Taurus entered production in 2007 as a 2008 model and was developed directly from the Ford Five Hundred, chiefly with a mild exterior facelift and revised engine and transmission. Ford designated the model as the Taurus, after the demise of the concurrently marketed fourth generation (4G) Taurus and to take advantage of its customer recognition and dealer demand.\n\nChanges to the 5G Taurus from the Five Hundred included a new front end and the 263 hp 3.5 L Duratec 35 V6, which replaces the Duratec 30 3.0 L V6. The Five Hundred/Freestyle's ZF-Batavia CVT, which had a maximum torque rating of , was also replaced with a Ford-GM joint venture six-speed automatic with additional torque of the Duratec 35. The Aisin AW six-speed automatic which was used on FWD Five Hundred and Montegos was also replaced by the new Ford six-speed.\n\nThe Taurus sedan twin, the Mercury Sable nameplate, was revived from the Mercury Montego. For the 2009 model year, Ford revived the \"SE\" trimline for the Taurus. The SE sold for $24,125 according to Ford's website and served as the base model for the vehicle.\n\nThe 5G Taurus was sold in the Middle East as the Ford Five Hundred from 2008.\n\nIt was determined that Ford's strategy to redesignate new cars in the lineup with new names beginning with the letter \"F\", as in Ford Focus, Ford Fusion, and Ford Freestyle, was not a good marketing move, as some of the renamed cars had highly recognizable iconic names. Car buyers in the U.S. did not associate the new \"F\" names with Ford, and were confused by the name changes. Mulally believed that the Taurus had an immediately strong brand equity, and that it would take years for consumers to have a similar recognition of the Five Hundred.\n\nThe 2008 Ford Taurus and Mercury Sable were awarded the Top Safety Pick ratings by the Insurance Institute for Highway Safety (IIHS) and five-star ratings by the National Highway Traffic Safety Administration (NHTSA). The five-star rating given to the Taurus and the Sable is the highest safety rating being given by the government agency.\n\nThe 2010 Ford Taurus was revealed at the Detroit International Auto Show in 2009 at Cobo Hall. The press preview of the Taurus and Taurus SHO was held in Asheville, North Carolina, from June 15 to 19, 2009.\n\nThe Ford scored well in test drives and the media was pleased with some of the new features available in the 2010 Ford Taurus. Some of these features included all wheel drive, cross traffic alert, collision warning, blind spot monitoring and adaptive cruise control. However, others criticized the lack of interior room and reduced sight lines despite its full-sized exterior, and Edmunds noted that the eighth-generation Honda Accord (which competes in the mid-size category) had superior driving dynamics and a more efficient design and offered almost as much interior space as the Taurus despite considerably smaller external dimensions.\n\nThe base price of the base SE model was $25,995, mid grade SEL $27,995, and top level Limited $31,995. Ford hoped to sell 10% to 15% high-performance SHO models.\n\nThe SHO (Super High Output), released in August 2009, Was powered by twin-turbocharged, gasoline direct injection EcoBoost 3.5L V6 engine. It had and of torque. Mileage was 17/25 mpg in AWD. The SHO base price was $37,995, which included the EcoBoost V6, all wheel drive, upgraded 6-speed automatic transmission and numerous exterior and interior trim upgrades. A fully loaded SHO could reach $45,395. There was also an available performance package which included upgraded brake pads, a 3.16:1 final drive ratio (compared to the standard 2.77:1), recalibrated electronic power steering, further suspension tuning, a re-calibrated AdvanceTrac system (Ford's combined traction control system and electronic stability control) with sport mode and \"true off\", summer compound Goodyear Eagle F1 245/45ZR20 tire, and an electric air pump with fix-a-flat in lieu of a spare tire. Most options for the SHO remain available, with the Performance Package including options such as Power Moonroof, Heated/Cooled Seats, Multi-Contour Seats, Auto-Sensing Lights and Wipers, Automatic High-Beams, Adjustable Pedals, Blind Spot Information System (BLIS), and Satellite Navigation. Options from the Driver Assist option group, however, are unavailable simultaneously with the Performance Package. Those options include Adaptive Cruise Control, Collision Warning System, Lane Keep Assist, and Active Park Assist.\n\nThe base produced by the SHO from the factory may be considered mild only a few years after its initial release, given the subsequent existence of sedans like the Dodge Charger Hellcat. However, enthusiasts of the SHO have found the vehicle to be extremely compliant to power enhancing modifications; in many cases, with nothing more than a tune. The highest power 4th generation SHO (6th gen Taurus) currently stands at at the wheels, as produced by Livernois Motorsports for one of their customers. With the potential for truly competitive levels of power on an AWD platform, the largely unassuming looks of the SHO compared to the base Taurus make the SHO a cost effective sleeper.\n\nFirst revealed at the 2011 New York Auto Show, the Taurus received a refresh for the 2013 model year. The body featured a new front fascia and slightly updated rear fascia with LED tail lamps, as well as all-new wheel options. The SHO model featured revised styling elements. Refinements were made to the 3.5 EcoBoost V6. Power in the 3.5L V6, standard in non-SHO models, was up to 288 hp and got 19/29 MPG in FWD models, while getting 18/26MPG in AWD models. There was a new engine option for non-SHO models, a 2.0L EcoBoost Inline 4 developing 240 hp and 270 lb-ft of torque while delivering a best-in-class 22/32 miles per gallon. All models received upgrades to the steering and braking systems to improve driveability, including standard Torque Vectoring and curve control improving tracking at higher speeds. Updates to the Instrument dials were added, which were fully digital, clearer, and more colorful. MyFord Touch was added as part of Taurus's Sync system.\n\nOn April 25, 2018, Ford announced plans to discontinue the Taurus (along with the Fiesta, Focus, and Fusion), in order to focus more on its line of trucks and SUVs. The announcement was part of a plan by Ford Motor Company to cut costs and increase profits. This was in response to a shift in perceived consumer demand towards SUVs and pickup trucks, and away from sedans. On September 5, 2018, Ford ended all national and promotional advertising (including sales and special offers) for its entire sedan lineup, including the Taurus, whose production will end in 2019.\n\nA seventh generation of the Ford Taurus was introduced at the 2015 Shanghai Auto Show. Unrelated to the six generations of the American-market Ford Taurus, the seventh-generation was developed by Changan Ford in conjunction with Ford of Australia. Serving as the flagship of the Changan Ford joint venture, the Taurus is an extended-wheelbase variant of the Ford Fusion (Mondeo), differing primarily in its 3.9-inch longer wheelbase and formal rear roofline. The seventh-generation Taurus has been manufactured since November 2015 by Changan Ford in its Hangzhou facility. \n\nDerived from the Ford CD4 platform, the seventh-generation Taurus is fitted with a 2.0L EcoBoost inline-4 (an option for the sixth-generation Taurus) and a 2.7L EcoBoost V6 (used in the Fusion and Lincoln Continental), with a 6-speed automatic paired to both engines.\n\nSince its 2015 introduction, the Changan Ford-produced Taurus has been produced solely for the Chinese market, with no considerations regarding future export. \n\n"}
{"id": "34667944", "url": "https://en.wikipedia.org/wiki?curid=34667944", "title": "Frozen mirror image method", "text": "Frozen mirror image method\n\nFrozen mirror image method (or method of frozen images) is an extension of the method of images for magnet-superconductor systems that has been introduced by Alexander Kordyuk in 1998 to take into account the magnetic flux pinning phenomenon. The method gives a simple representation of the magnetic field distribution generated by a magnet (a system of magnets) outside an infinitely flat surface of a perfectly hard (with infinite pinning force) type-II superconductor in more general field cooled (FC) case, i.e. when the superconductor goes into superconducting state been already exposed to the magnetic field. The difference from the mirror image method, which deals with a perfect type-I superconductor (that completely expels the magnetic field, see the Meissner effect), is that the perfectly hard superconductor screens the variation of the external magnetic field rather than the field itself.\n\nThe name originates from the replacement of certain elements in the original layout with imaginary magnets, which replicates the boundary conditions of the problem (see Dirichlet boundary conditions). In a simplest case of the magnetic dipole over the flat superconducting surface (see Fig. 1), the magnetic field, generated by a dipole moved from its initial position (at which the superconductor is cooled to the superconducting state) to a final position and by the screening currents at the superconducting surface, is equivalent to the field of three magnetic dipoles: the real one (1), its mirror image (3), and the mirror image of it in initial (FC) position but with the magnetization vector inversed (2).\n\nThe method is shown to work for the bulk high temperature superconductors (HTSC), which are characterized by strong pinning and used for calculation of the interaction in magnet-HTSC systems such as superconducting magnetic bearings, superconducting flywheels, MAGLEV, for spacecraft applications, as well as a textbook model for science education.\n\n\n"}
{"id": "2933897", "url": "https://en.wikipedia.org/wiki?curid=2933897", "title": "Hexanoic acid", "text": "Hexanoic acid\n\nHexanoic acid, also known as caproic acid, is the carboxylic acid derived from hexane with the chemical formula . It is a colorless oily liquid with an odor that is fatty, cheesy, waxy, and like that of goats or other barnyard animals. It is a fatty acid found naturally in various animal fats and oils, and is one of the chemicals that give the decomposing fleshy seed coat of the ginkgo its characteristic unpleasant odor. It is also one of the components of vanilla. The primary use of hexanoic acid is in the manufacture of its esters for artificial flavors, and in the manufacture of hexyl derivatives, such as hexylphenols. Salts and esters of hexanoic acid are known as hexanoates or caproates.\n\nTwo other acids are named after goats: caprylic (C8) and capric (C10). Along with hexanoic acid, these total 15% in goat milk fat.\n\nCaproic, caprylic, and capric acids (capric is a crystal- or wax-like substance, whereas the other two are mobile liquids) are not only used for the formation of esters, but also commonly used \"neat\" in: butter, milk, cream, strawberry, bread, beer, nut, and other flavors.\n\n"}
{"id": "150733", "url": "https://en.wikipedia.org/wiki?curid=150733", "title": "Holt, Missouri", "text": "Holt, Missouri\n\nHolt is a city in Clay and Clinton counties in the U.S. state of Missouri. The population was 447 at the 2010 census.\n\nHolt was platted in 1867. The city was named for Jeremiah Abel Holt (1811-1886), who donated the land in 1837 and who was one of the first settlers in the area, a native of Orange County, North Carolina.\n\nHolt has the distinction of holding the world record for the fastest accumulation of rainfall. On June 22, 1947, Holt received of rain in 42 minutes.\n\nHolt is located at (39.456809, -94.336741).\n\nAccording to the United States Census Bureau, the city has a total area of , all land.\n\nAs of the census of 2010, there were 447 people, 176 households, and 119 families residing in the city. The population density was . There were 193 housing units at an average density of . The racial makeup of the city was 92.4% White, 0.7% African American, 1.6% Native American, 1.1% Asian, 0.4% from other races, and 3.8% from two or more races. Hispanic or Latino of any race were 4.3% of the population.\n\nThere were 176 households of which 38.6% had children under the age of 18 living with them, 52.3% were married couples living together, 10.8% had a female householder with no husband present, 4.5% had a male householder with no wife present, and 32.4% were non-families. 28.4% of all households were made up of individuals and 15.9% had someone living alone who was 65 years of age or older. The average household size was 2.54 and the average family size was 3.08.\n\nThe median age in the city was 34.3 years. 28.4% of residents were under the age of 18; 6.3% were between the ages of 18 and 24; 28.1% were from 25 to 44; 24.6% were from 45 to 64; and 12.5% were 65 years of age or older. The gender makeup of the city was 48.1% male and 51.9% female.\n\nAs of the census of 2000, there were 405 people, 152 households, and 100 families residing in the city. The population density was 1,119.4 people per square mile (434.4/km²). There were 165 housing units at an average density of 456.0 per square mile (177.0/km²). The racial makeup of the city was 97.28% White, 0.74% African American, 0.49% Native American, 1.23% from other races, and 0.25% from two or more races. Hispanic or Latino of any race were 0.99% of the population.\n\nThere were 152 households out of which 34.9% had children under the age of 18 living with them, 57.2% were married couples living together, 3.9% had a female householder with no husband present, and 34.2% were non-families. 30.9% of all households were made up of individuals and 18.4% had someone living alone who was 65 years of age or older. The average household size was 2.66 and the average family size was 3.31.\n\nIn the city, the population was spread out with 30.9% under the age of 18, 9.4% from 18 to 24, 30.6% from 25 to 44, 16.5% from 45 to 64, and 12.6% who were 65 years of age or older. The median age was 33 years. For every 100 females, there were 96.6 males. For every 100 females age 18 and over, there were 95.8 males.\n\nThe median income for a household in the city was $38,438, and the median income for a family was $55,375. Males had a median income of $35,556 versus $21,111 for females. The per capita income for the city was $16,841. About 10.1% of families and 16.6% of the population were below the poverty line, including 33.6% of those under age 18 and 8.5% of those age 65 or over.\n\n"}
{"id": "142100", "url": "https://en.wikipedia.org/wiki?curid=142100", "title": "Hydride", "text": "Hydride\n\nIn chemistry, a hydride is the anion of hydrogen, H, or, more commonly, it is a compound in which one or more hydrogen centres have nucleophilic, reducing, or basic properties in it. In compounds that are regarded as hydrides, the hydrogen atom is bonded to a more electropositive element or groups. Compounds containing hydrogen bonded to metals or metalloid may also be referred to as hydrides. Common examples are ammonia (NH), methane (CH), ethane (CH) (or any other hydrocarbon), and Nickel hydride (NiH), used in NiMH rechargeable batteries.\n\nAlmost all of the elements form binary compounds with hydrogen, the exceptions being He, Ne, Ar, Kr, Pm, Os, Ir, Rn, Fr, and Ra.\n\nBonds between hydrogen and other elements range from highly to somewhat covalent. Some hydrides, e.g. boron hydrides, do not conform to classical electron-counting rules, and the bonding is described in terms of multi-centered bonds, whereas the interstitial hydrides often involve metallic bonding. Hydrides can be discrete molecules, oligomers or polymers, ionic solids, chemisorbed monolayers, bulk metals (interstitial), and other materials. While hydrides traditionally react as Lewis bases or reducing agents, some metal hydrides behave as hydrogen-atom donors and act as acids.\n\n\n\nFree hydride anions exist only under extreme conditions and are not invoked for homogeneous solution. Instead, many compounds have hydrogen centres with hydridic character.\n\nAside from electride, the hydride ion is the simplest possible anion, consisting of two electrons and a proton. Hydrogen has a relatively low electron affinity, 72.77 kJ/mol and reacts exothermically with protons as a powerful Lewis base. \nThe low electron affinity of hydrogen and the strength of the H–H bond (∆H = 436 kJ/mol) means that the hydride ion would also be a strong reducing agent\n\nAccording to the general definition every element of the periodic table (except some noble gases) forms one or more hydrides. These substances have been classified into three main types according to the nature of their bonding:\nWhile these divisions have not been used universally, they are still useful to understand differences in hydrides.\n\nIonic or saline hydrides are composed of hydride bound to an electropositive metal, generally an alkali metal or alkaline earth metal. The divalent lanthanides such as europium and ytterbium form compounds similar to those of heavier alkali metal. In these materials the hydride is viewed as a pseudohalide. Saline hydrides are insoluble in conventional solvents, reflecting their non-molecular structures. Ionic hydrides are used as bases and, occasionally, as reducing reagents in organic synthesis.\n\nTypical solvents for such reactions are ethers. Water and other protic solvents cannot serve as a medium for ionic hydrides because the hydride ion is a stronger base than hydroxide and most hydroxyl anions. Hydrogen gas is liberated in a typical acid-base reaction.\n\nOften alkali metal hydrides react with metal halides. Lithium aluminium hydride (often abbreviated as LAH) arises from reactions of lithium hydride with aluminium chloride.\n\nAccording to some definitions, covalent hydrides cover all other compounds containing hydrogen. Some definitions limit hydrides to hydrogen centres that formally react as hydrides, i.e. are nucleophilic, and hydrogen atoms bound to metal centers.These hydrides are formed by all the true non-metals (except zero group elements) and the elements like Al, Ga, Sn, Pb, Bi, Po, etc., which are normally metallic in nature, i.e., this class includes the hydrides of p-block elements. In these substances the hydride bond is formally a covalent bond much like the bond made by a proton in a weak acid. This category includes hydrides that exist as discrete molecules, polymers or oligomers, and hydrogen that has been chem-adsorbed to a surface. A particularly important segment of covalent hydrides are complex metal hydrides, powerful soluble hydrides commonly used in synthetic procedures.\n\nMolecular hydrides often involve additional ligands such as, diisobutylaluminium hydride (DIBAL) consists of two aluminum centers bridged by hydride ligands. Hydrides that are soluble in common solvents are widely used in organic synthesis. Particularly common are sodium borohydride (NaBH) and lithium aluminium hydride and hindered reagents such as DIBAL.\n\nInterstitial hydrides most commonly exist within metals or alloys. They are traditionally termed 'compounds', even though they do not strictly conform to the definition of a compound; more closely resembling common alloys such as steel. In such hydrides, hydrogen can exist as either atomic, or diatomic entities. Mechanical or thermal processing, such as bending, striking, or annealing may cause the hydrogen to precipitate out of solution, by degassing. Their bonding is generally considered metallic. Such bulk transition metals form interstitial binary hydrides when exposed to hydrogen. These systems are usually non-stoichiometric, with variable amounts of hydrogen atoms in the lattice. In materials engineering, the phenomenon of hydrogen embrittlement results from the formation of interstitial hydrides. Hydrides of this type form according to either one of two main mechanisms. The first mechanism involves the adsorption of dihydrogen, succeeded by the cleaving of the H-H bond, the delocalisation of the hydrogen's electrons, and finally, the diffusion of the protons into the metal lattice. The other main mechanism involves the electrolytic reduction of ionised hydrogen on the surface of the metal lattice, also followed by the diffusion of the protons into the lattice. The second mechanism is responsible for the observed temporary volume expansion of certain electrodes used in electrolytic experiments.\n\nPalladium absorbs up to 900 times its own volume of hydrogen at room temperatures, forming palladium hydride. This material has been discussed as a means to carry hydrogen for vehicular fuel cells. Interstitial hydrides show certain promise as a way for safe hydrogen storage. During last 25 years many interstitial hydrides were developed that readily absorb and discharge hydrogen at room temperature and atmospheric pressure. They are usually based on intermetallic compounds and solid-solution alloys. However, their application is still limited, as they are capable of storing only about 2 weight percent of hydrogen, insufficient for automotive applications.\n\nTransition metal hydrides include compounds that can be classified as \"covalent hydrides\". Some are even classified as interstitial hydrides and other bridging hydrides. Classical transition metal hydride feature a single bond between the hydrogen centre and the transition metal. Some transition metal hydrides are acidic, e.g., HCo(CO) and HFe(CO). The anions [ReH] and [FeH] are examples from the growing collection of known molecular homoleptic metal hydrides. As pseudohalides, hydride ligands are capable of bonding with positively polarized hydrogen centres. This interaction, called dihydrogen bond is similar to hydrogen bonding which exists between positively polarized protons and electronegative atoms with open lone pairs.\n\nHydrides containing deuterium are known as \"deuterides\". Some deuterides, such as LiD, are important fusion fuels in thermonuclear weapons, and useful moderators in nuclear reactors.\n\n\"Protide\", \"deuteride\", and \"tritide\" are used to describe ions or compounds, which contain enriched hydrogen-1, deuterium or tritium, respectively.\n\nIn the classic meaning, hydride refers to any compounds hydrogen forms with other elements, ranging over groups 1–16 (the binary compounds of hydrogen). The following is a list of the nomenclature for the hydride derivatives of main group compounds according to this definition:\n\nAccording to the convention above, the following are \"hydrogen compounds\" and not \"hydrides\":\n\nExamples:\n\nAll metalloid hydrides are highly flammable. All solid non-metallic hydrides except ice are highly flammable. But, when hydrogen combines with halogens, it produces acids rather than hydrides, and they are not flammable.\n\nAccording to IUPAC convention, by precedence (stylized electronegativity), hydrogen falls between group 15 and group 16 elements. Therefore, we have NH, 'nitrogen hydride' (ammonia), versus HO, 'hydrogen oxide' (water). This convention is sometimes broken for polonium, which on the grounds of polonium's metallicity is often referred to as 'polonium hydride' instead of the expected 'hydrogen polonide'.\n\n\nW. M. Mueller, J. P. Blackledge, G. G. Libowitz, \"Metal Hydrides\", Academic Press, N.Y. and London, (1968)\n"}
{"id": "9247275", "url": "https://en.wikipedia.org/wiki?curid=9247275", "title": "International Solar Energy Society", "text": "International Solar Energy Society\n\n\"\"\n\nThe International Solar Energy Society (ISES) is a global organization for promoting the development and utilisation of renewable energy. ISES is a UN-accredited NGO headquartered in Freiburg im Breisgau, Germany. Dr David S. Renné is the current president. \n\nISES was formed in 1954 as a worldwide non-profit organisation dedicated to the advancement of the utilisation of solar energy. ISES is now active in over 110 countries worldwide. Its members include scientists and researchers, plus many others from industry, solar energy associations and other private and public organisations.\n\nISES also organizes \"Young ISES\", a network of students and young professional ISES members connecting young solar professionals worldwide\".\"\n\nISES aims to help its global members provide the technical answers needed to accelerate the transformation to 100% renewable energy. This is reflected in the society's official vision statement:The International Solar Energy Society (ISES) envisions a world with 100% renewable energy for everyone used wisely and efficiently. ISES aims to bring recent developments in solar energy, both in research and applications, to the attention of decision makers and the general public, in order to increase the understanding and use of this non-polluting resource in everyday life. \n\nThrough its events and publications, ISES promotes research, development, and use of technologies which are directly or indirectly fuelled by the sun. These technologies can provide sustainable solutions for the supply of energy in both industrialised and developing countries. \nIn spite of a historic focus on direct solar energy, ISES today is involved in all renewable energy fields.\n\nISES is a member of the International Renewable Energy Alliance (REN Alliance). Other members include the International Geothermal Association (IGA), International Hydropower Association (IHA), World Bioenergy Association (WBA) and the World Wind Energy Association (WWEA).\n\nISES is the publisher of several publications including \"Renewable Energy Focus\", the scientific journal \"Solar Energy\" and various white papers. ISES also operates an online bookshop which includes conference proceedings, pocket reference books and others.\n\nThe ISES Sections are organisations working at the national or regional level and represent the international organisation at this level. Members are encouraged to be involved with the Sections in their area.\n\nList of ISES' Sections\n\n\n\n"}
{"id": "56833095", "url": "https://en.wikipedia.org/wiki?curid=56833095", "title": "Jill Duff", "text": "Jill Duff\n\nJillian Louise Calland Duff (called Jill; born 1972) is a British Anglican bishop. Since 2018, she has been the Bishop of Lancaster, a suffragan bishop in the Diocese of Blackburn. Previously, she had been Director of St Mellitus College, North West, an Anglican theological college, since 2013. Before ordination, she studied chemistry at university and worked in the oil industry. After ordination in the Church of England, she served in the Diocese of Liverpool in parish ministry, chaplaincy, and church planting. \n\nDuff was born in 1972 in Bolton, Lancashire, England. She was educated at Bolton School, an independent school in Bolton. She studied Natural Sciences at Christ's College, Cambridge, graduating with a Bachelor of Arts (BA) degree in 1993: as per tradition, her BA was promoted to a Master of Arts (MA Cantab) degree in 1997. She then studied chemistry at Worcester College, Oxford, completing her Doctor of Philosophy (DPhil) degree in 1996. Her doctoral thesis was titled \"Investigations of redox-coupled proton transfer by iron-sulfur cluster systems in proteins\". Her early career was spent working in the oil industry.\n\nDuff trained for ordained ministry at Wycliffe Hall, Oxford, an evangelical Anglican theological college. She also studied theology during this time, and graduated from Wycliffe with a BA degree in 2002. She was ordained in the Church of England as a deacon in 2003 and as a priest in 2004.\n\nFrom 2003 to 2005, Duff served her curacy at St Philip's Church, Litherland in the Diocese of Liverpool. In 2005, she was appointed the first pioneer minister in the Diocese of Liverpool. In that role, she was tasked with planting churches in Liverpool city centre to evangelise to the unchurched in their 20s and 30s. In 2009, she was additionally appointed chaplain to Liverpool College, then an independent all-through school: she would continue this role part-time until 2016. \n\nIn 2011, Duff left her church planting role, and was appointed a vocations development advisor in the Diocese of Liverpool and an initial ministerial education (IME) tutor. In 2012, she liaised between St Mellitus College, an Anglican theological college in London, and the Church of England's north west dioceses (Blackburn, Carlisle, Chester, Liverpool, and Manchester) in order to created a new theological college in the North West of England. In March 2013, she was appointed the first director of St Mellitus College, North West. St Mellitus NW is the first full-time ordination course in the North West since St Aidan's College, Birkenhead was closed in 1969. She has additionally held Permission to Officiate in the Dioceses of Liverpool since 2013, of Chester since 2017, and Diocese of St Asaph since 2018.\n\nOn 13 March 2018, Duff was announced as the next Bishop of Lancaster, a suffragan bishop in the Diocese of Blackburn. The diocese did not ordain women to the priesthood until 2014: four years later, it would have a female bishop. She was consecrated a bishop by John Sentamu, Archbishop of York, on 29 June 2018 during a service at York Minster. She was installed as Bishop of Lancaster in July 2018 during as service at Blackburn Cathedral.\n\nDuff is married to Jeremy Duff: he is an Anglican priest who is currently the Principal of the St Padarn's Institute, the Church in Wales' theological college. Together they have two sons.\n"}
{"id": "21299730", "url": "https://en.wikipedia.org/wiki?curid=21299730", "title": "Lemon", "text": "Lemon\n\nThe lemon, \"Citrus limon\" (L.) Osbeck, is a species of small evergreen tree in the flowering plant family Rutaceae, native to South Asia, primarily North eastern India.\n\nThe tree's ellipsoidal yellow fruit is used for culinary and non-culinary purposes throughout the world, primarily for its juice, which has both culinary and cleaning uses. The pulp and rind (zest) are also used in cooking and baking. The juice of the lemon is about 5% to 6% citric acid, with a pH of around 2.2, giving it a sour taste. The distinctive sour taste of lemon juice makes it a key ingredient in drinks and foods such as lemonade and lemon meringue pie.\n\nThe origin of the lemon is unknown, though lemons are thought to have first grown in Assam (a region in northeast India), northern Burma or China. A genomic study of the lemon indicated it was a hybrid between bitter orange (sour orange) and citron.\n\nLemons entered Europe near southern Italy no later than the second century AD, during the time of Ancient Rome. However, they were not widely cultivated. They were later introduced to Persia and then to Iraq and Egypt around 700 AD. The lemon was first recorded in literature in a 10th-century Arabic treatise on farming, and was also used as an ornamental plant in early Islamic gardens. It was distributed widely throughout the Arab world and the Mediterranean region between 1000 and 1150.\n\nThe first substantial cultivation of lemons in Europe began in Genoa in the middle of the 15th century. The lemon was later introduced to the Americas in 1493 when Christopher Columbus brought lemon seeds to Hispaniola on his voyages. Spanish conquest throughout the New World helped spread lemon seeds. It was mainly used as an ornamental plant and for medicine. In the 19th century, lemons were increasingly planted in Florida and California.\n\nIn 1747, James Lind's experiments on seamen suffering from scurvy involved adding lemon juice to their diets, though vitamin C was not yet known.\n\nThe origin of the word \"lemon\" may be Middle Eastern. The word draws from the Old French \"limon\", then Italian \"limone\", from the Arabic \"laymūn\" or \"līmūn\", and from the Persian \"līmūn\", a generic term for citrus fruit, which is a cognate of Sanskrit (\"nimbū\", “lime”).\n\nThe 'Bonnie Brae' is oblong, smooth, thin-skinned, and seedless, mostly grown in San Diego County, USA.\n\nThe 'Eureka' grows year-round and abundantly. This is the common supermarket lemon, also known as 'Four Seasons' (\"Quatre Saisons\") because of its ability to produce fruit and flowers together throughout the year. This variety is also available as a plant to domestic customers. There is also a pink-fleshed Eureka lemon, with a green and yellow variegated outer skin.\n\nThe 'Femminello St. Teresa', or 'Sorrento' is native to Italy. This fruit's zest is high in lemon oils. It is the variety traditionally used in the making of \"limoncello\".\n\nThe 'Yen Ben' is an Australasian cultivar.\n\nLemons are a rich source of vitamin C, providing 64% of the Daily Value in a 100 g serving (table). Other essential nutrients, however, have insignificant content (table).\n\nLemons contain numerous phytochemicals, including polyphenols, terpenes, and tannins. Lemon juice contains slightly more citric acid than lime juice (about 47 g/l), nearly twice the citric acid of grapefruit juice, and about five times the amount of citric acid found in orange juice.\n\nLemon juice, rind, and peel are used in a wide variety of foods and drinks. The whole lemon is used to make marmalade, lemon curd and lemon liqueur. Lemon slices and lemon rind are used as a garnish for food and drinks. Lemon zest, the grated outer rind of the fruit, is used to add flavor to baked goods, puddings, rice, and other dishes.\n\nLemon juice is used to make lemonade, soft drinks, and cocktails. It is used in marinades for fish, where its acid neutralizes amines in fish by converting them into nonvolatile ammonium salts, and meat, where the acid partially hydrolyzes tough collagen fibers, tenderizing the meat, but the low pH denatures the proteins, causing them to dry out when cooked. Lemon juice is frequently used in the United Kingdom to add to pancakes, especially on Shrove Tuesday.\n\nLemon juice is also used as a short-term preservative on certain foods that tend to oxidize and turn brown after being sliced (enzymatic browning), such as apples, bananas, and avocados, where its acid denatures the enzymes.\n\nIn Morocco, lemons are preserved in jars or barrels of salt. The salt penetrates the peel and rind, softening them, and curing them so that they last almost indefinitely. The preserved lemon is used in a wide variety of dishes. Preserved lemons can also be found in Sicilian, Italian, Greek, and French dishes.\n\nThe leaves of the lemon tree are used to make a tea and for preparing cooked meats and seafoods.\n\nLemons were the primary commercial source of citric acid before the development of fermentation-based processes.\n\nThe juice of the lemon may be used for cleaning. A halved lemon dipped in salt or baking powder is used to brighten copper cookware. The acid dissolves the tarnish and the abrasives assist the cleaning. As a sanitary kitchen deodorizer the juice can deodorize, remove grease, bleach stains, and disinfect; when mixed with baking soda, it removes stains from plastic food storage containers. The oil of the lemon's peel also has various uses. It is used as a wood cleaner and polish, where its solvent property is employed to dissolve old wax, fingerprints, and grime. Lemon oil and orange oil are also used as a nontoxic insecticide treatment.\n\nLemon oil may be used in aromatherapy. Lemon oil aroma does not influence the human immune system, but may contribute to relaxation.\n\nOne educational science experiment involves attaching electrodes to a lemon and using it as a battery to produce electricity. Although very low power, several lemon batteries can power a small digital watch. These experiments also work with other fruits and vegetables.\n\nLemon juice may be used as a simple invisible ink, developed by heat.\n\nLemons need a minimum temperature of around , so they are not hardy year round in temperate climates, but become hardier as they mature. Citrus require minimal pruning by trimming overcrowded branches, with the tallest branch cut back to encourage bushy growth. Throughout summer, pinching back tips of the most vigorous growth assures more abundant canopy development. As mature plants may produce unwanted, fast-growing shoots called ‘water shoots’, these are removed from the main branches at the bottom or middle of the plant.\n\nIn cultivation in the UK, the cultivars ‘Meyer’ \nand ‘Variegata’ \nhave gained the Royal Horticultural Society’s Award of Garden Merit (confirmed 2017).\n\nIn 2016, world production of lemons (combined with limes for reporting) was 17.3 million tonnes. The top producers were India, Mexico, China, Argentina, and Brazil, collectively accounting for 62% of total production (table).\n\nMany plants taste or smell similar to lemons.\n\n\n"}
{"id": "4651526", "url": "https://en.wikipedia.org/wiki?curid=4651526", "title": "List of national parks of Venezuela", "text": "List of national parks of Venezuela\n\nNational Parks of Venezuela are protected areas in Venezuela, covering a wide range of habitats. In 2007 there were 43 national parks, covering 21.76% of Venezuela's territory.\n\nEvery Venezuela state has one or more national parks.\n\n18 national parks are over 1000 km²; 15 over 2000 km²; 5 over 5000 km² and 3 over 10,000 km². The largest parks, in the Guayana Region, are Parima Tapirapecó National Park (39,000 km²) and Canaima National Park (30,000 km²).\n\n\n"}
{"id": "254389", "url": "https://en.wikipedia.org/wiki?curid=254389", "title": "Louis Antoine de Bougainville", "text": "Louis Antoine de Bougainville\n\nLouis-Antoine, Comte de Bougainville (12 November 1729 – 31 August 1811) was a French admiral and explorer. A contemporary of the British explorer James Cook, he took part in the Seven Years' War in North America and the American Revolutionary War against Britain. Bougainville later gained fame for his expeditions, including circumnavigation of the globe in a scientific expedition in 1763, the first recorded settlement on the Falkland Islands, and voyages into the Pacific Ocean. Bougainville Island of Papua New Guinea as well as the Bougainvillea flower were named after him.\n\nBougainville was born in Paris, the son of a notary, on either 11 or 12 November 1729. In early life, he studied law, but soon abandoned the profession. In 1753 he entered the army in the corps of musketeers. At the age of twenty-five he published a treatise on integral calculus, as a supplement to De l'Hôpital's treatise, \"Des infiniment petits\".\n\nIn 1755 he was sent to London as secretary to the French embassy, where he was made a member of the Royal Society.\n\nIn 1756 Bougainville was stationed in Canada as captain of dragoons and aide-de-camp to the Marquis de Montcalm. He took an active part in the capture of Fort Oswego in 1756 and the 1757 Battle of Fort William Henry. He was wounded in 1758 at the successful defence of Fort Carillon. He sailed back to France the following winter, under orders from the marquis to obtain additional military resources for the colony. During this crossing, he continued to learn about the ways of the sea, skills that would later serve him well. Having distinguished himself in the war against Britain, Bougainville was rewarded with the Cross of St Louis and promoted to colonel. When he returned to Canada the following year, he had gained few supplies. The metropolitan officials had decided that, \"When the house is on fire, one does not worry about the stables\".\n\nDuring the pivotal year of 1759 (see Seven Years' War and French and Indian War), Bougainville participated in the defence of fortified Quebec City, the capital of New France. With a small elite troop under his command, among which were the \"Grenadiers\" and the \"Volontaires à cheval\", he patrolled the north shore of the St. Lawrence River, upstream from the city; he prevented the British several times from landing and cutting communications with Montreal. He did not have sufficient time, however, to rally his troops and attack the British rear when they successfully ascended the Plains of Abraham and attacked Quebec on 13 September.\n\nFollowing the death of the Marquis de Montcalm and the fall of Québec on 18 September – after the colonel's aborted attempt to resupply the besieged city – Bougainville was dispatched to the western front by his new commanding officer, the Chevalier de Lévis. He attempted to stop the British advance from his entrenchments at Île-aux-Noix. He was among the officers who accompanied Lévis to Saint Helen's Island off Montreal for the last French stand in North America before the general capitulation of 1761. Of the war, Bougainville wrote in his journal: \"It is an abominable kind of war. The very air we breathe is contagious of insensibility and hardness\".\n\nShipped back to Europe along with the other French officers, all deprived of military honours by the victors, Bougainville was prohibited by the terms of surrender from any further active duty against the British. He spent the remaining years of the Seven Years' War (1761 to 1763) as a diplomat, helping to negotiate the Treaty of Paris. Under this France ceded most of New France east of the Mississippi River to the British Empire.\n\nAfter the peace, the French decided to colonise the \"Isles Malouines\" (Falkland Islands). These islands were at that time almost unknown. At his own expense, Bougainville undertook the task of resettling Acadians who had been deported to France by the British because of their refusal to sign loyalty oaths.\n\nOn 15 September 1763, Bougainville set out from France with the frigate \"L'Aigle\" (Eagle) (captained by Nicolas Pierre Duclos-Guyot) and the sloop \"Le Sphinz\" (Sphinx) (captained by François Chenard de la Giraudais). This expedition included the naturalist and writer Antoine-Joseph Pernety (known as Dom Pernety), the priest and chronicler accompanying the expedition, together with the engineer and geographer Lhuillier de la Serre.\n\nThe expedition arrived in late January 1764 in French Bay (later renamed Berkeley Sound). They landed at Port Louis named after King Louis XV. A formal ceremony of possession of the Islands was held on 5 April 1764, after which Bougainville and Pernety returned to France. Louis XV formally ratified possession on 12 September 1764.\n\nAlthough the French colony did not number more than 150 people, for financial motivations (Bougainville having paid for the expeditions) and diplomatic reasons (Spain feared that the Falklands would become a rear base to attack her Peruvian gold), Bougainville was ordered by the French government to dismantle his colony and sell it to the Spanish. Bougainville received 200,000 francs in Paris and an additional 500,000 francs in Buenos Aires. Spain agreed to maintain the colony in Port Louis, thus preventing Britain from claiming title to the islands. Spain had claimed dominion before the French settlement in association with its colonies on the mainland. On 31 January 1767 at Río de la Plata, Bougainville met Don Felipe Ruiz Puente, commanding the frigate \"La Esmeralda \"and \"La Liebre\" (\"the Hare\") and future governor of Islas Malvinas, to transfer possession and evacuate the French population.\n\nBougainville wrote: \n\nIt was not before 1766, that the English sent a colony to settle in Port de la Croisade, which they had named Port Egmont; and captain Macbride, of the Jason frigate, came to our settlement the same year, in the beginning of December. He pretended that these parts belonged to his Britannic majesty, threatened to land by force, if he should be any longer refused that liberty, visited the governor, and sailed away again the same day.\n\nIn 1766 Bougainville received from Louis XV permission to circumnavigate the globe. He would become the 14th navigator, and the first Frenchman, to sail around the world. Completion of his mission bolstered the prestige of France following its defeats during the Seven Years' War. This was the first expedition to circumnavigate the globe with professional naturalists and geographers aboard.\n\nBougainville left Nantes on 15 November 1766 with two ships: \"Boudeuse\" (captain : Nicolas Pierre Duclos-Guyot) and the \"Étoile\" (commanded by François Chenard de la Giraudais). This was a large expedition, with a crew of 214 aboard \"Boudeuse\" and 116 aboard \"Étoile\".\n\nIncluded in the party was the botanist Philibert Commerçon (who named the flower \"Bougainvillea\") and his valet. The ship's surgeon later revealed this person as Jeanne Baré, possibly Commerçon's mistress; she would become the first woman known to circumnavigate the globe. Other notable people on this expedition were Count Jean-François de Galaup de la Pérouse (member of the crew); the astronomer Pierre-Antoine Veron; the surgeon of \"Boudeuse\" Dr. Louis-Claude Laporte; the surgeon of the \"Étoile\" Dr. François Vives; the engineer and cartographer aboard the Étoile Charles Routier de Romainville; and the writer and historian Louis-Antoine Starot de Saint-Germain.\n\nHe saw islands of the Tuamotu group on the following 22 March, on 2 April saw the peak of Mehetia and visited the island of Otaheite shortly after. He narrowly missed becoming their discoverer; a previous visit and claim had been made by British explorer Samuel Wallis in HMS \"Dolphin\" less than a year previously. Bougainville claimed the island for France and named it \"New Cythera\".\n\nHis expedition left Tahiti and sailed westward to southern Samoa and the New Hebrides, then on sighting Espiritu Santo turned west still looking for the \"Southern Continent\". On 4 June he almost ran into heavy breakers and had to change course to the north and east. He had almost found the Great Barrier Reef. He sailed through what is now known as the Solomon Islands but, because of the hostility of the people there, avoided landing. He named Bougainville Island for himself. The expedition was attacked by people from New Ireland so the French expedition made for the Moluccas. At Batavia, they received news of Wallis and Carteret who had preceded Bougainville in discovering Tahiti.\n\nOn 16 March 1769 the expedition completed its circumnavigation and arrived at St Malo. It had lost only seven of its 340 men, an extremely low level of casualties. This result was considered a credit to the enlightened management of the expedition by Bougainville.\n\nBougainville brought Ahutoru back to France, the first Tahitian to sail aboard a European vessel. In France, Bougainville introduced the Tahitian to high society, including introducing him to the King and Queen at Versailles. Bougainville also underwrote part of the costs for Ahutoru's return to Tahiti after a two-year absence. Unfortunately, Ahutoru died en route of smallpox in Oct. 1771.\n\nIn 1771, Bougainville published his travel log from the expedition under the title \"Le voyage autour du monde, par la frégate \"La Boudeuse\", et la flûte \"L'Étoile (a.k.a. \"Voyage autour du monde\" and \"A Voyage Around the World\"). The book describes the geography, biology and anthropology of Argentina (then a Spanish colony), Patagonia, Tahiti and Indonesia (then a Dutch colony). The book was a sensation, especially the description of Tahitian society. Bougainville described it as an earthly paradise where men and women lived in blissful innocence, far from the corruption of civilisation.\n\nBougainville's descriptions powerfully expressed the concept of the noble savage, influencing the utopian thoughts of philosophers such as Jean-Jacques Rousseau before the advent of the French Revolution. Denis Diderot's book \"Supplément au voyage de Bougainville\" retells the story of Bougainville's landing on Tahiti, narrated by an anonymous reader to one of his friends. Diderot used his fictional approach, including a description of the Tahitians as noble savages, to criticise Western ways of living and thinking.\n\nAfter an interval of several years, Bougainville again accepted a naval command and saw much active service between 1779 and 1782 during the American Revolutionary War, when France was as an ally of the rebels. He played a crucial part in the French victory at the Battle of the Chesapeake, which led to the eventual defeat of Great Britain.\n\nIn the memorable engagement of the Battle of the Saintes, in which Admiral George Rodney defeated the Comte de Grasse, Bougainville, who commanded the \"Auguste\", succeeded in rallying eight ships of his own division, and bringing them safely into Saint Eustace. He was promoted to \"chef d'escadre.\" When he re-entered the army, he was commissioned as \"maréchal de camp\".\n\nAfter the peace of 1783, Bougainville returned to Paris. He obtained the place of associate of the Academy. He proposed a voyage of discovery to the North Pole but did not gain the support of the French government.\n\nIn 1787, he became a member of the French Academy of Sciences. He obtained the rank of vice-admiral in 1791.\n\nIn 1794, having escaped from the Reign of Terror, he retired to his estate in Normandy. Returning to Paris, he was one of the founding members of the Bureau des Longitudes. In 1799, the Consul Napoleon made him a senator. He died in Paris on 31 August 1811.\n\n\n\n\n"}
{"id": "2151928", "url": "https://en.wikipedia.org/wiki?curid=2151928", "title": "M-SG reducing agent", "text": "M-SG reducing agent\n\nIn M-SG an alkali metal is absorbed into silica gel at elevated temperatures. The resulting black powder material is an effective reducing agent and safe to handle as opposed to the pure metal. The material can also be used as a desiccant and as a hydrogen source. \n\nThe metal is either sodium or a sodium - potassium alloy (NaK). The molten metal is mixed with silica gel under constant agitation at room temperature. This phase 0 material must be handled in an inert atmosphere. Heating phase 0 at takes it to phase I. When this material is exposed to dry oxygen the reducing power is not affected. At further heating to phase II can be handled safely in an ambient environment. \n\nThe metal reacts with the silica gel in an exothermic reaction in which NaSi nanoparticles are formed. The powder reacts with water to form hydrogen. \n\nCompounds such as biphenyl and naphthalene are reduced by the powder and form highly coloured radical anions. The powder can also be introduced in a column chromatography setup and eluted with organic reactants in order to probe the reducing power. The powder is mixed with additional (wet) silica gel which provides additional hydrogen. A Birch reduction of naphthalene takes 5 minutes elution time. The column converts benzyl chloride to bibenzyl in a Wurtz coupling and in a similar fashion dibenzothiophene is reduced to biphenyl.\n\n"}
{"id": "7641186", "url": "https://en.wikipedia.org/wiki?curid=7641186", "title": "Nucellar embryony", "text": "Nucellar embryony\n\nNucellar embryony (notated Nu+) is a form of seed reproduction that occurs in certain plant species, including many citrus varieties. During the development of seeds in plants that possess this genetic Trait, the nucellar tissue which surrounds the megagametophyte can produce additional embryos (polyembryony), which are genetically identical to the parent plant. These nucellar seedlings are clones of the parent. By contrast, zygotic seedlings are sexually produced and inherit genetic material from both parents. Zygotic and nucellar embryos can occur in the same seed, and a zygotic embryo can divide to produce multiple embryos.\n\nNucellar embryony is important to the citrus industry, as it allows for the production of uniform rootstock which yields consistent results in fruit production. However, this trait can interfere with progress in cross-breeding; most commercial scion varieties produce mainly nucellar seedlings which do not inherit any of the traits of the \"father\" plant.\n\n\n"}
{"id": "21510668", "url": "https://en.wikipedia.org/wiki?curid=21510668", "title": "Pascal's law", "text": "Pascal's law\n\nPascal's law (also Pascal's principle or the principle of transmission of fluid-pressure) is a principle in fluid mechanics that states that a pressure change occurring anywhere in a confined incompressible fluid is transmitted throughout the fluid such that the same change occurs everywhere. The law was established by French mathematician Blaise Pascal in 1647–48.\n\nPascal's principle is defined as\n\nThis principle is stated mathematically as:\n\nThe intuitive explanation of this formula is that the change in pressure between two elevations is due to the weight of the fluid between the elevations. Alternatively, the result can be interpreted as a pressure change caused by the change of potential energy per unit volume of the liquid due to the existence of the gravitational field. Note that the variation with height does not depend on any additional pressures. Therefore, Pascal's law can be interpreted as saying that \"any change in pressure applied\" at any given point of the fluid is transmitted \"undiminished throughout\" the fluid.\n\nIf a U-tube is filled with water and pistons are placed at each end, pressure exerted against the left piston will be transmitted throughout the liquid and against the bottom of the right piston. (The pistons are simply \"plugs\" that can slide freely but snugly inside the tube.) The pressure that the left piston exerts against the water will be exactly equal to the pressure the water exerts against the right piston. Suppose the tube on the right side is made wider and a piston of a larger area is used; for example, the piston on the right has 50 times the area of the piston on the left. If a 1 N load is placed on the left piston, an additional pressure due to the weight of the load is transmitted throughout the liquid and up against the larger piston. The difference between force and pressure is important: the additional pressure is exerted against the entire area of the larger piston. Since there is 50 times the area, 50 times as much force is exerted on the larger piston. Thus, the larger piston will support a 50 N load - fifty times the load on the smaller piston.\n\nForces can be multiplied using such a device. One newton input produces 50 newtons output. By further increasing the area of the larger piston (or reducing the area of the smaller piston), forces can be multiplied, in principle, by any amount. Pascal's principle underlies the operation of the hydraulic press. The hydraulic press does not violate energy conservation, because a decrease in distance moved compensates for the increase in force. When the small piston is moved downward 100 centimeters, the large piston will be raised only one-fiftieth of this, or 2 centimeters. The input force multiplied by the distance moved by the smaller piston is equal to the output force multiplied by the distance moved by the larger piston; this is one more example of a simple machine operating on the same principle as a mechanical lever.\n\nPascal's principle applies to all fluids, whether gases or liquids. A typical application of Pascal's principle for gases and liquids is the automobile lift seen in many service stations (the hydraulic jack). Increased air pressure produced by an air compressor is transmitted through the air to the surface of oil in an underground reservoir. The oil, in turn, transmits the pressure to a piston, which lifts the automobile. The relatively low pressure that exerts the lifting force against the piston is about the same as the air pressure in automobile tires. Hydraulics is employed by modern devices ranging from very small to enormous. For example, there are hydraulic pistons in almost all construction machines where heavy loads are involved.\n\n\"Pascal's barrel\" is the name of a hydrostatics experiment allegedly performed by Blaise Pascal in 1646. In the experiment, Pascal supposedly inserted a long vertical tube into a barrel filled with water. When water was poured into the vertical tube, the increase in hydrostatic pressure caused the barrel to burst.\n\nThe experiment is mentioned nowhere in Pascal's preserved works and it may be apocryphal, attributed to him by 19th-century French authors, among whom the experiment is known as \"crève-tonneau\" (approx.: \"barrel-buster\"); \nnevertheless the experiment remains associated with Pascal in many elementary physics textbooks.\n\n\n"}
{"id": "9494408", "url": "https://en.wikipedia.org/wiki?curid=9494408", "title": "Pedelec", "text": "Pedelec\n\nA pedelec (from pedal electric cycle) is a bicycle where the rider's pedalling is \"assisted\" by a small electric motor; thus it is a type of low-powered e-bike. However, unlike some other types of e-bikes, pedelecs are classified as conventional bicycles in many countries by road authorities rather than as a type of electric moped. Pedelecs include an electronic controller which cuts power to the motor when the rider is not pedalling or when a certain speed – usually – is reached. Pedelecs are useful for people who ride in hilly areas or in strong headwinds. While a pedelec can be any type of bicycle, a pedelec city bike is very common. A conventional bicycle can be converted to a pedelec with the addition of the necessary parts, i.e. motor, battery etc.\n\nMany jurisdictions classify pedelecs as bicycles as opposed to mopeds or motorcycles. More powerful e-bikes, such as the S-Pedelecs and \"power-on-demand\" e-bikes (those whose motors can provide assistance regardless of whether the rider is pedalling or not) are often classified as mopeds or even motorcycles with the rider thus subject to the regulations of such motor vehicles, e.g. having a license and a vehicle registration, wearing a helmet etc.\n\nPedelecs provide various advantages over conventional bicycles:\n\nThe main disadvantage of the pedelec is its price, which is significantly higher than that of similar, but conventional bicycles. The other additional expenses are minor: these are the electricity recharging costs and the eventual replacement of the battery, which together have been calculated to cost a mere €0.20 - €0.40 per 100 km.\nOther disadvantages are higher risk of theft due to its higher value, and the pedelec's higher weight caused by the battery, motor and sturdier frame.\n\nIn 1989, Michael Kutter, founder of \"Dolphin E-bikes\", designed and produced the first pedelec prototype. The first market-ready models of this kind were available in 1992 from the Swiss company \"Velocity\" under the name \"Dolphin\".\n\nIn 1994, larger numbers were produced by Yamaha under the name \"Power Assist\".\n\nIn 1995, the first \"Flyer\" in the same year founded the Swiss start-up company \"BKTech AG\" in small series by e-business (as an integral part of the start-up) to the market.\n\nThe term \"pedelec\" was coined by Susanne Brüsch in her 1999 thesis \"Pedelecs: Vehicles of the Future\" ().\n\nThroughout the whole of Europe in 2011, between 900,000 and 1.24 million units were sold; this is 29% more than in 2010.\n\nIt has been predicted that in 2015, 3 million e-bikes will be sold in Europe, and these will mostly be pedelecs.\n\n there were about 600,000 pedelecs on the road in Germany. Growth has been spectacular: the year before, 310,000-340,000 pedelecs were sold in Germany and this in turn was 55% more than in 2010. In fact, in Germany sales have gone up by more than 30% every year since 2008. In comparison, there were around 70 million conventional bicycles in Germany in 2011 according to ZIV, the German Bicycle Industry Association.\n\nAbout 95% of all e-bikes in Germany are in fact pedelecs.\n\nADAC, the German (auto)motive club has tested a large number of pedelecs in 2013, where about 70% of the pedelecs failed the test with a score lower than reasonable due to unsatisfactory safety and durability.\n\nOnly the Chinese market for pedelecs and e-bikes is bigger than the European. According to the National Bureau of Statistics in China more than 100 million e-bikes are on the road. The annual production in Chinese factories has increased from 58,000 (1998) to 33 million in (2011).\n\nTo really be useful, it is important for a pedelec to be \"legally classified as a bicycle\" in each country or jurisdiction rather than classified as a moped or motorcycle. Otherwise, if a pedelec is classified as a moped or motorbike then it may not be allowed in bike lanes or on bike paths; the pedelec may have to be registered; the rider may have to wear a motorcycle helmet; and/or vehicle insurance may have to be paid for.\n\nIn the EU this means it is important to legally distinguish between pedelecs and other e-bikes (including S-Pedelecs) as these are likely to \nIf any of those three conditions are true, then the bike is therefore more likely to be legally classified as a moped or motorcycle rather than as a bicycle.\n\nAccording to § 2 para 1 of 22 StVO 1960 two types of electric bikes can be distinguished:\n\nNot as motor vehicles under § 1 paragraph 2a KFG apply electric bicycles - whether hybrid \"(pedelec) \"or exclusively powered by electricity - with a bicycle for purposes of traffic 1960\n\nProvided above criteria are not exceeded is, therefore, under Austrian law, such an electric bike / pedelec not as electric power \"rad \"and therefore requires neither approval nor vehicle registration. As for normal (only muscle powered) bikes, also for electric bikes, the provisions of bicycle Regulation, for the control of these are the same as those for muscle StVO-powered bicycles, etc. Mandatory use of the bike path lane with bicycles. For their (commercial) In placing on the market subject to the product liability provisions.\n\nIn Austria, S-Pedelecs (power-assisted pedalling up to 45 km/h) are not classified as bicycles. Whether they lit at best as moped According to Article 1 paragraph 2. A) Directive 2002/24/EC (or as motorbike § 2 Abs 1 line 14 KFG) are typable and registered as a motor vehicle can be driven, so far (October 2010 has not survived).\n\nThe true Pedelecs are not required to have any other prerequisities than a bicycle has.\n\nHowever, any pedelec where the power assistance is triggered by merely turning wheels rather than pedal motion (a large number of cheap versions or notoriously front hub assistance), are required to have a licence plate for a scooter / small motorcycle (so called or ), a valid driving licence and an insurance.\n\nIn case of the power assistance stopping at a speed up to 25 km/h, the riders are not required to carry motorcycle helmets, however, this speed limit shall not be exceeded even while pedaling only.\nElectric bicycles, for example Specialized Turbo, without 25 km/h speed limitation for power assistance are considered a small motorcycle and besides license plate (yellow with black letters), driving license and insurance, a motorcycle helmet must be worn at all time from the start of 2017 and onwards.\n\nVery surprisingly, a large fleet of electric bicycles and pedelecs without required power control linked to the pedaling effort can be seen on the cycling paths without any proper registration.\nAdditionally, many users found very simple ways how to tweak their pedelecs in order to overcome the pedaling sensor, making their pedelecs without further proper vehicle registration illegal.\n\nPedelecs, and all kinds of mechanical assist, are regarded as \"motor vehicles\" and classified as motor cycles, making legal registration impossible. The Hong Kong Transport Department is currently conducting a review, with a first report expected in mid-2020.\n\nPedelecs are allowed, when wearing a helmet, the motor output is limited to 200W and the motor cuts out by 25 km/h.\n\nElectric vehicles with a motor having power less than 250W, and a maximum speed 25 km/h or lower, are not required to be registered under the Central Motor Vehicle Rules, and may be driven freely without any license/paperwork. \n\nElectric-assisted bicycles are treated as human-powered bicycles, while bicycles capable of propulsion by electric power alone face additional registration and regulatory requirements as mopeds. Requirements include electric power generation by a motor that cannot be easily modified, along with a power assist mechanism that operates safely and smoothly. In December 2008, The assist ratio was updated as follow:\n\n\"(See Moped#Individual countries/regions)\"\n\nAs of 30 May 2012, Australia has an additional new electric bicycle category using the European model of a \"Pedelec\" as per EN15194 Standard. This means the bicycle can have a motor of 250 watts continuous rated power which must be activated only by pedalling (if above 6 km/h) and must cut out over 25 km/h. The State of Victoria is the first to amend their local road rules to accommodate this new standard as of 18 September 2012.\n\nPedelecs differ from an ordinary bicycle by an additional electric motor, a battery, an electronic control system for the motor as well as a sensor to detect the motion of the cranks. Most models are also equipped with a battery charge indicator and a motor power setting, either continuously or divided into support levels.\n\nBesides the motor, the battery is the main component of pedelec technology. It is usually either a NiMH - Ni - or a lithium-ion battery. The battery capacity is up to 24 Ah n (Ah) at 24 or 36 V (V) or up to 15 amp hours at 48 volts. The stored energy can be up to about 800 watt hour n (Wh), but mostly about 400 Wh (2013). In ideal conditions, after a thousand charges NiCd batteries have 85% of their original capacity and are therefore considered worn. With NiMH batteries about 400 to 800 cycles are possible. The charging time depending on the type of battery is around 2 to 9 hours. The durability of the battery is dependent on other factors. As lead-acid batteries discharge they provide less power, so that full motor power is no longer achieved. The very light, more expensive lithium ion batteries are now used by most manufacturers and have a range of up to 100 kilometers with moderate pedaling and a medium capacity battery (e.g. 15 Ah). Lithium batteries do not tolerate frost and should not be charged at frosty temperatures. For safety, the chemical composition and the quality of the electronics are crucial. Especially with short circuit and over voltage, lithium-ion batteries react very strongly. These problems in laptops have led to recalls. Lithium iron phosphate (LFP) batteries are a notable exception. They have far safer thermal characteristics as well as being non-toxic.\n\nIn evaluating pedelec batteries, it is useful to consider not only the capacity, but also criteria such as durability, memory effect, charging time, weight, safety and environmental protection.\n\nManufacturers which equip their pedelecs with NiCd batteries usually deliver them with an AC adapter that discharges the battery completely before the actual charging process in order to decrease the memory effect. NiMH batteries have a much lower memory effect. With lithium-ion batteries there is no memory effect.\n\nA lithium iron phosphate battery is much longer-lived than a lithium-ion battery. Its use significantly reduces operating costs resulting from battery wear. In 2013, they are not yet available as standard in most pedelec models, but some pedelecs (e.g. Beyond Oil) have begun installing LFP batteries as standard.\n\nFor switching or control of the motor, there are several possibilities:\n\nIn addition, the speed of the vehicle are measured on the wheel, in particular, for example, to drive the motor from 25 km/h off.\n\nThe measurement can be further processed mechanically or electronically and is used to control the motor on and off or to regulate a control function based on continuously.\n\nThe fed power is based on the sensor data (force sensor, crank speed, ground speed) is calculated based on the chosen level of support from the motor controller. The so-called support levels, that is, how much the motor supported in addition to the driver's performance lie in horizontal drive 5-400 percent.\n\nWhen the motors are regularly used heavily, especially when going uphill they may heat up significantly, some have a temperature sensor in the motor winding, where if a certain temperature is reached the electronics may reduce power to the motor. Ideally the electronics disconnect the battery at a predetermined discharge voltage to prevent total discharge and to ensure sufficient supply for the operation of the lighting system. This can be done by electronics in the battery.\n\nWhen running with a force sensor, the motor is automatically a certain percentage of the service provided to the driver. In many models, this proportion may be set in several stages. There are also models where the support level can be set only at the dealer to the customer.\n\nIn the version with speed sensor (s) of the motor is automatically using a function to a set percentage of the self-applied force. Since the force required at the speed rises sharply, it can be calculated in some models without force sensor.\n\nThe slide or traction can help with Maximization of legislation to support a motor without pedaling to 6 km/h. The shift means has the advantage that you can let the bike roll alongside with motor support without pedaling or you push yourself (e.g. must, when transporting a heavy load, or so you walk up the wheel alone on a hill may be). For some models, the allowed 6 km/h can be achieved only in top gear, the other gears in the wheel rolls correspondingly slower. In any case, it allows for a faster (and more controlled physically) starting from standstill to \"green\" switches over light n\n\nThe power electronics, depending on the type of motor, consist of a DC motor controller with pulse width modulation or a regulated DC-AC converter.\n\nAlmost exclusively, pedelecs use DC motors, using commutator-less and brush disc motors, which are suitable for direct drive, and brush motors with gears.\n\nThe use of maintenance-free AC induction motors pedelec is the exception.\n\nA direct rotor hub motor may feature a regenerative brake, so it can be used as a brake that converts some of the kinetic energy into battery charge. In addition to charging the battery when braking this incurs less wear on the \ntraditional brake, reducing braking noises.\n\nSee generally starting points of the electric drive. When Pedelec specifically, the type of control of the drive by the pedaling (see above), which may be integrated in the drive.\n\nThe position of the motor has a significant impact on the handling of the pedelec. The following combinations of actuator position and motor have been successful:\n\nGenerally the range with motor support is between 7 km for a constant rise and up to 70 km. At medium power addition, it is about 20 to 50 km. On some models, by default two successive switchable batteries are housed in luggage bags, here is the range specified at medium power addition of 100 km.\n\nA conventional battery (36 V / 7 Ah) (1.9 to 5.1 kg mass in a pedelec) has an energy content of around 250 Wh (1 kg of gasoline has about 11,500 Wh). The conversion of electrical energy into mechanical work is done with some loss of energy due to the generation of heat. Typically, incurred losses are around 25 percent, depending on the efficiency of the motor and the motor controller. Thus, a pedelec with a 70 kg rider (total mass of ~100 kg) can be calculated to go about 5.6 kilometres on a 10% grade at 25 km/h on battery power alone (assuming Frontal area = 0.4 meter-squared, Drag coefficient = 0.7, Altitude = 100m, Wind speed = 10 km/h and Rolling resistance coefficient = 0.007). Depending on the assistance of the rider (which is required on a pedelec), a proportionally greater range is possible.\n\nSafety issues are a concern in relatively flat areas, but are more pronounced in the hills. Hilly areas provide changing conditions; this poses the possibility of encountering more critical situations and thus more accidents may occur. Cars may need to overtake pedelecs at higher speeds than cars would overtake regular bikes, and this may result in more accidents with serious consequences for both cyclists and drivers. For drivers and pedestrians, it may be difficult to estimate how fast a cyclist is moving. Also, an elderly on a pedelec may ride much faster than previously possible. Risky situations can also arise at exits and junctions. To illustrate the consequences of such critical situations, the German Insurers Accident Research (UDV) has conducted a research project with road tests, performance tests and crash tests for pedelecs.\n\nOn the other hand, many pedelec (and e-bike) users report that they can ride more defensively with the auxiliary electric drive assisting them; unlike traditional bicyclists that tend to be averse to braking since this incurs effort to accelerate again, a pedelec rider can brake and then accelerate back to a normal speed with much less effort. The Bavarian accident statistics for the first half of 2012 lists 6,186 accidents involving bicycles, of which 76 are e-bikes and notes that the accident risk of e-bikes is not higher than for other bicycles.\n\nThe use of S-Pedelecs involves an additional risk. Not only do they achieve a higher average speed, but a higher top speed (usually 45 km/h) and can also expect a higher annual mileage.\n\n\n\n"}
{"id": "15414727", "url": "https://en.wikipedia.org/wiki?curid=15414727", "title": "Pentagonal pyramidal molecular geometry", "text": "Pentagonal pyramidal molecular geometry\n\nIn chemistry, pentagonal pyramidal molecular geometry describes the shape of compounds where in six atoms or groups of atoms or ligands are arranged around a central atom, defining the vertices of a pentagonal pyramid. It is one of the few molecular geometries with uneven bond angles. \n\n\n"}
{"id": "13113563", "url": "https://en.wikipedia.org/wiki?curid=13113563", "title": "Plutonium(III) chloride", "text": "Plutonium(III) chloride\n\nPlutonium(III) chloride is the chemical compound with the formula PuCl. It can be prepared by dissolving the metal in hydrochloric acid.\n\nPlutonium atoms in crystalline PuCl are 9 coordinate, and the structure is tricapped trigonal prismatic.\n\nAs with all plutonium compounds, it is subject to control under the Nuclear Non-Proliferation Treaty. Due to the radioactivity of plutonium, all of its compounds, PuCl included, are warm to the touch. Such contact is not recommended, since touching the material may result in serious injury.\n"}
{"id": "41240661", "url": "https://en.wikipedia.org/wiki?curid=41240661", "title": "Pollution of Lake Karachay", "text": "Pollution of Lake Karachay\n\nLake Karachay, located in the southern Ural Mountains in eastern Russia, was a dumping ground for the Soviet Union's nuclear weapon facilities. It was also affected by a string of accidents and disasters causing the surrounding areas to be highly contaminated with radioactive waste. Washington, D.C.-based Worldwatch Institute has described it as the \"most polluted spot on Earth\".\n\nBuilt in the late 1940s Mayak was one of Russia's most prominent nuclear weapons factories. Mayak was kept secret by the government until 1990. When Russian president Boris Yeltsin signed a 1992 decree opening the area, Western scientists were able to gain access. The sediment of the lake bed is estimated to be composed almost entirely of high level radioactive waste deposits to a depth of roughly 11 feet (3.4 m).\n\nIn 1994, a report revealed that 5 million cubic meters of polluted water had migrated from Lake Karachay, and was spreading to the south and north at 80 meters per year, \"threatening to enter water intakes and rivers\". The authors acknowledged that \"theoretical hazards developed into actual events\".\n\nIn November 1994, officials from the Russian Ministry of Atomic Energy stated that Soviet officials initiated a process following the 1957 Kyshtym disaster resulting in the transfer of 3 billion curies of high level nuclear waste into deep wells at three other sites.\n\nThe Techa river, which provides water to nearby areas, was contaminated, and about 65% of local residents fell ill with radiation sickness. Doctors called it the \"special disease\" because they were not allowed to note radiation in their diagnoses as long as the facility was secret. In the village of Metlino, it was found that 65% of residents were suffering from chronic radiation sickness. Workers at the plutonium plant were also affected.\n\nThe pollution of Lake Karachay is connected to the disposal of nuclear materials from Mayak. Among workers, cancer mortality remains an issue. By the time Mayak's existence was officially recognized, there had been a 21% rise in cancer cases, a 25% rise in birth defects, and a 41% rise in leukemia in the surrounding region of Chelyabinsk. By one estimate, the river contains 120 million curies of radioactive waste.\n\nNuclear waste, either from civilian or military nuclear projects, remains a serious threat to the environment of Russia. Reports suggest that there are few or no road signs warning about the polluted areas surrounding Lake Karachay.\n\nSome parts of the lake are extremely radioactive (600 röntgens/hour) and one could receive a lethal dose of radiation in 30 minutes (300 röntgens).\n\n"}
{"id": "11563568", "url": "https://en.wikipedia.org/wiki?curid=11563568", "title": "Reactor-grade plutonium", "text": "Reactor-grade plutonium\n\nReactor-grade plutonium/RGPu is the isotopic grade of plutonium that is found in spent nuclear fuel after the primary fuel, that of Uranium-235 that a nuclear power reactor uses, has burnt up. The Uranium-238 from which most of the plutonium isotopes derive, by neutron capture, is frequently found alongside the U-235 fuel in civilian reactors, in the form of Low enriched uranium. \n\nIn contrast to the low burnup of weeks or months that is commonly required to produce weapons-grade plutonium(WGPu/Pu), the long time in the reactor that produces reactor-grade plutonium leads to transmutation of much of the fissile, relatively long half-life isotope Pu into a number of other isotopes of plutonium that are less fissile or more radioactive.\n\nThermal-neutron reactors (today's most numerous nuclear power stations) can reuse reactor-grade plutonium only to a limited degree as MOX fuel, and only for a second cycle; fast-neutron reactors, of which there are fewer than a handful operating today, can use reactor-grade plutonium fuel as a means to reduce the transuranium content of spent nuclear fuel/nuclear waste.\n\nAt the beginning of the industrial scale production of plutonium-239 in war era \"production reactors\", trace contamination or co-production with plutonium-240 was initially observed, with these trace amounts resulting in the dropping of the Thin Man weapon-design as unworkable. The difference in purity, of how much, continues to be important in assessing significance in the context of nuclear proliferation and weapons-usability.\nThe DOE definition of \"reactor grade\" plutonium changed in 1976. Before this, three grades were recognised. The change in the definition for \"reactor grade\", from describing plutonium with greater than 7% Pu-240 content prior to 1976, to \"reactor grade\" being defined as containing 19% or more Pu-240, coincides with the 1977 release of information about a 1962 \"\"reactor grade\" nuclear test\". The question of which definition or designation applies, that, of the old or new scheme, to the 1962 \"reactor-grade\" test, has not been officially disclosed.\n\nFrom 1976, four grades were recognised:\n\nReprocessing or recycling of the spent fuel from the most common class of civilian-electricity-generating or \"power reactor\" design, the LWR, (with examples being the PWR or BWR) recovers \"reactor grade\" plutonium (as defined since 1976), not \"fuel grade\".\n\nThe physical mixture of isotopes in reactor-grade plutonium make it extremely difficult to handle and form and therefore explains its undesirability as a weapon-making substance, in contrast to weapons grade plutonium, which can be handled relatively safely with thick gloves.\n\nTo produce weapons grade plutonium, the uranium nuclear fuel must spend no longer than several weeks in the reactor core before being removed, creating a low fuel burnup. For this to be carried out in a pressurized water reactor - the most common reactor design for electricity generation - the reactor would have to prematurely reach cold shut down after only recently being fueled, meaning that the reactor would need to cool decay heat and then have its reactor pressure vessel be depressurized, followed by a fuel rod defueling. If such an operation were to be conducted, it would be easily detectable, and require prohibitively costly reactor modifications.\n\nOne example of how this process could be detected in PWRs, is that during these periods, there would be a considerable amount of down time, that is, large stretches of time that the reactor is not producing electricity to the grid. On the other hand, the modern definition of \"reactor grade\" plutonium is produced only when the reactor is run at high burnups and therefore producing a high electricity generating capacity factor. According to the US Energy Information Administration (EIA), in 2009 the capacity factor of US nuclear power stations was higher than all other forms of energy generation, with nuclear reactors producing power approximately 90.3% of the time and Coal thermal power plants at 63.8%, with down times being for simple routine maintenance and refuelling.\n\nThe degree to which typical Generation II reactor high burn-up produced reactor-grade plutonium is less useful than weapons-grade plutonium for building nuclear weapons is somewhat debated, with many sources arguing that the maximum probable theoretical yield would be bordering on a fizzle explosion of the range 0.1 to 2 kiloton in a Fat Man type device. As computations state that the energy yield of a nuclear explosive decreases by one and two orders of magnitude if the 240 Pu content increases from 5% (nearly weapons-grade plutonium) to 15%( 2 kt) and 25%,(0.2 kt) respectively. These computations are theoretical and assume the non-trivial issue of dealing with the heat generation from the higher content of non-weapons usable Pu-238 could be overcome.) As the premature initiation from the spontaneous fission of Pu-240 would ensure a low explosive yield in such a device, the surmounting of both issues in the construction of an Improvised nuclear device is described as presenting \"daunting\" hurdles for a Fat Man-era implosion design, and the possibility of terrorists achieving this fizzle yield being regarded as an \"overblown\" apprehension with the safeguards that are in place.\n\nOthers disagree on theoretical grounds and state that while they would not be suitable for stockpiling or being emplaced on a missile for long periods of time, dependably high non-fizzle level yields can be achieved, arguing that it would be \"relatively easy\" for a well funded entity with access to fusion boosting tritium and expertise to overcome the problem of pre-detonation created by the presence of Pu-240, and that a remote manipulation facility could be utilized in the assembly of the highly radioactive gamma ray emitting bomb components, coupled with a means of cooling the weapon pit during storage to prevent the plutonium charge contained in the pit from melting, and a design that kept the implosion mechanisms high explosives from being degraded by the pit's heat. However, with all these major design considerations included, this fusion boosted reactor grade plutonium primary will still fizzle if the fission component of the primary does not deliver more than 0.2 kilotons of yield, which is regarded as the minimum energy necessary to start a fusion burn. The probability that a fission device would fail to achieve this threshold yield increases as the burnup value of the fuel increases. \n\nNo information available in the public domain suggests that any well funded entity has ever seriously pursued creating, a nuclear weapon with an isotopic composition similar to modern, high burnup, reactor grade plutonium. All nuclear weapon states have taken the more conventional path to nuclear weapons by either uranium enrichment or producing low burnup, \"fuel-grade\" and weapons-grade plutonium, in reactors capable of operating as production reactors, the isotopic content of reactor-grade plutonium, created by the most common commercial power reactor design, the pressurized water reactor, never directly being considered for weapons use.\n\nAs of April 2012, there were thirty-one countries that have civil nuclear power plants, of which nine have nuclear weapons, and almost every nuclear weapons state began producing weapons first instead of commercial nuclear power plants. The re-purposing of civilian nuclear industries for military purposes would be a breach of the Non-proliferation treaty.\n\nAs nuclear reactor designs come in a wide variety and are sometimes improved over time, the isotopic ratio of what is deemed \"reactor grade plutonium\" in one design, as it compares to another, can differ substantially. For example the British Magnox reactor, a Generation I gas cooled reactor(GCR) design, can rarely produce a fuel burnup of more than 2-5 GWd/tU. Therefore the \"reactor grade plutonium\" and the purity of Pu-239 from discharged magnox reactors is approximately 80%, depending on the burn up value. In contrast, the generic civilian Pressurized water reactor, routinely does (typical for 2015 Generation II reactor) 45 GWd/tU of burnup, resulting in the purity of Pu-239 being 50.5%, alongside a Pu-240 content of 25.2%, The remaining portion includes much more of the heat generating Pu-238 and Pu-242 isotopes than are to be found in the \"reactor grade plutonium\" from a Magnox reactor.\n\nThe reactor grade plutonium nuclear test was a \"low-yield (under 20 kilotons)\" underground nuclear test using non-weapons-grade plutonium conducted at the US Nevada Test Site in 1962. Some information regarding this test was declassified in July 1977, under instructions from President Jimmy Carter, as background to his decision to prohibit nuclear reprocessing in the USA.\n\nThe plutonium used for the US-UK 1962 device was apparently sourced from the military magnox reactors at Calder Hall or Chapelcross in the United Kingdom, and provided to the US under the 1958 US-UK Mutual Defence Agreement. Only two US-UK underground nuclear tests occurred in 1962, the first being test shot \"Pampas\" of Operation Nougat which produced a yield of 9.5 kilotons and the second being test shot \"Tendrac\" of Operation Storax, which produced a yield cited as being \"low\" (under 20 kilotons). Another \"reactor-grade\" test, though not necessarily of the same US-UK 1962 design and plutonium-240 content, was the 1953 British Operation Totem series of nuclear tests. Despite producing a yield of 8-10 kilotons, from a Plutonium-239 content estimated at from 87-91%, something about the various tests displeased the British, for they never went through with their plan to weaponize Plutonium from magnox reactors at Calder Hall.\n\nThe initial codename for the Magnox reactor design amongst the government agency which mandated it, the UKAEA, was the \"Pressurised Pile Producing Power and Plutonium(PIPPA)\" and as this codename suggests, the reactor was designed as both a power plant and, when operated with low fuel \"burn-up\"; as a producer of plutonium-239 for the nascent nuclear weapons program in Britain. This intentional dual-use approach to building electric power-reactors that could operate as production reactors in the early Cold War era, was typical of many nations, in the now designated, \"Generation I nuclear reactors\". With these being designs all focused on giving access to fuel after a short burn-up, which is known as Online refuelling.\n\nThe 2006 North Korean nuclear test, the first by the DPRK, is also said to have had a magnox reactor as the root source of its plutonium, operated in Yongbyon Nuclear Scientific Research Center in North Korea. This test detonation resulted in the creation of a low-yield fizzle explosion, producing an estimated yield of approximately 0.48 kilotons, from an undisclosed isotopic composition. The 2009 North Korean nuclear test likewise was based on plutonium. Both produced a yield of 0.48 to 2.3 kiloton of TNT equivalent respectively and both were described as fizzle events due to their low yield, with some commentators even speculating whether or not, at the lower yield estimates for the 2006 test, that instead the blast may have been simply the equivalent of US$100,000 worth of ammonium nitrate.\n\nThe isotopic composition of the 1962 US-UK test has similarly not been disclosed, other than the description \"reactor grade\", and it has not been disclosed which definition was used in describing the material for this test as \"reactor grade\". According to Alexander DeVolpi, the isotopic composition of the plutonium used in the US-UK 1962 test could not have been what we now consider to be reactor-grade, and the DOE now implies, but doesn't assert, that the plutonium was fuel grade. Likewise, the World Nuclear Association suggests that the US-UK 1962 test had at least 85% plutonium-239, a much higher isotopic concentration than what is typically present in the spent fuel from the majority of operating civilian reactors.\n\nIn 2002 former Deputy Director General of the IAEA, Bruno Pelaud, stated that the DoE statement was misleading and that the test would have the modern definition of fuel-grade with a Pu-240 content of only 12%\n\nAccording to political analyst Matthew Bunn and presidential technology advisor John Holdren, both of the Belfer Center for Science and International Affairs, in 1997, they cited an 1970s official U.S. assessment of programmatic alternatives for plutonium disposition. While it does not specify which RGPu definition is being referred to, it nonetheless states that \"reactor-grade plutonium (with an unspecificed isotopic composition) can be used to produce nuclear weapons at all levels of technical sophistication,\" and \"advanced nuclear weapon states such as the United States and Russia, using modern designs, could produce weapons from \"reactor-grade plutonium\" having reliable explosive yields, weight, and other characteristics generally comparable to those of weapons made from weapon-grade plutonium\"\n\nIn a 2008 paper, Kessler et al. used a thermal analysis to conclude that a hypothetical nuclear explosive device was \"technically unfeasible\" using reactor grade plutonium from a reactor that had a burn up value of 30 GWd/t using \"low technology\" designs akin to Fat Man with spherical explosive lenses, or 55 GWd/t for \"medium technology\" designs.\n\nAccording to the Kessler et al. criteria, \"high-technology\" hypothetical nuclear explosive devices(HNEDs), that could be produced by the experienced nuclear weapons states(NWSs) would be technically unfeasible with reactor-grade plutonium containing more than approximately 9% of the heat generating Pu-238 isotope.\n\nThe British Magnox reactor, a Generation I gas cooled reactor(GCR) design, can rarely produce a fuel burnup of more than 2-5 GWd/tU. The Magnox reactor design was codenamed \"PIPPA\" (Pressurised Pile Producing Power and Plutonium) by the UKAEA to denote the plant's dual commercial (power reactor) and military (production reactor) role. The purity of Pu-239 from discharged magnox reactors is approximately 80%, depending on the burn up value.\n\nIn contrast, for example, a generic civilian Pressurized water reactor's spent nuclear fuel isotopic composition, following a typical Generation II reactor 45 GWd/tU of burnup, is 1.11% plutonium, of which 0.56% is Pu-239, and 0.28% is Pu-240, which corresponds to a Pu-239 content of 50.5% and a Pu-240 content of 25.2%. For a lower generic burn-up rate of 43,000 MWd/t, as published in 1989, the plutonium-239 content was 53% of all plutonium isotopes in the reactor spent nuclear fuel. \nThe US NRC has stated that the commercial fleet of LWRs presently powering homes, had an average burnup of approximately 35 GWd/MTU in 1995, while in 2015, the average had improved to 45 GWd/MTU.\n\nThe odd numbered fissile plutonium isotopes present in spent nuclear fuel, such as Pu-239, decrease significantly as a percentage of the total composition of all plutonium isotopes (which was 1.11% in the first example above) as higher and higher burnups take place, while the even numbered non-fissile plutonium isotopes (e.g. Pu-238, Pu-240 and Pu-242) increasingly accumulate in the fuel over time.\n\nAs power reactor technology increases, a goal is to reduce the spent nuclear fuel volume by increasing fuel efficiency and simultaneously reducing down times as much as possible to increase the economic viability of electricity generated from fission-electric stations. To this end, the reactors in the U.S. have doubled their average burn-up rates from 20-25 GWd/MTU in the 1970s to over 45 GWd/MTU in the 2000s. Generation III reactors under construction have a designed-for burnup rate in the 60 GWd/tU range and a need to refuel once every 2 years or so. For example, the European Pressurized Reactor has a designed-for 65 GWd/t, and the AP1000 has a designed for average discharge burnup of 52.8 GWd/t and a maximum of 59.5 GWd/t. In-design generation IV reactors will have burnup rates yet higher still.\n\nToday's moderated/\"thermal reactors\" primarily run on the once-thru fuel cycle though they can reuse once-thru reactor-grade plutonium to a limited degree in the form of mixed-oxide or MOX fuel, which is a routine commercial practice in most countries outside the US as it increases the sustainability of nuclear fission and lowers the volume of high level nuclear waste. \n\nOne third of the energy/fissions at the \"end\" of the practical fuel life in a thermal reactor are from plutonium, the end of cycle occurs when the U-235 percentage drops, the primary fuel that drives the neutron economy inside the reactor and the drop necessitates fresh fuel being required, so without design change, one third of the fissile fuel in a \"new\" fuel load can be fissile reactor-grade plutonium with one third less of Low enriched uranium needing to be added to continue the chain reactions anew, thus achieving a partial recycling. \n\nA typical 5.3% reactor-grade plutonium MOX fuel bundle, is transmutated when it itself is again burnt, a practice that is typical in French \"thermal\" reactors, to a twice-thru reactor-grade plutonium, with an isotopic composition of 40.8% Pu-239 and 30.6% Pu-240 at the end of cycle (EOC). \"MOX grade plutonium(MGPu)\" is generally defined as having more than 30% Pu-240.\n\nA limitation in the number of recycles exists within thermal reactors, as opposed to the situation in fast reactors, as in the thermal neutron spectrum only the odd-mass isotopes of plutonium are fissile, the even-mass isotopes thus accumulate, in all high thermal-spectrum burnup scenarios. Plutonium-240, an even-mass isotope is, within the thermal neutron spectrum, a fertile material like uranium-238, becoming fissile plutonium-241 on neutron capture; however, the even-mass plutonium-242 not only has a low neutron capture cross section within the thermal spectrum, it also requires 3 neutron captures before becoming a fissile nuclide. \n\nWhile most thermal neutron reactors must limit MOX fuel to less than half of the total fuel load for nuclear stability reasons, due to the reactor design operating within the limitations of a thermal spectrum of neutrons, Fast neutron reactors on the other hand can use plutonium of any isotopic composition, operate on completely recycled plutonium and in the fast \"burner\" mode, or fuel cycle, fission and thereby eliminate all the plutonium present in the world stockpile of once-thru spent fuel. The modernized IFR design, known as the S-PRISM concept and the Stable salt reactor concept, are two such fast reactors that are proposed to burn-up/eliminate the plutonium stockpiles in Britain that was produced from operating its fleet of inefficient MAGNOX reactors and thus has generated the largest civilian stockpile of fuel-grade/\"reactor-grade plutonium\" in the world.\n\nIn Bathke's equation on \"attractiveness level\" of Weapons-grade nuclear material, the Figure of Merit(FOM) the calculation generates, returns the suggestion that Sodium Fast Breeder Reactors are unlikely to reach the desired level of proliferation resistance, while Molten Salt breeder reactors are more likely to do so.\n\nIn the fast breeder reactor cycle, or fast breeder mode, as opposed to the fast-burner, the French Phénix reactor uniquely demonstrated multi-recycling and reuses of its reactor grade plutonium. Similar reactor concepts and fuel cycling, with the most well known being the Integral Fast Reactor are regarded as one of the few that can realistically achieve \"planetary scale sustainability\", powering a world of 10 billion, whilst still retaining a small environmental footprint. In breeder mode, fast reactors are therefore ofen proposed as a form of Renewable or sustainable nuclear energy. Though the \"[reactor-grade]plutonium economy\" it would generate, presently returns social distaste and varied arguments about proliferaton-potential, in the public mindset.\n\nAs is typically found in civilian European \"thermal\" reactors, a 5.3% plutonium MOX fuel-bundle, produced by conventional wet-chemical/PUREX reprocessing of an initial fuel assembly that generated 33 GWd/t before becoming spent nuclear fuel, creates, when it itself is burnt in the \"thermal\" reactor, a spent nuclear fuel with a plutonium isotopic composition of 40.8% Pu-239 and 30.6% Pu-240.\nComputations state that the energy yield of a nuclear explosive decreases by two orders of magnitude if the Pu-240 content increases to 25%,(0.2 kt).\n\nReprocessing, which mainly takes the form of recycling reactor-grade plutonium back into the same or a more advanced fleet of reactors, was planned in the US in the 1960s. At that time the uranium market was anticipated to become crowded and supplies tight so together with recycling fuel, the more efficient fast breeder reactors were thereby seen as immediately needed to efficiently use the limited known uranium supplies. This became less urgent as time passed, with both reduced demand forecasts and increased uranium ore discoveries, for these economic reasons, fresh fuel and the reliance on solely fresh fuel remained cheaper in commercial terms than recycled. \n\nIn 1977 the Carter administration placed a ban on reprocessing spent fuel, in an effort to set an international example, as within the US, there is the perception that it would lead to nuclear weapons proliferation. This decision has remained controversial and is viewed by many US physicists and engineers as fundamentally in error, having cost the US taxpayer and the fund generated by US reactor utility operators, with cancelled programs and the over 1 billion dollar investment into the proposed alternative, that of Yucca Mountain nuclear waste repository ending in protests, lawsuits and repeated stop-and-go decisions depending on the opinions of new incoming presidents.\n\nAs the \"undesirable\" contaminant from a weapons manufacturing viewpoint, Pu-240, decays faster than the Pu-239, with half lives of 6500 and 24,000 years respectively, the quality of the plutonium grade, increases with time (although its total quantity decreases during that time as well). Thus, physicists and engineers have pointed out, as hundreds/thousands of years pass, the alternative to fast reactor \"burning\" or recycling of the plutonium from the world fleet of reactors until it is all burnt up, the alternative to burning most frequently proposed, that of deep geological repository, such as Onkalo spent nuclear fuel repository, have the potential to become \"plutonium mines\", from which \"weapons-grade\" material for nuclear weapons could be acquired by simple PUREX extraction, in the centuries-to-millenia to come.\n\nAum Shinrikyo, who succeeded in developing Sarin and VX nerve gas is regarded to have lacked the technical expertise to develop, or steal, a nuclear weapon. Similarly, Al Qaeda was exposed to numerous scams involving the sale of radiological waste and other non-weapons-grade material. The RAND corporation suggested that their repeated experience of failure and being scammed has possibly led to terrorists concluding that nuclear acquisition is too difficult and too costly to be worth pursuing.\n\n\n"}
{"id": "31611414", "url": "https://en.wikipedia.org/wiki?curid=31611414", "title": "Robin Wood (environmental organisation)", "text": "Robin Wood (environmental organisation)\n\nRobin Wood is a German environmental advocacy group. The group was founded in 1982 by former members of Greenpeace who desired a decentralized grassroots organization with greater autonomy to address specific local German issues. Robin Wood is based in Bremen and, in 2008, was composed of fifteen mostly autonomous regional groups within Germany.\n\nInitially concerned with the conservation of German forests, particularly the Black Forest, the group's activism efforts later expanded to include rainforest conservation, paper recycling, reduction of acid rain and other related areas. Robin Wood stages \"attention-grabbing\" demonstrations and confrontational public protests to raise awareness. Although peaceful, the demonstrations are described as \"often illegal.\" The group publishes the quarterly \"Robin Wood Magazin\".\n\n"}
{"id": "52256482", "url": "https://en.wikipedia.org/wiki?curid=52256482", "title": "Royal funeral chariot", "text": "Royal funeral chariot\n\nA royal funeral chariot is a wheeled vehicle traditionally used to transport the bodies of royalty during funeral processions in some cultures of Mainland Southeast Asia. Today, they remain in use in Thailand and Cambodia.\n\nThe royal funeral chariots of Thailand were used since the Ayutthaya period, but they were destroyed when Ayutthaya was completely sacked in 1767. New funeral chariots were commissioned during the reign of King Rama I at the beginning of the Rattanakosin period. They include, among other smaller accompanying vehicles, the two main chariots which have been used to transport the bodies of kings of the Chakri Dynasty, the Maha Phichai Ratcharot (\"Royal Great Victory Carriage\" or \"Royal Chariot of Great Victory\"), built in 1795, and Vejayanta Ratcharot (Named after Indra's chariot Vejayanta), built in 1799. The chariots, built of teak and ornately carved, gilded and decorated, have a tall, tiered design symbolizing Mount Meru surrounded by \"devata\" (angels) and \"nāga\". The chariots are topped with a \"butsabok\" (open-sided roofed structure) which houses the royal funerary urn containing the royal body. Despite the term used, the \"chariots\" have four wheels and are pulled by hundreds of men.\n\nThe Maha Phichai Ratcharot was used for the funeral procession of King Bhumibol Adulyadej in October 2017. It was drawn by 216 men. The 13.7 tonne chariot is 18 metres long, 11.2 metres high, and 4.8 metres wide. It has been used 25 times, most recently in 2012 for the cremation of Princess Bejaratana Rajasuda, the late king's cousin.\n\nThe royal chariots are housed in the Bangkok National Museum and undergo restoration when they are needed for royal funerals. Following custom, The doorsill of the exhibition hall and a section of the museum wall are demolished and a path is laid each time the chariots need to be brought outside. Then the structures are rebuilt and the path removed after the ceremony ends, signifying that their future use is not anticipated.\n\nAn additional chariot, the Rajarot Puen Yai (\"royal gun carriage\"), is used only for the cremations of the King in his constitutional duty as Head of the Royal Thai Armed Forces (a tradition started by King Vajiravudh on his request to recognize the royal patronage and support for the armed services of the Kingdom, used for the first time in his state cremation in 1926) as the urn carrying his remains is carried in procession around the royal crematorium at the Sanam Luang Royal Plaza thrice before the cremation services begin. The carriage is modeled on those used in the royal and state funerals in the United Kingdom, and was restored yet again in 2017.\n\n"}
{"id": "43416959", "url": "https://en.wikipedia.org/wiki?curid=43416959", "title": "Rural Women Energy Security (RUWES)", "text": "Rural Women Energy Security (RUWES)\n\nRural Women Energy Security (RUWES) is a project of the Renewable Energy Programme, launched by the Ministry of Environment on December 10, 2013.\n\nThe RUWES project aims at providing rural women with access to clean energy. This goes hand in hand with educational endeavors.\n\nMore than 1.36 million Nigerian women have registered under the RUWES project, including the Nigeria Market Women Association, the Federation of Muslim Women, the Catholic Women Organization and the Police Officers Wives among others.\n"}
{"id": "13139890", "url": "https://en.wikipedia.org/wiki?curid=13139890", "title": "Saturation vapor curve", "text": "Saturation vapor curve\n\nThe saturation vapor curve is the curve separating the two-phase state and the superheated vapor state in the T-s diagram. The \"saturated liquid curve\" is the curve separating the subcooled liquid state and the two-phase state in the T-s diagram.\n\nWhen used in a power cycle, the fluid expansion depends strongly on the nature of this saturation curve:\n\n\n\n"}
{"id": "19621061", "url": "https://en.wikipedia.org/wiki?curid=19621061", "title": "Shrouded tidal turbine", "text": "Shrouded tidal turbine\n\nThe shrouded tidal turbine is an emerging tidal stream technology that has a turbine enclosed in a venturi shaped shroud or duct (ventuduct), producing a sub atmosphere of low pressure behind the turbine. The venturi shrouded turbine is not subject to the Betz limit and allows the turbine to operate at higher efficiencies than the turbine alone by increasing the volume of the flow over the turbine. Claimed improvements vary, from 1.15 to 4 times higher power output than the same turbine minus the shroud. The Betz limit of 59.3% conversion efficiency for a turbine in an open flow still applies, but is applied to the much larger shroud cross-section rather than the small turbine cross-section.\n\nConsiderable commercial interest has been shown in shrouded tidal stream turbines due to the increased power output. They can operate in shallower slower moving water with a smaller turbine at sites where large turbines are restricted. Arrayed across a seaway or in fast flowing rivers, shrouded turbines are cabled to shore for connection to a grid or a community. Alternatively the property of the shroud that produces an accelerated flow velocity across the turbine allows tidal flows formerly too slow for commercial use to be used for energy production.\n\nWhile the shroud may not be practical in wind, as the next generation of tidal stream turbine design it is gaining more popularity and commercial use. The Tidal Energy Pty Ltd tidal turbine is multidirectional able to face up-stream in any direction and the Lunar Energy turbine bi directional. All tidal stream turbines constantly need to face at the correct angle to the water stream in order to operate. The Tidal Energy Pty Ltd is a unique case with a pivoting base. Lunar Energy use a wide angle diffuser to capture incoming flow that may not be inline with the long axis of the turbine. A shroud can also be built into a tidal fence or barrage increasing the performance of turbines.\n\nNot all shrouded turbines are the same - the performance of a shrouded turbine varies with the design of the shroud. Not all shrouded turbines have undergone independent scrutiny of claimed performances, as companies closely guard their respective technologies, so quoted performance figures need to be closely scrutinised. Lunar Energy reports a 15%-25% improvement over the same turbine without the shroud. Shrouded turbines do not operate at maximum efficiency when the shroud does not intercept the current flow at the correct angle, which can occur as currents eddy and swirl, resulting in reduced operational efficiency. At lower turbine efficiencies the extra cost of the shroud must be justified, while at higher efficiencies the extra cost of the shroud has less impact on commercial returns. Similarly the added cost of the supporting structure for the shroud has to be balanced against the performance gained. Yawing (pivoting) the shroud and turbine at the correct angle, so it always faces upstream like a wind sock, can increase turbine performance but may need expensive active devices to turn the shroud into the flow. Passive designs can be incorporated, such as floating the shrouded turbine under a pontoon on a swing mooring, or flying the turbine like a kite under water. One design by Tidal Energy Pty Ltd passively yaws the shrouded turbine using a turntable with a peer reviewed claim of 3.84 (384%) increase in efficiency over the same turbine minus the shroud - See Kirke..\n\n"}
{"id": "44275839", "url": "https://en.wikipedia.org/wiki?curid=44275839", "title": "Sludge", "text": "Sludge\n\nSludge is a semi-solid slurry that can be produced from a range of industrial processes, from water treatment, wastewater treatment or on-site sanitation systems. For example, it can be produced as a settled suspension obtained from conventional drinking water treatment, as sewage sludge from wastewater treatment processes or as fecal sludge from pit latrines and septic tanks. The term is also sometimes used as a generic term for solids separated from suspension in a liquid; this 'soupy' material usually contains significant quantities of 'interstitial' water (between the solid particles).\n\nIndustrial wastewater treatment plants produce solids that are also referred to as sludge. This can be generated from biological or physical-chemical processes.\n\nIn the activated sludge process for wastewater treatment, the terms \"waste activated sludge\" and \"return activated sludge\" are used.\n"}
{"id": "46424265", "url": "https://en.wikipedia.org/wiki?curid=46424265", "title": "Solar-wind hybrid turbine", "text": "Solar-wind hybrid turbine\n\nMore and more research is being put into Solar-wind hybrid turbines, since predicting either wind or sun can be difficult. Many different designs are being tested to determine if there is an efficient way to combine the two. Installing solar panels on the already existing wind turbines was tested, but produced blinding rays of light that posed a threat to airplanes. These beams also had the potential to cause fires if concentrated. The solution to this was produce tinted solar panels that do not reflect as much light. Another proposed design was to have a vertical axis wind turbine coated in solar cells that are able to absorb sunlight from any angle. While these designs are still being tested, scientists are having success, and these new hybrid units are a very promising project.\n"}
{"id": "34629497", "url": "https://en.wikipedia.org/wiki?curid=34629497", "title": "Solar energy use in rural Africa", "text": "Solar energy use in rural Africa\n\nThe use of solar energy in rural areas across sub-Saharan Africa has increased over the years. With many communities lacking access to basic necessities such as electricity, clean water, and effective irrigation systems; the innovations in solar powered technologies have led to poverty alleviation projects that combine development strategies and environmental consciousness. Another use for solar energy that has gained momentum in rural African households (as well as some urban areas) is that of solar cooking. Historically, the high dependency on wood collection from depleting sources have resulted in serious environmental degradation and has been considered an extremely unsustainable practice when compared to the renewable attribute of solar powered cooking. There have also been recent links made between solar energy and increased food security in the region. African development projects, mostly in rural areas seem to be recognizing the real potential of renewable energy sources especially power derived from the sun.\n\nThe article by Hilde M. Toonen (2009) details the efforts carried out by the SUPO (Stichting voor Urbane Projecten in Ontwikkelingslanden) foundation that was established in 1977; when they began a solar cooking project in 2005 in the urban households of the Burkina Faso city of Ouagadougou: PESGO (Programme Energie Solaire Grand-Ouaga). The technology used was that of CooKit which is a cardboard panel cooker covered with aluminum foil. Sunrays are reflected towards a black pot which is placed in a thermo-resistant plastic bag. Temperatures from 70 _C to 90_C (160 F and 200 F) can be reached. The cardboard is foldable and weighs only 500 g (1 lb.), it is therefore easily stored. If the CooKit is kept dry and away from termites, the CooKit may last for several years. Considering its durability, the CooKit seems to be a good investment: the purchase costs are lower than the money people spend on firewood. The manufacturing of the CooKit is not difficult. Solar Cookers International published a construction manual (SCI, 2007c). A CooKit can be made in one or two hours and materials needed are cardboard, aluminum foil and non-toxic, water based glue (SCI, 2007c) (see Fig. 1). (Toonen, 2009).\n\nFig. 1 The CooKit\n\nAs mentioned above the CooKit aims to reduce the high dependency on firewood and charcoal for cooking purposes that proves to not only negatively affect the environment; but also put a strain on the finances of the individual households. However the researchers involved in the SUPO foundation quickly realized that CooKit alone could not be as effective in replacing firewood; and that the use of a special plant oil extracted from the drought-resistant Jatropha plant would be the most complementary component to aid in the cooking process as a fuel substitute. The process of extraction is also very straightforward where an individual just needs to squeeze the plant to get the oil. According to SUPA the main reason for using Jatropha oil along with the CooKit is due to the unreliability of weather conditions; however there have not yet been any developments in creating an inexpensive stove to be used with the Jatropha oil but that a one-flame cooker is simply a prototype at this stage.\nThe CooKit example shared here is only one adaptation to solar cooking technology and that further research reveals other innovations such as the Solar Fryer (Gallagher, 2011) and the original Solar Box Oven. Evidence has shown that although the main setbacks to solar cooking are the longer time it takes to prepare meals for families and that the dependency on favorable weather conditions means that one cannot use solar energy every day; it is a step in the right direction as it can at least alleviate the pressure currently being placed on the remaining scarce firewood resources.\n\nPurified water is a big issue facing many communities in the developing world in particular. Those in rural areas are usually too isolated for on-grid government-funded water pipe infrastructure to be built; and so the responsibility of getting clean water becomes that of the women and their children in the villages who have to walk long distances to water sources that are not necessarily the purest.\n\nIn the article by Sambwa et al. (2009), the authors highlight these issues and propose the integration of DC (Direct Current) Motors into solar powered water pumping technology. This is usually referred to as ‘Technology Transfer’ that the authors argue is a development concept, [that has been] conceived by the politicians and the general public in sub-Saharan Africa as the ability to purchase or acquire technological equipment. Coupled with ‘‘globalization and economic liberation’’, this trend has become contagious to the point that any segment of unserviceable technological equipment finds its way into the sub-region…They are grouped as: vehicles, house hold machinery, industrial equipment, and many more. The authors have identified these unserviceable equipment as an inestimable source of raw materials where DC motors have been extracted (recovered) for the purpose of being reconfigured as DC motors for driving water pumps. (Sambwa et al., 2009). The pump itself can be retrieved from washing machines or radiators of the generating set engines. Figure 2 below shows the end product of the DC Motor Drive Water Pump before it has been connected to the solar panels.\n\nFig. 2 DC motor drive water pump\n\nHowever one of the main setbacks of relying on used imported technologies is that they prove to be problematic to local engineers and technicians as most of them have already worked for many years before being exported to the continent. (Sambwa et al., 2009).\nThe project proved to be successful as it was able to pump water from a 10m deep water reservoir; but in order to fund future projects costs would have to be covered by external sources. In spite of the higher production costs, the overall benefit of utilizing this technology outweighs the proposed setbacks. And due to the relatively simplistic model, maintenance work that would arise in the future can be dealt with by the local technicians.\n\nIn the article by Burney et al. (2010) another use for solar energy that has been proposed is the Photovoltaic- (or solar-) powered drip irrigation (PVDI) system [which] combines the efficiency of drip irrigation with the reliability of a solar-powered water pump… [Where the] PV array powers a pump (either surface or submersible, depending on the water source) that feeds water to a reservoir. The reservoir then gravity-distributes the water to a low-pressure drip irrigation system. No batteries are used in the system: The pump only runs during the daytime, and energy storage is in the height of the column of water in the reservoir.\nAn important technological advancement for agricultural practices in the region that is related to increasing food security; the PVDI systems were integrated into preexisting local women’s agricultural groups in the Kalalé District of Northern Benin in November 2007. The PVDI systems were conceived, financed, and installed by an NGO, the Solar Electric Light Fund, to boost vegetable production from communal gardens in an effort to combat high malnutrition and poverty levels in the region. (Burney et al., 2010).\n"}
{"id": "20596845", "url": "https://en.wikipedia.org/wiki?curid=20596845", "title": "Tetraphenylborate", "text": "Tetraphenylborate\n\nTetraphenylborate (IUPAC name: Tetraphenylboranuide) is an organoboron anion consisting of a central boron atom with four phenyl groups. Salts of tetraphenylborate uncouple oxidative phosphorylation.\n\n"}
{"id": "39194925", "url": "https://en.wikipedia.org/wiki?curid=39194925", "title": "Thin Ice (2013 film)", "text": "Thin Ice (2013 film)\n\nThin Ice is a 2013 documentary film following geologist Simon Lamb on a search to understand the science behind climate change. This is achieved by traveling the world and meeting a range of scientists, from biologists to physicists, who are investigating the climate. The film's conclusion emphasises the scientific consensus on human-induced climate change.\n\nThe film was a joint initiative between Oxford University and Victoria University of Wellington, and premiered around the world on Earth Day 2012, and in New Zealand in 2015.\n\n"}
{"id": "52356704", "url": "https://en.wikipedia.org/wiki?curid=52356704", "title": "Wooden Toys of Varanasi", "text": "Wooden Toys of Varanasi\n\nWooden toy-making is a traditional craft in the Varanasi district of Uttar Pradesh, India. Bright and colourful lacquered toys are made by clusters of skilled craftsmen. These toys were given the Geographical Indication tag in 2014, along with other lacquer ware produced in this region.\n\nAccording to the craftsmen, their ancestors specialised in ivory carving that enjoyed good patronage during the reign of the Mughal emperors and the British. After ivory was banned by the Government of India, they shifted to woodcarving.\n\nWooden logs are sourced from nearby areas such as the jungles of Chitrakoot and Sonbhadra. Keria (Coraiya) wood from Bihar was used earlier and it is still the preferred type of wood for toy-making. But after the government banned the cutting of Keria trees in the 1980s, the craftsmen switched to using the wood from eucalyptus, which is now used predominantly. Stacks of wooden logs can be seen stored outside the houses of the wood-carvers.\n\nPieces of wood are cut out from the logs according to the size of the toy that is to be made. Each piece is heated slowly to remove all the moisture from the wood. This process is time-consuming. The piece is sanded in order to smoothen its surface.\n\nThe wood is either hand-carved or shaped using lathe. Lathe is preferred for toys that are axially symmetric. In hand-carving, first the design of the toy is drawn on the wood. Then, the wood is sculpted with chisel and hammer according to the design. Once shaped, the surface of the toy is smoothened using a file and the toy is sent for painting.\n\nToys are given several coats of paint, and finished with a clear or coloured lacquer. Paint brushes made out of the hair from squirrel’s tail are used for painting fine lines.\n\nLacquering may also be done on a lathe. For slender and delicate items, hand-lathe is preferred. A lac stick is pressed against the toy which is fixed to the lathe. While the toy keeps revolving, the heat generated by friction softens the lac, making it stick to the toy.\n\nArtisan groups from the region applied for the GI tag in September 2013 and their request was granted in November 2014.\n"}
