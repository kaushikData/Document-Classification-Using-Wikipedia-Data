{"id": "51635031", "url": "https://en.wikipedia.org/wiki?curid=51635031", "title": "3-Nitrooxypropanol", "text": "3-Nitrooxypropanol\n\n3-Nitrooxypropanol, abbreviated 3NOP, is an organic compound with the formula HOCHCHCHONO. It is the mononitrate ester of 1,3-propanediol. The compound is an inhibitor of the enzyme methyl coenzyme M reductase (MCR). MCR catalyzes the final step in methanogenesis. When it is fed to ruminants, their methane production is diminished and they gain weight. Ruminants are some of the greatest contributors of methane, a significant greenhouse gas.\n"}
{"id": "33596099", "url": "https://en.wikipedia.org/wiki?curid=33596099", "title": "Acorn Energy", "text": "Acorn Energy\n\nAcorn Energy Inc. is a NASDAQ-listed conglomerate investing in electricity generation and security. It was founded by George Morgenstern in 1984 as Decision Systems, Inc. and was taken public by Laidlaw Securities. The name was later changed to Data Systems and Software Inc. John Moore became CEO in 2006 and changed the name to Acorn Factor and later to Acorn Energy.\n\nDSIT, based in Israel, makes systems of sonar arrays that detect underwater activity; these are marketed by Acorn Energy for the protection of energy infrastructure facilities near water. GridSense makes hardware for monitoring transformer performance for smart grid applications. OmniMetrix monitors critical energy infrastructure like standby generators, compressors and pump jacks to ensure up-time. US Seismic Systems manufactures fibre-optic transducers for monitoring oil and gas fields during production.\n"}
{"id": "34684547", "url": "https://en.wikipedia.org/wiki?curid=34684547", "title": "Avis Acres", "text": "Avis Acres\n\nThyra Avis Mary Acres (née McNeill, 26 March 1910 – 15 October 1994) was a New Zealand artist, writer, illustrator and conservationist. She was born in Wellington, New Zealand, on 26 March 1910.\n\nAcres is best known for her comic strip about two pohutukawa fairies, \"Hutu and Kawa\", which was published weekly on the children's page of the \"New Zealand Herald\" from September 1950 to July 1960, and also was published as three books by A.H. & A.W. Reed.\n"}
{"id": "1159033", "url": "https://en.wikipedia.org/wiki?curid=1159033", "title": "Boule (crystal)", "text": "Boule (crystal)\n\nA boule is a single crystal ingot produced by synthetic means.\n\nA boule of silicon is the starting material for most of the integrated circuits used today. In the semiconductor industry synthetic boules can be made by a number of methods, such as the Bridgman technique and the Czochralski process, which result in a cylindrical rod of material.\n\nIn the Czochralski process a seed crystal is required to create a larger crystal, or ingot. This seed crystal is dipped into the pure molten silicon and slowly extracted. The molten silicon grows on the seed crystal in a crystalline fashion. As the seed is extracted the silicon solidifies and eventually a large, cylindrical boule is produced.\n\nA semiconductor crystal boule is normally cut into circular wafers using an inside hole diamond saw or diamond wire saw, and each wafer is lapped and polished to provide substrates suitable for the fabrication of semiconductor devices on its surface.\n\nThe process is also used to create sapphires, which are used for substrates in the production of blue and white LEDs, optical windows in special applications and as the protective covers for watches.\n"}
{"id": "29640853", "url": "https://en.wikipedia.org/wiki?curid=29640853", "title": "Carbotanium", "text": "Carbotanium\n\nCarbotanium is a patented composite material invented by Modena Design, the carbon composite manufacturing and consultancy arm of the Italian car company Pagani. It is a combination of beta titanium alloy with advanced carbon composites, having a matched yield strength and moduli of elasticity ratio. When the combination is adhesively bonded both parts will approach maximum yield strength and fail at a similar amount of total strain. The titanium and carbon composites are combined by first abrading the titanium to be bonded, coating the titanium with platinum, aging the titanium, spraying primer on the coated titanium, applying adhesive to the primer side of the titanium and then applying the carbon to the adhesive. This allows the carbon composite to bond securely to the titanium. This composite uses the best properties of each component, the combination having a better set of properties than either part. This material is both strong and light.\n\nPagani has applied this weave on their latter extra strong and lightweight supercars, the Zonda R and Huayra. Although not all Zondas are made from carbotanium, some are formed from carbon fibre. They have also used other variations of metal/composites.\n\n"}
{"id": "24962334", "url": "https://en.wikipedia.org/wiki?curid=24962334", "title": "Chicago Pile-3", "text": "Chicago Pile-3\n\nChicago Pile-3 (CP-3) was the first heavy water reactor in the world, going critical on 15 May 1944. It was used in the experimental physics work of the Metallurgical Laboratory for the Manhattan Project.\n\nCP-3 was in operation from 1943–54 and was built near Palos Hills, Illinois on the former (i.e., original) site of Argonne National Laboratory. The site is now known as the Site A/Plot M Disposal Site and is situated within Red Gate Woods, part of the Cook County Forest Preserve system.\n\nCP-3 was initially fueled with natural uranium and used heavy water as a neutron moderator. In January 1950, the reactor was dismantled due to suspicion of corrosion of the aluminum cladding that surrounded the control rods. The reactor was rebuilt and redesignated CP-3′ (CP-3 prime). It was restarted in May 1950 and operated until 1954. The reactor was authorized to operate up to 300 kilowatts.\n\nThe two versions of the reactor were used in physics studies, fission product separations, tritium recovery from irradiated lithium, and studies of radionuclide metabolism in laboratory animals. After the reactor was decommissioned, the fuel and heavy water were shipped to the Oak Ridge National Laboratory. The containment shell for the reactor had pipes, valves, and building debris placed inside and was then filled with concrete. The 800-ton shell was buried on the site in a deep pit. Today, a historical marker commemorates the site of CP-3 and its sister reactor CP-1/CP-2.\n\n"}
{"id": "40471625", "url": "https://en.wikipedia.org/wiki?curid=40471625", "title": "Classical diffusion", "text": "Classical diffusion\n\nClassical diffusion is a key concept in fusion power and other fields where a plasma is confined by a magnetic field. It considers collisions between ions in the plasma that cause the particles to move to different paths and eventually leave the confinement volume. It scales with 1/B, where B is the magnetic field strength, implies that confinement times can be greatly improved with small increases in field strength. In practice, the rates suggested by classical diffusion have not been found in real-world machines.\n\nDiffusion is a random walk process that can be quantified by the two key parameters: Δx, the step size, and Δt, the time interval when the walker takes a step. Thus, the diffusion coefficient is defined as D≡(Δx)/(Δt).\n\nWhen an ion is placed in a magnetic field, it will orbit the field lines while continuing to move along that line with whatever initial velocity it had. This produces a helical path through space. Since the axial velocities will have a range of values, often based on the Maxwell-Boltzmann statistics, this means the particles in the plasma will pass by others as they overtake them or are overtaken.\n\nIf one considers two such ions travelling along parallel axial paths, they can collide whenever their orbits intersect. In most geometries, this means there is a significant difference in the instantaneous velocities when the collide - one might be going \"up\" while the other would be going \"down\" in their helical paths. This causes the collisions to scatter the particles, making them random walks. Eventually, this process will cause any given ion to eventually leave the boundary of the field, and thereby escape \"confinement\".\n\nIn a uniform magnetic field, a particle undergoes random walk across the field lines by the step size of gyroradius ρ≡v/Ω, where v denotes the thermal velocity, and Ω≡qB/m, the gyrofrequency. The steps are randomized by the collisions to lose the coherence. Thus, the time step, or the decoherence time, is inverse of the collisional frequency ν. The rate of diffusion is given by νρ, with the rather favorable B scaling law.\n\nWhen the topic of controlled fusion was first being studied, it was believed that the plasmas would follow the classical diffusion rate, and this suggested that useful confinement times would be relatively easy to achieve.\n\nHowever, in 1949 a team studying plasma arcs as a method of isotope separation found that the diffusion time was much greater than what was predicted by the classical method. David Bohm suggested it scaled with B. If this is true, Bohm diffusion would mean that useful confinement times would require impossibly large fields.\n\nIn practice, machines have demonstrated a wide array of diffusion rates between these two extremes.\n\n"}
{"id": "33371191", "url": "https://en.wikipedia.org/wiki?curid=33371191", "title": "Climate Capitalism", "text": "Climate Capitalism\n\nClimate Capitalism: Capitalism in the Age of Climate Change is a 2011 book by L. Hunter Lovins and Boyd Cohen. It presents positive stories and examples of how profit-seeking companies are helping to save the planet, and says that \"the best way to rebuild America’s economy, cities and job markets is to invest in energy efficiency and renewable energy resources, whether climate change is happening or not\". However, reviewer Gail Whiteman is unconvinced by the argument that naked greed and market forces will drive businesses to cut their greenhouse gas emissions.\n\n"}
{"id": "40977477", "url": "https://en.wikipedia.org/wiki?curid=40977477", "title": "Cross-species transmission", "text": "Cross-species transmission\n\nCross-species transmission, (CST) or spillover, is the ability for a foreign virus, once introduced into an individual of a new host species, to infect that individual and spread throughout a new host population. Steps involved in the transfer of viruses to new hosts include contact between the virus and the host, infection of an initial individual leading to amplification and an outbreak, and the generation within the original or new host of viral variants that have the ability to spread efficiently between individuals in populations of the new host Often seen in emerging viruses where one species transfers to another, which in turn transfers to humans. Examples include HIV-AIDS, SARS, ebola, swine flu, rabies, and avian influenza. Bacterial pathogens can also be associated with CST.\n\nThe exact mechanism that facilitates transfer is unknown, however, it is believed that viruses with a rapid mutation rate are able to overcome host-specific immunological defenses. This can occur between species that have high contact rates. It can also occur between species with low contact rates but usually through an intermediary species. Bats, for example, are mammals and can directly transfer rabies to humans through bite and also through aerosolization of bat saliva and urine which are then absorbed by human mucous membranes in the nose, mouth and eyes. A host shifting event is defined as a strain that was previously zoonotic and now circulates exclusively among humans.\n\nSimilarity between species, for example, transfer between mammals, is believed to be facilitated by similar immunological defenses. Other factors include geographic area, intraspecies behaviours, and phylogenetic relatedness. Virus emergence relies on two factors: initial infection and sustained transmission.\n\nCross-species transmission is the most significant cause of disease emergence in humans and other species. Wildlife zoonotic diseases of microbial origin are also the most common group of human emerging diseases, and CST between wildlife and livestock has appreciable economic impacts in agriculture by reducing livestock productivity and imposing export restrictions. This makes CST of major concern for public health, agriculture, and wildlife management.\n\nA large proportion of viral pathogens that have emerged recently in humans are considered to have originated from various animal species. This is shown by several recent epidemics such as, avian flu, Ebola, monkey pox, and Hanta viruses. There is evidence to suggest that some diseases can potentially be re-introduced to human populations through animal hosts after they have been eradicated in humans. There is a risk of this phenomenon occurring with morbilliviruses as they can readily cross species barriers. CST can also have a significant effect on produce industries. Genotype VI-Avian paramyxovirus serotype 1 (GVI-PMV1) is a virus that arose through cross-species transmission events from Galliformes (i.e. chicken) to Columbiformes, and has become prevalent in the poultry industry. CST of rabies virus variants between many different species populations is a major wildlife management concern. Introduction of these variants into non-reservoir animals increases the risk of human exposures and threatens current advances toward rabies control.\n\nMany pathogens are thought to have host specialization, which explains the maintenance of distinct strains in host species. Pathogens would have to overcome their host specificity to cross to a new host species. Some studies have argued that host specializations may be exaggerated, and pathogens are more likely to exhibit CST than previously thought. Original hosts usually have low death rates when infected with a pathogen, with fatality rates tending to be much higher in new hosts\n\nDue to the close relation of humans and nonhuman primates (NHP), disease transmission between NHP and humans is relatively common and can become a major public health concern. Diseases, such as HIV and human adenoviruses have been associated with NHP interactions. \nIn places where contact between humans and NHPs is frequent, precautions are often taken to prevent disease transmission. Simian foamy viruses (SFV) is an enzootic retrovirus that has high rates of cross-species transmission and has been known to affect humans bitten by infected NHPs. It has caused health concerns in places like Indonesia where visitors at monkey temples can contract SFV from temple macaques (\"Macaca fascicularis\"). TMAdV (titi monkey adenovirus) is a highly divergent, sharing <57% pairwise nucleotide identity with other adenoviruses, NHP virus that had a high fatality rate (83%) in monkeys and is capable of spreading through human hosts.\n\nPrediction and monitoring are important for the study of CSTs and their effects. However, factors that determine the origin and fate of cross-species transmission events remain unclear for the majority of human pathogens. This has resulted in the use of different statistical models for the analyzation of CST. Some of these include risk-analysis models, single rate dated tip (SRDT) models, and phylogenetic diffusion models. The study of the genomes of pathogens involved in CST events is very useful in determining their origin and fate. This is because a pathogens genetic diversity and mutation rate are key factors in determining if it is able to transmit across multiple hosts. This makes it important for the genomes of transmission species to be partially or completely sequenced. A change in genomic structure could cause a pathogen that has narrow host range to become capable of exploiting a wider host range. Genetic distance between different species, geographical range, and other interaction barriers will also influence cross-species transmission.\n\nOne approach to risk assessment analysis of CST is to develop risk-analysis models that break the ‘‘process’’ of disease transmission into component parts. Processes and interactions that could lead to cross-species disease transmission are explicitly described as a hypothetical infection chain. Data from laboratory and field experiments is used to estimate the probability of each component, expected natural variation, and margins of error.\n\nDifferent types of CST research would require different analysis pathways to meet their needs. A study on identification of viruses in bats that could spread to other mammals used the workflow: sequencing of genomic samples → “cleaning” of raw reads → elimination of host reads and eukaryotic contaminants → de novo assembly of the remaining reads → annotation of viral contigs → molecular detection of specific viruses → phylogenetic analysis → interpretation of data.\n\nDetecting CST and estimating its rate based on prevalence data is challenging. Due to these difficulties computational methods are used to analyse CST events and the pathogens associated with them. The explosive development of molecular techniques has opened new possibilities for using phylogenetic analysis of pathogen genetics to infer epidemiological parameters. This provides some insight into the origins of these events and how they could be addressed. Methods of CST prevention are currently using both biological and computational data. An example of this is using both cellular assays and phylogenetic comparisons to support a role for TRIM5α, the product of the TRIM5 gene, in suppressing interspecies transmission and emergence of retroviruses in nature.\n\nThe comparison of genomic data is very important for the study of cross-species transmission. Phylogenetic analysis is used to compare genetic variation in both pathogens associated with CST and the host species that they infect. Taken together, it is possible to infer what allowed a pathogen to crossover to a new host (i.e. mutation in a pathogen, change in host susceptibility) and how this can be prevented in the future. If the mechanisms a pathogens uses to initially enter a new species are well characterized and understood a certain level of risk control and prevention can be obtained. In contact, a poor understanding of pathogens, and their associated diseases, makes it harder for preventive measures to be taken\n\nAlternative hosts can also potentially have a critical role in the evolution and diffusion of a pathogen. When a pathogen crosses species it often acquires new characteristics that allow it to breach host barriers. Different pathogen variants can have very different effects on host species. Thus it can be beneficial to CST analysis to compare the same pathogens occurring in different host species. Phylogenetic analysis can be used to track a pathogens history through different species populations. Even if a pathogen is new and highly divergent, phylogenetic comparison can be very insightful A useful strategy for investigating the history of epidemics caused by pathogen transmission combines molecular clock analysis, to estimate the timescale of the epidemic, and coalescent theory, to infer the demographic history of the pathogen.\nWhen constructing phylogenies, computer databases and tools are often used. Programs, such as BLAST, are used to annotate pathogen sequences, while databases like GenBank provide information about functions based on the pathogens genomic structure. Trees are constructed using computational methods such as MPR or Bayesian Inference, and models are created depending on the needs of the study. Single rate dated tip (SRDT) models, for example, allows for estimates of timescale under a phylogenetic tree. Models for CST prediction will vary depending on what parameters need to be accounted for when constructing the model.\n\nParsimony is the principle in which one chooses the simplest scientific explanation that fits the evidence. In terms of building phylogenetic trees, the best hypothesis is the one that requires the fewest evolutionary changes. Using parsimony to reconstruct ancestral character states on a phylogenetic tree is a method for testing ecological and evolutionary hypotheses. This method can be used in CST studies to estimate the number of character changes that exist between pathogens in relation to their host. This makes MPR useful for tracking a CST pathogen to its origins. MPR can also be used to the compare traits of host species populations. Traits and behaviours within a population could make them more susceptible to CST. For example, species which migrate regionally are important for spreading viruses through population networks.\n\nDespite the success of parsimony reconstructions, research suggests they are often sensitive and can sometimes be prone to bias in complex models. This can cause problems for CST models that have to consider many variables. Alternatives methods, such as maximum likelihood, have been developed as an alternative to parsimony reconstruction.\n\nTwo methods of measuring genetic variation, variable number tandem repeats (VNTRs) and single nucleotide polymorphisms (SNPs), have been very beneficial to the study of bacterial transmission. VNTRs, due to the low cost and high mutation rates, make them particularly useful to detect genetic differences in recent outbreaks, and while SNPs have a lower mutation rate per locus than VNTRs, they deliver more stable and reliable genetic relationships between isolates. Both methods are used to construct phylogenies for genetic analysis, however, SNPs are more suitable for studies on phylogenies contraction.\nHowever, it can be difficult for these methods accurately simulate CSTs everts. Estimates of CST based on phylogenys made using VNTR marker can be biased towards detecting CST events across a wide range of the parameters. SNPs tend to be less biased and variable in estimates of CST when estimations of CST rates are low and low number of SNPs is used. In general, CST rate estimates using these methods are most reliable in systems with more mutations, more markers, and high genetic differences between introduced strains. CST is very complex and models need to account for a lot of parameters to accurately represent the phenomena. Models that oversimplify reality can result in biased data. Multiple parameters such as number of mutations accumulated since introduction, stochasticity, the genetic difference of strains introduced, and the sampling effort can make unbiased estimates of CST difficult even with whole-genome sequences, especially if sampling is limited, mutation rates are low, or if pathogens were recently introduced. More information on the factors that influence CST rates is needed for the contraction of more appropriate models to study these events.\n\nThe process of using genetic markers to estimate CST rates should take into account several important factors to reduce bias. One, is that the phylogenetic tree constructed in the analysis needs to capture the underlying epidemiological process generating the tree. The models need to account for how the genetic variability of a pathogen influences a disease in a species, not just general differences in genomic structure. Two, the strength of the analysis will depend on the amount of mutation accumulated since the pathogen was introduced in the system. This is due to many models using amount of mutations as an indicator of CST frequency. Therefore, efforts are focused on estimating either time since introduction or the substitution rate of the marker (from laboratory experiments or genomic comparative analysis). This is important not only when using the MPR method but also for Likelihood approaches that require an estimation of the mutation rate. Three, CST will also affect disease prevalence in the potential host, so combining both epidemiological time series data with genetic data may be an excellent approach to CST study\n\nBayesian frameworks are a form of maximum likelihood-based analyses and can be very effective in cross-species transmission studies. Bayesian inference of character evolution methods can account for phylogenetic tree uncertainty and more complex scenarios, with models such as the character diffusion model currently being developed for the study of CST in RNA viruses. A Bayesian statistical approach presents advantages over other analyses for tracking CST origins. Computational techniques allow integration over an unknown phylogeny, which cannot be directly observed, and unknown migration process, which is usually poorly understood.\n\nThe Bayesian frameworks are also wellsuited to bring together different kinds of information. The BEAST software, which has a strong focus on calibrated phylogenies and genealogies, illustrates this by offering a large number of complementary evolutionary models including substitution models, demographic and relaxed clock models that can be combined into a full probabilistic model. By adding spatial reconstruction, these models create the probability of biogeographical history reconstruction from genetic data. This could be useful for determining origins of cross-species transmissions. \nThe high effectiveness of Bayesian statistical methods has made them instrumental in evolutionary studies. Bayesian ancestral host reconstruction under discrete diffusion models can be used to infer the origin and effects of pathogens associated with CST. One study on Human adenoviruses using Bayesian supported a gorilla and chimpanzee origin for the viral species, aiding prevention efforts. Despite presumably rare direct contact between sympatric populations of the two species, CST events can occur between them. The study also determined that two independent HAdV-B transmission events to humans occurred and that the HAdV-Bs circulating in humans are of zoonotic origin and have probably affected global health for most of our species lifetime.\n\nPhylogenetic diffusion models are frequently used for phylogeographic analyses, with the inference of host jumping becoming of increasing interest. The Bayesian inference approach enables model averaging over a number of potential diffusion predictors and estimates the support and contribution of each predictor while marginalizing over phylogenetic history. For studying viral CST, the flexibility of the Bayesian statistical framework allows for the reconstruction of virus transmission between different host species while simultaneously testing and quantifying the contribution of multiple ecological and evolutionary influences of both CST spillover and host shifting. One study on rabies in bats showed geographical range overlap is a modest predictor for CST, but not for host shifts. This highlights how Bayesian inferences in models can be used for CST analysis.\n\n\n"}
{"id": "2842753", "url": "https://en.wikipedia.org/wiki?curid=2842753", "title": "Cutoff (steam engine)", "text": "Cutoff (steam engine)\n\nIn a steam engine, cutoff is the point in the piston stroke at which the inlet valve is closed. On a steam locomotive, the cutoff is controlled by the reversing gear.\n\nThe point at which the inlet valve closes and stops the entry of steam into the cylinder from the boiler plays a crucial role in the control of a steam engine. Once the valve has closed, steam trapped in the cylinder expands adiabatically. The steam pressure drops as it expands. A late cutoff delivers full steam pressure to move the piston through its entire stroke, for maximum start-up forces. But, since there will still be unexploited pressure in the cylinder at the end of the stroke, this is achieved at the expense of engine efficiency. In this situation the steam will still have considerable pressure remaining when it is exhausted resulting in the characteristic “chuff chuff” sound of a steam engine. An early cutoff has greater thermodynamic efficiency but results in a lower Mean effective pressure so less average force on the piston and is used for running the engine at higher speeds. The steam engine is the only thermodynamic engine design that can provide its maximum torque at zero revolutions per minute.\n\n Cutoff is one of the four valve events. Early cutoff is used to increase the efficiency of the engine by allowing the steam to expand for the rest of the power stroke, yielding more of its energy and conserving steam. This is known as expansive working. Late cutoff is used to provide maximum torque to the shaft at the expense of efficiency and is used to start the engine under load.\n\nCutoff is conventionally expressed as percentage of the power stroke of the piston; if the piston is at a quarter of its stroke at the cutoff point, the cutoff is stated as 25%.\n\nSmaller stationary steam engines generally have a fixed cutoff point while, in large ones, the speed and power output is generally governed by altering the cutoff, frequently under governor control using an expansion valve or trip gear. In steam engines for transport, it is desirable to be able to alter the cutoff over a wide range. For starting and at low speed and heavy load, the cylinders need steam supply at maximum pressure for almost the full length of the stroke. In a two-cylinder locomotive, for example, the maximum or 'full gear' cutoff is typically about 85%. At high speeds, the cutoff may be 15% of the piston stroke or less. Steam engines used in boats and ships operate under a constant, unvarying load through the propeller and so have a fixed cutoff, with speed being controlled through the regulator. \n\nProviding variable cutoff is an important function of the valve gear.\n\n\n"}
{"id": "22938559", "url": "https://en.wikipedia.org/wiki?curid=22938559", "title": "Demolition waste", "text": "Demolition waste\n\nDemolition waste is waste debris from destruction of buildings, roads, bridges, or other structures. Debris varies in composition, but the major components, by weight, in the US include concrete, wood products, asphalt shingles, brick and clay tile, steel, and drywall. There is the potential to recycle many elements of demolition waste. \n\nIn 2014, 505.1 million tons of demolition debris was generated in the US. Out of the 505.1 million tons, the debris was composed of 353.6 million tons of concrete, 76.6 million tons of asphalt concrete, 35.8 million tons of wood product, 12.7 million tons of asphalt shingles, 11.8 million tons of brick and clay tile, 10.3 million tons of drywall and plaster, and 4.3 million tons of steel. \nBefore demolition debris is extracted, contamination from lead, asbestos or different hazardous materials must be resolved. Hazardous materials must be disposed of separately, according to federal regulation. Demolition debris can be disposed of in either Construction and Demolition Debris landfills or municipal solid waste landfills. Alternatively, debris may also be sorted and recycled. Sorting may happen as deconstruction on the demolition site, off-site at a sorting location, or at a Construction and Demolition recycling center. Once sorted, materials are managed separately and recycled accordingly. \n\nConcrete and Brick\n\nConcrete and brick can be recycled by crushing it into rubble. Once sorted, screened and contaminants are removed, reclaimed concrete or brick can be used in concrete aggregate, fill, road base, or riprap. Mobile concrete crushers also allow for recycling of concrete on-site.\n\nWood\n\nWood can be reused, repurposed, recycled, or burned as bioenergy. Reused wood to eliminate the need for full-size new lumber if used for smaller building components. Repurposed or recycled wood can be used in pathways, coverings, mulches, compost, animal bedding, or particleboard.\n\nDrywall\n\nDrywall is made primarily of gypsum. Once the gypsum is depapered, it can be added in cement production, as a soil amendment, used in aerated composting, or recycled into new drywall. Gypsum recycling can be particularly beneficial because in landfill conditions gypsum will release hydrogen sulfide, a toxic gas.\n\nAsphalt\n\nAsphalt, from shingles or asphalt concrete, is typically recycled and used in pavement.\n\nMetal\n\nScrap metal is an established industry focused on the collection, buying, selling, and recycling of salvaged materials. \n\n"}
{"id": "18175045", "url": "https://en.wikipedia.org/wiki?curid=18175045", "title": "Environmental impact of biodiesel", "text": "Environmental impact of biodiesel\n\nThe environmental impact of biodiesel is diverse.\n\nAn often mentioned incentive for using biodiesel is its capacity to lower greenhouse gas emissions compared to those of fossil fuels. Whether this is true or not depends on many factors. A general critic to biodiesel is the land use change, which have potential to cause even more emissions than what would be caused by using fossil fuels alone. Yet this problem would be fixed with algal biofuel which can use land unsuitable for agriculture. \n\nCarbon dioxide is one of the major greenhouse gases. Although the burning of biodiesel produces carbon dioxide emissions similar to those from ordinary fossil fuels, the plant feedstock used in the production absorbs carbon dioxide from the atmosphere when it grows. Plants absorb carbon dioxide through a process known as photosynthesis which allows it to store energy from sunlight in the form of sugars and starches. After the biomass is converted into biodiesel and burned as fuel the energy and carbon is released again. Some of that energy can be used to power an engine while the carbon dioxide is released back into the atmosphere.\n\nWhen considering the total amount of greenhouse gas emissions it is therefore important to consider the whole production process and what indirect effects such production might cause. The effect on carbon dioxide emissions is highly dependent on production methods and the type of feedstock used. Calculating the carbon intensity of biofuels is a complex and inexact process, and is highly dependent on the assumptions made in the calculation. A calculation usually includes:\n\nOther factors can be very significant but are sometimes not considered. These include:\n\nIf land use change is not considered and assuming today's production methods, biodiesel from rapeseed and sunflower oil produce 45%-65% lower greenhouse gas emissions than petrodiesel. However, there is ongoing research to improve the efficiency of the production process. Biodiesel produced from used cooking oil or other waste fat could reduce CO emissions by as much as 85%. As long as the feedstock is grown on existing cropland, land use change has little or no effect on greenhouse gas emissions. However, there is concern that increased feedstock production directly affects the rate of deforestation. Such clearcutting cause carbon stored in the forest, soil and peat layers to be released. The amount of greenhouse gas emissions from deforestation is so large that the benefits from lower emissions (caused by biodiesel use alone) would be negligible for hundreds of years. Biofuel produced from feedstock such as palm oil could therefore cause much higher carbon dioxide emissions than some types of fossil fuels.\n\nIn the United States, biodiesel is the only alternative fuel to have successfully completed the Health Effects Testing requirements (Tier I and Tier II) of the Clean Air Act (1990).\n\nBiodiesel can reduce the direct tailpipe-emission of particulates, small particles of solid combustion products, on vehicles with particulate filters by as much as 20 percent compared with low-sulfur (< 50 ppm) diesel. Particulate emissions as the result of production are reduced by around 50 percent compared with fossil-sourced diesel.\n\nA University of Idaho study compared biodegradation rates of biodiesel, neat vegetable oils, biodiesel and petroleum diesel blends, and neat 2-D diesel fuel. Using low concentrations of the product to be degraded (10 ppm) in nutrient and sewage sludge amended solutions, they demonstrated that biodiesel degraded at the same rate as a dextrose control and 5 times as quickly as petroleum diesel over a period of 28 days, and that biodiesel blends doubled the rate of petroleum diesel degradation through co-metabolism.\nThe same study examined soil degradation using 10 000 ppm of biodiesel and petroleum diesel, and found biodiesel degraded at twice the rate of petroleum diesel in soil. In all cases, it was determined biodiesel also degraded more completely than petroleum diesel, which produced poorly degradable undetermined intermediates. Toxicity studies for the same project demonstrated no mortalities and few toxic effects on rats and rabbits with up to 5000 mg/kg of biodiesel. Petroleum diesel showed no mortalities at the same concentration either, however toxic effects such as hair loss and urinary discolouring were noted with concentrations of >2000 mg/l in rabbits.:\n\nAs biodiesel becomes more widely used, it is important to consider how consumption affects water quality and aquatic ecosystems. Research examining the biodegradability of different biodiesel fuels found that all of the biofuels studied (including Neat Rapeseed oil, Neat Soybean oil, and their modified ester products) were “readily biodegradable” compounds, and had a relatively high biodegradation rate in water. Additionally, the presence of biodiesel can increase the rate of diesel biodegradation via co-metabolism. As the ratio of biodiesel is increased in biodiesel/diesel mixtures, the faster the diesel is degraded. Another study using controlled experimental conditions also showed that fatty acid methyl esters, the primary molecules in biodiesel, degraded much faster than petroleum diesel in sea water.\n\nWhen considering the emissions from fossil fuel and biofuel use, research typically focuses on major pollutants such as hydrocarbons. It is generally recognized that using biodiesel in place of diesel results in a substantial reduction in regulated gas emissions, but there has been a lack of information in research literature about the non-regulated compounds which also play a role in air pollution. One study focused on the emissions of non-criteria carbonyl compounds from the burning of pure diesel and biodiesel blends in heavy-duty diesel engines. The results found that carbonyl emissions of formaldehyde, acetaldehyde, acrolein, acetone, propionaldehyde and butyraldehyde, were higher in biodiesel mixtures than emissions from pure diesel. Biodiesel use results in higher carbonyl emissions but lower total hydrocarbon emissions, which may be better as an alternative fuel source. Other studies have been done which conflict with these results, but comparisons are difficult to make due to various factors that differ between studies (such as types of fuel and engines used). In a paper which compared 12 research articles on carbonyl emissions from biodiesel fuel use, it found that 8 of the papers reported increased carbonyl compound emissions while 4 showed the opposite. This is evidence that there is still much research required on these compounds.\n\n"}
{"id": "4518557", "url": "https://en.wikipedia.org/wiki?curid=4518557", "title": "Explosives safety", "text": "Explosives safety\n\nExplosives safety originated as a formal program in the United States in the aftermath of World War I when several ammunition storage areas were destroyed in a series of mishaps. The most serious occurred at Picatinny Arsenal Ammunition Storage Depot, New Jersey, in July, 1926 when an electrical storm led to fires that caused explosions and widespread destruction. The severe property damage and 19 fatalities led Congress to empower a board of Army and Naval officers to investigate the Picatinny Arsenal disaster and determine if similar conditions existed at other ammunition depots. The board reported in its findings that this mishap could recur, prompting Congress to establish a permanent board of colonels to develop explosives safety standards and ensure compliance beginning in 1928. This organization evolved into the Department of Defense Explosives Safety Board (DDESB) and is chartered in Title 10 of the US Code. Today, the DDESB authors DOD Manual 6055.9, Ammunition and Explosives Safety Standards. It also evaluates scientific data which may adjust those standards, reviews and approves all explosives site plans for new construction, and conducts worldwide visits to locations containing US title munitions.\n\nThe United States Air Force counterpart to the DDESB is the Air Force Safety Center (AFSEC/SEW). Similarly safety functions are found at major command headquarters, intermediate command headquarters and at unit level as the weapons safety office. Quantity-Distance (QD) has evolved into the foundation of DOD 6055.9-STD, Ammunition and Explosives Safety Standards. The current Air Force regulation governing explosives safety is Air Force Manual (AFMAN) 91-201. AFMAN 91-201 was written using DODI 6055.9 as a parent regulation, and in most cases will follow the limitations set forth in the DODI (excluding mission specific requirements). The Air Force deviates from DODI 6055.9 using AFMAN 91-201 as their primary source document to allow for deviation from many of the requirements of the DODI as long as the risks of doing so are accepted at the appropriate level.\n\nThe United States Army counterpart to the DDESB is the U.S. Army Technical Center for Explosives Safety (USATCES). The USATCES is located with the Defense Ammunition Center on McAlester Army Ammunition Plant, near McAlester, Oklahoma . USATCES is responsible for providing ammunition and explosives (A&E) safety worldwide by acting as the field office of the Department of Army Safety responsible for A&E safety. The USATCES also acts as the Army agency having safety oversight of clean-up of Former Used Defense Sites (FUDS) and Former Toxic Chemical Agent Sites where munitions from all branches of service disposed of A&E by burial or dumping up until the end of the Vietnam War. The USATCES acts as the Army’s safety watchdog for disposal of chemical ammunition at the Army’s Chemical Disposal Facilities. As part of Army's Ordinance Corps under TRADOC Specially trained Civilian Explosives Safety Personnel [Quality Assurance Specialist (Ammunition Surveillance) (QASAS)] and Safety Specialist that have received specialized training in A&E Safety) from the USATCES are deployed worldwide, wherever the U.S. Army has A&E. Their mission is to provide A&E safety to the soldier, the public, and the environment making sure the Army’s A&E is not only stored safely but ready, reliable, and lethal when the U.S. military needs it.\n\nThe net explosives weight (NEW) (or TNT Equivalence) is based on explosives compounds that are equal to one expressed by the value of “K,” using the terminology K9, K11, K18, to mean K = 9, K = 11, and K = 18.\n\nA Blast Wave Phenomenon is an incident involving the violent release of energy created by detonation of an explosive device. The sudden and intense pressure disturbance is termed the “blast wave.” The blast wave is characterized by an almost instantaneous rise from ambient pressure to a peak incident pressure (Pi). This pressure increase or “shock front,” travels radially outward from the detonation point, with a diminishing velocity that is always in excess of the speed of sound in that medium. Gas molecules making up the front move at lower velocities. This velocity, which is called the “particle velocity,” is associated with the “dynamic pressure,” or the pressure formed by the winds produced by the shock front. As the shock front expands into increasingly larger volumes of the medium, the incident pressure decreases and, generally, the duration of the pressure-pulse increase. If the shock wave strikes a rigid surface (e.g., a building) at an angle to the direction of the wave’s propagation, a reflected pressure is instantly developed on the surface and this pressure rises to a value that exceeds the incident pressure. This reflected pressure is a function of the incident wave’s pressure and the angle formed between the rigid surface and the plane of the shock front.\n\nAn important consideration in the analysis of the hazards associated with an explosion is the effect of any fragments produced. Fragmentation most commonly occur in high explosives events, fragmentation may occur in any incident involving ammunition and explosives (A&E). Depending on their origin, fragments are referred to as “primary” or “secondary” fragments.\n\nPrimary fragments result from the shattering of a container (e.g., shell casings, kettles, hoppers, and other containers used in the manufacture of explosives and rocket engine housings) in direct contact with the explosive. These fragments usually are small, initially travel at thousands of feet per second, and may be lethal at long distances from an explosion.\n\nSecondary fragments are debris from structures and other items in close proximity to the explosion. These fragments, which are somewhat larger in size than primary fragments and initially travel at hundreds of feet per second, do not normally travel as far as primary fragments.\n\nGenerally, thermal hazards from explosives events are of less concern than blast and fragment hazards. With the release of energy from an explosion is heat. The amount of heat varies with the energetic compound (explosive). All explosives compound molecules are potentially unstable held together with weak bonds in their outer shell. When this weak bond is broken heat and energy is violently released. It normally takes longer for the thermal blast to incur. Injury from thermal effects follows the blast and fragmentation effects which happen almost instantaneously. This does not imply that there is a time lapse between blast and fragmentation effects of explosives; in fact it happens so fast that humans cannot notice the delay without specialized equipment. The time available to react to a thermal event does increases survivability by rapid equipment designed to react in a fragment of a second. The primary effect of the thermal effect from an explosive detonation on structures, material, and ammunition and explosives (A&E) is their partial or total destruction by fire. The primary concern for explosives safety with a fire involving A&E is that it may transition to a more severe reaction, causing detonations of additional or more hazardous explosives devises and placing more people or property at a greater degree of risk of damage, destruction, injury, or death.\n\nFollowing the 1966 Palomares B-52 crash and the 1968 Thule Air Base B-52 crash, accident investigators concluded that the conventional explosives used at the time in nuclear weapons were not stable enough to withstand the forces involved in an aircraft accident. The finding triggered research by scientists in the United States into safer conventional explosives that could be used in nuclear weapons. The Lawrence Livermore National Laboratory developed the \"Susan Test\" — a standard test that uses a special projectile whose design simulates an aircraft accident by squeezing and nipping explosive material between metal surfaces. The test projectile is fired under controlled conditions at a hard surface to measure the reactions and thresholds of different explosives to an impact.\n\nThis is a highly trained and skilled civilian professional usually a QASAS or a Safety Specialist that has been trained to evaluate risk and hazards involved with conventional, guided missiles and toxic chemical ammunition operations. Department of Defense Standards requires that only trained and certified personnel are permitted to participate in operations involving ammunition, explosives, and/or explosive components, guided missiles, and toxic chemicals. They are responsible for providing protection from the effects of ammunition and explosives by evaluation of a set of standards developed by the Department of Defense and reinforced by additional regulations by the branch of military service responsible for the explosives item. \nThey develop safety programs to minimize losses due to injuries and property damage. They try to eliminate unsafe practices and conditions on sites where ammunition and explosives (A&E) are used or stored. Military explosives safety specialist are deployed along with U.S. Military forces to maintain safe storage and use of A&E. They are responsible to recommend to military command ways to store A&E that reduce the risk of injury or death to service men and women in case of an accidental detonation or if the A&E supply is hit by enemy attack. \nMuch of the work of military explosives safety specialist is identical to their civilian counterparts. They have offices where they analyze data and write reports to upper commands on the storage of A&E. Much of their time is spent reviewing or preparing explosives safety site plans. An explosives site plan (ESS) is the composite risk management (CRM) process associated with explosives/toxic chemical activities to ensure the minimum risk to personnel, equipment, and assets, while meeting mission requirements. The damage or injury potential of explosions is determined by the separation distance between potential explosion sites (PES) and exposed sites (ES); the ability of the PES to suppress blast overpressure, primary and secondary fragments; and the ability of the ES to resist explosion effects. Planning for the proper location and construction of A&E facilities and surrounding facilities exposed to A&E facilities is a key element of the explosives/toxic chemical site planning process. This management process also ensures that risks above those normally accepted for A&E activities are identified and approved at the proper level of command.\n\nExplosives Safety Specialist must often travel to different storage sites to verify that the military installation is meeting the service explosives safety regulations.\n\nExplosives Safety Specialist often works with other safety professionals. They are required to know OSHA, EPA, NFPA and other consensus standards when looking at safety and if these regulations are stricter than their service regulation they must apply these standards and regulations. They must also know Alcohol, Tobacco, and Firearms (ATF) regulations dealing with A&E and apply those standards if it is required. They must be able to convince people the need for following prescribes explosives safety standards/regulations. They must also work with ammunition cleanup sites insuring that safety laws and regulations as well as industry standards are followed. They should be good at solving problems.\n\nThe military is not the only industry to use explosives safety specialist but are by far the largest employer. Mining and construction also use explosives safety specialist to evaluate hazard and risk from explosives and blasting operations. Ammunition and explosives manufactures also use these professionals. Outside the military explosives safety specialist must apply and be knowledgeable of ATF, OSHA, EPA, NFPA, as well as state and local regulations dealing with safety of A&E.\n\n\n"}
{"id": "39697622", "url": "https://en.wikipedia.org/wiki?curid=39697622", "title": "Felicity effect", "text": "Felicity effect\n\nThe Felicity effect is an effect observed during acoustic emission in a structure undergoing repeated mechanical loading. It negates the effect of emission silence in the structure that is often observed from the related Kaiser effect at high loads. A material demonstrating the Felicity effect gives off acoustic emission at a lower load than one previously reached in an increasing load cycle regime.\n"}
{"id": "25272343", "url": "https://en.wikipedia.org/wiki?curid=25272343", "title": "Flame lift-off", "text": "Flame lift-off\n\nFlame lift-off in oil fired pressure jet burners is an unwanted condition in which the flame and burner become separated. This condition is most commonly created by excessive combustion air and often results in the loss of flame as the photo-electric cell fails to register the light of the flame, this in turn results in a safety lockout of the control box.\n\nOther outcomes may be experienced: –\n\nA non-premixed jet flame has a tendency to lift off from the burner nozzle position when the jet velocity of the flame is over a critical value of A non-premixed jet flame has a tendency to lift off from the burner nozzle position when the jet velocity of the flame is over a critical value of formula_1. With the increasing of the jet velocity, the lifted height will increase and when it’s beyond certain critical height and the flame will be blown off. Therefore, the stability of the lifted flame is an important parameter for basic combustor design. Scholefield and Garside’s theory claimed that the transition to turbulence is a prerequisite for the lifted diffusion flame stabilisation and the flame anchors at a point where the flow is turbulent. Gollahalli argued that the flame will tend to stable at the position where the local flow velocity balance the normal flame propagation velocity. Navarro-Martinez and Kronenburg have demonstrated that the excessive turbulent stretching at the nozzle leads to the lift-off and they also claimed that auto-ignition can be used to promote the flame stabilisation mechanism. Recently the observation from Kiran and Mishra’s visual experiment proved the flame lift-off height varies linearly with jet exit velocity. They presented a semi-empirical correlation between the normalized lift-off height to the nozzle exit diameter.\n\nformula_2\n\nWhere, formula_3: lift-off height\n\nformula_4:diameter of the fuel tube\n\nformula_5:fuel jet velocity\n\nIn addition to the velocity effect, The stoichiometric burning on the physical mechanism blowout has been investigated by Broadwell et al. and Pitts. According to their study on diffusion flame, the fresh air entrained by the vortices structure cools down and over dilutes the flame jet, which leads to the flame extinction.\n"}
{"id": "30250063", "url": "https://en.wikipedia.org/wiki?curid=30250063", "title": "Float (liquid level)", "text": "Float (liquid level)\n\nLiquid level floats, also known as float balls, are spherical, cylindrical, oblong or similarly shaped objects, made from either rigid or flexible material, that are buoyant in water and other liquids. They are non-electrical hardware frequently used as visual sight-indicators for surface demarcation and level measurement. They may also be incorporated into switch mechanisms or translucent fluid-tubes as a component in monitoring or controlling liquid level.\n\nLiquid level floats, or float switches, use the principle of material buoyancy (differential densities) to follow fluid levels. Solid floats are often made of plastics with a density less than water or other application liquid, and so they float. Hollow floats filled with air are much less dense than water or other liquids, and are appropriate for some applications.\n\nStainless Steel Magnetic floats are tubed magnetic floats, used for reed switch activation; they have a hollow tubed connection running through them. These magnetic floats have become standard equipment where strength, corrosion resistance and buoyancy are necessary. They are manufactured by welding two drawn half shells together. The welding process is critical for the strength and durability of the float. The weld is a full penetration weld providing a smoothly finished seam, hardly distinguishable from the rest of the float surface.\n\n\n"}
{"id": "29262539", "url": "https://en.wikipedia.org/wiki?curid=29262539", "title": "Fold number", "text": "Fold number\n\nFold number refers to how many double folds that are required to cause rupture of a paper test piece under standardized conditions. Fold number is defined in ISO 5626:1993 as the antilogarithm of the mean folding endurance:\n\nformula_1\n\nwhere \"f\" is the fold number, \"F\" is the folding endurance for each test piece and \"n\" is total number of test pieces used.\n\nIn the introduction of ISO 5626:1993 it is emphasized that fold number, as defined in that very International Standard, does not equal the mean number of double folds observed. The latter is however still the definition used in some countries. If the numerical value of the folding endurance is not rounded off, these will however be equal.\n\nIn the former Swedish standard SS 152005 (\"Pappersordlista\") from 1992, with paper related terms defined in Swedish and English, fold number is explained as \"the number of double folds which a test strip withstands under specified conditions before a break occurs in the strip\"; that is, \"not\" the antilogarithm of the mean folding endurance.\n\n"}
{"id": "17254357", "url": "https://en.wikipedia.org/wiki?curid=17254357", "title": "Fuel taxes in Australia", "text": "Fuel taxes in Australia\n\nThe main fuel tax in Australia is an excise tax, to which is added a Goods and Services Tax (\"GST\"). Both taxes are levied by the federal government. In Australia, like Canada, the GST (in Australia's case of 10%) is applied on top of the fuel excise tax. In some cases, businesses may be entitled to exemptions or rebates for fuel excise tax, including tax credits and certain excise-free fuel sources.\n\nThe double dipping (GST imposed on the excise tax) was fully compensated for by lowering the excise at the time the GST was introduced in 2001. While the excise stopped being indexed for inflation in 2001, it was reintroduced in 2014 (see History below).\n\nThe tax collected is partly used to fund national road infrastructure projects and repair roads, but most of it (approximately 75%) goes into general revenue.\n\nThe excise tax on commonly used fuels in Australia as of October 2018:\n\nNote: Gasoline for use as fuel in an aircraft is taxed at $0.03556 per litre.\n\nThe federal government increased the fuel excise tax with effect from 10 November 2014 by restoring CPI indexation to the tax every six months, on 1 February and 1 August. From that date the fuel excise tax increased to 38.6 cents per litre.\n\nThe excise tax on commonly used fuels in Australia as of 1 October 2018 is:\nIn addition all fuels are subject to 10% Goods and Services Tax(GST).\n\nNote: Gasoline for use as fuel in an aircraft is taxed at $0.03556 per litre.\n\nThere are also a number of various grants and incentive schemes involving tax credits and rebates that generally apply to businesses or industries that rely heavily on the use of fuels, such as transport and aviation. There are also rebates that encourage the production and importation of clean fuels.\n\nThe Commonwealth introduced a twice yearly indexation for Consumer Price Index of fuel excise taxes in 1983. The states used to levy fuel franchise fees until the High Court of Australia in \"Ha v New South Wales\" (1997) ruled that a licence fee based on the value of tobacco was unconstitutional, as it was an excise tax that only the Commonwealth can levy. The ruling brought into doubt the revenues of the states. In consequence, the federal government introduced a fuel excise tax and gave the revenue to the states.\n\nIn 2001, the Howard Government adjusted the excise rates because of the introduction of the GST and stopped the automatic indexation of the fuel excise tax.\n\nThe second phase of the Australian Fuel Tax Credits Scheme came into effect on 1 July 2008. Under these changes, all off-road business use of fuel became eligible for subsidies. In other words, businesses that do not run large vehicle fleets but consume large amounts of fuel in business processes (such as mining, manufacturing, construction, plant operations) became eligible for a fuel tax credit.\n\nQueensland used to provide an 8.354c/L subsidy on most fuels sold in the state, including on unleaded, blended unleaded, LPG and ethanol. The subsidy reflected the lower franchise fee Queensland charged compared to other states prior to 1997. This was usually reflected by an 8.354c/L price difference at the pump, as the subsidy was paid directly to retailers. The subsidy was removed from 1 July 2009.\n\nThe indexation of the fuel excise tax was reintroduced by the Abbott Government from 10 November 2014, with indexation being effected twice a year, on 1 February and 1 August.\n\n\n"}
{"id": "38131438", "url": "https://en.wikipedia.org/wiki?curid=38131438", "title": "GLAST (tokamak)", "text": "GLAST (tokamak)\n\nThe GLAss Spherical Tokamak (or GLAST) is a name given to a set of small spherical tokamaks (i.e. magnetic confinement fusion reactors) located in Islamabad, Pakistan. They were developed by the Pakistan Atomic Energy Commission (PAEC) as part of the National Tokamak Fusion Program (NTFP) in 2008 and are primarily used for teaching and training purposes. \n\nThe first two tokamaks developed were named GLAST-I and GLAST-II. Both devices have similar principles of operation and consist of an insulated vacuum vessel made of pyrex glass. However, the central tube of GLAST-I is made of steel, while that of GLAST-II is made of glass.\n\nStudies were done in GLAST-II to identify the mechanism responsible for current generation during the start-up phase of tokamak discharge.\n\nPlasma diagnostics including Langmuir triple probes, emissive probes and Optical Emission Spectroscopy systems were developed to measure basic plasma parameters such as electron temperature, electron number density, floating potential and impurity content in the discharge. The triple probe is capable of recording instantaneous plasma characteristics. Plasma current is then enhanced up to 5 kA by applying a small vertical magnetic field that provides additional plasma heating and shaping. The evolution of electron cyclotron heating (ECH)-assisted pre-ionization and subsequent current formation phases in one shot are well envisioned by probe measurements. The probe data seem to correlate with microwave absorption and subsequent light emission. Intense fluctuations in the current formation phase advocate for efficient equilibrium and feedback control systems. Moreover, the emergence of some strong impurity nitrogen lines in the emission spectrum even after few shots propose crucial need for improvement in the base vacuum level. A noticeable change in the profile's shape of floating potential, electron temperature, ion saturation current (Isat) and light emission is observed with changing hydrogen fill pressure and vertical field. The main discharge has been supported by microwave pre-ionization in the presence of optimized resonant toroidal magnetic field (TF). While optimizing the magnetic field, theoretical and experimental results of the TF profile are compared using a combination of fast and slow capacitor banks. The magnetic field produced by poloidal field (PF) coils are compared with theoretically predicted values. \n\nIt is found that calculated results are in good agreement with experimental measurement. An economical microwave source of 2.45 ± 0.02 GHz is fabricated using a magnetron obtained from a household microwave oven. Pulsed-mode operation of the magnetron is achieved through certain necessary modifications in the circuit. The magnetic field is upgraded to enhance the microwave power, where an additional electromagnet is introduced around the magnetron cavity that confines the fast moving electrons. This modified microwave source is sufficient to achieve the breakdown in GLAST-II with improved plasma current of 5kA.\n\nGLAST-III is an upgraded version of the GLAST-I and GLAST-II designs which features a larger vessel diameter and a larger central bore for the placement of diagnostic tools such as Rogowski coils and flux loops.\n\nGLAST-III retained most of the diagnostics used in GLAST-I and GLAST-II, but a newly developed spectroscopic system based on linear photodiode array was installed on the upgraded GLAST-III for spatial and temporal characterization of hydrogen discharge through light emission. The spectral range of each silicon photodiode is from 300 nm to 1100 nm with response time of 10 ns and active area of 5 mm (circular). The light from the plasma is collected through holes along 4 line-of-sight channels with spatial resolution of about 5 cm passing from entire poloidal cross section. The photodiode's signals located at position of 10 and 14 cm from inboard side show fluctuations in the central plasma region. Moreover, the sequence of plasma lighting shows that plasma instigates from the central resonant field region and then expands outwards. At lower pressure, outboard movement of the plasma is slower suggesting better plasma confinement. In addition to photodiode array, an optical spectrometer (Ocean Optics HR2000+) has been used to record the visible spectrum over the selected range (597–703 nm) with a spectral resolution of 0.15 nm. The studies have been conducted during initial phase of plasma formation for two different hydrogen gas fill pressures. The triple probe is used to get time-resolved information on plasma parameters in the edge region. The time evolution of whole discharge including microwave pre-ionization phase and current formation phase has been demonstrated by temporal profiles of light emission and plasma floating potential.\n"}
{"id": "12799505", "url": "https://en.wikipedia.org/wiki?curid=12799505", "title": "GYRO", "text": "GYRO\n\nGYRO is a computational plasma physics code developed and maintained at General Atomics. It solves the 5-D coupled gyrokinetic-Maxwell equations using a combination of finite difference, finite element and spectral methods. Given plasma equilibrium data, GYRO can determine the rate of turbulent transport of particles, momentum and energy.\n\n\n"}
{"id": "3702274", "url": "https://en.wikipedia.org/wiki?curid=3702274", "title": "Gallium antimonide", "text": "Gallium antimonide\n\nGallium antimonide (GaSb) is a semiconducting compound of gallium and antimony of the III-V family. It has a lattice constant of about 0.61 nm.\n\nThe intermetallic compound GaSb was first prepared in 1926 by Victor Goldschmidt, who directly combined the elements under an inert gas atmosphere and reported on GaSb's lattice constant, which has since been revised. Goldschmidt also synthesized gallium phosphide and gallium arsenide. The Ga-Sb phase equilibria was investigated in 1955 by Koster and by Greenfield.\n\nGaSb can be used for Infrared detectors, infrared LEDs and lasers and transistors, and thermophotovoltaic systems.\n\n\n"}
{"id": "8251904", "url": "https://en.wikipedia.org/wiki?curid=8251904", "title": "Glow fuel", "text": "Glow fuel\n\nGlow fuel is a fuel source used in model engines – generally the same or similar fuels can be used in model airplanes, helicopters, cars and boats. Glow fuel can be burned by very simple two-stroke engines or by more complicated four-stroke engines, and these engines can provide impressive amounts of power for their very small size. Glow fuel is primarily for two-stroke engines with the need for oil mixed in the fuel and limited exhaust and fuel/air between cycles. Top Fuel race cars with 4-stroke engines may also use glow fuel, but in this case it does not contain appreciable oil.\n\nOther commonly used names are nitro or just model fuel. Note that the nitro name is generally inaccurate, as nitromethane is usually not the primary ingredient, and in fact many glow fuels, especially the so-called \"FAI\" type, named for the Fédération Aéronautique Internationale, which requires such fuel in some forms of aeromodeling competition, contain no nitromethane at all.\n\nGlow fuel is a mixture of methanol, nitromethane, and oil.\n\nMethanol is the primary ingredient as it provides the bulk of the fuel, and is needed as a solvent for the other ingredients. The presence of methanol vapor causes the glow plug found in model engines to heat via a catalytic reaction with the platinum wire and glow.\n\nNitromethane is added to the methanol to increase power and to make the engine easier to tune. Typically glow fuel is about 0-30% nitromethane. While higher concentrations can result in better engine performance, use of highly concentrated nitromethane is rare because of its cost. Although a given amount of nitromethane contains less energy than the same amount of methanol, it increases the amount of available oxygen in the combustion chamber, which allows the engine to draw in less air and more fuel. The increased amount of fuel increases power output and also helps cool the engine. For racing use, the nitromethane content can be increased to the range of 30%-65%.\n\nNitromethane is often difficult to obtain in many countries, so in these countries glow fuel typically has no nitromethane at all, which is generally not detrimental to engine longevity.\n\nMost model engines require oil to be included with the fuel as a lubricant since the engine has no independent oiling capability. Model engine fuel is typically 8-22% oil, with the higher percentages run in older design two-stroke glow engines that use bushings for the crankshaft bearings. The most commonly used lubricants are castor oil and synthetic oils, and many glow fuels include a mixture of the two. The oils included in glow fuel generally are not burned by the engine, and are expelled out the exhaust of the engine. This also helps the engine dissipate heat, as the oil emitted is generally hot.\n\nFour stroke model engines, since they are generally designed to be simple powerplants while still incorporating the usual camshaft, rocker arms and poppet valves of larger sized four stroke engines, are generally meant to use glow ignition and their fuel. Often, the oil percentage for four stroke glow fuel can be lowered from the 18–20% figure used for some two-stroke engines, down to as low as a 12–15%, but use of such low-lubricant fuel can also mandate the need for a small amount of castor oil in the mix, and mandates setting the high-speed fuel mixture carefully by using a handheld digital tachometer to check engine speed to avoid over-leaning of the fuel mixture.\n\nGlow engines generally have to be run slightly rich with a higher fuel/air ratio than is ideal in order to keep the engine cool. The uncombusted fuel in the exhaust carries heat, providing the cooling effect. Because of this, vehicles with glow engines generally get coated with lots of oil. Almost all the oil mixed with the fuel is unconsummed and comes out the exhaust, as well as some of the nitromethane and methanol as well. This requires some cleaning.\n\nThe nitromethane in many glow fuel blends can cause corrosion of metal parts in model engines, especially four-stroke designs, due to the nitric acid residue formed from combustion of nitromethane-containing fuel, making the use of a so-called \"after-run oil\" a common practice after a model flying session with a four-stroke glow-engine-powered model. \n\nGlow fuel is not difficult to make, and so many modelers mix their own to save money, but some of the ingredients are flammable or explosive and so can be dangerous, especially in large quantities. Most modelers buy their glow fuel premixed.\n\nNitromethane is sometimes replaced or supplemented by nitroethane. Propylene oxide is sometimes added in small percentages.\nAnother form of model fuel, for small compression ignition engines, is called \"Diesel Fuel\", which generally consists of kerosene, ether, oil and some sort of ignition improver, usually amyl nitrate or isopropyl nitrate, it is in no way related to the automotive fuel of the same name.\n\n\n"}
{"id": "13844012", "url": "https://en.wikipedia.org/wiki?curid=13844012", "title": "Greenhouse and icehouse Earth", "text": "Greenhouse and icehouse Earth\n\nThroughout the Phanerozoic history of the Earth, the planet's climate has been fluctuating between two dominant climate states: the greenhouse Earth and the icehouse Earth. \nThese two climate states last for millions of years and should not be confused with glacial and interglacial periods, which occur only during an icehouse period and tend to last less than 1 million years. There are five known great glaciations in Earth's climate history; the main factors involved in changes of the paleoclimate are believed to be the concentration of atmospheric carbon dioxide, changes in the Earth's orbit, and oceanic and orogenic changes due to tectonic plate dynamics. Greenhouse and icehouse periods have profoundly shaped the evolution of life on Earth.\n\nA \"greenhouse Earth\" or \"hothouse Earth\" is a period in which there are no continental glaciers whatsoever on the planet, the levels of carbon dioxide and other greenhouse gases (such as water vapor and methane) are high, and sea surface temperatures (SSTs) range from 28 °C (82.4 °F) in the tropics to 0 °C (32 °F) in the polar regions. \n\nThis state should not be confused with a hypothetical \"hothouse earth\", which is an irreversible tipping point corresponding to the ongoing runaway greenhouse effect on Venus. The IPCC states that \"a 'runaway greenhouse effect'—analogous to [that of] Venus—appears to have virtually no chance of being induced by anthropogenic activities.\"\n\nThere are several theories as to how a greenhouse Earth can come about. The geological record shows CO and other greenhouse gases are abundant during this time. Tectonic movements were extremely active during the more well-known greenhouse ages (such as 368 million years ago in the Paleozoic Era). Because of continental rifting (continental plates moving away from each other) volcanic activity becomes more prominent, producing more CO and heating up the Earth's atmosphere. Earth is more commonly placed in a greenhouse state throughout the epochs, and the Earth has been in this state for approximately 80% of the past 500 million years, which makes understanding the direct causes somewhat difficult.\n\nAn \"icehouse Earth\" is the earth as it experiences an ice age. Unlike a greenhouse Earth, an icehouse Earth has ice sheets present, and these sheets wax and wane throughout times known as glacial periods and interglacial periods. During an icehouse Earth, greenhouse gases tend to be less abundant, and temperatures tend to be cooler globally. The Earth is currently in an icehouse stage, as ice sheets are present on both poles and glacial periods have occurred at regular intervals over the past million years.\n\nThe causes of an icehouse state are much debated, because not much is really known about the transition periods between greenhouse to icehouse climates and what could make the climate so different. One important aspect is clearly the decline of CO in the atmosphere, possibly due to low volcanic activity.\n\nOther important issues are the movement of the tectonic plates and the opening and closing of oceanic gateways. These seem to play a crucial part in icehouse Earths because they can bring forth cool waters from very deep water circulations that could assist in creating ice sheets or thermal isolation of areas. Examples of this occurring are the opening of the Tasmanian gateway 36.5 million years ago that separated Australia and Antarctica and which is believed to have set off the Cenozoic icehouse, and the creation of the Drake Passage 32.8 million years ago by the separation of South America and Antarctica, though it was believed by other scientists that this did not come into effect until around 23 million years ago. The closing of the Isthmus of Panama and the Indonesian seaway approximately 3 or 4 million years ago may have been a major cause for our current icehouse state. For the icehouse climate, tectonic activity also creates mountains, which are produced by one continental plate colliding with another one and continuing forward. The revealed fresh soils act as scrubbers of carbon dioxide, which can significantly affect the amount of this greenhouse gas in the atmosphere. An example of this is the collision between the Indian subcontinent and the Asian continent, which created the Himalayan Mountains about 50 million years ago.\n\nWithin icehouse states, there are \"glacial\" and \"interglacial\" periods that cause ice sheets to build up or retreat. The causes for these glacial and interglacial periods are mainly variations in the movement of the earth around the Sun. The astronomical components, discovered by the Serbian geophysicist Milutin Milanković and now known as Milankovitch cycles, include the axial tilt of the Earth, the orbital eccentricity (or shape of the orbit) and the precession (or wobble) of the Earth's spin. The tilt of the axis tends to fluctuate between 21.5° to 24.5° and back every 41,000 years on the vertical axis. This change actually affects the seasonality upon the earth, since more or less solar radiation hits certain areas of the planet more often on a higher tilt, while less of a tilt would create a more even set of seasons worldwide. These changes can be seen in ice cores, which also contains information that shows that during glacial times (at the maximum extension of the ice sheets), the atmosphere had lower levels of carbon dioxide. This may be caused by the increase or redistribution of the acid/base balance with bicarbonate and carbonate ions that deals with alkalinity. During an Icehouse, only 20% of the time is spent in interglacial, or warmer times.\n\nA \"snowball earth\" is the complete opposite of greenhouse Earth, in which the earth's surface is completely frozen over; however, a snowball earth technically does not have continental ice sheets like during the icehouse state. \"The Great Infra-Cambrian Ice Age\" has been claimed to be the host of such a world, and in 1964, the scientist W. Brian Harland brought forth his discovery of indications of glaciers in low latitudes (Harland and Rudwick). This became a problem for Harland because of the thought of the \"Runaway Snowball Paradox\" (a kind of Snowball effect) that, once the earth enters the route of becoming a snowball earth, it would never be able to leave that state. However, in 1992 brought up a solution to the paradox. It is believed that since the continents at this time were huddled at the low and mid-latitudes that there was a great cooling event by planetary albedo, or reflection of the earth’s surface. Kirschvink explained that the way to get out of the snowball could be connected to carbon dioxide, since volcanic activity would not halt, and that the buildup and lack of \"scrubbing\" of this carbon dioxide in the atmosphere, that the earth would return to a greenhouse state. Some scientists believe that the end of the snowball Earth caused an event known as the Cambrian Explosion, which produced the beginnings of multi-cellular life. However some biologists claim that a complete snowball Earth could not have happened since photosynthetic life would not have survived underneath many meters of ice without sunlight. However, it has been observed that, even under meters of thick ice around Antarctica, sunlight shows through. Most scientists today believe that a \"hard\" Snowball Earth, one completely covered by ice, is probably impossible. However, a \"slushball earth\", with points of openings near the equator, is possible.\n\nRecent studies may have again complicated the idea of a snowball earth. In October 2011, a team of French researchers announced that the carbon dioxide during the last speculated \"snowball earth\" may have been lower than originally stated, which provides a challenge in finding out how Earth was able to get out of its state and if it were a snowball or slushball.\n\nThe Eocene, which occurred between 53 and 49 million years ago, was the Earth's warmest temperature period for 100 million years. However, this \"super-greenhouse\" eventually became an icehouse by the late Eocene. It was believed that the decline of CO caused this change, though there are possible positive feedbacks, or added influence that contributes to the cooling.\n\nThe best record we have for a transition from an icehouse to a greenhouse period where plant life exists is during the Permian epoch that occurred around 300 million years ago. In 40 million years a major transition took place, causing the Earth to change from a moist, icy planet where rainforests covered the tropics, into a hot, dry, and windy location where little could survive. Professor Isabel P. Montañez of University of California, Davis, who has researched this time period, found the climate to be \"highly unstable\" and \"marked by dips and rises in carbon dioxide\".\n\nThe Eocene-Oligocene transition, the latest transition occurring approximately 34 million years ago, resulted in rapid global temperature decrease, the glaciation of Antarctica and a series of biotic extinction events. The most dramatic species turnover event associated with this time period is the Grande Coupure, a period which saw the replacement of European tree-dwelling and leaf-eating mammal species by migratory species from Asia.\n\nThe science of paleoclimatology attempts to understand the history of greenhouse and icehouse conditions over geological time. Through the study of ice cores, dendrochronology, ocean and lake sediments (varve), palynology (fossilized pollen) and isotope analysis (such as Radiometric dating and stable isotope analysis), scientists can create models of past climate. One study has shown that atmospheric carbon dioxide levels during the Permian age rocked back and forth between 250 parts per million (which is close to present-day levels) up to 2,000 parts per million. Studies on lake sediments suggest that the \"Hothouse\" or \"super-Greenhouse\" Eocene was in a \"permanent El Nino state\" after the 10 °C warming of the deep ocean and high latitude surface temperatures shut down the Pacific Ocean's El Nino-Southern Oscillation. A theory was suggested for the Paleocene–Eocene Thermal Maximum on the sudden decrease of carbon isotopic composition of global inorganic carbon pool by 2.5 parts per million. A hypothesis noted for this negative drop of isotopes could be the increase of methane hydrates, the trigger for which remains a mystery. This increase of methane in the atmosphere, which happens to be a potent, but short-lived greenhouse gas, increased the global temperatures by 6 °C with the assistance of the less potent carbon dioxide.\n\n\nCurrently, the Earth is in an icehouse climate state. About 34 million years ago, ice sheets began to form in Antarctica; the ice sheets in the Arctic did not start forming until 2 million years ago. Some processes that may have led to our current icehouse may be connected to the development of the Himalayan Mountains and the opening of the Drake Passage between South America and Antarctica. Scientists have been attempting to compare the past transitions between icehouse and greenhouse, and vice versa to understand where our planet is now heading.\n\nWithout the human influence on the greenhouse gas concentration, the Earth would be heading toward a glacial period. Predicted changes in orbital forcing suggest that in absence of human-made global warming the next glacial period would begin at least 50,000 years from now (see Milankovitch cycles).\n\nBut due to the ongoing anthropogenic greenhouse gas emissions, the Earth is instead heading toward a greenhouse Earth period. Permanent ice is actually a rare phenomenon in the history of the Earth, occurring only in coincidence with the icehouse effect, which has affected about 20% of Earth's history.\n\n"}
{"id": "25991176", "url": "https://en.wikipedia.org/wiki?curid=25991176", "title": "Guodian Beilun Power Station", "text": "Guodian Beilun Power Station\n\nThe Guodian Beilun Power Station () is a coal-fired power station in Beilun District, Ningbo, Zhejiang, China. With an installed capacity of , it is the fourth largest coal-fired power station in the world. The station generates energy by five and two units, which is fuelled by coal. It is also the first power generation enterprise in China to use World Bank loans for construction.\n\n"}
{"id": "53410507", "url": "https://en.wikipedia.org/wiki?curid=53410507", "title": "History of photovoltaic growth", "text": "History of photovoltaic growth\n\nThe history of photovoltaic growth includes previous forecast and annual deployment figures by country.\n\nMost PV deployment figures in this article are provided by the European Photovoltaic Industry Association in the \"Global Outlook for Photovoltaics\" report, the \"Observatoire des énergies renouvelables\" or EurObserv'ER's \"Photovoltaic Barometer\" report, and the IEA-PVPS (photovoltaic power systems) \"Snapshot\" and \"Trends\" report.\n\n\n\n\n\n\n\n\n\n\n\nIn 2013, worldwide deployment of solar PV amounted to almost 40 GW (39,953 MW)—an increase of about 35 percent over the previous year. Cumulated capacity increased by 38 percent to more than 139 GW. This is sufficient to generate at least 160 terawatt-hours (TWh) or 0.85 percent of the world's total electricity consumption of 18,400 TWh.\n\n\nIn 2013, Asia has been the fastest growing region, with China and Japan accounting for 49% of worldwide deployment. About a quarter has been installed in Europe (10,975 MW). The remaining quarter of the 38,400 MW deployed in 2013 is split between North America and other countries.\n\nEurope is still the most developed region with a cumulative capacity of 81.5 GW, about 59 percent of the global total, followed by the Asia-Pacific region (APAC), including countries such as Japan, India and Australia with 22 GW or about 16 percent of worldwide cumulative capacity (due to its significance, China is excluded from the APAC region in all PV statistics and listed separately). European solar PV now covers 3 percent of the electricity demand and 6 percent of the peak electricity demand. However, deployment in Europe has slowed down by half compared to the record year of 2011, and will most likely continue to decrease. This is mainly due to the strong decline of new installations in Germany and Italy.\n\nCumulative capacity in the MEA (Middle East and Africa) region and ROW (rest of the world) accounted for less than 3 GW or about 2.2% of the global total. A great untapped potential remains for many of these countries, especially in the Sunbelt.\n\n\nIn 2013, the world's top installer were China (+12.92 GW), followed by Japan (+6.97 GW) and the United States (+4.75 GW), while Germany remained the world's largest overall producer of power from solar PV, with a total capacity around 36 GW and contributing 5.7% to its net electricity consumption. Italy met more than 7% of its electricity demands with solar PV, thus leading the world in that respect.\n\nThe top ten leading countries in terms of deployed and overall PV-capacity are shown above. Other mentionable PV deployments above the 100-megawatt mark included France (643 MW), Canada (445 MW), South Korea (445 MW), Thailand (437 MW), The Netherlands (360 MW), Switzerland (319 MW), Ukraine (290 MW), Austria (263 MW), Israel (244 MW), Belgium (237 MW), Taiwan (170 MW) Denmark (156 MW) and Spain (116 MW).\n\nIn 2013, Europe added 11 gigawatts of new PV installation (including non-EU countries). It is still the most developed region with a cumulated total of 81.5 GW, about 59 percent of the worldwide installed capacity. Solar PV now covers 3 percent of the electricity demand and 6 percent of the peak electricity demand. However, European PV deployment has slowed down by half compared to the record year of 2011, and will most likely continue to decrease. This is mainly due to the strong decline of new installations in Germany and Italy.\n\n\n\n\n\n\n\nOff grid refers to photovoltaics which are not grid connected. On grid means connected to the local electricity grid. Δ means the amount installed during the previous year. Σ means the total amount installed. Wp/capita refers to the ratio of total installed capacity divided by total population, or total installed Wp per person. Module price is average installed price, in Euros. kW·h/kWp·yr indicates the range of insolation to be expected. While National Report(s) may be cited as source(s) within an International Report, any contradictions in data are resolved by using only the most recent report's data. Exchange rates represent the 2006 annual average of daily rates (OECD Main Economic Indicators June 2007).<br>Module Price: Lowest:2.5 EUR/Wp (2.83 USD/Wp) in Germany 2003. Uncited insolation data is from maps dating 1991–1995.\n\n\nNotes: While National Report(s) may be cited as source(s) within an International Report, any contradictions in data are resolved by using only the most recent report's data. Exchange rates represent the 2006 annual average of daily rates (OECD Main Economic Indicators June 2007)<br>Module Price: Lowest:2.5 EUR/Wp (2.83 USD/Wp) in Germany 2003. Uncited insolation data is lifted from maps dating 1991–1995.\n\n\nOriginal source gives these individual numbers and totals them to 37,500 kW. The 2004 reported total was 30,700 kW. With new installations of 6,800 kW, this would give the reported 37,500 kW.\n\n"}
{"id": "18397882", "url": "https://en.wikipedia.org/wiki?curid=18397882", "title": "Holbrook Superconductor Project", "text": "Holbrook Superconductor Project\n\nThe Holbrook Superconductor Project is the world's first production superconducting transmission power cable. The lines were commissioned in 2008. The suburban Long Island electrical substation is fed by a 600 meter long tunnel containing about 99 miles of high-temperature superconductor wire manufactured by American Superconductor, installed underground and chilled to superconducting temperature with liquid nitrogen. \n\nThe superconductor is bismuth strontium calcium copper oxide (BSCCO) which superconducts at liquid nitrogen temperatures. Other parts of the system include a liquid nitrogen storage tank, a Brayton Helium refrigerator, and a number of cryostats which manage the transition between cryogenic and ambient temperatures.\n\nThe project was funded by the United States Department of Energy, and operates as part of the Long Island Power Authority power grid.\n"}
{"id": "7886807", "url": "https://en.wikipedia.org/wiki?curid=7886807", "title": "HurriQuake", "text": "HurriQuake\n\nThe HurriQuake nail is a construction nail designed by Ed Sutt for Bostitch, a division of Stanley Works, and patented in 2004. The features of the nail are designed primarily to provide more structural integrity for a building, especially against the forces of hurricanes and earthquakes.\n\nStarting at the bottom of the nail, the ring shanks on the lower half of the nail are enhanced with angular barbs, which add maximum resistance to the nail being pulled out. The middle of the nail has no extra features, which leaves the section most likely to be damaged during an earthquake thicker and least prone to damage. The area directly below the head of the nail features a spiral-style shank used to enhance the nail's strength in holding boards together. This enhancement keeps boards that are nailed together from moving around under nature's forces and overall weakening the joints. The final feature of this nail is the nail head, which is 25% larger than the average, allowing it to be more resistant to being pulled completely through attached pieces of wood.\n\nThe nail's design began when its inventor, civil engineer Ed Sutt, traveled to the Caribbean in the wake of Hurricane Marilyn. Sutt's trip to the Caribbean was part of a team examining the wreckage of the 80% of the island's homes and business that had been destroyed in the hurricane's winds of 95 mph (155 km/h). The finding among the homes that had been destroyed was that wood failure was not the cause of destruction; instead, the findings showed that the nails holding the wood together had failed, leading to the buildings' ultimate collapse. Sutt's research began after taking a research assistantship program at the Clemson Wind Load Test Facility, which had received funding through a grant from FEMA. The grant was used to research wooden-framed structures and the relationship of their failure to wind velocity. The outcome of the project's research showed that the best way to strengthen a structure was to improve the fasteners that held the roofing and wall sheathing to the internal frame, and with this information Sutt signed on as a fastener engineer for the Stanley subsidiary, Bostitch.\n\nAs development began on the nail, there were three major causes of failure to be overcome. These were having the nail — head and all — rip through the sheathing, having the entire nail pull out of the frame, and having the nail's midsection snap under stress. Early research showed that the larger the head, the less chance there was of the nail being ripped through the sheathing. However, the difficult task was increasing the nail's head while still making it compatible with popular nail gun models. After finding the ideal nail head size, the next task to conquer was preventing the entire nail from pulling out of the frame. This was overcome by adding barbed ring shanks around the lower portion of the nail. During the testing of the shanks, it was noted that above a certain point the shanks no longer strengthened the nail but instead weakened the nail by making it more susceptible to shearing. The final touch to the original prototype was a special high-carbon alloy designed by a metallurgist that had the perfect combination of stiffness and pliability, giving it the highest possible strength.\n\nAfter analyzing hundreds of designs, and finally coming up with what was believed to be the best design, Bostich finally released the nail in 2005 and labeled it the Sheather Plus. Even though the new nail was stronger than most nails, the barbs, which added the much needed strength to the nail's holding power, weakened the strength of the joint by opening the hole too far. This caused the joint to be sloppy and wobbly, so the team went back to the drawing board where the final feature was added to the nail. To compensate for the extra width of the nail hole, due to the ring shanks, Sutt decided to add a thicker screw-shank to the portion of the nail directly under the head. This addition thickened the top portion of the nail giving it a tighter joint, as well as enhancing its overall holding power.\n\nIndependent tests of the nail's stretch were conducted by several organizations, including Florida International University and the International Code Council. Those tests confirmed what the researchers at Bostitch had claimed, that they had created a better nail. Among all of the different tests, it was found that the new nail had twice the \"uplift capacity\" of other power driven nails, as well as increasing a home's resistance to wind and increasing earthquake resistance by up to 50%. For further testing, Sutt asked for the assistance of Scott Schiff, the coordinator of the graduate program in civil engineering and engineering mechanics at Clemson University. Tests at the Clemson Wind Load Test Facility confirmed what had already been stated. With equipment to simulate the force of winds, roofs attached with traditional nails were pulled apart at around 13,500 pounds of force (60.1 kN). At forces up to 16,000 pounds (71 kN), walls built with the HurriQuake environment nail showed minimal wall movement. As the pressure increased to 17,000 pounds (75 kN), then 18,000 (80 kN), then 19,000 (85 kN), the walls began to make creaking and groaning noises but they still stayed attached. As the test rig pushed 20,000 pounds (89 kN), the maximum it was capable of testing, it gave out, showing that the HurriQuake environment nail sustained 20,000 pounds of force (89 kN) and still was not sheared or completely pulled out.\n\nThe Hurriquake won the \"Popular Science\" Best of What's New 2006 award for Home Technology and was honored as the grand award winner for Best Innovation of the Year.\n\nIt is the subject of US Patent 6,758,018, granted July 6, 2004.\n\n"}
{"id": "9437152", "url": "https://en.wikipedia.org/wiki?curid=9437152", "title": "In Old Oklahoma", "text": "In Old Oklahoma\n\nIn Old Oklahoma (reissued as War of the Wildcats) is a 1943 American Western film directed by Albert S. Rogell starring John Wayne and Martha Scott. The film was nominated for two Academy Awards, one for Music Score of a Dramatic or Comedy Picture and the other for Sound Recording (Daniel J. Bloomberg).\n\nEastern school teacher Catherine Allen becomes notorious in 1906 when it is learned that she has authored a romance novel. She decides to move West and begin a new life.\n\nOn the train, oil man Jim Gardner makes a pass at her. Catherine asks a cowboy, Dan Somers, to sit nearby as a safety measure. Both are on their way to Oklahoma, with stagecoach driver Despirit Dean tagging along with his friend Dan.\n\nMany people in Sapulpa are upset with Jim's business tactics. A farmer feels he was paid too little for his property after Jim discovers oil there. Jim is furious when Dan strongly discourages Chief Big Tree from selling Indian land at too low an offer.\n\nDan travels to Washington, D.C., to ask President Theodore Roosevelt about oil rights. He fought for Teddy and the Rough Riders a few years before. Teddy offers him a chance to transport thousands of barrels of oil to a Tulsa refinery to win the rights over Jim, which leads to Jim's hired man, the Cherokee Kid, setting off an explosion and sabotaging the trip.\n\nCatherine and Dan fall in love, with hotel owner Bessie Baxter playing matchmaker. A final fistfight between Dan and Jim settles matters once and for all.\n\n\nIn December 1941 it was announced Republic Pictures had bought an \"oilfield story\" \"War of the Wildcats\" by Thomson Burtis, as a vehicle for Ray Middleton. They announced it for production in 1942. In December it was announced Frances Hyland was working on the script and that the film would be a vehicle for John Wayne.\n\nThe film still took a number of months to move into production. Eleanor Griffin and Ethel Hill were hired to work on the script \"which puts the feature in the big league class\" according to the \"Los Angeles Times\". They were \"to give the story the epic flavor.\"\n\nThe movie was retitled \"In Old Oklahoma\" and filming was to start 15 June. Martha Scott was signed for the female lead, which was seen as a coup for Republic because she was associated with prestigious films such as \"Our Town\" (1940).\n\nThe film was allocated a bigger budget than usual for a Republic Pictures film.\n\nFilming took place near Bakersfield.\n\nParts of the film were shot in Johnson Canyon, Paria, Utah, Cedar City, and Virgin, Utah.\n\nRepublic Pictures released it on December 6, 1943.\n\nThe film did extremely well at the box office and encouraged Republic to make more bigger budgeted films.\n\nThe movie was reissued in 1947 as War of the Wildcats.\n\nScreenwriter Griffin's ex-husband, William Rankin, later launched a $115,000 lawsuit against Griffin, Hill, Burtis and Republic, alleging the script included elements in three original scripts of his that he submitted to Republic: \"Indian Territory\", \"Gasoline War\", and \"Fire in Heaven\". He alleged that the writers were hired to combine his scripts into the story of \"In Old Oklahoma\" but that he received no compensation.\n\n\n"}
{"id": "38700275", "url": "https://en.wikipedia.org/wiki?curid=38700275", "title": "John A. Laitner", "text": "John A. Laitner\n\nJohn A. \"Skip\" Laitner (born August 23, 1947) is an American-born economist, author and lecturer. He focuses on developing a more robust technology and behavioral characterization of energy efficiency resources for use in energy and climate economic policy models.\n\nLaitner is a Senior Fellow (formerly Director of Economic and Social Analysis) for the American Council for an Energy-Efficient Economy (ACEEE). He leads a team of consultants, the Economic and Human Dimensions Research Associates based in Tucson, Arizona.\n\nLaitner is the author of more than 280 reports, journal articles, and book chapters about environmental, energy, and economic policy. His expertise includes benefit-cost assessments, behavioral assessments, resource costs and constraints, and the net employment and macroeconomic impacts of energy and climate policy scenarios. His research, building on the work of Robert U. Ayres and Benjamin Warr, examines links between energy efficiency and economic productivity. In a new book chapter, Laitner provides a time series dataset that suggests the United States may be only 14 percent energy efficient, a level of inefficiency which could constrain the development of a more robust economy.\n\n2006 to 2012: Director of Economic and Social Analysis for ACEEE. In that capacity, he was responsible for a range of benefit-cost assessments of energy policies as they affect both climate and energy policies and the macroeconomy, including energy prices, net employment, and gross domestic product (GDP) impacts.\n\n1997 to 2006: Senior Economist for Technology Policy within U.S. Environmental Protection Agency’s Office of Atmospheric Programs. Laitner was responsible for analysis and development of policy options for a variety of energy, climate change, and air pollution problems. This focused on the analysis and assessment of climate change policy options designed to provide further understanding of the macroeconomic benefits of energy efficiency and renewable energy at the national and regional level through the better linking of technology costing models and macroeconomic or general equilibrium models.\n\n1985 to 1998: Founder and principal of the consulting group, Economic Research Associates. Clients included a variety of municipalities and state government agencies.\n\n1993 to 1995: Senior Economist and Program Manager for Energy Efficiency and Economic Development, a program initiated by ACEEE. The program focused on the macroeconomic and employment impacts of energy efficiency initiatives.\n\n1983 to 1986: Chief of the Research Division for the Nebraska Energy Office.\n\n1977 to 1983: Co-founder of the Community Action Research Group, an economic and legislative consulting firm based in Ames, Iowa, serving as both the Executive Director and the Director of Research.\n\n\"Citation of Excellence\": The 1997 paper co-authored with Dr. Stephen DeCanio, \"Modeling technological change in energy demand forecasting: a generalized approach,\" in \"Technological Forecasting & Social Change\", Vol. 55, No. 3, received the ANBAR Electronic Intelligence Citation of Excellence, Highest Quality Rating.\n\n\"U.S. EPA Gold Medal Award\": The U.S. EPA's highest honor for working with a team of economists that completed \"complex and rigorous analysis of greenhouse gas mitigation options, leading to President Clinton's announcement of an aggressive climate change policy.\" March 1998.\n\n\"Combined Heat and Power (CHP) Champion\": For policy and analytical work in support of development of combined heat and power (CHP) technologies over the last five years, the US Combined Heat and Power Association gave Laitner an award to acknowledge his contributions to the industry. September 2003.\n\n\n"}
{"id": "38569478", "url": "https://en.wikipedia.org/wiki?curid=38569478", "title": "Jännersdorf Solar Park", "text": "Jännersdorf Solar Park\n\nThe Jännersdorf Solar Park is a photovoltaic power station in Prignitz, Germany. It has a capacity of 40.5 megawatts (MW) and an annual output of 38 GWh. The solar park was developed and built by Parabel AG.\n\nThe project is built on a former military training area on . The project is equipped with 167,550 photovoltaic modules, with 25.6 MW from Trina Solar, 9.6 MW from Suntech Power and 5.3 MW from Hareon. The solar park was connected to the grid on 30 June 2012.\n"}
{"id": "1130845", "url": "https://en.wikipedia.org/wiki?curid=1130845", "title": "Klimavichy", "text": "Klimavichy\n\nKlimavichy (; , ; Łacinka: Klimavičy) is a city in the eastern Belarusian Mahilyow Voblast. Klimavichy is located east of Mahilyow on the bank of Kalinica River and is the administrative center of the Klimavichy Raion since 1924. As of 2009, its population was 17,064.\n\nFirst references in historical documents are dated by 1581 in relation to Mścisław Voivodship of the Grand Duchy of Lithuania. It was granted city status in 1777, and a Coat of Arms in 1781.\nAccording to the census of 1897, the city had 4,714 inhabitants. 50.2 percent were Orthodox, 47.9 percent Jews and 1.5 percent Catholics.\n\nIt was occupied by Nazi Germany during World War II on August 10, 1941 and liberated in October 1943.\n\nIn January 1939, there were 1,693 Jews in the village. There was a Yiddish school until 1938. The majority of the Jews were merchants. \nThere were 4 executions of Jews in Klimovichi. At the end of August 1941, 13 Jews were arrested, taken to the Jewish cemetery in order to dig a pit. They were shot there afterwards. The second execution took place on November 6, 1941. Some Jews were sent to work and the remaining Jews were gathered in a garage near the hospital. Germans took their valuables. The Jews were then displaced near the airport, not far from the village of Dolgaya Gora and were shot by Germans and local policemen. On the same day, the Jews who were sent to work were shot at the same location. According to some sources, between 750 and 900 Jews were killed on that day. After this massacre, 80 Jews remained alive. They were concentrated in one house. At the end of November 1941, these Jews were forced to walk to Melovaya Gora and were shot. In April 1943, the children from mixed-families were imprisoned by the Germans and were shot on April 12, 1943.\n\nThe city and especially its raion heavily suffered during the Chernobyl disaster.\n\n"}
{"id": "8606325", "url": "https://en.wikipedia.org/wiki?curid=8606325", "title": "Kutta–Joukowski theorem", "text": "Kutta–Joukowski theorem\n\nThe Kutta–Joukowski theorem is a fundamental theorem in aerodynamics used for the calculation of lift of an airfoil and any two-dimensional bodies including circular cylinders translating in a uniform fluid at a constant speed large enough so that the flow seen in the body-fixed frame is steady and unseparated. The theorem relates the lift generated by an airfoil to the speed of the airfoil through the fluid, the density of the fluid and the circulation around the airfoil. The circulation is defined as the line integral around a closed loop enclosing the airfoil of the component of the velocity of the fluid tangent to the loop. It is named after Martin Kutta and Nikolai Zhukovsky (or Joukowski) who first developed its key ideas in the early 20th century. Kutta–Joukowski theorem is an inviscid theory, but it is a good approximation for real viscous flow in typical aerodynamic applications. \nKutta–Joukowski theorem relates lift to circulation much like the Magnus effect relates side force (called Magnus force) to rotation. However, the circulation here is not induced by rotation of the airfoil. The fluid flow in the presence of the airfoil can be considered to be the superposition of a translational flow and a rotating flow. This rotating flow is induced by the effects of camber, angle of attack and a sharp trailing edge of the airfoil. It should not be confused with a vortex like a tornado encircling the airfoil. At a large distance from the airfoil, the rotating flow may be regarded as induced by a line vortex (with the rotating line perpendicular to the two-dimensional plane). In the derivation of the Kutta–Joukowski theorem the airfoil is usually mapped onto a circular cylinder. In many text books, the theorem is proved for a circular cylinder and the Joukowski airfoil, but it holds true for general airfoils.\n\nThe theorem applies to two-dimensional flow around a fixed airfoil (or any shape of infinite span). The lift per unit span formula_1of the airfoil is given by\n\nwhere formula_2 and formula_3 are the fluid density and the fluid velocity far upstream of the airfoil, and formula_4 is the circulation defined as the line integral\n\naround a closed contour formula_6 enclosing the airfoil and followed in the positive (anti-clockwise) direction. As explained below, this path must be in a region of potential flow and not in the boundary layer of the cylinder. The integrand formula_7 is the component of the local fluid velocity in the direction tangent to the curve formula_8 and formula_9 is an infinitesimal length on the curve, formula_8. Equation is a form of the \"Kutta–Joukowski theorem.\"\n\nKuethe and Schetzer state the Kutta–Joukowski theorem as follows:\n\nA lift-producing airfoil either has camber or operates at a positive angle of attack, the angle between the chord line and the fluid flow far upstream of the airfoil. Moreover, the airfoil must have a \"sharp\" trailing edge.\n\nAny real fluid is viscous, which implies that the fluid velocity vanishes on the airfoil. Prandtl showed that for large Reynolds number, defined as formula_13, and small angle of attack, the flow around a thin airfoil is composed of a narrow viscous region called the boundary layer near the body and an inviscid flow region outside. In applying the Kutta-Joukowski theorem, the loop must be chosen outside this boundary layer. (For example, the circulation calculated using the loop corresponding to the surface of the airfoil would be zero for a viscous fluid.)\n\nThe sharp trailing edge requirement corresponds physically to a flow in which the fluid moving along the lower and upper surfaces of the airfoil meet smoothly, with no fluid moving around the trailing edge of the airfoil. This is known as the \"Kutta condition.\" \n\nKutta and Joukowski showed that for computing the pressure and lift of a thin airfoil for flow at large Reynolds number and small angle of attack, the flow can be assumed inviscid in the entire region outside the airfoil provided the Kutta condition is imposed. This is known as the potential flow theory and works remarkably well in practice.\n\nTwo derivations are presented below. The first is a heuristic argument, based on physical insight. The second is a formal and technical one, requiring basic vector analysis and complex analysis.\n\nFor a heuristic argument, consider a thin airfoil of chord formula_14 and infinite span, moving through air of density formula_15. Let the airfoil be inclined to the oncoming flow to produce an air speed formula_16 on one side of the airfoil, and an air speed formula_17 on the other side. The circulation is then\n\nThe difference in pressure formula_19 between the two sides of the airfoil can be found by applying Bernoulli's equation:\n\nso the lift force per unit span is\n\nA differential version of this theorem applies on each element of the plate and is the basis of thin-airfoil theory.\n\nThe lift predicted by the Kutta-Joukowski theorem within the framework of inviscid potential flow theory is quite accurate, even for real viscous flow, provided the flow is steady and unseparated.\n\na) Kutta-Joukowski theorem for steady irrotational flow. In deriving the Kutta–Joukowski theorem, the assumption of irrotational flow was used. When there are free vortices outside of the body, as may be the case for a large number of unsteady flows, the flow is rotational. When the flow is rotational, more complicated theories should be used to derive the lift forces. Below are several important examples.\n\nb) Impulsively started flow at small angle of attack. For an impulsively started flow such as obtained by suddenly accelerating an airfoil or setting an angle of attack, there is a vortex sheet continuously shed at the trailing edge and the lift force is unsteady or time-dependent. For small angle of attack starting flow, the vortex sheet follows a planar path, and the curve of the lift coefficient as function of time is given by the Wagner function. In this case the initial lift is one half of the final lift given by the Kutta Joukowski formula. The lift attains 90% of its steady state value when the wing has traveled a distance of about seven chord lengths.\nc) Impulsively started flow at large angle of attack. When the angle of attack is high enough, the trailing edge vortex sheet is initially in a spiral shape and the lift is singular (infinitely large) at the initial time. The lift drops for a very short time period before the usually assumed monotonically increasing lift curve is reached.\n\nd) Starting flow at large angle of attack for wings with sharp leading edges. If, as for a flat plate, the leading edge is also sharp, then vortices also shed at the leading edge and the role of leading edge vortices is two-fold：(1) they are lift increasing when they are still close to the leading edge, so that they elevate the Wagner lift curve,(2) they are detrimental to lift when they are convected to the trailing edge, inducing a new trailing edge vortex spiral moving in the lift decreasing direction. \nFor this type of flow a vortex force line (VFL) map can be used to understand the effect of the different vortices in a variety of situations (including more situations than starting flow) and may be used to improve vortex control to enhance or reduce the lift. The vortex force line map is a two dimensional map on which vortex force lines are displayed. For a vortex at any point in the flow, its lift contribution is proportional to its speed, its circulation and the cosine of the angle between the streamline and the vortex force line. Hence the vortex force line map clearly shows whether a given vortex is lift producing or lift detrimental. \ne) Lagally theorem. When a (mass) source is fixed outside the body, a force correction due to this source can be expressed as the product of the strength of outside source and the induced velocity at this source by all the causes except this source. This is known as the Lagally theorem.\nFor two-dimensional inviscid flow, the classical Kutta Joukowski theorem predicts a zero drag. When, however, there is vortex outside the body, there is a vortex induced drag, in a form similar to the induced lift.\n\nf) Generalized Lagally theorem. For free vortices and other bodies outside one body without bound vorticity and without vortex production, a generalized Lagally theorem holds, with which the forces are expressed as the products of strength of inner singularities (image vortices, sources and doublets inside each body) and the induced velocity at these singularities by all causes except those inside this body. The contribution due to each inner singularity sums up to give the total force. The motion of outside singularities also contributes to forces, and the force component due to this contribution is proportional to the speed of the singularity.\n\ng) Individual force of each body for multiple-body rotational flow. When in addition to multiple free vortices and multiple bodies, there are bound vortices and vortex production on the body surface, the generalized Lagally theorem still holds, but a force due to vortex production exists. This vortex production force is proportional to the vortex production rate and the distance between the vortex pair in production. With this approach, an explicit and algebraic force formula, taking into account of all causes (inner singularities, outside vortices and bodies, motion of all singularities and bodies, and vortex production) holds individually for each body with the role of other bodies represented by additional singularities. Hence a force decomposition according to bodies is possible.\n\nh) General three-dimensional viscous flow. For general three-dimensional, viscous and unsteady flow, force formulas are expressed in integral forms. The volume integration of certain flow quantities, such as vorticity moments, is related to forces. Various forms of integral approach are now available for unbounded domain and for artificially truncated domain. The Kutta Joukowski theorem can be recovered from these approaches when applied to a two-dimensional airfoil and when the flow is steady and unseparated.\n\ni) Lifting line theory for wings, wing-tip vortices and induced drag. A wing has a finite span, and the circulation at any section of the wing varies with the spanwise direction. This variation is compensated by the release of streamwise vortices (called trailing vortices), due to conservation of vorticity or Kelvin Theorem of Circulation Conservation. These streamwise vortices merge to two counter-rotating strong spirals, called wing tip vortices, separated by distance close to the wingspan and may be visible if the sky is cloudy. Treating the trailing vortices as a series of semi-infinite straight line vortices leads to the well-known lifting line theory. By this theory, the wing has a lift force smaller than that predicted by a purely two-dimensional theory using the Kutta–Joukowski theorem. Most importantly, there is an induced drag. This induced drag is a pressure drag which has nothing to do with frictional drag.\n\n\n"}
{"id": "27125511", "url": "https://en.wikipedia.org/wiki?curid=27125511", "title": "List of organic salts", "text": "List of organic salts\n\nNumerous economically or medically significant organic compounds are salts.\n\nThese include:\n\n"}
{"id": "12430286", "url": "https://en.wikipedia.org/wiki?curid=12430286", "title": "Magnetic impurity", "text": "Magnetic impurity\n\nA magnetic impurity is an impurity in a host metal that has a magnetic moment. The magnetic impurity can then interact with the conduction electrons of the metal, leading to interesting physics such as the Kondo effect, and heavy fermion behaviour. Some examples of magnetic impurities that metals can be doped with are iron and nickel. Such an impurity will contribute a Curie-Weiss term to the magnetic susceptibility, \n\nEarly theoretical work concentrated on explaining the trend observed as the impurity was varied across the transition metal group. Based on the idea of a virtual bound state, Anderson proposed a model that was successful in explaining the formation of a localized magnetic moment from a magnetic impurity.\n\n\n"}
{"id": "26578849", "url": "https://en.wikipedia.org/wiki?curid=26578849", "title": "MultiDark", "text": "MultiDark\n\nMultiDark (MULTImessenger Approach for DARK Matter Detection) is a Spanish project, with a stated goal of contributing to the identification and detection of dark matter.\n\nThe project is a grouping effort, involving many researchers in the Spanish community with a special interest in dark Matter. It began on 17 December 2009 and is funded for five years. The project is supported by Consolider-Ingenio, a programme of the Ministry of Economy and Finance.\n\n\n\n"}
{"id": "50768", "url": "https://en.wikipedia.org/wiki?curid=50768", "title": "People mover", "text": "People mover\n\nA people mover or automated people mover (APM) is a type of small scale automated guideway transit system. The term is generally used only to describe systems serving relatively small areas such as airports, downtown districts or theme parks.\n\nThe term was originally applied to three different systems, developed roughly at the same time. One was Skybus, an automated mass transit system prototyped by the Westinghouse Electric Corporation beginning in 1964. The second, alternately called the People Mover and Minirail, opened in Montreal at Expo 67. Finally the last, called PeopleMover or WEDway PeopleMover, was an attraction that was originally presented by Goodyear Tire and Rubber Company and that opened at Disneyland in 1967. Now, however, the term \"people mover\" is generic, and may use technologies such as monorail, duorail, automated guideway transit or maglev. Propulsion may involve conventional on-board electric motors, linear motors or cable traction.\n\nGenerally speaking, larger APMs are referred to by other names. The most generic is \"automated guideway transit\", which encompasses any automated system regardless of size. Some complex APMs deploy fleets of small vehicles over a track network with off-line stations, and supply near non-stop service to passengers. These taxi-like systems are more usually referred to as personal rapid transit (PRT). Larger systems, with vehicles with 20 to 40 passengers, are sometimes referred to as \"group rapid transit\" (GRT), although this term is not particularly common. Other complex APMs have similar characteristics to mass transit systems, and there is no clear cut distinction between a complex APM of this type and an automated mass transit system. Another term \"Light Metro\" is also applied to describe the system worldwide.\n\nOne of the first automated systems for human transportation was the screw-driven 'Never-Stop-Railway', constructed for the British Empire Exhibition at Wembley, London in 1924. This railway consisted of 88 unmanned carriages, on a continuous double track along the northern and eastern sides of the exhibition, with reversing loops at either end.\n\nThe carriages ran on two parallel concrete beams and were guided by pulleys running on the inner side of these concrete beams, and were propelled by gripping a revolving screw thread running between the tracks in a pit; by adjusting the pitch of this thread at different points, the carriages could be sped up, or slowed down to a walking pace at stations, to allow passengers to join and leave. The railway ran reliably for the two years of the exhibition, and was then dismantled.\n\nSmall sections of this track bed, and a nearby heavy rail track bed, have been proposed for reuse.\n\nIn late 1949, Mike Kendall, chief engineer and Chairman of the Board of Stephens-Adamson Manufacturing Company, an Illinois-based manufacturer of conveyor belts and systems, asked Al Neilson, an engineer in the Industrial Products Division of Goodyear Tire and Rubber Co., if Goodyear had ever considered working on People Movers. He felt that with Goodyear's ability to move materials in large quantities on conveyor belts they should consider moving batches of people.\n\nFour years of engineering design, development and testing led to a joint patent being issued for three types of people movers, named Speedwalk, Speedramp, and Carveyor.\nGoodyear would sell the concept and Stephens-Adamson would manufacture and install the components.\n\nA Speedwalk consisted of a flat conveyor belt riding on a series of rollers, or a flat slippery surface, moving at (approximately half the speed of walking). The passengers would walk onto the belt and could stand or walk to the exit point. They were supported by a moving handrail. Customers were expected to include airport terminals, ballparks, train stations, etc. Today, several manufacturers produce similar units called moving walkways.\n\nA Speedramp was very similar to a Speedwalk but it was used to change elevations; up or down a floor level. This could have been accomplished by an escalator, but the Speedramp would allow wheeled luggage, small handcarts etc. to ride the belt at an operating cost predicted to be much lower than escalators or elevators. The first successful installation of a Speedramp was in the spring of 1954 at the Hudson and Manhattan Railroad Station in Jersey City, New Jersey to connect the Erie Railroad to the Hudson and Manhattan Tubes. This unit was long with a rise of on a 15 degree grade, and only cost $75,000.\n\nA Carveyor consisted of many small cubicles or cars carrying ten people riding on a flat conveyor belt from point A to point B. The belt rode on a series of motorized rollers. The purpose of the motorized rollers was to facilitate the gradual acceleration and deceleration speeds on the conveyor belt and overcome the tendency of all belts to stretch at start up and during shutdown. At point \"A\" passengers would enter a Speedwalk running parallel to the belts and cars of the Carveyor. The cars would be moving at the same speed as the Speedwalk; the passengers would enter the cars and be seated, while the motorized rollers would increase the speed of the cars up to the traveling speed (which would be preset depending on the distance to be covered). At point B Passengers could disembark and by means of a series of flat slower belts (Speedwalks) go to other Carveyors to other destinations or out to the street. The cars at point B would continue on rollers around a semicircle and then reverse the process carrying passengers back to point A. The initial installation was to be the 42nd Street Shuttle in New York City between Times Square and Grand Central station.\n\nThe first mention of the Carveyor in a hardback book was in \"There's Adventure in Civil Engineering\" by Neil P. Ruzic (1958), one of a series of books published by \"Popular Mechanics\" in the 1950s in their \"Career\" series. In the book the Carveyor was already installed and operational in downtown Los Angeles.\n\nColonel Sydney H. Bingham, Chairman of the New York City Board of Transportation, had several meetings with a group of architects who were trying to revamp the whole New York City Subway system in the heart of town to connect Pennsylvania Station, Madison Square Garden, Times Square, Grand Central and several new office complexes together. Several of these architects were involved in other programs, and in later years many variations of the Carveyor people movers were developed.\n\nIn November 1954 the New York City Transit Authority issued an order to Goodyear and Stephens-Adamson to build a complete Carveyor system between Times Square and Grand Central. A brief summary and confirmation can be found in \"Time\" magazine on November 15, 1954. under the heading \"Subway of the Future\". The cost was to be under $4 million, but the order was never fulfilled due to political difficulties.\n\nChocolate World in Hershey, Pennsylvania, Disneyland in California, and Walt Disney World in Florida are among many locations that have used variations of the Carveyor concept.\n\nThe term 'people mover' was used by Walt Disney, when he and his Imagineers were working on the new 1967 Tomorrowland at Disneyland. The name was used as a working title for a new attraction, the PeopleMover. According to Imagineer Bob Gurr, \"the name got stuck,\" and it was no longer a working title.\n\nStarting in the late 1960s and into the 1970s, people movers were the topic of intense development around the world. Worried about the growing congestion and pollution in downtown areas due to the spread of cars, many countries started studying mass transit systems that would lower capital costs to the point where any city could afford to deploy them. Most of these systems used elevated guideways, which were much less expensive to deploy than underground tunnels. However, elevating the track causes problems with noise, so traditional steel-wheel-on-rail solutions were rare as they squealed when rounding bends in the rails. Rubber tired solutions were common, but some systems used hovercraft techniques or various magnetic levitation systems.\n\nTwo major government funded APM projects are notable. In Germany, Mannesmann Demag and Messerschmitt-Bölkow-Blohm developed a system known as Cabinentaxi during the 1970s. Cabinetaxi featured small cars with from four to eight seats that were called to pick up passengers on-demand and drove directly to their destination. The stations were \"offline\", allowing the cabs to stop by moving off the main lines while other cars continued to their destinations. The system was designed so the cars could be adapted to run on top or bottom of the track (but not easily converted from one to the other), allowing dual-track movements from a single elevated guideway only slightly wider than the cars. A test track was completed in 1975 and ran until development was completed in 1979, but no deployments followed and the companies abandoned the system shortly thereafter.\n\nIn the U.S., a 1966 federal bill provided funding that led to the development of APM systems under the Downtown People Mover Program. Four systems were developed, Rohr's ROMAG, LTV's AirTrans, Ford's APT and Otis Elevator's hovercraft design. A major presentation of the systems was organized as TRANSPO'72 at Dulles Airport where the various systems were presented to delegations from numerous cities in the US. Prototype systems and test tracks were built during the 1970s. One notable example was Pittsburgh's Skybus, which was proposed by the Port Authority of Allegheny County to replace its streetcar system, which, having large stretches of private right of way, was not suited for bus conversion. A short demonstration line was set up in South Park and large tracts of land were secured for its facilities. However, opposition arose to the notion that it would replace the streetcar system. This, combined with the immaturity of the technology and other factors, led the Port Authority to abandon the project and pursue alternatives. By the start of the 1980s most politicians had lost interest in the concept and the project was repeatedly de-funded in the early 1980s. Only two APMs were developed as a part of the People Mover Program in the US, the Metromover in Miami, and the Detroit People Mover. The Jacksonville Skyway was built in the late 1980s.\n\nAlthough many government-funded systems were generally considered failures, several APM systems developed by other groups have been much more successful. Lighter systems with shorter tracks are widely deployed at airports; the world's first airport people movers, the Tampa International Airport People Movers, were installed in 1971 at Tampa International Airport in the United States. APMs have now become common at large airports and hospitals in the United States.\n\nDriver-less metros have become common in Europe and parts of Asia. The economics of automated trains tend to reduce the scale so tied to \"mass\" transit (the largest operating expense is the driver's salary, which is only affordable if very large numbers of passengers are paying fares), so that small-scale installations are feasible. Thus cities normally thought of as too small to build a metro (e.g. Rennes, Lausanne, Brescia, etc.) are now doing so.\n\nOn September 30, 2006, the Peachliner in Komaki, Aichi Prefecture, Japan became that nation's first people mover to cease operations.\n\n\n\nMany large international airports around the world feature people mover systems to transport passengers between terminals or within a terminal itself. Some people mover systems at airports connect with other public transportation systems to allow passengers to travel into the airport's city.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe folllowing are Monorails which are considered as people movers also.\n\n\n\n"}
{"id": "38614492", "url": "https://en.wikipedia.org/wiki?curid=38614492", "title": "Petro-aggression", "text": "Petro-aggression\n\nPetro-aggression is the tendency for a petrostate to be involved in international conflicts, or to be the targets of them. One study suggests that petro-revolutionary states are 250 percent more likely to instigate international conflicts than a typical country.\n\nIt remains unclear whether the pattern of petro-aggression found in oil-rich countries also applies to other natural resources besides oil.\n\n\n\n"}
{"id": "653886", "url": "https://en.wikipedia.org/wiki?curid=653886", "title": "Petrocurrency", "text": "Petrocurrency\n\nPetrocurrency, is a neologism used with three distinct meanings, often confused:\n\n\n\"Petrocurrency\" or (more commonly) \"petrodollars\" are popular shorthand for revenues from petroleum exports, mainly from the OPEC members plus Russia and Norway. Especially during periods of historically expensive oil, the associated financial flows can reach a scale of hundreds of billions of US dollar-equivalents per year – including a wide range of transactions in a variety of currencies, some pegged to the US dollar and some not.\n\nThe pound sterling has sometimes been regarded as a petrocurrency as a result of North Sea oil exports.\n\nThe Dutch guilder was once regarded as a petrocurrency due to its large quantities of natural gas and North Sea oil exports. The Dutch Guilder strengthened greatly in the 1970s, after OPEC began a series of price hikes throughout the decade that consequently increased the value of all oil producing nations' currencies. However, as a result of the appreciation of the Guilder, industrial manufacturing and services in the Netherlands during the 1970s and into the 1980s were crowded out of the larger national economy, and the country became increasingly non-competitive on world markets due to the high cost of Dutch industrial and service exports . This phenomenon is often referred to in economics literature as Dutch disease.\n\nThe Canadian dollar is increasingly viewed as a petrocurrency in the 21st Century. Generally speaking, as the price of oil rises, oil-related export revenues rise for an oil exporting nation, and thus constitute a larger monetary component of exports. Thus it has been for Canada. As their oil sands deposits have been increasingly exploited and sold on the international market, movements of the Canadian dollar have become increasingly correlated with the price of oil. For example, the exchange rate of Canadian dollars for Japanese yen (99% of Japan's oil is imported) is 85% correlated with crude prices. As long as oil exports remain a strong component of Canada’s exports, oil prices will influence the value of the Canadian dollar. If the share of oil and gas exports increases further, the link between oil prices and the exchange rate may become even stronger.\n\nAs the world's dominant reserve currency the United States dollar has been a major currency for trading oil (sometimes the term 'Petrodollar' is mistakenly used to refer to this concept). In August 2018, Venezuela joined the group of countries that allow their oil to be purchased in currencies other than US Dollars, thus allowing purchases in Euros, Yuan and other directly convertible currencies. Other nations that permit this include Iran.\n\nAfter WWII, international oil prices were for some time based on discounts or premiums relative to that for oil in the Gulf of Mexico.\n\nAfter the Bretton Woods conference in the year 1944, the UK and its allies discontinued linking their currencies with gold; however, the US dollar continued to be pegged to gold, at $35 per ounce -- from 1941 to 1971.\n\nPresident Nixon cancelled the fixed-rate convertibility of US dollars to gold in 1971. In the absence of fixed value convertibility to gold, compared to other currencies, the US dollar subsequently deteriorated in value for several years, making fixed USD to local currency exchange rates unsustainable for most countries.\n\nSince the agreements of 1971 and 1973, OPEC oil is generally quoted in US dollars, sometimes referred to as petrodollars.\n\nIn October 1973, OPEC declared an oil embargo in response to the United States' and Western Europe's support of Israel in the Yom Kippur War.\n\nSince the beginning of 2003, Iran has required payment in euros for exports to Asia and Europe. The government opened an Iranian Oil Bourse on the free trade zone on the island of Kish, for the express purpose of trading oil priced in other currencies, including euros.\n\nThe shale oil boom in the USA starting in the early 2000s through 2010s (as well as increased production capacity in many other countries) greatly limited OPEC's ability to control oil prices. Consequently, due to a drastic fall in Nymex crude oil price to as low as $35.35 dollars per barrel in 2015, many oil-exporting countries have had severe problems in balancing their budget.\n\nBy 2016, many oil exporting countries had been adversely affected by low oil prices including Russia, Saudi Arabia, Azerbaijan, Venezuela and Nigeria.\n\nThe petro, or \"petromoneda\", launched in February 2018, is a cryptocurrency developed by the government of Venezuela. Announced in December 2017, it is claimed to be backed by the country's oil and mineral reserves, and it is intended to supplement Venezuela's plummeting bolívar fuerte currency, purportedly as a means of circumventing U.S. sanctions and accessing international financing.\n\nIn March 2018, China opened a futures market denominated in Yuan which could encourage the use of its currency as a petrocurrency.\n\n\n"}
{"id": "371942", "url": "https://en.wikipedia.org/wiki?curid=371942", "title": "Polyvinylidene fluoride", "text": "Polyvinylidene fluoride\n\nPolyvinylidene fluoride or polyvinylidene difluoride (PVDF) is a highly non-reactive thermoplastic fluoropolymer produced by the polymerization of vinylidene difluoride.\n\nPVDF is a specialty plastic used in applications requiring the highest purity, as well as resistance to solvents, acids and bases. Compared to other fluoropolymers, like polytetrafluoroethylene (Teflon), PVDF has a low density (1.78 g/cm).\n\nIt is available as piping products, sheet, tubing, films, plate and an insulator for premium wire. It can be injected, molded or welded and is commonly used in the chemical, semiconductor, medical and defense industries, as well as in lithium-ion batteries. It is also available as a crosslinked closed-cell foam, used increasingly in aviation and aerospace applications. It can also be used in repeated contact with food products, as it is FDA-compliant and absolutely non-toxic.\n\nAs a fine powder grade, it is an ingredient in high-end paints for metals. These PVDF paints have extremely good gloss and color retention. They are in use on many prominent buildings around the world, such as the Petronas Towers in Malaysia and Taipei 101 in Taiwan, as well as on commercial and residential metal roofing.\n\nPVDF membranes are used in western blots for the immobilization of proteins, due to its non-specific affinity for amino acids.\n\nPVDF is also used as a binder component for the carbon electrode in supercapacitors and for other electrochemical applications.\n\nPVDF is sold under a variety of brand names including KF (Kureha), Hylar (Solvay), Kynar (Arkema) and Solef (Solvay).\n\nIn 1969, strong piezoelectricity was observed in PVDF, with the piezoelectric coefficient of poled (placed under a strong electric field to induce a net dipole moment) thin films as large as 6–7 pC/N: 10 times larger than that observed in any other polymer.\n\nPVDF has a glass transition temperature (T) of about −35 °C and is typically 50–60% crystalline. To give the material its piezoelectric properties, it is mechanically stretched to orient the molecular chains and then poled under tension. PVDF exists in several forms: alpha (TGTG'), beta (TTTT), and gamma (TTTGTTTG') phases, depending on the chain conformations as trans (T) or gauche (G) linkages. When poled, PVDF is a ferroelectric polymer, exhibiting efficient piezoelectric and pyroelectric properties. These characteristics make it useful in sensor and battery applications. Thin films of PVDF are used in some newer thermal camera sensors.\n\nUnlike other popular piezoelectric materials, such as lead zirconate titanate (PZT), PVDF has a negative d value. Physically, this means that PVDF will compress instead of expand or vice versa when exposed to the same electric field.\n\nPVDF may be synthesized from the gaseous vinylidene fluoride (VDF) monomer via a free radical (or controlled radical) polymerization process. This may be followed by processes such as melt casting, or processing from a solution (e.g. solution casting, spin coating, and film casting). Langmuir-Blodgett films have also been made. In the case of solution-based processing, typical solvents used include dimethylformamide and the more volatile butanone. In aqueous emulsion polymerization, the fluorosurfactant perfluorononanoic acid is used in anion form as a processing aid by solubilizing monomers. Compared to other fluoropolymers, it has an easier melt process because of its relatively low melting point of around 177 °C.\n\nProcessed materials are typically in the non-piezoelectric alpha phase. The material must either be stretched or annealed to obtain the piezoelectric beta phase. The exception to this is for PVDF thin films (thickness in the order of micrometres). Residual stresses between thin films and the substrates on which they are processed are great enough to cause the beta phase to form.\n\nIn order to obtain a piezoelectric response, the material must first be poled in a large electric field. Poling of the material typically requires an external field of above 30 MV/m. Thick films (typically >100 µm) must be heated during the poling process in order to achieve a large piezoelectric response. Thick films are usually heated to 70–100 °C during the poling process.\n\nA quantitative defluorination process was described by mechanochemistry, for safe eco-friendly PVDF waste processing.\n\nPVDF is commonly used as insulation on electrical wires, because of its combination of flexibility, low weight, low thermal conductivity, high chemical corrosion resistance, and heat resistance. Most of the narrow 30-gauge wire used in wire wrap circuit assembly and printed circuit board rework is PVDF-insulated. In this use the wire is generally referred to as \"Kynar wire\", from the trade name.\n\nThe piezoelectric properties of PVDF are exploited in the manufacture of tactile sensor arrays, inexpensive strain gauges, and lightweight audio transducers. Piezoelectric panels made of PVDF are used on the Venetia Burney Student Dust Counter, a scientific instrument of the New Horizons space probe that measures dust density in the outer solar system.\n\nPVDF is the standard binder material used in the production of composite electrodes for lithium-ion batteries. weight solution of PVDF dissolved in \"N\"-methyl-2-pyrrolidone (NMP) is mixed with an active lithium storage material such as graphite, silicon, tin, LiCoO, LiMnO, or LiFePO and a conductive additive such as carbon black or carbon nanofibers. This slurry is cast onto a metallic current collector and the NMP is evaporated to form a composite or paste electrode. PVDF is used because it is chemically inert over the potential range used, and does not react with the electrolyte or lithium.\n\nIn the biomedical sciences, PVDF is used in immunoblotting as an artificial membrane (usually with 0.22 or 0.45 micrometre pore sizes), on which proteins are transferred using electricity (see western blotting). PVDF is resistant to solvents and, therefore, these membranes can be easily stripped and reused to look at other proteins. PVDF membranes may be used in other biomedical applications as part of a membrane filtration device, often in the form of a syringe filter or wheel filter. The various properties of this material, such as heat resistance, resistance to chemical corrosion, and low protein binding properties, make this material valuable in the biomedical sciences for preparation of medications as a sterilizing filter, and as a filter to prepare samples for analytical techniques such as high performance liquid chromatography (HPLC), where small amounts of particulate matter can damage sensitive and expensive equipment.\n\nPVDF is used for specialty monofilament fishing lines, sold as fluorocarbon replacements for nylon monofilament. The surface is harder, so it is more resistant to abrasion and sharp fish teeth. Its optical density is lower than nylon, which makes the line less discernible to sharp fish eyes. It is also denser than nylon, making it sink faster towards fish.\n\nPVDF transducers have the advantage of being dynamically more suitable for modal testing than semiconductor piezoresistive transducers, and more compliant for structural integration than piezoceramic transducers. For those reasons, the use of PVDF active sensors is a keystone for the development of future structural health monitoring methods, due to their low cost and compliance.\n\nCopolymers of PVDF are also used in piezoelectric and electrostrictive applications. One of the most commonly used copolymers is P(VDF-trifluoroethylene), usually available in ratios of about 50:50 wt% and 65:35 wt% (equivalent to about 56:44 mol% and 70:30 mol%). Another one is P(VDF-tetrafluoroethylene). They improve the piezoelectric response by improving the crystallinity of the material.\n\nWhile the copolymers' unit structures are less polar than that of pure PVDF, the copolymers typically have a much higher crystallinity. This results in a larger piezoelectric response: d values for P(VDF-TFE) have been recorded to be as high as −38 pC/N versus −33 pC/N in pure PVDF.\n\nTerpolymers of PVDF are the most promising one in terms of electromechanically induced strain. The most commonly used PVDF-based terpolymers are P(VDF-TrFE-CTFE) and P(VDF-TrFE-CFE). This relaxor-based ferroelectric terpolymer is produced by random incorporation of the bulky third monomer (chlorotrifluoroethylene, CTFE) into the polymer chain of P(VDF-TrFE) copolymer (which is ferroelectric in nature). This random incorporation of CTFE in P(VDF-TrFE) copolymer disrupts the long-range ordering of the ferroelectric polar phase, resulting in the formation of nano-polar domains. When an electric field is applied, the disordered nano-polar domains change their conformation to all-trans conformation, which leads to large electrostrictive strain and a high room-temperature dielectric constant of ~50.\n\n"}
{"id": "457968", "url": "https://en.wikipedia.org/wiki?curid=457968", "title": "Power strip", "text": "Power strip\n\nA power strip (also known as an extension block, power board, power bar, plug board, trailing gang, trailing socket, plug bar, trailer lead, multi-socket, multi-box, multiple socket, multiple outlet, polysocket and by many other variations) is a block of electrical sockets that attaches to the end of a flexible cable (typically with a mains plug on the other end), allowing multiple electrical devices to be powered from a single electrical socket. Power strips are often used when many electrical devices are in proximity, such as for audio, video, computer systems, appliances, power tools, and lighting. Power strips often include a circuit breaker to interrupt the electric current in case of an overload or a short circuit. Some power strips provide protection against electrical power surges. Typical housing styles may include strip, rack-mount, under-monitor and direct plug-in.\n\nSome power strips include a master switch to turn all devices on and off. This can be used with simple devices, such as lights, but not with most computers, which must use shutdown commands from the software. Computers may have open files, which may be damaged if the power is simply turned off.\n\nSome power strips have individually switched outlets.\n\n\"Master/slave\" strips can detect one \"master\" device being turned off (such as the PC itself in a computer setup, or a TV in a home theatre) and turn everything else on or off accordingly.\n\nRemote control strips are used in data centers, to allow computer systems or other devices to be remotely restarted, often over the Internet (although this leaves them vulnerable to outside attacks).\n\nMany power strips have a neon or LED indicator light or one per output socket to show when power is on. Better-quality surge-protected strips have additional lights to indicate the status of the surge protection system, however these are not always reliable as an indicator.\n\nSome power strips have energy-saving features, which switch off the strip if appliances go into standby mode. Using a sensor circuit, they detect if the level of power through the socket is in standby mode (less than 30 watts), and if so they will turn off that socket. This reduces the consumption of standby power used by computer peripherals and other equipment when not in use, saving money and energy Some more-sophisticated power strips have a master and slave socket arrangement, and when the \"master\" socket detects standby mode in the attached appliance's current it turns off the whole strip.\n\nHowever, there can be problems detecting standby power in appliances that use more power in standby mode (such as plasma televisions) as they will always appear to the power strip to be switched on. When using a master–slave power strip, one way to avoid such problems is to plug an appliance with a lower standby wattage (such as a DVD player) into the master socket, using it as the master control instead.\n\nA different power strip design intended to save energy uses a passive infrared (PIR) or ultrasonic sound detector to determine if a person is nearby. If the sensors don't detect any motion for a preset period of time, the strip shuts off several outlets, while leaving one outlet on for devices that should not be powered off. These so-called \"smart power strips\" are intended to be installed in offices, to shut down equipment when the office is unoccupied.\n\nIt is recommended that appliances that need a controlled shutdown sequence (such as many ink-jet printers) \"not\" be plugged into a slave socket on such a strip as it can damage them if they are switched off incorrectly (for example the inkjet printer may not have capped the print head in time, and consequently the ink will dry and clog the print head.)\n\nWithin Europe, power strips with energy-saving features are within the scope of the Low Voltage Directive 2006/95/EC and the EMC Directive 2004/108/EC and require a CE mark.\n\nIn some countries where multiple socket types are in use, a single power strip can have two or more kinds of socket. Socket arrangement varies considerably, but for physical access reasons there are rarely more than two rows. In Europe, power strips without surge suppression are normally single row, but models with surge suppression are supplied both in single and double row configurations.\n\nIf sockets on a power strip are grouped closely together, a cable with a large \"wall wart\" transformer at its end may cover up multiple sockets. Various designs address this problem, some by simply increasing the spacing between outlets. Other designs include receptacles which rotate in their housing, or multiple short receptacle cords feeding from a central hub. A simple DIY method for adapting problematic power strips arrangements to large \"wall warts\" is to use a three-way socket adapter to extend the socket above its neighbors, providing the required clearance. The PowerCube adapter is arranged as a cube, meaning the adapters do not fight for space next to each other.\n\nMany power strips have built-in surge protectors or EMI/RFI filters: these are sometimes described as surge suppressors or electrical line conditioners. Some also provide surge suppression for phone lines, TV cable coax, or network cable. Unprotected power strips are often mistakenly called \"surge suppressors\" or \"surge protectors\" even though they may have \"no ability to suppress surges\".\n\nSurge suppression is usually provided by one or more metal-oxide varistors (MOVs), which are inexpensive two-terminal semiconductors. These act as very high speed switches, momentarily limiting the peak voltage across their terminals. By design, MOV surge limiters are selected to trigger at a voltage somewhat above the local mains supply voltage, so that they do not clip normal voltage peaks, but clip abnormal higher voltages. In the US, this is (nominally) 120 VAC. It should be borne in mind that this voltage specification is RMS, not peak, and also that it is only a nominal (approximate) value.\n\nMains electrical power circuits are generally grounded (earthed), so there will be a live (hot) wire, a neutral wire, and a ground wire. Low-cost power strips often come with only one MOV mounted between the live and neutral wires. More complete (and desirable) power strips will have three MOVs, connected between each possible pair of wires. Since MOVs degrade somewhat each time they are triggered, power strips using them have a limited, and unpredictable, protective life. Some power strips have \"protection status\" lights which are designed to turn off if protective MOVs connected to the live wire have failed, but such simple circuits cannot detect all failure modes (such as failure of a MOV connected between neutral and ground).\n\nThe surge-induced triggering of MOVs can cause damage to an upstream device, such as an uninterruptible power supply (UPS), which typically sees an overload condition while the surge is being suppressed. Therefore, it is recommended not to connect a surge-protected power strip to a UPS, but instead to rely solely on surge protection provided by the UPS itself.\n\nMore-elaborate power strips may use inductor-capacitor networks to achieve a similar effect of protecting equipment from high voltage spikes on the mains circuit. These more-expensive arrangements are much less prone to silent degradation than MOVs, and often have monitoring lights that indicate whether the protective circuitry is still connected.\n\nWithin the EU, power strips with surge suppression circuits can demonstrate compliance with the (LVD) Low Voltage Directive 2006/95/EC by complying with the requirements of EN 61643-11:2002+A1. The standard covers both the performance of the surge suppression circuit and their safety. Likewise, power strips with telecoms surge suppression circuits can demonstrate compliance with the LVD by complying with the requirements of EN 61643-21:2001.\n\nConnecting MOV-protected power strips in a \"daisy chain\" (in a series, with each power strip plugged into a previous one in the chain) does not necessarily increase the protection they provide. Connecting them in this manner effectively connects their surge protection components in parallel, in theory spreading any potential surge across each surge protector. However, due to manufacturing variations between the MOVs, the surge energy will not be spread evenly, and will typically go through the one that triggers first.\n\nDaisy chaining of power strips (known in building and electric codes as multi-plug adapters or relocatable power taps), whether surge protected or not, is specifically against most codes. As an example, the International Code Council's \"International Fire Code 2009 Edition\" in 605.4.2 states, \"Relocatable power taps shall be directly connected to permanently installed receptacles.\"\n\nWhere the current rating of the socket outlet, plug and lead of the power strip is equal to the rating of the circuit breaker supplying the circuit concerned, additional overload protection for the power strip is unnecessary, since the existing circuit breaker will provide the required protection.\nHowever, where the rating of a socket outlet (and, hence, the plug and lead of the power strip) is less than the rating of the circuit breaker supplying the circuit concerned, overload protection for the power strip and its supply cable is necessary.\n\nIn the UK, standard and sockets are rated at 13 A but are provided on circuits protected by circuit breakers of up to 32 A.\nHowever, UK Consumer Protection legislation requires that plug-in domestic electrical goods must be provided with plugs to BS 1363, which include a fuse rated at not more than 13 A.\nHence, in the UK and in other countries using , this fused plug provides overload protection for any power strip. The fuse must be replaced if the power strip is overloaded, causing the fuse to operate.\nIn Australia and New Zealand the rating for a standard socket outlet is 10 Amperes but these outlets are provided on circuits protected by circuit breakers of up to 20 A. Also, it is possible to insert an Australian/NZ 10 A plug into socket outlets rated at up to 32 A.\nHence, all power strips sold in Australia and New Zealand are required to have overload protection so that, if the total current drawn exceeds 10 A, the inbuilt circuit breaker will operate and disconnect \"all\" connected devices.\nThese power strips have a reset button for the circuit breaker, which is used to return the strip to service after an overload has caused it to trip.\n\nElectrical overloading can be a problem with any sort of power distribution adapter. This is especially likely if multiple high-power appliances are used, such as those with heating elements, like room heaters or electric frying pans. Power strips may have a circuit breaker integrated to prevent overload. In the UK, power strips are required to be protected by the fuse in the BS 1363 plug. Some also feature a 13A BS1362 fuse in the socket end.\n\nPower strips are generally considered a safer alternative to “double adapters”, “two-way plugs”, “three-way plugs”, or “cube taps” which plug directly into the socket with no lead for multiple appliances. These low-cost adapters are generally not fused (although more modern ones in the UK and Ireland are). Therefore, in many cases the only protection against overload is the branch circuit fuse which may well have a rating higher than the adapter. The weight of the plugs pulling on the adapter (and often pulling it part way out of the socket) can also be a problem if adapters are stacked or if they are used with brick-style power supplies. Such adapters, while still available, have largely fallen out of use in some countries (although two- and three-way adapters are still common in the US, UK, and Ireland).\n\nWhen plugging a device into a power strip, a buildup of carbon or dust can cause sparking to occur. This generally doesn’t pose much of a risk in a non-explosive atmosphere, but explosive atmospheres (for example, near a gasoline refueling station or a solvent cleaning facility) require specialized explosion-proof sealed electrical equipment.\n\n\nIn Europe, plugs and sockets without additional control or surge protection circuits are outside the scope of the Low Voltage Directive 2006/95/EC and controlled by National regulations, and therefore must not be CE marked. In the UK the legal requirements for plugs and sockets are listed in Statutory Instrument 1994 No. 1768, The Plugs and Sockets etc. (Safety) Regulations 1994. This regulation lists the requirements for all domestic plugs and sockets; including socket outlet units (power strips), see Electrical Equipment - Requirements for Plugs & Sockets etc. - Guidance notes on the UK Plugs & Sockets etc. (Safety) Regulations 1994 (S.I. 1994/1768).\n\nThe regulation requires all socket outlet units to comply with the requirements of BS 1363-2 Specification for 13A switched and unswitched socket-outlets and with the requirements of BS 5733 Specification for General requirements for electrical accessories. Sockets and socket outlets do not require independent approval under the regulations. Any plug fitted to the socket outlet unit must comply with the requirements of BS 1363-1 Specification for rewirable and non-rewirable 13A fused plugs. Plugs must also be independently approved and marked in accordance with the requirements of the regulation.\n\nIf a socket outlet unit contains additional control circuits or surge protection circuits they will fall within the scope of the Low Voltage Directive 2006/95/EC and must be CE marked. Socket outlet units with control circuits also fall within the scope of the EMC Directive 2004/108/EC.\n\nExamples of power strips exist in the U.S. patent system dating back as far as 1929, starting with the creation of Carl M. Peterson's \"Table Tap\". Another early example was created by Allied Electric Products in 1950.\n\nPerhaps the first modern designs for the power strip were created by the U.S. firm Fedtro, which filed two patents in 1970 for designs that hew close to designs used in the modern day.\n\nOne early iteration, called a \"power board\", was invented in 1972 by Australian electrical engineer Peter Talbot working under Frank Bannigan, Managing Director of Australian company Kambrook. The product was hugely successful, however, it was not patented and market share was eventually lost to other manufacturers.\n\n\n"}
{"id": "52837586", "url": "https://en.wikipedia.org/wiki?curid=52837586", "title": "Pseudo Jahn–Teller effect", "text": "Pseudo Jahn–Teller effect\n\nThe pseudo Jahn–Teller effect (PJTE), occasionally also known as second-order JTE, is a direct extension of the Jahn–Teller effect (JTE) where spontaneous symmetry breaking in polyatomic systems (molecules and solids) occurs even in nondegenerate electronic states under the influence of sufficiently low-lying excited states of appropriate symmetry.\n\"The pseudo Jahn–Teller effect is the only source of instability and distortions of high-symmetry configurations of polyatomic systems in nondegenerate states, and it contributes significantly to the instability in degenerate states\".\n\nIn the first publication in 1957 on the (what is now called) pseudo Jahn–Teller effect (PJTE), Öpik and Pryce showed that a small splitting of the degenerate electronic term does not necessarily remove the instability and distortion of the polyatomic system induced by the Jahn–Teller effect (JTE), provided the splitting is sufficiently small (the two split states remain “pseudodegenerate”), and the vibronic coupling between them is strong enough. From another perspective, the idea of vibronic admixture of different electronic terms by low-symmetry vibrations was introduced in 1933 by Herzberg and Teller to explore forbidden electronic transitions, and extended in the late 1950s by Murrell and Pople and by Liehr. The role of excited states in softening the ground state with respect to distortions in benzene was demonstrated qualitatively by Longuet-Higgins and Salem by analyzing the π electron levels in the Hückel approximation, while a general second-order perturbation formula for such vibronic softening was derived by Bader in 1960. In 1961 Fulton and Gouterman presented a symmetry analysis of the two-level case in dimers and introduced the term \"pseudo Jahn–Teller effect\". The first application of the PJTE to solving a major solid-state structural problem with regard to the origin of ferroelectricity was published in 1966 by Bersuker, and the first book on the JTE covering the PJTE was published in 1972 by Englman. The second-order perturbation approach was employed by Pearson in 1975 to predict instabilities and distortions in molecular systems; he called it \"second-order JTE\" (SOJTE). The first explanation of PJT origin of puckering distortion as due to the vibronic coupling to the excited state was given for the NH radical by Borden, Davidson, and Feller in 1980 (they called it \"piramidalization\"). Methods of numerical calculation of the PJT vibronic coupling effect with applications to spectroscopic problems were developed in the early 1980s\n\nThe equilibrium geometry of any polyatomic system in nondegenerate states is defined as corresponding to the point of the minimum of the adiabatic potential energy surface (APES), where its first derivatives are zero and the second derivatives are positive. Denote the energy of the system as a function of normal displacements Q by E(Q). At the point of minimum (Q=0) of the APES the curvature K of E(Q) in the Q direction,\n\nis positive, K > 0. Very often the geometry of the system at this point of equilibrium on the APES does not coincide with the highest possible (or even with any high) symmetry expected from general symmetry considerations. For instance, linear molecules are bent at equilibrium, planar molecules are puckered, octahedral complexes are elongated, or compressed, or tilted, cubic crystals are tetragonally polarized (or have several structural phases), etc. The PJTE is the general driving force of all these distortions if they occur in the nondegenerate electronic states of the high-symmetry (reference) geometry.\n\nK=<ψ|(dH/dQ)|ψ> (3)\n\nK=-Σ|<ψ|(dH/dQ)|ψ>|/[E-E] (4)\nwhere ψ are the wavefunctions of the excited states, and the K expression, obtained as a second order perturbation correction, is always negative, K<0. Therefore, if K>0, the K contribution is the only source of instability. The matrix elements in Eq. (4) are off-diagonal vibronic coupling constants,\n\nThey measure the mixing of the ground and excited states under the nuclear displacements Q, and therefore K is termed the vibronic contribution. Together with the K value and the energy gap 2∆=E-E between the mixing states, F are the main parameters of the PJTE (see below). \nIn a series of papers beginning in 1980 (see references in ) it was proved that for any polyatomic system in the high-symmetry configuration\n\nK>0, (6)\n\nand hence the vibronic contribution is the only source of instability of any polyatomic system in nondegenerate states.\nIf K >0 for the high-symmetry configuration of any polyatomic system, then a negative curvature, K =(K + K)< 0, can be achieved only due to the negative vibronic coupling component K, and only if |K|> K. It follows that any distortion of the high-symmetry configuration is due to, and only to the mixing of its ground state with excited electronic states by the distortive nuclear displacements realized via the vibronic coupling in Eq. (5). The latter softens the system with respect to certain nuclear displacements (K<0), and if this softening is larger than the original (nonvibronic) hardness K in this direction, the system becomes unstable with respect to the distortions under consideration, leading to its equilibrium geometry of lower symmetry, or to dissociation. \nThere are many cases when neither the ground state is degenerate, nor is there a significant vibronic coupling to the lowest excited states to realize PJTE instability of the high-symmetry configuration of the system, and still there is a ground state equilibrium configuration with lower symmetry. In such cases the symmetry breaking is produced by a hidden PJTE (similar to a hidden JTE); it takes place due to a strong PJTE mixing of two excited states, one of which crosses the ground state to create a new (lower) minimum of the APES with a distorted configuration.\n\nThe use of the second order perturbation correction, Eq. (4), for the calculation of the K value in the case of PJTE instability is incorrect because in this case |K|>K, meaning the first perturbation correction is larger than the main term, and hence the criterion of applicability of the perturbation theory in its simplest form does not hold. In this case, we should consider the contribution of the lowest excited states (that make the total curvature negative) in a pseudodegenerate problem of perturbation theory. For the simplest case when only one excited state creates the main instability of the ground state, we can treat the problem via a pseudodegenerate two-level problem, including the contribution of the higher, weaker-influencing states as a second order correction.\nIn the PJTE two-level problem we have two electronic states of the high-symmetry configuration, ground β and excited γ, separated by an energy interval of 2Δ, that become mixed under nuclear displacements of certain symmetry Q=Q; the denotations α, β, and γ indicate, respectively, the irreducible representations to which the symmetry coordinate and the two states belong. In essence, this is the original formulation of the PJTE. Assuming that the excited state is sufficiently close to the ground one, the vibronic coupling between them should be treated as a perturbation problem for two near-degenerate states. With both interacting states non-degenerate the vibronic coupling constant F in Eq.(5) (omitting indices)is non-zero for only one coordinate Q=Q with α=β×γ. This gives us directly the symmetry of the direction of softening and possible distortion of the ground state. Assuming that the primary force constants K in the two states are the same (for different K see [1]), we get a 2×2 secular equation with the following solution for the energies ε of the two states interacting under the linear vibronic coupling (the energy is read off the middle of the 2Δ interval between the initial levels):\n\nε= (1/2)Q±[Δ+FQ] (7)\n\nIt is seen from this expressions that, on taking into account the vibronic coupling, F≠0, the two APES curves change in different ways: in the upper sheet the curvature (the coefficient at Q in the expansion on Q) increases, whereas in the lower one it decreases. But until (F/K)<Δ the minima of both states correspond to the point Q = 0, as in the absence of vibronic mixing. However, if\n\nthe curvature of the lower curve of the APES becomes negative, and the system is unstable with respect to the Q displacements (Fig. 1). The minima points on the APES in this case are given by\n\n±Q=[F/K-Δ/F] (9)\n\nFrom these expressions and Fig. 1 it is seen that while the ground state is softened (destabilized) by the PJTE, the excited state is hardened (stabilized), and this effect is the larger, the smaller Δ and the larger F. It takes place in any polyatomic system and influences many molecular properties, including the existence of stable excited states of molecular systems that are unstable in the ground state (e.g., excited states of intermediates of chemical reactions); in general, even in the absence of instability the PJTE softens the ground state and increases the vibrational frequencies in the excited state.\n\nThe two branches of the APES for the case of strong PJTE resulting in the instability of the ground state (when the condition of instability (11) holds) are illustrated in Fig. 1b in comparison with the case when the two states have the same energy (Fig. 1a), i. e. when they are degenerate and the Jahn–Teller effect (JTE) takes place. We see that the two cases, degenerate and nondegenerate but close-in-energy (pseudodegenerate) are similar in generating two minima with distorted configurations, but there are important differences: while in the JTE there is a crossing of the two terms at the point of degeneracy (leading to conical intersections in more complicated cases), in the nondegenerate case with strong vibronic coupling there is an “avoided crossing” or “pseudo crossing”. Even a more important difference between the two vibronic coupling effects emerges from the fact that the two interacting states in the JTE are components of the same symmetry type, whereas in the PJTE each of the two states may have any symmetry. For this reason the possible kinds of distortion is very limited in the JTE, and unlimited in the PJTE. It is also noticeable that while the systems with JTE are limited by the condition of electron degeneracy, the applicability of the PJTE has no a priori limitations, as it includes also the cases of degeneracy. Even when the PJT coupling is weak and the inequality (11) does not hold, the PJTE is still significant in softening (lowering the corresponding vibrational frequency) of the ground state and increasing it in the excited state. When considering the PJTE in an excited state, all the higher in energy states destabilize it, while the lower ones stabilize it.\nFor a better understanding it is important to follow up on how the PJTE is related to intramolecular interactions. In other words, what is the physical driving force of the PJTE distortions (transformations) in terms of well-known electronic structure and bonding? The driving force of the PJTE is added (improved) covalence: the PJTE distortion takes place when it results in energy gain due to better covalence bonding between the atoms in the distorted configuration. Indeed, in the starting high-symmetry configuration the wavefunctions of the electronic states, ground and excited, are orthogonal by definition. By distortion their orthogonality is violated, and a nonzero overlap between them occurs. If for two near-neighbor atoms the ground state wavefunction pertains (mainly) to one of them, while the excited state wavefunction belongs (mainly) to the other one, the overlap by distortion adds covalency to the bonding between them, facilitating the distortion (Fig. 2).\n\nApplications of the PJTE to solving chemical, physical, biological, and materials science problems are innumerable; as stated above, the PJTE is the only source of instability and distortions in high-symmetry configurations of molecular systems and solids with nondegenerate states, hence any problem steaming from such instability can be treated by the PJTE. Below are some illustrative examples.\n\nPJTE versus Renner–Teller effect in bending distortions. Linear molecules are exceptions from the JTE, and for a long time it was assumed that their bending distortions in degenerate states (observed in many molecules) is produced by the Renner–Teller effect (RTE) (the splitting of the generate state by the quadratic terms of the vibronic coupling). However, recently it was proved (see in the review ) that the RTE, by splitting the degenerate electronic state, just softens the lower branch of the APES, but this lowering of the energy is not enough to overcome the rigidity of the linear configuration and to produce bending distortions. It follows that the bending distortion of linear molecular systems is due to, and only to the PJTE that mixes the electronic state under consideration with higher in energy (excited) states. This statement is enhanced by the fact that many linear molecules in nondegenerate states (and hence with no RTE) are, too, bent in the equilibrium configuration. The physical reason for the difference between the PJTE and the RTE in influencing the degenerate term is that while in the former case the vibronic coupling with the excited state produces additional covalent bonding that makes the distorted configuration preferable (see above, section 2.3), the RTE has no such influence; the splitting of the degenerate term in the RTE takes place just because the charge distribution in the two states becomes nonequivalent under the bending distortion.\n\nPeierls distortions in linear chains. In linear molecules with three or more atoms there may be PJTE distortions that do not violate the linearity but change the interatomic distances. For instance, as a result of the PJTE a centrosymmetric linear system may become non-centrosymmetric in the equilibrium configurations, as, for example, in the BNB molecule (see in ). An interesting extension of such distortions in sufficiently long (infinite) linear chains was first considered by Peierls. In this case the electronic states, combinations of atomic states, are in fact band states, and it was shown that if the chain is composed by atoms with unpaired electrons, the valence band is only half filled, and the PJTE interaction between the occupied and unoccupied band states leads to the doubling of the period of the linear chain (see also in the books ).\nBroken cylindrical symmetry. It was shown also that the PJTE not only produces the bending instability of linear molecules, but if the mixing electronic states involve a Δ state (a state with a nonzero momentum with respect to the axis of the molecule, its projection quantum number being Λ=2), the APES, simultaneously with the bending, becomes warped along the coordinate of rotations around the molecular axis, thus violating both the linear and cylindrical symmetry. It happens because the PJTE, by mixing the wavefunctions of the two interacting states, transfers the high momentum of the electrons from states with Λ=2 to states with lower momentum, and this may alter significantly their expected rovibronic spectra.\n\nPJTE and combined PJTE plus JTE effects in molecular structures. There is a practically unlimited number of molecular systems for which the origin of their structural properties was revealed and/or rationalized based on the PJTE, or a combination of the PJTE and JTE. The latter stems from the fact that in any system with a JTE in the ground state the presence of a PJT active excited state is not excluded, and vice versa, the active excited state for the PJTE of the ground one may be degenerate, and hence JT active. Examples are shown, e.g., in Refs., including molecular systems like Na, CH, CX (X= H, F, Cl, Br), CO, SiR (with R as large ligands), planar cyclic CH, all kind of coordination systems of transition metals, mixed-valence compounds, biological systems, origin of conformations, geometry of ligands’ coordination, etc., etc. In fact it is difficult to find a molecular system for which the PJTE implications are a priori excluded, which is understandable in view of the mentioned above unique role of the PJTE in such instabilities.\nHidden PJTE, spin crossover, and magnetic-dielectric bistability. As mentioned above, there are molecular systems in which the ground state in the high-symmetry configuration is neither degenerate to trigger the JTE, nor does it interact with the low-lying excited states to produce the PJTE (e.g., because of their different spin multiplicity). In these situations the instability is produced by a strong PJTE in the excited states; this is termed “hidden PJTE” in the sense that its origin is not seen explicitly as a PJTE in the ground state. An interesting typical situation of hidden PJTE emerges in molecular and solid state systems with valence half-filed closed shells electronic configurations e and t. For instance, in the e case the ground state in the high-symmetry equilibrium geometry is an orbital non-degenerate triplet A, while the nearby low-lying two excited electronic states are close-in-energy singlets E and A; due to the strong PJT interaction between the latter, the lower component of E crosses the triplet state to produce a global minimum with lower symmetry. Fig. 3 illustrates the hidden PJTE in the CuF molecule, showing also the singlet-triplet spin crossover and the resulting two coexisting configurations of the molecule: high-symmetry (undistorted) spin-triplet state with a nonzero magnetic moment, and a lower in energy dipolar-distorted singlet state with zero magnetic moment. Such magnetic-dielectric bistability is inherent to a whole class of molecular systems and solids.\n\nPuckering in planar molecules and graphene-like 2D and quasi 2D systems. Special attention has been paid recently to two-dimensional (2D) systems in view of a variety of their planar-surface-specific physical and chemical properties and possible graphene-like applications in electronics. Similar-to-graphene properties are sought for in silicene, phosphorene, boron nitride, zinc oxide, gallium nitride, as well as in other attractive subjects like 2D transition metal dichalcogenids and oxides, and there is a number of other organic and inorganic 2D and quasi-2D compounds with expected similar properties. One of the main important features of these systems is their planarity or quasi-planarity, but many of the quasi-2D compounds are subject to out-of-plane deviations known as puckering (buckling). It was shown, as expected, that the instability and distortions of the planar configuration (as in any other systems in nondegenerate state) is due to the PJTE. Detailed exploration of the PJTE in such systems allows one to reveal the excited states that are responsible for the puckering, and suggest possible external influence that restores their planarity, including oxidation, reduction, substitutions, or coordination to other species.\n\nCooperative PJTE in BaTiO-type crystals and ferroelectricity. In crystals with PJTE centers the interaction between the local distortions may lead to their ordering to produce a phase transition to a regular crystal phase with lower symmetry. Such cooperative PJTE is quite similar to the cooperative JTE; it was shown in one of the first applications of the PJTE to solid state systems that in case of ABO crystals with perovskite structure the local dipolar PJTE distortions at the transition metal B center and their cooperative interactions leads to ferroelectric phase transitions. Provided the criterion of PJTE is obeyed, each [BO] center has an APES with eight equivalent minima along the trigonal axes, six orthorhombic, and (higher) twelve tetragonal saddle-points between them. With temperature, the gradually reached transitions between the minima via the different kind of saddle-points explains the origin of all the four phases (three ferroelectric and one paraelectric) in perovskites of the type BaTiO and their properties. The predicted by the theory trigonal displacement of the Ti ion in all four phases, the fully disordered PJTE distortions in the paraelectric phase, and their partially disordered state in two other phases was confirmed by a variety of experimental investigations (see in ).\n\nMultiferroicity and magnetic-ferroelectric crossover. The PJTE theory of ferroelectricity in ABO3 crystals was expanded to show that, dependent of the number of electrons in the d shell of the transition metal ion B and their low spin or high spin arrangement (which controls the symmetry and spin multiplicity of the ground and PJTE active excited states of the [BO] center), their ferroelectricity may coexist with a magnetic moment (multiferroicity). Moreover, in combination with the temperature dependent spin crossover phenomenon (which changes the spin multiplicity), this kind of multiferroicity may lead to a novel effect of magnetic-ferroelectric crossover.\n\nSolid state magnetic-dielectric bistability. Similar to the mentioned above molecular bistability induced by the hidden PJTE, a magnetic-dielectric bistability due to two coexisting equilibrium configurations with corresponding properties may take place also in crystals with transition metal centers, subject to the electronic configuration with half-filled e or t shells. As in molecular systems, the latter produce a hidden PJTE and local bistability which, distinguished from the molecular case, are enhanced by the cooperative interactions, thus acquiring larger lifetimes. This crystal bistability was proved by calculations for LiCuO and NaCuO crystals, in which the Cu ion has the electronic e(d) configuration (similar to the CuF molecule).\nGiant enhancement of observable properties in interaction with external perturbations. In a recent development of the PJTE applications in materials science it was shown that in crystals with PJTE centers, in which the local distortion are not ordered (before the phase transition to the cooperative phases), the effect of interaction with external perturbations acquires an orientational contribution which enhances the observable properties by several orders of magnitude. This was demonstrated on the properties of crystals like paraelectric BaTiO in interaction with electric fields (in permittivity and electrostriction), or under a strain gradient (flexoelectricity). These giant enhancement effects occur due to the dynamic nature of the PJTE local dipolar distortions (their tunneling between the equivalent minima); the independently rotating dipole moments on each center become oriented (frozen) along the external perturbation resulting in an orientational polarization which is not there in the absence of the PJTE\n"}
{"id": "48533814", "url": "https://en.wikipedia.org/wiki?curid=48533814", "title": "Self-regulating heater", "text": "Self-regulating heater\n\nA self-regulating heater is an electrical heater which strives to keep a constant temperature regardless of how the ambient conditions changes. It is the material in the heater itself that regulates the temperature. The heater requires no regulating electronics, temperature sensors, overheat protection etc. The temperature the heater will hold is decided when the material and the heater is produced. It can be fine tuned by changing the voltage to the heater. Self-regulating heaters are made from materials with strong Positive temperature coefficient (PTC) characteristics, i.e. the resistivity of the material increases rapidly with increasing temperature. Such materials include ceramic PTC stones. and PTC rubber.\n\nA ceramic PTC heater is made of small ceramic pieces pressed between two metal plates. The constraint compartment causes them to get a strong PTC effect. The typical temperature of ceramic stones is 140–250 °C (284–482 °F). Ceramic stones actually have Negative temperature coefficient (NTC) properties at low temperatures which makes them slow to heat up.\n\nA rubber PTC heater is made from a special type of rubber which conducts electricity, but only up to a temperature which is defined at the production of the PTC rubber. Typical design temperatures are between 0–80 °C (32–176 °F). The resistivity of the rubber increases exponentially with temperature for all temperatures up to the design temperature. Hence, it has strong PTC properties for all temperatures and heats up rapidly. Above this temperature the rubber is an electrical isolator and cease to produce heat. This makes the heater self-limiting. The rubber foil is thin and flexible and can be formed to any shape and size.\n\nWhen a voltage is applied to a PTC heater electrical current flows through the material and the resistivity of the material causes it to heat up (P(T)=U∧2÷R(T)). As it heats up the resistivity increases rapidly and the power (heat) produced decreases. At the same time heat is being transferred from the heater to the object it is attached to, and its surrounding. Eventually the amount of heat produced is in equilibrium with the amount of heat conducted and radiated away from the heater. The heater reaches its equilibrium temperature and remains there. The exponential PTC properties of the material assures that the equilibrium temperature is virtually insensitive to changes in the ambient temperatures.\n"}
{"id": "18267453", "url": "https://en.wikipedia.org/wiki?curid=18267453", "title": "Sinus Holding Wind Farm", "text": "Sinus Holding Wind Farm\n\nThe Sinus Holding Wind Farm is an under construction wind power project in Vaslui County, Romania. It will consist of five individual wind farms connected together. It will have 350 individual wind turbines with a nominal output of around 2 MW which will deliver up to 700 MW of power, enough to power over 459,000 homes, with a capital investment required of approximately US$800 million.\n"}
{"id": "23714989", "url": "https://en.wikipedia.org/wiki?curid=23714989", "title": "Solid bleached board", "text": "Solid bleached board\n\nSolid bleached board (SBB) or solid bleached sulphate (SBS) is a virgin fibre grade of paperboard.\n\nThis grade is made purely from bleached chemical pulp and usually has a mineral or synthetic pigment coated top surface in one or more layers (C1S) and often also a coating on the reverse side (C2S). It is a medium density board with good printing properties for graphical and packaging end uses and is perfectly white both inside and out. It can easily be cut, creased, hot foil stamped and embossed. Its other properties, such as being hygienic and pure with no smell and taste, make it usable for packaging aroma and flavour sensitive products such as chocolate, cigarettes and cosmetics.\n\n\n"}
{"id": "19214894", "url": "https://en.wikipedia.org/wiki?curid=19214894", "title": "The Unfinished Twentieth Century", "text": "The Unfinished Twentieth Century\n\nIn the 2001 book The Unfinished Twentieth Century, author Jonathan Schell suggests that an essential feature of the twentieth century was the development of humankind's capacity for self-destruction, with the rise in many forms of \"policies of extermination\". Schell goes on to suggest that the world now faces a clear choice between the abolition of all nuclear weapons, and full nuclearization, as the necessary technology and materials diffuse around the globe.\n\n"}
{"id": "19397668", "url": "https://en.wikipedia.org/wiki?curid=19397668", "title": "U-1 (semi-trailer)", "text": "U-1 (semi-trailer)\n\nThe U-1 was a 1950s liquid hydrogen trailer designed to carry cryogenic liquid hydrogen (LH) on roads being pulled by a powered vehicle. It was designed in response to requirements of the secret US government program code-named \"Suntan\", which aimed to develop a high speed, high altitude hydrogen-powered military aircraft. The trailer was constructed by the Cambridge Corporation and had a capacity of 26,500 liters with a hydrogen loss rate of approximately 2 percent per day. The U-1 was a single axle semi-trailer. The specifications for its successor the U-2 a double axle semi-trailer were issued on 15 March 1957.\n\n"}
{"id": "26602769", "url": "https://en.wikipedia.org/wiki?curid=26602769", "title": "Ula oil field", "text": "Ula oil field\n\nUla () is an offshore oil field located in the southern Norwegian section of North Sea along with Gyda, Tambar and Tambar East fields making up the UGT area, usually attributed to DONG Energy's main areas of exploration and production activity.\n\nThe Ula field was discovered in 1976 and came online in October 1986. It contains confirmed 69.98 million m of oil and 2.5 million of NGL.\n\nAkerBP is the operator of the field with 80% of interest in the project. AkerBP's partner DONG Energy holds 20% of interest. DONG Energy increased its initial share of 5% to 20% by acquiring Svenska Petroleum's complete share of 15% for US$130 million in 2008.\n\nUla is located in approximately of water. The main reservoir stands at in the Upper Jurassic Ula Formation. The field has three conventional steel facilities with production, drilling, living quarters. It has 7 production and 2 water injection wells. Current production at Ula field is 10,000 bbl/d. The existing gas process plant was recently extended with a new module, turbine and compressor and has been operative since 2008. The gas from Blane field is injected into the Ula reservoir for production. The gas produced at Ula field is re-injected into the field for increased recovery as well. The field is expected to be abandoned in 2028. The produced oil is then transported by a pipeline to Ekofisk oil field and on to Teesside for refining. In 2009, Aker Solutions was awarded a contract to tieback Ula field to Oselvar field. The work is expected to commence in 2011. Once complete, the oil from DONG-operated Oselvar field which is from Ula will be pumped to facilities at Ula for processing.\n\n\n"}
{"id": "32564567", "url": "https://en.wikipedia.org/wiki?curid=32564567", "title": "Uranpyrochlore (of Hogarth 1977)", "text": "Uranpyrochlore (of Hogarth 1977)\n\nUranpyrochlore (ellsworthite) (Ca,U)(Ti,Nb,Ta)O(OH) is a rare earth mineral mostly found in the northern parts of North America. It is named after Hardy V. Ellsworth of the Canadian Geological Survey by Walker and Parsons. Ellsworthite is also known under the name {Betafite}. It is a very and rich mineral, which in fact makes it slightly radioactive. makes up about 17.1% of the mineral.\n\nEllsworthite is part of pyrochlore super group and the sub group betafite and was named after the Canadian Mineralogist and surveyor H. V. Ellsworth. Ellsworthite is also known as the mineral Betafite. Ellsworthite is a - bearing mineral that is found mostly in Canada and Alaska. It was first discovered in Hybla, Ontario, which is now a ghost town. Minerals of the pyrochlore group present one of the most important modes of occurrence of trace elements (Nb,Ta) and were found in various geological (geochemical) environments. It is very important geochemically that the pyrochlore structure is very suitable for diverse isomorphic substitutions.\n\nEllsworthite has complex hydrous oxides of , , , , with hydroxyl and fluorine; it may contain as much as 17% . It is composed of calcium, uranium, titanium, niobium, tantalum, and oxygen. Ellsworthite is heavily abundant in uranium, niobium and titanium oxides. It is very uranium, thorium, and water rich. One can consider Betafite as a hydrous uranium pyrochlore.\n\nThe structure of Ellsworthite is cubic and has a point group of 4/m 3* 2/m. It is part of the isometric system and has the space group Fd3m. It forms into a Hexoctahedral with {110}, {100}, {113}, {233}, and {230}. It has conchoidal fracture with no cleavage. Betafite is a synonym to Ellsworthite.\n\nEllsworthite is mostly found in Canada and is amber yellow to dark brown in color. In most cases it has an amber rusty color and has a hardness of about 5.5. The luster is waxy or greasy to vitreous. The outer, more altered parts are relatively high in water and low in uranium. Its streak is yellow to brownish. The inner parts are brown, greenish brown, yellowish brown, or yellow. It is translucent to opaque \n\n"}
{"id": "7034672", "url": "https://en.wikipedia.org/wiki?curid=7034672", "title": "Weapon storage area", "text": "Weapon storage area\n\nWeapon storage areas (WSA), also known as special ammunition storage (SAS), were extremely well guarded and well defended locations where United States and NATO nuclear weapons were stored during the Cold War era.\n\nIn most situations, the WSA or SAS areas were located inside the perimeter of an army barracks or an air base in NATO territory, but in a few cases they were located deep inside wooded areas and miles away from a military base. \n\nDue to changes in the political landscape, the number of special weapons in Europe has been drastically decreased. Moreover, the introduction of the WS3 Weapon Storage and Security System has made WSAs obsolete. \n\nAt present, few WSAs are still operational as modern day special weapons are stored in the floors of concrete aircraft shelters and placed under 24/7 electronic surveillance.\n\n"}
