{"id": "55498682", "url": "https://en.wikipedia.org/wiki?curid=55498682", "title": "Amazon wild rice", "text": "Amazon wild rice\n\nAmazon wild rice refers to either of the two native (endemic) species of rice (Oryza) found in the Amazon region of South America and adjacent tropical areas, \"Oryza glumaepatula\" and \"Oryza latifolia\". It also can refer to the cultivated form \"Oryza sp.\" which was domesticated from one or both of the wild forms some 4,000 years ago by the indigenous people of the Amazon region.\n"}
{"id": "3201", "url": "https://en.wikipedia.org/wiki?curid=3201", "title": "Attribution of recent climate change", "text": "Attribution of recent climate change\n\nAttribution of recent climate change is the effort to scientifically ascertain mechanisms responsible for recent climate changes on Earth, commonly known as 'global warming'. The effort has focused on changes observed during the period of instrumental temperature record, particularly in the last 50 years. This is the period when human activity has grown fastest and observations of the atmosphere above the surface have become available. According to the Intergovernmental Panel on Climate Change (IPCC), it is \"extremely likely\" that human influence was the dominant cause of global warming between 1951 and 2010. The best estimate is that observed warming since 1951 has been entirely human caused.\n\nSome of the main human activities that contribute to global warming are:\nIn addition to human activities, some natural mechanisms can also cause climate change, including for example, climate oscillations, changes in solar activity, and volcanic activity.\n\nMultiple lines of evidence support attribution of recent climate change to human activities:\n\nThe IPCC's attribution of recent global warming to human activities is a view shared by the scientific community, and is also supported by 196 other scientific organizations worldwide (see also: scientific opinion on climate change).\n\nFactors affecting Earth's climate can be broken down into feedbacks and forcings. A forcing is something that is imposed externally on the climate system. External forcings include natural phenomena such as volcanic eruptions and variations in the sun's output. Human activities can also impose forcings, for example, through changing the composition of the atmosphere.\n\nRadiative forcing is a measure of how various factors alter the energy balance of the Earth's atmosphere. A positive radiative forcing will tend to increase the energy of the Earth-atmosphere system, leading to a warming of the system. Between the start of the Industrial Revolution in 1750, and the year 2005, the increase in the atmospheric concentration of carbon dioxide (chemical formula: ) led to a positive radiative forcing, averaged over the Earth's surface area, of about 1.66 watts per square metre (abbreviated W m).\n\nClimate feedbacks can either amplify or dampen the response of the climate to a given forcing.\nThere are many feedback mechanisms in the climate system that can either amplify (a positive feedback) or diminish (a negative feedback) the effects of a change in climate forcing.\n\nThe climate system will vary in response to changes in forcings.\nThe climate system will show internal variability both in the presence and absence of forcings imposed on it, (see images opposite). This internal variability is a result of complex interactions between components of the climate system, such as the coupling between the atmosphere and ocean (see also the later section on Internal climate variability and global warming). An example of internal variability is the El Niño–Southern Oscillation.\n\nDetection and attribution of climate signals, as well as its common-sense meaning, has a more precise definition within the climate change literature, as expressed by the IPCC. Detection of a climate signal does not always imply significant attribution. The IPCC's Fourth Assessment Report says \"it is \"extremely likely\" that human activities have exerted a substantial net warming influence on climate since 1750,\" where \"extremely likely\" indicates a probability greater than 95%. \"Detection\" of a signal requires demonstrating that an observed change is statistically significantly different from that which can be explained by natural internal variability.\n\n\"Attribution\" requires demonstrating that a signal is:\n\nCarbon dioxide is the primary greenhouse gas that is contributing to recent climate change. is absorbed and emitted naturally as part of the carbon cycle, through animal and plant respiration, volcanic eruptions, and ocean-atmosphere exchange. Human activities, such as the burning of fossil fuels and changes in land use (see below), release large amounts of carbon to the atmosphere, causing concentrations in the atmosphere to rise.\n\nThe high-accuracy measurements of atmospheric concentration, initiated by Charles David Keeling in 1958, constitute the master time series documenting the changing composition of the atmosphere. These data have iconic status in climate change science as evidence of the effect of human activities on the chemical composition of the global atmosphere.\n\nAlong with , methane and to a lesser extent nitrous oxide are also major forcing contributors to the greenhouse effect. The Kyoto Protocol lists these together with hydrofluorocarbon (HFCs), perfluorocarbons (PFCs), and sulfur hexafluoride (SF), which are entirely artificial gases, as contributors to radiative forcing. The chart at right attributes anthropogenic greenhouse gas emissions to eight main economic sectors, of which the largest contributors are power stations (many of which burn coal or other fossil fuels), industrial processes, transportation fuels (generally fossil fuels), and agricultural by-products (mainly methane from enteric fermentation and nitrous oxide from fertilizer use).\n\nWater vapor is the most abundant greenhouse gas and is the largest contributor to the natural greenhouse effect, despite having a short atmospheric lifetime (about 10 days). Some human activities can influence local water vapor levels. However, on a global scale, the concentration of water vapor is controlled by temperature, which influences overall rates of evaporation and precipitation. Therefore, the global concentration of water vapor is not substantially affected by direct human emissions.\n\nClimate change is attributed to land use for two main reasons. Between 1750 and 2007, about two-thirds of anthropogenic emissions were produced from burning fossil fuels, and about one-third of emissions from changes in land use, primarily deforestation. Deforestation both reduces the amount of carbon dioxide absorbed by deforested regions and releases greenhouse gases directly, together with aerosols, through biomass burning that frequently accompanies it.\n\nA second reason that climate change has been attributed to land use is that the terrestrial albedo is often altered by use, which leads to radiative forcing. This effect is more significant locally than globally.\n\nWorldwide, livestock production occupies 70% of all land used for agriculture, or 30% of the ice-free land surface of the Earth.\nMore than 18% of anthropogenic greenhouse gas emissions are attributed to livestock and livestock-related activities such as deforestation and increasingly fuel-intensive farming practices. Specific attributions to the livestock sector include:\n\nWith virtual certainty, scientific consensus has attributed various forms of climate change, chiefly cooling effects, to aerosols, which are small particles or droplets suspended in the atmosphere.\nKey sources to which anthropogenic aerosols are attributed include:\n\nOver the past 150 years human activities have released increasing quantities of greenhouse gases into the atmosphere. This has led to increases in mean global temperature, or global warming. Other human effects are relevant—for example, sulphate aerosols are believed to have a cooling effect. Natural factors also contribute. According to the historical temperature record of the last century, the Earth's near-surface air temperature has risen around 0.74 ± 0.18 °Celsius (1.3 ± 0.32 °Fahrenheit).\n\nA historically important question in climate change research has regarded the relative importance of human activity and non-anthropogenic causes during the period of instrumental record. In the 1995 Second Assessment Report (SAR), the IPCC made the widely quoted statement that \"The balance of evidence suggests a discernible human influence on global climate\". The phrase \"balance of evidence\" suggested the (English) common-law standard of proof required in civil as opposed to criminal courts: not as high as \"beyond reasonable doubt\". In 2001 the Third Assessment Report (TAR) refined this, saying \"There is new and stronger evidence that most of the warming observed over the last 50 years is attributable to human activities\". The 2007 Fourth Assessment Report (AR4) strengthened this finding:\n\nOther findings of the IPCC Fourth Assessment Report include:\n\nOver the past five decades there has been a global warming of approximately 0.65 °C (1.17 °F) at the Earth's surface (see historical temperature record). Among the possible factors that could produce changes in global mean temperature are internal variability of the climate system, external forcing, an increase in concentration of greenhouse gases, or any combination of these. Current studies indicate that the increase in greenhouse gases, most notably , is mostly responsible for the observed warming. Evidence for this conclusion includes:\n\nRecent scientific assessments find that most of the warming of the Earth's surface over the past 50 years has been caused by human activities (see also the section on scientific literature and opinion). This conclusion rests on multiple lines of evidence. Like the warming \"signal\" that has gradually emerged from the \"noise\" of natural climate variability, the scientific evidence for a human influence on global climate has accumulated over the past several decades, from many hundreds of studies. No single study is a \"smoking gun.\" Nor has any single study or combination of studies undermined the large body of evidence supporting the conclusion that human activity is the primary driver of recent warming.\n\nThe first line of evidence is based on a physical understanding of how greenhouse gases trap heat, how the climate system responds to increases in greenhouse gases, and how other human and natural factors influence climate. The second line of evidence is from indirect estimates of climate changes over the last 1,000 to 2,000 years. These records are obtained from living things and their remains (like tree rings and corals) and from physical quantities (like the ratio between lighter and heavier isotopes of oxygen in ice cores), which change in measurable ways as climate changes. The lesson from these data is that global surface temperatures over the last several decades are clearly unusual, in that they were higher than at any time during at least the past 400 years. For the Northern Hemisphere, the recent temperature rise is clearly unusual in at least the last 1,000 years (see graph opposite).\n\nThe third line of evidence is based on the broad, qualitative consistency between observed changes in climate and the computer model simulations of how climate would be expected to change in response to human activities. For example, when climate models are run with historical increases in greenhouse gases, they show gradual warming of the Earth and ocean surface, increases in ocean heat content and the temperature of the lower atmosphere, a rise in global sea level, retreat of sea ice and snow cover, cooling of the stratosphere, an increase in the amount of atmospheric water vapor, and changes in large-scale precipitation and pressure patterns. These and other aspects of modelled climate change are in agreement with observations.\n\nFinally, there is extensive statistical evidence from so-called \"fingerprint\" studies. Each factor that affects climate produces a unique pattern of climate response, much as each person has a unique fingerprint. Fingerprint studies exploit these unique signatures, and allow detailed comparisons of modelled and observed climate change patterns. Scientists rely on such studies to attribute observed changes in climate to a particular cause or set of causes. In the real world, the climate changes that have occurred since the start of the Industrial Revolution are due to a complex mixture of human and natural causes. The importance of each individual influence in this mixture changes over time. Of course, there are not multiple Earths, which would allow an experimenter to change one factor at a time on each Earth, thus helping to isolate different fingerprints. Therefore, climate models are used to study how individual factors affect climate. For example, a single factor (like greenhouse gases) or a set of factors can be varied, and the response of the modelled climate system to these individual or combined changes can thus be studied.\nFor example, when climate model simulations of the last century include all of the major influences on climate, both human-induced and natural, they can reproduce many important features of observed climate change patterns. When human influences are removed from the model experiments, results suggest that the surface of the Earth would actually have cooled slightly over the last 50 years (see graph, opposite). The clear message from fingerprint studies is that the observed warming over the last half-century cannot be explained by natural factors, and is instead caused primarily by human factors.\n\nAnother fingerprint of human effects on climate has been identified by looking at a slice through the layers of the atmosphere, and studying the pattern of temperature changes from the surface up through the stratosphere (see the section on solar activity). The earliest fingerprint work focused on changes in surface and atmospheric temperature. Scientists then applied fingerprint methods to a whole range of climate variables, identifying human-caused climate signals in the heat content of the oceans, the height of the tropopause (the boundary between the troposphere and stratosphere, which has shifted upward by hundreds of feet in recent decades), the geographical patterns of precipitation, drought, surface pressure, and the runoff from major river basins.\n\nStudies published after the appearance of the IPCC Fourth Assessment Report in 2007 have also found human fingerprints in the increased levels of atmospheric moisture (both close to the surface and over the full extent of the atmosphere), in the decline of Arctic sea ice extent, and in the patterns of changes in Arctic and Antarctic surface temperatures.\n\nThe message from this entire body of work is that the climate system is telling a consistent story of increasingly dominant human influence – the changes in temperature, ice extent, moisture, and circulation patterns fit together in a physically consistent way, like pieces in a complex puzzle.\n\nIncreasingly, this type of fingerprint work is shifting its emphasis. As noted, clear and compelling scientific evidence supports the case for a pronounced human influence on global climate. Much of the recent attention is now on climate changes at continental and regional scales, and on variables that can have large impacts on societies. For example, scientists have established causal links between human activities and the changes in snowpack, maximum and minimum (diurnal) temperature, and the seasonal timing of runoff over mountainous regions of the western United States. Human activity is likely to have made a substantial contribution to ocean surface temperature changes in hurricane formation regions. Researchers are also looking beyond the physical climate system, and are beginning to tie changes in the distribution and seasonal behaviour of plant and animal species to human-caused changes in temperature and precipitation.\n\nFor over a decade, one aspect of the climate change story seemed to show a significant difference between models and observations. In the tropics, all models predicted that with a rise in greenhouse gases, the troposphere would be expected to warm more rapidly than the surface. Observations from weather balloons, satellites, and surface thermometers seemed to show the opposite behaviour (more rapid warming of the surface than the troposphere). This issue was a stumbling block in understanding the causes of climate change. It is now largely resolved. Research showed that there were large uncertainties in the satellite and weather balloon data. When uncertainties in models and observations are properly accounted for, newer observational data sets (with better treatment of known problems) are in agreement with climate model results.\n\nThis does not mean, however, that all remaining differences between models and observations have been resolved. The observed changes in some climate variables, such as Arctic sea ice, some aspects of precipitation, and patterns of surface pressure, appear to be proceeding much more rapidly than models have projected. The reasons for these differences are not well understood. Nevertheless, the bottom-line conclusion from climate fingerprinting is that most of the observed changes studied to date are consistent with each other, and are also consistent with our scientific understanding of how the climate system would be expected to respond to the increase in heat-trapping gases resulting from human activities.\n\nOne of the subjects discussed in the literature is whether or not extreme weather events can be attributed to human activities. Seneviratne \"et al.\" (2012) stated that attributing individual extreme weather events to human activities was challenging. They were, however, more confident over attributing changes in long-term trends of extreme weather. For example, Seneviratne \"et al.\" (2012) concluded that human activities had likely led to a warming of extreme daily minimum and maximum temperatures at the global scale.\n\nAnother way of viewing the problem is to consider the effects of human-induced climate change on the probability of future extreme weather events. Stott \"et al.\" (2003), for example, considered whether or not human activities had increased the risk of severe heat waves in Europe, like the one experienced in 2003. Their conclusion was that human activities had very likely more than doubled the risk of heat waves of this magnitude.\n\nAn analogy can be made between an athlete on steroids and human-induced climate change. In the same way that an athlete's performance may increase from using steroids, human-induced climate change increases the risk of some extreme weather events.\n\nHansen \"et al.\" (2012) suggested that human activities have greatly increased the risk of summertime heat waves. According to their analysis, the land area of the Earth affected by very hot summer temperature anomalies has greatly increased over time (refer to graphs on the left). In the base period 1951-1980, these anomalies covered a few tenths of 1% of the global land area. In recent years, this has increased to around 10% of the global land area. With high confidence, Hansen \"et al.\" (2012) attributed the 2010 Moscow and 2011 Texas heat waves to human-induced global warming.\n\nAn earlier study by Dole \"et al.\" (2011) concluded that the 2010 Moscow heatwave was mostly due to natural weather variability. While not directly citing Dole \"et al.\" (2011), Hansen \"et al.\" (2012) rejected this type of explanation. Hansen \"et al.\" (2012) stated that a combination of natural weather variability and human-induced global warming was responsible for the Moscow and Texas heat waves.\n\nThere are a number of examples of published and informal support for the consensus view. As mentioned earlier, the IPCC has concluded that most of the observed increase in globally averaged temperatures since the mid-20th century is \"very likely\" due to human activities. The IPCC's conclusions are consistent with those of several reports produced by the US National Research Council.\nA report published in 2009 by the U.S. Global Change Research Program concluded that \"[global] warming is unequivocal and primarily human-induced.\"\nA number of scientific organizations have issued statements that support the consensus view. Two examples include:\n\nThe IPCC Fourth Assessment Report (2007), concluded that attribution was possible for a number of observed changes in the climate (see effects of global warming). However, attribution was found to be more difficult when assessing changes over smaller regions (less than continental scale) and over short time periods (less than 50 years).\nOver larger regions, averaging reduces natural variability of the climate, making detection and attribution easier.\n\n\n\nAs described above, a small minority of scientists do disagree with the consensus: see list of scientists opposing global warming consensus. For example, Willie Soon and Richard Lindzen say that there is insufficient proof for anthropogenic attribution. Generally this position requires new physical mechanisms to explain the observed warming.\n\nSolar sunspot maximum occurs when the magnetic field of the sun collapses and reverse as part of its average 11 year solar cycle (22 years for complete North to North restoration).\n\nThe role of the sun in recent climate change has been looked at by climate scientists. Since 1978, output from the Sun has been measured by satellites significantly more accurately than was previously possible from the surface. These measurements indicate that the Sun's total solar irradiance has not increased since 1978, so the warming during the past 30 years cannot be directly attributed to an increase in total solar energy reaching the Earth (see graph above, left). In the three decades since 1978, the combination of solar and volcanic activity probably had a slight cooling influence on the climate.\n\nClimate models have been used to examine the role of the sun in recent climate change.\nModels are unable to reproduce the rapid warming observed in recent decades when they only take into account variations in total solar irradiance and volcanic activity. Models are, however, able to simulate the observed 20th century changes in temperature when they include all of the most important external forcings, including human influences and natural forcings. As has already been stated, Hegerl \"et al.\" (2007) concluded that greenhouse gas forcing had \"very likely\" caused most of the observed global warming since the mid-20th century. In making this conclusion, Hegerl \"et al.\" (2007) allowed for the possibility that climate models had been underestimated the effect of solar forcing.\n\nThe role of solar activity in climate change has also been calculated over longer time periods using \"proxy\" datasets, such as tree rings.\nModels indicate that solar and volcanic forcings can explain periods of relative warmth and cold between A.D. 1000 and 1900, but human-induced forcings are needed to reproduce the late-20th century warming.\n\nAnother line of evidence against the sun having caused recent climate change comes from looking at how temperatures at different levels in the Earth's atmosphere have changed.\nModels and observations (see figure above, middle) show that greenhouse gas results in warming of the lower atmosphere at the surface (called the troposphere) but cooling of the upper atmosphere (called the stratosphere). Depletion of the ozone layer by chemical refrigerants has also resulted in a cooling effect in the stratosphere. If the sun was responsible for observed warming, warming of the troposphere at the surface and warming at the top of the stratosphere would be expected as increase solar activity would replenish ozone and oxides of nitrogen. The stratosphere has a reverse temperature gradient than the troposphere so as the temperature of the troposphere cools with altitude, the stratosphere rises with altitude. Hadley cells are the mechanism by which equatorial generated ozone in the tropics (highest area of UV irradiance in the stratosphere) is moved poleward. Global climate models suggest that climate change may widen the Hadley cells and push the jetstream northward thereby expanding the tropics region and resulting in warmer, dryer conditions in those areas overall.\n\nHabibullo Abdussamatov (2004), head of space research at St. Petersburg's Pulkovo Astronomical Observatory in Russia, has argued that the sun is responsible for recently observed climate change. Journalists for news sources canada.com (Solomon, 2007b), National Geographic News (Ravillious, 2007), and LiveScience (Than, 2007) reported on the story of warming on Mars. In these articles, Abdussamatov was quoted. He stated that warming on Mars was evidence that global warming on Earth was being caused by changes in the sun.\n\nRavillious (2007) quoted two scientists who disagreed with Abdussamatov: Amato Evan, a climate scientist at the University of Wisconsin-Madison, in the US, and Colin Wilson, a planetary physicist at Oxford University in the UK. According to Wilson, \"Wobbles in the orbit of Mars are the main cause of its climate change in the current era\" (see also orbital forcing). Than (2007) quoted Charles Long, a climate physicist at Pacific Northwest National Laboratories in the US, who disagreed with Abdussamatov.\n\nThan (2007) pointed to the view of Benny Peiser, a social anthropologist at Liverpool John Moores University in the UK. In his newsletter, Peiser had cited a blog that had commented on warming observed on several planetary bodies in the Solar system. These included Neptune's moon Triton, Jupiter, Pluto and Mars. In an e-mail interview with Than (2007), Peiser stated that:\"I think it is an intriguing coincidence that warming trends have been observed on a number of very diverse planetary bodies in our solar system, (...) Perhaps this is just a fluke.\"Than (2007) provided alternative explanations of why warming had occurred on Triton, Pluto, Jupiter and Mars.\n\nThe US Environmental Protection Agency (US EPA, 2009) responded to public comments on climate change attribution. A number of commenters had argued that recent climate change could be attributed to changes in solar irradiance. According to the US EPA (2009), this attribution was not supported by the bulk of the scientific literature. Citing the work of the IPCC (2007), the US EPA pointed to the low contribution of solar irradiance to radiative forcing since the start of the Industrial Revolution in 1750. Over this time period (1750 to 2005), the estimated contribution of solar irradiance to radiative forcing was 5% the value of the combined radiative forcing due to increases in the atmospheric concentrations of carbon dioxide, methane and nitrous oxide (see graph opposite).\n\nHenrik Svensmark has suggested that the magnetic activity of the sun deflects cosmic rays, and that this may influence the generation of cloud condensation nuclei, and thereby have an effect on the climate. The website ScienceDaily reported on a 2009 study that looked at how past changes in climate have been affected by the Earth's magnetic field. Geophysicist Mads Faurschou Knudsen, who co-authored the study, stated that the study's results supported Svensmark's theory. The authors of the study also acknowledged that plays an important role in climate change.\n\nThe view that cosmic rays could provide the mechanism by which changes in solar activity affect climate is not supported by the literature. Solomon \"et al.\" (2007) state:[..] the cosmic ray time series does not appear to correspond to global total cloud cover after 1991 or to global low-level cloud cover after 1994. Together with the lack of a proven physical mechanism and the plausibility of other causal factors affecting changes in cloud cover, this makes the association between galactic cosmic ray-induced changes in aerosol and cloud formation controversial\n\nStudies by Lockwood and Fröhlich (2007) and Sloan and Wolfendale (2008) found no relation between warming in recent decades and cosmic rays. Pierce and Adams (2009) used a model to simulate the effect of cosmic rays on cloud properties. They concluded that the hypothesized effect of cosmic rays was too small to explain recent climate change. Pierce and Adams (2009) noted that their findings did not rule out a possible connection between cosmic rays and climate change, and recommended further research.\n\nErlykin \"et al.\" (2009) found that the evidence showed that connections between solar variation and climate were more likely to be mediated by direct variation of insolation rather than cosmic rays, and concluded: \"Hence within our assumptions, the effect of varying solar activity, either by direct solar irradiance or by varying cosmic ray rates, must be less than 0.07 °C since 1956, i.e. less than 14% of the observed global warming.\" Carslaw (2009) and Pittock (2009) review the recent and historical literature in this field and continue to find that the link between cosmic rays and climate is tenuous, though they encourage continued research. US EPA (2009) commented on research by Duplissy \"et al.\" (2009):The CLOUD experiments at CERN are interesting research but do not provide conclusive evidence that cosmic rays can serve as a major source of cloud seeding. Preliminary results from the experiment (Duplissy et al., 2009) suggest that though there was some evidence of ion mediated nucleation, for most of the nucleation events observed the contribution of ion processes appeared to be minor. These experiments also showed the difficulty in maintaining sufficiently clean conditions and stable temperatures to prevent spurious aerosol bursts. There is no indication that the earlier Svensmark experiments could even have matched the controlled conditions of the CERN experiment. We find that the Svensmark results on cloud seeding have not yet been shown to be robust or sufficient to materially alter the conclusions of the assessment literature, especially given the abundance of recent literature that is skeptical of the cosmic ray-climate linkage\n\n\n\nPublic-domain sources\n\n"}
{"id": "2960046", "url": "https://en.wikipedia.org/wiki?curid=2960046", "title": "Bowtell", "text": "Bowtell\n\nBowtell is derived from the medieval term \"bottle\"; in architecture it refers to a round or corniced molding below the abacus in a Tuscan or Roman Doric capital; the word is a variant of boltel, which is probably the diminutive of bolt, the shaft of an arrow or javelin. A roving bowtell is one which passes up the side of a bench end and round a finial, the term roving being applied to that which follows the line of a curve.\n"}
{"id": "5676", "url": "https://en.wikipedia.org/wiki?curid=5676", "title": "Californium", "text": "Californium\n\nCalifornium is a radioactive chemical element with symbol Cf and atomic number 98. The element was first synthesized in 1950 at the Lawrence Berkeley National Laboratory (then the University of California Radiation Laboratory), by bombarding curium with alpha particles (helium-4 ions). It is an actinide element, the sixth transuranium element to be synthesized, and has the second-highest atomic mass of all the elements that have been produced in amounts large enough to see with the unaided eye (after einsteinium). The element was named after the university and the state of California.\n\nTwo crystalline forms exist for californium under normal pressure: one above and one below . A third form exists at high pressure. Californium slowly tarnishes in air at room temperature. Compounds of californium are dominated by the +3 oxidation state. The most stable of californium's twenty known isotopes is californium-251, which has a half-life of 898 years. This short half-life means the element is not found in significant quantities in the Earth's crust. Californium-252, with a half-life of about 2.64 years, is the most common isotope used and is produced at the Oak Ridge National Laboratory in the United States and the Research Institute of Atomic Reactors in Russia.\n\nCalifornium is one of the few transuranium elements that have practical applications. Most of these applications exploit the property of certain isotopes of californium to emit neutrons. For example, californium can be used to help start up nuclear reactors, and it is employed as a source of neutrons when studying materials using neutron diffraction and neutron spectroscopy. Californium can also be used in nuclear synthesis of higher mass elements; oganesson (element 118) was synthesized by bombarding californium-249 atoms with calcium-48 ions. Users of californium must take into account radiological concerns and the element's ability to disrupt the formation of red blood cells by bioaccumulating in skeletal tissue.\n\nCalifornium is a silvery white actinide metal with a melting point of and an estimated boiling point of . The pure metal is malleable and is easily cut with a razor blade. Californium metal starts to vaporize above when exposed to a vacuum. Below californium metal is either ferromagnetic or ferrimagnetic (it acts like a magnet), between 48 and 66 K it is antiferromagnetic (an intermediate state), and above it is paramagnetic (external magnetic fields can make it magnetic). It forms alloys with lanthanide metals but little is known about them.\n\nThe element has two crystalline forms under 1 standard atmosphere of pressure: a double-hexagonal close-packed form dubbed alpha (α) and a face-centered cubic form designated beta (β). The α form exists below 600–800 °C with a density of 15.10 g/cm and the β form exists above 600–800 °C with a density of 8.74 g/cm. At 48 GPa of pressure the β form changes into an orthorhombic crystal system due to delocalization of the atom's 5f electrons, which frees them to bond.\n\nThe bulk modulus of a material is a measure of its resistance to uniform pressure. Californium's bulk modulus is , which is similar to trivalent lanthanide metals but smaller than more familiar metals, such as aluminium (70 GPa).\n\nCalifornium exhibits oxidation states of 4, 3, or 2. It typically forms eight or nine bonds to surrounding atoms or ions. Its chemical properties are predicted to be similar to other primarily 3+ valence actinide elements and the element dysprosium, which is the lanthanide above californium in the periodic table. The element slowly tarnishes in air at room temperature, with the rate increasing when moisture is added. Californium reacts when heated with hydrogen, nitrogen, or a chalcogen (oxygen family element); reactions with dry hydrogen and aqueous mineral acids are rapid. \n\nCalifornium is only water-soluble as the californium(III) cation. Attempts to reduce or oxidize the +3 ion in solution have failed. The element forms a water-soluble chloride, nitrate, perchlorate, and sulfate and is precipitated as a fluoride, oxalate, or hydroxide. Californium is the heaviest actinide to exhibit covalent properties, as is observed in the californium borate.\n\nTwenty radioisotopes of californium have been characterized, the most stable being californium-251 with a half-life of 898 years, californium-249 with a half-life of 351 years, californium-250 with a half-life of 13.08 years, and californium-252 with a half-life of 2.645 years. All the remaining isotopes have half-lives shorter than a year, and the majority of these have half-lives shorter than 20 minutes. The isotopes of californium range in mass number from 237 to 256.\n\nCalifornium-249 is formed from the beta decay of berkelium-249, and most other californium isotopes are made by subjecting berkelium to intense neutron radiation in a nuclear reactor. Although californium-251 has the longest half-life, its production yield is only 10% due to its tendency to collect neutrons (high neutron capture) and its tendency to interact with other particles (high neutron cross-section).\n\nCalifornium-252 is a very strong neutron emitter, which makes it extremely radioactive and harmful. Californium-252 undergoes alpha decay 96.9% of the time to form curium-248 while the remaining 3.1% of decays are spontaneous fission. One microgram (µg) of californium-252 emits 2.3 million neutrons per second, an average of 3.7 neutrons per spontaneous fission. Most of the other isotopes of californium decay to isotopes of curium (atomic number 96) via alpha decay.\n\nCalifornium was first synthesized at the University of California Radiation Laboratory in Berkeley, by the physics researchers Stanley G. Thompson, Kenneth Street, Jr., Albert Ghiorso, and Glenn T. Seaborg on or about February 9, 1950. It was the sixth transuranium element to be discovered; the team announced its discovery on March 17, 1950.\n\nTo produce californium, a microgram-sized target of curium-242 () was bombarded with 35 MeV-alpha particles () in the cyclotron at Berkeley, which produced californium-245 () plus one free neutron ().\nOnly about 5,000 atoms of californium were produced in this experiment, and these atoms had a half-life of 44 minutes.\n\nThe discoverers named the new element after the university and the state. This was a break from the convention used for elements 95 to 97, which drew inspiration from how the elements directly above them in the periodic table were named. However, the element directly above element 98 in the periodic table, dysprosium, has a name that simply means \"hard to get at\" so the researchers decided to set aside the informal naming convention. They added that \"the best we can do is to point out [that] ... searchers a century ago found it difficult to get to California.\"\n\nWeighable quantities of californium were first produced by the irradiation of plutonium targets at the Materials Testing Reactor at the National Reactor Testing Station in eastern Idaho; and these findings were reported in 1954. The high spontaneous fission rate of californium-252 was observed in these samples. The first experiment with californium in concentrated form occurred in 1958. The isotopes californium-249 to californium-252 were isolated that same year from a sample of plutonium-239 that had been irradiated with neutrons in a nuclear reactor for five years. Two years later, in 1960, Burris Cunningham and James Wallman of the Lawrence Radiation Laboratory of the University of California created the first californium compounds—californium trichloride, californium oxychloride, and californium oxide—by treating californium with steam and hydrochloric acid.\n\nThe High Flux Isotope Reactor (HFIR) at the Oak Ridge National Laboratory (ORNL) in Oak Ridge, Tennessee, started producing small batches of californium in the 1960s. By 1995, the HFIR nominally produced of californium annually. Plutonium supplied by the United Kingdom to the United States under the 1958 US-UK Mutual Defence Agreement was used for californium production.\n\nThe Atomic Energy Commission sold californium-252 to industrial and academic customers in the early 1970s for $10 per microgram and an average of of californium-252 were shipped each year from 1970 to 1990. Californium metal was first prepared in 1974 by Haire and Baybarz who reduced californium(III) oxide with lanthanum metal to obtain microgram amounts of sub-micrometer thick films.\n\nTraces of californium can be found near facilities that use the element in mineral prospecting and in medical treatments. The element is fairly insoluble in water, but it adheres well to ordinary soil; and concentrations of it in the soil can be 500 times higher than in the water surrounding the soil particles.\n\nFallout from atmospheric nuclear testing prior to 1980 contributed a small amount of californium to the environment. Californium isotopes with mass numbers 249, 252, 253, and 254 have been observed in the radioactive dust collected from the air after a nuclear explosion. Californium is not a major radionuclide at United States Department of Energy legacy sites since it was not produced in large quantities.\n\nCalifornium was once believed to be produced in supernovas, as their decay matches the 60-day half-life of Cf. However, subsequent studies failed to demonstrate any californium spectra, and supernova light curves are now thought to follow the decay of nickel-56.\n\nThe transuranic elements from americium to fermium, including californium, occurred naturally in the natural nuclear fission reactor at Oklo, but no longer do so.\n\nCalifornium is produced in nuclear reactors and particle accelerators. Californium-250 is made by bombarding berkelium-249 () with neutrons, forming berkelium-250 () via neutron capture (n,γ) which, in turn, quickly beta decays (β) to californium-250 () in the following reaction:\nBombardment of californium-250 with neutrons produces californium-251 and californium-252.\n\nProlonged irradiation of americium, curium, and plutonium with neutrons produces milligram amounts of californium-252 and microgram amounts of californium-249. As of 2006, curium isotopes 244 to 248 are irradiated by neutrons in special reactors to produce primarily californium-252 with lesser amounts of isotopes 249 to 255.\n\nMicrogram quantities of californium-252 are available for commercial use through the U.S. Nuclear Regulatory Commission. Only two sites produce californium-252: the Oak Ridge National Laboratory in the United States, and the Research Institute of Atomic Reactors in Dimitrovgrad, Russia. As of 2003, the two sites produce 0.25 grams and 0.025 grams of californium-252 per year, respectively.\n\nThree californium isotopes with significant half-lives are produced, requiring a total of 15 neutron captures by uranium-238 without nuclear fission or alpha decay occurring during the process. Californium-253 is at the end of a production chain that starts with uranium-238, includes several isotopes of plutonium, americium, curium, berkelium, and the californium isotopes 249 to 253 (see diagram).\n\nCalifornium-252 has a number of specialized applications as a strong neutron emitter, and each microgram of fresh californium produces 139 million neutrons per minute. This property makes californium useful as a neutron startup source for some nuclear reactors and as a portable (non-reactor based) neutron source for neutron activation analysis to detect trace amounts of elements in samples. Neutrons from californium are employed as a treatment of certain cervical and brain cancers where other radiation therapy is ineffective. It has been used in educational applications since 1969 when the Georgia Institute of Technology received a loan of 119 µg of californium-252 from the Savannah River Plant. It is also used with online elemental coal analyzers and bulk material analyzers in the coal and cement industries.\n\nNeutron penetration into materials makes californium useful in detection instruments such as fuel rod scanners; neutron radiography of aircraft and weapons components to detect corrosion, bad welds, cracks and trapped moisture; and in portable metal detectors. Neutron moisture gauges use californium-252 to find water and petroleum layers in oil wells, as a portable neutron source for gold and silver prospecting for on-the-spot analysis, and to detect ground water movement. The major uses of californium-252 in 1982 were, in order of use, reactor start-up (48.3%), fuel rod scanning (25.3%), and activation analysis (19.4%). By 1994 most californium-252 was used in neutron radiography (77.4%), with fuel rod scanning (12.1%) and reactor start-up (6.9%) as important but distant secondary uses.\n\nCalifornium-251 has a very small calculated critical mass of about , high lethality, and a relatively short period of toxic environmental irradiation. The low critical mass of californium led to some exaggerated claims about possible uses for the element.\n\nIn October 2006, researchers announced that three atoms of oganesson (element 118) had been identified at the Joint Institute for Nuclear Research in Dubna, Russia, as the product of bombardment of californium-249 with calcium-48, making it the heaviest element ever synthesized. The target for this experiment contained about 10 mg of californium-249 deposited on a titanium foil of 32 cm area. Californium has also been used to produce other transuranium elements; for example, element 103 (later named lawrencium) was first synthesized in 1961 by bombarding californium with boron nuclei.\n\nCalifornium that bioaccumulates in skeletal tissue releases radiation that disrupts the body's ability to form red blood cells. The element plays no natural biological role in any organism due to its intense radioactivity and low concentration in the environment.\n\nCalifornium can enter the body from ingesting contaminated food or drinks or by breathing air with suspended particles of the element. Once in the body, only 0.05% of the californium will reach the bloodstream. About 65% of that californium will be deposited in the skeleton, 25% in the liver, and the rest in other organs, or excreted, mainly in urine. Half of the californium deposited in the skeleton and liver are gone in 50 and 20 years, respectively. Californium in the skeleton adheres to bone surfaces before slowly migrating throughout the bone.\n\nThe element is most dangerous if taken into the body. In addition, californium-249 and californium-251 can cause tissue damage externally, through gamma ray emission. Ionizing radiation emitted by californium on bone and in the liver can cause cancer.\n\n\n"}
{"id": "15811818", "url": "https://en.wikipedia.org/wiki?curid=15811818", "title": "Catatumbo lightning", "text": "Catatumbo lightning\n\nCatatumbo lightning () is an atmospheric phenomenon in Venezuela. It occurs only over the mouth of the Catatumbo River where it empties into Lake Maracaibo.\n\nIt originates from a mass of storm clouds at a height of more than 1 km, and occurs during 140 to 160 nights a year, 10 hours per day and up to 280 times per hour. It occurs over and around Lake Maracaibo, typically over the bog area formed where the Catatumbo River flows into the lake.\n\nCatatumbo lightning changes its frequency throughout the year, and it is different from year to year. For example, it ceased from January to March 2010, apparently due to drought, temporarily raising fears that it might have been extinguished permanently.\n\nSome authors have misunderstood an early reference to the lightning in Lope de Vega's description in his epic \"La Dragontea\" of an incident during the attack against San Juan de Puerto Rico by the English corsair Francis Drake. The Prussian naturalist and explorer Alexander von Humboldt once described it.\nItalian geographer Agustin Codazzi described it: \"like a continuous lightning, and its position such that, located almost on the meridian of the mouth of the lake, it directs the navigators as a lighthouse.\"\n\nThe phenomenon became so celebrated that it was depicted in the flag and coat of arms of the state of Zulia, which contains Lake Maracaibo, and mentioned in the state's anthem. This phenomenon has been popularly known for centuries as the Lighthouse of Maracaibo, since it is visible for miles around Lake Maracaibo.\n\nCatatumbo lightning usually develops between the coordinates and . The storms (and associated lightning) are likely the result of the winds blowing across the Maracaibo Lake and surrounding swampy plains. These air masses inevitably meet the high mountain ridges of the Andes, the Perijá Mountains (3,750 m), and Mérida's Cordillera, enclosing the plain from three sides. The heat and moisture collected across the plains create electrical charges and, as the air masses are destabilized by the mountain ridges, result in thunderstorm activity. The phenomenon is characterized by almost continuous lightning, mostly within the clouds, which is produced in a large vertical development of clouds.\n\nThe lightning produces a great quantity of ozone though its instability makes it dubious that it has any effect on the ozonosphere.\n\nAmong the major modern studies there is the one done by Melchor Centeno, which attributes the origin of the thunderstorms to closed wind circulation in the region. Between 1966 and 1970, Andrew Zavrostky investigated the area three times, with assistance from the University of the Andes. He concluded that the lightning has several epicenters in the marshes of Juan Manuel de Aguas National Park, Claras Aguas Negras, and west Lake Maracaibo. In 1991 he suggested that the phenomenon occurred due to cold and warm air currents meeting around the area. The study also speculated that an isolated cause for the lightning might be the presence of uranium in the bedrock.\n\nBetween 1997 and 2000 Nelson Falcón and collaborators conducted four studies, and proposed a microphysics model of Catatumbo lightning. He identified the methane produced by the swamps and the oil deposits in the area as a major cause of the phenomenon. The methane model is based on symmetry properties of the methane. Different studies have indicated that this model does not agree with the observed behaviour of Catatumbo lightning, as - for example - it indicates that there must be more lightning in the dry season (January–February), and less in the wet season (April–May and September–October).\n\nA team from the Center for Scientific Modeling at Universidad del Zulia, coordinated by Ángel G. Muñoz, has investigated the impact of different atmospheric variables on Catatumbo lightning's daily, seasonal and year-to-year variability, finding relationships with the Inter-Tropical Convergence Zone, ENSO, the Caribbean Low-Level Jet, and the local winds and convective available potential energy.\n\nUsing satellite data, two groups of researchers led by Rachel Albrecht and Ricardo Bürgesser have provided detailed analysis about Catatumbo lightning's location, timing and number of discharges per square kilometer (density).\n\nA more recent study has shown that it is possible to forecast lightning in the Lake Maracaibo basin up to a few months in advance, based in the variability of the Lake Maracaibo Low-Level Jet and its interactions with predictable climate modes like ENSO and the Caribbean Low-Level Jet. The study also shows that the forecast accuracy is significantly higher when an index based on a combination of winds and convective available potential energy is used. The index seems to capture well the compound effect of multiple climate drivers.\n\nIn order to calibrate physical and statistical models to make their forecasts, the team has been acquiring data with tethered balloons and micro-weather stations tied to the balloon's line.\n"}
{"id": "40809862", "url": "https://en.wikipedia.org/wiki?curid=40809862", "title": "Centrifugal pendulum absorber", "text": "Centrifugal pendulum absorber\n\nA centrifugal pendulum absorber is a type of Tuned mass damper. It reduces the amplitude of a Torsional vibration in drive trains that use a combustion engine.\n\nThe centrifugal pendulum absorber was first patented in 1937 by R. Sarazin and a different version by R. Chilton in 1938. Generally, both Sarazin and Chilton are credited with the invention. Sarazin's work was used during World War II by Pratt & Whitney for aircraft engines with increased power output. The power increase caused an increase in torsional vibrations which threatened the durability. This resulted in the Pratt & Whitney R-2800 engine that used pendulum weights attached to the crank shaft.\n\nThe use of centrifugal pendulum absorbers in land vehicles did not start until later. Although internal combustion engines had always caused torsional vibrations in the drive train, the vibration amplitude was generally not high enough to affect durability or driver comfort. One application existed in tuned racing engines where torsional crank shaft vibrations could cause damage to the cam shaft or valves. In this application, a centrifugal pendulum absorber, the Rattler® absorber, is directly attached to the crank shaft. Although the design differs from that of Sarazin or Chilton, the Rattler® still follows the same physical principle.\n\nIn 2010, centrifugal pendulum absorbers following the patents of Sarazin and Chilton were introduced in a BMW 320D. The reason for it was again the increase in torsional vibrations from higher power engines. In this case, the 4-cylinder diesel engine BMW N47. Unlike the previous designs, the centrifugal pendulum absorber was not attached to the combustion engine but attached to a dual mass flywheel.\n\nThe function of a centrifugal pendulum absorber is as with any tuned mass absorbers based on an absorption principle rather than a damping principle. The distinction is significant since dampers reduce the vibration amplitude by converting the vibration energy into heat. Absorbers store the energy and return it to the vibration system at the appropriate time. Centrifugal pendulum absorbers like tuned mass absorbers are not part of the force/torque flow.\n\nThe centrifugal pendulum absorber differs from the tuned mass absorber in the absorption range. It is effective for an entire order instead of a narrow frequency range.\n\nInternal combustion engines follow a development trend towards a reduction in the number of cylinders, increased energy output per cylinder and driving at lower engine speeds. This leads to an increased engine efficiency but causes the engine's torsional vibrations to increase. The vibrations lead to durability concerns as well as a comfort reduction for the passengers and have to be avoided through the use of torsion dampers and absorbers. This situation moves the balance between the cost of the centrifugal pendulum absorber technology and the benefit for the drive train efficiency.\nThe following cars use centrifugal pendulum absorbers\n\n"}
{"id": "4213804", "url": "https://en.wikipedia.org/wiki?curid=4213804", "title": "Charley Dewberry", "text": "Charley Dewberry\n\nCharley Dewberry is the Dean of Gutenberg College in Eugene, Oregon.\n\nHe has worked as a stream and field worker in the Pacific Northwest. He is the chief architect of the Siuslaw partnership's Knowles Creek restoration project, one of five finalists in 2003 for the prestigious international Thiess Riverprize. Dewberry continues diving and teaching salmon-survey techniques.\n\nOver the last 25 years he has worked at the U.S. Forest Service's anadromous fish unit in Corvallis, Oregon; for the Pacific Rivers Council; and for Ecotrust. He was one of the original divers in the development of the Hankin-Reeves whole-basin survey technique, a standard method for conducting juvenile salmon surveys. He managed the Pacific River Council's national river restoration project, which published \"Entering the Watershed\" (Island Press).\n\nHe has published two books, \"Saving Science: A Critique of Science and Its Role in Salmon Recovery\" and \"Intelligent Discourse: Exposing the fallacious standoff between Evolution and Intelligent Design.\"\n\n"}
{"id": "5387", "url": "https://en.wikipedia.org/wiki?curid=5387", "title": "Condensed matter physics", "text": "Condensed matter physics\n\nCondensed matter physics is the field of physics that deals with the macroscopic and microscopic physical properties of matter. In particular it is concerned with the \"condensed\" phases that appear whenever the number of constituents in a system is extremely large and the interactions between the constituents are strong. The most familiar examples of condensed phases are solids and liquids, which arise from the electromagnetic forces between atoms. Condensed matter physicists seek to understand the behavior of these phases by using physical laws. In particular, they include the laws of quantum mechanics, electromagnetism and statistical mechanics.\n\nThe most familiar condensed phases are solids and liquids while more exotic condensed phases include the superconducting phase exhibited by certain materials at low temperature, the ferromagnetic and antiferromagnetic phases of spins on crystal lattices of atoms, and the Bose–Einstein condensate found in ultracold atomic systems. The study of condensed matter physics involves measuring various material properties via experimental probes along with using methods of theoretical physics to develop mathematical models that help in understanding physical behavior.\n\nThe diversity of systems and phenomena available for study makes condensed matter physics the most active field of contemporary physics: one third of all American physicists self-identify as condensed matter physicists, and the Division of Condensed Matter Physics is the largest division at the American Physical Society. The field overlaps with chemistry, materials science, and nanotechnology, and relates closely to atomic physics and biophysics. The theoretical physics of condensed matter shares important concepts and methods with that of particle physics and nuclear physics.\n\nA variety of topics in physics such as crystallography, metallurgy, elasticity, magnetism, etc., were treated as distinct areas until the 1940s, when they were grouped together as \"solid state physics\". Around the 1960s, the study of physical properties of liquids was added to this list, forming the basis for the new, related specialty of condensed matter physics. According to physicist Philip Warren Anderson, the term was coined by him and Volker Heine, when they changed the name of their group at the Cavendish Laboratories, Cambridge from \"Solid state theory\" to \"Theory of Condensed Matter\" in 1967, as they felt it did not exclude their interests in the study of liquids, nuclear matter, and so on. Although Anderson and Heine helped popularize the name \"condensed matter\", it had been present in Europe for some years, most prominently in the form of a journal published in English, French, and German by Springer-Verlag titled \"Physics of Condensed Matter\", which was launched in 1963. The funding environment and Cold War politics of the 1960s and 1970s were also factors that lead some physicists to prefer the name \"condensed matter physics\", which emphasized the commonality of scientific problems encountered by physicists working on solids, liquids, plasmas, and other complex matter, over \"solid state physics\", which was often associated with the industrial applications of metals and semiconductors. The Bell Telephone Laboratories was one of the first institutes to conduct a research program in condensed matter physics.\n\nReferences to \"condensed\" state can be traced to earlier sources. For example, in the introduction to his 1947 book \"Kinetic Theory of Liquids\", Yakov Frenkel proposed that \"The kinetic theory of liquids must accordingly be developed as a generalization and extension of the kinetic theory of solid bodies. As a matter of fact, it would be more correct to unify them under the title of 'condensed bodies'\".\n\nOne of the first studies of condensed states of matter was by English chemist Humphry Davy, in the first decades of the nineteenth century. Davy observed that of the forty chemical elements known at the time, twenty-six had metallic properties such as lustre, ductility and high electrical and thermal conductivity. This indicated that the atoms in John Dalton's atomic theory were not indivisible as Dalton claimed, but had inner structure. Davy further claimed that elements that were then believed to be gases, such as nitrogen and hydrogen could be liquefied under the right conditions and would then behave as metals.\n\nIn 1823, Michael Faraday, then an assistant in Davy's lab, successfully liquefied chlorine and went on to liquefy all known gaseous elements, except for nitrogen, hydrogen, and oxygen. Shortly after, in 1869, Irish chemist Thomas Andrews studied the phase transition from a liquid to a gas and coined the term critical point to describe the condition where a gas and a liquid were indistinguishable as phases, and Dutch physicist Johannes van der Waals supplied the theoretical framework which allowed the prediction of critical behavior based on measurements at much higher temperatures. By 1908, James Dewar and Heike Kamerlingh Onnes were successfully able to liquefy hydrogen and then newly discovered helium, respectively.\n\nPaul Drude in 1900 proposed the first theoretical model for a classical electron moving through a metallic solid. Drude's model described properties of metals in terms of a gas of free electrons, and was the first microscopic model to explain empirical observations such as the Wiedemann–Franz law. However, despite the success of Drude's free electron model, it had one notable problem: it was unable to correctly explain the electronic contribution to the specific heat and magnetic properties of metals, and the temperature dependence of resistivity at low temperatures.\n\nIn 1911, three years after helium was first liquefied, Onnes working at University of Leiden discovered superconductivity in mercury, when he observed the electrical resistivity of mercury to vanish at temperatures below a certain value. The phenomenon completely surprised the best theoretical physicists of the time, and it remained unexplained for several decades. Albert Einstein, in 1922, said regarding contemporary theories of superconductivity that \"with our far-reaching ignorance of the quantum mechanics of composite systems we are very far from being able to compose a theory out of these vague ideas.\"\n\nDrude's classical model was augmented by Wolfgang Pauli, Arnold Sommerfeld, Felix Bloch and other physicists. Pauli realized that the free electrons in metal must obey the Fermi–Dirac statistics. Using this idea, he developed the theory of paramagnetism in 1926. Shortly after, Sommerfeld incorporated the Fermi–Dirac statistics into the free electron model and made it better able to explain the heat capacity. Two years later, Bloch used quantum mechanics to describe the motion of a quantum electron in a periodic lattice. The mathematics of crystal structures developed by Auguste Bravais, Yevgraf Fyodorov and others was used to classify crystals by their symmetry group, and tables of crystal structures were the basis for the series \"International Tables of Crystallography\", first published in 1935. Band structure calculations was first used in 1930 to predict the properties of new materials, and in 1947 John Bardeen, Walter Brattain and William Shockley developed the first semiconductor-based transistor, heralding a revolution in electronics.\nIn 1879, Edwin Herbert Hall working at the Johns Hopkins University discovered a voltage developing across conductors transverse to an electric current in the conductor and magnetic field perpendicular to the current. This phenomenon arising due to the nature of charge carriers in the conductor came to be termed the Hall effect, but it was not properly explained at the time, since the electron was experimentally discovered 18 years later. After the advent of quantum mechanics, Lev Landau in 1930 developed the theory of Landau quantization and laid the foundation for the theoretical explanation for the quantum Hall effect discovered half a century later.\n\nMagnetism as a property of matter has been known in China since 4000 BC. However, the first modern studies of magnetism only started with the development of electrodynamics by Faraday, Maxwell and others in the nineteenth century, which included classifying materials as ferromagnetic, paramagnetic and diamagnetic based on their response to magnetization. Pierre Curie studied the dependence of magnetization on temperature and discovered the Curie point phase transition in ferromagnetic materials. In 1906, Pierre Weiss introduced the concept of magnetic domains to explain the main properties of ferromagnets. The first attempt at a microscopic description of magnetism was by Wilhelm Lenz and Ernst Ising through the Ising model that described magnetic materials as consisting of a periodic lattice of spins that collectively acquired magnetization. The Ising model was solved exactly to show that spontaneous magnetization cannot occur in one dimension but is possible in higher-dimensional lattices. Further research such as by Bloch on spin waves and Néel on antiferromagnetism led to developing new magnetic materials with applications to magnetic storage devices.\n\nThe Sommerfeld model and spin models for ferromagnetism illustrated the successful application of quantum mechanics to condensed matter problems in the 1930s. However, there still were several unsolved problems, most notably the description of superconductivity and the Kondo effect. After World War II, several ideas from quantum field theory were applied to condensed matter problems. These included recognition of collective excitation modes of solids and the important notion of a quasiparticle. Russian physicist Lev Landau used the idea for the Fermi liquid theory wherein low energy properties of interacting fermion systems were given in terms of what are now termed Landau-quasiparticles. Landau also developed a mean field theory for continuous phase transitions, which described ordered phases as spontaneous breakdown of symmetry. The theory also introduced the notion of an order parameter to distinguish between ordered phases. Eventually in 1965, John Bardeen, Leon Cooper and John Schrieffer developed the so-called BCS theory of superconductivity, based on the discovery that arbitrarily small attraction between two electrons of opposite spin mediated by phonons in the lattice can give rise to a bound state called a Cooper pair.\nThe study of phase transition and the critical behavior of observables, termed critical phenomena, was a major field of interest in the 1960s. Leo Kadanoff, Benjamin Widom and Michael Fisher developed the ideas of critical exponents and widom scaling. These ideas were unified by Kenneth G. Wilson in 1972, under the formalism of the renormalization group in the context of quantum field theory.\n\nThe quantum Hall effect was discovered by Klaus von Klitzing in 1980 when he observed the Hall conductance to be integer multiples of a fundamental constant formula_1.(see figure) The effect was observed to be independent of parameters such as system size and impurities. In 1981, theorist Robert Laughlin proposed a theory explaining the unanticipated precision of the integral plateau. It also implied that the Hall conductance can be characterized in terms of a topological invariable called Chern number. Shortly after, in 1982, Horst Störmer and Daniel Tsui observed the fractional quantum Hall effect where the conductance was now a rational multiple of a constant. Laughlin, in 1983, realized that this was a consequence of quasiparticle interaction in the Hall states and formulated a variational method solution, named the Laughlin wavefunction. The study of topological properties of the fractional Hall effect remains an active field of research.\nIn 1986, Karl Müller and Johannes Bednorz discovered the first high temperature superconductor, a material which was superconducting at temperatures as high as 50 kelvins. It was realized that the high temperature superconductors are examples of strongly correlated materials where the electron–electron interactions play an important role. A satisfactory theoretical description of high-temperature superconductors is still not known and the field of strongly correlated materials continues to be an active research topic.\nIn 2009, David Field and researchers at Aarhus University discovered spontaneous electric fields when creating prosaic films of various gases. This has more recently expanded to form the research area of spontelectrics.\n\nIn 2012 several groups released preprints which suggest that samarium hexaboride has the properties of a topological insulator in accord with the earlier theoretical predictions. Since samarium hexaboride is an established Kondo insulator, i.e. a strongly correlated electron material, the existence of a topological surface state in this material would lead to a topological insulator with strong electronic correlations.\n\nTheoretical condensed matter physics involves the use of theoretical models to understand properties of states of matter. These include models to study the electronic properties of solids, such as the Drude model, the Band structure and the density functional theory. Theoretical models have also been developed to study the physics of phase transitions, such as the Ginzburg–Landau theory, critical exponents and the use of mathematical methods of quantum field theory and the renormalization group. Modern theoretical studies involve the use of numerical computation of electronic structure and mathematical tools to understand phenomena such as high-temperature superconductivity, topological phases, and gauge symmetries.\n\nTheoretical understanding of condensed matter physics is closely related to the notion of emergence, wherein complex assemblies of particles behave in ways dramatically different from their individual constituents. For example, a range of phenomena related to high temperature superconductivity are understood poorly, although the microscopic physics of individual electrons and lattices is well known. Similarly, models of condensed matter systems have been studied where collective excitations behave like photons and electrons, thereby describing electromagnetism as an emergent phenomenon. Emergent properties can also occur at the interface between materials: one example is the lanthanum aluminate-strontium titanate interface, where two non-magnetic insulators are joined to create conductivity, superconductivity, and ferromagnetism.\n\nThe metallic state has historically been an important building block for studying properties of solids. The first theoretical description of metals was given by Paul Drude in 1900 with the Drude model, which explained electrical and thermal properties by describing a metal as an ideal gas of then-newly discovered electrons. He was able to derive the empirical Wiedemann-Franz law and get results in close agreement with the experiments. This classical model was then improved by Arnold Sommerfeld who incorporated the Fermi–Dirac statistics of electrons and was able to explain the anomalous behavior of the specific heat of metals in the Wiedemann–Franz law. In 1912, The structure of crystalline solids was studied by Max von Laue and Paul Knipping, when they observed the X-ray diffraction pattern of crystals, and concluded that crystals get their structure from periodic lattices of atoms. In 1928, Swiss physicist Felix Bloch provided a wave function solution to the Schrödinger equation with a periodic potential, called the Bloch wave.\n\nCalculating electronic properties of metals by solving the many-body wavefunction is often computationally hard, and hence, approximation methods are needed to obtain meaningful predictions. The Thomas–Fermi theory, developed in the 1920s, was used to estimate system energy and electronic density by treating the local electron density as a variational parameter. Later in the 1930s, Douglas Hartree, Vladimir Fock and John Slater developed the so-called Hartree–Fock wavefunction as an improvement over the Thomas–Fermi model. The Hartree–Fock method accounted for exchange statistics of single particle electron wavefunctions. In general, it's very difficult to solve the Hartree–Fock equation. Only the free electron gas case can be solved exactly. Finally in 1964–65, Walter Kohn, Pierre Hohenberg and Lu Jeu Sham proposed the density functional theory which gave realistic descriptions for bulk and surface properties of metals. The density functional theory (DFT) has been widely used since the 1970s for band structure calculations of variety of solids.\n\nSome states of matter exhibit symmetry breaking, where the relevant laws of physics possess some symmetry that is broken. A common example is crystalline solids, which break continuous translational symmetry. Other examples include magnetized ferromagnets, which break rotational symmetry, and more exotic states such as the ground state of a BCS superconductor, that breaks U(1) phase rotational symmetry.\n\nGoldstone's theorem in quantum field theory states that in a system with broken continuous symmetry, there may exist excitations with arbitrarily low energy, called the Goldstone bosons. For example, in crystalline solids, these correspond to phonons, which are quantized versions of lattice vibrations.\n\nPhase transition refers to the change of phase of a system, which is brought about by change in an external parameter such as temperature. Classical phase transition occurs at finite temperature when the order of the system was destroyed. For example, when ice melts and becomes water, the ordered crystal structure is destroyed. In quantum phase transitions, the temperature is set to absolute zero, and the non-thermal control parameter, such as pressure or magnetic field, causes the phase transitions when order is destroyed by quantum fluctuations originating from the Heisenberg uncertainty principle. Here, the different quantum phases of the system refer to distinct ground states of the Hamiltonian. Understanding the behavior of quantum phase transition is important in the difficult tasks of explaining the properties of rare-earth magnetic insulators, high-temperature superconductors, and other substances.\n\nTwo classes of phase transitions occur: first-order transitions and continuous transitions. For the later, the two phases involved do not co-exist at the transition temperature, also called critical point. Near the critical point, systems undergo critical behavior, wherein several of their properties such as correlation length, specific heat, and magnetic susceptibility diverge exponentially. These critical phenomena possess serious challenges to physicists because normal macroscopic laws are no longer valid in the region and novel ideas and methods must be invented to find the new laws that can describe the system.\n\nThe simplest theory that can describe continuous phase transitions is the Ginzburg–Landau theory, which works in the so-called mean field approximation. However, it can only roughly explain continuous phase transition for ferroelectrics and type I superconductors which involves long range microscopic interactions. For other types of systems that involves short range interactions near the critical point, a better theory is needed.\n\nNear the critical point, the fluctuations happen over broad range of size scales while the feature of the whole system is scale invariant. Renormalization group methods successively average out the shortest wavelength fluctuations in stages while retaining their effects into the next stage. Thus, the changes of a physical system as viewed at different size scales can be investigated systematically. The methods, together with powerful computer simulation, contribute greatly to the explanation of the critical phenomena associated with continuous phase transition.\n\nExperimental condensed matter physics involves the use of experimental probes to try to discover new properties of materials. Such probes include effects of electric and magnetic fields, measuring response functions, transport properties and thermometry. Commonly used experimental methods include spectroscopy, with probes such as X-rays, infrared light and inelastic neutron scattering; study of thermal response, such as specific heat and measuring transport via thermal and heat conduction.\n\nSeveral condensed matter experiments involve scattering of an experimental probe, such as X-ray, optical photons, neutrons, etc., on constituents of a material. The choice of scattering probe depends on the observation energy scale of interest. Visible light has energy on the scale of 1 electron volt (eV) and is used as a scattering probe to measure variations in material properties such as dielectric constant and refractive index. X-rays have energies of the order of 10 keV and hence are able to probe atomic length scales, and are used to measure variations in electron charge density.\n\nNeutrons can also probe atomic length scales and are used to study scattering off nuclei and electron spins and magnetization (as neutrons have spin but no charge). Coulomb and Mott scattering measurements can be made by using electron beams as scattering probes. Similarly, positron annihilation can be used as an indirect measurement of local electron density. Laser spectroscopy is an excellent tool for studying the microscopic properties of a medium, for example, to study forbidden transitions in media with nonlinear optical spectroscopy. \n\nIn experimental condensed matter physics, external magnetic fields act as thermodynamic variables that control the state, phase transitions and properties of material systems. Nuclear magnetic resonance (NMR) is a method by which external magnetic fields are used to find resonance modes of individual electrons, thus giving information about the atomic, molecular, and bond structure of their neighborhood. NMR experiments can be made in magnetic fields with strengths up to 60 Tesla. Higher magnetic fields can improve the quality of NMR measurement data. Quantum oscillations is another experimental method where high magnetic fields are used to study material properties such as the geometry of the Fermi surface. High magnetic fields will be useful in experimentally testing of the various theoretical predictions such as the quantized magnetoelectric effect, image magnetic monopole, and the half-integer quantum Hall effect.\n\nUltracold atom trapping in optical lattices is an experimental tool commonly used in condensed matter physics, and in atomic, molecular, and optical physics. The method involves using optical lasers to form an interference pattern, which acts as a \"lattice\", in which ions or atoms can be placed at very low temperatures. Cold atoms in optical lattices are used as \"quantum simulators\", that is, they act as controllable systems that can model behavior of more complicated systems, such as frustrated magnets. In particular, they are used to engineer one-, two- and three-dimensional lattices for a Hubbard model with pre-specified parameters, and to study phase transitions for antiferromagnetic and spin liquid ordering.\n\nIn 1995, a gas of rubidium atoms cooled down to a temperature of 170 nK was used to experimentally realize the Bose–Einstein condensate, a novel state of matter originally predicted by S. N. Bose and Albert Einstein, wherein a large number of atoms occupy one quantum state.\n\nResearch in condensed matter physics has given rise to several device applications, such as the development of the semiconductor transistor, laser technology, and several phenomena studied in the context of nanotechnology. Methods such as scanning-tunneling microscopy can be used to control processes at the nanometer scale, and have given rise to the study of nanofabrication.\n\nIn quantum computation, information is represented by quantum bits, or qubits. The qubits may decohere quickly before useful computation is completed. This serious problem must be solved before quantum computing may be realized. To solve this problem, several promising approaches are proposed in condensed matter physics, including Josephson junction qubits, spintronic qubits using the spin orientation of magnetic materials, or the topological non-Abelian anyons from fractional quantum Hall effect states.\n\nCondensed matter physics also has important uses for biophysics, for example, the experimental method of magnetic resonance imaging, which is widely used in medical diagnosis.\n\n"}
{"id": "3873342", "url": "https://en.wikipedia.org/wiki?curid=3873342", "title": "Conductance quantum", "text": "Conductance quantum\n\nThe conductance quantum, denoted by the symbol , is the quantized unit of electrical conductance. It is defined by the elementary charge \"e\" and Planck constant \"h\" as:\n\nformula_1= .\n\nIt appears when measuring the conductance of a quantum point contact, and, more generally, is a key component of the Landauer formula, which relates the electrical conductance of a quantum conductor to its quantum properties. It is twice the reciprocal of the von Klitzing constant (2/\"R\").\n\nNote that the conductance quantum does not mean that the conductance of any system must be an integer multiple of \"G\". Instead, it describes the conductance of two quantum channels (one channel for spin up and one channel for spin down) if the probability for transmitting an electron that enters the channel is unity, i.e. if transport through the channel is ballistic. If the transmission probability is less than unity, then the conductance of the channel is less than \"G\". The total conductance of a system is equal to the sum of the conductances of all the parallel quantum channels that make up the system.\n\nIn a 1D wire, connecting two reservoirs of potential formula_2 and formula_3 adiabatically:\n\nThe density of states is \nwhere the factor 2 comes from electron spin degeneracy, formula_5 is Planck's constant, and formula_6 is the electron velocity.\n\nThe voltage is:\nwhere formula_8 is the electron charge.\n\nThe 1D current going across is the current density:\n\nThis results in a quantized conductance:\n\nQuantized conductance occurs in wires that are ballistic conductors, when formula_11. B. J. van Wees et al. first observed the effect in a point contact in 1988. Carbon nanotubes have quantized conductance independent of diameter. The quantum hall effect can be used to precisely measure the conductance quantum value.\n\nA simple, intuitive motivation of the conductance quantum can be made using the Heisenberg uncertainty principle, which states that the minimum energy-time uncertainty is \"ΔEΔt\" ~ \"h\", where \"h\" is the Planck constant. The current \"I\" in a quantum channel can be expressed as \"e/τ\", where \"τ\" is transit time and the \"e\" is electron charge. Applying a voltage \"V\" results in an energy \"E = eV\". If we assume that the energy uncertainty is of order E and the time uncertainty is of order \"τ\", we can write \"ΔEΔt\" ~ \"(eV)(e/I)\" ~ \"h\". Using the fact that the electrical conductance \"G = I/V\", this becomes \"G\" ~ \"e/h\".\n\n"}
{"id": "28855966", "url": "https://en.wikipedia.org/wiki?curid=28855966", "title": "Crivina Power Station", "text": "Crivina Power Station\n\nThe Crivina Power Station () was a large thermal power plant located in Crivina, near Anina in Caraş-Severin County. It had three generating units of 330 MW each, altogether having a total electricity generating capacity of 990 MW. It was intended to be the first oil shale power station built in Romania. The total cost of the oil shale power plant was around US$1 billion. The Crivina Power Station was supplied with 4 million tonnes of oil shale per year from the nearby Anina Mine.\n\nThis power plant works only for 76 hours in reality.\nAt the beginning of the 1970s Nicolae Ceauşescu, then President of Romania, decided to build a large thermal power station in Caraş-Severin County to exploit the large oil shale deposits located in the area. At first the power station was intended to be built in Ticvaniu Mic commune, now part of the Ticvaniu Mare commune, near Oraviţa but the chosen site had a very small water supply that was not sufficient for the power station. In 1976 a new location was found at Crivina, located in the mountains near Anina. In 1983 the construction of the power station was complete and the first 330 MW electric power generation unit was put online. The turbine could not reach its highest potential capacity due to the low quality of the oil shale deposits. The communist regime also built a town that was used to house the power station's workers. It was intended to have 10,000 inhabitants, and a part of Anina had to move to this new location to make way for oil shale surface mines.\n\nThe first of the three power generation units was completed in 1983, and in 1984 generated its first electricity from burning oil shale. The unit functioned only around 2000 hours per year between 1984 and 1988 until it was shut down, its main generator having broken down and was sent to Bucharest for repairs, only for it to be sent to Rovinari in 1990. Eventually between 2003 and 2009, most of the power station was demolished and slowly sold for scrap. The station also had a tall flue gas stack that was at the time Romania's second tallest.\n"}
{"id": "10640305", "url": "https://en.wikipedia.org/wiki?curid=10640305", "title": "Cultural depictions of Stonehenge", "text": "Cultural depictions of Stonehenge\n\nThe Prehistoric landmark of Stonehenge is distinctive and famous enough to have become frequently referenced in popular culture. The landmark has become a symbol of British culture and history, owing to its distinctiveness and its long history of being portrayed in art, literature and advertising campaigns, as well as modern media formats, such as television, film and computer games. This is in part because the arrangement of standing stones topped with lintels is unique, not just in the British Isles, but in the world.\n\nThe interest in 'ancient' Britain can be traced back to the sixteenth and seventeenth century, following the pioneering work of the likes of William Camden, John Aubrey and John Evelyn. The rediscovery of Britain's past was also tied up in the nation's emerging sense of importance as an international power. Antiquarians and archaeologists, notably William Stukeley, were conducting excavations of megalithic sites, including Stonehenge and the nearby Avebury. Their findings caused considerable debate on the history and meaning of such sites, and the earliest depictions reflected a search for a mystical explanation.\n\nEarlier explanations, including the view proposed by Inigo Jones in 1630, that Stonehenge was built by the Romans such was its sophistication and beauty, were disproved in the late seventeenth century. It was proven that Stonehenge was the work of indigenous neolithic peoples. From this period onwards artists made images of barrows, standing stones and excavated objects which increasingly drew on highly imaginative ideas about the prehistoric people that created them. These helped to create the image of Britain that a broadening audience was becoming aware of through illustrated books, prints and maps. Poets and other writers deepened the impact of this visual material by imagining ancient pasts and mythologising the distant roots of the growing British Empire. Debates about British ancestry and national identity saw a growing conviction that the British were an ancient people, and that the newly named 'United Kingdom', might find greater harmony through searching for a common past. For the English, this past was to be found in the West, starting around Stonehenge and stretching into the ancient Celtic regions of Wales and Cornwall.\n\nDuring the early nineteenth century it was artists such as John Constable and J.M.W. Turner who helped to make the megalithic sites a part of the popular imagination and understanding of Britain's past. The philosopher Edmund Burke proposed the idea of the 'sublime' sense as being evoked by 'feelings of danger and terror, obscurity and power, in art as well as life'. This was already a feature of artistic and literary works of the period, and provided the theoretical basis for a growing appreciation of desolate landscapes and ancient ruins. For these reasons Stonehenge became of particular interest for artists. Burke himself wrote \"Stonehenge, neither for disposition nor ornament, has anything admirable; but those huge rude masses of stone, set end on end, and piled high on each other, turn the mind on the immense force necessary for such a work.\" The very nature of the barren Wiltshire landscape, and Salisbury Plain became particularly notable for the apparently miraculous powers that created Stonehenge. William Wordsworth wrote\n\nPile of Stone-henge! So proud to hint yet keep<br>Thy secrets, thou lov'st to stand and hear<br>The plain resounding to the whirlwind's sweep<br>Inmate of lonesome Nature's endless year.\nTurner and Constables' paintings were arranged for a romantic effect and deviated from the actual state of the stones. Turner particularly added stones that were not there in reality, and those that were, were incorrect in their dimensions. Throughout the nineteenth century, a new motive emerged in the depictions of Stonehenge, that of an anti-pagan approach, with paintings by the likes of William Overend Geller, with his painting \"The Druid's Sacrifice\" in 1832. In the novel \"Tess of the d'Urbervilles\" by Thomas Hardy, the main character, Tess, is captured by the police at Stonehenge, the 'heathen' nature of the setting being used to highlight the character's temperament.\n\nThe image of Stonehenge became adapted in the twentieth century by those wishing to advertise using a monument viewed as a symbol of Britain. The Royal Navy exploited this sense of identification by naming an S class destroyer and one of their S Class submarines \"HMS Stonehenge\". The Shell Oil Company commissioned the artist Edward McKnight Kauffer to paint a series of posters during the interwar period, to be used to encourage tourism by car owners. Stonehenge was one of those depicted.\n\nBy now a powerful and instantly recognisable symbol, the monument was featured in a wide number of ways. The Beatles are seen performing on Salisbury Plain with Stonehenge visible in the background in their 1965 film \"Help!\". The site has also been used for concerts, starting with the Stonehenge Free Festival in 1972. Hawkwind regularly performed at the festival and their 1976 \"Atomhenge\" tour featured a stage set that replicated the monument.\n\nBlack Sabbath's 1983 album \"Born Again\" featured an instrumental called \"Stonehenge\", which led manager Don Arden to suggest a stage set based upon the stones. The plans were mistakenly labelled in metres not feet, so that when the stage set was erected it was three times the desired size - dwarfing the band. The set was only used once, and the incident was famously parodied in the 1984 mockumentary film \"This is Spinal Tap\" which featured the titular fictional rock band performing a song named \"Stonehenge\" on stage. In one of the many embarrassing events on their comeback tour, confusion about abbreviating inches and feet results in a Stonehenge replica so small that it was in danger of being trod upon by the Little People hired to dance around it.\n\nThe monument continues to be featured in film, television and radio, either to question the origin or history of Stonehenge, or to play upon its position as an instantly recognisable structure and symbol of Britain. In books by Kurt Vonnegut and S. M. Stirling amongst others, alternative theories are suggested and explored as part of the larger plot. The monument has also become popular in computer games, where alternative uses are often posited for Stonehenge, or its iconic nature is explored.\n\nStonehenge features prominently in Peter Ackroyd's novel \"Hawksmoor\" (1985).\n\nYlvis' song \"Stonehenge\" is a paean to the monument. The lyrics question the motivation and methods of the builders, asking \"What's the purpose of Stonehenge? A giant granite birthday cake or a prison far too easy to escape?\"\n\nStonehenge plays a big role in the \"Doctor Who\" episodes \"The Pandorica Opens\" and \"The Big Bang\".\n\nThe final scenes of Roman Polanski's film \"Tess\"(1979) based on the novel \"Tess of the d'Urbervilles\" by Thomas Hardy, takes place at Stonehenge. The title character is apprehended by police at dawn.\n\nIn the BBC comedy show \"Dave Allen at Large\" a sketch shows Dave Allen at the stones, and bored with the tourist guide lecturing rests and lights up a cigarette and leaning against one of the rocks, sending them toppling over.\n\nThe villain of the 1982 science fiction-horror film \"\" steals Stonehenge and puts pieces of it into masks which have the ability to turn people's heads into bugs and snakes.\n\nIn the cult horror movie \"Troll 2\", a piece of Stonehenge gave the vegetarian antagonists nefarious power until the young hero eats a \"double-decker\" baloney sandwich to destroy it.\n\nIn the 1957 cult horror classic \"Curse of the Demon\", the lead character, played by Dana Andrews, transcribes runes along Stonehenge, which is also mentioned in \"The Rocky Horror Picture Show\" song \"Science-Fiction\", with the lyrics stating: \"Dana Andrews said 'Prunes!' gave him the runes.\"\n\nIn a scene from the 1985 comedy film \"National Lampoon's European Vacation\", Clark Griswold backs a hired Austin Maxi into one of the stones, creating a domino effect on all the upright stones.\n\nIn the 2002 animated film Ice Age, the main characters pass by Stonehenge during their journey. Manny, a woolly mammoth, dismisses Stonehenge as mere \"modern architecture\", commenting \"it'll never last\".\n\nOne of the venues in the 2006 rhythm game \"Guitar Hero II\" takes place at Stonehenge, echoing the above \"Spinal Tap\" example.\n\nIn an episode of \"SpongeBob SquarePants\" called \"Spongehenge\" SpongeBob creates a large monument of stone sponges similar to Stonehenge in order to distract jellyfish.\n\nStonehenge Base in the game \"EarthBound\" is a direct reference to Stonehenge.\nThere is an episode of \"Uchu Sentai Kyuranger\" where the great heroes must facing a Stonehenge Themed Monster.\n\nIn \"\", Stonehenge was the center point of Unicron's six horns, during the Pangaea era, where the horns were in a circular formation.\n\nIn his song \"Elvis Is Everywhere\", Mojo Nixon begs the question \"Who built Stonehenge?\", which the chorus emphatically answers \"Elvis!\"\n\n"}
{"id": "5498706", "url": "https://en.wikipedia.org/wiki?curid=5498706", "title": "Current density", "text": "Current density\n\nIn electromagnetism, current density is the electric current per unit area of cross section. The current density vector is defined as a vector whose magnitude is the electric current per cross-sectional area at a given point in space, its direction being that of the motion of the charges at this point. In SI units, the electric current density is measured in amperes per square metre.\n\nAssume that \"A\" (SI unit: m) is a small surface centred at a given point \"M\" and orthogonal to the motion of the charges at \"M\". If \"I\" (SI unit: A) is the electric current flowing through \"A\", then electric current density \"J\" at \"M\" is given by the limit:\n\nwith surface \"A\" remaining centred at \"M\" and orthogonal to the motion of the charges during the limit process.\n\nThe current density vector J is the vector whose magnitude is the electric current density, and whose direction is the same as the motion of the charges at \"M\".\n\nAt a given time \"t\", if v is the speed of the charges at \"M\", and \"dA\" is an infinitesimal surface centred at \"M\" and orthogonal to v, then during an amount of time \"dt\", only the charge contained in the volume formed by \"dA\" and will flow through \"dA\". This charge is equal to , where \"ρ\" is the charge density at \"M\", and the electric current at \"M\" is . It follows that the current density vector can be expressed as:\n\nThe surface integral of J over a surface \"S\", followed by an integral over the time duration \"t\" to \"t\", gives the total amount of charge flowing through the surface in that time ():\n\nMore concisely, this is the integral of the flux of J across \"S\" between \"t\" and \"t\".\n\nThe area required to calculate the flux is real or imaginary, flat or curved, either as a cross-sectional area or a surface. For example, for charge carriers passing through an electrical conductor, the area is the cross-section of the conductor, at the section considered.\n\nThe vector area is a combination of the magnitude of the area through which the charge carriers pass, \"A\", and a unit vector normal to the area, formula_4. The relation is formula_5.\n\nIf the current density J passes through the area at an angle \"θ\" to the area normal formula_4, then\n\nwhere ⋅ is the dot product of the unit vectors. That is, the component of current density passing through the surface (i.e. normal to it) is , while the component of current density passing tangential to the area is , but there is \"no\" current density actually passing \"through\" the area in the tangential direction. The \"only\" component of current density passing normal to the area is the cosine component.\n\nCurrent density is important to the design of electrical and electronic systems.\n\nCircuit performance depends strongly upon the designed current level, and the current density then is determined by the dimensions of the conducting elements. For example, as integrated circuits are reduced in size, despite the lower current demanded by smaller devices, there is a trend toward higher current densities to achieve higher device numbers in ever smaller chip areas. See Moore's law.\n\nAt high frequencies, the conducting region in a wire becomes confined near its surface which increases the current density in this region. This is known as the skin effect. \n\nHigh current densities have undesirable consequences. Most electrical conductors have a finite, positive resistance, making them dissipate power in the form of heat. The current density must be kept sufficiently low to prevent the conductor from melting or burning up, the insulating material failing, or the desired electrical properties changing. At high current densities the material forming the interconnections actually moves, a phenomenon called \"electromigration\". In superconductors excessive current density may generate a strong enough magnetic field to cause spontaneous loss of the superconductive property.\n\nThe analysis and observation of current density also is used to probe the physics underlying the nature of solids, including not only metals, but also semiconductors and insulators. An elaborate theoretical formalism has developed to explain many fundamental observations.\n\nThe current density is an important parameter in Ampère's circuital law (one of Maxwell's equations), which relates current density to magnetic field.\n\nIn special relativity theory, charge and current are combined into a 4-vector.\n\nCharge carriers which are free to move constitute a free current density, which are given by expressions such as those in this section.\n\nElectric current is a coarse, average quantity that tells what is happening in an entire wire. At position r at time \"t\", the \"distribution\" of charge flowing is described by the current density:\n\nwhere J(r, \"t\") is the current density vector, v(r, \"t\") is the particles' average drift velocity (SI unit: m∙s), and\n\nis the charge density (SI unit: coulombs per cubic metre), in which \"n\"(r, \"t\") is the number of particles per unit volume (\"number density\") (SI unit: m), \"q\" is the charge of the individual particles with density \"n\" (SI unit: coulombs).\n\nA common approximation to the current density assumes the current simply is proportional to the electric field, as expressed by:\n\nwhere E is the electric field and \"σ\" is the electrical conductivity.\n\nConductivity \"σ\" is the reciprocal (inverse) of electrical resistivity and has the SI units of siemens per metre (S⋅m), and E has the SI units of newtons per coulomb (N⋅C) or, equivalently, volts per metre (V⋅m).\n\nA more fundamental approach to calculation of current density is based upon:\n\nindicating the lag in response by the time dependence of \"σ\", and the non-local nature of response to the field by the spatial dependence of \"σ\", both calculated in principle from an underlying microscopic analysis, for example, in the case of small enough fields, the linear response function for the conductive behaviour in the material. See, for example, Giuliani or Rammer. The integral extends over the entire past history up to the present time.\n\nThe above conductivity and its associated current density reflect the fundamental mechanisms underlying charge transport in the medium, both in time and over distance.\n\nA Fourier transform in space and time then results in:\n\nwhere \"σ\"(k, \"ω\") is now a complex function.\n\nIn many materials, for example, in crystalline materials, the conductivity is a tensor, and the current is not necessarily in the same direction as the applied field. Aside from the material properties themselves, the application of magnetic fields can alter conductive behaviour.\nCurrents arise in materials when there is a non-uniform distribution of charge.\nIn dielectric materials, there is a current density corresponding to the net movement of electric dipole moments per unit volume, i.e. the polarization P:\n\nSimilarly with magnetic materials, circulations of the magnetic dipole moments per unit volume, i.e. the magnetization M, lead to magnetization currents:\n\nTogether, these terms add up to form the bound current density in the material (resultant current due to movements of electric and magnetic dipole moments per unit volume):\n\nThe total current is simply the sum of the free and bound currents:\n\nThere is also a displacement current corresponding to the time-varying electric displacement field D:\n\nwhich is an important term in Ampere's circuital law, one of Maxwell's equations, since absence of this term would not predict electromagnetic waves to propagate, or the time evolution of electric fields in general.\n\nSince charge is conserved, current density must satisfy a continuity equation. Here is a derivation from first principles.\n\nThe net flow out of some volume \"V\" (which can have an arbitrary shape but fixed for the calculation) must equal the net change in charge held inside the volume:\n\nwhere \"ρ\" is the charge density, and \"dA\" is a surface element of the surface \"S\" enclosing the volume \"V\". The surface integral on the left expresses the current \"outflow\" from the volume, and the negatively signed volume integral on the right expresses the \"decrease\" in the total charge inside the volume. From the divergence theorem:\n\nHence:\n\nThis relation is valid for any volume, independent of size or location, which implies that:\n\nand this relation is called the continuity equation.\n\nIn electrical wiring, the maximum current density can vary from 4 A⋅mm for a wire with no air circulation around it, to 6 A⋅mm for a wire in free air. Regulations for building wiring list the maximum allowed current of each size of cable in differing conditions. For compact designs, such as windings of SMPS transformers, the value might be as low as 2 A⋅mm. If the wire is carrying high frequency currents, the skin effect may affect the distribution of the current across the section by concentrating the current on the surface of the conductor. In transformers designed for high frequencies, loss is reduced if Litz wire is used for the windings. This is made of multiple isolated wires in parallel with a diameter twice the skin depth. The isolated strands are twisted together to increase the total skin area and to reduce the resistance due to skin effects.\n\nFor the top and bottom layers of printed circuit boards, the maximum current density can be as high as 35 A⋅mm with a copper thickness of 35 μm. Inner layers cannot dissipate as much heat as outer layers; designers of circuit boards avoid putting high-current traces on inner layers.\n\nIn the semiconductors field, the maximum current densities for different elements are given by the manufacturer. Exceeding those limits raises the following problems:\n\nThe following table gives an idea of the maximum current density for various materials.\n\nEven if manufacturers add some margin to their numbers, it is recommended to, at least, double the calculated section to improve the reliability, especially for high quality electronics. One can also notice the importance to keep electronic devices cool to avoid them to be exposed to electromigration and slow diffusion.\n\nIn biological organisms, ion channels regulate the flow of ions (for example, sodium, calcium, potassium) across the membrane in all cells. Current density is measured in pA⋅pF (picoamperes per picofarad), that is, current divided by capacitance, a de facto measure of membrane area.\n\nIn gas discharge lamps, such as flashlamps, current density plays an important role in the output spectrum produced. Low current densities produce spectral line emission and tend to favour longer wavelengths. High current densities produce continuum emission and tend to favour shorter wavelengths. Low current densities for flash lamps are generally around 10 A⋅mm. High current densities can be more than 40 A⋅mm.\n"}
{"id": "52280289", "url": "https://en.wikipedia.org/wiki?curid=52280289", "title": "Diffuser-augmented wind turbine", "text": "Diffuser-augmented wind turbine\n\nA diffuser-augmented wind turbine (DAWT) is a wind turbine modified with a cone-shaped wind diffuser that is used to increase the efficiency of converting wind power to electrical power. The increased efficiency is possible due to the increased wind speeds the diffuser can provide. In traditional bare turbines, the rotor blades are vertically mounted at the top of a support tower or shaft. In a DAWT, the rotor blades are mounted within the diffuser, which is then placed on the top of the support tower. Additional modifications can be made to the diffuser in order to further increase efficiency. \n\nWind power measures how much energy is available in the wind, and it can be represented by the following equation formula_1 where r is air density, A is rotor area, and V is wind velocity. This means that the amount of energy available in the wind is directly proportional to the wind speed cubed. For example, assuming that all other variables are held constant, doubling the wind speed would increase the available energy in the wind by 8 times. A slight increase in wind speed results in dramatic increases in wind power. Unfortunately, this means that if the wind speeds were to slow down even slightly, it would drastically reduce wind power.\n\nMost designs include a cone-shaped diffuser with the purpose of increasing the velocity of the air as it travels through the turbine. In order for this to be possible, the exit hole of the diffuser must be larger than the entrance hole to properly diffuse the air. As wind flows through the diffuser, it travels along the walls, which causes the exiting wind to form vortices of wind when exiting. These vortices cause most of the air to be diffused away from the center of the exit, which creates a low pressure segment of air behind the turbine. The pressure difference accelerates the high pressure air in the front towards the low pressure air in the back, causing a significant increase in speed. If the diffuser were to instead have an exit hole smaller than its entrance, then the opposite effects would be achieved. A high-pressure area would be formed at the exit, severely restricting airflow through the diffuser. Additional designs take the basic diffuser and make additional modifications to further increase power generation.\n\nA design by Yuji Ohya, a professor at Kyushu University, further modified the diffuser by adding a broad ring around the exit hole and an inlet shroud at the entrance—a \"wind lens\". This design amplifies the positive effects of a normal diffuser shroud to result in a more efficient diffuser. The brimmed exit hole creates stronger vortices than a regular diffuser, which means that the pressure difference is greater than it would be with a normal diffuser. As a result, wind is able to reach higher speeds. In addition, the inlet shroud at the entrance makes it easier for air to enter, so air will not be slowed down as much going in.\n\nOther designs are very similar to a diffuser but consist of multiple rotors within it to capture as much electrical energy form the wind. One way to generate more energy would be to increase the rotor area, which can be done in two ways. One way is to increase the diameter of a single rotor, however, this causes unfavorable gains in mass. Another way is to increase the number of rotors per turbine, which does not cause undesirable increases in weight. Systems with up to 45 rotors in one turbine have been tested, and no negative interference has been found between the rotors.\n\nTurbines equipped with a diffuser-shaped shroud and a broad exit ring generate 2–5 times more power than bare wind turbines for any given wind speed or turbine diameter. Further analysis concludes that the Betz's limit can be exceed if the wind turbine were to be equipped with a diffuser. For multi-rotor turbines equipped with a diffuser, the power augmentation is smaller, but still favorable at around 5%–9% increase.\n\nBare wind turbines have several limitations that decrease their efficiency in generating electricity. These limitations play a big role when it comes to mass production of energy. \n\nThe amount of energy that a bare wind turbine can generate is largely dependent on how big the rotor is, which implies that the bigger a turbine is, the more energy it will produce. However, using large turbines results in heavy overall weights and high manufacturing costs. Heavier turbines are also prone to higher malfunction rates which results in higher maintenance costs. In addition, the bigger the turbine is, the more resources that will have to be invested in transporting the massive parts from the factory to where they will be deployed. This is very rarely a viable option since it defeats the whole purpose of affordable alternative energy. \n\nIn addition to manufacturing limitations, there exist limits within the laws of physics that govern how much energy can be generated. Traditional open turbine designs are also limited by Betz's law, which states that for a bare turbine in open wind, no more than 16/27 of the total wind kinetic energy can be converted to electrical energy. 59% is not the most efficient rate, so several designs have been made in order to get around this limitation. Designs include the addition of a \"Wind-Lens\" or using multiple rotors within the diffuser.\n"}
{"id": "1908016", "url": "https://en.wikipedia.org/wiki?curid=1908016", "title": "Einstein solid", "text": "Einstein solid\n\nThe Einstein solid is a model of a solid based on two assumptions:\n\nWhile the assumption that a solid has independent oscillations is very accurate, these oscillations are sound waves or phonons, collective modes involving many atoms. In the Einstein model, however, each atom oscillates independently. Einstein was aware that getting the frequency of the actual oscillations would be difficult, but he nevertheless proposed this theory because it was a particularly clear demonstration that quantum mechanics could solve the specific heat problem in classical mechanics.\n\nThe original theory proposed by Einstein in 1907 has great historical relevance. The heat capacity of solids as predicted by the empirical Dulong–Petit law was required by classical mechanics, the specific heat of solids should be independent of temperature. But experiments at low temperatures showed that the heat capacity changes, going to zero at absolute zero. As the temperature goes up, the specific heat goes up until it approaches the Dulong and Petit prediction at high temperature.\n\nBy employing Planck's quantization assumption, Einstein's theory accounted for the observed experimental trend for the first time. Together with the photoelectric effect, this became one of the most important pieces of evidence for the need of quantization. Einstein used the levels of the quantum mechanical oscillator many years before the advent of modern quantum mechanics.\n\nIn Einstein's model, the specific heat approaches zero exponentially fast at low temperatures. This is because all the oscillations have one common frequency. The correct behavior is found by quantizing the normal modes of the solid in the same way that Einstein suggested. Then the frequencies of the waves are not all the same, and the specific heat goes to zero as a formula_1 power law, which matches experiment. This modification is called the Debye model, which appeared in 1912.\n\nWhen Walther Nernst learned of Einstein's 1906 paper on specific heat, he was so excited that he traveled all the way from Berlin to Zürich to meet with him.\n\nThe heat capacity of an object at constant volume \"V\" is defined through the internal energy \"U\" as\n\nformula_3, the temperature of the system, can be found from the entropy\n\nTo find the entropy consider a solid made of formula_5 atoms, each of which has 3 degrees of freedom. So there are formula_6 quantum harmonic oscillators (hereafter SHOs for \"Simple Harmonic Oscillators\").\n\nPossible energies of an SHO are given by\n\nor, in other words, the energy levels are evenly spaced and one can define a \"quantum\" of energy\n\nwhich is the smallest and only amount by which the energy of an SHO is increased. Next, we must compute the multiplicity of the system. That is, compute the number of ways to distribute formula_10 quanta of energy among formula_11 SHOs. This task becomes simpler if one thinks of distributing formula_10 pebbles over formula_11 boxes\n\nor separating stacks of pebbles with formula_14 partitions\n\nor arranging formula_10 pebbles and formula_14 partitions\n\nThe last picture is the most telling. The number of arrangements of formula_17 objects is formula_18. So the number of possible arrangements of formula_10 pebbles and formula_14 partitions is formula_21. However, if partition #3 and partition #5 trade places, no one would notice. The same argument goes for quanta. To obtain the number of possible \"distinguishable\" arrangements one has to divide the total number of arrangements by the number of \"indistinguishable\" arrangements. There are formula_22 identical quanta arrangements, and formula_23 identical partition arrangements. Therefore, multiplicity of the system is given by\n\nwhich, as mentioned before, is the number of ways to deposit formula_10 quanta of energy into formula_11 oscillators. Entropy of the system has the form\n\nformula_11 is a huge number—subtracting one from it has no overall effect whatsoever:\n\nWith the help of Stirling's approximation, entropy can be simplified:\n\nTotal energy of the solid is given by\n\nsince there are q energy quanta in total in the system in addition to the ground state energy of each oscillator. Some authors, such as Schroeder, omit this ground state energy in their definition of the total energy of an Einstein solid.\n\nWe are now ready to compute the temperature\n\nElimination of q between the two preceding formulas gives for U:\n\nThe first term is associated with zero point energy and does not contribute to specific heat. It will therefore be lost in the next step.\n\nDifferentiating with respect to temperature to find formula_34 we obtain:\n\nor\n\nAlthough the Einstein model of the solid predicts the heat capacity accurately at high temperatures, it noticeably deviates from experimental values at low temperatures. See Debye model for how to calculate accurate low-temperature heat capacities.\n\nHeat capacity is obtained through the use of the canonical partition function of a simple harmonic oscillator (SHO).\n\nwhere\n\nsubstituting this into the partition function formula yields\n\nThis is the partition function of \"one\" SHO. Because, statistically, heat capacity, energy, and entropy of the solid are equally distributed among its atoms (SHOs), we can work with this partition function to obtain those quantities and then simply multiply them by formula_11 to get the total. Next, let's compute the average energy of each oscillator\n\nwhere\n\nTherefore,\n\nHeat capacity of \"one\" oscillator is then\n\nUp to now, we calculated the heat capacity of a unique degree of freedom, which has been modeled as an SHO. The heat capacity of the entire solid is then given by formula_45, where the total number of degree of freedom of the solid is three (for the three directional degree of freedom) times formula_5, the number of atoms in the solid. One thus obtains\n\nwhich is algebraically identical to the formula derived in the previous section.\n\nThe quantity formula_48 has the dimensions of temperature and is a characteristic property of a crystal. It is known as the Einstein temperature. Hence, the Einstein Crystal model predicts that the energy and heat capacities of a crystal are universal functions of the dimensionless ratio formula_49. Similarly, the Debye model predicts a universal function of the ratio formula_50.\n\n"}
{"id": "706247", "url": "https://en.wikipedia.org/wiki?curid=706247", "title": "Electronic band structure", "text": "Electronic band structure\n\nIn solid-state physics, the electronic band structure (or simply band structure) of a solid describes the range of energies that an electron within the solid may have (called \"energy bands\", \"allowed bands\", or simply \"bands\") and ranges of energy that it may not have (called \"band gaps\" or \"forbidden bands\").\n\nBand theory derives these bands and band gaps by examining the allowed quantum mechanical wave functions for an electron in a large, periodic lattice of atoms or molecules. Band theory has been successfully used to explain many physical properties of solids, such as electrical resistivity and optical absorption, and forms the foundation of the understanding of all solid-state devices (transistors, solar cells, etc.).\n\nThe electrons of a single, isolated atom occupy atomic orbitals each of which has a discrete energy level. When two or more atoms join together to form into a molecule, their atomic orbitals overlap. The Pauli exclusion principle dictates that no two electrons can have the same quantum numbers in a molecule. So if two identical atoms combine to form a diatomic molecule, each atomic orbital splits into two molecular orbitals of different energy, allowing the electrons in the former atomic orbitals to occupy the new orbital structure without any having the same energy.\n\nSimilarly if a large number \"N\" of identical atoms come together to form a solid, such as a crystal lattice, the atoms' atomic orbitals overlap. Since the Pauli exclusion principle dictates that no two electrons in the solid have the same quantum numbers, each atomic orbital splits into \"N\" discrete molecular orbitals, each with a different energy. Since the number of atoms in a macroscopic piece of solid is a very large number (N~10) the number of orbitals is very large and thus they are very closely spaced in energy (of the order of 10 eV). The energy of adjacent levels is so close together that they can be considered as a continuum, an energy band.\n\nThis formation of bands is mostly a feature of the outermost electrons (valence electrons) in the atom, which are the ones involved in chemical bonding and electrical conductivity. The inner electron orbitals do not overlap to a significant degree, so their bands are very narrow.\n\nBand gaps are essentially leftover ranges of energy not covered by any band, a result of the finite widths of the energy bands. The bands have different widths, with the widths depending upon the degree of overlap in the atomic orbitals from which they arise. Two adjacent bands may simply not be wide enough to fully cover the range of energy. For example, the bands associated with core orbitals (such as 1s electrons) are extremely narrow due to the small overlap between adjacent atoms. As a result, there tend to be large band gaps between the core bands. Higher bands involve comparatively larger orbitals with more overlap, becoming progressively wider at higher energies so that there are no band gaps at higher energies.\n\nBand theory is only an approximation to the quantum state of a solid, which applies to solids consisting of many identical atoms or molecules bonded together. These are the assumptions necessary for band theory to be valid: \n\n\nThe above assumptions are broken in a number of important practical situations, and the use of band structure requires one to keep a close check on the limitations of band theory:\n\nBand structure calculations take advantage of the periodic nature of a crystal lattice, exploiting its symmetry. The single-electron Schrödinger equation is solved for an electron in a lattice-periodic potential, giving Bloch waves as solutions:\n\nwhere k is called the wavevector. For each value of k, there are multiple solutions to the Schrödinger equation labelled by \"n\", the band index, which simply numbers the energy bands.\nEach of these energy levels evolves smoothly with changes in k, forming a smooth band of states. For each band we can define a function \"E\"(k), which is the dispersion relation for electrons in that band.\n\nThe wavevector takes on any value inside the Brillouin zone, which is a polyhedron in wavevector space that is related to the crystal's lattice.\nWavevectors outside the Brillouin zone simply correspond to states that are physically identical to those states within the Brillouin zone.\nSpecial high symmetry points/lines in the Brillouin zone are assigned labels like Γ, Δ, Λ, Σ (see Fig 1).\n\nIt is difficult to visualize the shape of a band as a function of wavevector, as it would require a plot in four-dimensional space, \"E\" vs. \"k\", \"k\", \"k\". In scientific literature it is common to see band structure plots which show the values of \"E\"(k) for values of k along straight lines connecting symmetry points, often labelled Δ, Λ, Σ, or [100], [111], and [110], respectively. Another method for visualizing band structure is to plot a constant-energy isosurface in wavevector space, showing all of the states with energy equal to a particular value. The isosurface of states with energy equal to the Fermi level is known as the Fermi surface.\n\nEnergy band gaps can be classified using the wavevectors of the states surrounding the band gap:\n\nAlthough electronic band structures are usually associated with crystalline materials, quasi-crystalline and amorphous solids may also exhibit band structures. These are somewhat more difficult to study theoretically since they lack the simple symmetry of a crystal, and it is not usually possible to determine a precise dispersion relation. As a result, virtually all of the existing theoretical work on the electronic band structure of solids has focused on crystalline materials.\n\nThe density of states function \"g\"(\"E\") is defined as the number of electronic states per unit volume, per unit energy, for electron energies near \"E\".\n\nThe density of states function is important for calculations of effects based on band theory.\nIn Fermi's Golden Rule, a calculation for the rate of optical absorption, it provides both the number of excitable electrons and the number of final states for an electron. It appears in calculations of electrical conductivity where it provides the number of mobile states, and in computing electron scattering rates where it provides the number of final states after scattering.\n\nFor energies inside a band gap, \"g\"(\"E\") = 0.\n\nAt thermodynamic equilibrium, the likelihood of a state of energy \"E\" being filled with an electron is given by the Fermi–Dirac distribution, a thermodynamic distribution that takes into account the Pauli exclusion principle:\n\nwhere:\n\nThe density of electrons in the material is simply the integral of the Fermi–Dirac distribution times the density of states:\n\nAlthough there are an infinite number of bands and thus an infinite number of states, there are only a finite number of electrons to place in these bands.\nThe preferred value for the number of electrons is a consequence of electrostatics: even though the surface of a material can be charged, the internal bulk of a material prefers to be charge neutral.\nThe condition of charge neutrality means that \"N\"/\"V\" must match the density of protons in the material. For this to occur, the material electrostatically adjusts itself, shifting its band structure up or down in energy (thereby shifting \"g\"(\"E\")), until it is at the correct equilibrium with respect to the Fermi level.\n\nA solid has an infinite number of allowed bands, just as an atom has infinitely many energy levels. However, most of the bands simply have too high energy, and are usually disregarded under ordinary circumstances.\nConversely, there are very low energy bands associated with the core orbitals (such as 1s electrons). These low-energy \"core band\"s are also usually disregarded since they remain filled with electrons at all times, and are therefore inert.\nLikewise, materials have several band gaps throughout their band structure.\n\nThe most important bands and band gaps—those relevant for electronics and optoelectronics—are those with energies near the Fermi level.\nThe bands and band gaps near the Fermi level are given special names, depending on the material:\n\nThe ansatz is the special case of electron waves in a periodic crystal lattice using Bloch waves as treated generally in the dynamical theory of diffraction. Every crystal is a periodic structure which can be characterized by a Bravais lattice, and for each Bravais lattice we can determine the reciprocal lattice, which encapsulates the periodicity in a set of three reciprocal lattice vectors (b,b,b). Now, any periodic potential V(r) which shares the same periodicity as the direct lattice can be expanded out as a Fourier series whose only non-vanishing components are those associated with the reciprocal lattice vectors. So the expansion can be written as:\n\nwhere K = mb + mb + mb for any set of integers (m,m,m).\n\nFrom this theory, an attempt can be made to predict the band structure of a particular material, however most ab initio methods for electronic structure calculations fail to predict the observed band gap.\n\nIn the nearly free electron approximation, interactions between electrons are completely ignored. This approximation allows use of Bloch's Theorem which states that electrons in a periodic potential have wavefunctions and energies which are periodic in wavevector up to a constant phase shift between neighboring reciprocal lattice vectors. The consequences of periodicity are described mathematically by the Bloch wavefunction:\n\nwhere the function formula_6 is periodic over the crystal lattice, that is,\n\nHere index \"n\" refers to the \"n-th\" energy band, wavevector k is related to the direction of motion of the electron, r is the position in the crystal, and R is the location of an atomic site.\n\nThe NFE model works particularly well in materials like metals where distances between neighbouring atoms are small. In such materials\nthe overlap of atomic orbitals and potentials on neighbouring atoms is relatively large. In that case the wave function of the electron can be approximated by a (modified) plane wave. The band structure of a metal like aluminium even gets close to the empty lattice approximation.\n\nThe opposite extreme to the nearly free electron approximation assumes the electrons in the crystal behave much like an assembly of constituent atoms. This tight binding model assumes the solution to the time-independent single electron Schrödinger equation formula_8 is well approximated by a linear combination of atomic orbitals formula_9.\n\nwhere the coefficients formula_11 are selected to give the best approximate solution of this form. Index \"n\" refers to an atomic energy level and R refers to an atomic site. A more accurate approach using this idea employs Wannier functions, defined by:\n\nin which formula_13 is the periodic part of the Bloch wave and the integral is over the Brillouin zone. Here index \"n\" refers to the \"n\"-th energy band in the crystal. The Wannier functions are localized near atomic sites, like atomic orbitals, but being defined in terms of Bloch functions they are accurately related to solutions based upon the crystal potential. Wannier functions on different atomic sites R are orthogonal. The Wannier functions can be used to form the Schrödinger solution for the \"n\"-th energy band as:\n\nThe TB model works well in materials with limited overlap between atomic orbitals and potentials on neighbouring atoms. Band structures of materials like Si, GaAs, SiO and diamond for instance are well described by TB-Hamiltonians on the basis of atomic sp orbitals. In transition metals a mixed TB-NFE model is used to describe the broad NFE conduction band and the narrow embedded TB d-bands. The radial functions of\nthe atomic orbital part\nof the Wannier functions are most easily calculated by the use of pseudopotential methods. NFE, TB or combined NFE-TB band structure\ncalculations,\nsometimes extended with wave function approximations based on pseudopotential methods, are often used as an economic starting point for further calculations.\n\nThe simplest form of this approximation centers non-overlapping spheres (referred to as \"muffin tins\") on the atomic positions. Within these regions, the potential experienced by an electron is approximated to be spherically symmetric about the given nucleus. In the remaining interstitial region, the screened potential is approximated as a constant. Continuity of the potential between the atom-centered spheres and interstitial region is enforced.\n\nA variational implementation was suggested by Korringa and by Kohn and Rostocker, and is often referred to as the \"KKR model\".\n\nIn recent physics literature, a large majority of the electronic structures and band plots are calculated using density-functional theory (DFT), which is not a model but rather a theory, i.e., a microscopic first-principles theory of condensed matter physics that tries to cope with the electron-electron many-body problem via the introduction of an exchange-correlation term in the functional of the electronic density. DFT-calculated bands are in many cases found to be in agreement with experimentally measured bands, for example by angle-resolved photoemission spectroscopy (ARPES). In particular, the band shape is typically well reproduced by DFT. But there are also systematic errors in DFT bands when compared to experiment results. In particular, DFT seems to systematically underestimate by about 30-40% the band gap in insulators and semiconductors.\n\nIt is commonly believed that DFT is a theory to predict ground state properties of a system only (e.g. the total energy, the atomic structure, etc.), and that excited state properties cannot be determined by DFT. This is a misconception. In principle, DFT can determine any property (ground state or excited state) of a system given a functional that maps the ground state density to that property. This is the essence of the Hohenberg–Kohn theorem. In practice, however, no known functional exists that maps the ground state density to excitation energies of electrons within a material. Thus, what in the literature is quoted as a DFT band plot is a representation of the DFT Kohn–Sham energies, i.e., the energies of a fictive non-interacting system, the Kohn–Sham system, which has no physical interpretation at all. The Kohn–Sham electronic structure must not be confused with the real, quasiparticle electronic structure of a system, and there is no Koopman's theorem holding for Kohn–Sham energies, as there is for Hartree–Fock energies, which can be truly considered as an approximation for quasiparticle energies. Hence, in principle, Kohn–Sham based DFT is not a band theory, i.e., not a theory suitable for calculating bands and band-plots. In principle time-dependent DFT can be used to calculate the true band structure although in practice this is often difficult. A popular approach is the use of hybrid functionals, which incorporate a portion of Hartree–Fock exact exchange; this produces a substantial improvement in predicted bandgaps of semiconductors, but is less reliable for metals and wide-bandgap materials.\n\nTo calculate the bands including electron-electron interaction many-body effects, one can resort to so-called Green's function methods. Indeed, knowledge of the Green's function of a system provides both ground (the total energy) and also excited state observables of the system. The poles of the Green's function are the quasiparticle energies, the bands of a solid. The Green's function can be calculated by solving the Dyson equation once the self-energy of the system is known. For real systems like solids, the self-energy is a very complex quantity and usually approximations are needed to solve the problem. One such approximation is the GW approximation, so called from the mathematical form the self-energy takes as the product Σ = \"GW\" of the Green's function \"G\" and the dynamically screened interaction \"W\". This approach is more pertinent when addressing the calculation of band plots (and also quantities beyond, such as the spectral function) and can also be formulated in a completely \"ab initio\" way. The GW approximation seems to provide band gaps of insulators and semiconductors in agreement with experiment, and hence to correct the systematic DFT underestimation.\n\nAlthough the nearly free electron approximation is able to describe many properties of electron band structures, one consequence of this theory is that it predicts the same number of electrons in each unit cell. If the number of electrons is odd, we would then expect that there is an unpaired electron in each unit cell, and thus that the valence band is not fully occupied, making the material a conductor. However, materials such as CoO that have an odd number of electrons per unit cell are insulators, in direct conflict with this result. This kind of material is known as a Mott insulator, and requires inclusion of detailed electron-electron interactions (treated only as an averaged effect on the crystal potential in band theory) to explain the discrepancy. The Hubbard model is an approximate theory that can include these interactions. It can be treated non-perturbatively within the so-called dynamical mean field theory, which attempts to bridge the gap between the nearly free electron approximation and the atomic limit. Formally, however, the states are not non-interacting in this case and the concept of a band structure is not adequate to describe these cases.\n\nCalculating band structures is an important topic in theoretical solid state physics. In addition to the models mentioned above, other models include the following:\n\n\nThe band structure has been generalised to wavevectors that are complex numbers, resulting in what is called a \"complex band structure\", which is of interest at surfaces and interfaces.\n\nEach model describes some types of solids very well, and others poorly. The nearly free electron model works well for metals, but poorly for non-metals. The tight binding model is extremely accurate for ionic insulators, such as metal halide salts (e.g. NaCl).\n\nTo understand how band structure changes relative to the Fermi level in real space, a band structure plot is often first simplified in the form of a band diagram. In a band diagram the vertical axis is energy while the horizontal axis represents real space. Horizontal lines represent energy levels, while blocks represent energy bands. When the horizontal lines in these diagram are slanted then the energy of the level or band changes with distance. Diagrammatically, this depicts the presence of an electric field within the crystal system. Band diagrams are useful in relating the general band structure properties of different materials to one another when placed in contact with each other.\n\n\n\n"}
{"id": "9957103", "url": "https://en.wikipedia.org/wiki?curid=9957103", "title": "Elektro-Slovenija", "text": "Elektro-Slovenija\n\nElektro-Slovenija, d.o.o. (ELES) is a state-owned electricity transmission company of Slovenia. The company was founded in 1991 by the Government of Slovenia. It is the only power transmission system operator in the country. ELES operates the network of 400 kV, 220 kV and 110 kV transmission lines with a total length of . The managing director of the company is Aleksander Mervar.\n\nELES is a member of the European Network of Transmission System Operators for Electricity (ENTSO-E).\n\n"}
{"id": "10303", "url": "https://en.wikipedia.org/wiki?curid=10303", "title": "Evaporation", "text": "Evaporation\n\nEvaporation is a type of vaporization that occurs on the surface of a liquid as it changes into the gas phase when it reaches its boiling point. The surrounding gas must not be saturated with the evaporating substance. When the molecules of the liquid collide, they transfer energy to each other based on how they collide. When a molecule near the surface absorbs enough energy to overcome the vapor pressure, it will \"escape\" and enter the surrounding air as a gas. When evaporation occurs, the energy removed from the vaporized liquid will reduce the temperature of the liquid, resulting in evaporative cooling.\n\nOn average, only a fraction of the molecules in a liquid have enough heat energy to escape from the liquid. The evaporation will continue until an equilibrium is reached when the evaporation of the liquid is the equal to its condensation. In an enclosed environment, a liquid will evaporate until the surrounding air is saturated.\n\nEvaporation is an essential part of the water cycle. The sun (solar energy) drives evaporation of water from oceans, lakes, moisture in the soil, and other sources of water. In hydrology, evaporation and transpiration (which involves evaporation within plant stomata) are collectively termed evapotranspiration. Evaporation of water occurs when the surface of the liquid is exposed, allowing molecules to escape and form water vapor; this vapor can then rise up and form clouds. With sufficient energy, the liquid will turn into vapor.\n\nFor molecules of a liquid to evaporate, they must be located near the surface, they have to be moving in the proper direction, and have sufficient kinetic energy to overcome liquid-phase intermolecular forces. When only a small proportion of the molecules meet these criteria, the rate of evaporation is low. Since the kinetic energy of a molecule is proportional to its temperature, evaporation proceeds more quickly at higher temperatures. As the faster-moving molecules escape, the remaining molecules have lower average kinetic energy, and the temperature of the liquid decreases. This phenomenon is also called evaporative cooling. This is why evaporating sweat cools the human body.\nEvaporation also tends to proceed more quickly with higher flow rates between the gaseous and liquid phase and in liquids with higher vapor pressure. For example, laundry on a clothes line will dry (by evaporation) more rapidly on a windy day than on a still day. Three key parts to evaporation are heat, atmospheric pressure (determines the percent humidity), and air movement.\n\nOn a molecular level, there is no strict boundary between the liquid state and the vapor state. Instead, there is a Knudsen layer, where the phase is undetermined. Because this layer is only a few molecules thick, at a macroscopic scale a clear phase transition interface cannot be seen. \n\nLiquids that do not evaporate visibly at a given temperature in a given gas (e.g., cooking oil at room temperature) have molecules that do not tend to transfer energy to each other in a pattern sufficient to frequently give a molecule the heat energy necessary to turn into vapor. However, these liquids \"are\" evaporating. It is just that the process is much slower and thus significantly less visible.\n\nIf evaporation takes place in an enclosed area, the escaping molecules accumulate as a vapor above the liquid. Many of the molecules return to the liquid, with returning molecules becoming more frequent as the density and pressure of the vapor increases. When the process of escape and return reaches an equilibrium, the vapor is said to be \"saturated\", and no further change in either vapor pressure and density or liquid temperature will occur. For a system consisting of vapor and liquid of a pure substance, this equilibrium state is directly related to the vapor pressure of the substance, as given by the Clausius–Clapeyron relation:\n\nwhere \"P\", \"P\" are the vapor pressures at temperatures \"T\", \"T\" respectively, Δ\"H\" is the enthalpy of vaporization, and \"R\" is the universal gas constant. The rate of evaporation in an open system is related to the vapor pressure found in a closed system. If a liquid is heated, when the vapor pressure reaches the ambient pressure the liquid will boil.\n\nThe ability for a molecule of a liquid to evaporate is based largely on the amount of kinetic energy an individual particle may possess. Even at lower temperatures, individual molecules of a liquid can evaporate if they have more than the minimum amount of kinetic energy required for vaporization.\n\nNote: Air used here is a common example; however, the vapor phase can be other gases.\n\n\nIn the US, the National Weather Service measures the actual rate of evaporation from a standardized \"pan\" open water surface outdoors, at various locations nationwide. Others do likewise around the world. The US data is collected and compiled into an annual evaporation map. The measurements range from under 30 to over per year.\n\nEvaporation is an endothermic process, in that heat is absorbed during evaporation.\n\n\nFuel droplets vaporize as they receive heat by mixing with the hot gases in the combustion chamber. Heat (energy) can also be received by radiation from any hot refractory wall of the combustion chamber.\n\nInternal combustion engines rely upon the vaporization of the fuel in the cylinders to form a fuel/air mixture in order to burn well.\nThe chemically correct air/fuel mixture for total burning of gasoline has been determined to be 15 parts air to one part gasoline or 15/1 by weight. Changing this to a volume ratio yields 8000 parts air to one part gasoline or 8,000/1 by volume.\n\nThin films may be deposited by evaporating a substance and condensing it onto a substrate, or by dissolving the substance in a solvent, spreading the resulting solution thinly over a substrate, and evaporating the solvent. The Hertz–Knudsen equation is often used to estimate the rate of evaporation in these instances.\n\n"}
{"id": "31357789", "url": "https://en.wikipedia.org/wiki?curid=31357789", "title": "German Nuclear Reactor Insurance Association", "text": "German Nuclear Reactor Insurance Association\n\nThe German Nuclear Reactor Insurance Association () is an association of German insurers of the nuclear industry located in Cologne. The German Nuclear Reactor Insurance Association also is involved in the reinsurance industry.\n\nThe first associations of insurers of the nuclear industry in Europe were founded in Sweden and England in 1955. The German Nuclear Reactor Insurance Association was founded shortly thereafter in 1957. Throughout the association's history, it has only once had to pay a claim; the claim amounted to approximately €10,000 to €15,000.\n\nDespite the slight probability of losses resulting from an insured event in the nuclear industry, in the event of an insured event does occur the possibilities for damages are considerable. As such a single insurance company is unable to bear such risk alone. Under German law, the transfer of a nuclear risk of an insurer to a reinsurer is prohibited. In 1998 the Atomic Energy Act established the maximum insurance liability of nuclear insurer at about €2.5 billion; for damages above that cap the Federal Government is liable according to § 34 of the Atomic Energy Act. If the total damage exceeds the stipulated maximum sum of €2.55 billion, the German Federal Government pays the German Nuclear Reactor Insurance Association for the excess losses. The insurance payments include the cost of evacuating people.\n\nFukushima I Nuclear Power Plant was insured for some tens of millions of euros with German Nuclear Reactor Insurance Association. Under the terms of the insurance policy, the policyholder was not insured for damage caused by earthquakes, tsunamis, and volcanic eruptions. Therefore the German Nuclear Reactor Insurance Association had no resulting liability to Tokyo Electric Power Company, the owner of the Fukushima I Nuclear Power Plant, because of Fukushima I nuclear accidents.\n"}
{"id": "35928584", "url": "https://en.wikipedia.org/wiki?curid=35928584", "title": "Gutter oil", "text": "Gutter oil\n\nGutter oil (, or ) is a term used in mainland China, Hong Kong, Macau and Taiwan to describe illicit cooking oil which has been recycled from waste oil collected from sources such as restaurant fryers, grease traps, slaughterhouse waste and sewage from sewer drains.\n\nReprocessing of used cooking oil is often very rudimentary; techniques include filtration, boiling, refining, and the removal of some adulterants. It is then packaged and resold as a cheaper alternative to normal cooking oil. Another version of gutter oil uses discarded animal parts, animal fat and skins, internal organs, and expired or otherwise low-quality meat, which is then cooked in large vats in order to extract the oil. Used kitchen oil can be purchased for between $859 and $937 per ton, while the cleaned and refined product can sell for $1,560 per tonne. Thus there is great economic incentive to produce and sell gutter oil. \n\nIt is estimated that up to one in every ten lower-market restaurant meals consumed in China is prepared with gutter oil. This high prevalence is due to what Feng Ping of the China Meat Research Center has made clear: \"The illegal oil shows no difference in appearance and indicators after refining and purification because the law breakers are skillful at coping with the established standards.\"\n\nThe first documented case of gutter oil in Taiwan was reported in 1985. In a subsequent investigation, 22 people were arrested for involvement in a recycling oil ring over 10 years based in Taipei. The worst offender was sentenced to 7 years in prison. Additionally, there was a report of an earlier incident in Taiwan in the 1960s, where \"trench oil\" was imported from Japan to Taiwan and then used in food processing.\n\nThe first documented case of gutter oil in mainland China was reported in 2000, when a street vendor was found to be selling oil obtained from restaurant garbage disposals.\n\nIn September 2012, an ongoing investigation into the suspected use of gutter oil as a raw material in the Chinese pharmaceutical industry was revealed. A scandal involving 240 tons of gutter oil in Taiwan affecting hundreds of companies and thousands of eateries, some of which may have been exported overseas, broke in September 2014.\n\nThe collected waste oil is sold to local workshops or small factories for cleaning and packaging. When sold to workshops it is often transported on the back of bicycles by peddlers who are paid a monthly wage; afterwards, the oil is held in 200-liter barrels at the workshops until it is processed. On other occasions the oil goes to industrial cooking oil refineries for further processing before it finally reaches its end purpose. The industrial oil refineries are usually legitimate producers that sell the processed oil for use in the chemical or energy industries. Gutter oil is perfectly suitable as a raw ingredient for producing soap, rubber, bio-fuel, and cosmetics. However, the refiners can also have other intentions as the prices attained by selling it as cooking oil are much higher than if it is sold to the chemical or energy industries. \n\nThere are few formal rules or protocols in place to prevent purchases from or sales to entities intending to use the oil for human consumption, so it is very easy for individuals or wholesalers to purchase oil from these industrial refineries and then resell the oil to restaurants or to end consumers. There have been some cases where the industrial oil refiner packaged the oil under a unique brand name and sold it as legitimate oil in retail outlets as opposed to just selling directly to restaurants. Some lower-market restaurants have long-term purchase agreements with oil recyclers for selling their used oil.\n\nLow-end restaurants and street vendors are the biggest end users of gutter oil, as they operate with lower profit margins than bigger restaurants. Oil is one of the largest kitchen supply costs for restaurants, so obtaining cheaper oil can allow a marginal restaurant to reduce its overall expenses. Chinese food is generally heavily dependent on oil due to most foods being fried, so cheaper meal prices for many price-sensitive consumers are possible if gutter oil is used instead of virgin oil. The situation becomes more serious because it is hard to distinguish reprocessed gutter oil from legitimate oil. Bleach is used to transform gutter oil's dark color into a more natural-looking one, and alkali additives are used to neutralize the abnormal pH caused by high concentrations of animal fats.\n\nGutter oil has been shown to be quite toxic, and can cause diarrhea and abdominal pain. There are also reports that long-term consumption of the oil can lead to stomach and liver cancer as well as developmental disabilities in newborns and children. Testing of some samples of gutter oil has revealed traces of polycyclic aromatic hydrocarbons (PAH), dangerous organic pollutants capable of causing cancer with long-term consumption. There is also potential for gutter oil to contain aflatoxins, highly carcinogenic compounds produced by certain molds. Zeng Jing of the Guangdong Armed Police Hospital said of gutter oil: \"Animal and vegetable fat in refined waste oil will undergo rancidity, oxidation and decomposition after contamination. It will cause indigestion, insomnia, liver discomfort and other symptoms.\"\n\nChinese law states that industrial animal fat is not allowed for use in food products because it does not meet basic hygiene standards and contains high levels of potentially toxic contaminants. The national and local governments are researching ways to test and identify gutter oil but there were no nationwide standards in place to help with this process. The government is looking into methods that rely on technical equipment as well as on-site instant tests to screen suspect oil. There are five proposed tests for gutter oil but each has failed to accurately detect it.\n\nIn August and September 2011, the Beijing city government passed two new sets of regulations. The first was \"On Accelerating the City's Food Waste and Waste Oil Recycling Program\". Its goals are to increase daily food waste processing to 2,200 tons by 2012 and to 2,900 tons by 2015. Additionally, the intent of the regulations is to create a system that \"should be a unified, standardized, and orderly processing of waste oil collection and improvement of the transportation system\". The second set of regulations by the city of Beijing, called the \"Beijing Municipal Solid Waste Regulations\", was passed in September 2011. The regulations specifically target the two sources of gutter oil: food waste and used oil. The central government intends for these two sets of regulations to serve as national examples, yet wants every municipality nationally to find their own solutions to the food waste and gutter oil problem.\n\nA nationwide campaign was set in motion in August 2011 to crack down on the widespread production and selling of gutter oil. The law enforcement campaign uncovered 100 gutter oil manufacturers and arrested more than 800 people allegedly involved in the production and sale of gutter oil. In April 2012 another crackdown occurred with an additional 100 arrests made and 13 illegal workshops closed down across four provinces. According to a notice released jointly by the Supreme People's Court, the Supreme People's Procuratorate and the Ministry of Public Security, the death penalty will now be an option when prosecuting more serious cases of gutter oil manufacturing in the country. More severe punishments will also be given out to government and public officials who fail to properly address matters related to gutter oil. The State Council said inspectors would target edible oil trade fairs and wholesale markets and called for inspections of oil being used at restaurants, school cafeterias, work canteens and kitchens at construction sites. The State Council also stated that businesses that use recycled oil would be forced to close temporarily or lose their business license while peddlers who sell the oil could be criminally prosecuted. In October 2013, a man from eastern China's Jiangsu Province was sentenced to life imprisonment for profiting heavily from making and selling gutter oil.\n\n\n"}
{"id": "47127374", "url": "https://en.wikipedia.org/wiki?curid=47127374", "title": "HSMG", "text": "HSMG\n\nHSMG (High Strength Metallurgical Graphene) – polycrystalline graphene, grown from a liquid phase.\n\nThis process, in comparison to other methods based on using solid substrates, allows to manufacture defect-free graphene structures. HSMG is formed on a perfectly flat surface - liquid metal. While growing graphene on solid substrates is difficult due to surface irregularities and defects. Growing graphene on a liquid metal matrix enables the rotation of graphene grains, resulting in forming a continuous graphene sheet. Because of that, the grain disorientation angle is close to zero. Large-area graphene sheets formed with this method have high mechanical durability, close to graphene's theoretical values. This production method has been developed and patented by a team of scientists from the Institute of Materials Science of Lodz University of Technology, under the direction of Prof. Piotr Kula. HSMG graphene's commercialisation and applications are being handled by a spin-off institution - Advanced Graphene Products sp. z o.o.\n"}
{"id": "39199872", "url": "https://en.wikipedia.org/wiki?curid=39199872", "title": "High frequency line trap", "text": "High frequency line trap\n\nA line trap (high-frequency stopper) is a maintenance-free parallel resonant circuit, mounted inline on high-voltage (HV) AC transmission power lines to prevent the transmission of high frequency (40 kHz to 1000 kHz) carrier signals of power line communication to unwanted destinations. Line traps are cylinder-like structures connected in series with HV transmission lines. A line trap is also called a \"wave trap\".\n\nThe line trap acts as a barrier or filter to prevent signal losses. The inductive reactance of the line trap presents a high reactance to high-frequency signals but a low reactance to mains frequency. This prevents carrier signals from being dissipated in the substation or in a tap line or branch of the main transmission path and grounds in the case of anything happening outside of the carrier transmission path. The line trap is also used to attenuate the shunting effects of high-voltage lines.\n\nThe trap consists of three major components: the main coil, the tuning device, and the protective device (also known as a surge arrester). The protective and tuning devices are mounted inside the main coil. A line trap may be covered with a bird barrier, in which case there are four components.\n\nThe main coil is the outer part of the line trap which is made from stranded aluminum cable. The reactor coil, depending on the device, can be made up of several aluminum wires, allowing equal distribution amongst the parallel wires. The stranded aluminum coil is wound in one layer. However, when the application of more than one layer is necessary, separation between layers is required to provide a cooling duct between them to avoid overheating. The cooling duct is created with spacer bars made out of epoxy resin and fiberglass. The coil carries rated continuous power frequency currents, therefore this is the power inductor in this system. It provides a low impedance path for the electricity flow. Since the power flow is rather large at times, the coil used in a line trap must be large in terms of physical size. Hence, a line trap unit is inserted between the busbar and connection of coupling capacitor to the line. It is a parallel tuned circuit containing inductance and capacitance. It has low impedance for power frequency and high impedance to carrier frequency. This unit prevents the high frequency carrier signal from entering the neighboring line.\n\nThe next major component is the tuning device. This device is securely installed inside the main coil. It adjusts blocking frequency or bandwidth, and consists of coils, capacitors, and resistors. This smaller coil is attached to both ends of the main coil. Its purpose is to create a blocking circuit which provides high impedance. There are three types of tuning devices: wideband tuning, single frequency tuning, and double frequency tuning. The tuned circuit is usually a dual-circuit broadband type. If the traps are self tuned, they do not require the use of any tuning devices. With the use of a tuning device, a line trap can be tuned to a frequency of 1000 Hz.\n\nThe last main component is the protective device, which is parallel with the main coil and the tuning device. It protects the main coil and the tuning device by lowering the over-voltage levels. The bandwidth of a line trap is the frequency range over which the line trap can provide a certain specified minimum blocking impedance or resistance.\n\nLine traps are connected in series with power line and thus their coils are rated to carry the full line current. The impedance of a line trap is very low at the power frequency and will not cause any significant voltage drop.\n\nPower line carrier communication (PLCC) technology has been frequently used since 1950 by the grid stations to transmit information at high speed. Transmitting information along high-voltage lines, at high frequency, has been one of the main means of communication in electric power for over fifty years. This technology is finding wide use in building and home automation, as it avoids the need for extra wiring. The data collected from different sensors is transmitted on power lines thereby reducing the maintenance cost of the additional wiring. In some countries, this technology is also used to provide Internet connection. In order to communicate, high-frequency line traps are used as they allow substations to communicate with each other through the power lines at the same time as they transmit electrical power. In order to separate power from messages being sent, different frequencies are used. Electrical power has a frequency of 50 Hz or 60 Hz in most places, and the communication waves use frequencies such as 150 kHz and 200 kHz. Line traps consist of filter circuits that allow only power frequency waves to travel to that of electrical equipment. They also stop communication waves from traveling to equipment.\n\nCommunication is crucial for substations.\n\nHigh frequency line traps have a temperature limit of 115 °C-180 °C depending on construction and manufacture.\n\n"}
{"id": "45212775", "url": "https://en.wikipedia.org/wiki?curid=45212775", "title": "January 2015 North American blizzard", "text": "January 2015 North American blizzard\n\nThe January 2015 North American blizzard was a powerful and severe blizzard that dumped up to of snowfall in parts of New England. Originating from a disturbance just off the coast of the Northwestern United States on January 23, it initially produced a light swath of snow as it traveled southeastwards into the Midwest as an Alberta clipper on January 24–25. It gradually weakened as it moved eastwards towards the Atlantic Ocean, however, a new dominant low formed off the East Coast of the United States late on January 26, and rapidly deepened as it moved northeastwards towards southeastern New England, producing pronounced blizzard conditions. The nor’easter then gradually weakened as it moved away into Canada. The storm was also given unofficial names, such as \"Blizzard of 2015\", and \"Winter Storm Juno\".\n\nThe nor'easter disrupted transportation, with snow emergencies declared in six states and travel bans enacted in four of these states – Connecticut, New Jersey, Massachusetts, and Rhode Island – as well as in New York City. Most passenger rail service was suspended, and thousands of flights were cancelled. Schools and activities saw weather-related cancellations for one or more days.\n\nBefore the blizzard struck, meteorologists had been anticipating that the impending storm would be \"historic\" and \"record-breaking\", with predictions of snowfall accumulations in major metropolitan areas such as New York City of up to . However, the predictions fell significantly short of what was anticipated, mainly due to a shift of the storm’s track, which cut down on the amount of snowfall. In the aftermath of the storm, citizens criticised the local government for shutting down the subway system in New York City for the storm.\n\nOn January 23, a low-pressure area developed off the Pacific Northwest, before moving over the Canadian Prairies by January 24. The storm system quickly moved southeastward into the Upper Midwest during the evening of January 24, taking a path typical of an Alberta clipper. As it progressed southward, the storm intensified, with frontogenesis occurring the next day. By noon on January 25, the upper-level low was centered near the border between Iowa and Missouri in correlation with a weak shortwave trough. Moisture from the Gulf of Mexico wrapped around the system from the south, resulting in widespread rainfall and snow over the Midwest. Throughout the day, the system traversed eastward along the Kentucky-Tennessee border. Snowfall remained concentrated along a cold front north of the Ohio River.\n\nAt 09:00 UTC on January 26, the Weather Prediction Center began issuing storm summaries on the developing disturbance while the low-pressure system was centered near Bluefield, West Virginia. At the time, mixed precipitation was occurring over northern Appalachia. As this system tracked eastward, it gradually weakened; however, at the same time, a new low-pressure system formed off the coast of North Carolina and began to track north-northeastward, eventually becoming the dominant low of the storm. Early on January 30, the nor'easter left the East Coast, even as another winter storm began to impact the region. During the next day, the former nor'easter continued to accelerate eastward across the North Atlantic, even as it rapidly weakened. On January 31, the winter storm was absorbed by a much more powerful extratropical cyclone developing over Western Europe.\n\nA number of New York City residents criticized New York Governor Andrew Cuomo's decision to shut down the city's subway system; it had never previously been closed due to snow. The nor'easter dropped much less snow in the city than originally expected, totaling in Central Park. The models were 50 miles off; the storm failed to bring moisture back to New York City and New Jersey.\n\nOn January 25, 2015, blizzard warnings were issued for areas of coastal New England, New York and New Jersey. High winds and heavy snow were forecast for the evening of January 26 and all day January 27, impacting a 250-mile area from New York City to Boston. On January 26, many roads in New York and New England were closed to all vehicles except emergency vehicles due to life-threatening conditions on area roadways.\n\nMany churches and other houses of prayer remained open to those in need of a warming center.\nAt the height of the storm, a 300-mile (480 km) stretch of Interstate 95 from northern New Jersey to the Massachusetts/New Hampshire border was closed in conjunction with travel bans in the impacted areas. Portions of Interstates 78, 80, 84, 87, 90, 91 and 93 were also shut down by the storm.\n\nIn preparation for the winter storm, the Pennsylvania Department of Transportation deployed approximately 350 salting trucks to treat major roadways. On January 26, Pennsylvania Governor Tom Wolf signed a disaster emergency proclamation in an attempt to distribute state resources as quickly as possible. Philadelphia Mayor Michael Nutter closed municipal offices and schools for January 27 and cancelled trash collection for the day. Meanwhile, SEPTA's bus lines closed while its train system ran 24 hours a day for Monday and Tuesday. According to the Philadelphia Office of Emergency Management, the snow emergency declaration was lifted at 6:00 a.m. Tuesday morning.\n\nThe South Brunswick Police Department said in a press release on January 25 that additional officers and public works staff would be on duty throughout the storm. The department urged residents to make preparations in anticipation that travel would be impossible for several days. The Monroe Township Office of Emergency Management released a guideline to ensure residents were safe and prepared for the nor'easter. New Jersey Governor Chris Christie declared a state of emergency for the duration of the storm; in a morning press conference, Christie also urged all nonessential personnel to remain off roadways. A travel ban was issued later that day, to be put in effect on January 26 at 11 pm; Amtrak and New Jersey Transit services were also suspended.\n\nIn a statement on January 25, New York Governor Andrew Cuomo urged residents to take their necessary safety precautions and prepare for the possibility of disrupted commute on Monday and Tuesday. The Metropolitan Transportation Authority deployed extra crews, salting trucks, and chained tires. Delta Air Lines promised full refunds for flights significantly delayed. The State Emergency Operations Center in Albany was expected to be staffed beginning early on January 26 to coordinate with all affected counties. At least 1,806 plows and 126,000 tons of salt were expected to be divided across the region, and the National Guard was expected to deploy more than six dozen personnel and 20 vehicles throughout the region. New York State Police were expected to bring in additional personnel, as well as supply at least 50 4x4 vehicles, 8 all-terrain vehicles, and 8 snowmobiles.\n\nAt its major transportation facilities, the Port Authority of New York and New Jersey readied more than 200 piece of snow equipment at its airports, more than 60 pieces of snow equipment for its bridges and tunnels, hundreds of thousands of gallons of liquid anti-icer chemicals and thousands of tons of solid de-icers, plow-equipped trains, liquid snow-melting agent trains, and a \"jet engine\" plow. The New York State Thruway Authority activated its emergency operations ahead of the winter storm and is expected to supply 338 snowplows, 18 snowblowers, 55 front-end loaders, and approximately 126,000 tons of salt. The New York State Department of Transportation also activated its emergency operations, mobilizing 162 snow plows and nearly 338 operators to Long Island and the Hudson Valley, totaling to approximately 600 plows and 1,300 operators and supervisors across downstate New York. A total of 1,444 snow plows and 3,629 operators and supervisors would be available statewide.\n\nNew York City Mayor Bill de Blasio warned of of snow, and said, \"Prepare for something worse than we have seen before.\" De Blasio ordered all vehicles off the streets by 11 pm on January 26, and declared that, with the exception of emergency and government vehicles, anyone driving in New York City after 11 pm on January 26 would be fined. Services on the New York City Subway, Long Island Rail Road, Metro-North Railroad, and Amtrak were also suspended.\n\nConnecticut Governor Dannel Malloy announced in a press conference early on January 26 that Connecticut Transit services would be suspended and a statewide travel ban would be effective beginning at 9 p.m EST; he also urged all residents to leave work early and shelter in their homes. The Metropolitan Transportation Authority announced that it would be adding additional trains to accommodate for those who travel by rail. The governor announced that more than 600 crews would be working to pre-treat major roads, while he issued a declaration of civil preparedness emergency to coordinate resources during the storm. Connecticut Light and Power and United Illuminating prepared for a \"Level 1\" emergency and summoned outside tree and line crews. Later that day, the governor declared a state of emergency. The travel ban was lifted for Fairfield and Litchfield Counties in western Connecticut at 8 a.m. on the 27th, and for the remainder of the state at 2 p.m. that afternoon.\n\nRhode Island Governor Gina Raimondo urged people to ensure that they had enough food, water, and fuel to last several days for what could be a \"very severe and dangerous weather event\". She asked residents to keep fire hydrants uncovered and asked that generators used for electricity were located outside of homes and in well-ventilated locations. On January 26, the governor declared a state of emergency and announced that a statewide travel ban would go into effect at midnight. National Grid gathered approximately 1,000 crews to help restore power after the storm passed.\n\nMassachusetts Governor Charlie Baker declared a state of emergency and asked residents to remain safe and off roadways. A statewide travel ban was issued effective at midnight on January 27, and the Massachusetts Bay Transportation Authority announced that it would be closed that day. In Boston, up to 35,000 tons of salt were prepared and snow farms were readied to store the removed snow following the passage of the storm. Electric companies brought in extra crews. The Fall River, Massachusetts Department of Public Works gathered more than 800 tons of salt and began to treat major roadways in advance of the storm.\n\nEnvironment Canada issued special weather statements to all of the Maritime provinces on January 23, warning of the future developing storm. On January 26 Environment Canada issued Winter Storm Warnings for all of Nova Scotia, Prince Edward Island, and central to southern New Brunswick. These warnings were later upgraded to Blizzard Warnings. Between of snow was forecast across the Maritimes. On January 27, Snow started early in the morning in southwestern Nova Scotia and continued northeastward. A large wind gust of was measured in Baccaro Point, NS. Meanwhile, in Sluice Point there were gusts exceeding in the afternoon. In Moncton snow totaled near the most out of all four provinces. The strong winds and blowing snow caused poor traveling conditions across Atlantic Canada.\n\nFlightAware reported that 1,200 flights were expected to be cancelled on January 26. Delta pre-emptively cancelled 600 flights; furthermore, a dozen flights from London Heathrow to New York, Philadelphia and Boston were cancelled on the same date.\n\nAt least two deaths related to the nor'easter occurred on Long Island, New York. Sean Urda, a 17-year-old student from John Glenn High School, was killed when he struck a light pole while snow tubing with two other people in Huntington. An 83-year-old man with dementia was found dead in his backyard in Bay Shore.\n\nThe National Basketball Association postponed two games scheduled to take place on January 26, both in New York City. The game between the Portland Trail Blazers and the Brooklyn Nets was rescheduled for April 6, while the game between the Sacramento Kings and the Manhattan-based New York Knicks was rescheduled for March 3.\n\nWWE had events scheduled to take place in Hartford, Connecticut and in Boston, Massachusetts, but had to cancel them, instead choosing to air Royal Rumble highlights.\n\nSignificant flooding was reported in Scituate, Massachusetts, where National Guardsmen were sent to rescue people from the high waters and power was cut to some areas of the town to prevent fires from breaking out. Elsewhere in the state, eighty feet of seawall in Marshfield was washed away by the storm, and a number of houses in the town were condemned. \"Virtually all\" utility customers lost power on the island of Nantucket and in Provincetown on Cape Cod during the storm.\n\nA replica of the \"USS Providence\", maintained by the Providence Maritime Heritage Foundation and designated in 1992 as the flagship and tall ship ambassador of the state of Rhode Island, was toppled and severely damaged by high winds during the storm while in drydock for the winter.\n\nSnow from the storm fell as far south as parts of North Carolina, southern Virginia, Washington, D.C., and the Washington, D.C. metropolitan area, where trace amounts were reported beginning on January 26. Schools in some Virginia counties were closed due to the weather.\n\nUp to of snow fell in Worcester, Massachusetts, marking the city's largest storm total accumulation on record. The Blue Hill Observatory in Massachusetts observed , or the second-largest storm total accumulation on record, while both Providence, Rhode Island and Portland, Maine recorded their fourth-largest storm total accumulations on record, at . Boston observed of snow, its largest January storm total accumulation and its sixth largest storm total accumulation on record.\n\nThe following is a list of storm totals above . Click \"hide\" to collapse table.\n\nSources:\n\nIn April 2015, President Obama made a federal disaster declaration for the January 26–27 snow, allowing some reimbursement for damages. Massachusetts distributed an extra $30 million to cities and towns for repairs from the winter overall.\n\n\n"}
{"id": "51389757", "url": "https://en.wikipedia.org/wiki?curid=51389757", "title": "Lake Nyos disaster", "text": "Lake Nyos disaster\n\nOn 21 August 1986, a limnic eruption at Lake Nyos in northwestern Cameroon killed 1,746 people and 3,500 livestock.\n\nThe eruption triggered the sudden release of about 100,000–300,000 tons (1.6m tons, according to some sources) of carbon dioxide (). The gas cloud initially rose at nearly and then, being heavier than air, descended onto nearby villages, displacing all the air and suffocating people and livestock within of the lake. \n\nA degassing system has since been installed at the lake, with the aim of reducing the concentration of in deep waters and therefore the risk of further eruptions.\n\nIt is not known what triggered the catastrophic outgassing. Most geologists suspect a landslide, but some believe that a small volcanic eruption may have occurred on the bed of the lake. A third possibility is that cool rainwater falling on one side of the lake triggered the overturn. Others still believe there was a small earthquake, but as witnesses did not report feeling any tremors on the morning of the disaster, this hypothesis is unlikely. Whatever the cause, the event resulted in the supersaturated deep water rapidly mixing with the upper layers of the lake, where the reduced pressure allowed the stored CO to effervesce out of solution.\n\nIt is believed that about of gas was released. The normally blue waters of the lake turned a deep red after the outgassing, due to iron-rich water from the deep rising to the surface and being oxidised by the air. The level of the lake dropped by about a meter and trees near the lake were knocked down.\n\nScientists concluded from evidence that a column of water and foam formed at the surface of the lake, spawning a wave of at least that swept the shore on one side.\n\nCarbon dioxide, being about 1.5 times as dense as air, caused the cloud to \"hug\" the ground and move down the valleys, where there were various villages. The mass was about thick, and travelled downward at . For roughly , the gas cloud was concentrated enough to suffocate many people in their sleep in the villages of Nyos, Kam, Cha, and Subum. About 4,000 inhabitants fled the area, and many of these developed respiratory problems, lesions, and paralysis as a result of the gas cloud.\n\nOne survivor, Joseph Nkwain from Subum, described himself when he awoke after the gases had struck:\n\n\"I could not speak. I became unconscious. I could not open my mouth because then I smelled something terrible ... I heard my daughter snoring in a terrible way, very abnormal ... When crossing to my daughter's bed ... I collapsed and fell. I was there till nine o'clock in the (Friday) morning ... until a friend of mine came and knocked at my door ... I was surprised to see that my trousers were red, had some stains like honey. I saw some ... starchy mess on my body. My arms had some wounds ... I didn't really know how I got these wounds ... I opened the door ... I wanted to speak, my breath would not come out ... My daughter was already dead ... I went into my daughter's bed, thinking that she was still sleeping. I slept till it was 4:30 p.m. in the afternoon ... on Friday. (Then) I managed to go over to my neighbors' houses. They were all dead ... I decided to leave ... (because) most of my family was in Wum ... I got my motorcycle ... A friend whose father had died left with me (for) Wum ... As I rode ... through Nyos I didn't see any sign of any living thing ... (When I got to Wum), I was unable to walk, even to talk ... my body was completely weak.\"\n\nFollowing the eruption, many survivors were treated at the main hospital in Yaoundé, the country's capital. It was believed that many of the victims had been poisoned by a mixture of gases that included hydrogen and sulfur. Poisoning by these gases would lead to burning pains in the eyes and nose, coughing and signs of asphyxiation similar to being strangled.\n\nThe scale of the disaster led to much study on how a recurrence could be prevented. Several researchers proposed the installation of degassing columns from rafts in the middle of the lake. The principle is to slowly vent the by lifting heavily saturated water from the bottom of the lake through a pipe, initially by using a pump, but only until the release of gas inside the pipe naturally lifts the column of effervescing water, making the process self-sustaining.\n\nStarting from 1995, feasibility studies were successfully conducted, and the first permanent degassing tube was installed at Lake Nyos in 2001. Two additional pipes were installed in 2011.\n\nFollowing the Lake Nyos disaster, scientists investigated other African lakes to see if a similar phenomenon could happen elsewhere. Lake Kivu in the Democratic Republic of Congo, 2,000 times larger than Lake Nyos, was also found to be supersaturated, and geologists found evidence that outgassing events around the lake happened about every thousand years.\n\nA Greek novel by Basileios Drolias focusing on the lake Nyos disaster and the degassing of the lake was published in 2016. \n\n"}
{"id": "33185828", "url": "https://en.wikipedia.org/wiki?curid=33185828", "title": "Lenya National Park", "text": "Lenya National Park\n\nLenya National Park is a national park located in the Tenasserim Hills, Burma in the border area with Thailand, limiting with the Namtok Huai Yang National Park. The park consists of 176,638 hectares of lowland tropical forest.\n\nThe main purpose of the national park is the maintenance of natural resources. The demarcation is in course. It is governed by the Burma Forest Department and the level of protection is partial, for logging and forest plantations are allowed. The altitudes within the park range from 10 to 855 m. The endangered Gurney's pitta, endemic to Thailand and Myanmar, is found within the park.\n"}
{"id": "8746727", "url": "https://en.wikipedia.org/wiki?curid=8746727", "title": "Level of support for evolution", "text": "Level of support for evolution\n\nThe level of support for evolution among scientists, the public and other groups is a topic that frequently arises in the creation-evolution controversy and touches on educational, religious, philosophical, scientific and political issues. The subject is especially contentious in countries where significant levels of non-acceptance of evolution by general society exist although evolution is taught at school and university.\n\nNearly all (around 97%) of the scientific community accepts evolution as the dominant scientific theory of biological diversity. Scientific associations have strongly rebutted and refuted the challenges to evolution proposed by intelligent design proponents.\n\nThere are religious sects and denominations in several countries for whom the theory of evolution is in conflict with creationism that is central to their beliefs, and who therefore reject it: in the United States, South Africa, India, South Korea, Singapore, the Philippines, and Brazil, with smaller followings in the United Kingdom, the Republic of Ireland, Japan, Italy, Germany, Israel, Australia, New Zealand, and Canada.\n\nSeveral publications discuss the subject of acceptance, including a document produced by the United States National Academy of Sciences.\n\nThe vast majority of the scientific community and academia supports evolutionary theory as the only explanation that can fully account for observations in the fields of biology, paleontology, molecular biology, genetics, anthropology, and others. A 1991 Gallup poll found that about 5% of American scientists (including those with training outside biology) identified themselves as creationists.\n\nAdditionally, the scientific community considers intelligent design, a neo-creationist offshoot, to be unscientific, pseudoscience, or junk science. The U.S. National Academy of Sciences has stated that intelligent design \"and other claims of supernatural intervention in the origin of life\" are not science because they cannot be tested by experiment, do not generate any predictions, and propose no new hypotheses of their own. In September 2005, 38 Nobel laureates issued a statement saying \"Intelligent design is fundamentally unscientific; it cannot be tested as scientific theory because its central conclusion is based on belief in the intervention of a supernatural agent.\" In October 2005, a coalition representing more than 70,000 Australian scientists and science teachers issued a statement saying \"intelligent design is not science\" and calling on \"all schools not to teach Intelligent Design (ID) as science, because it fails to qualify on every count as a scientific theory\".\n\nIn 1986, an \"amicus curiae\" brief, signed by 72 US Nobel Prize winners, 17 state academies of science and 7 other scientific societies, asked the US Supreme Court in \"Edwards v. Aguillard\", to reject a Louisiana state law requiring that where evolutionary science was taught in public schools, creation science must also be taught. The brief also stated that the term \"creation science\" as used by the law embodied religious dogma, and that \"teaching religious ideas mislabeled as science is detrimental to scientific education\". This was the largest collection of Nobel Prize winners to sign a petition up to that point. According to anthropologists Almquist and Cronin, the brief is the \"clearest statement by scientists in support of evolution yet produced.\"\n\nThere are many scientific and scholarly organizations from around the world that have issued statements in support of the theory of evolution. The American Association for the Advancement of Science, the world's largest general scientific society with more than 130,000 members and over 262 affiliated societies and academies of science including over 10 million individuals, has made several statements and issued several press releases in support of evolution. The prestigious United States National Academy of Sciences, which provides science advice to the nation, has published several books supporting evolution and criticising creationism and intelligent design.\n\nThere is a notable difference between the opinion of scientists and that of the general public in the United States. A 2009 poll by Pew Research Center found that \"Nearly all scientists (97%) say humans and other living things have evolved over time – 87% say evolution is due to natural processes, such as natural selection. The dominant position among scientists – that living things have evolved due to natural processes – is shared by only about a third (32%) of the public.\"\n\nOne of the earliest resolutions in support of evolution was issued by the American Association for the Advancement of Science in 1922, and readopted in 1929.\n\nAnother early effort to express support for evolution by scientists was organized by Nobel Prize–winning American biologist Hermann J. Muller in 1966. Muller circulated a petition entitled \"Is Biological Evolution a Principle of Nature that has been well established by Science?\" in May 1966:\n\nThis manifesto was signed by 177 of the leading American biologists, including George G. Simpson of Harvard University, Nobel Prize Winner Peter Agre of Duke University, Carl Sagan of Cornell, John Tyler Bonner of Princeton, Nobel Prize Winner George Beadle, President of the University of Chicago, and Donald F. Kennedy of Stanford University, formerly head of the United States Food and Drug Administration.\n\nThis was followed by the passing of a resolution by the American Association for the Advancement of Science (AAAS) in the fall of 1972 that stated, in part, \"the theory of creation ... is neither scientifically grounded nor capable of performing the rules required of science theories\". The United States National Academy of Sciences also passed a similar resolution in the fall of 1972. A statement on evolution called \"A Statement Affirming Evolution as a Principle of Science.\" was signed by Nobel Prize Winner Linus Pauling, Isaac Asimov, George G. Simpson, Caltech Biology Professor Norman H. Horowitz, Ernst Mayr, and others, and published in 1977. The governing board of the American Geological Institute issued a statement supporting resolution in November 1981.\nShortly thereafter, the AAAS passed another resolution supporting evolution and disparaging efforts to teach creationism in science classes.\n\nTo date, there are no scientifically peer-reviewed research articles that disclaim evolution listed in the scientific and medical journal search engine Pubmed.\n\nThe Discovery Institute announced that over 700 scientists had expressed support for intelligent design as of February 8, 2007. This prompted the National Center for Science Education to produce a \"light-hearted\" petition called \"Project Steve\" in support of evolution. Only scientists named \"Steve\" or some variation (such as Stephen, Stephanie, and Stefan) are eligible to sign the petition. It is intended to be a \"tongue-in-cheek parody\" of the lists of alleged \"scientists\" supposedly supporting creationist principles that creationist organizations produce. The petition demonstrates that there are more scientists who accept evolution with a name like \"Steve\" alone (over 1370) than there are in total who support intelligent design. This is, again, why the percentage of scientists who support evolution has been estimated by Brian Alters to be about 99.9 percent.\n\nMany creationists act as evangelists and their organizations are registered as tax-free religious organizations. Creationists have claimed that they represent the interests of true Christians, and evolution is associated only with atheism.\n\nHowever, not all religious organizations find support for evolution incompatible with their religious faith. For example, 12 of the plaintiffs opposing the teaching of creation science in the influential \"McLean v. Arkansas\" court case were clergy representing Methodist, Episcopal, African Methodist Episcopal, Catholic, Southern Baptist, Reform Jewish, and Presbyterian groups. There are several religious organizations that have issued statements advocating the teaching of evolution in public schools. In addition, the Archbishop of Canterbury, Dr. Rowan Williams, issued statements in support of evolution in 2006. The Clergy Letter Project is a signed statement by 12,808 (as of 28 May 2012) American Christian clergy of different denominations rejecting creationism organized in 2004. Molleen Matsumura of the National Center for Science Education found, of Americans in the twelve largest Christian denominations, at least 77% belong to churches that support evolution education (and that at one point, this figure was as high as 89.6%). These religious groups include the Catholic Church, as well as various denominations of Protestantism, including the United Methodist Church, National Baptist Convention, USA, Evangelical Lutheran Church in America, Presbyterian Church (USA), National Baptist Convention of America, African Methodist Episcopal Church, the Episcopal Church, and others. A figure closer to about 71% is presented by the analysis of Walter B. Murfin and David F. Beck.\n\nMichael Shermer argued in Scientific American in October 2006 that evolution supports concepts like family values, avoiding lies, fidelity, moral codes and the rule of law. Shermer also suggests that evolution gives more support to the notion of an omnipotent creator, rather than a tinkerer with limitations based on a human model.\n\nThe Ahmadiyya Movement universally accepts evolution and actively promotes it. Mirza Tahir Ahmad, Fourth Caliph of the Ahmadiyya Muslim Community has stated in his magnum opus \"Revelation, Rationality, Knowledge & Truth\" that evolution did occur but only through God being the One who brings it about. It does not occur itself, according to the Ahmadiyya Muslim Community. The Ahmadis do not believe Adam was the first human on earth, but merely the first prophet to receive a revelation of God.\n\nA fundamental part of `Abdul-Bahá's teachings on evolution is the belief that all life came from the same origin: \"the origin of all material life is one...\" He states that from this sole origin, the complete diversity of life was generated: \"Consider the world of created beings, how varied and diverse they are in species, yet with one sole origin\" He explains that a slow, gradual process led to the development of complex entities:\n\nThe 1950 encyclical \"Humani generis\" advocated scepticism towards evolution without explicitly rejecting it; this was substantially amended by Pope John-Paul II in 1996 in an address to the Pontifical Academy of Sciences in which he said, \"Today, almost half a century after publication of the encyclical, new knowledge has led to the recognition of the theory of evolution as more than a hypothesis.\" Between 2000 and 2002 the International Theological Commission found that \"Converging evidence from many studies in the physical and biological sciences furnishes mounting support for some theory of evolution to account for the development and diversification of life on earth, while controversy continues over the pace and mechanisms of evolution.\" This statement was published by the Vatican on July 2004 by the authority of Cardinal Ratzinger (who became Pope Benedict XVI) who was the president of the Commission at the time.\n\nThe Magisterium has not made an authoritative statement on intelligent design, and has permitted arguments on both sides of the issue. In 2005, Cardinal Christoph Schönborn of Vienna appeared to endorse intelligent design when he denounced philosophically materialist interpretations of evolution. In an op-ed in the New York Times he said \"Evolution in the sense of common ancestry might be true, but evolution in the neo-Darwinian sense - an unguided, unplanned process of random variation and natural selection - is not.\" This common line of reasoning among fundamentalist theologians is flawed, as evolution by natural selection is not random at all; only mutations occur in a stochastic manner, while natural selection establishes genes which aid survival in a particular environment.\n\nIn the January 16–17 2006 edition of the official Vatican newspaper \"L'Osservatore Romano\", University of Bologna evolutionary biology Professor Fiorenzo Facchini wrote an article agreeing with the judge's ruling in \"Kitzmiller v. Dover\" and stating that intelligent design was unscientific. Jesuit Father George Coyne, former director of the Vatican Observatory, has also denounced intelligent design.\n\nHindus believe in the concept of evolution of life on Earth. The concepts of Dashavatara—different incarnations of God starting from simple organisms and progressively becoming complex beings—and Day and Night of Brahma are generally cited as instances of Hindu acceptance of evolution.\n\nIn the United States, many Protestant denominations promote creationism, preach against evolution, and sponsor lectures and debates on the subject. Denominations that explicitly advocate creationism instead of evolution or \"Darwinism\" include the Assemblies of God, the Free Methodist Church, Lutheran Church–Missouri Synod, Pentecostal Churches, Seventh-day Adventist Churches, Wisconsin Evangelical Lutheran Synod, Christian Reformed Church, Southern Baptist Convention, and the Pentecostal Oneness churches. Jehovah's Witnesses produce gap creationism and day-age creationism literature to refute evolution but reject the \"creationist\" label, which they consider to apply only to Young Earth creationism.\n\nA common complaint of creationists is that evolution is of no value, has never been used for anything, and will never be of any use. According to many creationists, nothing would be lost by getting rid of evolution, and science and industry might even benefit.\n\nIn fact, evolution is being put to practical use in industry and widely used on a daily basis by researchers in medicine, biochemistry, molecular biology, and genetics to both formulate hypotheses about biological systems for the purposes of experimental design, as well as to rationalise observed data and prepare applications. As of August 2017 there are 487,558 scientific papers in PubMed that mention 'evolution'. Pharmaceutical companies utilize biological evolution in their development of new products, and also use these medicines to combat evolving bacteria and viruses.\n\nBecause of the perceived value of evolution in applications, there have been some expressions of support for evolution on the part of corporations. In Kansas, there has been some widespread concern in the corporate and academic communities that a move to weaken the teaching of evolution in schools will hurt the state's ability to recruit the best talent, particularly in the biotech industry. Paul Hanle of the Biotechnology Institute warned that the United States risks falling behind in the biotechnology race with other nations if it does not do a better job of teaching evolution. James McCarter of Divergence Incorporated stated that the work of 2001 Nobel Prize winner Leland Hartwell relied heavily on the use of evolutionary knowledge and predictions, both of which have significant implications for the treatment of cancers. Furthermore, McCarter concluded that 47 of the last 50 Nobel Prizes in medicine or physiology depended on an understanding of evolutionary theory (according to McCarter's unspecified personal criteria).\n\nThere are also many educational organizations that have issued statements in support of the theory of evolution.\n\nRepeatedly, creationists and intelligent design advocates have lost suits in US courts. Here is a list of important court cases in which creationists have suffered setbacks:\n\n\nThere does not appear to be significant correlation between believing in evolution and understanding evolutionary science. In some countries, creationist beliefs (or a lack of support for evolutionary theory) are relatively widespread, even garnering a majority of public opinion. A study published in \"Science\" compared attitudes about evolution in the United States, 32 European countries (including Turkey) and Japan. The only country where acceptance of evolution was lower than in the United States was Turkey (25%). Public acceptance of evolution was most widespread (at over 80% of the population) in Iceland, Denmark and Sweden.\nAccording to the PEW research center, Afghanistan has the lowest acceptance of evolution in the Muslim countries. Only 26% of people in Afghanistan accept evolution. 62% deny human evolution and believe that humans have always existed in their present form..\n\nAccording to a 2014 poll produced by the Pew Research Center, 71% of people in Argentina believe \"humans and other living things evolved over time\" while 23% believe they have \"always existed in the present form.\"\nAccording to the PEW research, 56 percent of Armenians deny human evolution & claim that humans have always existed in their present and only 34 percent of Armenians accept human evolution.\n\nA 2009 poll showed that almost a quarter of Australians believe \"the biblical account of human origins\" over the Darwinian account. 42 percent of Australians believe in a \"wholly scientific\" explanation for the origins of life, while 32 percent believe in an evolutionary process \"guided by God\".\n\nA 2010 survey conducted by Auspoll and the Australian Academy of Science found that 79% of Australians believe in evolution (71% believe it is currently occurring, 8% believe in evolution but do not think it is currently occurring), 11% were not sure and 10% stated they do not believe in evolution.\n\nAccording to a 2014 poll by the Pew Research Center, 44% of people in Bolivia believe \"humans and other living things evolved over time\" while 39% believe they have \"always existed in the present form.\"\n\nIn a 2010 poll, 59% of respondents said they believe in theistic evolution, or evolution guided by God. A further 8% believe in evolution without divine intervention, while 25% were creationists. Support for creationism was stronger among the poor and the least educated. According to a 2014 poll produced by the Pew Research Center, 66% of Brazilians agree that humans evolved over time and 29% think they have always existed in the present form.\n\nIn a 2012 poll, 61% of Canadians believe that humans evolved from less advanced life forms, while 22% believe that God created human beings in their present form within the last 10,000 years.\n\nAccording to a 2014 poll by the Pew Research Center, 69% of people in Chile believe \"humans and other living things evolved over time\" while 26% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 59% of people in Colombia believe \"humans and other living things evolved over time\" while 35% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 56% of people in Costa Rica believe \"humans and other living things evolved over time\" while 38% believe they have \"always existed in the present form.\"\n\nAccording to the PEW research center,the Czech Republic has the highest acceptance of evolution in Eastern Europe. 83 percent people in the Czech Republic believe that humans evolved over time.\n\nAccording to a 2014 poll by the Pew Research Center, 41% of people in Dominican Republic believe \"humans and other living things evolved over time\" while 56% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 50% of people in Ecuador believe \"humans and other living things evolved over time\" while 44% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 46% of people in El Salvador believe \"humans and other living things evolved over time\" while 45% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 55% of people in Guatemala believe \"humans and other living things evolved over time\" while 38% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 49% of people in Honduras believe \"humans and other living things evolved over time\" while 45% believe they have \"always existed in the present form.\"\n\nAccording to the PEW research center, Kazakhstan has the highest acceptance of evolution in the Muslim countries. 79% of \npeople in Kazakhstan acceptance the theory of evolution.\n\nAmong those who had heard of Charles Darwin and knew something about the theory of evolution, 77% of people in India agree that enough scientific evidence exists to support Charles Darwin’s Theory of Evolution. Also, 85% of God believing Indians who know about evolution agree that life on earth evolved over time as a result of natural selection.\n\nIn a survey carried among 10 major nations, the highest proportion that agreed that evolutionary theories alone should be taught in schools was in India, at 49%.\n\nIn a recent survey conducted across 12 states in India, public acceptence of evolution stood at 65.5% across Indian population. Highest acceptence was found in Delhi, Maharashtra and Kerala (all above 78%) while the least was found to be in Haryana (41.3%). Males were marginally more likely to accept the evolution compared with females (72% vs. 69%), and non-religious people compared with religious people (74% vs. 67%). Surprisingly people who identified as ‘rightists’ accepted the evolution more than those who identified themselves as ‘leftists’ (66% vs. 61%) in political spectra. The study also identified teachers and students (over 73%) as most likely to accept evolution while employed adults (59%) least. While at the international level, the trend is quite clear that religiosity is inversely proportional to public acceptance of evolution, situation in India was strikingly opposite. Lead author, Dr. Felix Bast from Central University of Punjab conjectured possible reason for high public acceptance of evolution in India despite the fact of high religiosity is that Hinduism does not conflict Darwin’s theory of evolution to a large extent. According to 2011 census, Hindus encompass 80.3% of Indian population. Many concepts of Vedas and Hinduism support the scientific consensus of geology, climate science and evolution to a large extent. For example, according to Rigveda, the age of earth is 1.97 billion years, which is very old compared with that of creation myth propounded by Abrahamic religions (according to creationism-also called Intelligent Design, the age of earth is around 6000 years). Current scientific consensus of the age of earth is 4.543 billion years. A number of evolutionary biologists in the past as well were baffled about the surprising similarity between evolutionary theory and Hinduism. British evolutionary biologist JBS Haldane, for instance, suggested that Hindu concept of \"dashavatara\"- the ten incarnations of lord Vishnu- is a rough idea of vertebrate evolution (fish-the vertebrate to tortoise-reptile to boar-mammal to man). Vedic concepts of \"pralaya\" and \"mahapralaya\" too surprisingly capture the cyclic nature of global climate (glacial-interglacial cycles).\n\nA 2009 survey conducted by the McGill researchers and their international collaborators found that 85% of Indonesian high school students agreed with the statement, \"Millions of fossils show that life has existed for billions of years and changed over time.\"\n\nThe theory of evolution is a 'hard sell' in schools in Israel. More than half of Israeli Jews accept the human evolution while more than 40% deny human evolution & claim that humans have always existed in their present form. \n\nAccording to a 2014 poll by the Pew Research Center, 64% of people in Mexico believe \"humans and other living things evolved over time\" while 32% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 47% of people in Nicaragua believe \"humans and other living things evolved over time\" while 48% believe they have \"always existed in the present form.\"\n\nAccording to a 2008 Norstat poll for NRK, 59% of the Norwegian population fully accept evolution, 24% somewhat agree with the theory, 4% somewhat disagree with the theory while 8% do not accept evolution. 4% did not know.\n\nA 2009 survey conducted by the McGill researchers and their international collaborators found that 86% of Pakistani high school students agreed with the statement, \"Millions of fossils show that life has existed for billions of years and changed over time.\"\n\nAccording to a 2014 poll by the Pew Research Center, 61% of people in Panama believe \"humans and other living things evolved over time\" while 34% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 59% of people in Paraguay believe \"humans and other living things evolved over time\" while 30% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 51% of people in Peru believe \"humans and other living things evolved over time\" while 39% believe they have \"always existed in the present form.\"\n\nA 2006 UK poll on the \"origin and development of life\" asked participants to choose between three different explanations for the origin of life: 22% chose (Young Earth) creationism, 17% opted for intelligent design (\"certain features of living things are best explained by the intervention of a supernatural being, e.g. God\"), 48% selected evolution theory (with a divine role explicitly excluded) and the rest did not know. A 2009 poll found that only 38% of Britons believe God played no role in evolution. In a 2012 poll, 69% of Britons believe that humans evolved from less advanced life forms, while 17% believe that God created human beings in their present forms within the last 10,000 years.\n\nUS courts have ruled in favor of teaching evolution in science classrooms, and against teaching creationism, in numerous cases such as Edwards v. Aguillard, Hendren v. Campbell, McLean v. Arkansas and Kitzmiller v. Dover Area School District.\n\nA prominent organization in the United States behind the intelligent design movement is the Discovery Institute, which, through its Center for Science and Culture, conducts a number of public relations and lobbying campaigns aimed at influencing the public and policy makers in order to advance its position in academia. The Discovery Institute claims that because there is a significant lack of public support for evolution, that public schools should, as their campaign states, \"Teach the Controversy\", although there is no controversy over the validity of evolution within the scientific community.\n\nThe US has one of the highest levels of public belief in biblical or other religious accounts of the origins of life on earth among industrialized countries. However according to the PEW research center, 62 percent of adults in the United States accept human evolution and while 34 percent of adults believe that humans have always existed in their present form. The poll involved over 35,000 adults in the United States. However acceptance of evolution varies per state. For example the State of Vermont has the highest acceptance of evolution of any other State in the United States. 79% people in Vermont accept human evolution. Mississippi has the lowest acceptance of evolution of any other State in the United States.\nA 2017 Gallup creationism survey found that 38% of adults in the United States inclined to the view that \"God created humans in their present form at one time within the last 10,000 years\" when asked for their views on the origin and development of human beings, which was noted as being at the lowest level in 35 years. 19% believed that \"human beings have developed over millions of years from less advanced forms of life, but God had no part in this process\", despite 49% of respondents indicating they believed in evolution. Belief in creationism is inversely correlated to education; only 22% of those with post-graduate degrees believe in strict creationism. (The level of support for strict creationism could be even lower when poll results are adjusted after comparison with other polls with questions that more specifically account for uncertainty and ambivalence.A 2000 poll for People for the American Way found 70% of the American public felt that evolution was compatible with a belief in God.\n\nA 2005 Pew Research Center poll found that 70% of evangelical Christians believed that living organisms have not changed since their creation, but only 31% of Catholics and 32% of mainline Protestants shared this opinion. A 2005 Harris Poll estimated that 63% of liberals and 37% of conservatives agreed that humans and other primates have a common ancestry.\n\nAccording to a 2014 poll produced by the Pew Research Center, 74% of people in Uruguay believe \"humans and other living things evolved over time\" while 20% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 63% of people in Venezuela believe \"humans and other living things evolved over time\" while 33% believe they have \"always existed in the present form.\"\n\nThe level of assent that evolution garners has changed with time. The trends in acceptance of evolution can be estimated.\n\nThe level of support for evolution in different communities has varied with time. Darwin's theory had convinced almost every naturalist within 20 years of its publication in 1858, and was making serious inroads with the public and the more liberal clergy. It had reached such extremes, that by 1880, one\nAmerican religious weekly publication estimated that \"perhaps a quarter, perhaps a half of the educated ministers in our leading Evangelical denominations\" felt \"that the story of the creation and fall of man, told in Genesis, is no more the record of actual occurrences than is the parable of the Prodigal Son.\"\n\nBy the late 19th century, many of the most conservative Christians accepted an ancient earth, and life on earth before Eden. Victorian Era Creationists were more akin to people who subscribe to theistic evolution today. Even fervent anti-evolutionist Scopes Trial prosecutor William Jennings Bryan interpreted the \"days\" of Genesis as ages of the earth, and acknowledged that biochemical evolution took place, drawing the line only at the story of Adam and Eve's creation. Prominent pre-World War II creationist Harry Rimmer allowed an Old Earth by slipping millions of years into putative gaps in the Genesis account, and claimed that the Noachian Flood was only a local phenomenon.\n\nIn the decades of the 20th century, George McCready Price and a tiny group of Seventh-day Adventist followers were among the very few believers in a Young Earth and a worldwide flood, which Price championed in his \"new catastrophism\" theories. It was not until the publication of John C. Whitcomb, Jr., and Henry M. Morris’s book \"Genesis Flood\" in 1961 that Price's idea was revived. In the last few decades, many creationists have adopted Price's beliefs, becoming progressively more strict biblical literalists.\n\nIn a 1991 Gallup poll, 47% of the US population, and 25% of college graduates agreed with the statement, \"God created man pretty much in his present form at one time within the last 10,000 years.\"\n\nFourteen years\nlater, in 2005, Gallup found that 53% of Americans expressed the belief that \"God created human beings in their present form exactly the way the Bible describes it.\" About 2/3 (65.5%) of those surveyed thought that creationism was definitely or probably true. In 2005 a Newsweek poll discovered that 80 percent of the American public thought that \"God created the universe.\" and the Pew Research Center reported that \"nearly two-thirds of Americans say that creationism should be taught alongside evolution in public schools.\" Ronald Numbers commented on that with \"Most surprising of all was the discovery that large numbers of high-school biology teachers — from 30% in Illinois and 38% in Ohio to a whopping 69% in Kentucky — supported the teaching of creationism.\"\n\nThe National Center for Science Education reports that from 1985 to 2005, the number of Americans unsure about evolution increased from 7% to 21%, while the number rejecting evolution declined from 48% to 39%. Jon Miller of Michigan State University has found in his polls that the number of Americans who accept evolution has declined from 45% to 40% from 1985 to 2005.\n\nIn light of these somewhat contradictory results, it is difficult to know for sure what is happening to public opinion on evolution in the US. It does not appear that either side is making unequivocal progress. It does appear that uncertainty about the issue is increasing, however.\n\nAnecdotal evidence is that creationism is becoming more of an issue in the UK as well. One report in 2006 was that UK students are increasingly arriving ill-prepared to participate in medical studies or other advanced education.\n\nThe level of support for creationism among relevant scientists is minimal. In 2007 the Discovery Institute reported that about 600 scientists signed their \"A Scientific Dissent from Darwinism\" list, up from 100 in 2001. The actual statement of the Scientific Dissent from Darwinism is a relatively mild one that expresses skepticism about the absoluteness of 'Darwinism' (and is in line with the falsifiability required of scientific theories) to explain all features of life, and does not in any way represent an absolute denial or rejection of evolution. By contrast, a tongue-in-cheek response known as Project Steve, a list restricted to scientists named Steve, Stephanie etc. who agree that evolution is \"a vital, well-supported, unifying principle of the biological sciences,\" has 1,382 signatories . People with these names make up approximately 1% of the total U.S. population.\n\nThe United States National Science Foundation statistics on US yearly science graduates demonstrate that from 1987 to 2001, the number of biological science graduates increased by 59% while the number of geological science graduates decreased by 20.5%. However, the number of geology graduates in 2001 was only 5.4% of the number of graduates in the biological sciences, while it was 10.7% of the number of biological science graduates in 1987. The Science Resources Statistics Division of the National Science Foundation estimated that in 1999, there were 955,300 biological scientists in the US (about 1/3 of who hold graduate degrees). There were also 152,800 earth scientists in the US as well.\n\nA large fraction of the Darwin Dissenters have specialties unrelated to research on evolution; of the dissenters, three-quarters are not biologists. As of 2006, the dissenter list was expanded to include non-US scientists.\n\nSome researchers are attempting to understand the factors that affect people's acceptance of evolution. Studies have yielded inconsistent results, explains associate professor of education at Ohio State University, David Haury. He recently performed a study that found people are likely to reject evolution if they have feelings of uncertainty, regardless of how well they understand evolutionary theory. Haury believes that teachers need to show students that their intuitive feelings may be misleading (for example, using the Wason selection task), and thus to exercise caution when relying on them as they judge the rational merits of ideas.\n\n\n"}
{"id": "5062912", "url": "https://en.wikipedia.org/wiki?curid=5062912", "title": "Mercury(II) iodide", "text": "Mercury(II) iodide\n\nMercury(II) iodide is a chemical compound with the molecular formula HgI. It is typically produced synthetically but can also be found in nature as the extremely rare mineral coccinite. Unlike the related mercury(II) chloride it is hardly soluble in water (<100 ppm).\n\nMercury(II) iodide is produced by adding an aqueous solution of potassium iodide to an aqueous solution of mercury(II) chloride with stirring; the precipitate is filtered off, washed and dried at 70 °C.\n\nMercury(II) iodide displays thermochromism; when heated above 126 °C (400°K) it undergoes a phase transition, from the red alpha crystalline form to a pale yellow beta form. As the sample cools, it gradually reacquires its original colour. It has often used for thermochromism demonstrations. A third orange form is also known; this can be formed by recrystallisation and is also metastable, eventually converting back to the red alpha form. The various forms can exist in a diverse range of crystal structures and as a result mercury(II) iodide possess a surprisingly complex phase diagram.\n\nMercury(II) iodide is used for preparation of Nessler's reagent, used for detection of presence of ammonia.\n\nMercury(II) iodide is a semiconductor material, used in some x-ray and gamma ray detection and imaging devices operating at room temperatures.\n\nIn veterinary medicine, mercury(II) iodide is used in blister ointments in exostoses, bursal enlargement, etc. \n\nIt can appear as a precipitate in many reactions.\n\n"}
{"id": "18908325", "url": "https://en.wikipedia.org/wiki?curid=18908325", "title": "Mesa Power LP", "text": "Mesa Power LP\n\nMesa Power is a wind energy company owned by T. Boone Pickens. The company had plans to build large wind farms in the western Texas panhandle, the so-called Pampa Wind Project, part of the Pickens Plan. However, on July 8, 2009 the wind farm project was delayed for what were described as economic reasons. Pickens says he is fully committed to wind energy and to developing wind projects in the US and perhaps, Canada but that because of capital market setbacks he plans to be less aggressive with the Panhandle project . The company remains committed to the delivery 667 wind turbines and is planning to find projects for them. The company also plans to continue development of the Pampa project, but not at the pace originally expected.\n\n"}
{"id": "54683062", "url": "https://en.wikipedia.org/wiki?curid=54683062", "title": "Mohamed Shaker El-Markabi", "text": "Mohamed Shaker El-Markabi\n\nProf. Dr. Mohamed Shaker Almraqbi is an Egyptian engineer. He was chosen by Engineer Ibrahim Mahlab minister to take over the Ministry of Electricity and Renewable Energy, is Mohamed Hamed Shaker Almraqbi Professor of Electrical Power Engineering at the Faculty of Engineering, Cairo University. And who has held many important positions. He graduated from the Faculty of Engineering, Cairo University in 1968 and earned a doctorate in electrical engineering from the Imperial College, University of London in 1978 and worked teaching at the Faculty of Engineering, Cairo University, and began Consultative work since 1982, and established an office advisory in the field of electrical and mechanical consulting, and his office has a presence in a number of Arab countries, and worked as an engineer consultant for more than 1,500 project inside and outside Egypt, and worked with a number of global consulting offices.\n\n\n\n"}
{"id": "21647328", "url": "https://en.wikipedia.org/wiki?curid=21647328", "title": "Mollie Beattie", "text": "Mollie Beattie\n\nMollie H. Beattie (April 27, 1947 Glen Cove, Long Island – June 27, 1996) was an American conservationist, and director of the United States Fish and Wildlife Service. \nIn 2009, she was designated a Women's History Month Honoree by the National Women's History Project.\n\nShe was born on April 27, 1947, in Glen Cove, New York.\nShe graduated from Marymount College, Tarrytown with bachelor's degree in philosophy in 1968, and from the University of Vermont with a master's degree in forestry in 1979, and from Harvard University with a master's degree in public administration in 1991.\n\nFrom 1985 to 1989, she was Vermont commissioner of forests, parks and recreation; from 1989 to 1990, she was deputy secretary for Vermont's Agency of Natural Resources. \nFrom 1993 to 1996, she served as the first woman director of the U.S. Fish & Wildlife Service. \nShe oversaw the successful reintroduction of the gray wolf into northern Rocky Mountains.\n\nShe died on June 27, 1996, in Townshend, Vermont.\n\nShe was married to Rick Schwolsky.\n\nThe Mollie Beattie Wilderness in Arctic National Wildlife Refuge is named after her.\nIn the long term, the economy and the environment are the same thing.\nIf it's unenvironmental it is uneconomical. That is the rule of nature.\n-- Mollie Beattie \n\n\n\n"}
{"id": "11654634", "url": "https://en.wikipedia.org/wiki?curid=11654634", "title": "Mongstad scandal", "text": "Mongstad scandal\n\nThe Mongstad scandal was a crisis in the Norwegian oil company Statoil in 1987-88.\n\nThe company exceeded the NOK 8 billion budget by NOK 6 billion in upgrading the oil refinery at Mongstad. Retrospectively the reasons for the overexpenditure were attributed to bad planning, technical miscalculations and bad project management. The executives in Statoil were also accused of inability to act and for withholding information from the Norwegian Ministry of Petroleum and Energy. At the time the incident aroused considerable media attention.\n\nThe first warnings of a budget overspend came on 25 September 1987, when the over-expenditure was estimated at NOK 3,8 billion. On 20 November most of the board of directors had to resign. The board was led by chairman Inge Johansen and deputy chairman Vidkunn Hveding since 1984. The other board members who also resigned were Thor Andreassen (member since 1978), Fredrik Thoresen (member since 1984), Guttorm Hansen (member since 1986) and Toril V. Lundestad (member since 1986). The only board members not to resign were the three employee representatives. Two days later chief executive officer Arve Johnsen also resigned, at the time the only CEO in the history of the company to resign. In January 1988 reports of a possibility of the overexpenditure accumulating another billion NOK were presented. By April the anticipated overspend was believed to be in the order of NOK 8 billion, although the final sum came to NOK 6 billion.\n\nThe enormous shock effect of the Mongstad scandal should be understood in the light of the social and political situation of Norway in 1988. Statoil was at the time a limited company wholly owned by the Norwegian Ministry of Petroleum and Energy, who managed the entire profit. After the company had started to make a profit around 1980 it had become the guarantor for welfare in the country in the minds of people. The company's transfers to government treasury coffers exceeded that of income tax at the time. CEO Arve Johnsen was the man who could do nothing wrong, the Labour Party man who controlled Statoil even during a conservative government. Prime Minister Kåre Willoch's attempt to clip Statoil's wings a few years earlier had failed. And at the same time came the bankruptcy in the largest bank, Den norske Creditbank that had announced the start of the fall of the Yuppie age.\n\nThe media attention on the Mongstad scandal in 1988 was enormous, and was front-page material almost daily. The tabloid newspapers battles fiercely in trying to visualise the, at the time, almost unimaginable size of NOK 6 billion. The sum was creatively recalculated in kindergarten places, retirement home places, fighter jets etc. Sometimes the coverage became absurd with parallels such as \"Dagbladet\"<nowiki>'</nowiki>s example of 6 billion = enough to buy one AG3 assault rifle for each of the country's 4,5 million inhabitants. For years the term \"one mong\" was used as a synonym for the number 6 billion.\n"}
{"id": "257243", "url": "https://en.wikipedia.org/wiki?curid=257243", "title": "Pentaquark", "text": "Pentaquark\n\nA pentaquark is a subatomic particle consisting of four quarks and one antiquark bound together.\n\nAs quarks have a baryon number of +, and antiquarks of −, the pentaquark would have a total baryon number of 1, and thus would be a baryon. Further, because it has five quarks instead of the usual three found in regular baryons ( 'triquarks'), it would be classified as an exotic baryon. The name pentaquark was coined by Claude Gignoux et al. and Harry J. Lipkin in 1987; however, the possibility of five-quark particles was identified as early as 1964 when Murray Gell-Mann first postulated the existence of quarks. Although predicted for decades, pentaquarks proved surprisingly difficult to discover and some physicists were beginning to suspect that an unknown law of nature prevented their production.\n\nThe first claim of pentaquark discovery was recorded at LEPS in Japan in 2003, and several experiments in the mid-2000s also reported discoveries of other pentaquark states. Others were not able to replicate the LEPS results, however, and the other pentaquark discoveries were not accepted because of poor data and statistical analysis. On 13 July 2015, the LHCb collaboration at CERN reported results consistent with pentaquark states in the decay of bottom Lambda baryons ().\n\nOutside particle physics laboratories, pentaquarks also could be produced naturally by supernovae as part of the process of forming a neutron star. The scientific study of pentaquarks might offer insights into how these stars form, as well as allowing more thorough study of particle interactions and the strong force.\n\nA quark is a type of elementary particle that has mass, electric charge, and colour charge, as well as an additional property called flavour, which describes what type of quark it is (up, down, strange, charm, top, or bottom). Due to an effect known as colour confinement, quarks are never seen on their own. Instead, they form composite particles known as hadrons so that their colour charges cancel out. Hadrons made of one quark and one antiquark are known as mesons, while those made of three quarks are known as baryons. These 'regular' hadrons are well documented and characterized, however, there is nothing in theory to prevent quarks from forming 'exotic' hadrons such as tetraquarks with two quarks and two antiquarks, or pentaquarks with four quarks and one antiquark.\n\nA wide variety of pentaquarks are possible, with different quark combinations producing different particles. To identify which quarks compose a given pentaquark, physicists use the notation \"qqqq\", where \"q\" and \"\" respectively refer to any of the six flavours of quarks and antiquarks. The symbols u, d, s, c, b, and t stand for the up, down, strange, charm, bottom, and top quarks respectively, with the symbols of , , , , , corresponding to the respective antiquarks. For instance a pentaquark made of two up quarks, one down quark, one charm quark, and one charm antiquark would be denoted uudc.\n\nThe quarks are bound together by the strong force, which acts in such a way as to cancel the colour charges within the particle. In a meson, this means a quark is partnered with an antiquark with an opposite colour charge – blue and antiblue, for example – while in a baryon, the three quarks have between them all three colour charges – red, blue, and green. In a pentaquark, the colours also need to cancel out, and the only feasible combination is to have one quark with one colour (e.g. red), one quark with a second colour (e.g. green), two quarks with the third colour (e.g. blue), and one antiquark to counteract the surplus colour (e.g. antiblue).\n\nThe binding mechanism for pentaquarks is not yet clear. They may consist of five quarks tightly bound together, but it is also possible that they are more loosely bound and consist of a three-quark baryon and a two-quark meson interacting relatively weakly with each other via pion exchange (the same force that binds atomic nuclei) in a \"meson-baryon molecule\".\n\nThe requirement to include an antiquark means that many classes of pentaquark are hard to identify experimentally – if the flavour of the antiquark matches the flavour of any other quark in the quintuplet, it will cancel out and the particle will resemble its three-quark hadron cousin. For this reason, early pentaquark searches looked for particles where the antiquark did not cancel. In the mid-2000s, several experiments claimed to reveal pentaquark states. In particular, a resonance with a mass of (4.6 σ) was reported by LEPS in 2003, the . This coincided with a pentaquark state with a mass of predicted in 1997.\nThe proposed state was composed of two up quarks, two down quarks, and one strange antiquark (uudd). Following this announcement, nine other independent experiments reported seeing narrow peaks from and , with masses between and , all above 4 σ. While concerns existed about the validity of these states, the Particle Data Group gave the a 3-star rating (out of 4) in the 2004 \"Review of Particle Physics\". Two other pentaquark states were reported albeit with low statistical significance—the (ddss), with a mass of and the (uudd), with a mass of . Both were later found to be statistical effects rather than true resonances.\n\nTen experiments then looked for the , but came out empty-handed. Two in particular (one at BELLE, and the other at CLAS) had nearly the same conditions as other experiments which claimed to have detected the (DIANA and SAPHIR respectively). The 2006 \"Review of Particle Physics\" concluded:\n[T]here has not been a high-statistics confirmation of any of the original experiments that claimed to see the ; there have been two high-statistics repeats from Jefferson Lab that have clearly shown the original positive claims in those two cases to be wrong; there have been a number of other high-statistics experiments, none of which have found any evidence for the ; and all attempts to confirm the two other claimed pentaquark states have led to negative results. The conclusion that pentaquarks in general, and the , in particular, do not exist, appears compelling.\n\nThe 2008 \"Review of Particle Physics\" went even further:\nThere are two or three recent experiments that find weak evidence for signals near the nominal masses, but there is simply no point in tabulating them in view of the overwhelming evidence that the claimed pentaquarks do not exist... The whole story—the discoveries themselves, the tidal wave of papers by theorists and phenomenologists that followed, and the eventual \"undiscovery\"—is a curious episode in the history of science.\n\nDespite these null results, LEPS results continue to show the existence of a narrow state with a mass of , with a statistical significance of 5.1 σ. Experiments continue to study this controversy.\n\nIn July 2015, the LHCb collaboration at CERN identified pentaquarks in the channel, which represents the decay of the bottom lambda baryon into a J/ψ meson , a kaon and a proton (p). The results showed that sometimes, instead of decaying via intermediate lambda states, the decayed via intermediate pentaquark states. The two states, named and , had individual statistical significances of 9 σ and 12 σ, respectively, and a combined significance of 15 σ — enough to claim a formal discovery. The analysis ruled out the possibility that the effect was caused by conventional particles. The two pentaquark states were both observed decaying strongly to , hence must have a valence quark content of two up quarks, a down quark, a charm quark, and an anti-charm quark (), making them charmonium-pentaquarks.\n\nThe search for pentaquarks was not an objective of the LHCb experiment (which is primarily designed to investigate matter-antimatter asymmetry) and the apparent discovery of pentaquarks was described as an \"accident\" and \"something we’ve stumbled across\" by the Physics Coordinator for the experiment.\n\nThe production of pentaquarks from electroweak decays of baryons has extremely small cross-section and yields very limited information about internal structure of pentaquarks. For this reason, there are several ongoing and proposed initiatives to study pentaquark production in other channels.\n\nIt is expected that pentaquarks will be studied in electron-proton collisions in Hall B E2-16-007 and Hall C E12-12-001A experiments at JLAB. The major challenge in these studies is a heavy mass of the pentaquark, which will be produced at the tail of photon-proton spectrum in JLAB kinematics. For this reason, the currently unknown branching fractions of pentaquark should be sufficiently large to allow pentaquark detection in JLAB kinematics. The proposed Electron Ion Collider which has higher energies is much better suited for this problem. \n\nAn interesting channel to study pentaquarks in proton-nuclear collisions was suggested in . This process has a large cross-section due to lack of electroweak intermediaries and gives access to pentaquark wave function. In the fixed-target experiments pentaquarks will be produced with small rapidities in laboratory frame and will be easily detected.\nBesides, if there are neutral pentaquarks, as suggested in several models based on flavour symmetry, these might be also produced in this mechanism. This process might be studied at future high-luminosity experiments like After@LHC and NICA.\n\nThe discovery of pentaquarks will allow physicists to study the strong force in greater detail and aid understanding of quantum chromodynamics. In addition, current theories suggest that some very large stars produce pentaquarks as they collapse. The study of pentaquarks might help shed light on the physics of neutron stars.\n\n\n"}
{"id": "24395391", "url": "https://en.wikipedia.org/wiki?curid=24395391", "title": "Plugless Power", "text": "Plugless Power\n\nPlugless Power is a family of Electric Vehicle Supply Equipment (EVSE) products manufactured by Evatran that enable inductive charging for electric vehicles (EV). The Plugless Power EVSE wirelessly delivers electrical power to the on-board EV battery charger using electromagnetic induction without a physical connection to the vehicle. An EV equipped with a Plugless Vehicle Adapter can be charged by parking it over an inductive Plugless Parking Pad. The active step of plugging a cord into the vehicle is eliminated.\n\nIn 2009, Evatran began development of Plugless Power, an inductive charging system for charging Electric Vehicles without plugging in. Field trials were begun in March 2010. The first system was sold to Google in 2011 for employee use at the Mountain View campus. Evatran began selling the Plugless L2 Wireless charging system to the public in 2014.\n\nIn simple terms, inductive charging works by separating the two halves of an electric transformer with an air gap – one half, the Plugless Power Vehicle Adapter, is installed on the vehicle and the other half, the Plugless Power Parking Pad, is installed on the floor of a garage or in a parking lot. When a car with an Adapter drives over a Pad, the two pieces are brought into close proximity, , then current from the electrical grid flows through the coils in the Power Pad to create magnetic fields and these fields induce current flow in the Vehicle Adapter's coils to charge the battery.\n\nIdaho National Laboratory (INL) testing in 2013 found the system had a power delivery efficiency between 84%-90%, compared to 95%-99% for corded charging systems, depending on the alignment of the Adapter and Pad, the separation gap, and the rate of power transfer in use (kW).\n\nIn 2014, \"Popular Science\" included the Plugless L2 charging system as Best of What's New 2014.\n\nFormer Chrysler CEO, Tom LaSorda, joined Evatran as a strategic adviser in October 2014. LaSorda joined the management board at Daimler in 2004, then served as CEO and president of Chrysler from 2004 to 2007.\n\nAs of October 2016, the Plugless L2 was being sold for use with the Nissan LEAF, Chevrolet Volt, Cadillac ELR and Tesla Model S.\n\nFounded in 2009, Evatran was formed as a clean technology subsidiary of MTC Transformers, an award winning American manufacturer of high-quality, precision-engineered transformers and rewind services based in Wytheville, Virginia.\n\n\n"}
{"id": "42870463", "url": "https://en.wikipedia.org/wiki?curid=42870463", "title": "Polytantric Circle", "text": "Polytantric Circle\n\nThe Polytantric Circle was an organization that helped organize the yearly Summer solstice celebrations at Stonehenge, England. These celebrations, called the Stonehenge Free Festival, ran from 1972 to 1984. \n\nBy the early 1980s, the festival had begun to attract a large following, and the people who helped to put the festival together year after year began to take on specific roles. Willy X put together an annual festival list and had contact lists for bands, and the loose-knit Polytantric, an offshoot of the White Panthers, took the lead in organising the stage and pyramid roof. PA. Bands. Nik Turner of Hawkwind offered valuable experience in use of setting up of the pyramid, a free alternative space, a legendary stage, which had originally made its debut with Sphinx at the Edinburgh festival Greenham, Sizewell, Stonehenge, Hyde Park. Brixton. From festival to campaign gigs to legalise it to peace protest to benefit, the pyramid epitomised the ambitions of a radical and vibrant alternative popular youth counter culture. In the winter huge blocks of quartz crystals were hung inside the pyramid to purify the energies. \n\nStonehenge begun to grow rapidly: in 1980, 12,000 people had attended, but by '82 there were 35,000. However, coordination between English Heritage, the National Trust, and the Polytantric Circle seemed to be breaking down.\n\nIn 1983 it was decided to move the stage to the field closer to the woods and further away from the Stones. Sid Rawle and Big Steve the polytantric stage manager wandered across the fields past the mounds clutching a dowsing rod; it twitched downwards in a circle of daisies, the spot that was to become the site for the stage for the pyramid stage at the Stonehenge free festival. Six days and six nights of live music followed.\n\nIn 1985 celebrations at Stonehenge were banned by English courts. This ban was lifted in 1999, and the campaign for a new free festival summer solstice celebration continues.\n\nThe group published \"The Polytantric Newsletter\" and were the first to organise what became The Stonehenge Campaign.\n\n"}
{"id": "20662226", "url": "https://en.wikipedia.org/wiki?curid=20662226", "title": "Potentially hazardous object", "text": "Potentially hazardous object\n\nA potentially hazardous object (PHO) is a near-Earth object – either an asteroid or a comet – with an orbit that can make exceptionally close approaches to the Earth and large enough to cause significant regional damage in the event of impact.\n\nMost of these objects are potentially hazardous asteroids (PHAs), defined as having a minimum orbital intersection distance with Earth of less than and an absolute magnitude of 22 or brighter. there are 1,885 known PHAs (about 11% of the total near-Earth population), of which 157 are estimated to be larger than one kilometer in diameter \"(see list of largest PHAs below)\". Most of the discovered PHAs are Apollo asteroids (1,601) and fewer belong to the group of Aten asteroids (169).\n\nA potentially hazardous object can be known not to be a threat to Earth for the next 100 years or more, if its orbit is reasonably well determined. Potentially hazardous asteroids with some threat of impacting Earth in the next 100 years are listed on the Sentry Risk Table. Potentially hazardous asteroids are normally only a hazard on a time scale of hundreds of years as the known orbit becomes more divergent. After several astronomical surveys, the number of known PHAs has increased tenfold since the end of the 1990s \"(see bar charts below)\". The Minor Planet Center's website \"List of the Potentially Hazardous Asteroids\" also publishes detailed information for these objects.\n\nAn object is considered a PHO if its minimum orbit intersection distance (MOID) with respect to Earth is less than – approximately 19.5 lunar distances – and its absolute magnitude is brighter than 22, approximately corresponding to a diameter of at least . This is big enough to cause regional devastation to human settlements unprecedented in human history in the case of a land impact, or a major tsunami in the case of an ocean impact. Such impact events occur on average around once per 10,000 years. NEOWISE data estimates that there are 4,700 ± 1,500 potentially hazardous asteroids with a diameter greater than 100 meters.\n\nThe two main scales used to categorize the impact hazards of asteroids are the Palermo Technical Impact Hazard Scale and the Torino Scale.\n\nShort-period comets currently with an Earth-MOID less than 0.05 AU include: 109P/Swift-Tuttle, 55P/Tempel–Tuttle, 15P/Finlay, 289P/Blanpain, 255P/Levy, 206P/Barnard–Boattini, 21P/Giacobini–Zinner, and 73P/Schwassmann–Wachmann.\n\nIn 2012 NASA estimated 20 to 30 percent of these objects have been found. During an asteroid's close approaches to planets or moons other than the Earth, it will be subject to gravitational perturbation, modifying its orbit, and potentially changing a previously non-threatening asteroid into a PHA or vice versa. This is a reflection of the dynamic character of the Solar System.\n\nSeveral astronomical survey projects such as Lincoln Near-Earth Asteroid Research, Catalina Sky Survey and Pan-STARRS continue to search for more PHOs. Each one found is studied by various means, including optical, radar, and infrared to determine its characteristics, such as size, composition, rotation state, and to more accurately determine its orbit. Both professional and amateur astronomers participate in such observation and tracking.\n\nAsteroids larger than approximately 35 meters across can pose a threat to a town or city. However the diameter of most small asteroids is not well determined, as it is usually only estimated based on their brightness and distance, rather than directly measured from e.g. radar observations. For this reason NASA and the Jet Propulsion Laboratory use the more practical measure of absolute magnitude (\"H\"). Any asteroid with an absolute magnitude of 22.0 or brighter is assumed to be of the required size.\n\nOnly a coarse estimation of size can be found from the object's magnitude because an assumption must be made for its albedo which is also not usually known for certain. The NASA near-Earth object program uses an assumed albedo of 0.14 for this purpose. In May 2016, the asteroid size estimates arising from the Wide-field Infrared Survey Explorer and NEOWISE missions have been questioned. Although the early original criticism had not undergone peer review, a more recent peer-reviewed study was subsequently published.\n\nWith a mean-diameter of approximately 7 kilometers, the Apollo asteroid is likely the largest known potentially hazardous object, despite its fainter absolute magnitude of 15.2, compared to other listed objects in the table below \"(note: calculated mean-diameters in table are inferred from the objects brightness and its (assumed) albedo. They are only an approximation.)\". The lowest numbered PHA is 1566 Icarus.\n\nBelow is listed the largest PHAs (based on absolute magnitude H) discovered in a given year. Historical data of the cumulative number of discovered PHA since 1999 are displayed in the bar charts—one for the total number and the other for objects larger than one kilometer.\n\n\n\n"}
{"id": "20482169", "url": "https://en.wikipedia.org/wiki?curid=20482169", "title": "Random logic", "text": "Random logic\n\nRandom logic is a semiconductor circuit design technique that translates high-level logic descriptions directly into hardware features such as AND and OR gates. The name derives from the fact that few easily discernible patterns are evident in the arrangement of features on the chip and in the interconnects between them. In VLSI chips, random logic is often implemented with standard cells and gate arrays.\n\nRandom logic accounts for a large part of the circuit design in modern microprocessors. Compared to microcode, another popular design technique, random logic offers faster execution of processor opcodes, provided that processor speeds are faster than memory speeds. A disadvantage is that it is difficult to design random logic circuitry for processors with large and complex instruction sets. The hard-wired instruction logic occupies a large percentage of the chip's area, and it becomes difficult to lay out the logic so that related circuits are close to one another.\n"}
{"id": "4705847", "url": "https://en.wikipedia.org/wiki?curid=4705847", "title": "Relative wind", "text": "Relative wind\n\nIn aeronautics, the relative wind is the direction of movement of the atmosphere relative to an aircraft or an airfoil. It is opposite to the direction of movement of the aircraft or airfoil relative to the atmosphere. Close to any point on the surface of an aircraft or airfoil, the air is moving parallel to the surface; but at a great distance from the aircraft or airfoil the movement of the air can be represented by a single vector. This vector is the relative wind or the \"free stream velocity vector\".\n\nThe angle between the chord line of an airfoil and the relative wind defines the angle of attack. The relative wind is of great importance to pilots because exceeding the critical angle of attack will result in a stall, regardless of airspeed.\n\nRelative wind is also used to describe the airflow relative to an object in freefall through an atmosphere, such as that of a person's body during the freefall portion of a skydive or BASE jump. In a normal skydive the vertical descent of the skydiver creates an upward relative wind. The relative wind strength increases with increased descent rate.\n\nThe relative wind is directly opposite to the direction of travel.\n\nTherefore, when a skydiver exits a forward-moving aircraft such as an aeroplane, the relative wind emanates from the direction the aeroplane is facing due to the skydiver's initial forward ( horizontal ) momentum. \nAs aerodynamic drag gradually overcomes this forward momentum and, simultaneously, gravity attracts the skydiver downward, the relative wind alters proportionally into an upward (vertical) direction. This creates an arc of travel for the skydiver similar to water flowing from a low pressure hose held horizontally and creates a variation in the angle of the relative wind from horizontal to vertical.\n\nWhen exiting from a forward-moving aircraft (as distinguished from a hovering aircraft, such as a balloon or a helicopter in hover mode) during a normal belly-to-earth skydive, the skydiver must arch his body in the direction of travel which is initially horizontal. If the skydiver continues to arch, his belly will gradually alter pitch until he is belly-to-earth. This section of the jump is commonly referred to as \"the hill\".\n\nRelative wind differs from the wind in meteorology in that the object (\"e.g..\", the skydiver) moves past the air, as opposed to the air moving past the object.\n"}
{"id": "32525207", "url": "https://en.wikipedia.org/wiki?curid=32525207", "title": "Richard Pough", "text": "Richard Pough\n\nRichard Pough (April 19, 1904 – June 24, 2003) was an American conservationist who served as the second president of The Nature Conservancy. He helped publicize and protect Hawk Mountain. He was instrumental in acquiring large tracts of land for the Open Space Institute. While at the American Museum of Natural History, he was responsible for the creation of the Hall of American Forests. He worked for the Audubon Society where he wrote several books about bird behavior.\n"}
{"id": "29716126", "url": "https://en.wikipedia.org/wiki?curid=29716126", "title": "Road-powered electric vehicle", "text": "Road-powered electric vehicle\n\nRoad powered electric vehicles (RPEV) (sometimes called roadway powered electric vehicles) collect any form of potential energy from the road surface to supply electricity to locomotive motors and ancillary equipment within the vehicle.\n\nA Road Powered electric vehicle may be defined as a transport capsule with the following characteristics:\n\n2) The electric motors are powered by an electrical supply provided either from a battery (usually on-board) or power source (typically remote, and connected directly via conductive cables or magnetic inductive fields\n\n(4) Where an external electric supply is provided, this is typically with overhead conductors or above-ground rails or (rarely) sub-surface electro-magnets.\n\n(5) An all-electric vehicle typically requires one or more on-board energy supply elements or devices for autonomous operation, most commonly by chemical batteries, less frequently via electro-mechanical generators\n\n(6) The electric vehicle further may include an on-board control system that directs energy from the energy storage elements, as needed, and converts such energy to electric motors used to propel the electric vehicle.\n\nA Road Powered Electric Vehicle therefore necessarily includes a system for supplying electricity from an external power source (such as a remote power station) through a network of power coupling elements which provide an integral and continuous power connection between origin and destination.\n\nIn USA Patent 6421600 these power coupling elements are electro-magnetic transmission coils embedded in the roadway and reception coils which are electrically resonant, so that they convert the magnetic flux above the road into electrical energy on the vehicle, which is entirely independent, since there is no mechanical or electrical link.\n\nThe USA Patent 6421600 version of RPEV cited also includes:\n\n(1) an on-board power meter to indicate the approximate consumption and an approximation of its remaining autonomous (battery) endurance or ranger range\n\n(2) a wide bandwidth communications channel to allow information signals to be sent to, and received from, the RPEV while it is in use;\n\n(3) automated garaging, a system that automatically connects power to the RPEV when it is in storage overnight, both to recharge the on-board batteries and power the on-board storage heater in cold weather so that it is comfortable for the driver and passengers.\n\n(4) electronic coupling between two or more RPEVs so that one can be a \"master\" (with a driver) and the following vehicle(s) are \"slave\" (controlled by the driver in the \"master\" vehicle) as a sort of \"road train\" in order to increase passenger or payload capacity.\n\n(5) a positioning system for determining the current location of the RPEV; and\n\n(6) a scheduling and dispatch computer so that many RPEV's can be accommodated efficiently on any particular length of road with embedded transmission coils. Such a system could be adapted for use with remote drivers or automatic control over closed roadway circuits such as warehouses and theme parks.\n\nOnline Electric Vehicles (OHVs) use pick-up equipment underneath the vehicle then collects power through non-contact magnetic induction, which may imply a similar electrical design.\n\nTrolleybuses use overhead cables which could also be used for cars, as shown in a photograph (right) carried by an encyclopedic magazine about 1910.\n\nThe twin overhead power lines are usually copper conductors with a flat surface, which are supported by a steel catenary wire. A pole or pantograph extends from the vehicle and presses 'slippers' or 'skates' against the conductors to complete the circuit. These slipper or skate contacts are frequently coated with graphite, or have a carbon block to reduce friction and ensure a good (low resistance) electrical connection.\n\nSince buses are scheduled, traffic conflicts are rare. If the overhead dual power line infrastructure was used randomly by private users, then some sort of management system would be required. The simplest would be to provide the vehicles some limited autonomy so that they could simply disconnect when arriving at each intersection and reconnect when they joined the next section. A more sophisticated traffic management system, as proposed by USA Patent 6421600 (above) could facilitate transit where there are many vehicles wishing to draw power from a particular section of the infrastructure.\n\nRecent improvements in trolleybus design include the automatic connection and disconnection of power poles either by the driver or in the event of mechanical dis-engagement from the overhead wires, so that the driver can remain in control, using on-board battery power. Better materials and motion dynamics modeling allow speeds up to about 80 km/h so that trolleybus behavior is flexible and totally comparable with regular autonomous vehicle performance, even at crossings and road-junctions\n\nGround-level power supply is an alternative to overhead power lines. The most common version is the conduit current collection system, where the power supply is carried in a channel or vault under the roadway. This is a tube, open at the top to allow a blade to enter and extend slippers against the power conductors, which are either side of the access opening slot, near the top of the channel. The arrangement is shaped so that live surfaces can not be reached by hand, and the lower part of the conduit provides drainage to remove any water.\n\nConduit power was first designed in the steam age (c.1900) when power was transmitted mechanically from a series of steam-powered rotating screws, as shown in the illustration (left) also published by Harmsworth Popular Science.\n\nA more recent roadway power supply design used by the Bordeaux tramway involves alternate (line and neutral) panels which are only powered whilst they are actually under the tram. This system is called APS (French: \"Alimentation par Sol\" which implies sub-surface power feed in English).\n\nA parking area with an overhead electrical net was in use at Seoul Grand Park Zoo in South Korea, to recharge electric vehicles , This was similar to a fairground bumper car arrangement which allowed vehicle batteries to be recharged, prior to introduction of the more modern OLEV Online Electric Vehicle system. The advantage was that vehicles could be parked randomly, as convenient, and there was no need for either the tripping hazard of trailing cables used by most electric vehicles nor for fiddly precise alignment of conductor poles needed by trolley buses.\n\nAn alternative form of electro-magnetic coupling is called a linear motor, first developed by Professor Eric Laithwaite at Imperial College London in the 1960s.\n\nInstead of supplying electric motors with power directly or charging on-board batteries, the RPEV is fitted with magnets which are impelled horizontally in the same way as the rotor on an ordinary electric motor is impelled to rotate. A linear motor works in exactly the same way a round motor which has been split to its centre, laid flat and extended.\n\nThe Shanghai Transrapid and Bombardier Advanced Rapid Transit systems are practical examples of this type of RPEV technology applied to rail transport systems (which are sometimes called railroads, especially in USA).\n\nMaglev uses magnetic repulsion to support the carriage weight as well as to propel the vehicle forward, so wheels are not needed. Maglev trains effectively ride on top of a magnetic wave, much as a surfer rides a wave on the sea.\n\n"}
{"id": "227049", "url": "https://en.wikipedia.org/wiki?curid=227049", "title": "Single-phase electric power", "text": "Single-phase electric power\n\nIn electrical engineering, single-phase electric power is the distribution of alternating current electric power using a system in which all the voltages of the supply vary in unison. Single-phase distribution is used when loads are mostly lighting and heating, with few large electric motors. A single-phase supply connected to an alternating current electric motor does not produce a revolving magnetic field; single-phase motors need additional circuits for starting (capacitor start motor), and such motors are uncommon above 10 kW in rating.\n\nBecause the voltage of a single phase system reaches a peak value twice in each cycle, the instantaneous power is not constant.\n\nA single-phase load may be powered from a three-phase distribution transformer in two ways: by connection between one phase and neutral or by connection between two phases. These two give different voltages from a given supply. For example, on a 120/208 three-phase system, which is common in North America, the phase-to-neutral voltage is 120 volts and the phase-to-phase voltage is 208 volts. This allows single-phase lighting to be connected phase-to-neutral and three-phase motors to be connected to all three phases. This eliminates the need of a separate single phase transformer.\n\nStandard frequencies of single-phase power systems are either 50 or 60 Hz. Special single-phase traction power networks may operate at 16.67 Hz or other frequencies to power electric railways.\n\nSingle-phase is commonly divided in half at the distribution transformer on the secondary winding to create split-phase electric power for household appliances and lighting.\n\nIn North America, individual residences and small commercial buildings with services up to about 100 kVA (417 amperes at 240 volts) will usually have three-wire single-phase distribution, especially in rural areas where motor loads are small and uncommon. In rural areas where no three-phase supply is available, farmers or households who wish to use three-phase motors may install a phase converter if only a single-phase supply is available. Larger consumers such as large buildings, shopping centers, factories, office blocks, and multiple-unit apartment blocks will have three-phase service. In densely populated areas of cities, network power distribution is used with many customers and many supply transformers connected to provide hundreds or thousands of kVA, a load concentrated over a few hundred square meters.\n\nHigh power systems, say hundreds of kVA or larger, are nearly always three phase. The largest supply normally available as single phase varies according to the standards of the electrical utility. In the United Kingdom a single-phase household supply may be rated 100 A or even 125 A, meaning that there is little need for three-phase in a domestic or small commercial environment. Much of the rest of Europe has traditionally had much smaller limits on the size of single phase supplies resulting in even houses being supplied with three-phase (in urban areas with three-phase supply networks).\n\nIf heating equipment designed for a 240-volt system is connected to two phases of a 208-volt supply, it will only produce 75% of its rated heating effect. Single-phase motors may have taps to allow their use on either 208-volt or 240-volt supply.\n\nSingle-phase power may be used for electric railways; the largest single-phase generator in the world, at Neckarwestheim Nuclear Power Plant, supplied a railway system on a dedicated traction power network.\n\nTypically a third conductor, called \"ground\" (or \"safety ground\") (U.S.) or \"protective earth\" (UK, Europe, IEC), is used as a protection against electric shock, and ordinarily only carries significant current when there is a circuit fault. Several different earthing systems are in use. In some extreme rural areas single-wire earth return distribution is used.\n\n"}
{"id": "3054859", "url": "https://en.wikipedia.org/wiki?curid=3054859", "title": "Stamina wood", "text": "Stamina wood\n\nStamina wood is an engineered wood product made of veneers that have been dyed, then impregnated with phenolic resin and compressed under high pressure and heat. The result is a material that can be worked with woodworking tools, then sanded and buffed to a high luster.\n"}
{"id": "37914029", "url": "https://en.wikipedia.org/wiki?curid=37914029", "title": "Stochastic Eulerian Lagrangian method", "text": "Stochastic Eulerian Lagrangian method\n\nIn computational fluid dynamics, the Stochastic Eulerian Lagrangian Method (SELM) is an approach to capture essential features of fluid-structure interactions subject to thermal fluctuations while introducing approximations which facilitate analysis and the development of tractable numerical methods. SELM is a hybrid approach utilizing an Eulerian description for the continuum hydrodynamic fields and a Lagrangian description for elastic structures. Thermal fluctuations are introduced through stochastic driving fields. \n\nThe SELM fluid-structure equations typically used are \n\nThe pressure \"p\" is determined by the incompressibility condition for the fluid \n\nThe formula_5 operators couple the Eulerian and Lagrangian degrees of freedom. The formula_6 denote the composite vectors of the full set of Lagrangian coordinates for the structures. The formula_7 is the potential energy for a configuration of the structures. The formula_8 are stochastic driving fields accounting for thermal fluctuations. The formula_9 are Lagrange multipliers imposing constraints, such as local rigid body deformations. To ensure that dissipation occurs only through the formula_10 coupling and not as a consequence of the interconversion by the operators formula_11 the following adjoint conditions are imposed\n\nThermal fluctuations are introduced through Gaussian random fields with mean zero and the covariance structure \n\nTo obtain simplified descriptions and efficient numerical methods, approximations in various limiting physical regimes have been considered to remove dynamics on small time-scales or inertial degrees of freedom. In different limiting regimes, the SELM framework can be related to the immersed boundary method, accelerated Stokesian dynamics, and arbitrary Lagrangian Eulerian method. The SELM approach has been shown to yield stochastic fluid-structure dynamics that are consistent with statistical mechanics. In particular, the SELM dynamics have been shown to satisfy detailed-balance for the Gibbs–Boltzmann ensemble. Different types of coupling operators have also been introduced allowing for descriptions of structures involving generalized coordinates and additional translational or rotational degrees of freedom.\n\n\n\n"}
{"id": "42511559", "url": "https://en.wikipedia.org/wiki?curid=42511559", "title": "Team boat", "text": "Team boat\n\nA team boat, horse boat, or horse ferry, is a watercraft powered by horses or mules, generally using a treadmill, which serves as a horse engine. Team boats were popular as ferries in the United States from the mid 1810s to the 1850s.\n\nThe first documented horse-powered boat in the United States was built on the Delaware River in 1791 by John Fitch.\nThere are three types of team boats. In one, four or five horses are placed in each side of the boat in a circular treadwheel, and the paddle wheels, arranged like the side wheel steamboat of later days were turned by means of cogs and gearing connected with other cogs on the shaft of the paddle wheels. The horses were hitched to strong timbers and by a forward movement of the feet caused the treadwheel upon which they stood to revolve and thus operate the gear wheels. \n\nAnother type of team boat uses a \"horse whim,\" a type of horse mill. It has a large revolving wheel in the middle, and a center post known as a \"whim\" (or horse capstan). The horses, which are attached to the horse whim, walk around in a circle, causing the wheel or capstan to revolve, which in turn rotates gears that rotate the paddles, or bucket wheels. The team boat of this style consisted of two complete hulls, united by a deck or bridge, but separated far enough apart to allow the paddle wheel to be set between them. They are sharp at both ends, and can be propelled backward or forward with equal ease.\n\nA third design for team boats was invented in 1819, by Barnabas Langdon. Langdon's turntable design permitted the horses to walk straight ahead instead of in circles. \"Langdon placed a rotating turntable slightly below the level of the boat's deck; horses stood atop the turntable through large slots in the deck and drove the wheel backward by walking in place. This design eased the burden on the horses, freed up valuable deck space, and allowed the ferry to be built atop one hull.\"\n\nOne description of a turntable type team boat using six horses says, \"The treadmills, on either side, were each trod by three horses always facing in the same direction. To reverse the paddlewheels it was only necessary to stop the horses a minute, and withdraw a drop pin that would reverse the gearing.\"\n\nThe \"Experiment\", built sometime around 1807-1810, was an early horse-powered ferry boat. It was a twelve-ton three-mast boat drawing a few feet of water, about 100 feet long by 20 feet beam. Its driving mechanism, an in-water screw, was invented by David Grieve in 1801. The boat was constructed by David Wilkinson (some sources give his name as \"Varnum\") in 1807 to 1810, depending on the source. It was propelled by a \"goose-foot paddle\" large mechanical screw propeller in the water (instead of a paddle wheel at water surface). The new technology devised by Grieve and Wilkinson was powered by eight horses on a treadmill. The horse boat technology to propel the boat upstream was originally invented by David Grieve and granted a patent February 24, 1801 in the patent category of \"Boats to ascend rivers\". The complete recorded patent was lost in the 1836 U.S. Patent Office fire. The novel idea of propelling vessels upstream by the use of a large mechanical screw in the water is now referred to as Ericsson’s propeller.\n\nOne of the first documented team boats in commercial service in the United States began running a Manhattan-Brooklyn route in 1814. Carrying vehicles, horses, and two hundred humans on a typical run, it could take anywhere from eight to eighteen minutes to finish the East River crossing. Team boats continued to serve New York City until 1824.\n\nTeam boat ferries were very popular. First, they were thought to be cheaper to operate than any other type of ferry boat, and second, they did not incur fees under the Fulton-Livingston patents monopoly. In ferry service, horses could be stabled on land, and there was no need to feed them on the boat, because the work was intermittent. \n\nThere were cases in which team boats replaced steam boats for reasons of economy. In 1812, two steam boats designed by Robert Fulton were placed in use in New York, for the Paulus Hook Ferry from the foot of Cortlandt Street, and on the Hoboken Ferry from the foot of Barclay Street. The \"Juliana\", running from Barclay Street, was withdrawn from service, as announced, in favor of the more convenient horse boat. It is almost certain, however, that this retrograde step was taken because of the monopoly enjoyed by Mssrs. Fulton and Livingston for the navigation of the waters of New York State by steam. In 1816, a steamboat company running ferry service between Halifax, Nova Scotia and Dartmouth had the law amended to permit the use of team boats instead.\n\nIn August 1816, the team boat \"Moses Rogers\" in Newburgh, New York began service to Fishkill, New York, carrying wagons, coaches, carriages, horses, and passengers.\nIn 1817, the \"Union Team Boat\" ran between Long Bridge at Georgetown and Alexandria, Virginia. In 1821, William Dyer built a team boat serving Portsmouth, Virginia on the Elizabeth River.\n\nIn 1838, \"Tremaine's Team Boat\", using three horses, operated a ferry service at Charlottetown, Prince Edward Island. Team boats with eight horses operated on the Ohio River at Cincinnati in 1819, and at Charleston, South Carolina, on the Ashley River in 1818 and 1827. The team boat crossing the Ohio could accommodate a stagecoach driving aboard. \n\nAttempts were made with moderate success to ascend the Ohio and Mississippi with teams of horses on board. In 1824 the team boat \"Genius of Georgia\" operated on the Savannah River, under Captain William Bird, carrying a cargo of sundries. An 1820 report by the South Carolina Department of Public Works described a five-man boat powered by eight mules; it carried 300 bales of cotton 250 miles in fifteen days at a cost of just $116.25. However, for through traffic, the team boats never passed the experimental stage.\n\nThe South Ferry horse ferry operating at Albany, New York in 1827 was replaced by a steamboat in 1828. The North Ferry horse ferry at Albany operated from 1831-1841.\n\nThe team boats on the Delaware River serving Camden, New Jersey stopped for an hour at lunch time to feed the horses. The \"Ridgeway\" was a double team boat, propelled by nine horses walking around a circle. She ran from the foot of Cooper Street. There was also a team boat named the \"Washington\"; she ran from Market Street, Camden, to Market Street, Philadelphia. Other team boats followed in succession, namely the \"Phoenix\", \"Constitution\", \"Moses Lancaster\", and \"Independence\". \"The Cooper's Ferry Daybook, 1819-1824\", documenting Camden's \"Point Pleasant Teamboat\", survives to this day. \n\nHorse powered ferries have also been documented in Wisconsin and New Hampshire.\n\nA shipwreck discovered in 1983 in Lake Champlain, the Burlington Bay Horse Ferry, is an example of a turntable team-boat. It served on one of approximately five horse ferry crossings operating on Lake Champlain from about 1820 to 1850. They reached their peak in the 1830s and 1840s, before their 1850s replacement by steamboats.\n\nIn the 1880s, in New Haven, Missouri and Waverly, Missouri, the \"Tilda Clara\" and \"General Harrison\" ferries across the Missouri River were powered by four horse teams. \n\nA ferry powered by horses and mules operated on the Mississippi River at St. Mary, Missouri as recently as 1910. The last known horse ferry remained in service until the late 1920s on the Tennessee River.\n\n\n\n"}
{"id": "22282547", "url": "https://en.wikipedia.org/wiki?curid=22282547", "title": "Thermoset polymer matrix", "text": "Thermoset polymer matrix\n\nA thermoset polymer matrix is a synthetic polymer reinforcement first developed for structural applications, such as glass-reinforced plastic radar domes on aircraft and graphite-epoxy payload bay doors on the space shuttle. In polymer matrix composites, polymers act as binder or matrix to secure in place incorporated particulates, fibres or other reinforcements.\n\nThey were first used after World War II, and continuing research has led to an increased range of thermoset resins, polymers or plastics, as well as engineering grade thermoplastics, all developed for use in the manufacture of polymer composites with enhanced and longer-term service capabilities. Thermoset polymer matrix technologies also find use in a wide diversity of non-structural industrial applications.\n\nThe foremost types of thermosetting polymers used in structural composites are benzoxazine resins, bis-Maleimide resins (BMI), cyanate ester resins, epoxy (epoxide) resins, phenolic (PF) resins, unsaturated polyester (UP) resins, polyimides, polyurethane (PUR) resins, silicones, and vinyl esters.\n\nThese are made by the reaction of phenols, formaldehyde and primary amines which at elevated temperatures (400 °F (200 °C)) undergo ring–opening polymerisation forming polybenzoxazine thermoset networks; when hybridised with epoxy and phenolic resins the resulting ternary systems have glass transition temperatures in excess of 490 °F (250 °C). \n\nCure is characterised by expansion rather than shrinkage and uses include structural prepregs, liquid molding and film adhesives for composite construction, bonding and repair. The high aromatic content of the high molecular weight polymers provides enhanced mechanical and flammability performance compared to epoxy and phenolic resins.\n\nFormed by the condensation reaction of a diamine with maleic anhydride, and processed basically like epoxy resins ( cure). After an elevated post-cure (), they will exhibit superior properties. These properties are influenced by a 400-450 °F (204-232 °C) continuous use temperature and a glass transition of . \n\nThis thermoset polymer type is merged into composites as a prepreg matrix used in electrical printed circuit boards, and for large scale structural aircraft – aerospace composite structures, etc. It is also used as a coating material and as the matrix of glass reinforced pipes, particularly in high temperature and chemical environments.\n\nThe reaction of bisphenols or multifunctional phenol novolac resins with cyanogen bromide or chloride leads to cyanate functional monomers which can be converted in a controlled manner into cyanate ester functional prepolymer resins by chain extension or copolymerization. When postcured, all residual cyanate ester functionality polymerises by cyclotrimerisation leading to tightly crosslinked polycyanurate networks with high thermal stability and glass transition temperatures up to 752 °F (400 °C) and wet heat stability up to around 400 °F (200 °C).\n\nCyanate ester resin prepregs combine the high temperature stability of polyimides with the flame and fire resistance of phenolics and are used in the manufacture of aerospace structural composite components which meet fire protection regulations concerning flammability, smoke density and toxicity. Other uses include film adhesives, surfacing films and 3D printing.\n\nEpoxy resins are thermosetting prepolymers made either by the reaction of epichlorohydrin with hydroxyl functional aromatics, cycloaliphatics and aliphatics or amine functional aromatics, or by the oxidation of unsaturated cycloaliphatics. The diglycidyl ethers of bisphenol-A (DGEBA) and bisphenol-F (DGEBF) are the most widely used due to their characteristic high adhesion, mechanical strength, heat and corrosion resistance. Epoxide functional resins and prepolymers cure by polyaddition/copolymerisation or homopolymerisation depending on the selection of crosslinker, hardener, curing agent or catalyst as well as by the temperature.\n\nEpoxy resin is used widely in numerous formulations and forms in the aircraft-aerospace industry. It is regarded as \"the work-horse of modern day composites\". In recent years, the epoxy formulations used in composite prepregs have been fine-tuned to improve their toughness, impact strength and moisture absorption resistance. Maximum properties have been realized for this polymer.\n\nThis is not only used in aircraft-aerospace demand. It is used in military and commercial applications and is also used in construction. Epoxy-reinforced concrete and glass-reinforced and carbon-reinforced epoxy structures are used in building and bridge structures.\n\nEpoxy composites have the following properties:\n\n\nEpoxy Phenol Novolac (EPN) and Epoxy Cresol Novolac (ECN) resins made by reacting epichlorohydrin with multifunctional phenol novolac or cresol novolac resins have more reactive sites compared to DGEBF epoxy resins and on cure result in higher crosslink density thermosets. They are used in printed wire/circuit board laminating and also for electrical encapsulation, adhesive and coatings for metal where there is a need to provide protection from corrosion, erosion or chemical attack at high continuous operating temperatures. \n\nThere are two types of phenolic resins - novolacs and resoles. Novolacs are made with acid catalysts and a molar ratio of formaldehyde to phenol of less than one to give methylene linked phenolic oligomers; resoles are made with alkali catalysts and a molar ratio of formaldehyde to phenol of greater than one to give phenolic oligomers with methylene and benzylic ether-linked phenol units. \n\nPhenolic resins, originally developed in the late 19th century and, regarded as the first truly synthetic polymer types, are often referred to as the “work-horse of thermosetting resins”. They are characterised by high bonding strength, dimensional stability and creep resistance at elevated temperatures, and frequently combined with co-curing resins such as epoxies.\n\nGeneral purpose molding compounds, engineering molding compounds and sheet molding compounds are the primary forms of phenolic composites. Phenolics are also used as the matrix binder with Honeycomb core. Phenolics find use in many electrical applications such as breaker boxes, brake lining materials and most recently in combination with various reinforcements in the molding of an engine block-head assembly, called the polimotor. Phenolics may be processed by the various common techniques, including compression, transfer and injection molding.\n\nProperties of phenolic composites have the following properties:\n\n\nUnsaturated polyester resins are an extremely versatile, and fairly inexpensive class of thermosetting polymer formed by the polycondensation of glycol mixtures containing propylene glycol, with a dibasic acid and anhydrides usually maleic anhydride to provide backbone unsaturation needed for crosslinking, and orthophthalic anhydride, isophthalic acid or terephthalic acid where superior structural and corrosion resistance properties are required. Polyester resins are routinely diluted/dissolved in a vinyl functional monomer such as styrene and include an inhibitor to stabilize the resin for storage purposes. Polymerisation in service is initiated by free radicals generated from ionizing radiation or by the photolytic or thermal decomposition of a radical initiator. Organic peroxides, such as methyl ethyl ketone peroxide and auxiliary accelerators which promote decomposition to form radicals are combined with the resin to initiate a room temperature cure. \n\nIn the liquid state, unsaturated polyester resins may be processed by numerous methods, including Hand Layup, vacuum bag molding, and spray-up and compression molded Sheet Molding Compound (SMC). They can also be B-staged after application to chopped reinforcement and continuous reinforcement, to form pre-pregs. Solid molding compounds in the form of pellets or granules are also used in processes such as compression and transfer molding.\n\nThere are two types of commercial polyimides: thermosetting cross-linkable polyimides made by the condensation of aromatic diamines with aromatic dianhydride derivatives and anhydrides with unsaturated sites that facilitate addition polymerisation between preformed imide monomers and oligomers, and thermoplastic polyimides formed by the condensation reaction between aromatic diamines and aromatic dianhydrides. Thermoset polyimides are the most advanced of all thermoset polymer matrices with characteristics of high temperature physical and mechanical properties and are available commercially as resin, prepreg, stock shapes, thin sheets/films, laminates, and machined parts. Along with the high temperature properties, this thermoset polymer type must be processed at very high temperatures and relative pressure to produce optimum characteristics. With prepreg materials, to temperatures and pressures are required. The entire cure profiles are inherently long as there are a number of intermediate temperatures dwells, duration of which are dependent on part size and thickness. \n\nThe cut of polyimides is , highest of all thermosets, with short term exposure capabilities of . Normal operating temperatures range from cryogenic to .\n\nPolyimide composites have the following properties:\n\nPolyimide film possesses a unique combination of properties that make it ideal for a variety of applications in many different industries especially as excellent physical, electrical, and mechanical properties are maintained over a wide temperature range.\n\nHigh-performance polyimide resin is used in electrical, wear resistant and as structural materials when combined with reinforcement for aircraft-aerospace applications, which are replacing heavier more expensive metals. High temperature processing causes some technical problems as well as higher costs compared to other polymers. Hysols PMR series is an example of this polymer.\n\nThermoset polyurethane prepolymers with carbamate (-NH-CO-O-) links are linear and elastomeric if formed by combining diisocyanates (OCN-R1-NCO) with long chain diols (HO-R2-OH), or crosslinked and rigid if formed from combinations of polyisocyanates and, polyols. They can be solid or have an open cellular structure if foamed, and are widely used for their characteristic high adhesion and resistance to fatigue. Polyurethane foam structural cores combined with glass-reinforced or graphite-reinforced composite laminates are used to make lightweight, strong, sandwich structures. All forms of the material, inclusive of flexible and rigid foams, foam moldings, solid elastomeric moldings and extrudates, when combined with various reinforcement–fillers have found commercial applications in thermoset polymer matrix composites.\n\nThey differ from polyureas which are thermoset elastomeric polymers with carbamide (-NH-CO-NH-) links made by combining diisocyanate monomers or prepolymers (OCN-R-NCO) with blends of long-chain amine-terminated polyether or polyester resins (H2N-RL-NH2) and short-chain diamine extenders (H2N-RS-NH2). Polyureas are characterised by near instantaneous cure, high mechanical strength and resistance to corrosion so are widely used for 1:1 volume mix ratio spray applied, abrasion resistant waterproofing protective coating and lining.\n\nSilicone resins are partly organic in nature with a backbone polymer structure made of alternating silicon and oxygen atoms rather than the familiar carbon-to-carbon backbone characteristics of organic polymers. In addition to having at least one oxygen atom bonded to each silicon atom, silicone resins have direct bonds to carbon and therefore also known as polyorganosiloxanes. They have the general formula (R2SiO)n and the physical form (liquid, gel, elastomer or solid) and use varies with molecular weight, structure (linear, branched, caged) and nature of substituent groups (R = alkyl, aryl, H, OH, alkoxy). Aryl substituted silicone resins have greater thermal stability than alkyl substituted silicone resins when polymerised (condensation cure mechanism) at temperatures between ~300 °F (~150 °C) and ~400 °F (~200 °C). Heating above ~600 °F (~ 300 °C) converts all silicone polymers into ceramics since all organic constituents pyrolytically decompose leaving crystalline silicate polymers with the general formula (-SiO2-)n. In addition to applications as ceramic matrix composite precursors, silicone resins in the form of polysiloxane polymers made from silicone resins with pendant acrylate, vinyl ether or epoxy functionality find application as UV, electron beam and thermoset polymer matrix composites where they are characterised by their resistance to oxidation, heat and ultraviolet degradation.\n\nAssorted other uses in the general area of composites for silicones include sealants, coating materials, and as a reusable bag material for vacuum-bag curing of composite parts.\n\nVinyl ester resins made by addition reactions between an epoxy resin with acrylic acid derivatives, when diluted/dissolved in a vinyl functional monomer such as styrene, polymerise. The resulting thermosets are notable for their high adhesion, heat resistance and corrosion resistance. They are stronger than polyesters and more resistant to impact than epoxies. Vinyl ester resins are used for wet lay-up laminating, SMC and BMC in the manufacture and repair of corrosion and heat resistant components ranging from pipelines, vessels and buildings to transportation, marine, military and aerospace applications. \n\nAmino resins are another class of thermoset prepolymers formed by copolymerisation of amines or amides with an aldehyde. Urea-formaldehyde and melamine-formaldehyde resins, although not widely used in high performance structural composite applications, are characteristically used as the polymer matrix in molding and extrusion compounds where some use of fillers and reinforcements occurs. Urea-formaldehyde resins are widely used as the matrix binder in construction utility products such as particle board, wafer board, and plywood, which are true particulate and laminar composite structures. Melamine-formaldehyde resins are used for plastic laminating. \n\nFuran resin prepolymers made from furfuryl alcohol, or by modification of furfural with phenol, formaldehyde, urea or other extenders, are similar to amino and phenolic thermosetting resins in that cure involves polycondensation and release of water as well as heat. While they are generally cured under the influence of heat, catalysts and pressure, furan resins can also be formulated as dual-component no-bake acid-hardened systems which are characterised by high resistance to heat, acids and alkalies. Furan resins are of increasing interest for the manufacture of sustainable composites - biocomposites made from a bio-derived matrix (in this case furan resin), or biofibre reinforcement, or both.\n\n\n\n"}
{"id": "5456269", "url": "https://en.wikipedia.org/wiki?curid=5456269", "title": "Titanium(II) oxide", "text": "Titanium(II) oxide\n\nTitanium(II) oxide (TiO) is an inorganic chemical compound of titanium and oxygen. It can be prepared from titanium dioxide and titanium metal at 1500 °C. It is non-stoichiometric in a range TiO to TiO and this is caused by vacancies of either Ti or O in the defect rock salt structure. In pure TiO 15% of both Ti and O sites are vacant. Careful annealing can cause ordering of the vacancies producing a monoclinic form which has 5 TiO units in the primitive cell that exhibits lower resistivity. A high temperature form with titanium atoms with trigonal prismatic coordination is also known. Acid solutions of TiO are stable for a short time then decompose to give hydrogen:\n\nGas-phase TiO shows strong bands in the optical spectra of cool (M-type) stars. In 2017, TiO was detected in an exoplanet atmosphere for the first time. Additionally, evidence has been obtained for the presence of the diatomic molecule TiO in the interstellar medium.\n"}
{"id": "1278124", "url": "https://en.wikipedia.org/wiki?curid=1278124", "title": "Toyota Sequoia", "text": "Toyota Sequoia\n\nThe is a full-size SUV manufactured by Toyota and derived from its Tundra pickup truck.\n\nIntroduced in 2000 and manufactured at Toyota Motor Manufacturing Indiana in Princeton, Indiana, the Sequoia is the first vehicle from a Japanese marque in the popular mainstream full-sized SUV class in North America, and initial planning done by first-generation Sequoia chief engineer Kaoru Hosegawa aimed the Sequoia directly at the Ford Expedition, also competing with the Chevrolet Tahoe and the later Nissan Armada.\n\nThe Sequoia slots in between the mid-size Toyota 4Runner and the premium Toyota Land Cruiser in the North American Toyota SUV lineup, and is the largest SUV currently being produced under the Toyota brand.\n\nIn 2015 the Sequoia was available in the United States (All regions), Bolivia, Canada, Costa Rica, Honduras, Guatemala, Chile, Mexico, Bahrain, Kuwait, Lebanon, Oman, Qatar, Saudi Arabia, the United Arab Emirates, and Yemen in LHD only. \nAs of 2017, the Sequoia is sold in the United States (All regions), Canada, Costa Rica, Mexico, Bahrain, Kuwait, Lebanon, Oman, Qatar, Saudi Arabia, and Yemen in LHD only.\n\nDevelopment of a full-size SUV alongside a T100 replacement began in the mid-1990s, with a design freeze in 1997 (styled by Toshihiko Shirasawa) and design patent filing of the production design on April 4, 1998 at the Japan Patent Office (JPO) under #1054583. After the introduction of the Toyota Tundra in 1999, speculation started that Toyota intended to compete in the full-size market with a Tundra-based SUV called the Highlander. However, the Highlander name was used on a midsize Camry-based crossover and the Tundra-based SUV was introduced on January 11, 2000 at the North American Auto Show as the Toyota Sequoia, with full production starting in September 2000 for the 2001 model year.\n\nThe engine, dashboard, sheetmetal, and chassis are shared with the Tundra, with the exception of rear disc brakes and a more sophisticated multi-link live axle rear suspension. The Sequoia was nominated for the \"North American Truck of the Year\" award in 2001. When the Sequoia was introduced, it was slightly longer than the contemporary Land Cruiser, larger than the Chevrolet Tahoe in most dimensions and similar in size to the Ford Expedition; its V8 engine was certified as an Ultra Low Emission Vehicle. Frame assemblies and driveshafts are produced by Dana Holding Corporation.\n\nThe Sequoia came in two trim levels: \"SR5\" and \"Limited\". The base \"SR5\" started at $32,820 while the more expensive \"Limited\" started at $41,855. It was sold in both two-wheel drive and four-wheel drive versions. Vehicle Stability Control was standard on all models.\n\nFor the 2005 model year, the Sequoia received a mild facelift. A new engine equipped with VVT-i was new for 2005, as well as a 5-speed automatic transmission, replacing the previous 4-speed. 4 wheel drive models got a Torsen center differential, replacing the previous open differential, that splits power in full-time mode 40% front and 60% rear under normal driving, and can send up to 53% to the front and 71% to the rear during slip. The grill was redesigned, and the orange lamps were removed from the taillights.\n\nTowing Capacity for the 2005 model year:\n\nToyota unveiled the 2008 Toyota Sequoia at the November 2007 Los Angeles Auto Show, with sales beginning that following December. Like the original Sequoia, the new model is based on the new Tundra. However major differences with the Tundra include a fully boxed frame, a rear independent suspension featuring double wishbones with coil springs for improved ride comfort and room, and a locking center differential on 4-wheel drive models. The new suspension helps give the Sequoia a tighter turning radius of and allows for a fold-flat rear seat. Toyota stated the new frame is 70 percent more resistant to bending flex with torsional rigidity up 30 percent However, the new model weighs more than the previous Sequoia. The drag coefficient has been reduced to 0.35.\n\nImprovements include an optional ULEV-II compliant 381 horsepower 5.7 L \"3UR-FE\" V8, mated to a 6-speed automatic.\n\nThe 2008 Sequoia comes in three trim lines: the SR5 and Limited, and new Platinum. Pricing ranges from about $34,000 to $55,000 depending upon the trim line and configuration.\nThe base engine is the previous ULEV compliant 4.7 L \"2UZ-FE\" V8 featured from the previous generation. The 4.7 L is standard on the SR5, while the Limited and Platinum models come standard with a 5.7 L V8 engine. Four-wheel drive is available on all models.\n\nThe interior of the 2008 Sequoia features the same dash as the new Tundra. Standard features include a tilt and telescopic steering wheel, power windows and doors, dual sun visors, and a keyless entry. Options include DVD based navigation with backup camera and 7\" screen, a rear DVD entertainment system, a 14-speaker JBL audio system, and heated seats with ventilated coolers in the front row and warmers in the second row, available in Platinum trim. The Limited trim includes audio, climate, and hands-free Bluetooth mobile phone system controls, an improved JBL audio system, electroluminescent Optitron gauges, and an electrochromic auto-dimming rear-view mirror and side view mirrors with a HomeLink transceiver. The Platinum model includes a standard DVD navigation with a backup camera, a rear air suspension which can lower for easy loading, and Dynamic Laser Cruise Control.\n\nSeating arrangements are for seven or eight passengers, eight for SR5 and Limited models and seven for Platinum models. Power folding 60/40 split third row seats are available on the Platinum. The Sequoia has a maximum towing capacity of with the 5.7 L V8 in 2WD SR5 form or in the 4WD SR5 trim. For complete trailer & weight capacities, see Toyota's website.\n\nExterior differences include door handle colors (color-keyed for the SR5; chrome for the Limited and Platinum), diamond-cut aluminum alloy wheels for the Platinum trim, and varying power-heated remote-controlled side mirrors.\n\nStandard safety features include Vehicle Stability Control, traction control, anti-lock brakes brake assist, electronic brakeforce distribution, front side torso airbags and roll-sensing side curtain airbags for all three rows. For the 2010 model knee airbags were added as standard feature for front driver and passenger safety.\n\nIn 2015, the Insurance Institute for Highway Safety (IIHS) found the Sequoia 4WD to have the lowest overall driver death rate in its class with 0 deaths per million registered vehicle years.\n\n\nOn February 9, 2017, Toyota unveiled a refreshed Sequoia for the 2018 model year, featuring new standard LED headlights, daytime running lights and foglights, (note: 2001-07 models have this system too, but not the basic model), and three new exterior colors: Midnight Black Metallic, Shoreline Blue Pearl and Toasted Walnut Pearl. Each trim level also gets its own specific grille design. It also added a TRD level trim to its returning SR5, Limited, and Platinum trims, an upgraded instrumentation panel, automotive emergency braking and the Toyota Safety Sense feature.\n\n\nThe 2008-2010 Sequoias were part of one of the 2009-2010 Toyota recalls that required the installation of a small shim to relieve unwanted friction and restore fluidity to the accelerator pedal. Early 2003 Sequoias were recalled to reprogram the Vehicle Stability Control system due to the possibility of poor acceleration at certain low speeds. The 2002-3 Sequoia model years also had a defect in the front ball joints. 2004 Toyota Sequoias were subject to the Takata Airbag recall done for free by Toyota dealerships. Recently, the Sequoia, along with the Tacoma and Tundra, have another recall due to the frame prematurely rusting. If there is a 10 MM perforation found on the frame, the vehicle frame will be replaced, if the service is done within 12 years of vehicle purchase or 1 year after recall became official. For those vehicles without 10 MM perforation and located in a cold weather state, Toyota will perform an undercoating service to prevent future rust from forming.\n\n"}
{"id": "31064188", "url": "https://en.wikipedia.org/wiki?curid=31064188", "title": "Unterhaching Power station", "text": "Unterhaching Power station\n\nUnterhaching Power Station is an Electrical Facility in Germany with a rated 20MWh heat output. The facility outputs electrical energy rather than thermal heat.\n"}
{"id": "3672150", "url": "https://en.wikipedia.org/wiki?curid=3672150", "title": "Vacuum evaporation", "text": "Vacuum evaporation\n\nVacuum evaporation is the process of causing the pressure in a liquid-filled container to be reduced below the vapor pressure of the liquid, causing the liquid to evaporate at a lower temperature than normal. Although the process can be applied to any type of liquid at any vapor pressure, it is generally used to describe the boiling of water by lowering the container's internal pressure below standard atmospheric pressure and causing the water to boil at room temperature.\n\nThe vacuum evaporation treatment process consists of reducing the interior pressure of the evaporation chamber below atmospheric pressure. This reduces the boiling point of the liquid to be evaporated, thereby reducing or eliminating the need for heat in both the boiling and condensation processes. In addition, there are other technical advantages such as the ability to distill other liquids with high boiling points and avoiding the decomposition of substances that are sensitive to temperature, etc.\n\nWhen the process is applied to food and the water is evaporated and removed, the food can be stored for long periods of time without spoiling. It is also used when boiling a substance at normal temperatures would chemically change the consistency of the product, such as egg whites coagulating when attempting to dehydrate the albumen into a powder.\n\nThis process was invented by Henri Nestlé in 1866, of Nestlé Chocolate fame, although the Shakers were already using a vacuum pan earlier than that (see condensed milk).\n\nThis process is used industrially to make such food products as evaporated milk for milk chocolate, and tomato paste for ketchup.\nIn the sugar industry vacuum evaporation is used in the crystallization of sucrose solutions. Traditionally, this process was performed in batch mode, but nowadays continuous vacuum pans are available.\n\nVacuum evaporators are used in a wide range of industrial sectors to treat industrial wastewater. It represents a clean, safe and very versatile technology having low management costs, which in most cases serves as a zero-discharge treatment system.\n\nVacuum evaporation is also a form of physical vapor deposition used in the semiconductor, microelectronics, and optical industries and in this context is a process of depositing thin films of material onto surfaces. Such a technique consists of pumping a vacuum chamber to pressures of less than 10 torr and heating a material to produce a flux of vapor in order to deposit the material onto a surface. The material to be vaporized is typically heated until its vapor pressure is high enough to produce a flux of several Angstroms per second by using an electrically resistive heater or bombardment by a high voltage beam.\n\n"}
{"id": "52680394", "url": "https://en.wikipedia.org/wiki?curid=52680394", "title": "Wet rot", "text": "Wet rot\n\nWet rot is a generic term used to define a variety of fungal species, such as \"Coniophora puteana\" (otherwise known as cellar fungus). Wet rot fungi obtain their food by breaking down the cell walls of wood cells resulting in a loss of strength of the wood. This can cause problems in the structural integrity of structures.\n\n"}
{"id": "15608498", "url": "https://en.wikipedia.org/wiki?curid=15608498", "title": "Wood branding", "text": "Wood branding\n\nBranding is an easy and economical way to permanently mark anything that burns (wood, leather, plastic, rubber, cork, food, soap, wax, etc.). This is done by either a fire-heated tool or an electrically-heated tool, which is pressed onto the object, leaving behind a mark with the shape of the impressed tool.\n\n"}
