{"id": "55794217", "url": "https://en.wikipedia.org/wiki?curid=55794217", "title": "Arraglen Ogham Stone", "text": "Arraglen Ogham Stone\n\nThe Arraglen Ogham Stone is an ogham stone (CIIC 145) and a National Monument located in County Kerry, Ireland.\n\nArraglen ogham stone is located in a saddle between Mount Brandon and Masatiompan.\n\nThis stone was erected as a grave marker, with inscription in Primitive Irish, some time in c. AD 550–600.\n\nThe stone is sandstone, 191 × 38 × 20 cm. The inscription reads (\"of the priest [\"cruimther\"] Rónán son of Comgán\"). It contains a circled cross.\n"}
{"id": "21294703", "url": "https://en.wikipedia.org/wiki?curid=21294703", "title": "Ashok Sinha", "text": "Ashok Sinha\n\nAshok Sinha (born 8 November 1964) is a British environmental campaigner.\n\nSinha studied physics at the University of Bristol and completed his Ph.D. in renewable energy at Cambridge. Following this he spent a number of years pursuing research into climate change science at Reading University and Imperial College, producing a variety of publications on climate feedback process. He then moved into policy analysis with Forum for the Future working on climate change-renewable energy policy proposals.\n\nAshok Sinha was one of the group of UK NGO activists who founded the UK Make Poverty History campaign, serving on its governing body (and chairing its Policy & Lobbying Group) which he did whilst he was leading the Jubilee Debt Campaign.\n\nIn 2005 he became Director of the newly founded Stop Climate Chaos coalition (SCC), now called the Climate Coalition. SCC gained a high-profile with its I Count campaign (winner of two International Green Awards, 2007), which was the UK's campaign partner with the UK Live Earth event. Stop Climate Chaos was also instrumental in helping to secure the UK's Climate Change Act, helping to put a brake on the building of new unabated coal-fired power stations, and for delivering The Wave (ahead of the UN climate summit in Copenhagen) which was at the time the biggest single climate change campaign event ever held globally.\n\nIn a voluntary capacity Sinha has also been a Board member (and Vice Chair) of Amnesty International UK and of the London Cycling Campaign.\n\nHe was listed as one of the UK's top 100 Ethical Heroes by New Consumer magazine in 2007, one of the UK's top 100 environmentalists by the Independent on Sunday in 2008, and has been listed annually as one of London's 1000 most influential people by the Evening Standard between 2012 and 2016.\n\nAshok Sinha is currently Chief Executive of the London Cycling Campaign, leading the sustainable transport charity's successful Love London, Go Dutch and Space for Cycling campaigns. He is also currently the Chair of the London Sustainable Development Commission and a trustee of the Creekside Education Trust.\n\n"}
{"id": "44126781", "url": "https://en.wikipedia.org/wiki?curid=44126781", "title": "Automated fueling", "text": "Automated fueling\n\nAutomated fueling or robotic fueling involves the use of automation to remove human labor from the fueling process. The fueling is performed by a robotic arm, which opens the car's flap, unscrews the cap, picks up the fuel nozzle and inserts it into the tank opening. It requires the contours and dimensions of the fuel cap to be present in the database.\n\n"}
{"id": "14808238", "url": "https://en.wikipedia.org/wiki?curid=14808238", "title": "Availability-based tariff", "text": "Availability-based tariff\n\nAvailability Based Tariff (ABT) is a frequency based pricing mechanism applicable in India for unscheduled electric power transactions. The ABT falls under electricity market mechanisms to charge and regulate power to achieve short term and long term network stability as well as incentives and dis-incentives to grid participants against deviations in committed supplies as the case may be.\n\nABT Mechanism in Electricity sector in India is adopted since the year 2000 and in a few other countries for pricing bulk power across various stakeholders. ABT concerns itself with the tariff structure for bulk power and is aimed at bringing about more responsibility and accountability in power generation and consumption through a scheme of incentives and disincentives. As per the notification, ABT was initially made applicable to only central generating stations having more than one SEB/State/Union Territory as its beneficiary. Through this scheme, the Central Electricity Regulatory Commission (CERC) looks forward to improve the quality of power and curtail the following disruptive trends in power sector:\n\n\nThe ABT scheme has now been expanded to cover the Intrastate systems as well. The power generation or grid capacity has increased substantially in last fifteen years particularly after the Electricity Act 2003 by introduction of competition and unbundling of vertically integrated utilities (SEBs) into separate entities in charge of electricity generation, electricity transmission, and electricity distribution. Deregulation and competition has facilitated participation of private sector on large scale in electricity generation, transmission and distribution. Of late, Indian electricity sector is transforming from perennial deficit to surplus electricity availability. The volume of purchased electricity that could not be transmitted to the buyers due to transmission lines congestion is only 0.3% of the total electricity consumed in the financial year 2013-14. It means that the actual power deficit in India is less than 1% excluding under priced electricity demand. ABT/DSM mechanism needs improvements to address the requirements of all stake holders (including final electricity consumers) for encouraging least cost electricity generation / tariff based on demand verses availability in the grid. There is a need of well represented Electric Reliability Organization to involve all the grid participants for framing guidelines for power system operation and accreditation which is presently looked after by the CEA\n\nBulk power purchasers can buy electricity on daily basis for short, medium and long term duration from reverse e-auction facility. In reverse e-auction, availability based tariff /Deviation Settlement Mechanism (DSM) is applied to settle the failed commitments by the electricity sellers or buyers The electricity prices transacted under reverse e-auction facility are far less than the prices agreed under bilateral agreements.\n\nFor those power generators who have made power purchase agreements (PPA) with Discoms and need not participate in day ahead market (DAM) trading on daily basis, the pecking order among the power generators in a state is called merit order power generation where the lesser variable generation cost electricity producer is selected out of the available generators to maintain the normal grid frequency.\n\nAvailability, for the purpose of the ABT order means the readiness of the generating station to deliver ex-bus output expressed as a percentage of its ex-bus rated capacity (MCR). Electricity is a commodity whose cost of storing is more than its production cost. The most economical method for electricity generation, transmission and distribution is just in time production where the availability and reliability of the entire system shall be very high to meet the unpredictable electricity demand on minute to minute basis.\n\nAvailability of thermal generating station for any period shall be the percentage ratio of average Sent Out Capability (SOC) for all the time blocks during that period and the rated MCR / SOC of generating station. The SAIDI (System Average Interruption Duration Index) is commonly used as a reliability indicator by electric power utilities.\nDuring the fiscal year 2014-15, 1,043 billion KWh of electricity (three times that of National Grid) was supplied and met 138,215 MW maximum peak load. The total installed generation capacity is 267,637 MW at the end of fiscal year 2014–15. Its size is of global scale comparable only with EU grid, NERC grid, China electricity grid and Russian electricity grid. However, Indian grid lacks the basic features of smart grid for optimum use of its deployed resources.\n\nGenerally the top 10% of the unrestricted daily peak load (MW) persists only for 1% (15 minutes) of the total duration and its energy share (MWHr) is of the order of 0.2% of the daily energy supplied. Instead of generating this substantial extra power for a short duration, automatic selective load shedding can be implemented on bulk consumers with standby power facility to eliminate the load spikes without inconvenience to most of the consumers. Alternatively, working captive power plants start feeding to the grid by giving break to the captive power supply up to a maximum of 30 minutes duration. The standby generator / captive power owner would be paid for providing grid reserve ancillary services.\n\nWith an installed capacity of proper mix of base load and variable load generation capability (excluding low capacity utilisation or secondary power or negative load type on daily basis such as solar, wind, etc without storage) equivalent to the unrestricted annual maximum peak load, the most effective and economical smart grid shall be able to cater more than 99 percentile duration unrestricted load/demand on daily basis with 100% stable operation of the grid. The purpose of smart grid is to supply required electricity at optimum cost with reliability to the final consumers.\n\n\n\n\n\nThe Petroleum and Natural Gas Regulatory Board (PNGRB) was created in the year 2005 to regulate downstream activities in the petroleum and natural gas sector. There is requirement of natural gas TSO also for imparting optimum use of the gas on hourly basis for meeting peak load in the electricity grid and minimize gas transport distances. The available gas should be stored up to rated pressure in the pipe grid for generating power during peak demand hours on a daily (or more) basis. Thus available limited gas quantity is used to meet the peak electricity loads by all gas based power stations. Also gas should not be transported from a power deficit region to a power surplus region by the gas grid and generated power from gas there shall not be transmitted to the power deficit region to avoid misuse of the gas and power grid infrastructure. Natural gas TSO would also serve other sectors such as petro-chemical plants, CNG, Fertilizer plants, PNG, LNG, etc. as per their hourly requirement in addition to receive gas from various types of natural gas producers.\n\n\n\n\n"}
{"id": "20556915", "url": "https://en.wikipedia.org/wiki?curid=20556915", "title": "Boson", "text": "Boson\n\nIn quantum mechanics, a boson (, ) is a particle that follows Bose–Einstein statistics. Bosons make up one of the two classes of particles, the other being fermions. The name boson was coined by Paul Dirac to commemorate the contribution of Indian physicist and professor of physics at University of Calcutta and at University of Dhaka, Satyendra Nath Bose in developing, with Albert Einstein, Bose–Einstein statistics—which theorizes the characteristics of elementary particles.\n\nExamples of bosons include fundamental particles such as photons, gluons, and W and Z bosons (the four force-carrying gauge bosons of the Standard Model), the recently discovered Higgs boson, and the hypothetical graviton of quantum gravity. Some composite particles are also bosons, such as mesons and stable nuclei of even mass number such as deuterium (with one proton and one neutron, atomic mass number = 2), helium-4, or lead-208; as well as some quasiparticles (e.g. Cooper pairs, plasmons, and phonons).\n\nAn important characteristic of bosons is that their statistics do not restrict the number of them that occupy the same quantum state. This property is exemplified by helium-4 when it is cooled to become a superfluid. Unlike bosons, two identical fermions cannot occupy the same quantum space. Whereas the elementary particles that make up matter (i.e. leptons and quarks) are fermions, the elementary bosons are force carriers that function as the 'glue' holding matter together. This property holds for all particles with integer spin (s = 0, 1, 2, etc.) as a consequence of the spin–statistics theorem.\nWhen a gas of Bose particles is cooled down to temperatures very close to absolute zero, then the kinetic energy of the particles decreases to a negligible amount, and they condense into the lowest energy level state. This state is called a Bose-Einstein condensate. It is believed that this property is the explanation of superfluidity.\n\nBosons may be either elementary, like photons, or composite, like mesons.\n\nWhile most bosons are composite particles, in the Standard Model of Particle Physics there are five bosons which are elementary:\n\n\nThere may be a sixth \"tensor boson\" (spin=2), the graviton (G), that would be the force-carrier for gravity. It remains a hypothetical elementary particle since all attempts so far to incorporate gravitation into the Standard Model have failed. If the graviton does exist, it must be a boson, and could conceivably be a gauge boson.\n\nComposite bosons, such as helium nuclei, are important in superfluidity and other applications of Bose–Einstein condensates.\n\nBosons differ from fermions, which obey Fermi–Dirac statistics. Two or more identical fermions cannot occupy the same quantum state (see Pauli exclusion principle).\n\nSince bosons with the same energy can occupy the same place in space, bosons are often force carrier particles, including composite bosons such as mesons. Fermions are usually associated with matter (although in quantum mechanics the distinction between the two concepts is not clearcut).\n\nBosons are particles which obey Bose–Einstein statistics: When one swaps two bosons (of the same species), the wavefunction of the system is unchanged. Fermions, on the other hand, obey Fermi–Dirac statistics and the Pauli exclusion principle: Two fermions cannot occupy the same quantum state, accounting for the \"rigidity\" or \"stiffness\" of matter which includes fermions. Thus fermions are sometimes said to be the constituents of matter, while bosons are said to be the particles that transmit interactions (force carriers), or the constituents of radiation. The quantum fields of bosons are bosonic fields, obeying canonical commutation relations.\n\nThe properties of lasers and masers, superfluid helium-4 and Bose–Einstein condensates are all consequences of statistics of bosons. Another result is that the spectrum of a photon gas in thermal equilibrium is a Planck spectrum, one example of which is black-body radiation; another is the thermal radiation of the opaque early Universe seen today as microwave background radiation. \nInteractions between elementary particles are called fundamental interactions. The fundamental interactions of virtual bosons with real particles result in all forces we know.\n\nAll known elementary and composite particles are bosons or fermions, depending on their spin: Particles with half-integer spin are fermions; particles with integer spin are bosons. In the framework of nonrelativistic quantum mechanics, this is a purely empirical observation. In relativistic quantum field theory, the spin–statistics theorem shows that half-integer spin particles cannot be bosons and integer spin particles cannot be fermions.\n\nIn large systems, the difference between bosonic and fermionic statistics is only apparent at large densities — when their wave functions overlap. At low densities, both types of statistics are well approximated by Maxwell–Boltzmann statistics, which is described by classical mechanics.\n\nAll observed elementary particles are either fermions or bosons. The observed elementary bosons are all gauge bosons: photons, W and Z bosons, gluons, except the Higgs boson which is a scalar boson.\n\nFinally, many approaches to quantum gravity postulate a force carrier for gravity, the graviton, which is a boson of spin plus or minus two.\n\nComposite particles (such as hadrons, nuclei, and atoms) can be bosons or fermions depending on their constituents. More precisely, because of the relation between spin and statistics, a particle containing an even number of fermions is a boson, since it has integer spin.\n\nExamples include the following:\n\nThe number of bosons within a composite particle made up of simple particles bound with a potential has no effect on whether it is a boson or a fermion.\n\nBose–Einstein statistics encourages identical bosons to crowd into one quantum state, but not any state is necessarily convenient for it. Aside of statistics, bosons can interact – for example, helium-4 atoms are repulsed by intermolecular force on a very close approach, and if one hypothesizes their condensation in a spatially-localized state, then gains from the statistics cannot overcome a prohibitive force potential. A spatially-delocalized state (i.e. with low ) is preferable: if the number density of the condensate is about the same as in ordinary liquid or solid state, then the repulsive potential for the \"N\"-particle condensate in such state can be no higher than for a liquid or a crystalline lattice of the same \"N\" particles described without quantum statistics. Thus, Bose–Einstein statistics for a material particle is not a mechanism to bypass physical restrictions on the density of the corresponding substance, and superfluid liquid helium has a density comparable to the density of ordinary liquid matter. Spatially-delocalized states also permit for a low momentum according to the uncertainty principle, hence for low kinetic energy; this is why superfluidity and superconductivity are usually observed in low temperatures.\n\nPhotons do not interact with themselves and hence do not experience this difference in states where to crowd (see squeezed coherent state).\n\n"}
{"id": "27986621", "url": "https://en.wikipedia.org/wiki?curid=27986621", "title": "Braking chopper", "text": "Braking chopper\n\nBraking choppers, sometimes also referred to as braking units, are used in the DC voltage intermediate circuits of frequency converters to control voltage when the load feeds energy back to the intermediate circuit. This arises, for example, when a magnetized motor is being rotated by an overhauling load and so functions as a generator feeding power \nto the DC voltage intermediate circuit.\nThey are an application of the chopper principle, using on-off control of a switching device.\n\nA braking chopper is an electrical switch that limits the DC bus voltage by switching the braking energy to a resistor where the braking energy is converted to heat. Braking choppers are automatically activated when the actual DC bus voltage exceeds a specified level depending on the nominal voltage of the variable-frequency drive\n\n\n\nBraking choppers are inappropriate when:\n\nBraking choppers are appropriate when:\n\nFlux braking is another method, based on motor losses, for handling an overrunning load. When braking in the drive system is needed, the motor flux and thus also the magnetizing current component used in the motor are increased. The control of flux can be easily achieved through the direct torque control principle. With DTC the inverter is directly controlled to achieve the desired torque and flux for the motor. During flux braking the motor is under DTC control which guarantees that braking can be made according to the specified speed ramp. This is very different to the DC injection braking typically used in drives. In the DC injection method DC current is injected to the motor so that control of the motor flux is lost during braking. The flux braking method based on DTC enables the motor to shift quickly from braking to motoring power when requested.\n\nIn flux braking the increased current means increased losses inside the motor. The braking power is therefore also increased although the braking power delivered to the frequency converter is not increased. The increased current generates increased losses in motor resistances. The higher the resistance value the higher the braking energy dissipation inside the motor. Typically, in low power motors(below 5 kW) the resistance value of the motor is relatively large in respect to the nominal current of the motor. The higher the power or the voltage of the motor the less the resistance value of the motor in respect to motor current.\nIn other words, flux braking is most effective in a low power motor.\n\n"}
{"id": "17260531", "url": "https://en.wikipedia.org/wiki?curid=17260531", "title": "China Airlines Flight 605", "text": "China Airlines Flight 605\n\nChina Airlines Flight 605 (callsign \"Dynasty 605\") was a daily non-stop flight departing from Taipei at 6:30 a.m. and arriving at Kai Tak Airport in Hong Kong at 7:00 a.m. local time. On November 4, 1993, the plane crashed after overrunning the runway on landing during a storm. It was the first major loss of a Boeing 747-400.\n\nFlight 605, a Boeing 747-400, touched down more than past the runway's displaced threshold, at a speed of , following an IGS runway 13 approach. Tropical Storm Ira was generating crosswinds on that runway, gusting to , from a heading of 070 degrees.\n\nThe pilots received several computer-generated wind shear and glide slope deviation warnings, and observed severe airspeed fluctuations, during the last mile before touchdown. The auto brakes were set at only the number two level and then were turned off seconds after touchdown, when the captain elected to use manual braking and thrust reversal. The speedbrakes were extended briefly, but then retracted. This caused the plane to \"float,\" making the brakes ineffective until the speed brakes were extended again.\n\nThe captain deliberately turned the plane to the left when he realized the plane would overrun the runway, and into the approach lighting system (ALS) for runway 31. That action caused a \"ground loop\", making the plane slide off the left side of the runway into Victoria Harbour, thereby preventing a collision with the ALS for runway 31. It finally came to rest in shallow water, with a heading of almost 180 degrees out from the direction of runway 13.\n\nA British Airways pilot had refused to make the approach to Kai Tak runway 13 minutes before the CAL 605 captain decided to attempt it.\n\nThe investigation indicated that the accident was caused by the captain's failure to initiate the mandatory missed approach procedure when he observed the severe airspeed fluctuations, combined with the wind shear and glide slope deviation alerts.\n\nImmediately after the aircraft came to rest in the water, crew members ensured that all passengers donned life jackets and evacuated onto eight of the ten main deck emergency exits. These exits (as on all 747s) are equipped with inflatable evacuation slide/rafts for ditching emergencies. The passenger cabin remained completely above water during the evacuation, although eventually sinking tail-first. Additional damage to the nose and first-class cabin was noted. There were 23 minor injuries among passengers and crew. \n\nThe plane was written off as a total hull loss. The plane's vertical stabilizer interfered with the accuracy of the ILS signals for runway 31, so it was removed with dynamite shortly after the crash. That permitted airliners to make safe ILS approaches whenever the wind patterns mandated the use of runway 31 (the reciprocal direction of runway 13). The China Airlines lettering and the Chinese characters were removed, as was part of the livery on the fuselage, to conceal the identity of the aircraft as belonging to China Airlines. After the accident, the aircraft was stored near the HAECO building for use in firefighting practice.\n\n\n"}
{"id": "22313929", "url": "https://en.wikipedia.org/wiki?curid=22313929", "title": "Coke strength after reaction", "text": "Coke strength after reaction\n\nCoke Strength after Reaction (CSR) refers to coke \"hot\" strength, generally a quality reference in a simulated reaction condition in an industrial blast furnace. The test is based on a procedure developed by Nippon Steel Corp in the 1970s as an attempt to get an indication of coke performance and is used widely throughout the world since then. It is one of the major considerations when blending coking coal for export sale.\n\nIn the test, a 200 g sample of –21 mm to +19 mm particle range coke is heated at 1100 °C under 1 atmosphere pressure of carbon dioxide for 2 hours. The coke is preheated and cooled under nitrogen and the weight loss during reaction is measured. The percentage weight loss is known as the reactivity (CRI). The reacted coke is placed in an I-type drum (no lifters) and subjected to 600 revolutions in 30 minutes. The percent of carbon material removed from the drum that is +10 mm is known as the coke strength after reaction (CSR).\n"}
{"id": "44247756", "url": "https://en.wikipedia.org/wiki?curid=44247756", "title": "Cryogenic Low-Energy Astrophysics with Neon", "text": "Cryogenic Low-Energy Astrophysics with Neon\n\nThe Cryogenic Low-Energy Astrophysics with Noble liquids (CLEAN) experiment by the DEAP/CLEAN collaboration is searching for dark matter using noble gases at the SNOLAB underground facility. CLEAN has studied neon and argon in the MicroCLEAN prototype, and running the MiniCLEAN detector to test a multi-ton design.\n\nDark matter searches in isolated noble gas scintilators with xenon and argon have set limits on WIMP interactions, such as recent cross sections from LUX and XENON. Particles scattering in the target emit photons detected by PMTs, identified via pulse shape discrimination developed on DEAP results. Shielding reduces the cosmic and radiation background. Neon has been studied as a clear, dense, low-background scintilator. CLEAN can use neon or argon and plans runs with both to study nuclear mass dependence of any WIMP signals.\n\nThe MiniCLEAN detector will operate with argon in 2014. It will have 500 kg of noble cryogen in a spherical steel vessel with 92 PMTs shielded in a water tank with muon rejection.\n"}
{"id": "37192426", "url": "https://en.wikipedia.org/wiki?curid=37192426", "title": "Cryogenic energy storage", "text": "Cryogenic energy storage\n\nCryogenic energy storage (CES) is the use of low temperature (cryogenic) liquids such as liquid air or liquid nitrogen as energy storage. Both cryogens have been used to power cars. The inventor Peter Dearman initially developed a liquid air car, and then used the technology he developed for grid energy storage. The technology is being piloted at a UK power station.\n\nA liquid air powered car called Liquid Air was built between 1899 and 1902 but it couldn't at the time compete in terms of efficiency with other engines. More recently, a liquid nitrogen vehicle was built. Peter Dearman, a garage inventor in Hertfordshire, UK who had initially developed a liquid air powered car, then put the technology to use as grid energy storage The Dearman engine differs from former nitrogen engine designs in that the nitrogen is heated by combining it with the heat exchange fluid inside the cylinder of the engine.\n\nWhen it is cheaper (usually at night), electricity is used to cool air from the atmosphere to -195 °C using the Claude Cycle to the point where it liquefies. The liquid air, which takes up one-thousandth of the volume of the gas, can be kept for a long time in a large vacuum flask at atmospheric pressure. At times of high demand for electricity, the liquid air is pumped at high pressure into a heat exchanger, which acts as a boiler. Air from the atmosphere at ambient temperature, or hot water from an industrial heat source, is used to heat the liquid and turn it back into a gas. The massive increase in volume and pressure from this is used to drive a turbine to generate electricity.\n\nIn isolation the process is only 25% efficient, but this is greatly increased (to around 50%) when used with a low-grade cold store, such as a large gravel bed, to capture the cold generated by evaporating the cryogen. The cold is re-used during the next refrigeration cycle.\n\nEfficiency is further increased when used in conjunction with a power plant or other source of low-grade heat that would otherwise be lost to the atmosphere. Highview Power Storage claims an AC to AC round-trip efficiency of 70%, by using an otherwise waste heat source at 115 °C. The IMechE (Institution of Mechanical Engineers) agrees that these estimates for a commercial-scale plant are realistic. However this number was not checked or confirmed by independent professional institutions.\n\nCurrently surplus gaseous nitrogen is produced as a byproduct in the production of oxygen. Oxygen can be used in oxy-combustion coal power plants, enabling CO2 capture and sequestration. This gaseous nitrogen can be liquefied by available liquefaction capacities for further use. Cryogenic distillation of air is currently the only commercially viable technology for large scale oxygen production.\n\nA 300 kW, 2.5MWh storage capacity pilot cryogenic energy system developed by researchers at the University of Leeds and Highview Power Storage, that uses liquid air (with the CO2 and water removed as they would turn solid at the storage temperature) as the energy store, and low-grade waste heat to boost the thermal re-expansion of the air, operated at a 80MW biomass power station in Slough, UK, from 2010 until 2014 when it was relocated to the university of Birmingham . The efficiency is less than 15% because of low efficiency hardware components used, but the engineers are targeting an efficiency of about 60 percent for the next generation of CES based on operation experiences of this system.\n\nThe system is based on proven technology, used safely in many industrial processes, and does not require any particularly rare elements or expensive components to manufacture. Dr Tim Fox, the head of Energy at the IMechE says \"It uses standard industrial components - which reduces commercial risk; it will last for decades and it can be fixed with a spanner.\"\n\nIn April 2014 the UK government announced it had given £8 million to Viridor Waste Management Ltd. and Highview Power Storage to fund the next stage of the demonstration. The resulting grid-scale demonstrator plant at Pilsworth Landfill facility in Bury, Greater Manchester, UK, started operation in April 2018. \n\nThis is based on research by the Birmingham Centre for Cryogenic Energy Storage (BCCES) associated with the University of Birmingham, and has storage for up to 15MWhrs, and can generate a peak supply of 5Mw (so when fully charged lasts for 3 hours at maximum output) and is designed for an operational life of 40 years.\n\n"}
{"id": "262621", "url": "https://en.wikipedia.org/wiki?curid=262621", "title": "Darrieus wind turbine", "text": "Darrieus wind turbine\n\nThe Darrieus wind turbine is a type of vertical axis wind turbine (VAWT) used to generate electricity from the energy carried in the wind. The turbine consists of a number of curved aerofoil blades mounted on a vertical rotating shaft or framework. The curvature of the blades allows the blade to be stressed only in tension at high rotating speeds. There are several closely related wind turbines that use straight blades. This design of wind turbine was patented by Georges Jean Marie Darrieus, a French aeronautical engineer; filing for the patent was October 1, 1926. There are major difficulties in protecting the Darrieus turbine from extreme wind conditions and in making it self-starting.\n\nIn the original versions of the Darrieus design, the aerofoils are arranged so that they are symmetrical and have zero rigging angle, that is, the angle that the aerofoils are set relative to the structure on which they are mounted. This arrangement is equally effective no matter which direction the wind is blowing—in contrast to the conventional type, which must be rotated to face into the wind.\n\nWhen the Darrieus rotor is spinning, the aerofoils are moving forward through the air in a circular path. Relative to the blade, this oncoming airflow is added vectorially to the wind, so that the resultant airflow creates a varying small positive angle of attack (AoA) to the blade. This generates a net force pointing obliquely forwards along a certain 'line-of-action'. This force can be projected inwards past the turbine axis at a certain distance, giving a positive torque to the shaft, thus helping it to rotate in the direction it is already travelling in. The aerodynamic principles which rotate the rotor are equivalent to that in autogiros, and normal helicopters in autorotation.\n\nAs the aerofoil moves around the back of the apparatus, the angle of attack changes to the opposite sign, but the generated force is still obliquely in the direction of rotation, because the wings are symmetrical and the rigging angle is zero. The rotor spins at a rate unrelated to the windspeed, and usually many times faster. The energy arising from the torque and speed may be extracted and converted into useful power by using an electrical generator.\n\nThe aeronautical terms lift and drag are, strictly speaking, forces across and along the approaching net relative airflow respectively, so they are not useful here. We really want to know the tangential force pulling the blade around, and the radial force acting against the bearings.\n\nWhen the rotor is stationary, no net rotational force arises, even if the wind speed rises quite high—the rotor must already be spinning to generate torque. Thus the design is not normally self-starting. Under rare conditions, Darrieus rotors can self-start, so some form of brake is required to hold it when stopped.\n\nOne problem with the design is that the angle of attack changes as the turbine spins, so each blade generates its maximum torque at two points on its cycle (front and back of the turbine). This leads to a sinusoidal (pulsing) power cycle that complicates design. In particular, almost all Darrieus turbines have resonant modes where, at a particular rotational speed, the pulsing is at a natural frequency of the blades that can cause them to (eventually) break. For this reason, most Darrieus turbines have mechanical brakes or other speed control devices to keep the turbine from spinning at these speeds for any lengthy period of time.\n\nAnother problem arises because the majority of the mass of the rotating mechanism is at the periphery rather than at the hub, as it is with a propeller. This leads to very high centrifugal stresses on the mechanism, which must be stronger and heavier than otherwise to withstand them. One common approach to minimise this is to curve the wings into an \"egg-beater\" shape (this is called a \"troposkein\" shape, derived from the Greek for \"the shape of a spun rope\") such that they are self-supporting and do not require such heavy supports and mountings. See. Fig. 1.\n\nIn this configuration, the Darrieus design is theoretically less expensive than a conventional type, as most of the stress is in the blades which torque against the generator located at the bottom of the turbine. The only forces that need to be balanced out vertically are the compression load due to the blades flexing outward (thus attempting to \"squeeze\" the tower), and the wind force trying to blow the whole turbine over, half of which is transmitted to the bottom and the other half of which can easily be offset with guy wires.\n\nBy contrast, a conventional design has all of the force of the wind attempting to push the tower over at the top, where the main bearing is located. Additionally, one cannot easily use guy wires to offset this load, because the propeller spins both above and below the top of the tower. Thus the conventional design requires a strong tower that grows dramatically with the size of the propeller. Modern designs can compensate most tower loads of that variable speed and variable pitch.\n\nIn overall comparison, while there are some advantages in Darrieus design there are many more disadvantages, especially with bigger machines in the MW class. The Darrieus design uses much more expensive material in blades while most of the blade is too close to the ground to give any real power. Traditional designs assume that wing tip is at least 40m from ground at lowest point to maximize energy production and lifetime. So far there is no known material (not even carbon fiber) which can meet cyclic load requirements.\n\nDarrieus's 1927 patent also covered practically any possible arrangement using vertical airfoils. One of the more common types is the H-rotor,\nalso called the Giromill or H-bar design, in which the long \"egg beater\" blades of the common Darrieus design are replaced with straight vertical blade sections attached to the central tower with horizontal supports.\n\nAnother variation of the Giromill is the Cycloturbine, in which each blade is mounted so that it can rotate around its own vertical axis. This allows the blades to be \"pitched\" so that they always have some angle of attack relative to the wind. The main advantage to this design is that the torque generated remains almost constant over a fairly wide angle, so a Cycloturbine with three or four blades has a fairly constant torque. Over this range of angles, the torque itself is near the maximum possible, meaning that the system also generates more power. The Cycloturbine also has the advantage of being able to self-start, by pitching the \"downwind moving\" blade flat to the wind to generate drag and start the turbine spinning at a low speed. On the downside, the blade pitching mechanism is complex and generally heavy, and some sort of wind-direction sensor needs to be added in order to pitch the blades properly.\n\nA schematic of a self-acting pitch control system that does not require a wind-direction system is shown in Figure 4.\n\nThe blades of a Darrieus turbine can be canted into a helix, e.g. three blades and a helical twist of 60 degrees, similar to Gorlov's water turbines. Since the wind pulls each blade around on both the windward and leeward sides of the turbine, this feature spreads the torque evenly over the entire revolution, thus preventing destructive pulsations. This design is used by the Turby, Urban Green Energy, Enessere, Aerotecture and Quiet Revolution brands of wind turbine.\n\nThe relative speed creates on the blade, a force. This force can be decomposed into an axial and normal force (fig5). In the case of a Darrieus turbine, the axial force associated with the radius creates a torque and the normal force creates on the arm a stress which is alternately for each half turn, a compression stress and an extension stress.\nWith a crank rod system (fig.5), the principle of the Active Lift Turbine is to transform this alternative constraint into an additional energy recovery.\n\n"}
{"id": "20403801", "url": "https://en.wikipedia.org/wiki?curid=20403801", "title": "Delta set", "text": "Delta set\n\nIn mathematics, a Δ-set \"S\", often called a semi-simplicial set, is a combinatorial object that is useful in the construction and triangulation of topological spaces, and also in the computation of related algebraic invariants of such spaces. A Δ-set is somewhat more general than a simplicial complex, yet not quite as general as a simplicial set.\n\nFormally, a Δ-set is a sequence of sets formula_1 together with maps\n\nwith \"i\" = 0,1...,\"n\" + 1 for \"n\" ≥ 1 that satisfy\n\nwhenever \"i < j\".\n\nThis definition generalizes the notion of a simplicial complex, where the formula_4 are the sets of \"n\"-simplices, and the \"d\" are the face maps. It is not as general as a simplicial set, since it lacks \"degeneracies.\"\n\nGiven Δ-sets \"S\" and \"T\", a map of Δ-sets is a collection\nsuch that\nwhenever both sides of the equation are defined. With this notion, we can define the category of Δ-sets, whose objects are Δ-sets and whose morphisms are maps of Δ-sets.\n\nEach Δ-set has a corresponding geometric realization, defined as\nwhere we declare that\n\nHere, formula_9 denotes the standard \"n\"-simplex, and\n\nis the inclusion of the \"i\"-th face. The geometric realization is a topological space with the quotient topology.\n\nThe geometric realization of a Δ-set \"S\" has a natural filtration\nwhere\nis a \"restricted\" geometric realization.\n\nThe geometric realization of a Δ-set described above defines a covariant functor from the category of Δ-sets to the category of topological spaces. Geometric realization takes a Δ-set to a topological space, and carries maps of Δ-sets to induced continuous maps between geometric realizations (which are topological spaces).\n\nIf \"S\" is a Δ-set, there is an associated free abelian chain complex, denoted formula_13, whose \"n\"-th group is the free abelian group\ngenerated by the set formula_4, and whose \"n\"-th differential is defined by\n\nThis defines a covariant functor from the category of Δ-sets to the category of chain complexes of abelian groups. A Δ-set is carried to the chain complex just described, and a map of Δ-sets is carried to a map of chain complexes, which is defined by extending the map of Δ-sets in the standard way using the universal property of free abelian groups.\n\nGiven any topological space \"X\", one can construct a Δ-set formula_17 as follows. A singular \"n\"-simplex in \"X\" is a continuous map\n\nDefine\n\nto be the collection of all singular \"n\"-simplicies in \"X\", and define\n\nby\n\nwhere again \"d\" is the \"i\"-th face map. One can check that this is in fact a Δ-set. This defines a covariant functor from the category of topological spaces to the category of Δ-sets. A topological space is carried to the Δ-set just described, and a continuous map of spaces is carried to a map of Δ-sets, which is given by composing the map with the singular \"n\"-simplices.\n\nThis example illustrates the constructions described above. We can create a Δ-set \"S\" whose geometric realization is the unit circle formula_22, and use it to compute the homology of this space. Thinking of formula_22 as an interval with the endpoints identified, define\nwith formula_25 for all \"n ≥ 2\". The only possible maps formula_26 are\nIt is simple to check that this is a Δ-set, and that formula_28. Now, the associated chain complex formula_29 is\nwhere\nIn fact, formula_32 for all \"n\". The homology of this chain complex is also simple to compute:\nAll other homology groups are clearly trivial.\n\nOne advantage of using Δ-sets in this way is that the resulting chain complex is generally much simpler than the singular chain complex. For reasonably simple spaces, all of the groups will be finitely generated, whereas the singular chain groups are, in general, not even countably generated.\n\nOne drawback of this method is that one must prove that the geometric realization of the Δ-set is actually homeomorphic to the topological space in question. This can become a computational challenge as the Δ-set increases in complexity.\n\n\n"}
{"id": "8429", "url": "https://en.wikipedia.org/wiki?curid=8429", "title": "Density", "text": "Density\n\nThe density, or more precisely, the volumetric mass density, of a substance is its mass per unit volume. The symbol most often used for density is \"ρ\" (the lower case Greek letter rho), although the Latin letter \"D\" can also be used. Mathematically, density is defined as mass divided by volume:\n\nwhere \"ρ\" is the density, \"m\" is the mass, and \"V\" is the volume. In some cases (for instance, in the United States oil and gas industry), density is loosely defined as its weight per unit volume, although this is scientifically inaccurate – this quantity is more specifically called specific weight.\n\nFor a pure substance the density has the same numerical value as its mass concentration.\nDifferent materials usually have different densities, and density may be relevant to buoyancy, purity and packaging. Osmium and iridium are the densest known elements at standard conditions for temperature and pressure but certain chemical compounds may be denser.\n\nTo simplify comparisons of density across different systems of units, it is sometimes replaced by the dimensionless quantity \"relative density\" or \"specific gravity\", i.e. the ratio of the density of the material to that of a standard material, usually water. Thus a relative density less than one means that the substance floats in water.\n\nThe density of a material varies with temperature and pressure. This variation is typically small for solids and liquids but much greater for gases. Increasing the pressure on an object decreases the volume of the object and thus increases its density. Increasing the temperature of a substance (with a few exceptions) decreases its density by increasing its volume. In most materials, heating the bottom of a fluid results in convection of the heat from the bottom to the top, due to the decrease in the density of the heated fluid. This causes it to rise relative to more dense unheated material.\n\nThe reciprocal of the density of a substance is occasionally called its specific volume, a term sometimes used in thermodynamics. Density is an intensive property in that increasing the amount of a substance does not increase its density; rather it increases its mass.\n\nIn a well-known but probably apocryphal tale, Archimedes was given the task of determining whether King Hiero's goldsmith was embezzling gold during the manufacture of a golden wreath dedicated to the gods and replacing it with another, cheaper alloy. Archimedes knew that the irregularly shaped wreath could be crushed into a cube whose volume could be calculated easily and compared with the mass; but the king did not approve of this. Baffled, Archimedes is said to have taken an immersion bath and observed from the rise of the water upon entering that he could calculate the volume of the gold wreath through the displacement of the water. Upon this discovery, he leapt from his bath and ran naked through the streets shouting, \"Eureka! Eureka!\" (Εύρηκα! Greek \"I have found it\"). As a result, the term \"eureka\" entered common parlance and is used today to indicate a moment of enlightenment.\n\nThe story first appeared in written form in Vitruvius' \"books of architecture\", two centuries after it supposedly took place. Some scholars have doubted the accuracy of this tale, saying among other things that the method would have required precise measurements that would have been difficult to make at the time.\n\nFrom the equation for density (\"ρ\" = \"m\"/\"V\"), mass density has units of mass divided by volume. As there are many units of mass and volume covering many different magnitudes there are a large number of units for mass density in use. The SI unit of kilogram per cubic metre (kg/m) and the cgs unit of gram per cubic centimetre (g/cm) are probably the most commonly used units for density. One g/cm is equal to one thousand kg/m. One cubic centimetre (abbreviation cc) is equal to one millilitre. In industry, other larger or smaller units of mass and or volume are often more practical and US customary units may be used. See below for a list of some of the most common units of density.\n\nA number of techniques as well as standards exist for the measurement of density of materials. Such techniques include the use of a hydrometer (a buoyancy method for liquids), Hydrostatic balance (a buoyancy method for liquids and solids), immersed body method (a buoyancy method for liquids), pycnometer (liquids and solids), air comparison pycnometer (solids), oscillating densitometer (liquids), as well as pour and tap (solids). However, each individual method or technique measures different types of density (e.g. bulk density, skeletal density, etc.), and therefore it is necessary to have an understanding of the type of density being measured as well as the type of material in question. \n\nThe density at all points of a homogeneous object equals its total mass divided by its total volume. The mass is normally measured with a scale or balance; the volume may be measured directly (from the geometry of the object) or by the displacement of a fluid. To determine the density of a liquid or a gas, a hydrometer, a dasymeter or a Coriolis flow meter may be used, respectively. Similarly, hydrostatic weighing uses the displacement of water due to a submerged object to determine the density of the object.\n\nIf the body is not homogeneous, then its density varies between different regions of the object. In that case the density around any given location is determined by calculating the density of a small volume around that location. In the limit of an infinitesimal volume the density of an inhomogeneous object at a point becomes: formula_2, where formula_3 is an elementary volume at position formula_4. The mass of the body then can be expressed as\n\nIn practice, bulk materials such as sugar, sand, or snow contain voids. Many materials exist in nature as flakes, pellets, or granules.\n\nVoids are regions which contain something other than the considered material. Commonly the void is air, but it could also be vacuum, liquid, solid, or a different gas or gaseous mixture.\n\nThe bulk volume of a material—inclusive of the void fraction—is often obtained by a simple measurement (e.g. with a calibrated measuring cup) or geometrically from known dimensions.\n\nMass divided by \"bulk\" volume determines bulk density. This is not the same thing as volumetric mass density.\n\nTo determine volumetric mass density, one must first discount the volume of the void fraction. Sometimes this can be determined by geometrical reasoning. For the close-packing of equal spheres the non-void fraction can be at most about 74%. It can also be determined empirically. Some bulk materials, however, such as sand, have a \"variable\" void fraction which depends on how the material is agitated or poured. It might be loose or compact, with more or less air space depending on handling.\n\nIn practice, the void fraction is not necessarily air, or even gaseous. In the case of sand, it could be water, which can be advantageous for measurement as the void fraction for sand saturated in water—once any air bubbles are thoroughly driven out—is potentially more consistent than dry sand measured with an air void.\n\nIn the case of non-compact materials, one must also take care in determining the mass of the material sample. If the material is under pressure (commonly ambient air pressure at the earth's surface) the determination of mass from a measured sample weight might need to account for buoyancy effects due to the density of the void constituent, depending on how the measurement was conducted. In the case of dry sand, sand is so much denser than air that the buoyancy effect is commonly neglected (less than one part in one thousand).\n\nMass change upon displacing one void material with another while maintaining constant volume can be used to estimate the void fraction, if the difference in density of the two voids materials is reliably known.\n\nIn general, density can be changed by changing either the pressure or the temperature. Increasing the pressure always increases the density of a material. Increasing the temperature generally decreases the density, but there are notable exceptions to this generalization. For example, the density of water increases between its melting point at 0 °C and 4 °C; similar behavior is observed in silicon at low temperatures.\n\nThe effect of pressure and temperature on the densities of liquids and solids is small. The compressibility for a typical liquid or solid is 10 bar (1 bar = 0.1 MPa) and a typical thermal expansivity is 10 K. This roughly translates into needing around ten thousand times atmospheric pressure to reduce the volume of a substance by one percent. (Although the pressures needed may be around a thousand times smaller for sandy soil and some clays.) A one percent expansion of volume typically requires a temperature increase on the order of thousands of degrees Celsius.\n\nIn contrast, the density of gases is strongly affected by pressure. The density of an ideal gas is\n\nwhere is the molar mass, is the pressure, is the universal gas constant, and is the absolute temperature. This means that the density of an ideal gas can be doubled by doubling the pressure, or by halving the absolute temperature.\n\nIn the case of volumic thermal expansion at constant pressure and small intervals of temperature the temperature dependence of density is :\n\nwhere formula_8 is the density at a reference temperature, formula_9 is the thermal expansion coefficient of the material at temperatures close to formula_10.\n\nThe density of a solution is the sum of mass (massic) concentrations of the components of that solution.\n\nMass (massic) concentration of each given component ρ in a solution sums to density of the solution.\n\nExpressed as a function of the densities of pure components of the mixture and their volume participation, it allows the determination of excess molar volumes:\nprovided that there is no interaction between the components.\n\nKnowing the relation between excess volumes and activity coefficients of the components, one can determine the activity coefficients.\n\nThe SI unit for density is:\n\nThe litre and metric tons are not part of the SI, but are acceptable for use with it, leading to the following units:\n\nDensities using the following metric units all have exactly the same numerical value, one thousandth of the value in (kg/m). Liquid water has a density of about 1 kg/dm, making any of these SI units numerically convenient to use as most solids and liquids have densities between 0.1 and 20 kg/dm.\n\nIn US customary units density can be stated in:\n\nImperial units differing from the above (as the Imperial gallon and bushel differ from the US units) in practice are rarely used, though found in older documents. The Imperial gallon was based on the concept that an Imperial fluid ounce of water would have a mass of one Avoirdupois ounce, and indeed 1 g/cc ≈ 1.00224129 ounces per Imperial fluid ounce = 10.0224129 pounds per Imperial gallon. The density of precious metals could conceivably be based on Troy ounces and pounds, a possible cause of confusion.\n\n"}
{"id": "18949847", "url": "https://en.wikipedia.org/wiki?curid=18949847", "title": "Dobrogea Wind Farm", "text": "Dobrogea Wind Farm\n\nEDP Dobrogea Wind Farm is a large wind power project in the Dobrogea region, Romania. It will consist of 113 individual wind turbines each with a nameplate capacity of around 2 MW. Collective output will be up to 226 MW of power, enough to power over 148,030 homes, with a capital investment required of approximately US$300 million.\n\n"}
{"id": "39529197", "url": "https://en.wikipedia.org/wiki?curid=39529197", "title": "EcoLon", "text": "EcoLon\n\nEcolon may refer either to recycled (ecologically-friendly nylon) mineral-glass reinforced Nylon 6 (Perlon) engineering resins, or to a ceramic filled fused silica coating commonly used in cookware (Ecolon).\n\nEcolon cookware coatings are touted as highly resistant to scratches caused by utensils, metallic cleaning pads and abrasives, and withstand high temperatures, leading to great durability. Teflon coatings start breaking down at 240 °C, while Ecolon remains stable up to 450 °C. EcoLon engineering resin is produced by Wellman Engineering Resins. Ecolon ceramic coatings are a trademark of Neoflam.\n\n"}
{"id": "44661512", "url": "https://en.wikipedia.org/wiki?curid=44661512", "title": "Eklutna Power Plant", "text": "Eklutna Power Plant\n\nThe Eklutna Power Plant, also referred to as Old Eklutna Power Plant, is a historic hydroelectric power plant on the Eklutna River in Anchorage, Alaska. Located about downstream of the more modern new Eklutna Power Plant, it was built in 1928-29 to provide electrical power to the growing city, and served as its primary power source until 1956. The facilities include two dams, a tunnel and penstock, and a powerhouse. The main dam, Eklutna Dam, located at the northwestern end of Eklutna Lake, was built in 1941 to replace a series of temporary structures built after an earthen dam failed before the plant began operation. The diversion dam, a concrete arch dam, is located downstream from the lake, and provides facilities for diverting water into the tunnel. The tunnel is long, and is terminated in a penstock, a structure designed to raise the water pressure. The powerhouse is a concrete-and-steel structure completed in 1929.\n\nThe power plant was listed on the National Register of Historic Places in 1980.\n\n"}
{"id": "12389063", "url": "https://en.wikipedia.org/wiki?curid=12389063", "title": "Electromagnetic acoustic transducer", "text": "Electromagnetic acoustic transducer\n\nThere are two basic components in an EMAT transducer. One is a magnet and the other is an electric coil. The magnet can be a permanent magnet or an electromagnet, which produces a static or a quasi-static magnetic field. In EMAT terminology, this field is called bias magnetic field. The electric coil is driven with an alternating current (AC) electric signal at ultrasonic frequency, typically in the range from 20 kHz to 10 MHz. Based on the application needs, the signal can be a continuous wave, a spike pulse, or a tone-burst signal. The electric coil with AC current also generates an AC magnetic field. When the test material is close to the EMAT, ultrasonic waves are generated in the test material through the interaction of the two magnetic fields.\n\nThere are two mechanisms to generate waves through magnetic field interaction. One is Lorentz force when the material is conductive. The other is magnetostriction when the material is ferromagnetic.\n\nThe AC current in the electric coil generates eddy current on the surface of the material. According to theory of electromagnetic induction, the distribution of the eddy current is only at a very thin layer of the material, called skin depth. This depth reduces with the increase of AC frequency, the material conductivity, and permeability. Typically for 1 MHz AC excitation, the skin depth is only a fraction of a millimeter for primary metals like steel, copper and aluminum. The eddy current in the magnetic field experiences Lorentz force. In a microscopic view, the Lorentz force is applied on the electrons in the eddy current. In a macroscopic view, the Lorentz force is applied on the surface region of the material due to interaction between electrons and atoms. The distribution of Lorentz force is controlled by the design of magnet, and design of the electric coil, the properties of the test material, relative position between the transducer and the test part, and the excitation signal for the transducer.\n\nA ferromagnetic material will have a dimensional change when an external magnetic field is applied. This effect is called magnetostriction. The flux field of a magnet expands or collapses depends on the arrangement of ferromagnetic material having inducing voltage in a coil and the amount of change is affected by the magnitude and direction of the field. The AC current in the electric coil induces an AC magnetic field and thus produces magnetostriction at ultrasonic frequency in the material. The disturbances caused by magnetostriction then propagate in the material as an ultrasound wave.\n\nIn polycrystalline material, the magnetostriction response is very complicated. It is affected by direction of the bias field, direction of the field from AC electric coil, the strength of bias field, and the amplitude of the AC current. In some cases, one or two peak response may be observed with the increase of bias field. In some cases, the response can be improved significantly with the change of relative direction between bias magnetic field and AC magnetic field. Quantitatively, the magnetostriction may be described in a similar mathematical format as piezoelectric constants. Empirically, a lot of experience is needed to fully understand the magnetostriction phenomenon.\n\nMagnetostriction effect has been used to generate both SH-type and Lamb type waves in steel products. Recently, due to the stronger magnetostriction effect in nickel than steel, magnetostriction sensors using nickel patches are also developed for nondestructive testing of steel products.\n\nAs an ultrasonic testing (UT) method, EMAT has all the advantages of UT compared to other NDT methods. Just like piezoelectric UT probes, EMAT probes can be used in pulse echo, pitch-catch, and through-transmission configurations. EMAT probes can also be assembled into phased array probes, delivering focusing and beam steering capabilities.\n\nCompared to piezoelectric transducers, EMAT probes have the following advantages:\n\n\nThe disadvantages of EMAT compared to piezoelectric UT can be summaried as follows:\n\n\nEMAT has been used in a broad range of applications and has potential to be used in many other applications. A brief and incomplete list is as follows.\n\n\n\n"}
{"id": "16987158", "url": "https://en.wikipedia.org/wiki?curid=16987158", "title": "Environmental Services Association", "text": "Environmental Services Association\n\nThe Environmental Services Association (ESA) is a professional organisation in the United Kingdom representing the UK's waste and secondary resources industry. The ESA's members include the major waste management companies in the UK and is open to all organisations involved in the management of wastes.\n\nFounded in 1968 by Waste Industry leaders including the following:\n\nTony Morgan, Purle Bros Ltd.\n\nDon Pannell, A A Pannell Ltd.\n\nAnthony Shutes, Hales Containers Ltd.\n\nMalcolm Wood, Leigh Interests Ltd.\n\nRichard Biffa, Biffa Waste Services Ltd.\n\nColin Drinkwater, W W Drinkwater Ltd.\n\nTony Smith, Waste Management Ltd.\n\nHarold Mould, Cleansing Service Group Ltd.\n\nSam Hemmings, S Hemmings Bristol Ltd.\n\nThe current Director-General is Barry Dennis who is also President of the Chartered Institution of Wastes Management, the professional body for waste professionals in the UK.\n\n"}
{"id": "2927563", "url": "https://en.wikipedia.org/wiki?curid=2927563", "title": "Etch pit density", "text": "Etch pit density\n\nThe etch pit density (EPD) is a measure for the quality of semiconductor wafers. \n\nAn etch solution is applied on the surface of the wafer where the etch rate is increased at dislocations of the crystal resulting in pits. For GaAs one uses typically molten KOH at 450 degrees Celsius for about 40 minutes in a zirconium crucible. The density of the pits can be determined by . Silicon wafers have usually a very low density of < 100 cm while semi-insulating GaAs wafers have a density on the order of 10 cm. \n\nHigh-purity Germanium detectors require the Ge crystals to be grown with a controlled range of dislocation density to reduce impurities. The etch pitch density requirement is typically within the range 10 to 10 cm. \n\nThe etch pit density can be determined according to DIN 50454-1 and ASTM F 1404.\n"}
{"id": "26816465", "url": "https://en.wikipedia.org/wiki?curid=26816465", "title": "Gowk stane", "text": "Gowk stane\n\nThe name gowk stane () has been applied to certain standing stones and glacial erratics in Scotland, often found in prominent geographical situations. Other spelling variants, such as gowke, gouk, gouke, goilk, goik, gok, goke, gook are found.\n\nGowk in Scots means a common cuckoo (\"Cuculus canorus\"), but also a stupid person or fool. The word derives from the Old Norse 'gaukr', a cuckoo. Other explanations and origins for the term are also found.\nThe word derives from Anglo-Saxon (Old English) 'gouk' and was replaced in the south and central England by the French loan word 'coucou' after the Norman Conquest.\nThe cuckoo family gets its English and scientific names from the call of the bird.\n\nThe Scottish Gaelic names are Coi: Cuach: Cuachag (poetical name): Cuthag. The Welsh for cuckoo is cog.\n\nCeltic mythology in particular is rich in references to cuckoos and the surviving folklore gives clues as to why some stones were given the \"gowk\" name.\n\nThe term \"gowk\" is perhaps best known in the context of the old Gowk's Day, the Scottish April Fools' Day, originally held on April 13 when the cuckoo begins to call, and when children were sent on a \"gowk hunt\", a harmless prank involving pointless errands.\n\nGowk meant both cuckoo and fool, the latter were thought to be fairy-touched. The call of the cuckoo was believed to beckon the souls of the dead, and the cuckoo was thought to be able to travel back and forth between the worlds of the living and the dead.\n\nIt was once commonly thought that the first appearance of a cuckoo also brought about a \"gowk storm\", a furious spring storm.\n\nCuckoos were said to have the power of prophesy and could foretell a person's lifespan, the number of their children and when they would marry.\n\nIt has also been suggested that the \"gowk\" or \"fool\" originated in the Dark Ages as a name for the Britons, given by the Saxons invaders, and carried some of the meaning of the \"Devil\" in the context of an arch foe, who is likened to the fool.\n\nIn the Outer Hebrides a cuckoo's call heard when a person was hungry was bad luck, however the opposite was true if the person had recently eaten.\n\nThe use of the term \"gowk\" at these sites suggests a link with springtime and some of the surviving legends associated with standing stones do have a link with the heralding of spring by the first cuckoo of that season to arrive. In the churchyard at Nevern in Wales is an old stone cross, carved with intricate knotwork. Villagers of Nevern would wait for their \"harbinger of spring\" and on 7 April, St Brynach's feast day, the first cuckoo of the year would arrive from Africa, alighting on the cross and singing to announce the arrival of spring.\n\nA local belief of the Gaelic-speaking community on the Isle of Lewis was that when the sun rose on midsummer morn, the \"shining one\" walked along the stone avenue at Callanish, his arrival heralded by the cuckoo's call.\n\nThe cuckoo traditionally sends forth its first call in spring from the gowk stone at Lisdivin in Northern Ireland.\n\nA few cuckoo stones are present at sites in England and Cornwall.\n\nThe various gowk stones often had other functions, such as acting as boundary markers or meeting places in what may have sometimes featureless landscapes. The gowk stone at Whitelee may have been used as a pulpit of sorts by ministers preaching at conventicles held on this remote spot in Covenanting times.\n\n\n\n\n\n"}
{"id": "245173", "url": "https://en.wikipedia.org/wiki?curid=245173", "title": "Great Sphinx of Giza", "text": "Great Sphinx of Giza\n\nThe Great Sphinx of Giza (, , ; literally: Father of Dread), commonly referred to as the Sphinx of Giza or just the Sphinx, is a limestone statue of a reclining sphinx, a mythical creature with the body of a lion and the head of a human. Facing directly from West to East, it stands on the Giza Plateau on the west bank of the Nile in Giza, Egypt. The face of the Sphinx is generally believed to represent the Pharaoh Khafre.\n\nCut from the bedrock, the original shape of the Sphinx has been restored with layers of blocks. It measures long from paw to tail, high from the base to the top of the head and wide at its rear haunches. It is the oldest known monumental sculpture in Egypt and is commonly believed to have been built by ancient Egyptians of the Old Kingdom during the reign of the Pharaoh Khafre ().\n\nThe Sphinx is a monolith carved into the bedrock of the plateau, which also served as the quarry for the pyramids and other monuments in the area. The nummulitic limestone of the area consists of layers which offer differing resistance to erosion (mostly caused by wind and windblown sand), leading to the uneven degradation apparent in the Sphinx's body. The lowest part of the body, including the legs, is solid rock. The body of the lion up to its neck is fashioned from softer layers that have suffered considerable disintegration. The layer in which the head was sculpted is much harder.\n\nThe Great Sphinx is one of the world's largest and oldest statues, but basic facts about it are still subject to debate, such as when it was built, by whom and for what purpose.\n\nIt is impossible to identify what name the creators called their statue, as the Great Sphinx does not appear in any known inscription of the Old Kingdom and there are no inscriptions anywhere describing its construction or its original purpose. In the New Kingdom, the Sphinx was called \"Hor-em-akhet\" (; ; Hellenized: \"Harmachis\"), and the pharaoh Thutmose IV (1401–1391 or 1397–1388 BC) specifically referred to it as such in his \"Dream Stele.\"\n\nThe commonly used name \"Sphinx\" was given to it in classical antiquity, about 2000 years after the commonly accepted date of its construction by reference to a Greek mythological beast with a lion's body, a woman's head and the wings of an eagle (although, like most Egyptian sphinxes, the Great Sphinx has a man's head and no wings). The English word \"sphinx\" comes from the ancient Greek Σφίγξ ( ) apparently from the verb σφίγγω ( / ), after the Greek sphinx who strangled anyone who failed to answer her riddle.\n\nThe name may alternatively be a linguistic corruption of the phonetically different ancient Egyptian word \"Ssp-anx\" (in Manuel de Codage). This name is given to royal statues of the Fourth dynasty of ancient Egypt (2575–2467 BC) and later in the New Kingdom () to the Great Sphinx more specifically.\n\nMedieval Arab writers, including al-Maqrīzī, call the Sphinx \"balhib\" and \"bilhaw\", which suggest a Coptic influence. The modern Egyptian Arabic name is (, ).\n\nThough there have been conflicting evidence and viewpoints over the years, the view held by modern Egyptology at large remains that the Great Sphinx was built in approximately 2500 BC for the pharaoh Khafra, the builder of the Second Pyramid at Giza.\n\nSelim Hassan, writing in 1949 on recent excavations of the Sphinx enclosure, summed up the problem:\nThe \"circumstantial\" evidence mentioned by Hassan includes the Sphinx's location in the context of the funerary complex surrounding the Second Pyramid, which is traditionally connected with Khafra. Apart from the Causeway, the Pyramid and the Sphinx, the complex also includes the Sphinx Temple and Valley Temple, both of which display similar design of their inner courts. The Sphinx Temple was built using blocks cut from the Sphinx enclosure, while those of the Valley Temple were quarried from the plateau, some of the largest weighing upwards of 100 tons.\n\nA diorite statue of Khafre, which was discovered buried upside down along with other debris in the Valley Temple, is claimed as support for the Khafra theory.\n\nThe Dream Stele, erected much later by the pharaoh Thutmose IV (1401–1391 or 1397–1388 BC), associates the Sphinx with Khafra. When the stele was discovered, its lines of text were already damaged and incomplete, and only referred to \"Khaf\", not Khafra. An extract was translated:\n\nEgyptologist Thomas Young, finding the \"Khaf\" hieroglyphs in a damaged cartouche used to surround a royal name, inserted the glyph \"ra\" to complete Khafra's name. When the Stele was re-excavated in 1925, the lines of text referring to \"Khaf\" flaked off and were destroyed.\n\nTheories held by academic Egyptologists regarding the builder of the Sphinx and the date of its construction are not universally accepted, and various persons have proposed alternative hypotheses about both the builder and dating.\n\nSome early Egyptologists and excavators of the Giza pyramid complex believed the Great Sphinx and associated temples to predate the fourth dynasty rule of Khufu, Khafre, and Menkaure. Petrie wrote in 1883 regarding the state of opinion regarding the age of the nearby temples, and by extension the Sphinx: \"The date of the Granite Temple [Valley Temple] has been so positively asserted to be earlier than the fourth dynasty, that it may seem rash to dispute the point\".\n\nIn 1857, Auguste Mariette, founder of the Egyptian Museum in Cairo, unearthed the much later Inventory Stela (estimated Dynasty XXVI, c. 678–525 BC), which tells how Khufu came upon the Sphinx, already buried in sand. Although certain tracts on the Stela are considered good evidence, this passage is widely dismissed as Late Period historical revisionism, a purposeful fake, created by the local priests with the attempt to certify the contemporary Isis temple an ancient history it never had. Such an act became common when religious institutions such as temples, shrines and priest's domains were fighting for political attention and for financial and economic donations.\n\nGaston Maspero, the French Egyptologist and second director of the Egyptian Museum in Cairo, conducted a survey of the Sphinx in 1886. He concluded that because the Dream stela showed the cartouche of Khafre in line thirteen, it was he who was responsible for the excavation and therefore the Sphinx must predate Khafre and his predecessors—possibly Dynasty IV, . English Egyptologist E. A. Wallis Budge agreed that the Sphinx predated Khafre's reign, writing in \"The Gods of the Egyptians\" (1914): \"This marvelous object [the Great Sphinx] was in existence in the days of Khafre, or Khephren, and it is probable that it is a very great deal older than his reign and that it dates from the end of the archaic period [].\" Maspero believed the Sphinx to be \"the most ancient monument in Egypt\".\n\nRainer Stadelmann, former director of the German Archaeological Institute in Cairo, examined the distinct iconography of the \"nemes\" (headdress) and the now-detached beard of the Sphinx and concluded the style is more indicative of the Pharaoh Khufu (2589–2566 BC), known to the Greeks as Cheops, builder of the Great Pyramid of Giza and Khafra's father. He supports this by suggesting Khafra's Causeway was built to conform to a pre-existing structure, which, he concludes, given its location, could only have been the Sphinx.\n\nColin Reader, an English geologist who independently conducted a more recent survey of the enclosure, agrees the various quarries on the site have been excavated around the Causeway. Because these quarries are known to have been used by Khufu, Reader concludes that the Causeway (and the temples on either end thereof) must predate Khufu, thereby casting doubt on the conventional Egyptian chronology.\n\nFrank Domingo, a forensic scientist in the New York City Police Department and an expert forensic anthropologist, used detailed measurements of the Sphinx, forensic drawings and computer imaging to conclude the face depicted on the Sphinx is not the same face as is depicted on a statue attributed to Khafra.\n\nIn 2004, Vassil Dobrev of the Institut Français d'Archéologie Orientale in Cairo announced he had uncovered new evidence the Great Sphinx may have been the work of the little-known Pharaoh Djedefre (2528–2520 BC), Khafra's half brother and a son of Khufu. Dobrev suggests Djedefre built the Sphinx in the image of his father Khufu, identifying him with the sun god Ra in order to restore respect for their dynasty. Dobrev also notes, like Stadelmann and others, the causeway connecting Khafre's pyramid to the temples was built around the Sphinx, suggesting it was already in existence at the time.\n\nThe Orion correlation theory, as expounded by popular authors Graham Hancock and Robert Bauval, is based on the proposed exact correlation of the three pyramids at Giza with the three stars ζ Ori, ε Ori and δ Ori, the stars forming Orion's Belt, in the relative positions occupied by these stars in 10500 BC. The authors argue that the geographic relationship of the Sphinx, the Giza pyramids and the Nile directly corresponds with Leo, Orion and the Milky Way respectively. Sometimes cited as an example of pseudoarchaeology, the theory is at variance with mainstream scholarship.\n\nThe Sphinx water erosion hypothesis contends that the main type of weathering evident on the enclosure walls of the Great Sphinx could only have been caused by prolonged and extensive rainfall, and must therefore predate the time of the pharaoh Khafra.\n\nThe hypothesis is championed by René Schwaller de Lubicz (1887–1961), who lived and studied Egyptology for 12 years in Egypt, and by Robert M. Schoch, a geologist and associate professor of natural science at the College of General Studies at Boston University, as well as by John Anthony West, an author and alternative Egyptologist.\n\nColin Reader, a British geologist, studied the erosion patterns and noticed that they are found predominantly on the western enclosure wall and not on the Sphinx itself. He proposed the rainfall water runoff hypothesis, which also recognizes climate change transitions in the area.\n\nAuthor Robert K. G. Temple proposes that the Sphinx was originally a statue of the Jackal-Dog Anubis, the God of the Necropolis, and that its face was recarved in the likeness of a Middle Kingdom pharaoh, Amenemhet II. Temple bases his identification on the style of the eye make-up and style of the pleats on the headdress.\n\nOver the years several authors have commented on what they perceive as \"Negroid\" characteristics in the face of the Sphinx. This issue has become part of the Ancient Egyptian race controversy, with respect to the ancient population as a whole. The face of the Sphinx has been damaged over the millennia.\n\nAt some unknown time the Giza Necropolis was abandoned, and the Sphinx was eventually buried up to its shoulders in sand. The first documented attempt at an excavation dates to , when the young Thutmose IV (1401–1391 or 1397–1388 BC) gathered a team and, after much effort, managed to dig out the front paws, between which he placed a granite slab, known as the Dream Stele, inscribed with the following excerpt:\n\nLater, Ramesses II the Great (1279–1213 BC) may have undertaken a second excavation.\n\nMark Lehner, an Egyptologist who has excavated and mapped the Giza plateau, originally asserted that there had been a far earlier renovation during the Old Kingdom (c. 2686–2184 BC), although he has subsequently recanted this viewpoint.\n\nIn AD 1817 the first modern archaeological dig, supervised by the Italian Giovanni Battista Caviglia, uncovered the Sphinx's chest completely.\n\nOne of the people working on clearing the sands from around the Great Sphinx was Eugène Grébaut, a French Director of the Antiquities Service \n\nThe entire Sphinx was finally excavated in 1925 to 1936, in digs led by Émile Baraize.\n\nIn 1931 engineers of the Egyptian government repaired the head of the Sphinx. Part of its headdress had fallen off in 1926 due to erosion, which had also cut deeply into its neck. This questionable repair was by the addition of a concrete collar between the headdress and the neck, creating an altered profile. Many renovations to the stone base and raw rock body were done in the 1980s, and then redone in the 1990s.\n\nThe one-metre-wide nose on the face is missing. Examination of the Sphinx's face shows that long rods or chisels were hammered into the nose, one down from the bridge and one beneath the nostril, then used to pry the nose off towards the south.\n\nThe Arab historian al-Maqrīzī, writing in the 15th century, attributes the loss of the nose to iconoclasm by Muhammad Sa'im al-Dahr—a Sufi Muslim from the khanqah of Sa'id al-Su'ada—in AD 1378, upon finding the local peasants making offerings to the Sphinx in the hope of increasing their harvest. Enraged, he destroyed the nose, and was later hanged for vandalism. Al-Maqrīzī describes the Sphinx as the \"talisman of the Nile\" on which the locals believed the flood cycle depended.\nThere is a story that the nose was broken off by a cannonball fired by Napoleon's soldiers. Other variants indict British troops, the Mamluks, and others. Sketches of the Sphinx by the Dane Frederic Louis Norden, made in 1738 and published in 1757, show the Sphinx missing its nose. This predates Napoleon's birth in 1769.\n\nIn addition to the lost nose, a ceremonial pharaonic beard is thought to have been attached, although this may have been added in later periods after the original construction. Egyptologist Vassil Dobrev has suggested that had the beard been an original part of the Sphinx, it would have damaged the chin of the statue upon falling. The lack of visible damage supports his theory that the beard was a later addition.\n\nResidues of red pigment are visible on areas of the Sphinx's face. Traces of yellow and blue pigment have been found elsewhere on the Sphinx, leading Mark Lehner to suggest that the monument \"was once decked out in gaudy comic book colors\".\n\nColin Reader has proposed that the Sphinx was probably the focus of solar worship in the Early Dynastic Period, before the Giza Plateau became a necropolis in the Old Kingdom (). He ties this in with his conclusions that the Sphinx, the Sphinx temple, the Causeway and the Khafra mortuary temple are all part of a complex which predates Dynasty IV (). The lion has long been a symbol associated with the sun in ancient Near Eastern civilizations. Images depicting the Egyptian king in the form of a lion smiting his enemies date as far back as the Early Dynastic Period.\n\nIn the New Kingdom, the Sphinx became more specifically associated with the god \"Hor-em-akhet\" (Hellenized: \"Harmachis\") or \"Horus-at-the-Horizon\", which represented the pharaoh in his role as the \"Shesep-ankh\" (English: \"Living Image\") of the god Atum. Pharaoh Amenhotep II (1427–1401 or 1397 BC) built a temple to the north east of the Sphinx nearly 1000 years after its construction, and dedicated it to the cult of \"Hor-em-akhet\".\n\nIn the last 700 years, there has been a proliferation of travellers and reports from Lower Egypt, unlike Upper Egypt, which was seldom reported from prior to the mid-18th century. Alexandria, Rosetta, Damietta, Cairo and the Giza Pyramids are described repeatedly, but not necessarily comprehensively. Many accounts were published and widely read. These include those of George Sandys, André Thévet, Athanasius Kircher, Balthasar de Monconys, Jean de Thévenot, John Greaves, Johann Michael Vansleb, Benoît de Maillet, Cornelis de Bruijn, Paul Lucas, Richard Pococke, Frederic Louis Norden and others. But there is an even larger set of more anonymous people who wrote obscure and little-read works, sometimes only unpublished manuscripts in libraries or private collections, including Henry Castela, Hans Ludwig von Lichtenstein, Michael Heberer von Bretten, Wilhelm von Boldensele, Pierre Belon du Mans, Vincent Stochove, Christophe Harant, Gilles Fermanel, Robert Fauvel, Jean Palerne Foresien, Willian Lithgow, Joos van Ghistele, etc.\n\nOver the centuries, writers and scholars have recorded their impressions and reactions upon seeing the Sphinx. The vast majority were concerned with a general description, often including a mixture of science, romance and mystique. A typical description of the Sphinx by tourists and leisure travelers throughout the 19th and 20th century was made by John Lawson Stoddard:\nFrom the 16th century far into the 19th century, observers repeatedly noted that the Sphinx has the face, neck and breast of a woman. Examples included Johannes Helferich (1579), George Sandys (1615), Johann Michael Vansleb (1677), Benoît de Maillet (1735) and Elliot Warburton (1844).\n\nMost early Western images were book illustrations in print form, elaborated by a professional engraver from either previous images available or some original drawing or sketch supplied by an author, and usually now lost. Seven years after visiting Giza, André Thévet (\"Cosmographie de Levant\", 1556) described the Sphinx as \"the head of a colossus, caused to be made by Isis, daughter of Inachus, then so beloved of Jupiter\". He, or his artist and engraver, pictured it as a curly-haired monster with a grassy dog collar. Athanasius Kircher (who never visited Egypt) depicted the Sphinx as a Roman statue, reflecting his ability to conceptualize (\"Turris Babel\", 1679). Johannes Helferich's (1579) Sphinx is a pinched-face, round-breasted woman with a straight haired wig; the only edge over Thévet is that the hair suggests the flaring lappets of the headdress. George Sandys stated that the Sphinx was a harlot; Balthasar de Monconys interpreted the headdress as a kind of hairnet, while François de La Boullaye-Le Gouz's Sphinx had a rounded hairdo with bulky collar.\n\nRichard Pococke's Sphinx was an adoption of Cornelis de Bruijn's drawing of 1698, featuring only minor changes, but is closer to the actual appearance of the Sphinx than anything previous. The print versions of Norden's careful drawings for his \"Voyage d'Egypte et de Nubie\", 1755 are the first to clearly show that the nose was missing. However, from the time of the Napoleonic invasion of Egypt onwards, a number of accurate images were widely available in Europe, and copied by others.\n\n\"Mystery of the Sphinx\", narrated by Charlton Heston, a documentary presenting the theories of John Anthony West, was shown as an NBC Special on 10 November 1993 (winning an Emmy award for Best Research) A 95-minute DVD, \"Mystery of the Sphinx: Expanded Edition\", was released in 2007. \"Age of the Sphinx\", a BBC Two Timewatch documentary presenting the theories of John Anthony West and critical to both sides of the argument, was shown on 27 November 1994. In 2008, the film \"10,000 BC\" showed a supposed original Sphinx with a lion's head. Before this film, this lion head theory had been published in documentary films about the origin of the Sphinx.\n\n\n"}
{"id": "23793041", "url": "https://en.wikipedia.org/wiki?curid=23793041", "title": "Gulf Refinery, Milford Haven", "text": "Gulf Refinery, Milford Haven\n\nThe Gulf Refinery at Milford Haven was an oil refinery situated on the Pembrokeshire coast in Wales. The refinery, originally owned by Gulf Oil, was opened in August 1968 by Queen Elizabeth II. Up to of oil could be processed a day at the facility.\n\nThe refinery closed down in December 1997, as part of plans by the Chevron Corporation, by then the owners of Gulf Oil, to withdraw from the downstream oil business in the UK. Today, the site is occupied by the SemLogistics oil depot and the Dragon LNG terminal.\n\n"}
{"id": "57554087", "url": "https://en.wikipedia.org/wiki?curid=57554087", "title": "Harry G. Broadman", "text": "Harry G. Broadman\n\nHarry Gerard Broadman (born December 23, 1954 in New York City) is a foreign trade and investment negotiator, global business growth strategist, private equity investor, professor of international economics, author and journalist. He is the CEO and Managing Partner of Proa Global Partners LLC and on the faculty of Johns Hopkins University at the Foreign Policy Institute (SAIS).\n\nBroadman was born in New York City, attended The Loomis Chaffee School, and graduated from Brown University in 1977 with an A.B. magna cum laude in economics and history. He was elected to Phi Beta Kappa in 1977. In 1978 he received his Master of Arts in economics, and in 1981 his Ph.D. in economics from The University of Michigan.\n\nBroadman served as a consultant on energy security at the Rand Corporation in 1979 and from 1980 to 1981 he was a research fellow at the Brookings Institution.\n\nHe became a fellow and assistant director of the Center for Energy Policy Research at Resources for the Future (RFF) in 1981, where he worked on U.S. oil import policy, regulatory reform of the natural gas industry, and oil exploration in non-OPEC developing countries.\nBroadman joined the Harvard University faculty in 1984 and taught courses on international business investment in the Department of Economics in the School of Arts and Sciences, and on energy economics in the Kennedy School; he was also a fellow at Harvard’s Energy and Environmental Policy Center. While at Harvard he co-authored an article arguing for placing a $10.00 tariff on imported oil to curb U.S. petroleum consumption in order to reduce the exposure of the national economy to external disturbances in the world oil market.\n\nHe served as the Chief Economist of the US Senate Committee on Governmental Affairs, then chaired by Senator John Glenn beginning in 1987, and in 1990, Broadman joined the administration of President George H.W. Bush to became Chief of Staff of the President’s Council of Economic Advisers.\n\nSubsequently, and continuing into the administration of Bill Clinton, Broadman was appointed as United States Assistant Trade Representative, with responsibility for overseeing the US government’s negotiations of bilateral investment treaties (BITs) with foreign governments as well as its negotiations of the General Agreement on Trade in Services as part of the Uruguay Round multilateral trade agreement that established the WTO. He also led the negotiations of the foreign investment provisions of the North America Free Trade Agreement (NAFTA). He represented the US Trade Representative on the board of the Overseas Private Investment Corporation (OPIC) and on the inter-agency Committee on Foreign Investment in the United States (CFIUS).\n\nFrom 1993 to 2008, he held several positions in the World Bank, where he managed the bank’s loan operations and economic policy reform programs in China, especially on restructuring the country’s industrial state owned enterprises and its shift towards a market-oriented economy; in The Russian Federation and other countries within the Commonwealth of Independent States (CIS), especially on privatization programs; the Balkans; and sub-Saharan Africa.\n\nIn 2005 the World Bank published his book, \"From Disintegration to Reintegration: Eastern Europe and The Former Soviet Union in International Trade\", and in 2007 the World Bank published his book, \"Africa's Silk Road: China's and India's New Economic Frontier\".\n\nBroadman joined former US Secretary of State Madeleine Albright’s consultancy, The Albright Group, as Managing Director in 2008 at the same time former German Vice Chancellor Joschka Fischer joined the firm. Concurrently Broadman joined Albright Capital Management LLC, an alternative investment and private equity fund focused exclusively on emerging markets.\n\nIn 2011, he was appointed Senior Managing Director at PricewaterhouseCoopers (PwC) to establish and lead PwC's Emerging Markets Business Growth Strategy Management Consulting Practice and serve as PwC’s Chief Economist.\n"}
{"id": "1522979", "url": "https://en.wikipedia.org/wiki?curid=1522979", "title": "InSinkErator", "text": "InSinkErator\n\nInSinkErator is a company and brand name known for producing instant hot water dispensers and food waste disposal systems, generally called \"garbage disposals\" or \"garbage disposers\". The company was founded in Racine, Wisconsin by John W. Hammes, who was working as an architect there. Hammes is credited with inventing the \"in-sink\" food waste disposal in 1927, for his wife. It works by grinding and shredding solid food waste. He spent ten years improving the design and went into business selling the appliance. His company was called the In-Sink-Erator Manufacturing Company. The name is a play on the word \"incinerator\" and refers to the fact that the mouth of the disposal unit is located \"in\" the \"sink\".\n\nThe company was acquired in 1968 and is now a division of Emerson Appliance Solutions, part of Emerson (NYSE: EMR), a global technology and engineering company which serves industrial, commercial, and consumer markets.\n\nIn 2006 In-Sink-Erator changed their name to InSinkErator, and redesigned the company logo. They also released a re-engineered line of high technology disposers branded as the Evolution Series.\n\nInSinkErator is headquartered in Racine, Wisconsin.\n\n\n \n"}
{"id": "40603247", "url": "https://en.wikipedia.org/wiki?curid=40603247", "title": "Intel Display Power Saving Technology", "text": "Intel Display Power Saving Technology\n\nIntel Display Power Saving Technology or Intel DPST is an Intel backlight control technology. Intel claims that display take up most power in mobile devices and reducing backlight linearly affects energy footprint. Intel DPST technology aims to adaptively reduce backlight brightness while maintaining satisfactory visual performance. The Intel DPST subsystem analyzes the image to be displayed and it uses a set of algorithms to change the chroma value of pixels while reducing the brightness of backlight simultaneously such that there is minimum perceived visual degradation. When the frame to be projected and the frame being projected has considerable difference a software interrupt is asserted and new chroma values for pixels and brightness values are calculated. The current version is Intel DPST 6.0. Intel claims that the current DPST version reduces backlight power by 70% for DVD workloads.\n\n"}
{"id": "27802950", "url": "https://en.wikipedia.org/wiki?curid=27802950", "title": "Jebel al-Zayt oil spill", "text": "Jebel al-Zayt oil spill\n\nThe Jebel al Zayt oil spill occurred north of the Red Sea on June 16, 2010. It is considered to be the largest offshore spill in Egyptian history. The spill polluted around 100 miles (160 km) of coastline including tourist beach resorts. Oil company officials in the port city of Suez said the spill was caused by a leak from an offshore oil platform in Jebel al-Zayt north of Hurghada owned by the Egyptian government's state-owned oil company, Geisum Oil.\n\nThe oil spill occurred in Egypt, north of the Red Sea. It polluted several tourist areas along the coastline of the Red Sea. The spill damaged areas that are home to popular diving sites with extensive underwater coral reefs. A number of beaches and resorts along the coast were affected greatly. Marine life in the Hurghada area was at risk of heavy damage but it was later discovered that there was very little damage to the marine life. Scientists believe that the environmental damage was limited due to strong currents and winds that pushed the oil quickly away from the underwater coral reefs, and towards the shoreline of Hurghada. The affected areas have rich biodiversity that has a fragile ecosystem. Although it was unaffected environmentalist are still worried about disasters in the future and the potential danger it could inflict to the marine life.\n\nAn extensive cleaning initiative was implemented for the areas affected and damaged by the oil companies. Within 5 days of the spill over 90% of the impacted beaches and mainland had been cleaned. Although damage to wild life was still evident with turtles and sea birds covered in oil. Small oil spills are frequent in the area due to a number of offshore drilling sites. There are over 180 oil rigs operating in the Red Sea and the Gulf of Suez which accounts for a large percentage of the economy.\n\nThe Egyptian government has been accused of trying to cover up this disaster. Environmentalists in the area believe the government wanted to release as little information as possible in order to keep resorts in the area open for business. They also wanted tourists who had arrived for diving to continue visiting the area. These popular sites account for a large percentage of the tourism industry in Egypt. The tourism industry is one of the most important sectors in Egypt’s economy. The initial leak was first reported on June 18, 2010 but many believe it began days before. Although damage has been reported to be minimal an environmental group in the area reported the leak had restarted once the statement was released, damaging the surrounding areas even further. The Hurghada Environmental Protection and Conservation Association (HEPA) reported that hundreds of birds, turtles, and other wild life had been killed, but the government reported almost no deaths of wild life. The amount of oil that had leaked is still unknown, this has critics asking the government further questions as to why they have released such little information. Also the government is unable to determine the source of the spill or how it began.\n\nFew media outlets reported on this oil spill due to the BP oil spill which overshadowed this issue. The BP oil spill which was one of the largest accidental oil spills in history. It happened weeks before the Egypt oil spill had occurred.\n\n"}
{"id": "15112730", "url": "https://en.wikipedia.org/wiki?curid=15112730", "title": "Laughlin wavefunction", "text": "Laughlin wavefunction\n\nIn condensed matter physics, the Laughlin wavefunction is an ansatz, proposed by Robert Laughlin for the ground state of a two-dimensional electron gas placed in a uniform background magnetic field in the presence of a uniform jellium background when the filling factor (Quantum Hall effect) of the lowest Landau level is formula_1 where formula_2 is an odd positive integer. It was constructed to explain the observation of the formula_3 fractional quantum Hall effect, and predicted the existence of additional formula_4 states as well as quasiparticle excitations with fractional electric charge formula_5, both of which were later experimentally observed. Laughlin received one third of the Nobel Prize in Physics in 1998 for this discovery. Being a trial wavefunction, it is not exact, but qualitatively, it reproduces many features of the exact solution and quantitatively, it has very high overlaps with the exact ground state for small systems.\n\nIf we ignore the jellium and mutual Coulomb repulsion between the electrons as a zeroth order approximation, we have an infinitely degenerate lowest Landau level (LLL) and with a filling factor of 1/n, we'd expect that all of the electrons would lie in the LLL. Turning on the interactions, we can make the approximation that all of the electrons lie in the LLL. If formula_6 is the single particle wavefunction of the LLL state with the lowest orbital angular momentum, then the Laughlin ansatz for the multiparticle wavefunction is\n\nwhere position is denoted by\n\nin (Gaussian units)\n\nand formula_10 and formula_11 are coordinates in the xy plane. Here formula_12 is Planck's constant, formula_13 is the electron charge, formula_14 is the total number of particles, and formula_15 is the magnetic field, which is perpendicular to the xy plane. The subscripts on z identify the particle. In order for the wavefunction to describe fermions, n must be an odd integer. This forces the wavefunction to be antisymmetric under particle interchange. The angular momentum for this state is formula_16.\n\nThe Laughlin wavefunction is the multiparticle wavefunction for quasiparticles. The expectation value of the interaction energy for a pair of quasiparticles is \n\nwhere the screened potential is (see Coulomb potential between two current loops embedded in a magnetic field)\n\nwhere formula_19 is a confluent hypergeometric function and formula_20 is a Bessel function of the first kind. Here, formula_21 is the distance between the centers of two current loops, formula_22 is the magnitude of the electron charge, formula_23 is the quantum version of the Larmor radius, and formula_24 is the thickness of the electron gas in the direction of the magnetic field. The angular momenta of the two individual current loops are formula_25 and formula_26 where formula_27. The inverse screening length is given by (Gaussian units)\n\nwhere formula_29 is the cyclotron frequency, and formula_30 is the area of the electron gas in the xy plane.\n\nThe interaction energy evaluates to:\n\nTo obtain this result we have made the change of integration variables \n\nand\n\nand noted (see Common integrals in quantum field theory)\n\nThe interaction energy has minima for (Figure 1)\n\nand \n\nFor these values of the ratio of angular momenta, the energy is plotted in Figure 2 as a function of formula_38.\n\n"}
{"id": "3306766", "url": "https://en.wikipedia.org/wiki?curid=3306766", "title": "Lignoceric acid", "text": "Lignoceric acid\n\nLignoceric acid, or tetracosanoic acid, is the saturated fatty acid with formula CHCOOH. It is found in wood tar, various cerebrosides, and in small amounts in most natural fats. The fatty acids of peanut oil contain small amounts of lignoceric acid (1.1% – 2.2%). This fatty acid is also a byproduct of lignin production.\n\nReduction of lignoceric acid yields lignoceryl alcohol.\n\n"}
{"id": "4175003", "url": "https://en.wikipedia.org/wiki?curid=4175003", "title": "Magnetic pressure", "text": "Magnetic pressure\n\nMagnetic pressure is an energy density associated with a magnetic field. Any magnetic field has an associated magnetic pressure contained by the boundary conditions on the field. It is identical to any other physical pressure except that it is carried by the magnetic field rather than (in the case of a gas) by the kinetic energy of gas molecules. A gradient in field strength causes a force due to the magnetic pressure gradient called the magnetic pressure force.\n\nThe magnetic pressure force is readily observed in an unsupported loop of wire. If an electric current passes through the loop, the wire serves as an electromagnet, such that the magnetic field strength inside the loop is much greater than the field strength just outside the loop. This gradient in field strength gives rise to a magnetic pressure force that tends to stretch the wire uniformly outward. If enough current travels through the wire, the loop of wire will form a circle. At even higher currents, the magnetic pressure can create tensile stress that exceeds the tensile strength of the wire, causing it to fracture, or even explosively fragment. Thus, management of magnetic pressure is a significant challenge in the design of ultrastrong electromagnets.\n\nThe force (in cgs) exerted on a coil by its own current is\n\nWhere \"Y\" is the internal inductance of the coil, defined by the distribution of current. \"Y\" is 0 for high frequency currents carried mostly by the outer surface of the conductor, and 0.25 for DC currents distributed evenly throughout the conductor. See inductance for more information.\n\nInterplay between magnetic pressure and ordinary gas pressure is important to magnetohydrodynamics and plasma physics. Magnetic pressure can also be used to propel projectiles; this is the operating principle of a railgun.\n\nIf any currents present are parallel to a magnetic field, the field lines follow shapes in which the magnetic pressure gradient is balanced by the magnetic tension force. Such a field configuration is called force-free because there is no Lorentz force (formula_2). The familiar potential magnetic field is a special case of a force-free field: potential field configurations occupy space that contains no electric current at all.\n\nThe magnetic pressure formula_3 is given in SI units (\"P\" in Pa, \"B\" in T, μ in H/m) by\n\nand in cgs units (\"P\" in dyn/cm², \"B\" in G) by\n\nIn practical units,\n\n"}
{"id": "1057083", "url": "https://en.wikipedia.org/wiki?curid=1057083", "title": "Microbial ecology", "text": "Microbial ecology\n\nMicrobial ecology (or environmental microbiology) is the ecology of microorganisms: their relationship with one another and with their environment. It concerns the three major domains of life—Eukaryota, Archaea, and Bacteria—as well as viruses.\n\nMicroorganisms, by their omnipresence, impact the entire biosphere. Microbial life plays a primary role in regulating biogeochemical systems in virtually all of our planet's environments, including some of the most extreme, from frozen environments and acidic lakes, to hydrothermal vents at the bottom of deepest oceans, and some of the most familiar, such as the human small intestine. As a consequence of the quantitative magnitude of microbial life (Whitman and coworkers calculated cells, eight orders of magnitude greater than the number of stars in the observable universe) microbes, by virtue of their biomass alone, constitute a significant carbon sink. Aside from carbon fixation, microorganisms' key collective metabolic processes (including nitrogen fixation, methane metabolism, and sulfur metabolism) control global biogeochemical cycling. The immensity of microorganisms' production is such that, even in the total absence of eukaryotic life, these processes would likely continue unchanged.\n\nWhile microbes have been studied since the seventeenth-century, this research was from a primarily physiological perspective rather than an ecological one. For instance, Louis Pasteur and his disciples were interested in the problem of microbial distribution both on land and in the ocean. Martinus Beijerinck invented the enrichment culture, a fundamental method of studying microbes from the environment. He is often incorrectly credited with framing the microbial biogeographic idea that \"everything is everywhere, but, the environment selects\", which was stated by Lourens Baas Becking. Sergei Winogradsky was one of the first researchers to attempt to understand microorganisms outside of the medical context—making him among the first students of microbial ecology and environmental microbiology—discovering chemosynthesis, and developing the Winogradsky column in the process.\n\nBeijerinck and Windogradsky, however, were focused on the physiology of microorganisms, not the microbial habitat or their ecological interactions. Modern microbial ecology was launched by Robert Hungate and coworkers, who investigated the rumen ecosystem. The study of the rumen required Hungate to develop techniques for culturing anaerobic microbes, and he also pioneered a quantitative approach to the study of microbes and their ecological activities that differentiated the relative contributions of species and catabolic pathways.\n\nMicroorganisms are the backbone of all ecosystems, but even more so in the zones where photosynthesis is unable to take place because of the absence of light. In such zones, chemosynthetic microbes provide energy and carbon to the other organisms.\n\nOther microbes are decomposers, with the ability to recycle nutrients from other organisms' waste products. These microbes play a vital role in biogeochemical cycles. The nitrogen cycle, the phosphorus cycle, the sulphur cycle and the carbon cycle all depend on microorganisms in one way or another. For example, the nitrogen gas which makes up 78% of the earth's atmosphere is unavailable to most organisms, until it is converted to a biologically available form by the microbial process of nitrogen fixation.\n\nDue to the high level of horizontal gene transfer among microbial communities, microbial ecology is also of importance to studies of evolution.\n\nMicrobes, especially bacteria, often engage in symbiotic relationships (either positive or negative) with other microorganisms or larger organisms. Although physically small, symbiotic relationships amongst microbes are significant in eukaryotic processes and their evolution. The types of symbiotic relationship that microbes participate in include mutualism, commensalism, parasitism, and amensalism, and these relationships affect the ecosystem in many ways.\n\nMutualism in microbial ecology is a relationship between microbial species and between microbial species and humans that allow for both sides to benefit. One such example would be syntrophy, also known as cross-feeding, which is clearly shown in Methanobacterium omelianskii. Although initially thought of as one microbial species, this system is actually two species - an S organism and Methabacterium bryantii. The S organism provides the bacterium with the H, which the bacterium needs in order to grow and produce methane. The reaction used by the S organism for the production of H is endergonic (and so thermodynamically unfavored) however, when coupled to the reaction used by Methabacterium bryantii in its production of methane, the overall reaction becomes exergonic.  Thus the two organisms are in a mutualistic relationship which allows them to grow and thrive in an environment, deadly for either species alone. Lichen is an example of a symbiotic organism.\n\nAmensalism (also commonly known as antagonism) is a type of symbiotic relationship where one species/organism is harmed while the other remains unaffected. One example of such a relationship that takes place in microbial ecology is between the microbial species Lactobacillus casei and Pseudomonas taetrolens. When co-existing in an environment, \"Pseudomonas taetrolens\" shows inhibited growth and decreased production of lactobionic acid (its main product) most likely due to the byproducts created by Lactobacillus casei during its production of lactic acid. However, Lactobacillus casei shows no difference in its behaviour, and such this relationship can be defined as amensalism.\n\nBiotechnology may be used alongside microbial ecology to address a number of environmental and economic challenges. For example, molecular techniques such as community fingerprinting can be used to track changes in microbial communities over time or assess their biodiversity. Managing the carbon cycle to sequester carbon dioxide and prevent excess methanogenesis is important in mitigating global warming, and the prospects of bioenergy are being expanded by the development of microbial fuel cells. Microbial resource management advocates a more progressive attitude towards disease, whereby biological control agents are favoured over attempts at eradication. Fluxes in microbial communities has to be better characterized for this field's potential to be realised. In addition, there are also clinical implications, as marine microbial symbioses are a valuable source of existing and novel antimicrobial agents, and thus offer another line of inquiry in the evolutionary arms race of antibiotic resistance, a pressing concern for researchers.\n\nMicrobes exist in all areas, including homes, offices, commercial centers, and hospitals. In 2016, the journal \"Microbiome\" published a collection of various works studying the microbial ecology of the built environment.\n\nA 2006 study of pathogenic bacteria in hospitals found that their ability to survive varied by the type, with some surviving for only a few days while others survived for months.\n\nThe lifespan of microbes in the home varies similarly. Generally bacteria and viruses require a wet environment with a humidity of over 10 percent. \"E. coli\" can survive for a few hours to a day. Bacteria which form spores can survive longer, with \"Staphylococcus aureus\" surviving potentially for weeks or, in the case of \"Bacillus anthracis\", years.\n\nIn the home, pets can be carriers of bacteria; for example, reptiles are commonly carriers of salmonella.\n\n\"S. aureus\" is particularly common, and asymptomatically colonizes about 30% of the human population; attempts to decolonize carriers have met with limited success and generally involve mupirocin nasally and chlorhexidine washing, potentially along with vancomycin and cotrimoxazole to address intestinal and urinary tract infections.\n\nSome metals, particularly copper and silver, have antimicrobial properties. Using antimicrobial copper-alloy touch surfaces is a technique which has begun to be used in the 21st century to prevent transmission of bacteria. Silver nanoparticles have also begun to be incorporated into building surfaces and fabrics, although concerns have been raised about the potential side-effects of the tiny particles on human health.\n\n"}
{"id": "20282696", "url": "https://en.wikipedia.org/wiki?curid=20282696", "title": "Moving Mountains (book)", "text": "Moving Mountains (book)\n\nMoving Mountains: How One Woman and Her Community Won Justice From Big Coal is a 2007 book published by the University Press of Kentucky. The award-winning book is written by Virginia resident Penny Loeb, a former senior editor at U.S. News & World Report and a former investigative reporter for \"Newsday\".\n\nLoeb spent nine years chronicling the difficult situation of Trish Bragg and other inhabitants of the West Virginia coalfields. In Loeb's analysis, these people are \"caught between the economic opportunities provided by coal and the detriments to health and to quality of life that are so often the by-products of the coal industry\". \"Moving Mountains\" is an account of the human and environmental costs of coal extraction, and the grassroots movement to mitigate those costs.\n\nLoeb has received many awards for journalism, and was a finalist for the Pulitzer Prize in 1988 and for a National Magazine Award in 1993 and 1997.\n\n"}
{"id": "18915410", "url": "https://en.wikipedia.org/wiki?curid=18915410", "title": "New Energy for America", "text": "New Energy for America\n\nNew Energy for America was a plan led by Barack Obama and Joe Biden to invest in renewable energy sources, reduce reliance on foreign oil, address global warming issues, and create jobs for Americans.\n\nThe plan was presented in the Barack Obama presidential campaign, 2008.\n\nPresident Barack Obama in his inaugural address called for the expanded use of renewable energy to meet the twin challenges of energy security and climate change.\n\nObama issued a pair of memoranda on January 26 2009 to publish higher fuel economy standards for the model year 2011 cars and light trucks by the end of March and lower greenhouse gas emissions (to revisit a California waiver request that would allow that state to implement its own greenhouse gas emission rules for vehicles).\n\nThe plan aimed to:\n\n"}
{"id": "41090148", "url": "https://en.wikipedia.org/wiki?curid=41090148", "title": "Nitridoborate", "text": "Nitridoborate\n\nThe nitridoborates are chemical compounds of boron and nitrogen with metals. These compounds are typically produced at high temperature by reacting hexagonal boron nitride (α -BN) with metal nitrides or by metathesis reactions involving nitridoborates. A wide range of these compounds have been made involving lithium, alkaline earth metals and lanthanides, and their structures determined using crystallographic techniques such as X-ray crystallography. Structurally one of their interesting features is the presence of polyatomic anions of boron and nitrogen where the geometry and the B–N bond length have been interpreted in terms of π-bonding.\n\nMany of the compounds produced can be described as ternary compounds of metal boron and nitrogen and examples of these are LiBN, MgBN, LaBN, LaBN. However there are examples of compounds with more than one metal, for example LaNiBN and compounds containing anions such as Cl, for example MgBNCl.\n\nExamination of the crystallographic data shows the presence of polyatomic units consisting of boron and nitrogen. These units have structures similar to those of isoelectronic anions, which have π-bonded structures. The bonding in some of these compounds is ionic in character, such as Ca[BN], other compounds have metallic characteristics, where the bonding has been described in terms of π-bonded anions with extra electrons in anti-bonding orbitals that not only cause a lengthening of the B–N bonds but also form part of the conduction band of the solid. The simplest ion BN is comparable to the ion, but attempts to prepare the compound CaBN analogous to CaC calcium carbide failed. The bonding of compounds containing the diatomic BN anion have been explained in terms of electrons entering anti-bonding orbitals and reducing the B–N bond order from 3 (triple bond) in BN to 2 (double bond) in BN.\n\nSome nitridoborates are salt-like such as LiBN, LiCa[BN] others have a metallic lustre, such as LiEu[BN]. Bonding calculations show that the energy of the valence orbitals of metal atoms of group 2 and lanthanide elements are higher than those of the bonding orbitals in BN ions which indicates an ionic like interaction between a metal atom and a BN ion. With lanthanide compounds where extra electrons enter the anti-bonding orbitals of an ion there can be a smaller band gap giving the compounds metal like properties such as lustre. With transition metals the d orbitals can be similar in energy to bonding orbitals in the BN anions suggesting covalent interactions.\n\nFor comparison purposes the following are considered to be typical BN bond lengths\n"}
{"id": "14503955", "url": "https://en.wikipedia.org/wiki?curid=14503955", "title": "Paul Chun (professor)", "text": "Paul Chun (professor)\n\nPaul W. Chun is a professor emeritus at the University of Florida. He is a researcher in the field of protein folding equilibria, in particular, he is known as the \"leading proponent\" of using the Planck-Benzinger thermal work function to understand protein folding thermodynamics and stability. As such Chun has written a number of papers relating to the thermodynamics of protein folding. He received his Ph.D. in 1965 from the University of Missouri for work on the interaction of casein molecules, and joined the Department of Biochemistry and Molecular Biology at the University of Florida soon thereafter. He retired in 2003.\n\nHe has published 68 peer-reviewed papers listed in Scopus.\n\n"}
{"id": "203174", "url": "https://en.wikipedia.org/wiki?curid=203174", "title": "Perfect gas", "text": "Perfect gas\n\nIn physics, a perfect gas is a theoretical gas that differs from real gases in a way that makes certain calculations easier to handle. Its behavior is more simplified compared to an ideal gas (also a theoretical gas). In particular, intermolecular forces are neglected, which means that one can use the ideal gas law without restriction and neglect many complications that may arise from the Van der Waals forces.\n\nThe terms \"perfect gas\" and \"ideal gas\" are sometimes used interchangeably, depending on the particular field of physics and engineering. Sometimes, other distinctions are made, such as between \"thermally perfect gas\" and \"calorically perfect gas\", or between imperfect, semi-perfect, perfect, and ideal gases. Two of the common sets of nomenclatures are summarized in the following table.\n\nAlong with the definition of a perfect gas, there are also two more simplifications that can be made although various textbooks either omit or combine the following simplifications into a general \"perfect gas\" definition.\n\nA thermally perfect gas\n\nIt can be proved that an ideal gas (i.e. satisfying the ideal gas equation of state) is thermally perfect.\nThis type of approximation is useful for modeling, for example, an axial compressor where temperature fluctuations are usually not large enough to cause any significant deviations from the \"thermally perfect\" gas model. Heat capacity is still allowed to vary, though only with temperature, and molecules are not permitted to dissociate. The latter implies temperature limited to 2500 K.\n\nEven more restricted is the calorically perfect gas for which, in addition, the heat capacity is assumed to be constant: formula_5 and formula_6.\n\nAlthough this may be the most restrictive model from a temperature perspective, it is accurate enough to make reasonable predictions within the limits specified. A comparison of calculations for one compression stage of an axial compressor (one with variable \"C\", and one with constant \"C\") produces a deviation small enough to support this approach. As it turns out, other factors come into play and dominate during this compression cycle. These other effects would have a greater impact on the final calculated result than whether or not \"C\" was held constant. (examples of these real gas effects include compressor tip-clearance, separation, and boundary layer/frictional losses, etc.)\n\n"}
{"id": "30357160", "url": "https://en.wikipedia.org/wiki?curid=30357160", "title": "Planing mill", "text": "Planing mill\n\nA planing mill is a facility that takes cut and seasoned boards from a sawmill and turns them into finished dimensional lumber. Machines used in the mill include the planer and matcher, the molding machines, and varieties of saws. In the planing mill planer operators use machines that smooth and cut the wood for many different uses.\n\n\n"}
{"id": "4242000", "url": "https://en.wikipedia.org/wiki?curid=4242000", "title": "Power-system protection", "text": "Power-system protection\n\nPower-system protection is a branch of electrical power engineering that deals with the protection of electrical power systems from faults through the isolation of faulted parts from the rest of the electrical network. The objective of a protection scheme is to keep the power system stable by isolating only the components that are under fault, whilst leaving as much of the network as possible still in operation. Thus, protection schemes must apply a very pragmatic and pessimistic approach to clearing system faults. The devices that are used to protect the power systems from faults are called protection devices.\n\nProtection systems usually comprise five components:\nFor parts of a distribution system, fuses are capable of both sensing and disconnecting faults\n\nFailures may occur in each part, such as insulation failure, fallen or broken transmission lines, incorrect operation of circuit breakers, short circuits and open circuits. Protection devices are installed with the aims of protection of assets and ensuring continued supply of energy.\n\nSwitchgear is a combination of electrical disconnect switches, fuses or circuit breakers used to control, protect and isolate electrical equipment. Switches are safe to open under normal load current, while protective devices are safe to open under fault current.\n\n\nWhile the operating quality of these devices, and especially of protective relays, is always critical, different strategies are considered for protecting the different parts of the system. Very important equipment may have completely redundant and independent protective systems, while a minor branch distribution line may have very simple low-cost protection.\n\nThere are three parts to protective devices:\n\nAdvantages of protected devices with these three basic components include safety, economy, and accuracy.\n\nProtection on the transmission and distribution system serves two functions: protection of plant and protection of the public (including employees). At a basic level, protection disconnects equipment which experiences an overload or a short to earth. Some items in substations such as transformers might require additional protection based on temperature or gas pressure, among others.\n\nIn a power plant, the protective relays are intended to prevent damage to alternators or to the transformers in case of abnormal conditions of operation, due to internal failures, as well as insulating failures or regulation malfunctions. Such failures are unusual, so the protective relays have to operate very rarely. If a protective relay fails to detect a fault, the resulting damage to the alternator or to the transformer might require costly equipment repairs or replacement, as well as income loss from the inability to produce and sell energy.\n\nOverload protection requires a current transformer which simply measures the current in a circuit. There are two types of overload protection: instantaneous overcurrent (IOC) and time overcurrent (TOC). Instantaneous overcurrent requires that the current exceeds a predetermined level for the circuit breaker to operate. Time overcurrent protection operates based on a current vs time curve. Based on this curve, if the measured current exceeds a given level for the preset amount of time, the circuit breaker or fuse will operate. The function of both types is explained in .\n\nEarth fault protection also requires current transformers and senses an imbalance in a three-phase circuit. Normally the three phase currents are in balance, i.e. roughly equal in magnitude. If one or two phases become connected to earth via a low impedance path, their magnitudes will increase dramatically, as will current imbalance. If this imbalance exceeds a pre-determined value, a circuit breaker should operate. Restricted earth fault protection is a type of earth fault protection which looks for earth fault between two sets of current transformers (hence restricted to that zone).\n\nDistance protection detects both voltage and current. A fault on a circuit will generally create a sag in the voltage level. If the ratio of voltage to current measured at the relay terminals, which equates to an impedance, lands within a predetermined level the circuit breaker will operate. This is useful for reasonably long lines, lines longer than 10 miles, because their operating characteristics are based on the line characteristics. This means that when a fault appears on the line the impedance setting in the relay is compared to the apparent impedance of the line from the relay terminals to the fault. If the relay setting is determined to be below the apparent impedance it is determined that the fault is within the zone of protection. When the transmission line length is too short, less than 10 miles, distance protection becomes more difficult to coordinate. In these instances the best choice of protection is current differential protection.\n\nThe objective of protection is to remove only the affected portion of plant and nothing else. A circuit breaker or protection relay may fail to operate. In important systems, a failure of primary protection will usually result in the operation of back-up protection. Remote back-up protection will generally remove both the affected and unaffected items of plant to clear the fault. Local back-up protection will remove the affected items of the plant to clear the fault.\n\nThe low-voltage network generally relies upon fuses or low-voltage circuit breakers to remove both overload and earth faults.\n\nProtective device coordination is the process of determining the \"best fit\" timing of current interruption when abnormal electrical conditions occur. The goal is to minimize an outage to the greatest extent possible. Historically, protective device coordination was done on translucent log–log paper. Modern methods normally include detailed computer based analysis and reporting.\n\nProtection coordination is also handled through dividing the power system into protective zones. If a fault were to occur in a given zone, necessary actions will be executed to isolate that zone from the entire system. Zone definitions account for generators, buses, transformers, transmission and distribution lines, and motors. Additionally, zones possess the following features: zones overlap, overlap regions denote circuit breakers, and all circuit breakers in a given zone with a fault will open in order to isolate the fault. Overlapped regions are created by two sets of instrument transformers and relays for each circuit breaker. They are designed for redundancy to eliminate unprotected areas; however, overlapped regions are devised to remain as small as possible such that when a fault occurs in an overlap region and the two zones which encompass the fault are isolated, the sector of the power system which is lost from service is still small despite two zones being isolated.\n\nDisturbance-monitoring equipment (DME) monitors and records system data pertaining to a fault. DME accomplish three main purposes:\nDME devices include: \n\nProtection engineers define dependability as the tendency of the protection system to operate correctly for in-zone faults. They define security as the tendency not to operate for out-of-zone faults. Both dependability and security are reliability issues. Fault tree analysis is one tool with which a protection engineer can compare the relative reliability of proposed protection schemes. Quantifying protection reliability is important for making the best decisions on improving a protection system, managing dependability versus security tradeoffs, and getting the best results for the least money. A quantitative understanding is essential in the competitive utility industry.\nPerformance and design criteria for system-protection devices include reliability, selectivity, speed, cost, and simplicity. \n\n\n\n"}
{"id": "4014139", "url": "https://en.wikipedia.org/wiki?curid=4014139", "title": "Richard Jones (U.S. diplomat)", "text": "Richard Jones (U.S. diplomat)\n\nRichard Henry Jones (born August 26, 1950) is an American diplomat and the former Deputy Executive Director of the International Energy Agency.\n\nJones is a career Foreign Service Officer and member of the Senior Foreign Service. He has served as United States Ambassador to Israel (2005–2008), Senior Advisor to the Secretary of State and Coordinator for Iraq Policy (February–September 2005), Chief Policy Officer and Deputy Administrator for the Coalition Provisional Authority in Baghdad (November 2013 - June 2014), Ambassador to Kuwait (2001–2004), Ambassador to Kazakhstan (1998–2000), and Ambassador to Lebanon (1996–1998).\n\nJones was born at Barksdale Air Force Base in Bossier Parish, Louisiana. He received his Bachelor of Science degree with distinction in mathematics from Harvey Mudd College in Claremont, California and earned a master's and doctorate in business/statistics from the University of Wisconsin, Madison.\n\nJones has been twice posted to the embassy in Riyadh and has also served in Paris and Tunis and was director of the Division of Developed Country Trade in the Office of Egyptian Affairs (1987–1989) in the State Department, and later director of the Office of Egyptian Affairs.\n\nJones served as ambassador to Lebanon from February 1996 until July 1998 and ambassador to Kazakhstan from December 1998 until July 2001. He served as ambassador to Kuwait from September 2001 until July 2004. From November 2003 until June 2004, Jones served concurrently as Chief Policy Officer and Deputy Administrator for the Coalition Provisional Authority in Baghdad. He worked in Kuwait to enlisting Kuwaiti support for the Iraq War and worked under L. Paul Bremer to implement the November 15, 2003 Agreement with the Iraqi Governing Council. Jones was a senior fellow at the Belfer Center for Science and International Affairs at Harvard University's John F. Kennedy School of Government from September 2004 until January 2005.\n\nIn February 2005 Jones was named Senior Advisor to the Secretary of State and Coordinator for Iraq Policy (S/I) the highest-ranking State Department post focused entirely on Iraq policy. Jones chaired an Under Secretary of State-level interagency steering group charged with reviewing and developing Iraq policy and led U.S. diplomatic efforts on Iraq with the international community, including preparations for the June 22, 2005 Iraq International Conference in Brussels.\n\nJones was sworn in as ambassador to Israel by Deputy Secretary of State Robert Zoellick on September 6, 2005. He left that position on August 1, 2008.\n\nJones served as the deputy executive director of the International Energy Agency, based in Paris from 1 October 2008 until end of September 2013.\n\nJones most recently served as Chargé d'affaires ad interim at the U.S. Embassy in Beirut, Lebanon from November 2015 until June 2016.\n\nHe is fluent in Arabic, French, German, and Russian. He served two terms on the board of the Saudi Arabian International School in Riyadh. He enjoys hiking and bicycling, as well as winter and racquet sports.\n\nJones married Joan Wiener in 1973 and has four children: Josh (1977), Vera (1980), Ben (1991), and Hope (1992).\n\n"}
{"id": "39157279", "url": "https://en.wikipedia.org/wiki?curid=39157279", "title": "Set inversion", "text": "Set inversion\n\nIn mathematics, set inversion is the problem of characterizing the preimage \"X\" of a set \"Y\" by a function \"f\", i.e., \"X\" = \"f\"(\"Y\") = {\"x\" ∈ R | \"f\"(\"x\") ∈ \"Y\"}. It can also be viewed as the problem of describing the solution set of the quantified constraint \"Y(f(x))\", where Y(y) is a constraint, for example, an inequality, describing the set Y.\n\nIn most applications, \"f\" is a function from R to R and the set \"Y\" is a box of R (i.e. a Cartesian product of \"p\" intervals of R). \n\nWhen \"f\" is nonlinear the set inversion problem can be solved \nusing interval analysis combined with a branch-and-bound algorithm.\nThe main idea consists in building a paving of R made with non-overlapping boxes. For each box [\"x\"], we perform the following tests:\n\nTo check the two first tests, we need an interval extension (or an inclusion function) [\"f\"] for \"f\". Classified boxes are stored into subpavings, i.e., union of non overlapping boxes. \nThe algorithm can be made more efficient by replacing the inclusion tests by contractors.\n\nThe set \"X\" = \"f\"([4,9]) where \"f\"(\"x\", \"x\") = \"x\" + \"x\" is represented on the figure. \n\nFor instance, since [−2,1] + [4,5] = [0,4] + [16,25] = [16,29] does not intersect the interval [4,9], we conclude that the box [-2,1] × [4,5] is outside \"X\". Since [−1,1] + [2,] = [0,1] + [4,5] = [4,6] is inside [4,9], we conclude that the whole box [-1,1] × [2,] is inside \"X\". \n\nSet inversion is mainly used for path planning, for nonlinear parameter set estimation \n\n, for localization \nor for the characterization of stability domains of linear dynamical systems.\n"}
{"id": "34209756", "url": "https://en.wikipedia.org/wiki?curid=34209756", "title": "Slovenian coal power station referendum, 1999", "text": "Slovenian coal power station referendum, 1999\n\nA referendum on building a coal-fired power plant was held in Slovenia on 10 January 1999. Voters were asked whether they approved of the construction of the TET3 coal-fired power plant. The proposal was rejected by 79.8% of voters, although voter turnout was just 27.3%.\n"}
{"id": "50320485", "url": "https://en.wikipedia.org/wiki?curid=50320485", "title": "Space geodesy", "text": "Space geodesy\n\nSpace geodesy is geodesy by means of sources external to Earth, mainly artificial satellites (in satellite geodesy) but also quasars (in very-long-baseline interferometry, VLBI) and the retroreflectors on the Moon (in lunar laser ranging, LLR). \n"}
{"id": "42226706", "url": "https://en.wikipedia.org/wiki?curid=42226706", "title": "The Oval Gasholders", "text": "The Oval Gasholders\n\nThe Oval Gasholders is the unofficial name given to the gas holder (gasometer) located near The Oval in London, England. Construction began in 1853 and the site is officially called Kennington Holder Station by its owners, Southern Gas Network. It is a grade II listed building with the listed part of them known as Gasholder No. 1.\n\nThe Phoenix Gas Light & Coke Company bought the site north of the Oval in 1845 from the Southwark & Vauxhall Waterworks Company, who had laid out the site in 1807 as a waterworks with an engine house and two circular brick-lined reservoirs. The Phoenix Gas Company adapted the circular reservoirs for gasholder tanks and erected five gasometers between 1847 and 1874. \n\nThe first iron gasholder was installed on the site in 1847 to service a gasworks next to Vauxhall Bridge. The Phoenix Gas Light and Coke Company replaced it in 1877–79 to designs by Sir Corbet Woodall, with two lifts holding 3 million cubic ft, making it the largest gasholder in the world. It was enlarged again by Frank Livesey in 1891–92 for the South Metropolitan Gas Company, by modifying the guides to increase their height by 50%, and adding two more lifts, including a \"flying lift\" rising above the guides, doubling its capacity to 6 million cubic ft. It is an early use of wrought iron in a frame. This structure, Gasholder 1, is Grade II listed by virtue of it: a) being the world's largest gasholder at the time, b) its famous designers Frank and George Livesey and c) it being an internationally renowned backdrop to matches at the adjacent Oval Cricket Ground. The cylindrical wrought iron guide frame is about high, with 24 t-section lattice standards, rising in three tiers and connected by three rows of horizontal lattice girders, with a rising \"bell\" of four tiers, including the top \"flying lift\" rising above the standards. It would have topped when full.\n\nA second gasholder, No.2, was constructed in 1854-5 and converted to a spiral-guided holder in 1950, which does not require a frame. No.3 was constructed in 1869 but demolished c.1975. \n\nThe conjoined pair of Gasholders No. 4 & 5 erected in 1874 and 1876 were designed by Sir Corbet Woodall and are locally listed. They have elegant neo-classical style frames and Tuscan columns. The gasometers were decommissioned in 2014 and the site is due for redevelopment, there are concerns that only the nationally listed No.1 gasometer will be preserved.\n\nThe gasholders overlook The Oval at the Vauxhall End of the ground. They are considered part of the background of The Oval despite being newer than The Oval, which was established in 1846. During England cricket team matches advertising banners hang from the gas holders. They are regularly referred to in Test Match Special broadcasts.\n\nIn 2008 Surrey County Cricket Club, tenants of The Oval, announced plans to redevelop it along the side nearest the gas holders. However, their redevelopment plans were objected to by the Health and Safety Executive because of the proximity to the gas holders and amid fears that they might explode despite not cracking or leaking since construction. This view was supported by London Fire Brigade, who listed the gas holders as a hazardous site. As a result, Surrey considered buying the gas holders and decommissioning them. The objection of the Health and Safety Executive was rejected by the Secretary of State and planning inspector.\n\nIn 1999, plans were announced to dismantle all of the United Kingdom's gasholders due to being made largely redundant due to improvements in gas storage. This was intended to include the Oval Gas Holders. Regeneration of these contaminated brownfield sites can help deliver London with much needed homes. Berkeley Homes and Lambeth Council are working together cooperatively to prepare a Masterplan for an area surrounding the Oval Gas Holders. This is known as the Oval and Kennington Development Area (OAKDA). A preliminary consultation with the local community occurred in January and February 2015 to discover local aspirations. Draft Masterplan proposals will be released to a second consultation towards the end of the year where comments will be welcomed. In March 2012 Lambeth Council locally listed the three gas holders of 1874, 1876 and 1892 (No.4, No.5 and No.1). In 2015, Lawrence D'Silva, a 26 year old local resident, presented a case to Historic England to make the gasholders listed buildings. Subsequently, Historic England assessed them all for national listing and in March 2016 decided to grant Grade II listed status to just one of the Oval Gasholders – No.1 dating from 1892. The national listing designation supersedes the local listing status of this gasometer so Gasholder No. 1 was subsequently removed from the Local list by Lambeth – Nos. 4 & 5 remain locally listed. There is a strong presumption in the planning decision-making process against the demolition of a nationally listed heritage asset.\n"}
{"id": "1744973", "url": "https://en.wikipedia.org/wiki?curid=1744973", "title": "Transmission line measurement", "text": "Transmission line measurement\n\nTransmission line measurement or Transfer Length Measurement is a technique used in semiconductor physics and engineering to determine the contact resistance between a metal and a semiconductor. The technique involves making a series of metal-semiconductor contacts separated by various distances. Probes are applied to pairs of contacts, and the resistance between them is measured by applying a voltage across the contacts and measuring the resulting current. The current flows from the first probe, into the metal contact, across the metal-semiconductor junction, through the sheet of semiconductor, across the metal-semiconductor junction again (except this time in the other direction), into the second contact, and from there into the second probe and into the external circuit to be measured by an ammeter. The resistance measured is a linear combination (sum) of the contact resistance of the first contact, the contact resistance of the second contact, and the sheet resistance of the semiconductor in-between the contacts.\n\nIf several such measurements are made between pairs of contacts that are separated by different distances, a plot of resistance versus contact separation can be obtained. If the contact separation is expressed in terms of the ratio L/W - where L and W are the length and width of the area between the contacts - such a plot should be linear, with the slope of the line being the sheet resistance. The intercept of the line with the y-axis, is two times the contact resistance. Thus the sheet resistance as well as the contact resistance can be determined from this technique.\n"}
{"id": "1393189", "url": "https://en.wikipedia.org/wiki?curid=1393189", "title": "Valeric acid", "text": "Valeric acid\n\nValeric acid, or pentanoic acid, is a straight-chain alkyl carboxylic acid with the chemical formula . Like other low-molecular-weight carboxylic acids, it has a very unpleasant odor. It is found naturally in the perennial flowering plant valerian (\"Valeriana officinalis\"), from which it gets its name. Its primary use is in the synthesis of its esters. Salts and esters of valeric acid are known as valerates or pentanoates. Volatile esters of valeric acid tend to have pleasant odors and are used in perfumes and cosmetics. Ethyl valerate and pentyl valerate are used as food additives because of their fruity flavors.\n\nValeric acid appears similar in structure to GHB and the neurotransmitter GABA in that it is a short-chain carboxylic acid, although it lacks the alcohol and amine functional groups that contribute to the biological activities of GHB and GABA, respectively. It differs from valproic acid simply by lacking a 3-carbon side-chain. Mevalonic acid is derived from valeric acid by methylation and hydroxylation.\n\nValeric acid can cause irritation if it comes into contact with the skin, eyes, or mucous membranes.\n\n"}
{"id": "9569619", "url": "https://en.wikipedia.org/wiki?curid=9569619", "title": "Vapor quality", "text": "Vapor quality\n\nIn thermodynamics, vapour quality is the mass fraction in a saturated mixture that is vapour; in other words, saturated vapour has a \"quality\" of 100%, and saturated liquid has a \"quality\" of 0%. Vapour quality is an intensive property which can be used in conjunction with other independent intensive properties to specify the thermodynamic state of the working fluid of a thermodynamic system. It has no meaning for substances which are not saturated mixtures (for example, compressed liquids or superheated fluids).\nVapor quality is an important quantity during the adiabatic expansion step in various thermodynamic cycles (like Organic Rankine cycle, Rankine cycle, etc.). Working fluids can be classified by using the appearance of droplets in the vapour suring the expansion step.\n\nQuality formula_1 can be calculated by dividing the mass of the vapour by the mass of the total mixture:\nwhere formula_3 indicates mass.\n\nAnother definition used by chemical engineers defines quality (q) of a fluid as the fraction that is saturated liquid. By this definition, a saturated vapour has q = 1. A saturated liquid has q = 0.\n\nAn alternative definition is the 'equilibrium thermodynamic quality'. It can be used only for single-component mixtures (e.g. water with steam), and can take values <0 (for sub-cooled fluids) and >1 (for super-saturated vapours):\n\nformula_4; where h is the mixture specific enthalpy, defined as:\n\nformula_5.\n\nSubscripts f and g refer to saturated liquid and saturated gas respectively, and fg refers to vaporisation.\n\nThe above expression for vapour quality can be expressed as:\nwhere formula_7 is equal to either specific enthalpy, specific entropy, specific volume or specific internal energy, formula_8 is the value of the specific property of saturated liquid state and formula_9 is the value of the specific property of the substance in dome zone, which we can find both liquid formula_10 and vapor formula_11.\n\nAnother expression of the same concept is:\nwhere formula_13 is the vapour mass and formula_14 is the liquid mass.\n\nThe origin of the idea of vapour quality was derived from the origins of thermodynamics, where an important application was the steam engine. Low quality steam would contain a high moisture percentage and therefore damage components more easily. High quality steam would not corrode the steam engine. Steam engines use water vapour (steam) to push pistons or turbines, and that movement creates work. The quantitatively described \"steam quality\" (steam dryness) is the proportion of saturated steam in a saturated water/steam mixture. In other words, a steam quality of 0 indicates 100% water while a steam quality of 1 (or 100%) indicates 100% steam.\n\nThe quality of steam on which steam whistles are blown is variable and may affect frequency. Steam quality determines the velocity of sound, which declines with decreasing dryness due to the inertia of the liquid phase. Also, the specific volume of steam for a given temperature decreases with decreasing dryness.\n\nSteam quality is very useful in determining enthalpy of saturated water/steam mixtures since the enthalpy of steam (gaseous state) is many orders of magnitude higher than enthalpy of water (liquid state).\n"}
{"id": "20646971", "url": "https://en.wikipedia.org/wiki?curid=20646971", "title": "Waste", "text": "Waste\n\nWaste (or wastes) are unwanted or unusable materials. Waste is any substance which is discarded after primary use, or is worthless, defective and of no use.\n\nExamples include municipal solid waste (household trash/refuse), hazardous waste, wastewater (such as sewage, which contains bodily wastes (feces and urine) and surface runoff), radioactive waste, and others.\n\nAccording to the Basel Convention on the Control of Transboundary Movements of Hazardous Wastes and Their Disposal of 1989, Art. 2(1), \"'Wastes' are substance or objects, which are disposed of or are intended to be disposed of or are required to be disposed of by the provisions of national law\".\n\nThe UNSD \"Glossary of Environment Statistics\" describes waste as \"materials that are not prime products (that is, products produced for the market) for which the generator has no further use in terms of his/her own purposes of production, transformation or consumption, and of which he/she wants to dispose. Wastes may be generated during the extraction of raw materials, the processing of raw materials into intermediate and final products, the consumption of final products, and other human activities. Residuals recycled or reused at the place of generation are excluded.\"\n\nUnder the Waste Framework Directive 2008/98/EC, Art. 3(1), the European Union defines waste as \"an object the holder discards, intends to discard or is required to discard.\" For a more structural description of the Waste Directive, see the European Commission's summary.\n\nThere are many waste types defined by modern systems of waste management, notably including:\n\nThere are many issues that surround reporting waste. It is most commonly measured by size or weight, and there is a stark difference between the two. For example, organic waste is much heavier when it is wet, and plastic or glass bottles can have different weights but be the same size. On a global scale it is difficult to report waste because countries have different definitions of waste and what falls into waste categories, as well as different ways of reporting. Based on incomplete reports from its parties, the Basel Convention estimated 338 million tonnes of waste was generated in 2001. For the same year, OECD estimated 4 billion tonnes from its member countries. Despite these inconsistencies, waste reporting is still useful on a small and large scale to determine key causes and locations, and to find ways of preventing, minimizing, recovering, treating, and disposing waste.\n\nInappropriately managed waste can attract rodents and insects, which can harbour gastrointestinal parasites, yellow fever, worms, the plague and other conditions for humans, and exposure to hazardous wastes, particularly when they are burned, can cause various other diseases including cancers. Toxic waste materials can contaminate surface water, groundwater, soil, and air which causes more problems for humans, other species, and ecosystems. Waste treatment and disposal produces significant green house gas (GHG) emissions, notably methane, which are contributing significantly to global warming.\n\nWaste management is a significant environmental justice issue. Many of the environmental burdens cited above are more often borne by marginalized groups, such as racial minorities, women, and residents of developing nations. NIMBY (not in my back yard) is the opposition of residents to a proposal for a new development because it is close to them. However, the need for expansion and siting of waste treatment and disposal facilities is increasing worldwide. There is now a growing market in the transboundary movement of waste, and although most waste that flows between countries goes between developed nations, a significant amount of waste is moved from developed to developing nations.\n\nThe economic costs of managing waste are high, and are often paid for by municipal governments; money can often be saved with more efficiently designed collection routes, modifying vehicles, and with public education. Environmental policies such as pay as you throw can reduce the cost of management and reduce waste quantities. Waste recovery (that is, recycling, reuse) can curb economic costs because it avoids extracting raw materials and often cuts transportation costs. \"Economic assessment of municipal waste management systems – case studies using a combination of life-cycle assessment (LCA) and life-cycle costing (LCC)\". The location of waste treatment and disposal facilities often reduces property values due to noise, dust, pollution, unsightliness, and negative stigma. The informal waste sector consists mostly of waste pickers who scavenge for metals, glass, plastic, textiles, and other materials and then trade them for a profit. This sector can significantly alter or reduce waste in a particular system, but other negative economic effects come with the disease, poverty, exploitation, and abuse of its workers.\n\nResource recovery is the retrieval of recyclable waste, which was intended for disposal, for a specific next use. It is the processing of recyclables to extract or recover materials and resources, or convert to energy. This process is carried out at a resource recovery facility. Resource recovery is not only important to the environment, but it can be cost effective by decreasing the amount of waste sent to the disposal stream, reduce the amount of space needed for landfills, and protect limited natural resources.\n\nEnergy recovery from waste is using non-recyclable waste materials and extracting from it heat, electricity, or energy through a variety of processes, including combustion, gasification, pyrolyzation, and anaerobic digestion. This process is referred to as waste-to-energy.\n\nThere are several ways to recover energy from waste. Anaerobic digestion is a naturally occurring process of decomposition where organic matter is reduced to a simpler chemical component in the absence of oxygen. Incineration or direct controlled burning of municipal solid waste to reduce waste and make energy. Secondary recovered fuel is the energy recovery from waste that cannot be reused or recycled from mechanical and biological treatment activities. Pyrolysis involves heating of waste, with the absence of oxygen, to high temperatures to break down any carbon content into a mixture of gaseous and liquid fuels and solid residue. Gasification is the conversion of carbon rich material through high temperature with partial oxidation into a gas stream. Plasma arc heating is the very high heating of municipal solid waste to temperatures ranging from 3,000-10,000 °C, where energy is released by an electrical discharge in an inert atmosphere.\n\nUsing waste as fuel can offer important environmental benefits. It can provide a safe and cost-effective option for wastes that would normally have to be dealt with through disposal. It can help reduce carbon dioxide emissions by diverting energy use from fossil fuels, while also generating energy and using waste as fuel can reduce the methane emissions generated in landfills by averting waste from landfills.\n\nThere is some debate in the classification of certain biomass feedstock as wastes. Crude Tall Oil (CTO), a co-product of the pulp and papermaking process, is defined as a waste or residue in some European countries when in fact it is produced “on purpose” and has significant value add potential in industrial applications. Several companies use CTO to produce fuel, while the pine chemicals industry maximizes it as a feedstock “producing low-carbon, bio-based chemicals” through cascading use.\n\nEducation and awareness in the area of waste and waste management is increasingly important from a global perspective of resource management. The Talloires Declaration is a declaration for sustainability concerned about the unprecedented scale and speed of environmental pollution and degradation, and the depletion of natural resources. Local, regional, and global air pollution; accumulation and distribution of toxic wastes; destruction and depletion of forests, soil, and water; depletion of the ozone layer and emission of \"green house\" gases threaten the survival of humans and thousands of other living species, the integrity of the earth and its biodiversity, the security of nations, and the heritage of future generations. Several universities have implemented the Talloires Declaration by establishing environmental management and waste management programs, e.g. the waste management university project. University and vocational education are promoted by various organizations, e.g. WAMITAB and Chartered Institution of Wastes Management.\n\n\n"}
{"id": "6375794", "url": "https://en.wikipedia.org/wiki?curid=6375794", "title": "Water's Journey: The Hidden Rivers of Florida", "text": "Water's Journey: The Hidden Rivers of Florida\n\nWater's Journey: The Hidden Rivers of Florida is a documentary film that tracks the path of water through the Floridan aquifer, where a team reveals the journey of water above and within the earth. Viewers are transported through a world that reveals how their lives are intertwined with the water they drink.\n\nThe documentary features footage of cave diving.\n"}
{"id": "1554012", "url": "https://en.wikipedia.org/wiki?curid=1554012", "title": "Weigh in motion", "text": "Weigh in motion\n\nWeigh-in-motion or weighing-in-motion (WIM) devices are designed to capture and record the axle weights and gross vehicle weights as vehicles drive over a measurement site. Unlike static scales, WIM systems are capable of measuring vehicles traveling at a reduced or normal traffic speed and do not require the vehicle to come to a stop. This makes the weighing process more efficient, and, in the case of commercial vehicles, allows for trucks under the weight limit to bypass static scales or inspection.\n\nEspecially for trucks gross vehicle and axle weight monitoring is useful in an array of applications including:\nWeigh in motion scales are often used for size and weight enforcement, such as the Federal Motor Carrier Safety Administration's Commercial Vehicle Information Systems and Networks program. Weigh-in-motion systems can be used as part of traditional roadside inspection stations, or as part of virtual inspection stations.\n\nRecent years have seen the rise of several \"specialty\" Weigh-in-Motion systems. One popular example is the front fork garbage truck scale. In this application, a container is weighed—while it is full—as the driver lifts, and again—while it is empty—as the container is returned to the ground. The difference between the full and empty weights is equal to the weight of the contents.\n\nWeigh-in-free-flow is system which can accurately weigh vehicles in full road width independently of the instantaneous position of the wheel during crossing. This system solves the limitations of current weigh-in-motion technologies on the roads as for example not sufficient measurement on the road ends. This free-flow solution is possible thanks to usage of optical weighing sensors which covers the full width of the road and are installed in one or two lines for the highest possible accuracy. This system is not possible to circumvent and therefore could be called the new generation of weigh-in-motion. \n\nWIM systems can employ various types of sensors for measurement. The most important quantity to measure is the vertical force (z component) without any influence of forces in other directions or speed of the vehicle that passes by. Force sensors with quartz crystals are the most rigid and measure only in one direction along the vertical axis. When a force is applied to the top surface of the sensor, quartz crystals produce an electric charge proportional to the applied force. The signal is a very high impedance electric charge, which is not susceptible to electrical interference.\n\nHigh impedance charge signals are amplified with MOSFET based charge amplifiers and converted to a voltage output, which is connected to analysis system..\n\nInductive loops define the vehicle entry and exit from the WIM station. These signals are used as triggering inputs to start and stop the measurement to initiate totaling gross vehicle weight of each vehicle. For toll gate or low speed applications, inductive loops may be replaced by other types of vehicle sensors such as light curtains, axle sensors or piezocables.\n\nThe high speed measurement system is programmed to perform calculations of the following parameters:\n\nAxle distances,\nIndividual axle weights,\nGross Vehicle Weight, \nVehicle Speed,\nDistance between vehicles,\nand the GPS synchronized time stamp for each vehicle measurement\n\nThe measurement system should be environmentally protected, should have a wide operating temperature range and withstand condensation.\n\nVariety of communication methods need to be installed on the measurement system. A modem or cellular modem can be provided. If no communication infrastructure exists, WIM systems can be self-operating while saving the data, to later physically retrieve it.\n\nA WIM system connected with any available communication means can be connected to a central monitoring server. Automatic data archiving software is required to retrieve the data from many remote WIM stations to be available for any further processing. A central database can be built to link many WIMs to a server for variety of monitoring and enforcement purposes.\n\nWeighing in motion is also a common application in rail transport. \nKnown applications are\n\n\nThere are two main parts to the measurement system: the track-side component, which contains hardware for communication, power, computation, and data acquisition, and the rail-mounted component, which consists of sensors and cabling. Known sensor principles include:\n\nTrains are weighed, either on the main line or at yards. \nWeighing in Motion systems installed on the main lines measure the complete weight (distribution) of the trains as they pass by at the designated line speed. Weighing in motion on the mainline is therefore also referred to as \"coupled-in-motion weighing\": all of the railcars are coupled. \nWeighing in motion at yards often measure individual wagons. It requires that the railcar are uncoupled on both ends in order to weigh. Weighing in motion at yards is therefore also referred to as \"uncoupled-in-motion weighing\". Systems installed at yards usually works at lower speeds and are capable of higher accuracies.\n\nSome airports use airplane weighing, whereby the plane taxis across the scale bed, and its weight is measured. The weight may then be used to correlate with the pilot's log entry, to ensure there is just enough fuel, with a little margin for safety. This has been used for some time to conserve jet fuel.\n\nAlso, the main difference in these platforms, which are basically a \"transmission of weight\" application, there are checkweighers, also known as dynamic scales or in-motion scales.\n\n6. WEIGH-IN-FREE-FLOW"}
{"id": "10419699", "url": "https://en.wikipedia.org/wiki?curid=10419699", "title": "World LPG Association", "text": "World LPG Association\n\nThe World LPG Association (WLPGA) is the authoritative global voice for the liquefied petroleum gas (LPG) industry and the worldwide industry association which represents the interests of the LPG industry globally. The WLPGA promotes the use of LPG to foster a cleaner, healthier and more prosperous world.\n\nThe Association's mission is to:\n\n\nWith over 200 members headquartered in more than 125 countries, the WLPGA represents the interests of private and public companies from the entire LPG value chain under one umbrella.\n\nThe WLPGA provides a platform for the exchange of best practices, facts and figures among its members. The Association hosts the annual World LPG Forum which regularly unites over 2,000 delegates, hundreds of visitors and exhibitors to network and share best practices.\n\nThe Association regularly organises interactive meetings between technical experts, members and key stakeholders to demonstrate the benefits of LPG.\n\nThrough partnerships with international bodies like the United Nations and the World Bank, the WLPGA is committed to bringing energy to the poor in developing countries. Together with local LPG associations and members it addresses major issues with key stakeholders such as policy-makers.\n\nThe WLPGA was officially granted Consultative Status with the United Nations Economic and Social Council in 1989. Publications such as its Annual Statistical Review have become LPG industry references.\n\nThe WLPGA has six major programmes, all aimed at enhancing awareness of LPG as a clean, all-purpose and efficient energy source.\n\nPromoting the importance of good safety and business practices in the industry through targeted publications and lobbying activities.\n\nThe Association facilitates information exchange and communication among all LPG stakeholders.\n\nThe WLPGA identifies new market opportunities for LPG, in both rural and urban settings, and supports the growth and development of existing markets.\n\nThe Association builds strong relationships with key intergovernmental agencies and business associations through participating on worldwide forums and co-operating on industry reports.\n\nThe Global Autogas Industry Network, (GAIN) is a communications and information network particularly dedicated to the global Autogas community.\n\nGLOTEC supports industry growth through the use of new technologies as well as collecting and disseminating information on the use of new technologies to members.\n\nThe WLPGA Annual World LPG Forum is arguably the largest LPG industry conference and exhibition in the world, and the only which unites industry players from across the globe. Through the World Forum, the Association provides an opportunity for attendees to share best practices, acquire knowledge, and network with LPG industry leaders. The World Forum travels the globe and is held in a different continent each year.\n\n\n"}
{"id": "12176524", "url": "https://en.wikipedia.org/wiki?curid=12176524", "title": "Ōma Nuclear Power Plant", "text": "Ōma Nuclear Power Plant\n\nThe is a nuclear plant under construction in Ōma, Aomori, Japan. \nIt will be operated by the Electric Power Development Company (J-Power). \nThe reactor would be unique for Japan in that it would be capable of using a 100% MOX fuel core, as requested by the 1995 decision by the Japanese Atomic Energy Commission. \nThe fuel would utilize surplus plutonium by blending it with natural uranium, reducing the total radioactivity of nuclear waste and dramatically reducing the waste's lifetime.\n\nIn 2008, J-Power announced a 2.5-year delay to allow for additional work to make the plant resistant to a strong earthquake, making the operation start date in November 2014.\nFollowing the Fukushima Daiichi nuclear disaster of March 2011 construction at Oma was suspended for 18 months. \nWork was resumed on October 2012. \nOn March 2013, the main reactor building was at its full height.\n\nIn December 2014 J-Power applied for safety checks at the Oma nuclear plant, slated for startup in 2021.\n"}
