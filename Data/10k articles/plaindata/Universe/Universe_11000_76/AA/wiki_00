{"id": "8620216", "url": "https://en.wikipedia.org/wiki?curid=8620216", "title": "1,2,4-Butanetriol trinitrate", "text": "1,2,4-Butanetriol trinitrate\n\n1,2,4-Butanetriol trinitrate (BTTN), also called butanetriol trinitrate, is an important military propellant. It is a colorless to brown explosive liquid.\n\nBTTN is used as a propellant in virtually all single-stage missiles used by the United States, including the Hellfire. It is less volatile, less sensitive to shock, and more thermally stable than nitroglycerine, for which it is a promising replacement.\n\nBTTN as a propellant is often used in a mixture with nitroglycerin. The mixture can be made by co-nitration of butanetriol and glycerol. BTTN is also used as a plasticizer in some nitrocellulose-based propellants.\n\nBTTN is manufactured by nitration of 1,2,4-butanetriol. Biotechnological manufacture of butanetriol is under intensive research.\n\n"}
{"id": "19995323", "url": "https://en.wikipedia.org/wiki?curid=19995323", "title": "A. W. Kuchler", "text": "A. W. Kuchler\n\nAugust William Kuchler (born \"August Wilhelm Küchler\"; 1907–1999) was a German-born American geographer and naturalist who is noted for developing a plant association system in widespread use in the United States. Some of this database has become digitized for integration into GIS mapping systems. Kuchler received his Ph.D. in geography from University of Munich in 1935. In 1978, he received the Association of American Geographers' Honors award. He is the publisher of the book \"Vegetation Mapping\" \n\n\n"}
{"id": "1412472", "url": "https://en.wikipedia.org/wiki?curid=1412472", "title": "A Laboratory Manual for Comparative Vertebrate Anatomy", "text": "A Laboratory Manual for Comparative Vertebrate Anatomy\n\nA Laboratory Manual for Comparative Vertebrate Anatomy is a textbook written by Libbie Hyman in 1922 and released as the first edition from the University of Chicago press. It is also called and published simply as Comparative Vertebrate Anatomy. In 1942 Hyman released the second edition as a textbook, as well as a laboratory manual. It was referred to as her 'bread and butter', as she relied on its royalties for income. The \"Laboratory Manual for Comparative Vertebrate Anatomy\" still remains the same without revisions, and is used by universities around the world. In the book, she uses \"Balanoglossus\", \"Amphioxus\", sea squirt, lamprey, skate, shark, turtle, alligator, chicken, and cat as specimens.\n"}
{"id": "2819", "url": "https://en.wikipedia.org/wiki?curid=2819", "title": "Aerodynamics", "text": "Aerodynamics\n\nAerodynamics, from Greek ἀήρ \"aer\" (air) + δυναμική (dynamics), is the study of the motion of air, particularly its interaction with a solid object, such as an airplane wing. It is a sub-field of fluid dynamics and gas dynamics, and many aspects of aerodynamics theory are common to these fields. The term \"aerodynamics\" is often used synonymously with gas dynamics, the difference being that \"gas dynamics\" applies to the study of the motion of all gases, and is not limited to air. \nThe formal study of aerodynamics began in the modern sense in the eighteenth century, although observations of fundamental concepts such as aerodynamic drag were recorded much earlier. Most of the early efforts in aerodynamics were directed toward achieving heavier-than-air flight, which was first demonstrated by Otto Lilienthal in 1891. Since then, the use of aerodynamics through mathematical analysis, empirical approximations, wind tunnel experimentation, and computer simulations has formed a rational basis for the development of heavier-than-air flight and a number of other technologies. Recent work in aerodynamics has focused on issues related to compressible flow, turbulence, and boundary layers and has become increasingly computational in nature.\n\nModern aerodynamics only dates back to the seventeenth century, but aerodynamic forces have been harnessed by humans for thousands of years in sailboats and windmills, and images and stories of flight appear throughout recorded history, such as the Ancient Greek legend of Icarus and Daedalus. Fundamental concepts of continuum, drag, and pressure gradients appear in the work of Aristotle and Archimedes.\n\nIn 1726, Sir Isaac Newton became the first person to develop a theory of air resistance, making him one of the first aerodynamicists. Dutch-Swiss mathematician Daniel Bernoulli followed in 1738 with \"Hydrodynamica\" in which he described a fundamental relationship between pressure, density, and flow velocity for incompressible flow known today as Bernoulli's principle, which provides one method for calculating aerodynamic lift. In 1757, Leonhard Euler published the more general Euler equations which could be applied to both compressible and incompressible flows. The Euler equations were extended to incorporate the effects of viscosity in the first half of the 1800s, resulting in the Navier–Stokes equations. The Navier-Stokes equations are the most general governing equations of fluid flow and but are difficult to solve for the flow around all but the simplest of shapes.\n\nIn 1799, Sir George Cayley became the first person to identify the four aerodynamic forces of flight (weight, lift, drag, and thrust), as well as the relationships between them, and in doing so outlined the path toward achieving heavier-than-air flight for the next century. In 1871, Francis Herbert Wenham constructed the first wind tunnel, allowing precise measurements of aerodynamic forces. Drag theories were developed by Jean le Rond d'Alembert, Gustav Kirchhoff, and Lord Rayleigh. In 1889, Charles Renard, a French aeronautical engineer, became the first person to reasonably predict the power needed for sustained flight. Otto Lilienthal, the first person to become highly successful with glider flights, was also the first to propose thin, curved airfoils that would produce high lift and low drag. Building on these developments as well as research carried out in their own wind tunnel, the Wright brothers flew the first powered airplane on December 17, 1903.\n\nDuring the time of the first flights, Frederick W. Lanchester, Martin Kutta, and Nikolai Zhukovsky independently created theories that connected circulation of a fluid flow to lift. Kutta and Zhukovsky went on to develop a two-dimensional wing theory. Expanding upon the work of Lanchester, Ludwig Prandtl is credited with developing the mathematics behind thin-airfoil and lifting-line theories as well as work with boundary layers.\n\nAs aircraft speed increased, designers began to encounter challenges associated with air compressibility at speeds near or greater than the speed of sound. The differences in air flows under such conditions leds to problems in aircraft control, increased drag due to shock waves, and the threat of structural failure due to aeroelastic flutter. The ratio of the flow speed to the speed of sound was named the Mach number after Ernst Mach who was one of the first to investigate the properties of supersonic flow. William John Macquorn Rankine and Pierre Henri Hugoniot independently developed the theory for flow properties before and after a shock wave, while Jakob Ackeret led the initial work of calculating the lift and drag of supersonic airfoils. Theodore von Kármán and Hugh Latimer Dryden introduced the term transonic to describe flow speeds around Mach 1 where drag increases rapidly. This rapid increase in drag led aerodynamicists and aviators to disagree on whether supersonic flight was achievable until the sound barrier was broken for the first time in 1947 using the Bell X-1 aircraft.\n\nBy the time the sound barrier was broken, aerodynamicists' understanding of the subsonic and low supersonic flow had matured. The Cold War prompted the design of an ever-evolving line of high performance aircraft. Computational fluid dynamics began as an effort to solve for flow properties around complex objects and has rapidly grown to the point where entire aircraft can be designed using computer software, with wind-tunnel tests followed by flight tests to confirm the computer predictions. Understanding of supersonic and hypersonic aerodynamics has matured since the 1960s, and the goals of aerodynamicists have shifted from the behavior of fluid flow the engineering of a vehicle such that it interacts pedictably with the fluid flow. Designing aircraft for supersonic and hypersonic conditions, as well as the desire to improve the aerodynamic efficiency of current aircraft and propulsion systems, continues to motivate new research in aerodynamics, while work continues to be done on important problems in basic aerodynamic theory related to flow turbulence and the existence and uniqueness of analytical solutions to the Navier-Stokes equations.\n\nUnderstanding the motion of air around an object (often called a flow field) enables the calculation of forces and moments acting on the object. In many aerodynamics problems, the forces of interest are the fundamental forces of flight: lift, drag, thrust, and weight. Of these, lift and drag are aerodynamic forces, i.e. forces due to air flow over a solid body. Calculation of these quantities is often founded upon the assumption that the flow field behaves as a continuum. Continuum flow fields are characterized by properties such as flow velocity, pressure, density, and temperature, which may be functions of position and time. These properties may be directly or indirectly measured in aerodynamics experiments or calculated starting with the equations for conservation of mass, momentum, and energy in air flows. Density, flow velocity, and an additional property, viscosity, are used to classify flow fields. \n\nFlow velocity is used to classify flows according to speed regime. Subsonic flows are flow fields in which the air speed field is always below the local speed of sound. Transonic flows include both regions of subsonic flow and regions in which the local flow speed is greater than the local speed of sound. Supersonic flows are defined to be flows in which the flow speed is greater than the speed of sound everywhere. A fourth classification, hypersonic flow, refers to flows where the flow speed is much greater than the speed of sound. Aerodynamicists disagree on the precise definition of hypersonic flow.\n\nCompressible flow accounts for varying density within the flow. Subsonic flows are often idealized as incompressible, i.e. the density is assumed to be constant. Transonic and supersonic flows are compressible, and calculations that neglect the changes of density in these flow fields will yield inaccurate results.\n\nViscosity is associated with the frictional forces in a flow. In some flow fields, viscous effects are very small, and approximate solutions may safely neglect viscous effects. These approximations are called inviscid flows. Flows for which viscosity is not neglected are called viscous flows. Finally, aerodynamic problems may also be classified by the flow environment. External aerodynamics is the study of flow around solid objects of various shapes (e.g. around an airplane wing), while internal aerodynamics is the study of flow through passages inside solid objects (e.g. through a jet engine).\n\nUnlike liquids and solids, gases are composed of discrete molecules which occupy only a small fraction of the volume filled by the gas. On a molecular level, flow fields are made up of the collisions of many individual of gas molecules between themselves and with solid surfaces. However, in most aerodynamics applications, the discrete molecular nature of gases is ignored, and the flow field is assumed to behave as a continuum. This assumption allows fluid properties such as density and flow velocity to be defined everywhere within the flow.\n\nThe validity of the continuum assumption is dependent on the density of the gas and the application in question. For the continuum assumption to be valid, the mean free path length must be much smaller than the length scale of the application in question. For example, many aerodynamics applications deal with aircraft flying in atmospheric conditions, where the mean free path length is on the order of micrometers and where the body is orders of magnitude larger. In these cases, the length scale of the aircraft ranges from a few meters to a few tens of meters, which is much larger than the mean free path length. For such applications, the continuum assumption is reasonable. The continuum assumption is less valid for extremely low-density flows, such as those encountered by vehicles at very high altitudes (e.g. 300,000 ft/90 km) or satellites in Low Earth orbit. In those cases, statistical mechanics is a more accurate method of solving the problem than is continuum aerodynamics. The Knudsen number can be used to guide the choice between statistical mechanics and the continuous formulation of aerodynamics.\n\nThe assumption of a fluid continuum allows problems in aerodynamics to be solved using fluid dynamics conservation laws. Three conservation principles are used: \n\nThe ideal gas law or another such equation of state is often used in conjunction with these equations to form a determined system that allows the solution for the unknown variables.\n\nAerodynamic problems are classified by the flow environment or properties of the flow, including flow speed, compressibility, and viscosity. \"External\" aerodynamics is the study of flow around solid objects of various shapes. Evaluating the lift and drag on an airplane or the shock waves that form in front of the nose of a rocket are examples of external aerodynamics. \"Internal\" aerodynamics is the study of flow through passages in solid objects. For instance, internal aerodynamics encompasses the study of the airflow through a jet engine or through an air conditioning pipe.\n\nAerodynamic problems can also be classified according to whether the flow speed is below, near or above the speed of sound. A problem is called subsonic if all the speeds in the problem are less than the speed of sound, transonic if speeds both below and above the speed of sound are present (normally when the characteristic speed is approximately the speed of sound), supersonic when the characteristic flow speed is greater than the speed of sound, and hypersonic when the flow speed is much greater than the speed of sound. Aerodynamicists disagree over the precise definition of hypersonic flow; a rough definition considers flows with Mach numbers above 5 to be hypersonic.\n\nThe influence of viscosity on the flow dictates a third classification. Some problems may encounter only very small viscous effects, in which case viscosity can be considered to be negligible. The approximations to these problems are called inviscid flows. Flows for which viscosity cannot be neglected are called viscous flows.\n\nAn incompressible flow is a flow in which density is constant in both time and space. Although all real fluids are compressible, a flow is often approximated as incompressible if the effect of the density changes cause only small changes to the calculated results. This is more likely to be true when the flow speeds are significantly lower than the speed of sound. Effects of compressibility are more significant at speeds close to or above the speed of sound. The Mach number is used to evaluate whether the incompressibility can be assumed, otherwise the effects of compressibility must be included.\n\nSubsonic (or low-speed) aerodynamics describes fluid motion in flows which are much lower than the speed of sound everywhere in the flow. There are several branches of subsonic flow but one special case arises when the flow is inviscid, incompressible and irrotational. This case is called potential flow and allows the differential equations that describe the flow to be a simplified version of the equations of fluid dynamics, thus making available to the aerodynamicist a range of quick and easy solutions.\n\nIn solving a subsonic problem, one decision to be made by the aerodynamicist is whether to incorporate the effects of compressibility. Compressibility is a description of the amount of change of density in the flow. When the effects of compressibility on the solution are small, the assumption that density is constant may be made. The problem is then an incompressible low-speed aerodynamics problem. When the density is allowed to vary, the flow is called compressible. In air, compressibility effects are usually ignored when the Mach number in the flow does not exceed 0.3 (about 335 feet (102 m) per second or 228 miles (366 km) per hour at 60 °F (16 °C)). Above Mach 0.3, the problem flow should be described using compressible aerodynamics.\n\nAccording to the theory of aerodynamics, a flow is considered to be compressible if the density changes along a streamline. This means that – unlike incompressible flow – changes in density are considered. In general, this is the case where the Mach number in part or all of the flow exceeds 0.3. The Mach 0.3 value is rather arbitrary, but it is used because gas flows with a Mach number below that value demonstrate changes in density of less than 5%. Furthermore, that maximum 5% density change occurs at the stagnation point (the point on the object where flow speed is zero), while the density changes around the rest of the object will be significantly lower. Transonic, supersonic, and hypersonic flows are all compressible flows.\n\nThe term Transonic refers to a range of flow velocities just below and above the local speed of sound (generally taken as Mach 0.8–1.2). It is defined as the range of speeds between the critical Mach number, when some parts of the airflow over an aircraft become supersonic, and a higher speed, typically near Mach 1.2, when all of the airflow is supersonic. Between these speeds, some of the airflow is supersonic, while some of the airflow is not supersonic.\n\nSupersonic aerodynamic problems are those involving flow speeds greater than the speed of sound. Calculating the lift on the Concorde during cruise can be an example of a supersonic aerodynamic problem.\n\nSupersonic flow behaves very differently from subsonic flow. Fluids react to differences in pressure; pressure changes are how a fluid is \"told\" to respond to its environment. Therefore, since sound is in fact an infinitesimal pressure difference propagating through a fluid, the speed of sound in that fluid can be considered the fastest speed that \"information\" can travel in the flow. This difference most obviously manifests itself in the case of a fluid striking an object. In front of that object, the fluid builds up a stagnation pressure as impact with the object brings the moving fluid to rest. In fluid traveling at subsonic speed, this pressure disturbance can propagate upstream, changing the flow pattern ahead of the object and giving the impression that the fluid \"knows\" the object is there by seemingly adjusting its movement and is flowing around it. In a supersonic flow however, the pressure disturbance cannot propagate upstream. Thus, when the fluid finally reaches the object it strikes it and the fluid is forced to change its properties – temperature, density, pressure, and Mach number—in an extremely violent and irreversible fashion called a shock wave. The presence of shock waves, along with the compressibility effects of high-flow velocity (see Reynolds number) fluids, is the central difference between the supersonic and subsonic aerodynamics regimes.\n\nIn aerodynamics, hypersonic speeds are speeds that are highly supersonic. In the 1970s, the term generally came to refer to speeds of Mach 5 (5 times the speed of sound) and above. The hypersonic regime is a subset of the supersonic regime. Hypersonic flow is characterized by high temperature flow behind a shock wave, viscous interaction, and chemical dissociation of gas.\n\nThe incompressible and compressible flow regimes produce many associated phenomena, such as boundary layers and turbulence.\n\nThe concept of a boundary layer is important in many problems in aerodynamics. The viscosity and fluid friction in the air is approximated as being significant only in this thin layer. This assumption makes the description of such aerodynamics much more tractable mathematically.\n\nIn aerodynamics, turbulence is characterized by chaotic property changes in the flow. These include low momentum diffusion, high momentum convection, and rapid variation of pressure and flow velocity in space and time. Flow that is not turbulent is called laminar flow.\n\nAerodynamics is a significant factor in vehicle design, including automobiles, and in the prediction of forces and moments acting on sailing vessels. It is used in the design of mechanical components such as hard drive heads. Structural engineers resort to aerodynamics, and particularly aeroelasticity, when calculating wind loads in the design of large buildings, bridges, and wind turbines \n\nThe aerodynamics of internal passages is important in heating/ventilation, gas piping, and in automotive engines where detailed flow patterns strongly affect the performance of the engine.\n\nUrban aerodynamics are studied by town planners and designers seeking to improve amenity in outdoor spaces, or in creating urban microclimates to reduce the effects of urban pollution. The field of environmental aerodynamics describes ways in which atmospheric circulation and flight mechanics affect ecosystems. \n\nAerodynamic equations are used in numerical weather prediction.\n\nSports in which aerodynamics are of crucial importance include soccer, cricket and golf, in which control over the trajectory of ball movement is sought by expert players who can, for instance, exploit the \"Magnus effect\" and other environmental conditions to advantage.\n\n\nGeneral aerodynamics\n\nSubsonic aerodynamics\n\nTransonic aerodynamics\n\nSupersonic aerodynamics\n\nHypersonic aerodynamics\n\nHistory of aerodynamics\n\nAerodynamics related to engineering\n\n\"Ground vehicles\"\n\n\"Fixed-wing aircraft\"\n\n\"Helicopters\"\n\n\"Missiles\"\n\n\"Model aircraft\"\n\nRelated branches of aerodynamics\n\n\"Aerothermodynamics\"\n\n\"Aeroelasticity\"\n\n\"Boundary layers\"\n\n\"Turbulence\"\n\n"}
{"id": "34421411", "url": "https://en.wikipedia.org/wiki?curid=34421411", "title": "Aeroflot Flight 4225", "text": "Aeroflot Flight 4225\n\nAeroflot Flight 4225 was a Tupolev Tu-154B-2 that was a domestic scheduled passenger flight from Alma-Ata Airport (now Almaty) in Soviet Kazakhstan to Simferopol Airport in Soviet Ukraine on July 8, 1980. The aircraft reached an altitude of no more than 500 feet when the airspeed suddenly dropped because of thermal currents it encountered during the climb out, causing the airplane to stall before traveling 5 km (3.1 mi) away, crash and catch fire, killing all 156 passengers and 10 crew on board. To date, it remains the deadliest aviation accident in Kazakhstan.\n\nAt the time of the accident Alma-Ata was experiencing a heat wave. It was around 00:39 and Flight 4225 took off from Alma-Ata Airport in Soviet Kazakhstan. Only a few seconds after take off, the flight reached 500 feet. The weather was not on the flight's side; the plane reached a zone of hot air and then the Soviet aircraft's airspeed dramatically dropped and the plane was caught in a big downdraft. The Tupolev stalled and plummeted, nose down, into a farm near the suburbs of Alma-Ata. It slid into a ravine, caught fire and disintegrated, killing everyone on board.\n\nThe Soviet aviation board concluded that the crash was caused by windshear which took place while the aircraft was near its maximum takeoff weight for the local conditions which included mountains.\n\n\n"}
{"id": "644002", "url": "https://en.wikipedia.org/wiki?curid=644002", "title": "Alnico", "text": "Alnico\n\nAlnico is an acronym referring to a family of iron alloys which in addition to iron are composed primarily of aluminium (Al), nickel (Ni) and cobalt (Co), hence \"al-ni-co\". They also include copper, and sometimes titanium. Alnico alloys are ferromagnetic, with a high coercivity (resistance to loss of magnetism) and are used to make permanent magnets. Before the development of rare-earth magnets in the 1970s, they were the strongest type of permanent magnet. Other trade names for alloys in this family are: \"Alni, Alcomax, Hycomax, Columax\", and \"Ticonal\".\n\nThe composition of alnico alloys is typically 8–12% Al, 15–26% Ni, 5–24% Co, up to 6% Cu, up to 1% Ti, and the balance is Fe. The development of alnico began in 1931, when T. Mishima in Japan discovered that an alloy of iron, nickel, and aluminum had a coercivity of \n400 oersteds (Oe; 0.07957 kA/m), double that of the best magnet steels of the time.\nAlnico alloys can be magnetised to produce strong magnetic fields and have a high coercivity (resistance to demagnetization), thus making strong permanent magnets. Of the more commonly available magnets, only rare-earth magnets such as neodymium and samarium-cobalt are stronger. Alnico magnets produce magnetic field strength at their poles as high as 1500 gausses (0.15 teslas), or about 3000 times the strength of Earth's magnetic field. Some brands of alnico are isotropic and can be efficiently magnetized in any direction. Other types, such as alnico 5 and alnico 8, are anisotropic, with each having a preferred direction of magnetization, or orientation. Anisotropic alloys generally have greater magnetic capacity in a preferred orientation than isotropic types. Alnico's remanence (B) may exceed 12,000 G (1.2 T), its coercivity (H) can be up to 1000 oersteds (80 kA/m), its energy product ((BH)) can be up to 5.5 MG·Oe (44 T·A/m). This means that alnico can produce a strong magnetic flux in closed magnetic circuits, but has relatively small resistance against demagnetization. The field strength at the poles of any permanent magnet depends very much on the shape and is usually well below the remanence strength of the material.\n\nAlnico alloys have some of the highest Curie temperatures of any magnetic material, around , although the maximal working temperature is normally limited to around . They are the only magnets that have useful magnetism even when heated red-hot. This property, as well as its brittleness and high melting point, is the result of the strong tendency toward order due to intermetallic bonding between aluminium and other constituents. They are also one of the most stable magnets if they are handled properly. Alnico magnets are electrically conductive, unlike ceramic magnets.\nAs of 2018, Alnico magnets cost about 44 USD/kg (20 USD/lb) or 4.30 USD/BH.\n\nAlnico magnets are traditionally classified using numbers assigned by the Magnetic Materials Producers Association (MMPA), for example, alnico 3 or alnico 5. These classifications indicate chemical composition and magnetic properties. (The classification numbers themselves do not have any direct relation to the properties of the magnet; for instance, a higher number does not necessarily indicate a stronger magnet.)\n\nThese classification numbers, while still in use, have been deprecated in favor of a new system by the MMPA, which designates Alnico magnets based on maximum energy product in megagauss-oersteds and intrinsic coercive force as kilooersteds, as well as an IEC classification system.\n\nAlnico magnets are produced by casting or sintering processes. Anisotropic alnico magnets are oriented by heating above a critical temperature and cooling in the presence of a magnetic field. Both isotropic and anisotropic alnico require proper heat treatment to develop optimal magnetic properties—without it alnico's coercivity is about 10 Oe, comparable to technical iron, which is a soft magnetic material. After the heat treatment alnico becomes a composite material, named \"precipitation material\"—it consists of iron- and cobalt-rich precipitates in rich-NiAl matrix.Alnico's anisotropy is oriented along the desired magnetic axis by applying an external magnetic field to it during the precipitate particle nucleation, which occurs when cooling from to , near the Curie point. Without an external field there are local anisotropies of different orientations due to spontaneous magnetization. The precipitate structure is a \"barrier\" against magnetization changes, as it prefers few magnetization states requiring much energy to get the material into any intermediate state. Also, a weak magnetic field shifts the magnetization of the matrix phase only and is reversible.\n\nAlnico magnets are widely used in industrial and consumer applications where strong permanent magnets are needed; examples are electric motors, electric guitar pickups, microphones, sensors, loudspeakers, magnetron tubes, and cow magnets. In many applications they are being superseded by rare-earth magnets, whose stronger fields (B) and larger energy products (BH) allow smaller-size magnets to be used for a given application.\n\n"}
{"id": "9913028", "url": "https://en.wikipedia.org/wiki?curid=9913028", "title": "Annihilation radiation", "text": "Annihilation radiation\n\nAnnihilation radiation is a term used in Gamma spectroscopy for the gamma radiation produced when a particle and its antiparticle collide and annihilate. Most commonly, this refers to 511-keV gamma rays produced by a normal (negative) electron colliding with a positron.\n\nAnnihilation radiation is not monoenergetic, unlike gamma rays produced by radioactive decay. The production mechanism of annihilation radiation introduces Doppler broadening. The annihilation peak produced in a gamma spectrum by annihilation radiation therefore has a higher full width at half maximum (FWHM) than other gamma rays in spectrum. The difference is more apparent with high resolution detectors, such as Germanium detectors, than with low resolution detectors such as Sodium iodide detectors.\n\nBecause of their well-defined energy (511 keV) and characteristic, Doppler-broadened shape, annihilation radiation can often be useful in defining the energy calibration of a gamma ray spectrum.\n"}
{"id": "18537924", "url": "https://en.wikipedia.org/wiki?curid=18537924", "title": "Anonima Petroli Italiana", "text": "Anonima Petroli Italiana\n\nAnonima Petroli Italiana or simply api (in lower case) is a large oil company in Italy. It is a provider of crude oil for the petrochemical industry and a distributor of petroleum products. It is the most important subsidiary of the holding company Gruppo api (api Group), which also includes api Raffineria di Ancona SpA, api Energia SpA, Festival SpA, apioil Ltd, api GmbH and api Services Ltd. It is headquartered in Rome, Italy.\n\nThe company was founded in Ancona in 1933 by Ferdinando Peretti. It operates as a strategic coordinator of all api group's activities, through the direct supplying of semi-finished crude oil and assigned in the production, the acquisition of finished products and their transport, the exchanges with other oil companies to optimize the distributive logistics, the complementary activity of trading of crude oil and by-products to clients, and the sale of all the by-products in Italy and to foreign countries. It also is dedicated to operate and develop oil an gas field properties. It owns a refinery in Ancona with a capacity of 3.9 million tonnes/year and an electric power plant of 250 MW.\n\nIn 2005 it acquired the IP gas station chain from ENI. After a few years running the chains in parallel it switched all its stations to a revamped IP image and now supplies around 4,200 filling stations (up from around 1,600 at the time of the acquisition) with a market share of 11%.\n\n"}
{"id": "270981", "url": "https://en.wikipedia.org/wiki?curid=270981", "title": "Aramid", "text": "Aramid\n\nAramid fibers are a class of heat-resistant and strong synthetic fibers. They are used in aerospace and military applications, for ballistic-rated body armor fabric and ballistic composites, in bicycle tires, marine cordage, marine hull reinforcement, and as an asbestos substitute. The name is a portmanteau of \"aromatic polyamide\". The chain molecules in the fibers are highly oriented along the fiber axis. As a result, a higher proportion of the chemical bond contributes more to fiber strength than in many other synthetic fibers. Aramides have a very high melting point (>500 °C)\n\nAromatic polyamides were first introduced in commercial applications in the early 1960s, with a meta-aramid fiber produced by DuPont as HT-1 and then under the trade name Nomex. This fiber, which handles similarly to normal textile apparel fibers, is characterized by its excellent resistance to heat, as it neither melts nor ignites in normal levels of oxygen. It is used extensively in the production of protective apparel, air filtration, thermal and electrical insulation, as well as a substitute for asbestos. Meta-aramid is also produced in the Netherlands and Japan by Teijin Aramid under the trade name Teijinconex, in Korea by Toray under the trade name Arawin, in China by Yantai Tayho under the trade name New Star, by SRO Group (China) under the trade name X-Fiper, and a variant of meta-aramid in France by Kermel under the trade name Kermel.\n\nBased on earlier research by Monsanto Company and Bayer, para-aramid fiber with much higher tenacity and elastic modulus was also developed in the 1960s and 1970s by DuPont and AkzoNobel, both profiting from their knowledge of rayon, polyester and nylon processing.\n\nIn 1973 DuPont was the first company to introduce a para-aramid fiber, which it called Kevlar, to the market; this remains one of the best-known para-aramids and/or aramids. In 1978, Akzo introduced a similar fiber with roughly the same chemical structure, which it called Twaron. Due to earlier patents on the production process, Akzo and DuPont engaged in a patent dispute in the 1980s. Twaron subsequently came under the ownership of the Teijin Company. (See Production.)\n\nPara-aramids are used in many high-tech applications, such as aerospace and military applications, for \"bullet-proof\" body armor fabric.\n\nThe Federal Trade Commission definition for aramid fiber is:\n\nA manufactured fiber in which the fiber-forming substance is a long-chain synthetic polyamide in which at least 85% of the amide linkages, (−CO−NH−) are attached directly to two aromatic rings.\nDuring the 1990s, an \"in vitro\" test of aramid fibers showed they exhibited \"many of the same effects on epithelial cells as did asbestos, including increased radiolabeled nucleotide incorporation into DNA and induction of ODC (ornithine decarboxylase) enzyme activity\", raising the possibility of carcinogenic implications. However, in 2009, it was shown that inhaled aramid fibrils are shortened and quickly cleared from the body and pose little risk.\n\nWorld capacity of para-aramid production was estimated at about 41,000 tonnes per year in 2002 and increases each year by 5–10%. In 2007 this means a total production capacity of around 55,000 tonnes per year.\n\nAramids are generally prepared by the reaction between an amine group and a carboxylic acid halide group. Simple AB homopolymers may look like\n\nThe most well-known aramids (Kevlar, Twaron, Nomex, New Star and Teijinconex) are AABB polymers. Nomex, Teijinconex and New Star contain predominantly the meta-linkage and are poly-\"metaphenylene isophthalamide\"s (MPIA).\nKevlar and Twaron are both \"p\"-phenylene terephthalamides (PPTA), the simplest form of the AABB para-polyaramide. PPTA is a product of \"p\"-phenylene diamine (PPD) and terephthaloyl dichloride (TDC or TCl).\n\nProduction of PPTA relies on a co-solvent with an ionic component (calcium chloride (CaCl)) to occupy the hydrogen bonds of the amide groups, and an organic component (N-methyl pyrrolidone (NMP)) to dissolve the aromatic polymer. This process was invented by Leo Vollbracht, who worked at the Dutch chemical firm Akzo. Apart from the carcinogenic Hexamethylphosphorous Triamide (HMPT), still no practical alternative of dissolving the polymer is known. \nThe use of the NMP/CaCl system led to an extended patent dispute between Akzo and DuPont.\n\nAfter production of the polymer, the aramid fiber is produced by spinning the dissolved polymer to a solid fiber from a liquid chemical blend. Polymer solvent for spinning PPTA is generally 100% anhydrous sulfuric acid (HSO).\n\n\nBesides meta-aramids like Nomex, other variations belong to the aramid fiber range. These are mainly of the copolyamide type, best known under the brand name Technora, as developed by Teijin and introduced in 1976. The manufacturing process of Technora reacts PPD and 3,4'-diaminodiphenylether (3,4'-ODA) with terephthaloyl chloride (TCl).\nThis relatively simple process uses only one amide solvent, and therefore spinning can be done directly after the polymer production.\n\nAramids share a high degree of orientation with other fibers such as ultra-high-molecular-weight polyethylene, a characteristic that dominates their properties.\n\n\n\n\nThe largest section for the use of aramid fiber is for security and protection (including military applications, private security, Municipal law enforcement and private contractors). The Major market since 2015 is Europe, especially Germany, France and the United Kingdom. The Asia-Pacific region like Japan and China is the second largest with major companies using aramid fiber. \n\nPara-aramid\n\nMeta-aramid\n\nOthers\n\n"}
{"id": "43755101", "url": "https://en.wikipedia.org/wiki?curid=43755101", "title": "Association négaWatt", "text": "Association négaWatt\n\nAssociation NegaWatt Power, Is a French advocacy group based in Valence, Drôme. It was founded in 2001 to promote the NegaWatt concept and its application to French society. The association seeks to reduce the use and consumption of fossil fuels and nuclear power. Its approach is based on energy conservation, energy efficiency and the use of renewable energies. It published a scenario that details a path to phasing out these energy sources.\n\nThe association unites energy specialists in France, relying mostly on volunteer engineers. Other specialists include volunteer sociologists, economists and urban planners, who work together to create energy transition scenarios and to suggest the corresponding political steps that would need to be taken.\n\nAssociation NégaWatt inspired local initiatives in transition towns called \"Virage-Energie,\" meaning \"energy bend.\"\n\nThe NégaWatt scenario spans from 2012 to 2050. The scenario starts by assessing energy needs in terms of heat, mobility and specific electricity, and suggests a transition path. It then couples the NégaWatt and the Afterterres 2050 scenarios. The association targets agriculture energy use and can evaluate biomass production, providing the NégaWatt scenario with realistic biomass figures.\n\nAn English version of the scenario is available.\n\nThe scenario employs the power to gas process to compensate for the intermittent nature of most renewable energies. The equilibrium of the electricity network is guaranteed with a maximal time step of one hour. The scenario concluded that already-proven technologies are available to effect the transition.\n\nA Swiss NégaWatt organization was founded in 2014, on the basis of the French one.\n\n\n"}
{"id": "206243", "url": "https://en.wikipedia.org/wiki?curid=206243", "title": "Biocoenosis", "text": "Biocoenosis\n\nA biocenosis (UK English, \"biocoenosis\", also biocenose, biocoenose, biotic community, biological community, ecological community, life assemblage,) coined by Karl Möbius in 1877, describes the interacting organisms living together in a habitat (biotope).\n\nIn the palaeontological literature, the term distinguishes \"life assemblages\", which reflect the original living community, living together at one place and time. In other words, it is an assemblage of fossils or a community of specific time, which is different from \"death assemblages\" (thanatocoenoses). No palaeontological assemblage will ever completely represent the original biological community (i.e. the biocoenosis, in the sense used by an ecologist); the term thus has somewhat different meanings in a palaeontological and an ecological context.\n\nBased on the concept of biocenosis, ecological communities can take in various forms\n\nThe geographical extent of a biocenose is limited by the requirement of a more or less uniform species composition.\n\nAn ecosystem, originally defined by Tansley (1935), is a biotic community (or biocenosis) along with its physical environment (or \"biotope\"). In ecological studies, biocenosis is the emphasis on relationships between species in an area. These relationships are an additional consideration to the interaction of each species with the physical environment.\n\nBiotic communities vary in size, and larger ones may contain smaller ones. Species interactions are evident in food or feeding relationships. A method of delineating biotic communities is to map the food network to identify which species feed upon which others and then determine the system boundary as the one that can be drawn through the fewest consumption links relative to the number of species within the boundary.\n\nMapping biotic communities is important identifying sites needing environmental protection, such as the British Site of Special Scientific Interest (SSSIs). The Australian Department of the Environment and Heritage maintains a register of \"Threatened Species and Threatened Ecological Communities\" under the Environment Protection and Biodiversity Conservation Act 1999 (EPBC Act).\n\n"}
{"id": "14466176", "url": "https://en.wikipedia.org/wiki?curid=14466176", "title": "Bjørn Skogstad Aamo", "text": "Bjørn Skogstad Aamo\n\nBjørn Skogstad Aamo (born 21 January 1946) is a Norwegian economist and politician for the Labour Party. He was State Secretary for three non-consecutive terms between 1973 and 1993, and served as Director of the Financial Supervisory Authority of Norway from 1993 to 2011.\n\nBorn in Oslo, Aamo grew up in Mandal. He became involved in politics at an early age, and was appointed chairman of the local Workers' Youth League chapter in 1962. He then moved back to the capital to enroll at the University of Oslo. He became chairman of the Workers' Youth League chapter there. Situated on the left wing of the organization, he became a vocal opponent of the Vietnam War as well as the Norwegian North Atlantic Treaty Organization membership, much to the dismay of party secretary Haakon Lie. However, Aamo found support from fellow economist Per Kleppe, at that time a deputy member of the party central committee and head of the think tank \"Arbeiderbevegelsens utredningskontor\".\n\nAamo graduated with the degree cand.oecon. in economics in 1970, and from 1971 to 1972 he worked as personal secretary (today known as political adviser) in the Ministry of Local Government and Labour. He was then promoted to State Secretary in the Ministry of Finance from 1973 to 1979, serving under Per Kleppe who had become Minister of Finance.\n\nHe was then head of the department of economics in the European Free Trade Association from 1979 to 1981, and director of the Regional Development Fund from 1981 to 1986. He then returned as State Secretary from 1986 to 1989 under Minister of Finance Gunnar Berge in Brundtland's Second Cabinet. He lost his job when Brundtland's Second Cabinet fell in 1989, and briefly returned to Regional Development Fund. In 1990 as the short-lived Syse's Cabinet fell and Brundtland's Third Cabinet assumed office, Aamo was appointed State Secretary at the Office of the Prime Minister.\n\nIn 1993 he was appointed Director of the Financial Supervisory Authority, succeeding Svein Aasmundstad. This decision was criticized by the political opposition. Aamo belonged to the same political party as Minister of Finance Sigbjørn Johnsen and the rest of Brundtland's Third Cabinet. Since Aamo came directly from a government position, that of state secretary, parliament member Lars Gunnar Lie cited concern about a perceived lack of distance between the third cabinet Brundtland and the Financial Supervisory Authority. This would put the independence of the latter at odds, he claimed. However, most of the criticism was directed at Sigbjørn Johnsen, who at that time was under scrutiny for his involvement with private corporation UNI Storebrand. The Financial Supervisory Authority would have a role in this scrutiny, and several people, including parliament member Kristin Halvorsen, asked for the appointment of Johnsen's party fellow be postponed until after the case was closed. Parliament member Carl I. Hagen called for Johnsen to resign. On the other hand, conservative newspaper \"Aftenposten\" defended the decision in an op-ed the next day, citing Aamo's strong qualifications for the job. The criticism did not hinder the appointment of Aamo, and Sigbjørn Johnsen remained in his seat.\n\nAamo applied for the position as Governor of the Central Bank of Norway in 1999, but Svein Gjedrem was appointed.\n\nDuring the economic crisis of 2008, Aamo stated that banks are safer in Norway than in any other country. The matters are complicated by Norway's close relations to Iceland, whose national crisis had consequences for Norwegian customers in the banks Glitnir and Kaupthing. Aamo is himself a board member of the Norwegian Banks' Guarantee Fund.\n\nAamo is married to Eldrid Nordbø, former government minister.\n"}
{"id": "14557586", "url": "https://en.wikipedia.org/wiki?curid=14557586", "title": "Blue box recycling system", "text": "Blue box recycling system\n\nThe blue box recycling system (BBRS) was initially a waste management system used by Canadian municipalities to collect source separated household waste materials for the purpose of recycling. The first full-scale community wide BBRS was implemented in 1983 by the waste management contractor Ontario Total Recycling Systems Ltd. (a subsidiary of Laidlaw Waste Systems) for the City of Kitchener, Ontario. The blue box recycling system was implemented as part of the city's waste management procedures. The blue box system and variations of it remain in place in hundreds of cities around the world.\n\nToday, many municipalities operate blue box programs that accept all forms of paper materials, most plastic packaging, glass containers, aluminum cans and steel cans. For example, the City of Greater Napanee accepts:\n\nThe municipality provides the blue boxes to residents in areas serviced by the recycling program. This usually includes all single-family homes and townhouse units receiving garbage collection. Tenants of apartment buildings typically do not use blue boxes but rather deposit their household recyclable materials in larger containers made available.\n\nAn organization called Pollution Probe was formed in 1969 by students and faculty at the University of Toronto and in 1971, members published a report stressing the need for recycling. Also in 1971, the Canadian Federal Government first established a national Department of the Environment, known as Environment Canada, with Environmental protection as one of its priorities. The Ontario government followed suit with a Ministry of the Environment in 1972.\n\nJack McGinnis of Toronto helped form a community-based non-profit organization called \"Is Five\" Foundation in 1974. Named for a book of poetry by e.e. cummings, Is Five organized Canada’s first multi-material curbside pickup of recyclable material for 80,000 households of The Beaches neighbourhood of east Toronto. In 1977, Jack McGinnis and Derek Stephenson created a private consulting company, Resource Integration Systems (RIS) to advise governments in the field of recycling and waste management.\n\nThat same year, in response to the 1973 energy crisis, Rick Findlay was hired by Environment Canada’s Federal Facilities Program to develop and coordinate a resource conservation program primarily for federal government facilities in Ontario. The program was expected to improve environmental quality and reduce energy demand. The Federal Facilities Program sought advice on a recycling pilot program for Canadian Forces Base Borden and eventually made contact with RIS. RIS experimented with a variety of techniques to improve the performance recycling and found that participation rates were significantly higher when residents could simply drop their recycling in a plastic box. Initially, the project at CFB Borden used milk crates borrowed from a local grocery store. Participation was likely encouraged by a standing order from the Base Commander to recycle.\n\nMeanwhile, a student and a volunteer at a Kitchener/Waterloo, Ontario office of Pollution Probe, Eric Hellman organized \"Garbage Fest 77\" in Kitchener to raise awareness about the consequences of garbage production. McGinnis was invited to speak about his experiences with the CFB Borden program. Hellman also requested a representative from Superior Sanitation (later to be Laidlaw Waste Systems Ltd), the local garbage collection contractor, and an employee named Nyle Ludolph attended Garbage Fest. After meeting McGinnis at the festival, Ludolph became an enthusiastic recycler at his own home and helped increase Laidlaw’s presence in recycling. Nyle managed \"Total Recycling Systems\", a subsidiary of Superior Sanitation/Laidlaw.\n\nIn 1978, Jack McGinnis and Eric Hellman and others met in the basement of the Trinity United Church on Yonge Street in Toronto and created the Recycling Council of Ontario.\n\nIn 1981, after close cooperation with Nyle Ludolph, RIS submitted a proposal to Laidlaw to collect and recycle materials at the curbside for a pilot project in Kitchener, Ontario. Lillian and Ken Croal of Kitchener were the first to use and receive a blue box. RIS designed the program, and Total Recycling had the responsibility to handle all operations. The Kitchener project included 1,500 households and was tested with four different approaches to recycling:\n\nThe blue box recycling system was proposed to be included as an included “extra” to the regular waste management contract. Laidlaw was successful in obtaining the waste management contract for Kitchener, and the blue box system had its commercial launch. RIS came up with the slogan “We Recycle”, which was applied to every box that went out in Kitchener. Various reasons why the boxes were coloured blue, as opposed to some other colour have been posited: the facts are that RIS felt they looked best and were most visible in blue. It was also a colour that was suitable in terms of withstanding damage from ultraviolet light.\n\nThe four recycling approaches were monitored by RIS for one year and the blue box was decided as the most effective. The “test project” continued to run however, and people who did not have the blue boxes began requesting them. Laidlaw received many letters from residents expressing support for the program and for it to continue. The blue box program was implemented citywide in 1983, with Laidlaw providing the extra investment in additional boxes and trucks and handling equipment even though it was not required by their waste management contract with the City. Participation rates ran at 85 percent, and the program was regarded as a success.\n\nIn 1984 the Citywide contract for Kitchener went out to public bid, as previously scheduled. City staff simply followed previous procedures – i.e., no specific requirement to offer recycling services was included in bid documents. Laidlaw chose to submit a bid that included continuation of the blue box service while their competitors, mainly large US-based firms, did not. One such competitor submitted a bid that was about $400,000 lower than the one from Laidlaw. The Council meeting where the decision was made was filled with local citizens, especially students, asking their City to support the blue box. The City did, voting to take the blue box bid, not the low bid.\n\nIn 1985, Laidlaw won the bid for recycling in the City of Mississauga and introduced the second commercial blue box program in Ontario in June 1986, the largest recycling effort in North America.\n\nBetween 1997 and 1999, Laidlaw, Inc. exited the solid waste business after incurring heavy losses through its investments in Safety-Kleen and Greyhound Lines. After almost 20 years of expansion, Laidlaw Inc. filed for protection under Chapter 11 of the U.S. Bankruptcy Code in June 2001.\nCurrently in Ontario, Canada industry compensates municipalities for approximately 50% of the net costs to operate the blue box program.\n\nThe blue box system and variations of it remain in place in hundreds of cities around the world.\n\n\n\n"}
{"id": "8931743", "url": "https://en.wikipedia.org/wiki?curid=8931743", "title": "Burren Action Group", "text": "Burren Action Group\n\nThe Burren Action Group was a group of people from County Clare in Ireland who opposed plans by the Office of Public Works during the 1990s to develop a large scale interpretative centre at Mullaghmore in the local Burren area.\n\nThe Group was a collective of concerned locals who fought to maintain the natural integrity of the landscape and to protect the environment from elements of the Government of Ireland which did not understand what was at stake. They also felt that the Burren and the area of Mullaghmore is a \"sacred site\" and holy ground that needed to be defended in a country whose sites of profound historical importance are rapidly disappearing.\n\nIn 1992/1993 seven members of the group lodged a complaint against the project with the Irish High Court, which resulted in work being stopped. These seven included local farmers like James Howard and Patrick McCormack, priest Fr. John O’Donohue, Prof. Emer Colleran as well as media figures like the producer P.J. Curtis or Lelia Doolan. The Burren Action Group was also supported by leftist politicians like Brigid Makowski.\n\nFollowing about ten years of opposition, the group was finally successful in March 2000. An Bord Pleanála confirmed the ruling by the Clare County Council to refuse planning permission for a scaled-down version of the original plans.\n\nIn 2012, James Howard and Patrick McCormack, the latter owner of the house that featured as the parochial house in the Father Ted TV show, once again opposed a new application to construct a car park at the site of the planned visitors' centre.\n\nThe Burren Action Group compiled a music album in the early 1990s, entitled \"Music in the Stone\" to raise money to save Mullaghmore because \"...the wheels of greed are rolling towards it\".\n\n"}
{"id": "5980", "url": "https://en.wikipedia.org/wiki?curid=5980", "title": "Carbon sink", "text": "Carbon sink\n\nA carbon sink is a natural or artificial reservoir that accumulates and stores some carbon-containing chemical compound for an indefinite period. The process by which carbon sinks remove carbon dioxide () from the atmosphere is known as carbon sequestration. Public awareness of the significance of CO sinks has grown since passage of the Kyoto Protocol, which promotes their use as a form of carbon offset. There are also different strategies used to enhance this process.\n\nIncrease in atmospheric carbon dioxide means increase in global temperature. The amount of carbon dioxide varies naturally. The natural sinks are:\n\nWhilst the creation of artificial sinks has been discussed, no major artificial systems remove carbon from the atmosphere on a material scale.\n\nCarbon sources include:\n\nBecause growing vegetation takes in carbon dioxide, the Kyoto Protocol allows Annex I countries with large areas of growing forests to issue Removal Units to recognize the sequestration of carbon. The additional units make it easier for them to achieve their target emission levels. It is estimated that forests absorb between 10 and 20 tons of carbon dioxide per hectare each year, through photosynthetic conversion into starch, cellulose, lignin, and wooden biomass. While this has been well documented for temperate forests and plantations, the fauna of the tropical forests place some limitations for such global estimates. \n\nSome countries seek to trade emission rights in carbon emission markets, purchasing the unused carbon emission allowances of other countries. If overall limits on greenhouse gas emission are put into place, cap and trade market mechanisms are purported to find cost-effective ways to reduce emissions. There is as yet no carbon audit regime for all such markets globally, and none is specified in the Kyoto Protocol. National carbon emissions are self-declared.\n\nIn the Clean Development Mechanism, only afforestation and reforestation are eligible to produce certified emission reductions (CERs) in the first commitment period of the Kyoto Protocol (2008–2012). Forest conservation activities or activities avoiding deforestation, which would result in emission reduction through the conservation of existing carbon stocks, are not eligible at this time. Also, agricultural carbon sequestration is not possible yet.\n\nSoils represent a short to long-term carbon storage medium, and contain more carbon than all terrestrial vegetation and the atmosphere combined. Plant litter and other biomass including charcoal accumulates as organic matter in soils, and is degraded by chemical weathering and biological degradation. More recalcitrant organic carbon polymers such as cellulose, hemi-cellulose, lignin, aliphatic compounds, waxes and terpenoids are collectively retained as humus. Organic matter tends to accumulate in litter and soils of colder regions such as the boreal forests of North America and the Taiga of Russia. Leaf litter and humus are rapidly oxidized and poorly retained in sub-tropical and tropical climate conditions due to high temperatures and extensive leaching by rainfall. Areas where shifting cultivation or slash and burn agriculture are practiced are generally only fertile for 2–3 years before they are abandoned. These tropical jungles are similar to coral reefs in that they are highly efficient at conserving and circulating necessary nutrients, which explains their lushness in a nutrient desert. Much organic carbon retained in many agricultural areas worldwide has been severely depleted due to intensive farming practices]].\n\nGrasslands contribute to soil organic matter, stored mainly in their extensive fibrous root mats. Due in part to the climatic conditions of these regions (e.g. cooler temperatures and semi-arid to arid conditions), these soils can accumulate significant quantities of organic matter. This can vary based on rainfall, the length of the winter season, and the frequency of naturally occurring lightning-induced grass-fires. While these fires release carbon dioxide, they improve the quality of the grasslands overall, in turn increasing the amount of carbon retained in the humic material. They also deposit carbon directly to the soil in the form of char that does not significantly degrade back to carbon dioxide.\n\nForest fires release absorbed carbon back into the atmosphere, as does deforestation due to rapidly increased oxidation of soil organic matter.\n\nOrganic matter in peat bogs undergoes slow anaerobic decomposition below the surface. This process is slow enough that in many cases the bog grows rapidly and fixes more carbon from the atmosphere than is released. Over time, the peat grows deeper. Peat bogs hold approximately one-quarter of the carbon stored in land plants and soils.\n\nUnder some conditions, forests and peat bogs may become sources of CO, such as when a forest is flooded by the construction of a hydroelectric dam. Unless the forests and peat are harvested before flooding, the rotting vegetation is a source of CO and methane comparable in magnitude to the amount of carbon released by a fossil-fuel powered plant of equivalent power.\n\nCurrent agricultural practices lead to carbon loss from soils. It has been suggested that improved farming practices could return the soils to being a carbon sink. Present worldwide practises of overgrazing are substantially reducing many grasslands' performance as carbon sinks. The Rodale Institute says that regenerative agriculture, if practiced on the planet’s 3.6 billion tillable acres, could sequester up to 40% of current CO emissions. They claim that agricultural carbon sequestration has the potential to mitigate global warming. When using biologically based regenerative practices, this dramatic benefit can be accomplished with no decrease in yields or farmer profits. Organically managed soils can convert carbon dioxide from a greenhouse gas into a food-producing asset.\n\nIn 2006, U.S. carbon dioxide emissions, largely from fossil fuel combustion, were estimated at nearly 6.5 billion tons. If a 2,000 (lb/ac)/year sequestration rate was achieved on all of cropland in the United States, nearly 1.6 billion tons of carbon dioxide would be sequestered per year, mitigating close to one quarter of the country's total fossil fuel emissions.\n\nPresently, oceans are CO sinks, and represent the largest active carbon sink on Earth, absorbing more than a quarter of the carbon dioxide that humans put into the air. The solubility pump is the primary mechanism responsible for the CO2 absorption by the oceans.\n\nThe biological pump plays a negligible role, because of the limitation to pump by ambient light and nutrients required by the phytoplankton that ultimately drive it. Total inorganic carbon is not believed to limit primary production in the oceans, so its increasing availability in the ocean does not directly affect production (the situation on land is different, since enhanced atmospheric levels of CO essentially \"fertilize\" land plant growth to some threshold). However, ocean acidification by invading anthropogenic CO may affect the biological pump by negatively impacting calcifying organisms such as coccolithophores, foraminiferans and pteropods. Climate change may also affect the biological pump in the future by warming and stratifying the surface ocean, thus reducing the supply of limiting nutrients to surface waters.\n\nA 2008 study found that CO could potentially increase primary productivity, particularly in eel grasses in coastal and estuarine habitats.\n\nIn January 2009, the Monterey Bay Aquarium Research Institute and the National Oceanic and Atmospheric Administration announced a joint study to determine whether the ocean off the California coast was serving as a carbon source or a carbon sink. Principal instrumentation for the study will be self-contained CO monitors placed on buoys in the ocean. They will measure the partial pressure of CO in the ocean and the atmosphere just above the water surface.\n\nIn February 2009, Science Daily reported that the Southern Indian Ocean is becoming less effective at absorbing carbon dioxide due to changes to the region's climate which include higher wind speeds.\n\nOn longer timescales, oceans may be both sources and sinks – during ice ages levels decrease to ≈180 ppmv, and much of this is believed to be stored in the oceans. As ice ages end, is released from the oceans and levels during previous interglacials have been around ≈280 ppmv. This role as a sink for CO is driven by two processes, the solubility pump and the biological pump. The former is primarily a function of differential CO solubility in seawater and the thermohaline circulation, while the latter is the sum of a series of biological processes that transport carbon (in organic and inorganic forms) from the surface euphotic zone to the ocean's interior. A small fraction of the organic carbon transported by the biological pump to the seafloor is buried in anoxic conditions under sediments and ultimately forms fossil fuels such as oil and natural gas.\n\nAt the end of glacials with sea level rapidly rising, corals tend to grow slower due to increased ocean temperature as seen on the Showtime series \"Years of Living Dangerously\". The calcium carbonate from which coral skeletons are made is just over 60% carbon dioxide. If we postulate that coral reefs were eroded down to the glacial sea level, then coral reefs have grown 120m upward since the end of the recent glacial.\n\nForests can be carbon stores, and they are carbon dioxide sinks when they are increasing in density or area. In Canada's boreal forests as much as 80% of the total carbon is stored in the soils as dead organic matter. A 40-year study of African, Asian, and South American tropical forests by the University of Leeds, shows tropical forests absorb about 18% of all carbon dioxide added by fossil fuels. Truly mature tropical forests, by definition, grow rapidly as each tree produces at least 10 new trees each year. Based on studies of the FAO and UNEP it has been estimated that Asian forests absorb about 5 tonnes of carbon dioxide per hectare each year. The global cooling effect of carbon sequestration by forests is partially counterbalanced in that reforestation can decrease the reflection of sunlight (albedo). Mid-to-high latitude forests have a much lower albedo during snow seasons than flat ground, thus contributing to warming. Modeling that compares the effects of albedo differences between forests and grasslands suggests that expanding the land area of forests in temperate zones offers only a temporary cooling benefit.\n\nIn the United States in 2004 (the most recent year for which EPA statistics are available), forests sequestered 10.6% (637 MegaTonnes) of the carbon dioxide released in the United States by the combustion of fossil fuels (coal, oil and natural gas; 5657 MegaTonnes). Urban trees sequestered another 1.5% (88 MegaTonnes). To further reduce U.S. carbon dioxide emissions by 7%, as stipulated by the Kyoto Protocol, would require the planting of \"an area the size of Texas [8% of the area of Brazil] every 30 years\". Carbon offset programs are planting millions of fast-growing trees per year to reforest tropical lands, for as little as $0.10 per tree; over their typical 40-year lifetime, one million of these trees will fix 1 to 2 MegaTonnes of carbon dioxide. In Canada, reducing timber harvesting would have very little impact on carbon dioxide emissions because of the combination of harvest and stored carbon in manufactured wood products along with the regrowth of the harvested forests. Additionally, the amount of carbon released from harvesting is small compared to the amount of carbon lost each year to forest fires and other natural disturbances.\n\nThe Intergovernmental Panel on Climate Change concluded that \"a sustainable forest management strategy aimed at maintaining or increasing forest carbon stocks, while producing an annual sustained yield of timber fibre or energy from the forest, will generate the largest sustained mitigation benefit\". Sustainable management practices keep forests growing at a higher rate over a potentially longer period of time, thus providing net sequestration benefits in addition to those of unmanaged forests.\n\nLife expectancy of forests varies throughout the world, influenced by tree species, site conditions and natural disturbance patterns. In some forests carbon may be stored for centuries, while in other forests carbon is released with frequent stand replacing fires. Forests that are harvested prior to stand replacing events allow for the retention of carbon in manufactured forest products such as lumber. However, only a portion of the carbon removed from logged forests ends up as durable goods and buildings. The remainder ends up as sawmill by-products such as pulp, paper and pallets, which often end with incineration (resulting in carbon release into the atmosphere) at the end of their lifecycle. For instance, of the 1,692 MegaTonnes of carbon harvested from forests in Oregon and Washington (U.S) from 1900 to 1992, only 23% is in long-term storage in forest products.\n\nOne way to increase the carbon sequestration efficiency of the oceans is to add micrometre-sized iron particles in the form of either hematite (iron oxide) or melanterite (iron sulfate) to certain regions of the ocean. This has the effect of stimulating growth of plankton. Iron is an important nutrient for phytoplankton, usually made available via upwelling along the continental shelves, inflows from rivers and streams, as well as deposition of dust suspended in the atmosphere. Natural sources of ocean iron have been declining in recent decades, contributing to an overall decline in ocean productivity (NASA, 2003). Yet in the presence of iron nutrients plankton populations quickly grow, or 'bloom', expanding the base of biomass productivity throughout the region and removing significant quantities of CO from the atmosphere via photosynthesis. A test in 2002 in the Southern Ocean around Antarctica suggests that between 10,000 and 100,000 carbon atoms are sunk for each iron atom added to the water. More recent work in Germany (2005) suggests that any biomass carbon in the oceans, whether exported to depth or recycled in the euphotic zone, represents long-term storage of carbon. This means that application of iron nutrients in select parts of the oceans, at appropriate scales, could have the combined effect of restoring ocean productivity while at the same time mitigating the effects of human caused emissions of carbon dioxide to the atmosphere.\n\nBecause the effect of periodic small scale phytoplankton blooms on ocean ecosystems is unclear, more studies would be helpful. Phytoplankton have a complex effect on cloud formation via the release of substances such as dimethyl sulfide (DMS) that are converted to sulfate aerosols in the atmosphere, providing cloud condensation nuclei, or CCN. But the effect of small scale plankton blooms on overall DMS production is unknown.\n\nOther nutrients such as nitrates, phosphates, and silica as well as iron may cause ocean fertilization. There has been some speculation that using pulses of fertilization (around 20 days in length) may be more effective at getting carbon to ocean floor than sustained fertilization.\n\nThere is some controversy over seeding the oceans with iron however, due to the potential for increased toxic phytoplankton growth (e.g. \"red tide\"), declining water quality due to overgrowth, and increasing anoxia in areas harming other sea-life such as zooplankton, fish, coral, etc.\n\nSince the 1850s, a large proportion of the world's grasslands have been tilled and converted to croplands, allowing the rapid oxidation of large quantities of soil organic carbon. However, in the United States in 2004 (the most recent year for which EPA statistics are available), agricultural soils including pasture land sequestered 0.8% (46 teragrams) as much carbon as was released in the United States by the combustion of fossil fuels (5988 teragrams). The annual amount of this sequestration has been gradually increasing since 1998.\n\nMethods that significantly enhance carbon sequestration in soil include no-till farming, residue mulching, cover cropping, and crop rotation, all of which are more widely used in organic farming than in conventional farming. Because only 5% of US farmland currently uses no-till and residue mulching, there is a large potential for carbon sequestration. Conversion to pastureland, particularly with good management of grazing, can sequester even more carbon in the soil.\n\nTerra preta, an anthropogenic, high-carbon soil, is also being investigated as a sequestration mechanism.\nBy pyrolysing biomass, about half of its carbon can be reduced to charcoal, which can persist in the soil for centuries, and makes a useful soil amendment, especially in tropical soils (\"biochar\" or \"agrichar\").\n<ref name='abc.net.au/catalyst/s2012892'></ref>\n\nControlled burns on far north Australian savannas can result in an overall carbon sink. One working example is the West Arnhem Fire Management Agreement, started to bring \"strategic fire management across 28,000 km² of Western Arnhem Land\". Deliberately starting controlled burns early in the dry season results in a mosaic of burnt and unburnt country which reduces the area of burning compared with stronger, late dry season fires. In the early dry season there are higher moisture levels, cooler temperatures, and lighter wind than later in the dry season; fires tend to go out overnight. Early controlled burns also results in a smaller proportion of the grass and tree biomass being burnt.<ref name='savanna.ntu.edu.au/arnhem_fire_proj'> </ref> Emission reductions of 256,000 tonnes of CO have been made as of 2007.<ref name='savanna.ntu.edu.au/Eureka_arnhem_fire_proj'> </ref>\n\nFor carbon to be sequestered artificially (i.e. not using the natural processes of the carbon cycle) it must first be captured, \"or\" it must be significantly delayed or prevented from being re-released into the atmosphere (by combustion, decay, etc.) from an existing carbon-rich material, by being incorporated into an enduring usage (such as in construction). Thereafter it can be passively stored \"or\" remain productively utilized over time in a variety of ways.\n\nFor example, upon harvesting, wood (as a carbon-rich material) can be immediately burned or otherwise serve as a fuel, returning its carbon to the atmosphere, \"or\" it can be incorporated into construction or a range of other durable products, thus sequestering its carbon over years or even centuries.\n\nIndeed, a very carefully designed and durable, energy-efficient and energy-capturing building has the potential to sequester (in its carbon-rich construction materials), as much as or more carbon than was released by the acquisition and incorporation of all its materials and than will be released by building-function \"energy-imports\" during the structure's (potentially multi-century) existence. Such a structure might be termed \"carbon neutral\" or even \"carbon negative\". Building construction and operation (electricity usage, heating, etc.) are estimated to contribute nearly \"half\" of the annual human-caused carbon additions to the atmosphere.\n\nNatural-gas purification plants often already have to remove carbon dioxide, either to avoid dry ice clogging gas tankers or to prevent carbon-dioxide concentrations exceeding the 3% maximum permitted on the natural-gas distribution grid.\n\nBeyond this, one of the most likely early applications of carbon capture is the capture of carbon dioxide from flue gases at power stations (in the case of coal, this coal pollution mitigation is sometimes known as \"clean coal\"). A typical new 1000 MW coal-fired power station produces around 6 million tons of carbon dioxide annually. Adding carbon capture to existing plants can add significantly to the costs of energy production; scrubbing costs aside, a 1000 MW coal plant will require the storage of about of carbon dioxide a year. However, scrubbing is relatively affordable when added to new plants based on coal gasification technology, where it is estimated to raise energy costs for households in the United States using only coal-fired electricity sources from 10 cents per kW·h to 12 cents.\n\nCurrently, capture of carbon dioxide is performed on a large scale by absorption of carbon dioxide onto various amine-based solvents. Other techniques are currently being investigated, such as pressure swing adsorption, temperature swing adsorption, gas separation membranes, cryogenics and flue capture.\n\nIn coal-fired power stations, the main alternatives to retrofitting amine-based absorbers to existing power stations are two new technologies: coal gasification combined-cycle and oxy-fuel combustion. Gasification first produces a \"syngas\" primarily of hydrogen and carbon monoxide, which is burned, with carbon dioxide filtered from the flue gas. Oxy-fuel combustion burns the coal in oxygen instead of air, producing only carbon dioxide and water vapour, which are relatively easily separated. Some of the combustion products must be returned to the combustion chamber, either before or after separation, otherwise the temperatures would be too high for the turbine.\n\nAnother long-term option is carbon capture directly from the air using hydroxides. The air would literally be scrubbed of its CO content. This idea offers an alternative to non-carbon-based fuels for the transportation sector.\n\nExamples of carbon sequestration at coal plants include converting carbon from smokestacks into baking soda, and algae-based carbon capture, circumventing storage by converting algae into fuel or feed.\n\nAnother proposed form of carbon sequestration in the ocean is direct injection. In this method, carbon dioxide is pumped directly into the water at depth, and expected to form \"lakes\" of liquid CO at the bottom. Experiments carried out in moderate to deep waters (350–3600 m) indicate that the liquid CO reacts to form solid CO clathrate hydrates, which gradually dissolve in the surrounding waters.\n\nThis method, too, has potentially dangerous environmental consequences. The carbon dioxide does react with the water to form carbonic acid, HCO; however, most (as much as 99%) remains as dissolved molecular CO. The equilibrium would no doubt be quite different under the high pressure conditions in the deep ocean. In addition, if deep-sea bacterial methanogens that reduce carbon dioxide were to encounter the carbon dioxide sinks, levels of methane gas may increase, leading to the generation of an even worse greenhouse gas.\nThe resulting environmental effects on benthic life forms of the bathypelagic, abyssopelagic and hadopelagic zones are unknown. Even though life appears to be rather sparse in the deep ocean basins, energy and chemical effects in these deep basins could have far-reaching implications. Much more work is needed here to define the extent of the potential problems.\n\nCarbon storage in or under oceans may not be compatible with the Convention on the Prevention of Marine Pollution by Dumping of Wastes and Other Matter.\n\nAn additional method of long-term ocean-based sequestration is to gather crop residue such as corn stalks or excess hay into large weighted bales of biomass and deposit it in the alluvial fan areas of the deep ocean basin. Dropping these residues in alluvial fans would cause the residues to be quickly buried in silt on the sea floor, sequestering the biomass for very long time spans. Alluvial fans exist in all of the world's oceans and seas where river deltas fall off the edge of the continental shelf such as the Mississippi alluvial fan in the gulf of Mexico and the Nile alluvial fan in the Mediterranean Sea. A downside, however, would be an increase in aerobic bacteria growth due to the introduction of biomass, leading to more competition for oxygen resources in the deep sea, similar to the oxygen minimum zone.\n\nThe method of \"geo-sequestration\" or \"geological storage\" involves injecting carbon dioxide directly into underground geological formations. Declining oil fields, saline aquifers, and unminable coal seams have been suggested as storage sites. Caverns and old mines that are commonly used to store natural gas are not considered, because of a lack of storage safety.\n\nCO has been injected into declining oil fields for more than 40 years, to increase oil recovery. This option is attractive because the storage costs are offset by the sale of additional oil that is recovered. Typically, 10–15% additional recovery of the original oil in place is possible. Further benefits are the existing infrastructure and the geophysical and geological information about the oil field that is available from the oil exploration. Another benefit of injecting CO into Oil fields is that CO is soluble in oil. Dissolving CO in oil lowers the viscosity of the oil and reduces its interfacial tension which increases the oils mobility. All oil fields have a geological barrier preventing upward migration of oil. As most oil and gas has been in place for millions to tens of millions of years, depleted oil and gas reservoirs can contain carbon dioxide for millennia. Identified possible problems are the many 'leak' opportunities provided by old oil wells, the need for high injection pressures and acidification which can damage the geological barrier. Other disadvantages of old oil fields are their limited geographic distribution and depths, which require high injection pressures for sequestration. Below a depth of about 1000 m, carbon dioxide is injected as a supercritical fluid, a material with the density of a liquid, but the viscosity and diffusivity of a gas.\nUnminable coal seams can be used to store CO, because CO absorbs to the coal surface, ensuring safe long-term storage. In the process it releases methane that was previously adsorbed to the coal surface and that may be recovered. Again the sale of the methane can be used to offset the cost of the CO storage. Release or burning of methane would of course at least partially offset the obtained sequestration result – except when the gas is allowed to escape into the atmosphere in significant quantities: methane has a higher global warming potential than CO.\n\nSaline aquifers contain highly mineralized brines and have so far been considered of no benefit to humans except in a few cases where they have been used for the storage of chemical waste. Their advantages include a large potential storage volume and relatively common occurrence reducing the distance over which CO has to be transported. The major disadvantage of saline aquifers is that relatively little is known about them compared to oil fields. Another disadvantage of saline aquifers is that as the salinity of the water increases, less CO can be dissolved into aqueous solution. To keep the cost of storage acceptable the geophysical exploration may be limited, resulting in larger uncertainty about the structure of a given aquifer. Unlike storage in oil fields or coal beds, no side product will offset the storage cost. Leakage of CO back into the atmosphere may be a problem in saline-aquifer storage. However, current research shows that several \"trapping mechanisms\" immobilize the CO underground, reducing the risk of leakage.\n\nA major research project examining the geological sequestration of carbon dioxide is currently being performed at an oil field at Weyburn in south-eastern Saskatchewan. In the North Sea, Norway's Equinor natural-gas platform Sleipner strips carbon dioxide out of the natural gas with amine solvents and disposes of this carbon dioxide by geological sequestration. Sleipner reduces emissions of carbon dioxide by approximately one million tonnes a year. The cost of geological sequestration is minor relative to the overall running costs. As of April 2005, BP is considering a trial of large-scale sequestration of carbon dioxide stripped from power plant emissions in the Miller oilfield as its reserves are depleted.\n\nIn October 2007, the Bureau of Economic Geology at The University of Texas at Austin received a 10-year, $38 million subcontract to conduct the first intensively monitored, long-term project in the United States studying the feasibility of injecting a large volume of CO for underground storage. The project is a research program of the Southeast Regional Carbon Sequestration Partnership (SECARB), funded by the National Energy Technology Laboratory of the U.S. Department of Energy (DOE). The SECARB partnership will demonstrate CO injection rate and storage capacity in the Tuscaloosa-Woodbine geologic system that stretches from Texas to Florida. Beginning in fall 2007, the project will inject CO at the rate of one million tons per year, for up to 1.5 years, into brine up to below the land surface near the Cranfield oil field about east of Natchez, Mississippi. Experimental equipment will measure the ability of the subsurface to accept and retain CO.\n\nMineral sequestration aims to trap carbon in the form of solid carbonate salts. This process occurs slowly in nature and is responsible for the deposition and accumulation of limestone over geologic time. Carbonic acid in groundwater slowly reacts with complex silicates to dissolve calcium, magnesium, alkalis and silica and leave a residue of clay minerals. The dissolved calcium and magnesium react with bicarbonate to precipitate calcium and magnesium carbonates, a process that organisms use to make shells. When the organisms die, their shells are deposited as sediment and eventually turn into limestone. Limestones have accumulated over billions of years of geologic time and contain much of Earth's carbon. Ongoing research aims to speed up similar reactions involving alkali carbonates.\n\nSeveral serpentinite deposits are being investigated as potentially large scale CO storage sinks such as those found in NSW, Australia, where the first mineral carbonation pilot plant project is underway. Beneficial re-use of magnesium carbonate from this process could provide feedstock for new products developed for the built environment and agriculture without returning the carbon into the atmosphere and so acting as a carbon sink.\n\nOne proposed reaction is that of the olivine-rich rock dunite, or its hydrated equivalent serpentinite with carbon dioxide to form the carbonate mineral magnesite, plus silica and iron oxide (magnetite).\n\nSerpentinite sequestration is favored because of the non-toxic and stable nature of magnesium carbonate. The ideal reactions involve the magnesium endmember components of the olivine (reaction 1) or serpentine (reaction 2), the latter derived from earlier olivine by hydration and silicification (reaction 3). The presence of iron in the olivine or serpentine reduces the efficiency of sequestration, since the iron components of these minerals break down to iron oxide and silica (reaction 4).\n\nZeolitic imidazolate frameworks is a metal-organic framework carbon dioxide sink which could be used to keep industrial emissions of carbon dioxide out of the atmosphere.\n\nOne study in 2009 found that the fraction of fossil-fuel emissions absorbed by the oceans may have declined by up to 10% since 2000, indicating oceanic sequestration may be sublinear. However, another study by Wolfgang Knorr indicated that the fraction of absorbed by carbon sinks has not changed since 1850.\n\n\n"}
{"id": "1503963", "url": "https://en.wikipedia.org/wiki?curid=1503963", "title": "Chebyshev distance", "text": "Chebyshev distance\n\nIn mathematics, Chebyshev distance (or Tchebychev distance), maximum metric, or L metric is a metric defined on a vector space where the distance between two vectors is the greatest of their differences along any coordinate dimension. It is named after Pafnuty Chebyshev.\n\nIt is also known as chessboard distance, since in the game of chess the minimum number of moves needed by a king to go from one square on a chessboard to another equals the Chebyshev distance between the centers of the squares, if the squares have side length one, as represented in 2-D spatial coordinates with axes aligned to the edges of the board. For example, the Chebyshev distance between f6 and e2 equals 4.\n\nThe Chebyshev distance between two vectors or points \"p\" and \"q\", with standard coordinates formula_1 and formula_2, respectively, is\n\nThis equals the limit of the L metrics:\nhence it is also known as the L metric.\n\nMathematically, the Chebyshev distance is a metric induced by the supremum norm or uniform norm. It is an example of an injective metric.\n\nIn two dimensions, i.e. plane geometry, if the points \"p\" and \"q\" have Cartesian coordinates\nformula_5 and formula_6, their Chebyshev distance is\n\nUnder this metric, a circle of radius \"r\", which is the set of points with Chebyshev distance \"r\" from a center point, is a square whose sides have the length 2\"r\" and are parallel to the coordinate axes.\n\nOn a chess board, where one is using a \"discrete\" Chebyshev distance, rather than a continuous one, the circle of radius \"r\" is a square of side lengths 2\"r,\" measuring from the centers of squares, and thus each side contains 2\"r\"+1 squares; for example, the circle of radius 1 on a chess board is a 3×3 square.\n\nIn one dimension, all L metrics are equal – they are just the absolute value of the difference.\n\nThe two dimensional Manhattan distance also has circles in the form of squares, with sides of length \"r\", oriented at an angle of π/4 (45°) to the coordinate axes, so the planar Chebyshev distance can be viewed as equivalent by rotation and scaling to the planar Manhattan distance.\n\nHowever, this equivalence between L and L metrics does not generalize to higher dimensions. A sphere formed using the Chebyshev distance as a metric is a cube with each face perpendicular to one of the coordinate axes, but a sphere formed using Manhattan distance is an octahedron: these are dual polyhedra, but among cubes, only the square (and 1-dimensional line segment) are self-dual polytopes.\n\nThe Chebyshev distance is sometimes used in warehouse logistics, as it effectively measures the time an overhead crane takes to move an object (as the crane can move on the x and y axes at the same time but at the same speed along each axis).\n\nOn a grid (such as a chessboard), the points at a Chebyshev distance of 1 of a point are the Moore neighborhood of that point.\n\n"}
{"id": "649259", "url": "https://en.wikipedia.org/wiki?curid=649259", "title": "Chromite", "text": "Chromite\n\nChromite is an iron chromium oxide: FeCrO. It is an oxide mineral belonging to the spinel group. Magnesium can substitute for iron in variable amounts as it forms a solid solution with magnesiochromite (MgCrO); substitution of aluminium occurs leading to hercynite (FeAlO).\n\nIt is by far the most industrially important mineral for the production of metallic chromium, used as an alloying ingredient in stainless and tool steels.\n\nChromite is found as orthocumulate lenses of chromitite in peridotite from the Earth's mantle. It also occurs in layered ultramafic intrusive rocks. In addition, it is found in metamorphic rocks such as some serpentinites. Ore deposits of chromite form as early magmatic differentiates. It is commonly associated with olivine, magnetite, serpentine, and corundum. The vast Bushveld igneous complex of South Africa is a large layered mafic to ultramafic igneous body with some layers consisting of 90% chromite making the rare rock type, chromitite. The Stillwater igneous complex in Montana also contains significant chromite. Chromite is also found in meteorites. The world’s largest chromite mine is South Africa but it is also found in the United States, Turkey, Philippines, Finland, Russia and Canada.\n\nThe only ores of chromium are the minerals chromite and magnesiochromite. Most of the time, economic geology names chromite the whole chromite-magnesiochromite series: FeCrO, (Fe,Mg)CrO, (Mg,Fe)CrO and MgCrO. The two main products of chromite refining are ferrochromium and metallic chromium; for those products the ore smelter process differs considerably. For the production of ferrochromium the chromite ore (FeCrO) is reduced with either aluminium or silicon in an aluminothermic reaction and for the production of pure chromium the iron has to be separated from the chromium in a two step roasting and leaching process. Chromite is also used as a refractory material, because it has a high heat stability.\n\nThe chromium extracted from chromite is used in chrome plating and alloying for production of corrosion resistant superalloys, nichrome, and stainless steel. Chromium is used as a pigment for glass, glazes, and paint, and as an oxidizing agent for tanning leather. It is also sometimes used as a gemstone.\n\nIn 2002 14,600,000 metric tons of chromite were mined. The largest producers were South Africa (44%), India (18%), Kazakhstan (16%), Zimbabwe (5%), Finland (4%), Iran (4%), and Brazil with several other countries producing the rest of less than 10% of the world production.\n\nAfghanistan has significant deposits of high grade chromite ore, which is mined illegally in Khost Province and then smuggled out of the country.\n\nIn Pakistan, chromite is mined from the ultramafic rocks in mainly the Muslimbagh killa Saifullah District and Khanozai area of Pishin District of Balochistan. Most of the chromite is of metallurgical grade with CrO averaging 54% and a chrome to iron ratio of 2.6:1.\n\nRecently, the biggest user of chromite ore has been China, importing large quantities from South Africa, Pakistan, and other countries. The concentrate is used to make ferrochrome, which is in turn used to make stainless steel and some other alloys.\n\nIn April 2010 the Government of Ontario announced that they would be opening up a large chromite deposit to development in the northern part of Ontario known as the Ring of Fire. This plan has since been suspended.\n\n"}
{"id": "22176277", "url": "https://en.wikipedia.org/wiki?curid=22176277", "title": "Colbún S.A.", "text": "Colbún S.A.\n\nColbún is a utility company in Chile engaged in the electric power transmission segment. The company was created in 1982 and privatized in 1997. Originally it had only two hydroelectric plants, Machicura and Colbún from where it gets its name. Today Colbún S.A. generates a total of 2514 MW, of which 1274 comes from hydropower and 1236 from fossil fuels plants. Colbún S.A. supplies the Sistema Interconectado Central power grid that spans Zona Central and Zona Sur but is currently developing a controversial joint venture project with ENDESA, called HidroAysén, to create five hydroelectrical power plants in Aisén Region.\n\nIn 1997, CORFO sold its 37.5% share, and in March 1997 was fully privatized. The main shareholders are Minera Valparaiso SA (Group Matte) (34.97%) and Electropacífico Investment Ltd. (28.60%).\n\n"}
{"id": "9330399", "url": "https://en.wikipedia.org/wiki?curid=9330399", "title": "Condensate polisher", "text": "Condensate polisher\n\nA condensate polisher is a device used to filter water condensed from steam as part of the steam cycle, for example in a conventional or nuclear power plant (powdered resin or deep bed system). It is frequently filled with polymer resins which are used to remove or exchange ions such that the purity of the condensate is maintained at or near that of distilled water.\n\nCondensate polishers are important in systems using the boiling and condensing of water to transport or transform thermal energy. Using technology similar to a water softener, trace amounts of minerals or other contamination are removed from the system before such contamination becomes concentrated enough to cause problems by depositing minerals inside pipes, or within precision-engineered devices such as boilers, steam generators, heat exchangers, steam turbines, cooling towers, and condensers. The removal of minerals has the secondary effect of maintaining the pH balance of the water at or near neutral (a pH of 7.0) by removing ions that would tend to make the water more acidic. This reduces the rate of corrosion where water comes in contact with metal.\n\nCondensate polishing typically involves ion exchange technology for the removal of trace dissolved minerals and suspended matter. Commonly used as part of a power plant's condensate system, it prevents premature chemical failure and deposition within the power cycle which would have resulted in loss of unit efficiency and possible mechanical damage to key generating equipment.\n\nDuring the process of steam generation in power plants, the steam cools and condensate forms. The condensate is collected and then used as boiler feedwater. Prior to re-use, the condensate must be purified or \"polished\", to remove impurities (predominantly silica oxides and sodium) which have the potential to cause damage to the boilers, steam generators, reactors and turbines. Both dissolved (i.e. silica oxides) and suspended matter (ex. iron oxide particulates from corrosion, also called 'crud'), as well as other contaminants which can cause corrosion and maintenance issues are effectively removed by condensate polishing treatment.\n"}
{"id": "17126715", "url": "https://en.wikipedia.org/wiki?curid=17126715", "title": "Crows Nest Wind Farm", "text": "Crows Nest Wind Farm\n\nThe proposed Crows Nest Wind Farm, will be located in south-eastern Queensland, 40 kilometres north of Toowoomba. It initially was to have an installed generating capacity of 124 MW that would produce intermittent electricity that could power some 47,000 homes during periods of high wind. It is expected that the wind farm will create 460 manufacturing and construction jobs and a further 15 full-time maintenance jobs in Crows Nest. The Crows Nest location on the western edge of the Darling Downs, offers some of the best average wind speeds available in Queensland, and the project will provide additional security of electricity supply in this fast-growing area.\n\nThe project was acquired by AGL Energy in 2009 after purchasing it and another wind farm at Barn Hill in South Australia from Transfield Services. In June 2009, AGL was allowed to increase the number of turbines by 20 units and therefore the output from 124 MW to 200 MW. The cost of the project is now expecting to be A$270 million.\n\nResidents of Crows Nest Shire formed a \"No Wind Farm\" group which objected to the proposal. The activists contended that turbine blades would cause light flicker as they passed the sun, turbines would cause noise, devalue land, be a detriment to fauna and visual appeal of the area and were contrary to the council's town plan. They also argued the green credentials of wind farms were a myth because large amounts of coal-fired energy was needed to power up generators to full capacity after a drop in wind speed. After drawn-out legal proceedings and mediation between the parties, the objections were progressively dropped.\n\n"}
{"id": "41586067", "url": "https://en.wikipedia.org/wiki?curid=41586067", "title": "Energy policy of Bangladesh", "text": "Energy policy of Bangladesh\n\nBangladesh suffers with heavy energy crisis with the gradual expansion of economic activities of the country. The estimations and reserves of energy resources show future potentials but a small fraction of them are being utilized which proved to be insufficient. Moreover, the impact of climate change and environment pollution has also been significantly felt. As a result, the successive governments have aimed at formulating an effective energy policy which would address these concerns. The energy policies have also received extensive criticisms especially on the questions of energy export and methods of extraction.\n\nThe first National Energy Policy (NEP) of Bangladesh was formulated in 1996 by the Ministry of Power, Energy and Mineral resources to ensure proper exploration, production, distribution and rational use of energy resources to meet the growing energy demands of different zones, consuming sectors and consumers groups on a sustainable basis. With rapid change of global as well as domestic situation, the policy was updated in 2004. The updated policy included additional objectives namely to ensure environmentally sound sustainable energy development programmes causing minimum damage to environment, to encourage public and private sector participation in the development and management of energy sector and to bring the entire country under electrification by the year 2020.\n\nNatural gas accounts for about 70% of the country's commercial energy supply. According to a study conducted in 2003 by Hydrocarbon unit of the Energy and Mineral Resources Division and Norwegian Petroleum Directorate, the initial gas reserve of the 22 discovered gas fields of the country amounts to 28.4 trillion cubic feet (TCF) out of which 20.5 TCF is considered recoverable. In 2000, United States Geological Survey conducted a study for undiscovered gas reserve of the country. According this study, there are possibilities to explore another 32 TCF of natural gas. Another study conducted by Hydrocarbon Unit and Norwegian Petroleum Directorate in 2001 concluded that additional 42 TCF of natural gas could be discovered in the country.\n\nIn recent years, experts have raised concerns that the existing proven reserves could be extirpated by 2020 and have called for immediate exploration of new gas fields.\n\nBangladesh has a reserve of approximately 3 billion tons of steam grade Bituminous coal as discovered till 2003. These reserves are mainly concentrated into the five large coal fields in the northwestern regions of the country, namely Jamalganj, Barapukuria, Khalashpir, Dighipara and Phulbari.\n\nThe NEP called for conservation measures to be strictly enforced to ensure rational, economic and efficient use of energy. The major means of energy conservation have been pointed out as energy audit, reduction of wastage, demand management and efficient use. Experts have suggested to initiate the energy conservation act, which would significantly reduce the energy demand of the country.\n"}
{"id": "38865832", "url": "https://en.wikipedia.org/wiki?curid=38865832", "title": "Esther Gulick", "text": "Esther Gulick\n\nEsther Gulick (née Kaufmann, 29 March 1911 - 31 May 1995) was a pioneer in environmentalism. She, along with Kay Kerr and Sylvia McLaughlin, founded the Save San Francisco Bay Association which eventually became \nSave The Bay.\n\nShe was referred to as an \"impractical idealist,\" a \"do-gooder\" and a \"posy-picker\" but she is credited as a leader in environmentalism.\n"}
{"id": "17402424", "url": "https://en.wikipedia.org/wiki?curid=17402424", "title": "Feed-in tariffs in Australia", "text": "Feed-in tariffs in Australia\n\nFeed-in tariffs in Australia are the feed-in tariffs (FITs) paid under various State schemes to non-commercial producers of electricity generated by solar photovoltaic (PV) systems using solar panels. They are a way of subsidising and encouraging uptake of renewable energy and in Australia have been enacted at the State level, in conjunction with a federal mandatory renewable energy target.\n\nAustralian FIT schemes tend to focus on providing support to solar PV particularly in the residential context, and project limits on installed capacity (such as 10 kW in NSW) mean effectively that FITs do not support large scale projects such as wind farms or solar thermal power stations.\n\nSome schemes are based on a gross feed-in tariff model while most are based on a net tariff.\n\nNet feed-in tariff schemes have been criticised for not providing enough incentive for households to install solar panels and thus for not effectively encouraging the uptake of solar PV. They have been described as a \"fake feed-in tariff\". Critics of net FIT argue that gross tariffs conform to the normal definition of a feed-in tariff, and provide a more certain financial return, paying for all electricity produced, even if it is consumed by the producer, reducing or helping meet peak demand. The issue still remains as to what level the FIT rate should be set — e.g., at the cost of production, at market rates, at the cost at which the retailer sells electricity, or at the rate at which the retailer can acquire electricity in the wholesale market, while others set them at premium or subsidy levels.\n\nThe effective difference is that household producers under a gross scheme pay for electricity from the electricity retailer for household consumption at the market rate while all the power produced by them is sold to the retailer at the higher subsided FIT rate. Net FIT schemes effectively use the same rate for the use of electricity by household producers as for sale into the grid (i.e., use of electricity by the household reduces the amount of electricity available to feed into the grid), and accordingly the subsidy to household producers is generally less in overall terms.\n\nThe ACT and New South Wales had gross feed-in tariffs, which were subsequently replaced with net feed-in tariffs.\n\nA uniform federal scheme to supersede all State schemes has been proposed by Tasmanian Greens Senator Christine Milne, but not enacted. National feed-in tariff systems have been enacted in numerous countries including Brazil, Canada, China and many EU countries.\n\nThe Federal Parliament has not yet enacted a national gross feed-in tariff scheme for renewable energy. However, a capital grant/rebate was offered of up to $8,000 per household for domestic installations and 50% for school installations up until June 2009.\n\nIn July 2008, a bill was introduced by Australian Greens Senator Christine Milne, (Tasmania), called the \"Renewable Energy (Electricity) Amendment (Feed-In Tariff) Bill 2008\". The bill was the subject of an inquiry by the Senate Standing Committee on Environment, Communications and the Arts.\n\nMore than 23,000 people have signed an online petition for a national gross feed-in tariff.\n\nIn a speech, the Federal Minister for Energy, Martin Ferguson, said that feed-in tariffs are technologically prescriptive and ideologically based, rather than being a market based mechanism. In response to the German feed-in tariff for solar, he suggested that Germany's solar subsidy meant German consumers paid more than €1 billion in additional power bills in 2007 to generate around 0.5% of Germany’s gross electricity consumption, suggesting that the policy does not deliver value for money. He also suggested that an Australian solar feed-in tariff may lead to greater PV panel imports rather than a significant expansion of Australian production. However commentators have suggested that Martin Ferguson's comments are ideologically driven and do not take into account the Merit Order Effect which in some instance negates or almost negates the cost of funding FiTs and in other instances shows funding FiTs delivers a net dividend to consumers.\n\nUnder the Electricity Feed-in (Renewable Energy Premium) Act 2008, Canberrans can install photovoltaic (solar) cells or other renewable sources, produce their own energy, and sell it back to the power grid but perhaps not until July 2009. They'll be paid a tariff 3.88 times the retail cost of electricity (60c/kWh Jan 2009) for the energy they feed back into the electrical grid for up to 20 years from the date they sign up to the scheme. It is a gross metered scheme meaning that owners get paid a premium rate for all electricity produced by their installation with their own usage being metered separately. The ACT scheme will be the most generous feed in scheme in Australia when it comes into operation some time after March 2009.\n\nAs the Australian Capital Territory (\"ACT\") scheme is a gross feed in scheme, it is a relatively easy task to estimate the payback time for various system sizes because the calculation is independent of electricity consumption. e.g. a well positioned location in the ACT could produce approximately 1800 kWh per year from a 1 kW system or A$1,080 per year at a price of 60 c/kWh. On this basis payback for a 1 kW system costing $5,000 net (includes $8,000 federal rebate & 22 RECS) would be around 5 years. Larger systems would take longer but would have a larger income stream over the life of the system.\n\nThe ACT Government established an inter-departmental committee to look at feed in tariffs in November 2007. There is some concern that a proposed commercial solar power plant, the feasibility of which is to be examined by a joint study be ACT government and ActewAGL, may undermine the proposed feed in tariff proposal.\n\nThe submission of the Independent Competition and Regulatory Commission 25 February 2008 was that there were issues to be resolved including:\n\nOn the grounds that a capacity cap of 30MW had been reached, the FIT scheme in the ACT was closed to micro and medium scale generators at midnight on 13 July 2011 just days after the legislation was amended (by the Greens and Liberal parties voting together against the will of the government Labor party) on 12 July 2011 to enable micro-generators to continue to access it. The micro-generator category under the Act had earlier been closed on 31 May 2011 once a statutory cap of 15 MW had been reached.\n\nIn September 2006, it was announced that the \"Electricity (Feed-In Scheme-Solar Systems) Amendment Bill 2008\" would come into force on 1 July 2008 and run out on 30 June 2028. The normal tariff for electricity is $0.26 / kWh and the feed-in tariff was set at $0.44 / kWh. The result is that not only will thousands of South Australian homes have solar systems installed, but many businesses will now have the opportunity to embrace this technology and turn their roofs into mini-renewable power stations. The former South Australian Premier, Mike Rann, wanted the government to be carbon neutral by the year 2020.\n\nThe original intent of some parties, for the feed-in tariff to increase with increases to the price of electricity, at a multiple of two was negotiated out before legislated. The 44 cent tariff is only paid for any electricity exported, so only when the system output exceeds domestic demand.\n\nThe solar feed-in scheme commenced on 1 July 2008. It was reviewed in 2009-10 and amendments to the legislation took effect from 29 July 2011. The major change to the scheme was to introduce the concept of customer groups, dependent on the dates of approval and connection of the solar PV system, and the reduction of the feed-in tariff for customers receiving approval after 30 September 2011.\n\nGroups 1, 2 and 3 (broadly existing customers) will continue to receive 44 cents and in addition will receive a minimum retailer payment (estimated to start at about 6 cents) to give a total feed-in payment of about 50 cents. Group 4 (approved permission to connect received between 1 October 2011 and 30 September 2013) will receive 16 cents plus the minimum retailer payment to give a total feed-in payment of about 22 cents. Group 5 (approved permission to connect received after 30 September 2013) will not receive any feed-in tariff, but will be eligible for the minimum retailer payment, that is a total payment of about 6 cents. Retailers are free to pay customers more than the legislated minimum rates.\n\nThe feed-in tariffs are fixed until 30 June 2028 for Groups 1, 2 and 3 (i.e. the 44 cent rate), and until 30 September 2016 for Group 4 (i.e. the 16 cent rate). The minimum retailer payment will be determined by the Essential Services Commission of South Australia (ESCOSA) has been estimated to commence at about 6 cents from 1 January 2013, and will be reviewed and is expected to increase with the price of electricity over time.\n\nRebates: The South Australian government does not offer any additional rebates or incentives to domestic customers, it’s a solar schools program.\nEstimated payback: It’s very hard to calculate, but it will be best for systems of largest eligible size, where the domestic demand is smallest, or mainly occurs at night and low power consumption during the day. Houses without air-conditioning would seem to fit the bill.\n\nOn 8 May 2008, the NSW government announced that it intended to introduce feed in tariffs. Further details of the solar feed in tariff in NSW were announced on 23 November 2008.\n\nNSW Feed-in Tariff Taskforce was established to advise the NSW Government on the details of a feed-in tariff scheme for NSW. Its representatives are from the Department of Water and Energy, the Department of Environment and Climate Change, the Department of Premier and Cabinet and NSW Treasury. In January 2009, the Taskforce issued a submission on the design of a feed-in tariff scheme for NSW.\n\nOn 23 June 2009, the government announced a Solar Bonus Scheme. This is a net feed-in tariff for the state rather than a gross tariff. Under this 20-year scheme, some residents, schools, small businesses and community groups will be paid $0.60 per kwh for the net amount of electricity sold back into the grid. As few as 42,000 households are expected to take advantage of the scheme. The scheme was set to begin on 1 January 2010 and to be reviewed in 2012. The Solar Bonus Scheme is payable to energy customers with solar panel systems up to 10 kW in size;.\n\nOn 10 November 2009 it was reported that NSW will switch to a gross feed in tariff. This is expected to increase the average amount received by a household with solar panels by $1500 pa, a 60% increase, highlighting the difference in effect of gross compared to net feed in tariffs. This is expected to reduce the payback period for installation costs to about 8 years. The gross tariff will cost all households about $2 pa.\n\nIn October 2010 the New South Wales government cut the solar feed in tariff to $0.20 per kWh, the lowest level in Australia.\n\nIn April 2011 the NSW FIT (Solar Bonus) scheme was retrospectively closed to new applicants and removed for current users after a Ministerial declaration was made under s.15A(8C) Electricity Supply Act 1995 (NSW) that the cumulative total installed capacity cap of 300 MW had been reached.\n\nA new feed-in tariff is proposed, with the constraint that it not raise the cost of electricity and not involve funding from the NSW government. This inherently limits the FIT to less than the consumer electricity cost, and does not conform to the normal definition of a feed-in tariff. A feed-in tariff of from 5.2 to 10.3 cents/kWh is proposed. By the normal definition of a feed-in tariff, the cost of a new and expensive generation source, e.g. solar panels, is borne by all of the consumers rather than the early adopters, who instead are rewarded. It is implemented in the expectation that volume will reduce the cost, thus allowing more people to use that source. Feed-in tariff advocacy organizations claim that feed-in tariff is both the least expensive, and the most effective incentive.\n\nThe Western Australian Government had committed to a limited Gross feed in tariff for household scale PV. The scheme was to be funded for four years at a cost of $13.5m and based on a payment of 60c kW/h to re cover the capital cost of the system after any rebates. After the capital cost is paid back the tariff will revert to a lower rate or the existing Renewable Energy Buyback Scheme (REBS).\n\nOn 2 June 2009 the state government backflipped on this promise and has deferred the introduction of the scheme for 1 year until 1 July 2010. In the new scheme a net metering system would be applied rather than gross. The $13.5m of funds had already been allocated to systems installed prior to the 1 July 2009 implementation date in a public display of support for the concept. The over-subscription highlighted the imbalance between public and state support for the renewable energy industry.\n\nOn 1 August 2010, the net feed in tariff commenced and existing customers can migrate to the new billing system. New customers will need to sign up the Renewable Energy Buyback Scheme from their state electricity provider, Synergy.\n\nThe Feed In Tariff offered by the state government is 40 cents whilst Synergy offers 7 cents via the Renewable Energy Buyback Scheme. Once the customer signs up, they will receive both incentives for 10 years.\n\nOn 1 August 2011, the state government suspended all new applications for the Feed in Tariff, citing the expense of the programme as the reason.\n\nThe situation is better for Horizon Power residential customers, who are offered renewable energy buy-back rates equal to their electricity tariff rate (A2 tariff) less the GST component (10%). The current electricity charge - cents per unit is 20.83c including GST. Horizon Power's rebate is more than double the rate paid by Synergy.\n\nThe Queensland Government Solar Bonus Scheme is a program that pays domestic and other small energy customers for the surplus electricity generated from roof-top solar photovoltaic (PV) systems that is exported to the Queensland grid. It commenced on 1 July 2008. The scheme provides for 44c/kWh (around twice times the current general domestic use tariff of 21.35c/kWh (including GST)) on the net amount exported to the grid, subject to having proper metering installed.\n\nA feed-in tariff will ensure that Queenslanders benefit from the federal Photovoltaic Rebate Program.\n\nWhilst the government scheme has now ended for new installs, energy retailers still offer competitive feed in tariffs. As of July 1st, 2017, Click Energy provides 16c/kWh and Origin provides 14c/kWh.\n\nIn November 2009 Victoria set up a net metered feed-in tariff scheme, under which households were paid 60 cents for every excess kilowatt hour of energy fed back into the state electricity grid. This was about twice the then current retail price for electricity. The feed-in tariff scheme was to run for 15 years from November 2009, and applied to all household systems of up to 5 kW capacity and had a cap of 100MW of generating capacity and/or a $10 per annum increase in the average household electricity bill.\n\nA feed-in tariff scheme was originally announced on 7 May 2008 which had been described by environment groups as ineffective, as the then proposed 2 kW cap on array size, combined with net metering, meant that very little surplus power would be put into the grid so very little of the high tariff would actually be paid. Environment groups and renewable energy companies called for the Victorian feed-in tariff to be paid on gross metering with a 10 kW cap on array size to overcome these problems, but these concerns were only partially addressed in the 2009 scheme.\n\nOn 1 September 2011, it was announced that the 60 cent tariff would be closed to new installations after 30 September 2011 after 88,000 installations, and a new Transitional tariff of at least 25 cents would apply to systems installed afterwards. The Transitional tariff will be paid until the end of 2016, when tariffs will be reviewed. The minimum feed-in tariff that applies to new applicants from 1 January 2016 was 5 cents per kilowatt hour (c/kWh). The minimum feed-in tariff that applies to new applicants from 1 July 2017 is 11.3 cents per kilowatt hour (c/kWh). This is broken down as follows.\nFeed-in tariff component.\nForecast solar-weighted average wholesale electricity pool cost - 8.1 (c/kWh).\nValue of avoided distribution and transmission losses - 0.6 (c/kWh).\nAvoided market fees and ancillary service charges - 0.1 (c/kWh).\nValue of avoided social cost of carbon - 2.5 (c/kWh).\nFiT - 11.3 (c/kWh).\n\nFrom 1 July 2018 the minimum electricity feed-in tariff for households and businesses who feed power back into the electricity grid from small renewable energy sources will be 9.9 cents per kilowatt hour (kWh). This is broken down as follows. Feed-in tariff component. Forecast solar-weighted average wholesale electricity pool cost - 6.8 (c/kWh). Value of avoided distribution and transmission losses - 0.5 (c/kWh). Avoided market fees and ancillary service charges - 0.1 (c/kWh). Value of avoided social cost of carbon - 2.5 (c/kWh). FiT - 9.9 cents per kilowatt hour (kWh). For the first time, retailers will be able pay solar system owners either a single-rate tariff or time-varying tariff. The single rate tariff is 9.9 cents per kilowatt hour. Under the time-varying tariff, customers will be credited between 7.1 cents and 29.0 cents per kilowatt hour of electricity exported, depending on the time of day.\n\nOn Tuesday, 3 March 2008, the Premier of Tasmania announced that the Government will consider a mid-year report on the introduction of minimum feed-in tariffs to support householders and small energy consumers using solar panels and other forms of domestic renewable energy and that provide surplus energy into the electricity grid.\n\nNorthern Territory is yet to make an announcement on feed-in tariffs as a means of subsidising and encouraging solar PV, other than in relation to Alice Springs.\n\nIn 2006 there was a bid to make Alice Springs a Solar City. Australian Government funding would give four different regions in Australia the chance to become a Solar City and share $75 m funding to make solar power projects a reality. If successful, the Alice Springs project would involve solar power generation plus investigating energy efficiency, smart metering and tariff pricing.\n\nIn Alice Springs, an official Solar City, from May 2008 people with grid connected PV systems can sell all the solar electricity they generate back to Power and Water Corporation at 45 cents per kilowatt–hour, which is more than double the cost of purchasing electricity from the grid.\n\nOutside of the Alice Springs Solar City area, people with Solar PV in the NT can arrange to sell their gross electricity production to the Power & Water Corporation of the Northern Territory at 14.38c. They can effectively use the grid as a bank and notionally re-purchase the electricity at night.\n\nSome electricity retailers are offering feed in tariff rates above the minimum stipulated by government. Many of the larger corporate retailers offer these incentives to capture larger proportions of the green electricity customer base, who are known to have lower churn rates.\n<nowiki>* 50.05c below 10kW / 40.04c below 30kW system</nowiki>\n<nowiki>** 45.76c capped at $5 per day, 23.11c for each kWh above $5 per day in Alice Springs, At purchase rate elsewhere in NT</nowiki>\nAccording to the COAG communiqué released in November 2008 COAG agreed to a set of national principles to apply to new feed-in tariff schemes and to inform the reviews of existing schemes. These principles will promote national consistency of schemes across Australia. According to the Communique the basic principles are:\n\nThese principles do not appear to support a gross feed in tariff as exists in Germany, but rather a net feed in tariff.\n\nA Bill for a national feed-in tariff introduced by Senator Milne has not progressed after the Rudd government indicated it would not support such legislation. Some reasons were provided through the majority report of a Senate Committee examining the Bill. In the ACT, in February 2010, the ACT Independent Competition and Regulatory Commission proposed reducing the amount paid to RE generators under the Territory's FIT law.\n\nIt has been reported that NSW households could pay an extra $600 on their electricity bill over six years ($8.33/month) to cover the $2 billion cost of the tariff scheme. The total cost to families in some regional areas could be $1000.\n"}
{"id": "40160112", "url": "https://en.wikipedia.org/wiki?curid=40160112", "title": "Gadolinium-doped ceria", "text": "Gadolinium-doped ceria\n\nGadolinium-doped ceria (GDC) (known alternatively as gadolinia-doped ceria, gadolinium-doped cerium oxide, cerium(IV) oxide, gadolinium-doped, and GCO, formula Gd:CeO) is a ceramic electrolyte used in solid oxide fuel cells (SOFCs). It has a cubic structure and a density of around 7.2 g/cm in its oxidised form. It is one of a class of ceria-doped electrolytes with higher ionic conductivity and lower operating temperatures (<700 °C) than those of yttria-stabilized zirconia, the material most commonly used in SOFCs. Because YSZ requires operating temperatures of 800–1000 °C to achieve maximal ionic conductivity, the associated energy and costs make GDC a more optimal (even \"irreplaceable\", according to researchers from the Fraunhofer Institute) material for commercially viable SOFCs.\n\nOxygen vacancies are created when gadolinium (a trivalent cation) is introduced into ceria (CeO, with Ce in the 4+ oxidation state) or on reduction in CO or H. The high concentration and mobility of the oxide ion vacancies results in a high ionic conductivity in this material. In addition to its high ionic conductivity CGO is an attractive alternative to YSZ as an electrolyte due to low reactivity and good chemical compatibility with many mixed conducting cathode materials. Dopant levels of Gd typically range from 10% to 20%. The majority of SOFC researchers and manufacturers still favor the use of YSZ over CGO due to YSZ having superior strength and because CGO will reduce at high temperature when exposed to H or CO.\n\nMethods of synthesis have included precipitation, hydrothermal treatment, sol-gel, spray pyrolysis technique (SPT), combustion and nanocasting using cerium sources such as cerium nitrate, ammonium ceric nitrate [3] , cerium oxalate, cerium carbonate, cerium peroxide, and cerium hydroxide. GDC has been synthesized in such forms as powder, ink, solid solutions, discs, and nanomaterials (including nanoparticle, nanocrystals, nanopowder, and nanowires).\n\n\n"}
{"id": "20490276", "url": "https://en.wikipedia.org/wiki?curid=20490276", "title": "Glidant", "text": "Glidant\n\nA glidant is a substance that is added to a powder to improve its flowability. A glidant will only work at a certain range of concentrations. Above a certain concentration, the glidant will in fact function to inhibit flowability.\n\nIn tablet manufacture, glidants are usually added just prior to compression.\n\nExamples of glidants include magnesium stearate, fumed silica (colloidal silicon dioxide), starch and talc.\n\nA glidant's effect is due to the counter-action of factors that cause poor flowability of powders. For instance, correcting surface irregularity, reducing interparticular friction and decreasing surface charge. The result is a decrease in the angle of repose which is an indication of an enhanced powder's flowability.\n"}
{"id": "31952935", "url": "https://en.wikipedia.org/wiki?curid=31952935", "title": "Inga alley cropping", "text": "Inga alley cropping\n\nInga alley cropping refers to planting agricultural crops between rows of Inga trees. It has been promoted by Mike Hands.\n\nUsing the Inga tree for alley cropping has been proposed as an alternative to the much more ecologically destructive slash and burn cultivation. The technique has been found to increase yields. It is sustainable agriculture as it allows the same plot to be cultivated over and over again thus eliminating the need for burning of the rainforests to get fertile plots.\n\n \nInga trees are native to many parts of Central and South America. Inga grows well on the acid soils of the tropical rainforest and former rainforest. They are leguminous and fix nitrogen into a form usable by plants. Mycorrhiza growing within the roots (arbuscular mycorrhiza) was found to take up spare phosphorus, allowing it to be recycled into the soil.\n\nOther benefits of Inga include the fact that it is fast growing with thick leaves which, when left on the ground after pruning, form a thick cover that protects both soil and roots from the sun and heavy rain. It branches out to form a thick canopy so as to cut off light from the weeds below and withstands careful pruning year after year.\n\nThe technique was first developed and trialled by tropical ecologist Mike Hands in Costa Rica in the late 1980s and early '90s. Research funding from the EEC allowed him to experiment with species of Inga. Although alley cropping had been widely researched, it was thought that the tough pinnate leaves of the Inga tree would not decompose quickly enough.\n\nAs the crops grow, so does the Inga. When the crops are harvested the Inga is allowed to grow back. Once more it closes the canopy, is pruned, and the cycle is repeated, time and again. Leaves pruned from the tree decompose on the ground releasing phosphorus for crops. Fungi take up phosphorus to repeat the cycle.\n\nUsing this system, not only do the farmers grow their basic crops of maize and beans, but also cash crops. Previously this was not possible because when the plot was a distance from the farmer's home, consistent guarding and tending could be too challenging. Now with the same plot being used continuously, it can be near home, thus allowing an entire family to help to tend and guard it, even when there are young children.\n\nThe Inga is used as hedges and pruned when large enough to provide a mulch in which bean and corn seeds are planted. This results in both improving crop yields and the retention of soil fertility on the plot that is being farmed. Hands had seen the devastating consequences that are caused by slash and burn agriculture while working in Honduras; this new technique seemed to offer the solution to the environmental and economic problems faced by so many slash and burn farmers.\n\nAlthough this technique has the potential to save rainforest and lift many out of poverty, Inga alley cropping has not yet reached its full potential, although the charity Inga Foundation, headed by Mike Hands, has been consulted about potential projects in Haiti ( which is almost completely deforested) and the Congo. Discussions have also been mooted about projects in Peru and Madagascar.\n\nFor Inga alley cropping the trees are planted in rows (hedges) close together, with a gap, the alley, of about 4m between the rows. An initial application of rock phosphate has kept the system going for many years.\n\nWhen the trees have grown, usually in about two years, the canopies close over the alley and cut off the light and so smother the weeds.\n\nThe trees are then carefully pruned. The larger branches are used for firewood. The smaller branches and leaves are left on the ground in the alleys. These rot down into a good mulch (compost). If any weeds haven't been killed off by lack of light the mulch smothers them.\n\nThe farmer then pokes holes into the mulch and plants their crops into the holes.\n\nThe crops grow, fed by the mulch. The crops feed on the lower layers while the latest prunings form a protective layer over the soil and roots, shielding them from both the hot sun and heavy rain. This makes it possible for the roots of both the crops and the trees to stay to a considerable extent in the top layer of soil and the mulch, thus benefiting from the food in the mulch, and escaping soil pests and toxic minerals lower down. Pruning the Inga also makes its roots die back, thus reducing competition with the crops.\n\nMike Hands is a former academic researcher, current UK farmer and environmentalist, who pioneered the research into Inga alley cropping and currently promotes the concept through the Inga Foundation, a UK registered charity he founded in 2007 to support projects in Central and South America.\n\nWhen the UK Environment Agency listed the \"all-time\" \"scientists, campaigners, writers, economists and naturalists\" who, in its view, have done the most to save the planet, Hands was placed 44th, just below Andrew Lees, and one place above German politician and activist, Petra Kelly.\n\n\n\n"}
{"id": "3838616", "url": "https://en.wikipedia.org/wiki?curid=3838616", "title": "John Benjamin Dancer", "text": "John Benjamin Dancer\n\nJohn Benjamin Dancer (8 October 1812 – 24 November 1887) was a British scientific instrument maker and inventor of microphotography. He also pioneered stereography. By 1835, he controlled his father's instrument making business in Liverpool. He was responsible for various inventions, but did not patent many of his ideas. In 1856, he invented the stereoscopic camera (GB patent 2064/1856). He died at the age of 75 and was buried at Brooklands Cemetery, Sale, Greater Manchester.\n\nDancer improved the Daniell cell by introducing the porous pot cell, which he invented in 1838. He was a leading inventor and practitioner in the emerging field of microphotography, work he began shortly after the Daguerreotype process was first announced in 1839. His novel uses of microphotography, such as \"the reduction of the 680 word tablet erected in memory of the electrician William Sturgeon to a positive one-sixteenth of an inch in diameter\", attracted much public attention. Dancer was remembered as a person very willing to share his expertise with others. For example, he assisted the physicist James Prescott Joule with the development of scientific instruments such as an apparatus for measuring the internal capacity of the bore of thermometer tubes, a tangent galvanometer, and other devices useful in Joule's research. A substantial collection of Dancer's papers, photographs, and apparatus is held by the Ransom Center at the University of Texas.\n\nIn 1842 Dancer took a daguerreotype from the top of the Royal Exchange which is the earliest known photograph showing part of Manchester.\n\n"}
{"id": "9814254", "url": "https://en.wikipedia.org/wiki?curid=9814254", "title": "Latvenergo", "text": "Latvenergo\n\nLatvenergo is a state-owned electric utility company in Latvia. The company generates about 70% of the country's electricity.\n\nLatvenergo has four hydroelectric power plants: Pļaviņu HES, Rīgas HES, Ķeguma HES and Aiviekstes HES, with total installed capacity of 1535 MWh, two combined heat and power plants with total electrical capacity of 474 MWe and heat capacity of 1525 MWt and a wind farm near Ainaži with installed capacity of 1.2 MW.\n\nLatvenergo is a parent company for the distribution company \"Sadales tīkls\" and the owner of the transmission system \"Latvijas elektriskie tīkli\", while the transmission system is operated by the independent transmission system operator \"Augstsprieguma tīkls\", a former subsidiary of Latvenergo. The company also co-owns \"Liepājas enerģija\", which has installed power capacity of 12 MWe and heat capacity of 427 MWt.\n\n"}
{"id": "954829", "url": "https://en.wikipedia.org/wiki?curid=954829", "title": "Lincoln Navigator", "text": "Lincoln Navigator\n\nThe Lincoln Navigator is a full-size luxury SUV marketed and sold by the Lincoln brand of Ford Motor Company since the 1998 model year. Sold primarily in North America, the Navigator is the Lincoln counterpart of the Ford Expedition. While not the longest vehicle ever sold by the brand, it is the heaviest production Lincoln ever built. It is also the Lincoln with the greatest cargo capacity and the first non-limousine Lincoln to offer seating for more than six people.\n\nThe Lincoln Navigator was the first Lincoln (aside from the Lincoln Versailles) to be produced in a factory outside the Wixom Assembly Plant since 1958. From 1997 to 2009, production was sourced at the Michigan Assembly Plant in Wayne, Michigan. Since 2007, production has also been sourced from the Kentucky Truck Plant in Louisville, Kentucky.\n\nAt the end of the 1980s, in the United States, sport-utility vehicles gradually began to transition from dedicated off-road vehicles towards dedicated family vehicles, similar to station wagons. In 1991, the Jeep Grand Wagoneer ended its 28-year production run. However, by the time of its discontinuation, the vehicle had gained a famous reputation for its high content, featuring the same content as a luxury sedan. Following its official entry into the United States in 1987, the Range Rover was upgraded in trim corresponding with its higher price.\n\nAs a response, General Motors introduced the Oldsmobile Bravada in 1991, convincing several other manufacturers to introduce mid-size luxury SUVs. As full-size SUVs such as the Chevrolet Suburban and Ford Expedition are highly profitable vehicles, the Lincoln Navigator came to life.\n\nThe Lincoln Navigator was launched on July 1, 1997 for the 1998 model year, with the first vehicle rolling off the assembly line on May 14, 1997. Based directly on the Ford Expedition, introduced the year before, the Navigator gave the Lincoln-Mercury division its first full-size SUV (slotted above the Ford Explorer-based Mercury Mountaineer).\n\nIn its first calendar year of sales (1998), Navigator contributed to an unprecedented event of recent decades – the overtaking of the Lincoln brand by perennial rival Cadillac in annual sales volume. Initially, published figures indicated that Cadillac had outsold Lincoln by a scant 222 vehicles sold, thanks to an enormous surge in Cadillac Escalade sales in December 1998, from hundreds in previous months to almost 5,000. A subsequent audit resulted in a retraction/apology in May 1999, attributing the \"error\" to \"overzealous\" \"low-level\" employees.\n\nThe Lincoln Navigator was also developed under the Ford program code name UN173, with the Expedition developed under the UN93 program code name. A full-size body-on-frame vehicle, the Navigator was mechanically related to the Ford Expedition; both vehicles were related to the 1997 Ford F-Series. The Navigator featured independent front suspension (short-long arm/SLA); the rear suspension was of a live rear axle design. Using an optional feature from its Ford counterpart, the Navigator was designed with load-leveling air suspension; tuned primarily for ride comfort, the air suspension lowered itself when the vehicle was parked.\n\nAlthough technically available with rear-wheel drive, the primary drivetrain on the Lincoln Navigator was ControlTrac, a computer-controlled automatic four-wheel drive system. As with the Expedition, the Navigator was fitted with four-wheel anti-lock disc brakes.\n\nUsing the same 230 hp 5.4L Triton V8 as the Expedition/F-150 paired with the 4-speed 4R100 automatic transmission, the 1998 Lincoln Navigator came with a towing capacity of . During 1999, Lincoln would fit two different engines in the Navigator in an effort to better match full-size SUVs from General Motors. At the beginning of the model year, the Triton V8 was upgraded to 260 hp; as a running change during the model year, Lincoln would replace the SOHC Triton with a 300 hp DOHC 5.4L V8, named InTech (borrowing a name from the Mark VIII). Due to the increase in power, towing capacity would increase to over .\n\nAlthough the Lincoln Navigator shares the same bodyshell as the Ford Expedition, giving it a similar exterior appearance, Lincoln stylists would make many design changes to differentiate the two vehicles. Forward of the windshield, the Lincoln Navigator shares no body panels with its Ford counterpart, with its own front fascia (a grille design shared with the 1998 Lincoln Town Car and the 2000 Lincoln LS), wheels, roof rack, lower body trim, and taillights. The interior of the two vehicles shared more commonality, with the dashboard common to both vehicles (with greater use of wood trim); the Navigator was given its own seat design. To make for a quieter interior over the Expedition and Ford F-Series, the Navigator made greater use of sound deadening materials and higher-quality carpeting.\n\nThe Lincoln Navigator included standard features available or optional on the Expedition, including power driver and passenger bucket seats, 2nd-row bucket seats (with a 2nd-row bench seat as a delete option), floor consoles, and keyless entry. The few options available included a power moonroof, a universal garage door opener, 7 seven to 8 eight passenger seating an electrochromic rearview mirror (filtering out headlight glare from vehicles seen in the mirror), and a premium audio system (a seven speaker, 290-Watt audio system with a 6-disc front console-mounted CD-changer, and rear seat audio controls), and 17-inch alloy wheels.\n\nDuring its production run, Lincoln made few changes to the first-generation Navigator. In 1999, alongside the addition of the InTech V8, power-adjustable brake and accelerator pedals were added; the previously optional 17-inch wheels became standard.\n\nFor 2000, the fender-mounted radio antenna was integrated into the right-rear window, while the interior received \"Nudo\" leather seating surfaces. The options list expanded to include a satellite navigation system, heated and cooled front seats, a reverse-sensing system, and side-impact airbags.\n\nFor 2001, several minor cosmetic changes were made. On the grille and tailgate, the Lincoln emblem was given a black background (replacing the previous red); on the tailgate, the Lincoln and Navigator badging switched sides. A VHS-based video entertainment system became an option.\n\nFor 2003, a number of changes and improvements were made to the Navigator thanks to a thorough redesign. The Navigator continued to share a platform with the Ford Expedition, which was also redesigned for 2003, but continued to differ from it in terms of styling and various upscale features. The 2003 redesign featured a thoroughly revised exterior, the first since the Navigator's launch, with only the front doors and roof panel unchanged from the previous generation. The new exterior came with things such as a larger chrome waterfall grille, brighter quad-beam headlights with larger housings, revised chrome door handles set in color-keyed bezels, and slightly wider running boards. Inside the Navigator was an all-new instrument panel and dashboard area which, significantly, was not shared with the Expedition. Inspired by the symmetrical, \"dual-cockpit\" layout of the 1961 Lincoln Continental, the instrument panel and dashboard area was adorned with real walnut burl wood inserts and panels and switches painted with a low-luster satin nickel color. Adding to the upscale interior design further were white LEDs, 120 in all, which provided backlighting for controls and switches. Additionally, to direct attention to the high-quality satin nickel-faced analog clock mounted in the dashboard, an articulating door is present to conceal the radio head unit and optional satellite navigation system when they are not in use.\n\nHighlighting the Navigator's design changes were other new features and options for 2003. Newly available features like Ford's \"Safety Canopy\" side curtain airbags and a tire pressure monitoring system improved occupant safety. Convenience was enhanced by the availability of power running boards (an industry first), power-folding third row seats, a power liftgate, and HID headlights (for top end models). The available rear-seat video entertainment system was updated to be DVD-based and all Navigators now came with standard 18x7.5-inch alloy wheels with 18x8-inch chrome wheels available as an option.\n\nLike the redesigned 2003 Expedition (U222), the Navigator benefited from a reworked chassis, new rack-and-pinion steering, and an all-new independent rear suspension (IRS), which brought better handling and ride comfort. The Navigator continued to benefit from a load-leveling air suspension but it now lowered the vehicle by an inch when stopped in the interest of easing entry and exit. The Navigator's powertrain was modified from the UN173, but the 5.4 L DOHC V8 used before was no longer advertised under the InTech name. But it now produced at 5500 rpm and of torque at 3750 rpm Due to changes brought with the redesign, the Navigator's base curb weight increased to in two-wheel drive models and nearly in four-wheel drive models. In turn, towing capacity dropped slightly.\n\nTire-pressure monitoring was made standard for 2004 while Ford's \"AdvanceTrac\", a type of traction control system, with Roll Stability Control was an option. In 2004, for 2005 the Navigator received a minor facelift with new square-shaped foglights replacing the circular ones used previously. AdvanceTrac with RSC was now standard while HID headlights were available on all models. In the interest of cost effectiveness, the 5.4 L DOHC V8 introduced in the 1999 model year was replaced by the same 5.4 L 3-valve SOHC V8 that had been available in the F-150 since the 2004 model year. Though having a different head design, the new engine offered similar overall output, producing at 5000 rpm and of torque at 3750 rpm. The new engine was not marketed under the \"Triton\" name in the Navigator even though it is mechanically identical to the F-150's engine. The 4R75W 4-speed automatic transmission used from 2003 until 2004 was replaced with a new ZF-sourced 6-speed automatic transmission. The 2005 Navigator's base curb weight fell to while four-wheel drive models dropped to . Towing capacity increased slightly over the previous model year to in two-wheel drive models and in four-wheel drive models. For 2006, an \"Elite\" package for the \"Ultimate\" trim level was made available, including a DVD-based satellite navigation system with a voice-activated touch screen, THX audio system, rear-seat DVD entertainment system, and HID headlights.\n\nThe Navigator was redesigned under the U326 program code name, with new styling and mechanical features for 2007. Unveiled at the Chicago Auto Show in February 2006, the Navigator featured its most distinctive styling update since its introduction with new front and rear fascias and side cladding. In the front was a large, upper chrome grille resembling those of classic Lincolns like the 1963 Continental, an all-new, more complex headlight design, a lower chrome grille with integrated foglights, and a more prominent \"power dome\" hood. Elsewhere, an updated rear fascia featured taillights inspired by the Lincoln MKZ and chrome trim was more prominently used along the Navigator's sides, including chrome lower body molding on the doors. The distinctiveness of the Navigator's exterior followed into the interior, highlighted by an all-new dashboard and instrument panel. These areas saw an extensive use of rectangular shapes, such as in the gauges, as well as greater use of real wood and satin nickel accents.\n\nAccompanying the Navigator's redesign for 2007 was a new model, the Navigator L developed under the U418 program code name. Comparable to the Cadillac Escalade ESV, the Navigator L is longer than the standard Navigator on a longer wheelbase, increasing its cargo capacity. The Navigator L was introduced parallel to the Expedition EL, an extended version of the Ford Expedition. Both the Navigator and Expedition were redesigned for 2007 and based on Ford's T1 platform, which is related to the same platform that the 2004+ F-150 is based on. Compared to the Navigator's previous platform, the new platform provides greater rigidity for better driving dynamics. The independent rear suspension has been replaced with a new five-link IRS design to further enhance handling and ride quality. The Navigator continued to come with standard 18-inch alloy wheels but both 20- and 22-inch wheels are now available. The 5.4 L 3-valve SOHC V8 introduced in the 2005 Navigator remained unchanged for 2007. The ZF Friedrichshafen 6-speed automatic transmission was replaced by Ford's own 6-speed design, the 6R80 for the 2009 model year. Due to the Navigator's redesign, its base curb weight increased to in two-wheel drive models and in four-wheel drive models. Navigator L models were even heavier at in two-wheel drive models and in four-wheel drive models. In spite of this, thanks to their improved frame, the Navigator's towing capacity increased for 2007, approaching in two-wheel drive models.\n\nSince the 2007 model redesign, the Navigator no longer uses the same transfer case as the Expedition. Expedition four-wheel drives continue to use a two-speed dual range transfer case with off-road low range reduction gearing and default off-road program that remaps (reprograms) the electronic throttle control and traction control system response for off-road conditions. Navigator four-wheel drives have been demoted to a light-duty one-speed single range transfer case which lacks low range gearing.\n\nFor 2008, packaging for the Navigator's luxury and convenience features was simplified, resulting in the elimination of the perennial \"Luxury\" and \"Ultimate\" trim levels and the standardization of a number of features that were previously optional. Some of these newly standard features included heated and cooled front seats, power-folding third row seats, a power liftgate, and a 600-Watt 14-speaker THX II-Certified audio system. Also newly standard was a 3.31:1 rear axle ratio, though a 3.73:1 ratio was still available as option for the Navigator and remained standard in the Navigator L. Newly available was a rearview camera to aid in backing up.\n\nFor 2009, the Navigator's 5.4 L V8 gained as well as flex-fuel capability. The rearview camera that was new for 2008 was now standard, as were heated second row seats, \"Front Park Assist\", a capless fuel filler, rain-sensing windshield wipers, and Lincoln SYNC.\n\nThe third generation Navigator and Navigator L continued to be offered for the 2010 model year, with only slight changes.\n\nFor 2011, both the Navigator and the Navigator L feature HD Radio, Sirius TravelLink, and Lincoln SYNC as standard on all trims.\n\nFor the 2013 model year, the only changes made to the Navigator were the addition of new color trims: Kodiak Brown Metallic Tri-Coat, Midnight Sapphire Metallic, and Ruby Red Metallic Tinted Clearcoat.\n\nThe 2014 model year was a carryover from 2013 with the same features and no cosmetic changes. This would be the last model year that it would feature the front grille fascias.\n\nIn March 2013, Ford had confirmed reports that the next generation Lincoln Navigator would not be a repackaged Ford Expedition as the previous generations were, despite trailing the MKX in terms of sales but ahead of the MKT, but hopes to make it more competitive in the luxury SUV segment as they prepare to take on the Cadillac Escalade, Infiniti QX80, and the Mercedes-Benz GL-Class, each of which had already or would launch new generations within 2013 or 2014, and would be designed by the new Lincoln design team that also designed the new 2013 MKZ. It would be completely new and would feature the 3.5 Ecoboost, new interior and exterior and performance upgrades. But in May 2013 a prototype Navigator with a different front grille was tested featuring the EcoBoost in preparation for the updated Navigator, which was followed by more spy shots taken in September 2013 that was sporting Lincoln's updated signature front grille.\n\nOn January 22, 2014, three photos of the 2015 Navigator were released on Twitter and Instagram, along with an announcement that the refreshed SUV would be revealed at the Washington Auto Show the following day (January 23, 2014). However, hours after the leaks were made public, Lincoln held a press event in Detroit for the vehicle and revealed the updated Navigator earlier than expected.\n\nThe 2015 Navigator and Navigator L kept the same exterior styling design as the Expedition instead of the reported repackaging that was announced earlier by Ford. However, it now featured an updated front grille with the rear tailgate lights bearing a resemblance to the Dodge Durango. The EcoBoost V6 engine is the only engine offered for the 2015 model year and makes and of torque. The exterior included HID headlamps with LED running lights and full LED taillights. Twenty-inch wheels came standard, replacing the 18-inch wheels, while a reserve package featured 22-inch wheels. The dashboard panels features MyLincoln Touch with Sync as a standard and controlled through an eight-inch, touchscreen display in the dash, and home to twin 4.2-inch displays that flank a central speedometer, falling in line with the rest of the Lincoln models. The push-button start became standard, likewise a passive entry and a rear-view camera. The blind-spot monitoring was added as an optional feature.\n\nBoth the refreshened and redesigned Navigator and Navigator L went into production in the Summer of 2014, and arrived to dealers in the fall of 2014 as a 2015 model.\n\nOn April 12, 2017, the fourth-generation 2018 Lincoln Navigator was introduced at the 2017 New York Auto Show. As with previous generations, the fourth-generation remains the Lincoln counterpart of the Ford Expedition, offered in both a standard and long-wheelbase configuration (Lincoln Navigator L). As before, both two and four-wheel drive versions are offered. \n\nThe fourth generation marked a significant shift in the exterior styling of the Navigator, as it adopted styling features of the 2017 Lincoln Continental. In another major shift, the model line adopted aluminum body construction. \n\nThe fourth-generation Lincoln Navigator uses the Ford T3 platform, developed under the U554 code name. Retaining body-on-frame construction, the Lincoln Navigator (and Ford Expedition) were engineered alongside the 2015 Ford F-150. The four-wheel independent suspension configuration was retained, with a redesigned rear suspension layout. \n\nShared with the Ford F-150 Raptor, the Lincoln Navigator is equipped with a 450 hp twin-turbocharged 3.5L EcoBoost V6 (although Lincoln has ended its use of the EcoBoost nomenclature). The highest-output engine ever sold by Lincoln, the 3.5L V6 is paired with a 10-speed automatic transmission (replacing the previous 6-speed automatic).\n\nEntering production with only minor changes from the Lincoln Navigator concept vehicle shown at the 2016 New York Auto Show (removing the gullwing doors and stair-style running boards, used largely for display purposes). The 2018 Lincoln Navigator adopted several design features from the 2017 Lincoln Continental, including the style of its headlights, taillights, side vents, and its front fascia, with a large rectangular grille and a centered Lincoln star emblem. While sharing its roofline and side doors with the Ford Expedition, in the style of Range Rovers, the B, C, and D-pillars are blacked out for a \"floating roof\" effect. \n\nAs with the 2015 Ford F-150, the body of fourth-generation Lincoln Navigator (and Ford Expedition) was designed as a part of a shift to aluminum body construction, with the use of steel largely reserved for the chassis frame rails. Nearly 200 pounds lighter than the previous generation, the fourth-generation Navigator is physically larger than its predecessor, with the standard-wheelbase configuration gaining three inches in wheelbase, while the long-wheelbase L gaining nearly an inch (becoming the longest-wheelbase Lincoln ever produced excluding the Mark LT pickup). In terms of body length, both versions were shortened approximately half an inch. \n\nShared with the Lincoln Continental, the dashboard of the 2018 Lincoln Navigator replaced a conventional console (or column)-mounted shifter with dashboard-mounted buttons and paddle shifters. For the first time in the model line, the fourth-generation Navigator offers a head-up display along with a 12-inch reconfigurable instrument cluster.\n\nThe fourth-generation Lincoln Navigator continues the same trim line introduced in the 2015 model update, with Premiere (new to the Navigator for 2018) as the standard trim, Select as the mid-level trim, and Reserve as the highest trim level. \n\nAlongside the revival of the Continental, the fourth-generation Navigator marked the debut of the Lincoln Black Label series. A series of vehicles with interiors and exteriors coordinated around a theme (similar to the Designer Series Lincolns and Continentals of the 1970s and 1980s), the Navigator is sold with three Black Label themes: Chalet, Destination, and Yacht Club. \n\nAt a base price of approximately $95,000, the Black Label edition of the Lincoln Navigator is the most expensive vehicle ever sold by Ford Motor Company (with the exception of the Ford GT)\nOn January 14, 2018, The Lincoln Navigator was awarded Truck of the Year at the 2018 North American International Auto Show, marking the first time that a Lincoln vehicle has been given a NAIAS award, as well as the first American-built Luxury Sport Utility Vehicle in this segment to win in this category.\n\n"}
{"id": "49489231", "url": "https://en.wikipedia.org/wiki?curid=49489231", "title": "Lists of artificial objects sent into space", "text": "Lists of artificial objects sent into space\n\nThe term artificial objects is closely associated with man made or not naturally occurring items that have been sent into space. This is a list of lists of artificial objects in space found on Wikipedia.\n\n\nFor the general concept of artificial see Artificiality.\nFor artificial objects not in space, see or List of inventions.\n"}
{"id": "1545259", "url": "https://en.wikipedia.org/wiki?curid=1545259", "title": "Mink oil", "text": "Mink oil\n\nMink oil is an oil used in medical and cosmetic products. It is obtained by the rendering of mink fat which has been removed from pelts destined for the fur industry.\n\nMink oil is a source of palmitoleic acid, which possesses physical properties similar to human sebum. Because of this, mink oil is used in several medical and cosmetic products. Mink oil is also used for treating, conditioning and preserving nearly all kinds of leather.\n\nBotanical alternatives to mink oil as a source of palmitoleic acid include macadamia nut oil (\"Macadamia integrifolia\") and sea buckthorn oil (\"Hippophae rhamnoides\"), both of which contain as much or more palmitoleic acid (17% and 19–29% respectively) than does mink oil (17%).\n\n\"Mink oil and its fatty acids are unique among animal-derived fats and oils. The total unsaturated fatty acids in mink oil account for more than 75% of the fatty acid content, but the oil, nevertheless, has a greater oxidative stability (resistance to rancidity)... than other animal or vegetable oils.\"\n\n"}
{"id": "56306256", "url": "https://en.wikipedia.org/wiki?curid=56306256", "title": "Nolder", "text": "Nolder\n\nIn automotive design, a nolder is a small aerodynamic shape (a strip, wing, protrusion, lip or profile) integral to bodywork or to an aerodynamic attachment – e.g., a spoiler, diffuser or splitter – perpendicular to the direction of air flow travel for the purpose of further managing and refining air flow. \n\nNolders are used in both high-performance as well as in less critical aerodynamic applications. \n\nIn 1996, \"Autocar\" attributed original use of the term to Ferrari, with other sources citing the nolder as having derived from Formula One Racing, where Ferrari has been prominent.\n\nThe \"Formula One Dictionary\" defines a nolder as \"a small upside-down L-shaped aerodynamic appendage generally positioned on the trailing edge of the rear wing to increase downforce at low speed.\" \n\nThe \"Automotive Dictionary\" defines it as a \"very small aerodynamic appendage that fits freely into an aileron profile to increase downforce without affecting drag resistance.\"\n\nIn the design of high-performance vehicles, a nolder of limited size can significantly increase or decrease the lift (Cz) of a vehicle's aerodynamic profile. \n\nNolders are also used in less high-performance applications, for example forcing an airflow separation alongside a vertical rear window to minimize debris accumulation, e.g., with a small hatchback.\n\nExamples include the underside of the Ferrari LaFerrari, which features a nolder to assist with vehicle dynamics. The Ferrari 599 GTO features prominent flanking aerodynamic fins or flying buttresses aside the rear window, maximizing air flow to a linear rear nolder. The Ferrari 355 has a similar nolder profile at the upper portion of its tail. \n\nThe Koenigsegg CCXR features an optional front splitter with a nolder, and the spoiler at the rear bumper of the Maserati 320S features a supplementary nolder to increase the vertical load to the rear.\n\nFor airflow management and to assist in keeping the rear window free from dirt, nolders are integral to the rearmost vertical pillar of Mini Cooper models and Fiat 500L.\n\n"}
{"id": "23210252", "url": "https://en.wikipedia.org/wiki?curid=23210252", "title": "Nuclear power proposed as renewable energy", "text": "Nuclear power proposed as renewable energy\n\nAlthough nuclear power is considered a form of low-carbon power, its legal inclusion with renewable energy power sources has been a subject of debate and classification. Statutory definitions of renewable energy usually exclude many present nuclear energy technologies, with notable exceptions in the states of Utah, and Arizona in the United States, where only a particular implementation of nuclear fission with \"waste\"/fuel recycling meets the state's criteria. Dictionary sourced definitions of renewable energy technologies often omit or explicitly exclude mention to every nuclear energy source, with an exception made for the natural nuclear decay heat generated within the Earth/geothermal energy.\nThe most common fuel used in conventional nuclear fission power stations, uranium-235 is \"non-renewable\" according to the Energy Information Administration, the organization however is silent on the recycled MOX fuel. Similarly, the National Renewable Energy Laboratory does not mention nuclear power in its \"energy basics\" definition.\n\nIn 1987, the Brundtland Commission (WCED) classified fission reactors that produce more fissile nuclear fuel than they consume (breeder reactors, and if developed, fusion power) among conventional renewable energy sources, such as solar and falling water. The American Petroleum Institute likewise does not consider conventional nuclear fission as renewable, but that breeder reactor nuclear fuel is considered renewable and sustainable, and while conventional fission leads to waste streams that remain a concern for millennia, the waste from efficiently burnt up spent fuel requires a more limited storage supervision period of about thousand years. The monitoring and storage of radioactive waste products is also required upon the use of other renewable energy sources, such as geothermal energy.\n\nRenewable energy flows involve natural phenomena, which with the exception of tidal power, ultimately derive their energy from the sun (a natural fusion reactor) or from geothermal energy, which is heat derived in greatest part from that which is generated in the earth from the decay of radioactive isotopes, as the International Energy Agency explains:\nRenewable energy resources exist over wide geographical areas, in contrast to other energy sources, which are concentrated in a limited number of countries.\n\nIn ISO 13602-1:2002, a renewable resource is defined as \"a natural resource for which the ratio of the creation of the natural resource to the output of that resource from nature to the technosphere is equal to or greater than one\".\n\nNuclear fission reactors are a natural energy phenomenon, having naturally formed on earth in times past, for example a natural nuclear fission reactor which ran for thousands of years in present-day Oklo Gabon was discovered in the 1970s. It ran for a few hundred thousand years, averaging 100 kW of thermal power during that time.\n\nConventional, human manufactured, nuclear fission power stations largely use uranium, a common metal found in seawater, and in rocks all over the world, as its primary source of fuel. Uranium-235 \"burnt\" in conventional reactors, without fuel recycling, is a non-renewable resource, and if used at present rates would eventually be exhausted.\n\nThis is also somewhat similar to the situation with a commonly classified renewable source, geothermal energy, a form of energy derived from the natural nuclear decay of the large, but nonetheless finite supply of uranium, thorium and potassium-40 present within the Earth's crust, and due to the nuclear decay process, this renewable energy source will also eventually run out of fuel. As too will the Sun, and be exhausted.\n\nNuclear fission involving breeder reactors, a reactor which \"breeds\" more fissile fuel than they consume and thereby has a breeding ratio for fissile fuel higher than 1 thus has a stronger case for being considered a renewable resource than conventional fission reactors. Breeder reactors would constantly replenish the available supply of nuclear fuel by converting fertile materials, such as uranium-238 and thorium, into fissile isotopes of plutonium or uranium-233, respectively. Fertile materials are also nonrenewable, but their supply on Earth is extremely large, with a supply timeline greater than geothermal energy. In a closed nuclear fuel cycle utilizing breeder reactors, nuclear fuel could therefore be considered renewable.\n\nIn 1983, physicist Bernard Cohen claimed that fast breeder reactors, fueled exclusively by natural uranium extracted from seawater, could supply energy at least as long as the sun's expected remaining lifespan of five billion years. This was based on calculations involving the geological cycles of erosion, subduction, and uplift, leading to humans consuming half of the total uranium in the Earth’s crust at an annual usage rate of 6500 tonne/yr, which was enough to produce approximately 10 times the world's 1983 electricity consumption, and would reduce the concentration of uranium in the seas by 25%, resulting in an increase in the price of uranium of less than 25%.\nAdvancements at Oak Ridge National Laboratory and the University of Alabama, as published in a 2012 issue of the American Chemical Society, towards the extraction of uranium from seawater have focused on increasing the biodegradability of the process and reducing the projected cost of the metal if it was extracted from the sea on an industrial scale. The researchers' improvements include using electrospun Shrimp shell Chitin mats that are more effective at absorbing uranium when compared to the prior record setting Japanese method of using plastic amidoxime nets. As of 2013 only a few kilograms (picture available) of uranium have been extracted from the ocean in pilot programs and it is also believed that the uranium extracted on an industrial scale from the seawater would constantly be replenished from uranium leached from the ocean floor, maintaining the seawater concentration at a stable level. In 2014, with the advances made in the efficiency of seawater uranium extraction, a paper in the journal of \"Marine Science & Engineering\" suggests that with, light water reactors as its target, the process would be economically competitive if implemented on a large scale. In 2016 the global effort in the field of research was the subject of a special issue in the journal of \"Industrial & Engineering Chemistry Research\".\n\nIn 1987, the World Commission on Environment and Development(WCED), an organization independent from, but created by, the United Nations, published Our Common Future, in which a particular subset of presently operating nuclear fission technologies, and nuclear fusion were both classified as renewable. That is, fission reactors that produce more fissile fuel than they consume - breeder reactors, and when it is developed, fusion power, are both classified within the same category as conventional renewable energy sources, such as solar and falling water.\n\nPresently, as of 2014, only 2 breeder reactors are producing industrial quantities of electricity, the BN-600 and BN-800. The retired French Phénix reactor also demonstrated a greater than one breeding ratio and operated for ~30 years, producing power when \"Our Common Future\" was published in 1987. \nWhile human sustained nuclear fusion is intended to be proven in the International thermonuclear experimental reactor between 2020 and 2030, and there are also efforts to create a pulsed fusion power reactor based on the inertial confinement principle (see more Inertial fusion power plant).\n\nThe world's measured resources of uranium-235 in 2014 was estimated to be enough to last over 135 years at 2014 consumption rates.\n\n30,000 to 60,000 years is one estimated supply lifespan of fission-based conventional/light water reactor reserves if it is possible to extract all the uranium from seawater, assuming current world energy consumption. Alternatively this is about 6,500 years with a potential nuclear reactor fleet of 3,000 GW, a quantity of electricity six to seven times higher than the current world civil nuclear power capacity.\n\nThe OECD have also calculated that with fast breeder reactors such as the BN-800 and conceptual Integral Fast Reactor, which has a closed nuclear fuel cycle with a burn up of, and recycling of, all the uranium, plutonium and minor actinides; actinides which presently make up the most hazardous substances in nuclear waste, there is 160,000 years worth of natural uranium in total conventional \"land\" resources and phosphate ore.\n\nThorium, an often overlooked alternative to \"fertile\" U-238 in breeder reactors, is several times (about 3 to 4) more abundant in the earth crust than natural uranium-238, and about 400 times as common as uranium-235, the dominant fuel for power reactors, as uranium-235 only constitutes 0.72% of all uranium. The average concentration or occurrence of thorium in seawater however is over 1000 times lower, in the range of nanograms per liter compared to uranium which is about 3 micrograms per liter, 3 mg (milligrams) per cubic meter/ton of water.\n\nIf it is developed, Fusion power would provide more energy for a given weight of fuel than any fuel-consuming energy source currently in use, and the fuel itself (primarily deuterium) exists abundantly in the Earth's ocean: about 1 in 6500 hydrogen (H) atoms in seawater (H2O) is deuterium in the form of (semi-heavy water). Although this may seem a low proportion (about 0.015%), because nuclear fusion reactions are so much more energetic than chemical combustion and seawater is easier to access and more plentiful than fossil fuels, fusion could potentially supply the world's energy needs for millions of years.\nIn the deuterium + lithium fusion fuel cycle, 60 million years is the estimated supply lifespan of this fusion power, if it is possible to extract all the lithium from seawater, assuming current (2004) world energy consumption. While in the second easiest fusion power fuel cycle, the deuterium + deuterium burn, assuming all of the deuterium in seawater was extracted and used, there is an estimated 150 billion years of fuel, with this again, assuming current (2004) world energy consumption.\n\nInclusion under the \"renewable energy\" classification as well as the low-carbon classification could render nuclear power projects eligible for development aid under more jurisdictions. Thus a key issue regarding this classification of nuclear power is inclusion in Renewable portfolio standard (RES).\n\nA bill proposed in the South Carolina Legislature in 2007-2008 aimed to classify nuclear power as renewable energy. The bill listed as renewable energy: solar photovoltaic energy, solar thermal energy, wind power, hydroelectric, geothermal energy, tidal energy, recycling, hydrogen fuel derived from renewable resources, biomass energy, nuclear energy, and landfill gas.\n\nIn 2009 the Utah state passed the bill ECONOMIC DEVELOPMENT INCENTIVES FOR ALTERNATIVE ENERGY PROJECTS including incentives for renewable energy projects. It includes a direct reference to nuclear power: \"Renewable energy\" means the energy generation as defined in Subsection 10-19-102 (11) and includes generation powered by nuclear fuel. The bill passed the house with 72 yeas, 0 nays, and 3 absent, passed the senate with 24 yeas, 1 nay, and 4 absent, then received the governor's signature.\n\nIn 2010 the Arizona Legislature included nuclear power in a proposed bill for electric utility renewable energy standards. The bill defined \"renewable energy\" as energy that is renewable and non-carbon emitting. It listed solar, wind, geothermal, biomass, hydroelectric, agricultural waste, landfill gas and nuclear sources.\n\nIn 2015 the Arizona bill specified that \"Nuclear energy from sources fueled by uranium fuel rods that include 80 percent or more of recycled nuclear fuel and natural thorium reactor resources under development\" are renewable.\n\nNuclear energy has been referred to as \"renewable\" by the politicians George W. Bush, Charlie Crist, and David Sainsbury. In 2006, speaking on the topics of economic growth and getting oil from parts of the world where \"they simply don’t like us\", US President Bush said: \"Nuclear power is safe and nuclear power is clean and nuclear power is renewable\".\n\n"}
{"id": "30212398", "url": "https://en.wikipedia.org/wiki?curid=30212398", "title": "Octadecatrienoic acid", "text": "Octadecatrienoic acid\n\nAn octadecatrienoic acid is a chemical compounds with formula , a fatty acid with whose molecule has an 18-carbon unbranched backbone with three double bonds.\n\nThe name refers to many different structural and conformational isomers, that differ in the position of the double bonds along the backbone and on whether they are in \"cis\" ('Z') or \"trans\" ('E') conformation. Some isomers have considerable biological, pharmaceutical, or industrial importance, such as:\n\n"}
{"id": "15235947", "url": "https://en.wikipedia.org/wiki?curid=15235947", "title": "Overcurrent", "text": "Overcurrent\n\nIn an electric power system, overcurrent or excess current is a situation where a larger than intended electric current exists through a conductor, leading to excessive generation of heat, and the risk of fire or damage to equipment. Possible causes for overcurrent include short circuits, excessive load, incorrect design, or a ground fault. Fuses, circuit breakers, lifersensors and current limiters are commonly used protection mechanisms to control the risks of overcurrent.\n\n\n"}
{"id": "28646443", "url": "https://en.wikipedia.org/wiki?curid=28646443", "title": "Pentadecanoic acid", "text": "Pentadecanoic acid\n\nPentadecanoic acid is a saturated fatty acid. Its molecular formula is CH(CH)COOH. It is rare in nature, being found at the level of 1.2% in the milk fat from cows. The butterfat in cows milk is its major dietary source\nand it is used as a marker for butterfat consumption. Pentadecanoic acid also occurs in hydrogenated mutton fat. It also comprises 3.61% of the fats from the fruit of the durian species \"Durio graveolens\".\n\nPentadecanoic acid may increase mother-to-child transmission of HIV through breastfeeding.\n\n"}
{"id": "25087021", "url": "https://en.wikipedia.org/wiki?curid=25087021", "title": "Pumpable ice technology", "text": "Pumpable ice technology\n\nPumpable ice (\"PI\") technology is a technology to produce and use fluids or secondary refrigerants, also called coolants, with the viscosity of water or jelly and the cooling capacity of ice. Pumpable ice is typically a slurry of ice crystals or particles ranging from 5 to 10,000 micrometers (1 cm) in diameter and transported in brine, seawater, food liquid, or gas bubbles of air, ozone, or carbon dioxide.\n\nBesides generic terms such as pumpable, jelly or slurry ice, there are many trademark names for such coolant, like \"Deepchill\", “Beluga”, “optim”, “flow”, “fluid”, “jel”, “binary”, “liquid”, “maxim”, “whipped”, “bubble slurry” ice. These trademarks are authorized by industrial ice maker production companies in Australia, Canada, China,\nGermany, Iceland, Israel, Russia, Spain, United Kingdom, and USA.\n\nThere are two relatively simple methods for producing pumpable ice. The first is to manufacture commonly used forms of crystal solid ice, such as plate, tube, shell or flake ice, by crushing and mixing it with water. This mixture of different ice concentrations and particle dimensions (ice crystals can vary in length from 200 µm to 10 mm) is passed by pumps from a storage tank to the consumer. The constructions, specifications and applications of current conventional ice makers are described in.\n\nThe idea behind the second method is to create the crystallization process inside of the volume of the cooled liquid. This crystallization inside can be accomplished using vacuum or cooling technologies. In vacuum technology, very low pressure forces a small part of the water to evaporate while the remaining water freezes forming a water-ice mixture. Depending on the additive concentrations, the final temperature of pumpable ice is between zero and –4 °C. The large volume of vapor and an operating pressure of about 6 mbar (600 Pa) require the use of a water vapor compressor with a great swept volume. This technology is economically reasonable and can be recommended for systems with cooling capacity of 1 MW (300 ton of refrigeration; 3.5 million BTU/h) or larger.\n\nCrystallization by cooling can be done using direct or indirect systems.\n\nA refrigerant is directly injected inside the liquid\n\nThe advantage of this method is the absence of any intermediate device between the refrigerant and the liquid. However, the absence of heat loss between refrigerant and liquid in the process of thermal interaction (heat transfer) may cause problems. The safety measures that have to be implemented, the need for the additional step of refrigerant separation, and difficulties in producing crystals are further disadvantages of this method.\n\nIn indirect methods the evaporator (heat exchanger-crystallizer) is assembled either horizontally or vertically. It has a shell tubing assembled with one to a hundred inner tubes and containing a refrigerant that evaporates between the shell and the internal tubing. Liquid flows through the tubing of the small diameter. In the inside volume of the evaporator cooling, super cooling and freezing of liquid take place due to heat exchange with the crystallizer-cooled wall.\n\nThe idea is to use a well-polished evaporator surface (dynamic scraped surface heat exchanger) and appropriate mechanisms to prevent tubing from adhering to the ice embryos, and to prevent growth and a thickening of the ice on the inside cooling surface. A whip rod, a screw or a shaft with metallic or plastic wipers is usually used as a mechanism for removal.\n\nIndirect pumpable ice technologies produce pumpable ice consisting of 5 to 50 micrometer crystals and have a number of advantages. They can produce 1,000 kg of crystal ice at the low energy expenditure of 60 to 75 kWh instead of the 90 to 130 kWh required to produce regular water ice (plate, flake, shell types). Further improvements are expected to lead to a specific energy expenditure for ice production of 40 to 55 kWh per 1,000 kg of pure ice and a high specific ice capacity per an area value at the evaporator cooling surface (up to 450 kg/(m·h)).\n\nCommercial evaporators of the double-pipe type used in the food and fish industries have an inside diameter of interior tube and length, consequently, in a range of 50–125 mm and 60–300 cm. For the dewaxing lubrication oil, evaporators are widely used with the following dimensions: internal diameter of the inner tube is 150–300 mm; the length is 600–1,200 cm.\n\nSometimes a gas can be added to the liquid flowing through the evaporator. It destroys a liquid laminar layer on the cooled surface of the heat exchanger-crystallizer, increases flow turbulence, and decreases the average viscosity of pumpable ice.\n\nDifferent liquids, such as sea water, juice, brines, or glycol solutions of additives with more than 3–5% concentrations and a freezing point less than −2 °C are used in the process.\n\nTypically, the equipment for the production, accumulation and supplying of pumpable ice includes an ice maker, a storage tank, a heat exchanger, piping, pumps, and electrical and electronic appliances and devices.\n\nPumpable ice with maximum ice concentration of 40% can be pumped straight from the ice maker to the consumer. The final possible ice concentration of pumpable ice in the storage tank is 50%. The maximum value of cooling energy of pumpable ice accumulated in the storage tank in a homogeneous phase is about 700 kWh, which corresponds to 10–15 m volume of a storage tank. A high-shear mixer is used to prevent the separation of ice from the cooled liquid and keeps the ice concentration unchanged over time and unaffected by the tank height. Pumpable ice is transported from the storage tank to a place of consumption that could be hundreds of meters away. The practical ratio between the required electric power of the submersible mixer motor (kW) and the “kneaded” pumpable ice volume (m) is 1:1.\n\nIn the tanks with volumes larger than 15 m, pumpable ice is not mixed and the cold energy of stored ice is only used by a heat transfer of liquid that circulates between a storage tank and the consumers of cold. The disadvantages of existing ice storage reservoirs include the following:\n\nThe chaotic uncontrollable upsurge of ice ridges which arise due to uneven sprinkling of warm fluid. This liquid is fed into the storage tank from the heat exchanger for further cooling by direct contact with the surface of the ice. The solution is sprayed unevenly in space. Moreover, the rate of supply is not constant. Therefore, the ice melts unevenly. Thus, the ice spikes rise above the ice surface, which leads to the destruction of the spraying devices. In this case, it is necessary to reduce the level of solution in the storage tank to avoid breakage of spray devices.\n\nIce accumulated in the tank turns into a large chunk. The warm liquid that comes from the air-conditioning system may generate channels through which the liquid could return to the system without being cooled. As a result, the accumulated ice is not fully utilized.\n\nIneffective use of the volume of the accumulation tank leads to a decrease in the achievable maximum of ice concentration and an inability to fill the entire working volume of the storage tank.\n\nResearch and development on overcoming these disadvantages is underway and is expected to lead to the mass production of cheap, reliable and efficient accumulating tanks. These tanks should ensure higher ice concentrations and allow full use of stored cold potential.\n\nMany ice maker producers, research centers, inventors are working on pumpable ice technologies. Due to their high energy efficiency, reduced size and low refrigerant charges, there are many applications for this technology.\n\nThere are different pumpable ice maker designs and many special areas of application. The choice is facilitated by computer programs developed by manufacturers.\n\nA customer who intends to use pumpable ice technology should know:\nWhen designing storage tanks, several features are to be taken into account:\n\nThe thickness of the wall of the evaporators is usually determined to ensure:\n\nEvaporators are usually cheaper when they have a smaller shell diameter and a long pipe length. Thus, the evaporator of pumpable ice makers is typically as long as physically possible whilst not exceeding production capabilities. However, there are many limitations, including the space available at the customer site where the pumpable ice maker is going to be used.\n\nA pumpable ice maker has predictive maintenance and cleaning requirements. Operational conditions of the specific equipment determine the service intervals and types of service.\n\nProper refrigeration maintenance of a pumpable ice maker will extend its life, and routine maintenance can reduce the probability of an emergency service caused by major component failure, such as of a refrigeration compressor or the fan motor of the air condenser due to a dirty coil, and refrigerant leakage.\n\nPossible problems caused by not maintaining the air-cooled pumpable ice maker are:\n\nIn the pumpable ice maker liquid treatment is used to remove particles down to 1 µm in size and to minimize fouling of the heat transfer surface of the evaporators. Plate heat exchangers also need to be disassembled and cleaned periodically. Properly treating liquid before it enters the pumpable ice maker or the Plate heat exchanger will help to limit the amount of scale build-up, thus reducing cleaning times and preventative maintenance costs. Improperly sizing the liquid filter system leads to costly early change outs and poor performance.\n\nPumpable ice technologies can be recommended for cleaning (lightening) sediments in waste waters. In this case, a method including freezing and further melting with subsequent separation of the liquid and solid phases is used. This method leads to a variation in the physical-chemical structure of sediments and is realized owing to redistribution of any form of the connection of moisture with solid particles of the sediment. It does not need any chemical reagent. The sediment's freezing promotes an increase in the free water quantity of the sediment and improves the efficiency of sediment precipitation. Most of the moisture is capable of diffusion at any of the conditions. Therefore, if the velocity of crystal growing does not exceed 0.02 m/h, there is time for moisture to migrate from colloid cells to the crystal surface, where it is frozen. After thawing, lightened water can be used for industrial and agriculture applications. The concentrated sediments are supplied to press-filters to further decrease their moisture content.\n\nThe existing commercialized desalination methods are multi-stage flash evaporation, vapor-compression, multi-effect evaporation, reverse osmosis and electrodialysis. Theoretically, freezing has some advantages over the above-mentioned methods. They include a lower theoretical energy requirement, minimal potential for corrosion, and little scaling or precipitating. The disadvantage is that freezing involves a handling of ice and water mixtures that is mechanically complicated, both as to moving and processing. A small number of desalination stations have been built over the last 50 years, but the process has not been a commercial success in the production of freshwater for municipal purposes. Pumpable ice machines offer an affordable alternative because of the high efficient crystallization process. Current models, however, do not have the necessary capacity for industrial desalination plants, but smaller models suffice for small-scale desalination needs.\n\nCurrently, reverse osmosis and vacuum-evaporation technologies are used to concentrate juices and other food liquids. In commercial operations, juice is normally concentrated by evaporation. Since 1962, the Thermally Accelerated Short Time Evaporator (TASTE) has been widely used. TASTE evaporators are efficient, sanitary, easy to clean, of high capacity, simple to operate, and of relatively low cost. On the other hand, there is some heat damage to the product caused by the high-temperature steam treatment. This treatment results in product quality and aroma losses. Because of the low value of the film coefficient between steam and treated juice, heat transfer between them is very inefficient. It leads to the cumbersome construction of TASTE plants. The alternative is to concentrate juice and food liquid by a cooling and freezing process. In this case crystals of pure water are removed from the juice, wine, or beer by crystallization. The aroma, color, and flavor remain in the concentrated medium. The quality of freeze-concentrated products cannot be achieved by any other technology. The main advantages in comparison with other freezing techniques are low energy expenditure and the possibility to tune the rate of the phase change from liquid to solid ice, which in turn increases the production of pure water ice crystals and simplifies the separation of concentrated juice or food liquid and ice crystals.\n\nIn the 1990s, frozen carbonated beverages and frozen uncarbonated beverages began to enjoy great popularity.\n\nThe manufacture (process and refrigeration equipment) of almost all frozen carbonated beverages and frozen uncarbonated beverages is organized like the production of pumpable ice.\n\nThe frozen carbonated beverage machine was invented in the late 1950s by Omar Knedlik.\n\nFor frozen carbonated beverage manufacturing, a mixture of flavored syrup, carbon dioxide gas (CO2) and filtered water are used. Typically, the initial temperature of the mixture is 12–18 °C. The carbonated mixture is fed into the evaporator of the apparatus, then freezes on the inner surface of the cylindrical evaporator and is scraped off by the blades—mixers rotating at 60 to 200 rpm. In the internal volume of the crystallizer, a slight positive pressure (up to 3 bar) is maintained to improve the dissolution of gas into the liquid. In modern frozen carbonated beverage devices, there is a conventional refrigeration circuit with a capillary tube or thermostatic expansion valve and, usually, an air condenser. Refrigerant is fed either directly into the cavity of a two-wall evaporator or into the spiral evaporator wound on the outer surface of the crystallizer. The evaporator wall is made of stainless steel grade SS316L, approved for contact with food according to requirements of the US Food and Drug Administration. The evaporator temperature is −32 to −20 °C. Manufacturers do not reveal the hourly capacity of frozen carbonated beverages machines, but the energy expenditure to produce 10.0 kg of frozen carbonated beverages can be 1.5–2.0 kWh.\n\nAfter mixing and freezing in the crystallizer-mixer, the frozen carbonated beverage is ejected through the nozzle into cups. The end product is a thick mixture of suspended ice crystals with a relatively small amount of liquid. Frozen carbonated beverage quality depends on many factors, including the concentration, size and structure of the ice crystals. The concentration of the ice water mixture is determined accurately in accordance with the phase diagram of the solution and can reach 50%. The maximum crystal size is 0.5 mm to 1.0 mm. The initial temperature of crystallization of the mixture depends on the initial concentration of ingredients in the water and lies between −2.0 °C and −0.5 °C. The final temperature of the product varies between −6.0 °C and −2.0 °C, depending on the manufacturer.\n\nInterest in frozen carbonated beverages was noted in India. The Indian government prohibits the addition of ice produced from municipal water to beverages due to the probability of bacteriological contamination. Using a carbonated beverage in the form of frozen Coke offered a method to create an ice-chilled beverage in India.\n\nInitially, frozen carbonated beverages were produced using fruit, vegetable juices, or drinks based on coffee, tea or yogurt. Research is being conducted on producing frozen wine and beer.\n\nFrozen uncarbonated beverage machines differ from frozen carbonated beverage machines in that they do not require a small positive pressure to be maintained in the working volume of the evaporator, nor a source of carbon dioxide gas, nor specially trained staff. Otherwise, the design of modern frozen uncarbonated beverage machines is similar to that for frozen carbonated beverages. Frozen uncarbonated beverages often have a lower concentration of ice, and more liquid water, than frozen carbonated beverages. Frozen uncarbonated beverages machines are less complicated and cheaper than frozen carbonated beverage devices, making them more common.\n\nThe ice cream production market steadily increased throughout the 1990s and its value is multibillions of US dollars.\n\nThe eight major ice cream markets in the world are USA, China, Japan, Germany, Italy, Russia, France and UK. The key competitors in the industry are Unilever and Nestle, who together control over one-third of the market. The top five ice-cream consuming countries are the US, New Zealand, Denmark, Australia and Belgium.\n\nThe modern design of industrial ice cream freezers ensures a high level of machine/operator interface and top quality of the produced ice cream.\nThe manufacturing process of ice cream production includes pasteurization, homogenization and maturation of the ice cream mixture. The prepared mixture enters into the industrial double tube scraped crystallizer – heat exchanger, in which the processes of pre-freezing and churning of ice cream are carried out. A refrigerant fluid evaporates and continually circulates in a vessel jacket.\nUsually, the initial temperature of an ice cream mixture is 12–18 °C. After switching on a freezer, an evaporating temperature of a refrigerant decreases down to a range from −25 to −32 °C. The final temperature of the treated mixture into the scraped surface freezer is about −5 °C, with an ice concentration of approximately 30–50%, depending on the formula. During the freezing process ice crystals form on the inside cool surface of the crystallizer wall. They are removed by blades, mixed into the bulk, and continue to decrease its temperature and to improve heat transfer within the product.\n\nThere are also rotating dashers that help to whip the mix and incorporate air into the mixture. The frozen product then goes to the distributor.\n\nThe quality of ice cream and its smooth texture depend on the structure of their ice crystals and their dimensions, and on the viscosity of the ice cream. Water freezes out of a liquid in its pure form as ice. The concentration of the remaining liquid sugar mixture increases due to water removal, hence the freezing point is further lowered. Thus the structure of ice cream can be described as a partly frozen foam with ice crystals and air bubbles occupying most of the space. Tiny fat globules flocculate and surround the air bubbles in the form of a dispersed phase. Proteins and emulsifiers in turn surround the fat globules. The continuous phase consists of a concentrated, unfrozen liquid of sugars.\n\nThe final average diameter of ice crystals depends on the rate of freezing. The faster this is, the more nucleation is promoted and the greater the number of small ice crystals. Usually, after a cooling treatment ice crystal dimensions in the freezer are about 35–80 µm.\n\nPumpable-ice-technology-based equipment can be used in the cooling processes in the fishery and food industries. In comparison with freshwater solid ice, the main advantages are the following: homogeneity, higher cooling rates of food and fish. Pumpable ice flows like water and eliminates freeze burns and physical damage to the cooled object; it increases food quality enabling a longer shelf life. Pumpable ice technology meets Food Safety and Public health regulations (HACCP and ISO). Pumpable ice has a lower specific energy expenditure compared with existing technologies using conventional freshwater solid ice.\n\nRefrigeration systems using pumpable ice technology are attractive for the air cooling of supermarket counters (showcases). For this application, pumpable ice is circulated through the already available piping as a coolant, replacing environmentally unfriendly refrigerants like R-22 (Freon) and other hydrochlorofluorocarbons (HCFC’s).\nReasons to use pumpable ice technology for this application are the following:\n\nWide perspectives for pumpable ice usage open up for the production of special wines reminiscent of \"ice wine\" (German \"Eiswein\"). In comparison with the existing technology for ice wine production, pumpable ice technology does not require a wait of a few months for the freezing of the grapes. Freshly squeezed grapes are harvested in a specific container connected to the pumpable ice machine. The juice is pumped through this machine, from which comes a mixture of ice (in the form of tiny, pure ice crystals) and a somewhat concentrated juice. Liquid ice returns to the accumulation tank, in which there is a natural (according to Archimedes law) separation of ice and juice. The cycle is repeated many times until the sugar concentration reaches 50–52°Brix. Then a process of fermentation takes place, resulting in this alcoholic drink.\n\nA pumpable-ice-based Thermal Energy Storage System (TESS) can be used in centralized water-cooled air-conditioning systems in order to eliminate peak demand loads at critical times. This reduces the operating costs of buildings, the need for new power plants and modern transmission lines, power plant energy consumption and pollution, and greenhouse gas emissions. The idea is to make and accumulate pumpable ice during off-peak electricity hours with the lowest kWh tariff. The stored pumpable ice is used during middle- or high-tariff hours to cool the equipment or air supplied to the buildings. The return on investments (ROI) takes 2–4 years. In comparison with static and dynamic ice storage systems, the overall heat transfer coefficient (OHTC) during the production of pumpable ice is more than tens or hundreds of times higher (more efficient) than the same coefficient for the above-mentioned TESS types. This is explained by the presence of many different kinds of thermal resistances between the boiling refrigerant at the evaporator and water/ice in the storage tanks of static and dynamic ice storage systems. The high OHTC value pumpable-ice-technology-based TESS means a decrease in component volume, an increase in the maximum achievable concentration of ice in the volume of a storage tank, and ultimately a decrease in the price of equipment. TESSs based on pumpable ice technology have been installed in Japan, Korea, USA, UK and Saudi Arabia.\n\nA protective cooling process based on the implementation of a developed special ice slurry has been developed for medical applications. In this case pumpable ice can be injected intra-arterially, intravenously, along the external surfaces of organs using laparoscopy, or even via the endotracheal tube. It is being confirmed that pumpable ice can selectively cool organs to prevent or limit ischemic damage after a stroke or heart attack. Completed medical tests on animals simulated conditions requiring in-hospital kidney laparoscopic procedures. Results of French and US research are yet to be approved by the U.S. Food and Drug Administration.\nBenefits of pumpable ice technology in medicinal applications are:\n\nSki resorts have a strong interest in producing snow, even when the ambient temperature is as high as 20 °C. The dimensions and power expenditure of known snow-production equipment depend on humidity and wind conditions. This snow-making equipment is based on the freezing of water droplets which are sprayed into air before they reach the ground surface, and requires an ambient temperature lower than −4 °C.\n\nPumpable ice produced by Vacuum Ice Maker (VIM) Technology allows professional skiers to increase their training periods to extend before and after winter season (into late autumn and early spring). The pumpable ice process is organized as following. A salt solution is exposed to very low pressure inside the VIM. A small part of it evaporates in the form of water due to the vacuum forces, while the remaining liquid is frozen, forming a mixture. The water vapor is continuously evacuated from the VIM, compressed, and fed into a condenser due to the special construction of the centrifugal compressor. A standard water chiller supplies cooling water at 5 °C in order to condense the water vapor. The liquid-ice mixture is pumped out from the freezing volume to the ice concentrator in which ice crystals separate from the liquid. The high concentration ice is extracted from the concentrator. VIMs have been installed at Austrian and Swiss ski resorts .\n\n\n"}
{"id": "38626963", "url": "https://en.wikipedia.org/wiki?curid=38626963", "title": "Punggye-ri Nuclear Test Site", "text": "Punggye-ri Nuclear Test Site\n\nPunggye-ri Nuclear Test Site () was the only known nuclear test site of North Korea. Nuclear tests were conducted at the site in October 2006, May 2009, February 2013, January 2016, September 2016, and September 2017.\n\nThe site has three visible tunnel entrances. Based on satellite imagery, its exact location is in mountainous terrain in Kilju County, North Hamgyong Province. It is south of Mantapsan, west of Hwasong concentration camp and northwest of the Punggye-ri village. The most proximate settlement to the possible nuclear underground test site is Chik-tong, a small populated place located at . Sungjibaegam is a settlement located from the tremor of the 2013 test. Punggye-ri railway station is located at .\n\nIn January 2013, Google Maps was updated to include various locations in North Korea. On 8 April 2013, it was reported that South Korea had observed activity at Punggye-ri, suggesting that a fourth nuclear test was being prepared, but the next test did not occur until January 2016.\n\nOn 6 January 2016, North Korean state media announced a fourth nuclear test had been carried out successfully at the location using a hydrogen bomb. Satellite imagery captured for monitoring website 38 North between January and April 2017 suggested that a sixth nuclear test was being prepared at the site, which was detonated on September 3, 2017. \n\nAccording to sources, people from the Punggye-ri nuclear test site have been banned from entering Pyongyang since the test due to the possibility of being radioactively contaminated. According to the report of defectors, about 80% of trees died and all of the underground wells dried up in the site after the sixth nuclear test.\n\nOn 3 and 23 September 2017, earthquakes which seem to be collapses of tunnels were detected with magnitude of 4.1 and 3.6 respectively. A 17 October 2017 study published by the US-Korea Institute at Johns Hopkins University suggested the most recent test had caused \"substantial damage to the existing tunnel network under Mount Mantap\".\n\nOn 30 October 2017, in testimony before the South Korean parliament, the director of South Korea's Meteorological Administration warned that \"further tests at Punggye-ri could cause the mountain to collapse and release radioactivity into the environment.\" Likewise, Chinese scientists warned that if the mountain collapsed, nuclear fallout could spread across \"an entire hemisphere.\"\n\nOn 1 November 2017 Japanese TV station TV Asahi reported that according to unconfirmed reports, several underground tunnels collapsed at the test site on 10 October 2017. An initial collapse was said to have killed 100 workers, with another 100 rescuers killed in a second collapse.\n\nOn 20 April 2018 the North Korean government announced that it would suspend nuclear tests and shut down the Punggye-ri nuclear test site.\n\nOn 14 May 2018 it was reported that commercial satellite imagery indicated that dismantling of the facilities at the test site had begun. The leader and Supreme Commander of North Korea Kim Jong-un determined the date for the closing ceremony of Punggye-ri - 23-25 May 2018. The government of North Korea allowed a handful of international journalists (but none from South Korea) to observe the closing ceremony. Notably absent would be experts or inspectors who could study the test site at close quarters.\n\nOn 24 May 2018 foreign journalists reported that tunnels in the Punggye-ri nuclear test site had been destroyed by the North Korean government in a move to reduce regional tensions.\n\nOn October 31, 2018, Kim Min-ki, a lawmaker in South Korea's ruling Democratic Party, stated that now defunct Punggye-ri was among other nuclear and missile test sites which had been observed by officials from South Korea's National Intelligence Service and that it was now ready for planned international inspection.\n"}
{"id": "40922605", "url": "https://en.wikipedia.org/wiki?curid=40922605", "title": "Residential Customer Equivalent", "text": "Residential Customer Equivalent\n\nResidential Customer Equivalent (RCE) is a unit of measures used by the energy industry to denote the typical annual commodity consumption by a single-family residential customer. Also known as \"RCE\" for short, a single RCE represents 1,000 therms of natural gas or 10,000 kWh of electricity.\n\nRCE is often used to help normalize the size of energy companies. Energy companies serve a number of customers that is typically different from the RCE value consumed by those customers. For example, an LDC or ESCO may serve 50,000 customers but many of those can be commercial or industrial customers, so that same company can be said to serve 400,000 RCE.\n"}
{"id": "11489366", "url": "https://en.wikipedia.org/wiki?curid=11489366", "title": "Riverstone pebble tile", "text": "Riverstone pebble tile\n\nRiverstone pebble tile is a composite material made up of marble pebbles or pieces of natural stone in different sizes, bound together with a transparent white or coloured resin.\n\nThe material is made by adding resin to marble or stone pebbles laid down on a moulding.\nOne side is polished and the other side is cut. As a result, the polished side becomes slightly undulating, where the resin is lower than the pebbles that come to the surface. This characteristic makes Riverstone an aesthetically pleasant floor and wall covering and substantially improves its functionality and resistance to abrasion.\n\nThe surface of the stones is free of paints or resins and it is treated with a waxy water-repellent that protects the material from stains.\n\nRiverstone has technical characteristics similar to those of marble and should be treated like marble when cutting, drilling or polishing it. It can be purchased as slabs or tiles in various sizes and colours.\n\nNatural stones absorb very little liquid and the resin is virtually impervious to liquids therefore Riverstone is suitable for any kind of wall and floor covering (including heated flooring) as well as kitchen worktops, vanity tops, bar fronts and tops. Riverstone is translucent and can be backlit\n"}
{"id": "22693316", "url": "https://en.wikipedia.org/wiki?curid=22693316", "title": "Sandwatch", "text": "Sandwatch\n\nSandwatch is a program in which children, youth and adults work together to scientifically monitor and critically evaluate the problems facing their beach environments. They then design and implement practical activities and projects to address particular issues, enhance their beach environment and build resilience to climate change. The Sandwatch approach is called MAST: Monitoring, Analysing, Sharing information, and Taking action. Although Sandwatch has mainly focused on beach environments, the program can be used on other ecosystems, such as rivers and streams.\n\nSandwatch has received support for activities over the years from organisations such as the United Nations Educational, Scientific and Cultural Organization (UNESCO) and the University of Puerto Rico Sea Grant College Program, among others.\n\nIn 1999, participants at an Environmental Education workshop in Trinidad and Tobago recognized the potential of using the beach environment as an outdoor classroom. The students at the workshop came up with the name Sandwatch. Since then, the program has grown to include more than 30 member countries with over 300 schools and community groups actively monitoring change in their beach environments.\n\nIn 2001, during the first formal Sandwatch training for teachers, the action-based design was trialed and since then Sandwatch and the MAST approach have been successfully used by teachers around the world and are often portrayed as a working example of education for sustainable development.\n\nIn 2005 the Sandwatch Manual was published. In October 2010 the manual was revised to include climate change adaptation. The manual: Sandwatch: Adapting to Climate Change and Educating for Sustainable Development is available in four languages (English, French, Portuguese and Spanish), and can be downloaded for free on the Sandwatch website, www.Sandwatch.org.\n\nAs the program continued to grow, in 2006 a Sandwatch website and newsletter were launched. In 2008 the Sandwatch Foundation was established as an independent organization to oversee and coordinate the program and is operated by two volunteer directors, Paul Diamond and Gillian Cambers. \nIn 2013 the long-awaited Sandwatch International Database was launched in Mauritius. The database provides participating Sandwatch groups the opportunity to upload their monitoring data to a secure site. Not only does this allow for proper archiving of environmental data, but it also allow users to analyse their results, create their own charts and graphs, and communicate their findings visually with the wider community. Non-Sandwatch groups can also visit the database and view the data.\n\nEach Sandwatch school or group adopts a local beach and regularly takes a series of measurements and tests of their beach using simple and readily available equipment. By measuring how the beach's width, currents, waves, water quality and other factors change over months and years, the teams can determine whether their beach is stable and healthy or stressed and deteriorating, and the nature of the stressors.\n\nWhen a group identifies a problem it can, with the community, develop a project to address and mitigate the problem or problems. Project activities include alerting authorities to potential problems such as water contamination (from sewage or agricultural runoff for example), conducting beach clean-ups, replanting mangroves or dune stabilization vegetation, creating signage for proper beach use, monitoring and protecting marine turtle nesting sites, or monitoring the effects of coral bleaching. By keeping the beach and related ecosystems healthy they are made more resilient and better able to adapt to climate change.\n\nAll Sandwatch groups are encouraged to regularly post their data, photographs and other project details on the Sandwatch website, as well as to contribute articles and photographs to the twice-yearly newsletter, The Sandwatcher.\n\nSandwatch representatives are often invited to participate in regional and international workshops and conferences, where they share their experiences and data and forge partnerships with similar environmental groups.\n\nSea turtle threats\n\n"}
{"id": "9058508", "url": "https://en.wikipedia.org/wiki?curid=9058508", "title": "Seconds pendulum", "text": "Seconds pendulum\n\nA seconds pendulum is a pendulum whose period is precisely two seconds; one second for a swing in one direction and one second for the return swing, a frequency of 1/2 Hz. A pendulum is a weight suspended from a pivot so that it can swing freely. When a pendulum is displaced sideways from its resting equilibrium position, it is subject to a restoring force due to gravity that will accelerate it back toward the equilibrium position. When released, the restoring force combined with the pendulum's mass causes it to oscillate about the equilibrium position, swinging back and forth. The time for one complete cycle, a left swing and a right swing, is called the period. The period depends on the length of the pendulum, and also to a slight degree on its weight distribution (the moment of inertia about its own center of mass) and the amplitude (width) of the pendulum's swing.\n\nTime in physics is defined by its measurement: time is what a clock reads. In classical, non-relativistic physics it is a scalar quantity and, like length, mass, and charge, is usually described as a fundamental quantity. Time can be combined mathematically with other physical quantities to derive other concepts such as motion, kinetic energy and time-dependent fields. \"\" is a complex of technological and scientific issues, and part of the foundation of \"recordkeeping\".\n\nThe pendulum clock was invented in 1656 by Dutch scientist and inventor Christiaan Huygens, and patented the following year. Huygens contracted the construction of his clock designs to clockmaker Salomon Coster, who actually built the clock. Huygens was inspired by investigations of pendulums by Galileo Galilei beginning around 1602. Galileo discovered the key property that makes pendulums useful timekeepers: isochronism, which means that the period of swing of a pendulum is approximately the same for different sized swings. Galileo had the idea for a pendulum clock in 1637, which was partly constructed by his son in 1649, but neither lived to finish it. The introduction of the pendulum, the first harmonic oscillator used in timekeeping, increased the accuracy of clocks enormously, from about 15 minutes per day to 15 seconds per day leading to their rapid spread as existing 'verge and foliot' clocks were retrofitted with pendulums.\n\nThese early clocks, due to their verge escapements, had wide pendulum swings of 80–100°. In his 1673 analysis of pendulums, \"Horologium Oscillatorium\", Huygens showed that wide swings made the pendulum inaccurate, causing its period, and thus the rate of the clock, to vary with unavoidable variations in the driving force provided by the movement. Clockmakers' realisation that only pendulums with small swings of a few degrees are isochronous motivated the invention of the anchor escapement around 1670, which reduced the pendulum's swing to 4–6°. The anchor became the standard escapement used in pendulum clocks. In addition to increased accuracy, the anchor's narrow pendulum swing allowed the clock's case to accommodate longer, slower pendulums, which needed less power and caused less wear on the movement. The seconds pendulum (also called the Royal pendulum), 0.994 m (39.1 in) long, in which each swing takes one second, became widely used in quality clocks. The long narrow clocks built around these pendulums, first made by William Clement around 1680, became known as grandfather clocks. The increased accuracy resulting from these developments caused the minute hand, previously rare, to be added to clock faces beginning around 1690.\n\nThe 18th and 19th century wave of horological innovation that followed the invention of the pendulum brought many improvements to pendulum clocks. The deadbeat escapement invented in 1675 by Richard Towneley and popularised by George Graham around 1715 in his precision \"regulator\" clocks gradually replaced the anchor escapement and is now used in most modern pendulum clocks. Observation that pendulum clocks slowed down in summer brought the realisation that thermal expansion and contraction of the pendulum rod with changes in temperature was a source of error. This was solved by the invention of temperature-compensated pendulums; the mercury pendulum by George Graham in 1721 and the gridiron pendulum by John Harrison in 1726. With these improvements, by the mid-18th century precision pendulum clocks achieved accuracies of a few seconds per week.\n\nAt the time the second was defined as a fraction of the Earth's rotation time or mean solar day and determined by clocks whose precision was checked by astronomical observations. Solar time is a calculation of the passage of time based on the position of the Sun in the sky. The fundamental unit of solar time is the day. Two types of solar time are apparent solar time (sundial time) and mean solar time (clock time).\nMean solar time is the hour angle of the mean Sun plus 12 hours. This 12 hour offset comes from the decision to make each day start at midnight for civil purposes whereas the hour angle or the mean sun is measured from the zenith (noon). The duration of daylight varies during the year but the length of a mean solar day is nearly constant, unlike that of an apparent solar day. An apparent solar day can be 20 seconds shorter or 30 seconds longer than a mean solar day. Long or short days occur in succession, so the difference builds up until mean time is ahead of apparent time by about 14 minutes near February 6 and behind apparent time by about 16 minutes near November 3. The equation of time is this difference, which is cyclical and does not accumulate from year to year.\n\nMean time follows the mean sun. Jean Meeus describes the mean sun as follows:\n\nConsider a first fictitious Sun travelling along the \"ecliptic\" with a constant speed and coinciding with the true sun at the perigee and apogee (when the Earth is in perihelion and aphelion, respectively). Then consider a second fictitious Sun travelling along the \"celestial equator\" at a constant speed and coinciding with the first fictitious Sun at the equinoxes. This second fictitious sun is the \"mean Sun\"...\"\n\nIn 1936 French and German astronomers found that Earth rotation's speed is irregular. Since 1967 atomic clocks define the second.\n\nThe length of a seconds pendulum was determined (in toises) by Marin Mersenne in 1644. In 1660, the Royal Society proposed that it be the standard unit of length. In 1671 Jean Picard measured this length at the Paris observatory. He found the value of 440.5 lines of the Toise of Châtelet which had been recently renewed. He proposed a universal toise (French: \"Toise universelle\") which was twice the length of the seconds pendulum. However, it was soon discovered that the length of a seconds pendulum varies from place to place: French astronomer Jean Richer had measured the 0.3% difference in length between Cayenne (in French Guiana) and Paris.\n\nJean Richer and Giovanni Domenico Cassini measured the parallax of Mars between Paris and Cayenne in French Guiana when Mars was at its closest to Earth in 1672. They arrived at a figure for the solar parallax of 91/2 inches, equivalent to an Earth–Sun distance of about 22000 Earth radii. They were also the first astronomers to have access to an accurate and reliable value for the radius of Earth, which had been measured by their colleague Jean Picard in 1669 as 3269 thousand \"toises\". Picard's geodetic observations had been confined to the determination of the magnitude of the earth considered as a sphere, but the discovery made by Jean Richer turned the attention of mathematicians to its deviation from a spherical form. The determination of the figure of the earth became a problem of the highest importance in astronomy, inasmuch as the diameter of the earth was the unit to which all celestial distances had to be referred.\n\nBritish physicist Isaac Newton, who used Picard's Earth measurement for establishing his law of universal gravity, explained this variation of the seconds pendulum's length in his \"Principia Mathematica\" (1687) in which he outlined his theory and calculations on the shape of the Earth. Newton theorized correctly that the Earth was not precisely a sphere but had an oblate ellipsoidal shape, slightly flattened at the poles due to the centrifugal force of its rotation. Since the surface of the Earth is closer to its center at the poles than at the equator, gravity is stronger there. Using geometric calculations, he gave a concrete argument as to the hypothetical ellipsoid shape of the Earth.\n\nThe goal of \"Principia\" was not to provide exact answer for natural phenomena, but to theorize potential solutions to these unresolved factors in science. Newton pushed for scientists to look further into the unexplained variables. Two prominent researchers that he inspired were Alexis Clairaut and Pierre Louis Maupertuis. They both sought to prove the validity of Newton's theory on the shape of the Earth. In order to do so, they went on an expedition to Lapland in an attempt to accurately measure the meridian arc. From such measurements they could calculate the eccentricity of the Earth, its degree of departure from a perfect sphere. Clairaut confirmed that Newton's theory that the Earth was ellipsoidal was correct, but his calculations were in error, and wrote a letter to the Royal Society of London with his findings. The society published an article in Philosophical Transactions the following year in 1737 that revealed his discovery. Clairaut showed how Newton's equations were incorrect, and did not prove an ellipsoid shape to the Earth. However, he corrected problems with the theory, that in effect would prove Newton's theory correct. Clairaut believed that Newton had reasons for choosing the shape that he did, but he did not support it in \"Principia.\" Clairaut's article did not provide a valid equation to back up his argument neither. This created much controversy in the scientific community.\n\nIt was not until Clairaut wrote \"Théorie de la figure de la terre\" in 1743 that a proper answer was provided. In it, he promulgated what is more formally known today as Clairaut's theorem. By applying Clairaut's theorem, Laplace found from 15 gravity values that the flattening of the Earth was 1/330. A modern estimate is 1/298.25642.\n\nIn 1790, one year before the metre was ultimately based on a quadrant of the Earth, Talleyrand proposed that the metre be the length of the seconds pendulum at a latitude of 45°. This option, with one-third of this length defining the \"foot\", was also considered by Thomas Jefferson and others for redefining the yard in the United States shortly after gaining independence from the British Crown.\nInstead of the seconds pendulum method, the commission of the French Academy of Sciences – whose members included Lagrange, Laplace, Monge and Condorcet – decided that the new measure should be equal to one ten-millionth of the distance from the North Pole to the Equator (the quadrant of the Earth's circumference), measured along the meridian passing through Paris. Apart from the obvious consideration of safe access for French surveyors, the Paris meridian was also a sound choice for scientific reasons: a portion of the quadrant from Dunkirk to Barcelona (about 1000 km, or one-tenth of the total) could be surveyed with start- and end-points at sea level, and that portion was roughly in the middle of the quadrant, where the effects of the Earth's oblateness were expected to be the largest. The Spanish-French geodetic mission combined with an earlier mesurement of the Paris meridian arc and the Lapland geodetic mission had confirmed that the Earth was an oblate spheroid. Moreover observations were made with a pendulum to determine the local acceleration due to local gravity and centrifugal acceleration; and these observations coincided with the geodetic results in proving that the Earth is flattened at the poles. The acceleration of a body near the surface of the Earth, which is measured with the seconds pendulum, is due to the combined effects of local gravity and centrifugal acceleration. The gravity diminishes with the distance from the center of the Earth while the centrifugal force augments with the distance from the axis of the Earth's rotation, it follows that the resulting acceleration towards the ground is 0.5% greater at the poles than at the Equator and that the polar diameter of the Earth is smaller than its equatorial diameter.\n\nThe Academy of Sciences planned to infer the flattening of the Earth from the length's differences between meridional portions corresponding to one degree of latitude. Pierre Méchain and Jean-Baptiste Delambre combined their measurements with the results of the Spanish-French geodetic mission and found a value of 1/334 for the Earth's flattening, and they then extrapolated from their measurement of the Paris meridian arc between Dunkirk and Barcelona the distance from the North Pole to the Equator which was 5 130 740 toises. As the metre had to be equal to one ten-millionth of this distance, it was defined as 0.513074 toise or 3 feet and 11.296 lines of the Toise of Peru. The Toise of Peru had been constructed in 1735 as the standard of reference in the Spanish-French Geodesic Mission, conducted in actual Ecuador from 1735 to 1744.\n\nJean-Baptiste Biot and François Arago published in 1821 their observations completing those of Delambre and Mechain. It was an account of the length's variation of the degrees of latitude along the Paris meridian as well as the account of the variation of the seconds pendulum's length along the same meridian between Shetland and the Baleares. The seconds pendulum's length is a mean to measure \"g\", the local acceleration due to local gravity and centrifugal acceleration, which varies depending on one's position on Earth (see Earth's gravity).\nA simple pendulum is an example of harmonic oscillator:\n\nAssuming no damping, the differential equation governing a simple pendulum of length formula_1, where formula_2 is the local acceleration due to local gravity and centrifugal acceleration, is\n\nIf the maximal displacement of the pendulum is small, we can use the approximation formula_4 and instead consider the equation\n\nThe solution to this equation is given by\n\nwhere formula_7 is the largest angle attained by the pendulum. The period, the time for one complete oscillation, is given by the expression\n\nwhich is a good approximation of the actual period when formula_7 is small.\n\nThe length of the seconds pendulum is a function of the time lapse of half a cycle formula_10\n\nBeing formula_12 \ntherefore formula_13.\n\nThe task of surveying the Paris meridian arc took more than six years (1792–1798). The technical difficulties were not the only problems the surveyors had to face in the convulsed period of the aftermath of the Revolution: Méchain and Delambre, and later Arago, were imprisoned several times during their surveys, and Méchain died in 1804 of yellow fever, which he contracted while trying to improve his original results in northern Spain. In the meantime, the commission of the French Academy of Sciences calculated a provisional value from older surveys of 443.44 \"lignes\". This value was set by legislation on 7 April 1795. While Méchain and Delambre were completing their survey, the commission had ordered a series of platinum bars to be made based on the provisional metre. When the final result was known, the bar whose length was closest to the meridional definition of the metre was selected and placed in the National Archives on 22 June 1799 (4 messidor An VII in the Republican calendar) as a permanent record of the result. This standard metre bar became known as the Committee metre (French : \"Mètre des Archives\").\n\nThe Articles of Confederation, ratified by the colonies in 1781, contained the clause, \"The United States in Congress assembled shall also have the sole and exclusive right and power of regulating the alloy and value of coin struck by their own authority, or by that of the respective states—fixing the standards of weights and measures throughout the United States\". Article 1, section 8, of the Constitution of the United States (1789), transferred this power to Congress; \"The Congress shall have power...To coin money, regulate the value thereof, and of foreign coin, and fix the standard of weights and measures\".\n\nIn January 1790, President George Washington, in his first annual message to Congress stated that, \"Uniformity in the currency, weights, and measures of the United States is an object of great importance, and will, I am persuaded, be duly attended to\", and ordered Secretary of State Thomas Jefferson to prepare a plan for Establishing Uniformity in the Coinage, Weights, and Measures of the United States, afterwards referred to as the Jefferson report. On October 25, 1791, Washington appealed a third time to Congress, \"A uniformity of the weights and measures of the country is among the important objects submitted to you by the Constitution and if it can be derived from a standard at once invariable and universal, must be no less honorable to the public council than conducive to the public convenience\".\n\nThe original predecessor agency of the National Geodetic Survey was the United States Survey of the Coast, created within the United States Department of the Treasury by an Act of Congress on February 10, 1807, to conduct a \"Survey of the Coast.\" The Survey of the Coast, the United States government's first scientific agency, represented the interest of the administration of President Thomas Jefferson in science and the stimulation of international trade by using scientific surveying methods to chart the waters of the United States and make them safe for navigation. A Swiss immigrant with expertise in both surveying and the standardisation of weights and measures, Ferdinand R. Hassler, was selected to lead the Survey.\n\nHassler submitted a plan for the survey work involving the use of triangulation to ensure scientific accuracy of surveys, but international relations prevented the new Survey of the Coast from beginning its work; the Embargo Act of 1807 brought American overseas trade virtually to a halt only a month after Hassler's appointment and remained in effect until Jefferson left office in March 1809. It was not until 1811 that Jefferson's successor, President James Madison, sent Hassler to Europe to purchase the instruments necessary to conduct the planned survey, as well as standardised weights and measures. Hassler departed on August 29, 1811, but eight months later, while he was in England, the War of 1812 broke out, forcing him to remain in Europe until its conclusion in 1815. Hassler did not return to the United States until August 16, 1815.\n\nThe Survey finally began surveying operations in 1816, when Hassler started work in the vicinity of New York City. Preliminary baselines' measurements were undertaken in 1817. The first baseline was measured in 1834 and verified in 1844 and 1857. The unit of length to which all distances measured in the U. S. coast survey would be referred was the Committee metre (French: \"Mètre des Archives\")\",\" of which Ferdinand Rudolph Hassler had brougth a copy in the United States in 1805\".\"\n\nIn 1821, John Quincy Adams had declared \"Weights and measures may be ranked among the necessities of life to every individual of human society\". From 1830 until 1901, the role of overseeing weights and measures was carried out by the Office of Standard Weights and Measures, which was part of the United States Department of the Treasury. Ferdinand Rudolph Hassler became the head of the Bureau of Weights and Measures in the Treasury Department where he carried out the early work of establishing the standards of weights and measures in the United States, with the involvement of fellow Swiss immigrant Albert Gallatin, who in 1827 brought from Europe a troy pound of brass which was made the standard of mass in 1828. Hassler undertook a complete investigation of the national standards in 1830, but it was not until 1838, that a uniform set of standards was worked out.\n\nIn 1835, the invention of the telegraph by Samuel Morse allowed new progresses in the field of geodesy as longitudes were determined with greater accuracy. Moreover the publication in 1838 of Friedrich Wilhelm Bessel’s \"Gradmessung in Ostpreussen\" marked a new era in the science of geodesy. Here was found the method of least squares applied to the calculation of a network of triangles and the reduction of the observations generally. The systematic manner in which all the observations were taken with the view of securing final results of extreme accuracy was admirable. Bessel was also the first scientist who realised the effect later called \"personal equation\", that several simultaneously observing persons determine slightly different values, especially recording the transition time of stars. For his survey Bessel used a copy of the Toise of Peru constructed in 1823 by Fortin in Paris.\n\nThe first clear and concise exposition of the method of least squares was published by Legendre in 1805. The technique is described as an algebraic procedure for fitting linear equations to data and Legendre demonstrates the new method by analyzing the same data as Laplace for the shape of the earth. The value of Legendre's method of least squares was immediately recognised by leading astronomers and geodesists of the time.\n\nIn 1809 Carl Friedrich Gauss published his method of calculating the orbits of celestial bodies. In that work he claimed to have been in possession of the method of least squares since 1795. This naturally led to a priority dispute with Legendre. However, to Gauss's credit, he went beyond Legendre and succeeded in connecting the method of least squares with the principles of probability and to the normal distribution. He had managed to complete Laplace's program of specifying a mathematical form of the probability density for the observations, depending on a finite number of unknown parameters, and define a method of estimation that minimises the error of estimation. Gauss showed that the arithmetic mean is indeed the best estimate of the location parameter by changing both the probability density and the method of estimation. He then turned the problem around by asking what form the density should have and what method of estimation should be used to get the arithmetic mean as estimate of the location parameter. In this attempt, he invented the normal distribution.\n\nAn early demonstration of the strength of Gauss' method came when it was used to predict the future location of the newly discovered asteroid Ceres. On 1 January 1801, the Italian astronomer Giuseppe Piazzi discovered Ceres and was able to track its path for 40 days before it was lost in the glare of the sun. Based on these data, astronomers desired to determine the location of Ceres after it emerged from behind the sun without solving Kepler's complicated nonlinear equations of planetary motion. The only predictions that successfully allowed Hungarian astronomer Franz Xaver von Zach to relocate Ceres were those performed by the 24-year-old Gauss using least-squares analysis.\n\nIn 1810, after reading Gauss's work, Laplace, after proving the central limit theorem, used it to give a large sample justification for the method of least squares and the normal distribution. In 1822, Gauss was able to state that the least-squares approach to regression analysis is optimal in the sense that in a linear model where the errors have a mean of zero, are uncorrelated, and have equal variances, the best linear unbiased estimator of the coefficients is the least-squares estimator. This result is known as the Gauss–Markov theorem.\n\nThe idea of least-squares analysis was also independently formulated by the American Robert Adrain in 1808. In the next two centuries workers in the theory of errors and in statistics found many different ways of implementing least squares.\n\nIn 1860, the Russian Government at the instance of Otto Wilhelm von Struve invited the Governments of Belgium, France, Prussia and England to connect their triangulations in order to measure the length of an arc of parallel in latitude 52° and to test the accuracy of the figure and dimensions of the Earth, as derived from the measurements of arc of meridian. In order to combine the measurements it was necessary to compare the geodetic standards of length used in the different countries.The British Government invited those of France, Belgium, Prussia, Russia, India, Australia, Austria, Spain, United States and Cape of Good Hope to send their standards to the Ordnance Survey office in Southampton. Notably the geodetic standards of France, Spain and United States were based on the metric system, whereas those of Prussia, Belgium and Russia where calibrated against the toise, of which the oldest physical representative was the Toise of Peru.\n\nThe Congress of 1866 made use of the metric system in commerce a legally protected activity through the passage of Metric Act of 1866. \n\nIn 1867 besides Russia and Portugal, Spain joined the geodetic association and was represented by Carlos Ibáñez e Ibáñez de Ibero. He had devised a geodetic standard calibrated against the metre which had been compared to the Toise of Borda, the copy of the Toise of Peru constructed for the measurement of the Paris meridian arc by Delambre and Mechain. Copies of the Spanish metric geodetic standard were made for some of the great European countries and for Egypt. The European geodetic association (German: \"Europäische Gradmessung\") at its Second General Conference in 1867 called for the creation of a new, \"international prototype metre\" (IPM) and the arrangement of a system where national standards could be compared with it. The international prototype would also be a \"line standard\"; that is, the metre was defined as the distance between two lines marked on the bar, so avoiding the wear problems of end standards.\n\nThe French government gave practical support to the creation of an International Metre Commission, which met in Paris in 1870 and again in 1872 with the participation of about thirty countries.\n\nFondator of the Spanish Geographic Institute, Carlos Ibáñez e Ibáñez de Ibero had collaborated with the French since 1853 on the completion of the West Europe-Africa Meridian-arc (French: \"Meridienne de France\"), which would then extend from Shetland to the Sahara, by expanding the continental geodetic survey in Spain and by connecting Spanish and Algerian geodetic network over the Mediterranean sea.\n\nAs a forerunner in Europe, Spain had adopted the metre as geodetic standard, contributed to the \"Europäische Gradmessung\"'s decision to adopt the metre and played a leading role in the fondation of the International Bureau of Weights and Measures. Indeed Carlos Ibáñez e Ibáñez de Ibero took part in the Committee of preparatory research from the first meeting of the International Metre Commission in 1870, became president of the permanent Committee of the International Metre Commission in 1872, represented Spain at the 1875 conference of the Metre Convention and at the first General Conference on Weights and Measures in 1889 and was the first president of the International Committee for Weights and Measures from 1875 to 1891.\n\nOn May 20, 1875, 17 out of 20 countries signed a document known as the \"Metric Convention\" or the \"Treaty of the Metre\", which established the International Bureau of Weights and Measures under the control of an international committee elected by the General Conference on Weights and Measures.\n\nCharles Sanders Peirce was instructed early in the spring of 1875 to proceed to Europe for the purpose of making pendulum experiments to chief initial stations for operations of this sort, in order to bring the determinations of the forces of gravity in America into communication with those of other parts of the world; and also for the purpose of making a careful study of the methods of pursuing these researches in the different countries of Europe.\n\nIn 1877 Charles S. Peirce attended the 5 General conference of the \"Europäische Gradmessung\" in Stuttgart where he exposed his work on seconds pendulum and expressed his concern about the accuracy of gravity measurement. In 1880 the French Academy of sciences adopted Peirce's measurement of the seconds pendulum length in Paris, which was 993.934 millimetres.\n\nIn 1886 the association of geodesy changed name for the International Geodetic Association, which Carlos Ibáñez e Ibáñez de Ibero presided up to his death in 1891. During this period the International Geodetic Association (German: \"Internationale Erdmessung\") gained worldwide importance with the joining of United States, Mexico, Chile, Argentina and Japan.\n\nIn 1901, in response to a bill proposed by Congressman James H. Southard (R, Ohio), the National Bureau of Standards was founded with the mandate to provide standard weights and measures, and to serve as the national physical laboratory for the United States. (Southard had previously sponsored a bill for metric conversion of the United States.) President Theodore Roosevelt appointed Samuel W. Stratton as the first director. The budget for the first year of operation was $40,000. The Bureau took custody of the copies of the kilogram and metre bars that were the standards for US measures, and set up a program to provide metrology services for United States scientific and commercial users.\n\nEfforts to supplement the various national surveying systems, which begun in the 19th century with the foundation of the \"Mitteleuropäische Gradmessung\", resulted in a series of global ellipsoids of the Earth (e.g., Helmert 1906, Hayford 1910/ 1924) which would later lead to develop the World Geodetic System.\n\nThe construction of the international prototype metre and the copies which would be national standards was at the limits of the technology of the time. The bars were to be made of a special alloy, 90% platinum and 10% iridium, which is significantly harder than pure platinum, and have a special X-shaped cross section (a \"Tresca section\", named after French engineer Henri Tresca) to minimise the effects of torsional strain during length comparisons. The first castings proved unsatisfactory, and the job was given to the London firm of Johnson Matthey who succeeded in producing thirty bars to the required specification. One of these, No. 6, was determined to be identical in length to the \"\", and was consecrated as the international prototype metre at the first meeting of the CGPM in 1889. The other bars, duly calibrated against the international prototype, were distributed to the signatory nations of the Metre Convention for use as national standards. For example, the United States received No. 27 with a calibrated length of (1.6 µm short of the international prototype).\n\nThe first (and only) follow-up comparison of the national standards with the international prototype was carried out between 1921 and 1936, and indicated that the definition of the metre was preserved to within 0.2 µm. At this time, it was decided that a more formal definition of the metre was required (the 1889 decision had said merely that the \"prototype, at the temperature of melting ice, shall henceforth represent the metric unit of length\"), and this was agreed at the 7th CGPM in 1927.\nAlready in the early days of its existence, the International Committee for Weights and Measures (CIPM) proceeded to define a standard thermometric scale, using the boiling point of water. Since the boiling point varies with the atmospheric pressure, the CIPM needed to define a standard atmospheric pressure. The definition they chose was based on the weight of a column of mercury of 760 mm. But since that weight depends on the local gravity, they now also needed a standard gravity. The 1887 CIPM meeting decided as follows:\n\nAll that was needed to obtain a numerical value for standard gravity was now to measure the gravitational strength at the International Bureau. This task was given to Gilbert Étienne Defforges of the Geographic Service of the French Army. The value he found, based on measurements taken in March and April 1888, was 9.80991(5) m⋅s.\n\nThis result formed the basis for determining the value still used today for standard gravity. The third General Conference on Weights and Measures, held in 1901, adopted a resolution declaring as follows:\n\nThe numeric value adopted for was, in accordance with the 1887 CIPM declaration, obtained by dividing Defforges's result – 980.991 cm⋅s in the cgs system then \"en vogue\" – by 1.0003322 while not taking more digits than warranted considering the uncertainty in the result.\n\n"}
{"id": "30148162", "url": "https://en.wikipedia.org/wiki?curid=30148162", "title": "Self-drying concrete technology", "text": "Self-drying concrete technology\n\nSelf-drying concrete technology is found in certain cementitious patching and leveling materials and tile-setting mortars used in the flooring industry. Self-drying technology allows the cement mix to consume all of its mix water while curing, eliminating the need for excess water to evaporate prior to installing flooring. Traditional floor coverings, such as VCT, sheet vinyl, carpet and ceramic tile, can be installed before the material is completely dry and as soon as it hardens, which typically happens in the first two hours after placement.\n\nTraditional concrete has a water:cement ratio of about 0.5, which refers to the weight of the water divided by the weight of the cement. A water:cement ratio of 0.5 provides good workability while keeping the amount of excess water in the mix fairly low. Without at least this much extra water, the concrete would be too dry to place.\n\nThe chemical reaction of Portland cement and water that is known as hydration, which is necessary for the strengthening of the concrete, requires a water:cement ratio of only about 0.25. With a water:cement ratio of 0.5, there is twice the amount of water in the concrete mix than what is needed for hydration. This excess water needs to evaporate before flooring can be installed. Note: The magical number of 28 days defines only the designed strength of the concrete but has nothing to do with the dryness of it. E.g. A 10-year-old concrete slab can contain more moisture than a 28-day-old slab! Conversely, a self-drying concrete blend consumes all of its mix water with a water:cement ratio of up to 0.6, maintaining good workability while allowing flooring to be installed before it is completely dry.\n\nThere are also cement products that are partially self-drying, meaning that they use a high percentage of their mix water for hydration as opposed to using 100% of it. This type of product might be used when the flooring does not need to be installed the same day but must still be installed more quickly than traditional concrete would allow. For instance, products that are 80% self-drying allow flooring to be installed the next day, typically after a 16-hour cure.\n\nSelf-drying technology was developed by Ardex (#Ardexisking) in Germany and was introduced in the United States in 1978.\n"}
{"id": "31387145", "url": "https://en.wikipedia.org/wiki?curid=31387145", "title": "Stithians Reservoir", "text": "Stithians Reservoir\n\nStithians Reservoir is a reservoir situated just under a mile to the west of the village of Stithians, Cornwall, England, UK.\n\nWork on the dam began on 19 July 1962, however work stopped shortly after due to excavations which revealed the site was over a large area of soft kaolinised granite, which would require expensive foundations. A new design was drawn up for the dam, which was the current arched design. The dam was opened on 13 October 1967 by Sir John Carew Pole, whilst the Bishop of Truro, Dr Maurice Key blessed the reservoir.\n\nBy the opening in 1967 the valley behind the dam was completely flooded, 274 acres of farmland and three country houses would were now submerged. Two causeways were built to carry sections of roads that were flooded, one at the southern end and the other in the north western corner near the Golden Lion Inn.\n\nThe dam is 41.5 m high and 244m wide. It was built at a total cost of £1,125,000 (about £17 million today) (2009 estimate).\n\nThere is an Outdoor Activity Centre offering watersports facilities at Stithians Lake. Sailing, windsurfing, kayaking, SUP and canoeing activities available. Non sport activities include bird watching, fly fishing for trout & a campsite.\n\n\nBill Scolding (2009) \"Five Walks around Wendron\", Wendron Parish Council\n"}
{"id": "30283480", "url": "https://en.wikipedia.org/wiki?curid=30283480", "title": "Structure of liquids and glasses", "text": "Structure of liquids and glasses\n\nThe structure of liquids, glasses and other non-crystalline solids is characterized by the absence of long-range order which defines crystalline materials. Liquids and amorphous solids do, however, possess a rich and varied array of short to medium range order, which originates from chemical bonding and related interactions. Metallic glasses, for example, are typically well described by the dense random packing of hard spheres, whereas covalent systems, such as silicate glasses, have sparsely packed, strongly bound, tetrahedral network structures. These very different structures result in materials with very different physical properties and applications.\n\nThe study of liquid and glass structure aims to gain insight into their behavior and physical properties, so that they can be understood, predicted and tailored for specific applications. Since the structure and resulting behavior of liquids and glasses is a complex many body problem, historically it has been too computationally intensive to solve using quantum mechanics directly. Instead, a variety of diffraction, NMR, Molecular dynamics, and Monte Carlo simulation techniques are most commonly used.\n\nThe pair distribution function (or pair correlation function) of a material describes the probability of finding an atom at a separation \"r\" from another atom.\n\nA typical plot of \"g\" versus \"r\" of a liquid or glass shows a number of key features:\n\n\nThe static structure factor, \"S(q)\", which can be measured with diffraction techniques, is related to its corresponding \"g(r)\" by Fourier transformation\n\nwhere \"q\" is the magnitude of the momentum transfer vector, and ρ is the number density of the material. Like \"g(r)\", the \"S(q)\" patterns of \nliquids and glasses have a number of key features:\n\n\nThe absence of long-range order in liquids and glasses is evidenced by the absence of Bragg peaks in X-ray and neutron diffraction. For these isotropic materials, the diffraction pattern has circular symmetry, and in the radial direction, the diffraction intensity has a smooth oscillatory shape. This diffracted intensity is usually analyzed to give the static structure factor, \"S(q)\", where \"q\" is given by \"q\"=4πsin(θ)/λ, where 2θ is the scattering angle (the angle between the incident and scattered quanta), and λ is the incident wavelength of the probe (photon or neutron). Typically diffraction measurements are performed at a single (monochromatic) λ, and diffracted intensity is measured over a range of 2θ angles, to give a wide range of \"q\". Alternatively a range of λ, may be used, allowing the intensity measurements to be taken at a fixed or narrow range of 2θ. In x-ray diffraction, such measurements are typically called “energy dispersive”, whereas in neutron diffraction this is normally called “time-of-flight” reflecting the different detection methods used.\nOnce obtained, an \"S(q)\" pattern can be Fourier transformed to provide a corresponding radial distribution function (or pair correlation function), denoted in this article as \"g(r)\". For an isotropic material, the relation between \"S(q)\" and its corresponding \"g(r)\" is\n\nThe \"g(r)\", which describes the probability of finding an atom at a separation \"r\" from another atom, provides a more intuitive description of the atomic structure. The \"g(r)\" pattern obtained from a diffraction measurement represents a spatial, and thermal average of all the pair correlations in the material, weighted by their coherent cross-sections with the incident beam.\n\nBy definition, \"g(r)\" is related to the average number of particles found within a given volume of shell located at a distance \"r\" from the center. The average density of atoms at a given radial distance from another atom is given by the formula:\n\nwhere \"n\"(\"r\") is the mean number of atoms in a shell of width Δ\"r\" at distance \"r\". The \"g(r)\" of a simulation box can be calculated easily by histograming the particle separations using the following equation\n\nwhere \"N\" is the number of \"a\" particles, |r| is the magnitude of the separation of the pair of particles \"i,j\". Atomistic simulations can also be used in conjunction with interatomic pair potential functions in order to calculate macroscopic thermodynamic parameters such as the internal energy, Gibbs free energy, entropy and enthalpy of the system.\n\nOther experimental techniques often employed to study the structure of glasses include Nuclear Magnetic Resonance (NMR), X-ray absorption fine structure (XAFS) and other spectroscopy methods including Raman spectroscopy. Experimental measurements can be combined with computer simulation methods, such as Reverse Monte Carlo (RMC) or molecular dynamics (MD) simulations, to obtain more complete and detailed description of the atomic structure.\n\nEarly theories relating to the structure of glass included the crystallite theory whereby glass is an aggregate of crystallites (extremely small crystals). However, structural determinations of vitreous SiO and GeO made by Warren and co-workers in the 1930s using x-ray diffraction showed the structure of glass to be typical of an amorphous solid\nIn 1932 Zachariasen introduced the random network theory of glass in which the nature of bonding in the glass is the same as in the crystal but where the basic structural units in a glass are connected in a random manner in contrast to the periodic arrangement in a crystalline material.\n\nDespite the lack of long range order, the structure of glass does exhibit a high degree of ordering on short length scales due to the chemical bonding constraints in local atomic polyhedra. For example, the SiO tetrahedra that form the fundamental structural units in silica glass represent a high degree of order, i.e. every silicon atom is coordinated by 4 oxygen atoms and the nearest neighbour Si-O bond length exhibits only a narrow distribution throughout the structure. The tetrahedra in silica also form a network of ring structures which leads to ordering on more intermediate length scales of up to approximately 10 Angstroms.\n\nAlternative views of the structure of liquids and glasses include the interstitialcy\nmodel \nand the model of \"string-like\" correlated motion. \nMolecular dynamics computer simulations indicate these two models are closely connected\nAs in other amorphous solids, the atomic structure of a glass lacks any long range translational periodicity. However, due to chemical bonding characteristics glasses do possess a high degree of short-range order with respect to local atomic polyhedra.\n\nIt is deemed that the bonding structure of glasses, although disordered, has the same symmetry signature (Hausdorff-Besicovitch dimensionality) as for crystalline materials.\n\nSilica (the chemical compound SiO) has a number of distinct crystalline forms: quartz, tridymite, cistobalite, and others (including the high pressure polymorphs Stishovite and Coesite). Nearly all of them involve tetrahedral SiO units linked together by \"shared vertices\" in different arrangements. Si-O bond lengths vary between the different crystal forms. For example, in α-quartz the bond length is 161 pm, whereas in α-tridymite it ranges from 154–171 pm. The Si-O-Si bond angle also varies from 140° in α-tridymite to 144° in α-quartz to 180° in β-tridymite.\n\nIn amorphous silica (fused quartz), the SiO tetrahedra form a network that does not exhibit any long-range order. However, the tetrahedra themselves represent a high degree of local ordering, i.e. every silicon atom is coordinated by 4 oxygen atoms and the nearest neighbour Si-O bond length exhibits only a narrow distribution throughout the structure. If one consider the atomic network of silica as a mechanical truss, this structure is isostatic, in the sense that the number of constraints acting between the atoms equals the number of degrees of freedom of the latter. According to the rigidity theory, this allows this material to show a great forming ability. Despite the lack of ordering on extended length scales, the tetrahedra also form a network of ring-like structures which lead to ordering on intermediate length scales (up to approximately 10 Angstroms or so). Under the application of high pressure (approximately 40 GPa) silica glass undergoes a continuous polyamorphic phase transition into an octahedral form, i.e. the Si atoms are surrounded by 6 oxygen atoms instead of four in the ambient pressure tetrahedral glass.\n\n\n"}
{"id": "31919148", "url": "https://en.wikipedia.org/wiki?curid=31919148", "title": "Title 10 of the Code of Federal Regulations", "text": "Title 10 of the Code of Federal Regulations\n\nTitle 10 of the Code of Federal Regulations is one of fifty titles comprising the United States Code of Federal Regulations (CFR), containing the principal set of rules and regulations issued by federal agencies regarding nuclear energy. It is available in digital and printed form, and can be referenced online using the Electronic Code of Federal Regulations (e-CFR).\n\nThe table of contents, as reflected in the e-CFR updated June 19, 2014, is as follows:\n\nParts 0 to 199 are the requirements (and reserved for the requirements) prescribed by the United States Nuclear Regulatory Commission (NRC) and binding on all persons and organizations who receive a license from NRC to use nuclear materials or operate nuclear facilities. Licensing is required for the design, manufacture, construction, operation and decommissioning of nuclear reactors. These facilities can be commercial, research or test reactors. The authority is broad covering the licensing of individual facilitatory operators to the entities which operate these facilities.\n"}
{"id": "36388653", "url": "https://en.wikipedia.org/wiki?curid=36388653", "title": "Torbal", "text": "Torbal\n\nTorbal is a manufacturer of prescription scales that are used in pharmacies, and it produces a laboratory scales including analytical balances for research purposes and industrial scales for quality control applications. In addition, the company makes related products such as moisture analyzers and force gauges.\n\nTorbal traces its manufacturing of scales and balances to 1897. The name \"Torbal\" is an acronym and trademark for the Torsion Balance Company. The company has had several different owners throughout its history. It is now owned by Scientific Industries, Inc. which is a publicly owned New York corporation headquartered in Bohemia, New York.\n\nThe company markets its products primarily in the northern hemisphere with the US, Canada, and Mexico predominating. Torbal sells through some distributors in the pharmacy market, but the bulk of its sales are done through direct purchasing. It operates a number of affiliated web sites and also sells directly through e-commerce.\n\nThe company has continuously manufactured and sold torsion balances. Torbal's scales are used for pill counting and for drug compounding. Its principal markets have been retail druggists who have used it in compounding prescriptions, and with users who needed a precision weighing device that could be used under adverse conditions that would quickly ruin balances with knife edges or friction bearings. \n\nThe Torsion Balance Company was preceded by the Springer Torsion Balance Company, whose Certificate of Incorporation was filed on September 7, 1897. The purposes of that corporation were to \"acquire the ownership of patents on scales, balances, automatic discharging meters, and other instruments of precision and to manufacture and sell such articles.\" Five years later, The Torsion Balance Company filed a New York State Certificate of Incorporation .\n\nThe corporation had its principal place of business at 92 Reade Street, New York City, and a plant in Jersey City, New Jersey. In May 1949 both the New York office and the Jersey City plant were moved to New Jersey, and consolidated in a new factory in Clifton, New Jersey.\nOn February 8, 1915, the Torsion Balance Company bought the business carried on under the name of Christian Becker which was engaged in the manufacture and sale of analytical balances. This purchase included the right to the use of the trade name \"Christian Becker\". The acquisition of Christian Becker offered opportunity for the introduction of the torsion balance into laboratories as a balance less sensitive and less expensive than the analytical balance, but one for which there was a need. Over these succeeding years, the sales volume of Torsion Balance laboratory scales was built up until it was second to Torsion’s pharmacy sales. Additional markets for the Torsion Balance were found in the dairy industry, where the balance was used to determine the butter-fat content of milk, and in the textile industry.\n\nAfter the acquisition of the business of Christian Becker, the Torsion Balance Company formed a New York corporation called Christian Becker Incorporated by filing a certificate dated February 18, 1915. Its directors were Harold H. Fries, William Clark Symington, and Robert B. Symington who were also its original stockholders and all of 92 Reade Street, New York City. This corporation remained in existence until the year 1943 when it was dissolved in the manner later described; and it, thereupon, became a division of the Torsion Balance Company.\n\nChristopher Becker came to New York from Arnhem, the Netherlands in 1836, leaving two sons Jule and Henry in the Netherlands. With his other sons Christian and Ernest, he established an observatory at 54 Columbia Street in Brooklyn where he manufactured nautical and astronomical instruments. In 1854 Professor J. Renevick of Columbia College asked Christopher to manufacture an analytical balance to supplement several British and German ones he already had. The result was good, and Becker and Sons was founded for the purpose of manufacturing balances and weights. Christian and Ernest were the sons of Becker and Sons.\n\nIn 1861, possibly to get away from the Civil War, the family returned to the Netherlands and manufactured balances in Antwerp. It is likely that the other two brothers Jule and Henry participated in this venture and gained their knowledge of balances in this way. When the Civil War was ended (1865), Christopher, Christian, and Ernest returned to the United States and established a new factory in Hudson City, New Jersey.\n\nJule and Henry established Becker and Sons, Rotterdam and H. L. Becker, Fila, Brussels. Whether there were originally two joint ventures is not clear, but Jule is predominantly associated with the Rotterdam Company and Henry with the Brussels firm.\n\nChristopher moved from Hudson City, New Jersey to Newark, New Jersey and in 1874 to New Rochelle, New York. In 1884 the sons left Christopher and started their own business as Becker Bros. Christopher continued his business as Christopher Becker and apparently died soon afterwards.\n\nIn 1892 Ernest died, and the name was changed to Christian Becker. In 1915 the company was bought by the Torsion Balance Company. Christian's two sons, Christopher and Frank continued to be associated with the Torsion Balance Company. Christopher died in 1949 and Frank in 1956. Christopher (Sr.) is credited by 9th Edition of Britannica with the invention of plane bearings. Christopher (Jr.) held the patent on the chainomatic device (1915) and received the Franklin Medal for this development.\n\nHarold H. Fries died on June 29, 1946 and his shares were ultimately registered in the name of Wills and Company as nominee. Upon the death of Harold H. Fries, it was found that there was no one suitable to act as directing head. The attorney for the estate, Walter D. Fletcher, was designated as President of the Torsion Balance Company as an interim measure until a new directing head could be found. In the intervening months, this effort went on, and on December 13, 1946, Charles E. Donovan was made Vice President and active directing head of the Company. He was elected president of the company on November 12, 1947. At the meeting of the Board of Directors on December 13, 1946, it was decided that this company would undergo a complete modernization. In the course of this modernization a new plant was erected, the three principal lines of balances were redesigned to be more acceptable in the market, new machinery was acquired, new tooling and new sales policies were adopted, new accounting procedures installed and practically a new organization created.\n\nThis modernization was successful and the company grew in its Clifton, NJ facility. However, increased competition from European companies, especially Mettler and Sartorious, for space in the major distributor’s catalogs was becoming a problem, as were US labor costs. It was decided that a new manufacturing facility in Ireland might reduce product cost to manufacture and increase Torsion’s competitive position. The move was unsuccessful and much of the company’s assets were dissipated as a result. It was decided by the Brown Brothers Harriman Bank that Torbal be discreetly put up for sale. Brown Brothers were involved on behalf of Roland Harriman, Averill Harriman’s older brother, and had purchased all of the Torsion Balance Stock to settle an internal squabble. The company was sold to Vertex Industries Inc. in 1965.\n\nTorbal prospered under Vertex and was restored to full strength by 1970. Vertex was becoming interested in automatic Identification technology and acquired a bar code company, Identicon, and also acquired a magnetic stripe readers product line from the Business Products Division of Dymo Industries. The era of digital scales had arrived, and once again Torbal was coming under competitive pressure. Vertex decided that the auto ID business showed far more growth potential and did not invest in the development of electronic digital scales. Torbal continued with its mechanical scales until in 2000 the Company was sold by Vertex to Fulcrum Inc., the present owner.\n\nThe company relocated and soon began to develop its line of digital scales. It developed an alliance with a Polish scale manufacturer that has a first class ISO approved manufacturing facility, excellent technical capability, and very competitive labor costs. Working together they produced a line of prescription scales, and then a line of laboratory and analytical scales for the North American market. The alliance continues today.\n\nSince the introduction of their first digital scales, Fulcrum has made huge advances in the technology of these products. Creating patented features such as Advanced Pill Counting Accuracy (APA) and Pill Fragment Detection (PFD) has contributed to the increased efficiency of pharmacies and safety of their customers. Rx Verification, also known as NDC Verification, has enabled pharmacists to match the barcodes on prescriptions with those on the supply bottles.\n\nIn an effort to expand the Torbal line, Fulcrum introduced industrial weighing scales, moisture analyzers, and washdown scales. These added products appealed to a larger segment of laboratory and industrial sectors. With the expanded product selections, Fulcrum was able to begin offering customizable firmware to customers. The service, known as “Torbal Custom,” was created so that customers could better utilize Torbal products for their intended applications.\n\nIn 2012, Fulcrum broke into the force gauge industry with the release of their FA series gauges. The force gauges were the first weighing devices offered under the Torbal brand that were not required to be used as a stationary top or bottom loading unit. The FA series included ergonomic gauges with internal weighing mechanisms as well as high capacity models with external load cells. An FC series was announced to be released in the spring of 2014.\n\nTo complement Torbal’s upcoming FC line, they are currently developing a motorized test stand. This accessory will not only eliminate the need for users to manually drive the gauge along the stand, but it will provide more accurate force over distance measurements by simply inputting the desired distance prior to testing.\n\nFulcrum, Inc. looks to finalize their development of micro-analytical scales in the near future as well.\n\nThe company produces a wide range of weighing products. The present day product line includes industrial scales, moisture analyzers, and force gauges, and newer versions of lab and analytical balances. An up-to-date line of prescription scales and systems is available for pharmacies. Sensitivity ranges from 0.0001g for its analytical balances, 0.001g for its laboratory and prescription scales, and 0.01g for its industrial scales. Capacities range from 50.0g up to 6000.0g for the lab, analytical, prescription and industrial scales. Torbal crane scales and force gauges can go to higher capacities. \n\nThe company has produced many products thatreceived industry recognition, e.g., magazine ranking or metrology society awards. The most famous was the Model EA-1 which was the first analytical balance to use electronic force substitution. The last three digits of weight were read from a digital Veeder Root dial that was linked to a precision 10 turn potentiometer. The potentiometer output voltage was converted to force coil current which caused a magnet suspended from the weigh beam to apply a linear force to bring the mechanism into a precise balance condition. Also in the balance an E-Core type structure was used to detect the beam position and the result was displayed on an electronic zero center null meter. The first three decades were done with digital weight loading dials. This balance, which was marketed starting in the early 1960s, is still in use in many labs around the country and they appear fairly regularly on e-bay as they are retired.\n\nAnother scale of significance was the Model PL-1, which was quickly followed by a series of scales based upon the same principle. This scale was an optical projection type that was intended for both laboratory and industrial use. The first decade was a digital weight loader which was used to bring the beam close enough to a null position so that the next three digits could be read directly off of an optical projection screen. Many people claimed that the optical portion of the scale was accurate to four places, however, reading a linear dial projection to four places without a vermier is a real feat. In this scale the beam deflection moved a glass plate which contained the graduated scale and the image was projected onto a screen in the front of the scale. The scale used a viscous fluid dashpot type of damping system and was remarkably fast. It soon became the favorite scale for many industrial applications because the torsion type taut band suspension made it very rugged and the simplicity of weighing combined with its fast damping made QC testing a simple chore.\n\nOne other scale that is deserving of recognition is the Model ST-1, which was designed specifically to be a student balance that could be used in high school and college chemistry labs to do accurate weighing (0.01% can’t be called precision) and allow students to develop a feel for weighing. The scale’s structure was actually half of an equal arm Roberval balance. A calibrated dial with a calibrated spring attached to it was used to apply a restoring torque to equal that of the torque applied by the unknown weight. With an attractive price this scale soon became very popular in schools across the nation.\nThe only mechanical pharmacy balance still manufactured by Torbal is the DRX-3.\n\n"}
{"id": "49537788", "url": "https://en.wikipedia.org/wiki?curid=49537788", "title": "Tornadoes of 1966", "text": "Tornadoes of 1966\n\nThis page documents the tornadoes and tornado outbreaks of 1966, primarily in the United States. Most tornadoes form in the U.S., although some events may take place internationally. Tornado statistics for older years like this often appear significantly lower than modern years due to fewer reports or confirmed tornadoes.\n"}
{"id": "5577640", "url": "https://en.wikipedia.org/wiki?curid=5577640", "title": "Vitalij Aab", "text": "Vitalij Aab\n\nVitalij Aab (born November 14, 1979, in Karaganda, Soviet Union) is a German ice hockey player who is currently an Unrestricted Free Agent. He most recently played for the Thomas Sabo Ice Tigers in the DEL.\n\nVitalij Aab's professional debut was at the age of 17 for EC Wilhelmshaven. In his second season, he scored 28 points. In 1999, he was promoted into the second division with Wilhelmshaven. Two years later, he transferred to the Nuremberg Ice Tigers, where he played for three years. During this time he played so well that the German national coach called him up to play for the German national ice hockey team. Aab had the worst season of his career in 2004–05 with the Adler Mannheim; he only scored 24 points in 48 games, and therefore received very little ice time at the end of the season. \n\nHe left Mannheim after only one season and switched to the Iserlohn Roosters. After struggling at first, Aab's performance improved dramatically. At the end of the season, he was the highest scorer in Germany. Before the 2006–07 season, he signed with the Hamburg Freezers.\n"}
{"id": "33045451", "url": "https://en.wikipedia.org/wiki?curid=33045451", "title": "Yangshan Quarry", "text": "Yangshan Quarry\n\nThe Yangshan Quarry () is an ancient stone quarry near Nanjing, China. Used during many centuries as a source of stone for buildings and monuments of Nanjing, it is preserved as a historic site. The quarry is famous for the gigantic unfinished stele that was abandoned there during the reign of the Yongle Emperor in the early 15th century. In scope and ambition, the stele project is compared to other public works projects of Yongle era, which included the launching of the treasure fleet for Zheng He's maritime expeditions and the construction of the Forbidden City in Beijing.\n\nThe Yangshan Quarry is situated on the Yangshan Mountain (elevation 140 m),\nalso known as Yanmen Shan (),\nnorthwest of Tangshan Town ().\n\nThe Yangshan is the main peak of the Kongshan Mountain Range (). The site is located 15–20 km to the east from the eastern part of Nanjing City Wall and the Ming Xiaoling Mausoleum. Administratively, the area is in the Jiangning District of Nanjing City, Jiangsu Province.\n\nThe Yangshan Quarry has been worked from the time of the Six Dynasties, the local limestone being used for construction of buildings, walls, and statues in and around Nanjing.\n\nAfter Zhu Yuanzhang (the Hongwu Emperor) founded the Ming dynasty in 1368, the city of Nanjing became the capital city of his empire. The Yangshan quarry became the main source of stone for the major construction projects that changed the face of Nanjing.\nIn 1405, Hongwu's son, the Yongle Emperor, ordered the cutting of a giant stele in this quarry, for use in the Ming Xiaoling Mausoleum of his deceased father. In accordance with the usual design of a Chinese memorial stele, three separate pieces were being cut: the rectangular stele base (pedestal), the stele body, and the stele head (crown, to be decorated with a dragon design). After most of the stone-cutting work had been done, the architects realized that moving stones that big from Yangshan to Ming Xiaoling, let alone installing them there in a proper way, would not be physically possible. As a result, the project was abandoned. In place of the stele, a much smaller tablet (still, the largest in the Nanjing area), known as the \"Shengong Shengde\" (\"Divine Merits and Godly Virtues\") Stele was installed in Ming Xiaoling's \"Square Pavilion\" (Sifangcheng) in 1413.\n\nThe three unfinished stele components remain in Yangshan Quarry to this day, only partially separated from the solid rock of the mountain. The present dimensions and the usual weight estimates of the steles are as follows:\n\nAccording to experts, if the stele had been finished and put together, by installing the stele body vertically on the base, and topping with the stele head, then it would have stood 73 meters tall. For comparison, the \"Shengong Shengde\" Stele actually installed in Ming Xiaoling is 8.78 m tall (6.7 m body + crown, on top of a 2.8 m tall tortoise pedestal). The Song-dynasty (early 12th century) \"Wan Ren Chou\" (\"Ten Thousand Men's Sorrow\") Stele in Qufu, which is thought to be one of the tallest in China, is 16.95 m tall, 3.75 m wide, 1.14 m thick.\n\nAccording to a legend, workers who failed to produce the daily quota of crushed rock of at least 33 \"sheng\" would be executed on the spot. In memory of the workers who died on the construction site—including those who died from overwork and disease—a nearby village became known as Fentou (), or \"Grave Mound\". Ann Paludan translates the place name as \"Death's Head Valley\".\n\nIn the centuries since the giant stele project was abandoned, a number of Ming, Qing, and modern authors visited the site and left accounts of it. The poet Yuan Mei (1716 – 1797) expressed his feelings in \"The Song of Hongwu's Great Stone Tablet\" (), which concludes with \"one hundred thousand camels could not move it!\" (\"\"). The poem is published in his collection \"Xiao Cangshan Fang Wenji\" ().\n\nIn 1956, the Yangshan Quarry was entered on the Jiangsu provincial register of protected cultural monuments. It is maintained as a tourist site, although, according to journalists who visited it at the turn of the 21st century, the site was little known even in Nanjing itself, and had few visitors.\n\nA small theme park called the Ming Culture Village (, \"Míng Wénhuà Cūn\") was constructed at the entrance to the site; as of 2011, it has a stage, children's rides, and various history-themed amusements. A single admission ticket allows one to visit the \"village\" and then to walk some 300–400 m to the quarry proper on one of several forest trails. The site is open year-round, but still is mostly deserted in winter.\n\nTransportation from Nanjing to the Yangshan Quarry (either directly to the Ming Culture Village's entrance plaza, or to the \"Yangshan Quarry\" stop on the Jiangsu Provincial Highway 122 (S122)) is provided by several bus routes, including the Nanjing-Tangshan Line (, \"Nán-Tāng Xiàn\") from the Nanjing Railway Station.\n\n"}
