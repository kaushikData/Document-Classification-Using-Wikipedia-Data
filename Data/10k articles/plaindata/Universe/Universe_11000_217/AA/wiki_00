{"id": "531302", "url": "https://en.wikipedia.org/wiki?curid=531302", "title": "2004 Arkhangelsk explosion", "text": "2004 Arkhangelsk explosion\n\nOn March 16, 2004, an explosion destroyed a corner section of a nine-story Soviet-era apartment building in Arkhangelsk, Russia. It happened at 3:03 a.m. local time (UTC +3). The explosion occurred in 120 Avenue of the Soviet Cosmonauts in the October district (\"Oktyabrskiy rayon\") of Arkhangelsk.\n\nThe death toll from the explosion was 58 (33 women, 16 men and 9 children). At least 2 of the dead died in a hospital after being rescued.\n\nThe explosion came two days after Vladimir Putin won reelection and several weeks after a suicide bombing killed 41 Moscow subway passengers.\n\nIn April 2004, authorities arrested and charged 26-year-old former employee of city gas services Sergey Alekseychik. On December 16, 2005 he was sentenced to 25 years in prison. According to the official version, Alekseychik was fired from his natural gas technician job several days prior to the explosion, and to get even with his former employers and the city he sabotaged the gas system thus causing the tragedy.\n"}
{"id": "23589425", "url": "https://en.wikipedia.org/wiki?curid=23589425", "title": "Agriculture in Concert with the Environment", "text": "Agriculture in Concert with the Environment\n\nAgriculture in Concert with the Environment (ACE) is a program of the United States Environmental Protection Agency (EPA), administered cooperatively with United States Department of Agriculture's Sustainable Agriculture Research and Education (SARE) program, to fund research projects that reduce the risk of pollution from pesticides and soluble fertilizers.\n\nIn 1991, the EPA partnered with the USDA through the Low-Input Sustainable Agriculture Program (LISA), now referred to as the SARE, and allotted a budget of $1,000,000 to the program. The LISA program pledged to match funds received from the EPA dollar for dollar each year.\n\nThe ACE program was created to aid in the prevention and reduction of agricultural pollution. To achieve this, the program created 3 main goals: \n"}
{"id": "1553856", "url": "https://en.wikipedia.org/wiki?curid=1553856", "title": "Aircraft flight mechanics", "text": "Aircraft flight mechanics\n\nFlight mechanics are relevant to fixed wing (gliders, aeroplanes) and rotary wing (helicopters) aircraft. An aeroplane (\"airplane\" in US usage), is defined in ICAO Document 9110 as, \"a power-driven heavier than air aircraft, deriving its lift chiefly from aerodynamic reactions on surface which remain fixed under given conditions of flight\".\n\nA heavier-than-air craft (aircraft) can only fly if a series of aerodynamic forces come to bear. In regard to fixed wing aircraft, the fuselage of the craft holds up the wings before takeoff. At the instant of takeoff, the reverse happens and the wings support the plane in flight.\n\nIn flight an aircraft can be considered as being acted on by four forces: lift, weight, thrust, and drag. \"Thrust\" is the force generated by the engine (whether it be a jet or a propeller driven craft) and acts along the engine's thrust vector for the purpose of overcoming drag. \"Lift\" acts perpendicular to the vector representing the aircraft's velocity relative to the atmosphere. \"Drag\" acts parallel to the aircraft's velocity vector, but in the opposite direction because drag resists motion through the air. \"Weight\" acts through the aircraft's centre of gravity, towards the center of the earth.\n\nIn straight and level flight,(or movement in the air) lift is approximately equal and opposite to weight. In addition, if the aircraft is not accelerating, thrust is equal and opposite to drag.\n\nIn straight climbing flight, lift is less than weight. At first, this seems incorrect because if an aircraft is climbing it seems lift must exceed weight. When an aircraft is climbing at constant speed it is its thrust that enables it to climb and gain extra potential energy. Lift acts perpendicular to the vector representing the velocity of the aircraft relative to the atmosphere, so lift is unable to alter the aircraft's potential energy or kinetic energy. This can be seen by considering an aerobatic aircraft in straight vertical flight—one that is climbing straight upwards (or descending straight downwards). Vertical flight requires no lift. When flying straight upwards the aircraft can reach zero airspeed before falling earthwards—the wing is generating no lift and so does not stall. In straight, climbing flight at constant airspeed, thrust exceeds drag.\n\nIn straight descending flight, lift is less than weight. In addition, if the aircraft is not accelerating, thrust is less than drag. In turning flight, lift exceeds weight and produces a load factor greater than one, determined by the aircraft's angle of bank.\n\nThere are three primary ways for an aircraft to change its orientation relative to the passing air. \"Pitch\" (movement of the nose up or down, rotation around the transversal axis), \"roll\" (rotation around the longitudinal axis, that is, the axis which runs along the length of the aircraft) and \"yaw\" (movement of the nose to left or right, rotation about the vertical axis). Turning the aircraft (change of heading) requires the aircraft firstly to roll to achieve an angle of bank (in order to balance the centrifugal force); when the desired change of heading has been accomplished the aircraft must again be rolled in the opposite direction to reduce the angle of bank to zero. Lift acts vertically up through center of pressure which depends on the position of wings. The position of the centre of pressure which will change with changes in the angle of attack and aircraft wing flaps setting.\n\nYaw is induced by a moveable rudder-fin. The movement of the rudder changes the size and orientation of the force the vertical surface produces. Since the force is created at a distance behind the centre of gravity, this sideways force causes a yawing moment then a yawing motion. On a large aircraft there may be several independent rudders on the single fin for both safety and to control the inter-linked yaw and roll actions.\n\nUsing yaw alone is not a very efficient way of executing a level turn in an aircraft and will result in some sideslip. A precise combination of bank and lift must be generated to cause the required centripetal forces without producing a sideslip.\n\nPitch is controlled by the rear part of the tailplane's horizontal stabilizer being hinged to create an elevator. By moving the elevator control backwards the pilot moves the elevator up (a position of negative camber) and the downwards force on the horizontal tail is increased. The angle of attack on the wings increased so the nose is pitched up and lift is generally increased. In micro-lights and hang gliders the pitch action is reversed—the pitch control system is much simpler so when the pilot moves the elevator control backwards it produces a nose-down pitch and the angle of attack on the wing is reduced.\n\nThe system of a fixed tail surface and moveable elevators is standard in subsonic aircraft. Craft capable of supersonic flight often have a stabilator, an all-moving tail surface. Pitch is changed in this case by moving the entire horizontal surface of the tail. This seemingly simple innovation was one of the key technologies that made supersonic flight possible. In early attempts, as pilots exceeded the critical Mach number, a strange phenomenon made their control surfaces useless, and their aircraft uncontrollable. It was determined that as an aircraft approaches the speed of sound, the air approaching the aircraft is compressed and shock waves begin to form at all the leading edges and around the hinge lines of the elevator. These shock waves caused movements of the elevator to cause no pressure change on the stabilizer upstream of the elevator. The problem was solved by changing the stabilizer and hinged elevator to an all-moving stabilizer—the entire horizontal surface of the tail became a one-piece control surface. Also, in supersonic flight the change in camber has less effect on lift and a stabilator produces less drag.\n\nAircraft that need control at extreme angles of attack are sometimes fitted with a canard configuration, in which pitching movement is created using a forward foreplane (roughly level with the cockpit). Such a system produces an immediate increase in pitch authority, and therefore a better response to pitch controls. This system is common in delta-wing aircraft (deltaplane), which use a stabilator-type canard foreplane. A disadvantage to a canard configuration compared to an aft tail is that the wing cannot use as much extension of flaps to increase wing lift at slow speeds due to stall performance. A combination tri-surface aircraft uses both a canard and an aft tail (in addition to the main wing) to achieve advantages of both configurations.\n\nA further design of tailplane is the V-tail, so named because that instead of the standard inverted T or T-tail, there are two fins angled away from each other in a V. The control surfaces then act both as rudders and elevators, moving in the appropriate direction as needed. \n\nRoll is controlled by movable sections on the trailing edge of the wings called ailerons. The ailerons move in opposition to one another—one goes up as the other goes down. The difference in camber of the wing cause a difference in lift and thus a rolling movement. As well as ailerons, there are sometimes also spoilers—small hinged plates on the upper surface of the wing, originally used to produce drag to slow the aircraft down and to reduce lift when descending. On modern aircraft, which have the benefit of automation, they can be used in combination with the ailerons to provide roll control.\n\nThe earliest powered aircraft built by the Wright brothers did not have ailerons. The whole wing was warped using wires. Wing warping is efficient since there is no discontinuity in the wing geometry. But as speeds increased unintentional warping became a problem and so ailerons were developed.\n\n\n"}
{"id": "55479326", "url": "https://en.wikipedia.org/wiki?curid=55479326", "title": "Aitken Spence Power Station", "text": "Aitken Spence Power Station\n\nThe Aitken Spence Power Station (formerly referred to as the Meethotamulla Power Station) is a municipal solid waste-fired thermal power station currently under construction at Muthurajawela, Sri Lanka. It was originally planned to be built at Meethotamulla, the site of a large solid waste landfill which was under international media spotlight after the 2017 Meethotamulla garbage landslide which killed over 30 people. The power station will operate approximately 7500 hours a year, utilizing the of fresh waste from the Colombo Municipal Council area, daily. The power station in being built together with the KCHT Power Station.\n\nThe facility will generate of power, of which will be sold to the state-owned Ceylon Electricity Board, at a rate of generated. Construction of the power station began on , and is expected to complete by mid-2019. It will be operated by , a subsidiary of Aitken Spence. The power station's PUCSL energy license is EL/GS/13-03.\n\n"}
{"id": "22651205", "url": "https://en.wikipedia.org/wiki?curid=22651205", "title": "Alejandro Jadresic", "text": "Alejandro Jadresic\n\nAlejandro Jadresic Marinovic (born 6 June 1956) is a Chilean industrial engineer, economist and an academic. He served as Minister of Energy (1994–1997) under the government of Eduardo Frei.\n\nJadresic is the son of the psychiatrist, Víctor Jadresic Vargas and the academic Mimi Marinovic. He studied at the \"Liceum A-8\" in Santiago de Chile and was accepted into the University of Chile after getting one of the highest score (812) in the \"Academic Aptitude Exam\" in the country. He graduated with a masters in Industrial Engineering from the University of Chile and subsequently continued his education at Harvard University where he completed a Ph.D. in economics in 1984.\n\nAfter completing his studies in the United States, Jadresic returned to Chile and began to make a living as a teacher. He also became politically active in campaigns that supported an end to the Pinochet regime, nevertheless he always remained an independent from the major anti-Pinochet political parties.\n\nHe is currently Dean of the Faculty of Engineering and Sciences at the Adolfo Ibáñez University, he also is a board member of Entel (Chile’s main telecommunications company) and director of Jadresic Consulting Ltd a business services firm.\n\nJadresic was awarded the \"Best Engineer Award 2005\" from the School of Engineers of Chile and the \"Order of Prince Trpimir\" from the Republic of Croatia.\n\n"}
{"id": "1207", "url": "https://en.wikipedia.org/wiki?curid=1207", "title": "Amino acid", "text": "Amino acid\n\nAmino acids are organic compounds containing amine (-NH) and carboxyl (-COOH) functional groups, along with a side chain (R group) specific to each amino acid. The key elements of an amino acid are carbon (C), hydrogen (H), oxygen (O), and nitrogen (N), although other elements are found in the side chains of certain amino acids. About 500 naturally occurring amino acids are known (though only 20 appear in the genetic code) and can be classified in many ways. They can be classified according to the core structural functional groups' locations as alpha- (α-), beta- (β-), gamma- (γ-) or delta- (δ-) amino acids; other categories relate to polarity, pH level, and side chain group type (aliphatic, acyclic, aromatic, containing hydroxyl or sulfur, etc.). In the form of proteins, amino acid residues form the second-largest component (water is the largest) of human muscles and other tissues. Beyond their role as residues in proteins, amino acids participate in a number of processes such as neurotransmitter transport and biosynthesis.\n\nIn biochemistry, amino acids having both the amine and the carboxylic acid groups attached to the first (alpha-) carbon atom have particular importance. They are known as 2-, alpha-, or α-amino acids (generic formula HNCHRCOOH in most cases, where R is an organic substituent known as a \"side chain\"); often the term \"amino acid\" is used to refer specifically to these. They include the 22 proteinogenic (\"protein-building\") amino acids, which combine into peptide chains (\"polypeptides\") to form the building-blocks of a vast array of proteins. These are all -stereoisomers (\"left-handed\" isomers), although a few -amino acids (\"right-handed\") occur in bacterial envelopes, as a neuromodulator (-serine), and in some antibiotics.\n\nTwenty of the proteinogenic amino acids are encoded directly by triplet codons in the genetic code and are known as \"standard\" amino acids. The other two (\"non-standard\" or \"non-canonical\") are selenocysteine (present in many prokaryotes as well as most eukaryotes, but not coded directly by DNA), and pyrrolysine (found only in some archea and one bacterium). Pyrrolysine and selenocysteine are encoded via variant codons; for example, selenocysteine is encoded by stop codon and SECIS element. \"N\"-formylmethionine (which is often the initial amino acid of proteins in bacteria, mitochondria, and chloroplasts) is generally considered as a form of methionine rather than as a separate proteinogenic amino acid. Codon–tRNA combinations not found in nature can also be used to \"expand\" the genetic code and form novel proteins known as alloproteins incorporating non-proteinogenic amino acids.\n\nMany important proteinogenic and non-proteinogenic amino acids have biological functions. For example, in the human brain, glutamate (standard glutamic acid) and gamma-amino-butyric acid (\"GABA\", non-standard gamma-amino acid) are, respectively, the main excitatory and inhibitory neurotransmitters. Hydroxyproline, a major component of the connective tissue collagen, is synthesised from proline. Glycine is a biosynthetic precursor to porphyrins used in red blood cells. Carnitine is used in lipid transport.\n\nNine proteinogenic amino acids are called \"essential\" for humans because they cannot be produced from other compounds by the human body and so must be taken in as food. Others may be conditionally essential for certain ages or medical conditions. Essential amino acids may also differ between species.\n\nBecause of their biological significance, amino acids are important in nutrition and are commonly used in nutritional supplements, fertilizers, feed, and food technology. Industrial uses include the production of drugs, biodegradable plastics, and chiral catalysts.\n\nThe first few amino acids were discovered in the early 19th century. In 1806, French chemists Louis-Nicolas Vauquelin and Pierre Jean Robiquet isolated a compound in asparagus that was subsequently named asparagine, the first amino acid to be discovered. Cystine was discovered in 1810, although its monomer, cysteine, remained undiscovered until 1884. Glycine and leucine were discovered in 1820. The last of the 20 common amino acids to be discovered was threonine in 1935 by William Cumming Rose, who also determined the essential amino acids and established the minimum daily requirements of all amino acids for optimal growth.\n\nThe unity of the chemical category was recognized by Wurtz in 1865, but he gave no particular name to it. Usage of the term \"amino acid\" in the English language is from 1898, while the German term, \"Aminosäure\", was used earlier. Proteins were found to yield amino acids after enzymatic digestion or acid hydrolysis. In 1902, Emil Fischer and Franz Hofmeister independently proposed that proteins are formed from many amino acids, whereby bonds are formed between the amino group of one amino acid with the carboxyl group of another, resulting in a linear structure that Fischer termed \"peptide\".\n\nIn the structure shown at the top of the page, R represents a side chain specific to each amino acid. The carbon atom next to the carboxyl group (which is therefore numbered 2 in the carbon chain starting from that functional group) is called the α–carbon. Amino acids containing an amino group bonded directly to the alpha carbon are referred to as \"alpha amino acids\". These include amino acids such as proline which contain secondary amines, which used to be often referred to as \"imino acids\".\n\nThe alpha amino acids are the most common form found in nature, but only when occurring in the -isomer. The alpha carbon is a chiral carbon atom, with the exception of glycine which has two indistinguishable hydrogen atoms on the alpha carbon. Therefore, all alpha amino acids but glycine can exist in either of two enantiomers, called or amino acids, which are mirror images of each other (\"see also Chirality\"). While -amino acids represent all of the amino acids found in proteins during translation in the ribosome, -amino acids are found in some proteins produced by enzyme posttranslational modifications after translation and translocation to the endoplasmic reticulum, as in exotic sea-dwelling organisms such as cone snails. They are also abundant components of the peptidoglycan cell walls of bacteria, and -serine may act as a neurotransmitter in the brain. -amino acids are used in racemic crystallography to create centrosymmetric crystals, which (depending on the protein) may allow for easier and more robust protein structure determination. The and convention for amino acid configuration refers not to the optical activity of the amino acid itself but rather to the optical activity of the isomer of glyceraldehyde from which that amino acid can, in theory, be synthesized (-glyceraldehyde is dextrorotatory; -glyceraldehyde is levorotatory).\nIn alternative fashion, the \"(S)\" and \"(R)\" designators are used to indicate the absolute stereochemistry. Almost all of the amino acids in proteins are \"(S)\" at the α carbon, with cysteine being \"(R)\" and glycine non-chiral. Cysteine has its side chain in the same geometric position as the other amino acids, but the \"R/S\" terminology is reversed because of the higher atomic number of sulfur compared to the carboxyl oxygen gives the side chain a higher priority, whereas the atoms in most other side chains give them lower priority.\n\nIn amino acids that have a carbon chain attached to the α–carbon (such as lysine, shown to the right) the carbons are labeled in order as α, β, γ, δ, and so on. In some amino acids, the amine group is attached to the β or γ-carbon, and these are therefore referred to as \"beta\" or \"gamma amino acids\".\n\nAmino acids are usually classified by the properties of their side chain into four groups. The side chain can make an amino acid a weak acid or a weak base, and a hydrophile if the side chain is polar or a hydrophobe if it is nonpolar. The chemical structures of the 22 standard amino acids, along with their chemical properties, are described more fully in the article on these proteinogenic amino acids.\n\nThe phrase \"branched-chain amino acids\" or BCAA refers to the amino acids having aliphatic side chains that are non-linear; these are leucine, isoleucine, and valine. Proline is the only proteinogenic amino acid whose side-group links to the α-amino group and, thus, is also the only proteinogenic amino acid containing a secondary amine at this position. In chemical terms, proline is, therefore, an imino acid, since it lacks a primary amino group, although it is still classed as an amino acid in the current biochemical nomenclature, and may also be called an \"N-alkylated alpha-amino acid\".\n\nThe α-carboxylic acid group of amino acids is a weak acid, meaning that it releases a hydron (such as a proton) at moderate pH values. In other words, carboxylic acid groups (−COH) can be deprotonated to become negative carboxylates (−CO ). The negatively charged carboxylate ion predominates at pH values greater than the pKa of the carboxylic acid group (mean for the 20 common amino acids is about 2.2, see the table of amino acid structures above). In a complementary fashion, the α-amine of amino acids is a weak base, meaning that it accepts a proton at moderate pH values. In other words, α-amino groups (NH−) can be protonated to become positive α-ammonium groups (NH−). The positively charged α-ammonium group predominates at pH values less than the pKa of the α-ammonium group (mean for the 20 common α-amino acids is about 9.4).\n\nBecause all amino acids contain amine and carboxylic acid functional groups, they share amphiprotic properties. Below pH 2.2, the predominant form will have a neutral carboxylic acid group and a positive α-ammonium ion (net charge +1), and above pH 9.4, a negative carboxylate and neutral α-amino group (net charge −1). But at pH between 2.2 and 9.4, an amino acid usually contains both a negative carboxylate and a positive α-ammonium group, as shown in structure (2) on the right, so has net zero charge. This molecular state is known as a zwitterion, from the German Zwitter meaning \"hermaphrodite\" or \"hybrid\". The fully neutral form (structure (1) on the left) is a very minor species in aqueous solution throughout the pH range (less than 1 part in 10). Amino acids exist as zwitterions also in the solid phase, and crystallize with salt-like properties unlike typical organic acids or amines.\n\nThe variation in titration curves when the amino acids can be grouped by category. With the exception of tyrosine, using titration to distinguish among hydrophobic amino acids is problematic.\n\nAt pH values between the two pKa values, the zwitterion predominates, but coexists in dynamic equilibrium with small amounts of net negative and net positive ions. At the exact midpoint between the two pKa values, the trace amount of net negative and trace of net positive ions exactly balance, so that average net charge of all forms present is zero. This pH is known as the isoelectric point pI, so pI = ½(pKa + pKa). The individual amino acids all have slightly different pKa values, so have different isoelectric points. For amino acids with charged side chains, the pKa of the side chain is involved. Thus for Asp, Glu with negative side chains, pI = ½(pKa + pKa), where pKa is the side chain pKa. Cysteine also has potentially negative side chain with pKa = 8.14, so pI should be calculated as for Asp and Glu, even though the side chain is not significantly charged at neutral pH. For His, Lys, and Arg with positive side chains, pI = ½(pKa + pKa). Amino acids have zero mobility in electrophoresis at their isoelectric point, although this behaviour is more usually exploited for peptides and proteins than single amino acids. Zwitterions have minimum solubility at their isoelectric point and some amino acids (in particular, with non-polar side chains) can be isolated by precipitation from water by adjusting the pH to the required isoelectric point.\n\nAmino acids are the structural units (monomers) that make up proteins. They join together to form short polymer chains called peptides or longer chains called either polypeptides or proteins. These polymers are linear and unbranched, with each amino acid within the chain attached to two neighboring amino acids. The process of making proteins encoded by DNA/RNA genetic material is called \"translation\" and involves the step-by-step addition of amino acids to a growing protein chain by a ribozyme that is called a ribosome. The order in which the amino acids are added is read through the genetic code from an mRNA template, which is an RNA copy of one of the organism's genes.\n\nTwenty-two amino acids are naturally incorporated into polypeptides and are called proteinogenic or natural amino acids. Of these, 20 are encoded by the universal genetic code. The remaining 2, selenocysteine and pyrrolysine, are incorporated into proteins by unique synthetic mechanisms. Selenocysteine is incorporated when the mRNA being translated includes a SECIS element, which causes the UGA codon to encode selenocysteine instead of a stop codon. Pyrrolysine is used by some methanogenic archaea in enzymes that they use to produce methane. It is coded for with the codon UAG, which is normally a stop codon in other organisms. This UAG codon is followed by a PYLIS downstream sequence.\n\nAside from the 22 proteinogenic amino acids, many \"non-proteinogenic\" amino acids are known. Those either are not found in proteins (for example carnitine, GABA, levothyroxine) or are not produced directly and in isolation by standard cellular machinery (for example, hydroxyproline and selenomethionine).\n\nNon-proteinogenic amino acids that are found in proteins are formed by post-translational modification, which is modification after translation during protein synthesis. These modifications are often essential for the function or regulation of a protein. For example, the carboxylation of glutamate allows for better binding of calcium cations, and collagen contains hydroxyproline, generated by hydroxylation of proline. Another example is the formation of hypusine in the translation initiation factor EIF5A, through modification of a lysine residue. Such modifications can also determine the localization of the protein, e.g., the addition of long hydrophobic groups can cause a protein to bind to a phospholipid membrane.\n\nSome non-proteinogenic amino acids are not found in proteins. Examples include 2-aminoisobutyric acid and the neurotransmitter gamma-aminobutyric acid. Non-proteinogenic amino acids often occur as intermediates in the metabolic pathways for standard amino acids – for example, ornithine and citrulline occur in the urea cycle, part of amino acid catabolism (see below). A rare exception to the dominance of α-amino acids in biology is the β-amino acid beta alanine (3-aminopropanoic acid), which is used in plants and microorganisms in the synthesis of pantothenic acid (vitamin B), a component of coenzyme A.\n\n-isomers are uncommon in live organisms. For instance, gramicidin is a polypeptide made up from mixture of - and -amino acids. Other compounds containing -amino acid are tyrocidine and valinomycin. These compounds disrupt bacterial cell walls, particularly in Gram-positive bacteria. Only 837 -amino acids were found in Swiss-Prot database (187 million amino acids analysed).\n\nThe 20 amino acids that are encoded directly by the codons of the universal genetic code are called \"standard\" or \"canonical\" amino acids. A modified form of methionine (\"N\"-formylmethionine) is often incorporated in place of methionine as the initial amino acid of proteins in bacteria, mitochondria and chloroplasts. Other amino acids are called \"non-standard\" or \"non-canonical\". Most of the non-standard amino acids are also non-proteinogenic (i.e. they cannot be incorporated into proteins during translation), but two of them are proteinogenic, as they can be incorporated translationally into proteins by exploiting information not encoded in the universal genetic code.\n\nThe two non-standard proteinogenic amino acids are selenocysteine (present in many non-eukaryotes as well as most eukaryotes, but not coded directly by DNA) and pyrrolysine (found only in some archaea and one bacterium). The incorporation of these non-standard amino acids is rare. For example, 25 human proteins include selenocysteine (Sec) in their primary structure, and the structurally characterized enzymes (selenoenzymes) employ Sec as the catalytic moiety in their active sites. Pyrrolysine and selenocysteine are encoded via variant codons. For example, selenocysteine is encoded by stop codon and SECIS element.\n\nWhen taken up into the human body from the diet, the 20 standard amino acids either are used to synthesize proteins and other biomolecules or are oxidized to urea and carbon dioxide as a source of energy. The oxidation pathway starts with the removal of the amino group by a transaminase; the amino group is then fed into the urea cycle. The other product of transamidation is a keto acid that enters the citric acid cycle. Glucogenic amino acids can also be converted into glucose, through gluconeogenesis. Of the 20 standard amino acids, nine (His, Ile, Leu, Lys, Met, Phe, Thr, Trp and Val) are called essential amino acids because the human body cannot synthesize them from other compounds at the level needed for normal growth, so they must be obtained from food. In addition, cysteine, taurine, tyrosine, and arginine are considered semiessential amino-acids in children (though taurine is not technically an amino acid), because the metabolic pathways that synthesize these amino acids are not fully developed. The amounts required also depend on the age and health of the individual, so it is hard to make general statements about the dietary requirement for some amino acids. Dietary exposure to the non-standard amino acid BMAA has been linked to human neurodegenerative diseases, including ALS.\n\nIn humans, non-protein amino acids also have important roles as metabolic intermediates, such as in the biosynthesis of the neurotransmitter gamma-amino-butyric acid (GABA). Many amino acids are used to synthesize other molecules, for example:\n\nSome non-standard amino acids are used as defenses against herbivores in plants. For example, canavanine is an analogue of arginine that is found in many legumes, and in particularly large amounts in \"Canavalia gladiata\" (sword bean). This amino acid protects the plants from predators such as insects and can cause illness in people if some types of legumes are eaten without processing. The non-protein amino acid mimosine is found in other species of legume, in particular \"Leucaena leucocephala\". This compound is an analogue of tyrosine and can poison animals that graze on these plants.\n\nAmino acids are used for a variety of applications in industry, but their main use is as additives to animal feed. This is necessary, since many of the bulk components of these feeds, such as soybeans, either have low levels or lack some of the essential amino acids: lysine, methionine, threonine, and tryptophan are most important in the production of these feeds. In this industry, amino acids are also used to chelate metal cations in order to improve the absorption of minerals from supplements, which may be required to improve the health or production of these animals.\n\nThe food industry is also a major consumer of amino acids, in particular, glutamic acid, which is used as a flavor enhancer, and aspartame (aspartyl-phenylalanine-1-methyl ester) as a low-calorie artificial sweetener. Similar technology to that used for animal nutrition is employed in the human nutrition industry to alleviate symptoms of mineral deficiencies, such as anemia, by improving mineral absorption and reducing negative side effects from inorganic mineral supplementation.\n\nThe chelating ability of amino acids has been used in fertilizers for agriculture to facilitate the delivery of minerals to plants in order to correct mineral deficiencies, such as iron chlorosis. These fertilizers are also used to prevent deficiencies from occurring and improving the overall health of the plants. The remaining production of amino acids is used in the synthesis of drugs and cosmetics.\n\nSimilarly, some amino acids derivatives are used in pharmaceutical industry. They include 5-HTP (5-hydroxytryptophan) used for experimental treatment of depression, -DOPA (-dihydroxyphenylalanine) for Parkinson's treatment, and eflornithine drug that inhibits ornithine decarboxylase and used in the treatment of sleeping sickness.\n\nSince 2001, 40 non-natural amino acids have been added into protein by creating a unique codon (recoding) and a corresponding transfer-RNA:aminoacyl – tRNA-synthetase pair to encode it with diverse physicochemical and biological properties in order to be used as a tool to exploring protein structure and function or to create novel or enhanced proteins.\n\nNullomers are codons that in theory code for an amino acid, however in nature there is a selective bias against using this codon in favor of another, for example bacteria prefer to use CGA instead of AGA to code for arginine. This creates some sequences that do not appear in the genome. This characteristic can be taken advantage of and used to create new selective cancer-fighting drugs and to prevent cross-contamination of DNA samples from crime-scene investigations.\n\nAmino acids are important as low-cost feedstocks. These compounds are used in chiral pool synthesis as enantiomerically pure building-blocks.\n\nAmino acids have been investigated as precursors chiral catalysts, e.g., for asymmetric hydrogenation reactions, although no commercial applications exist.\n\nAmino acids are under development as components of a range of biodegradable polymers. These materials have applications as environmentally friendly packaging and in medicine in drug delivery and the construction of prosthetic implants. These polymers include polypeptides, polyamides, polyesters, polysulfides, and polyurethanes with amino acids either forming part of their main chains or bonded as side chains. These modifications alter the physical properties and reactivities of the polymers. An interesting example of such materials is polyaspartate, a water-soluble biodegradable polymer that may have applications in disposable diapers and agriculture. Due to its solubility and ability to chelate metal ions, polyaspartate is also being used as a biodegradeable anti-scaling agent and a corrosion inhibitor. In addition, the aromatic amino acid tyrosine is being developed as a possible replacement for toxic phenols such as bisphenol A in the manufacture of polycarbonates.\n\nAs amino acids have both a primary amine group and a primary carboxyl group, these chemicals can undergo most of the reactions associated with these functional groups. These include nucleophilic addition, amide bond formation, and imine formation for the amine group, and esterification, amide bond formation, and decarboxylation for the carboxylic acid group. The combination of these functional groups allow amino acids to be effective polydentate ligands for metal-amino acid chelates.\nThe multiple side chains of amino acids can also undergo chemical reactions. The types of these reactions are determined by the groups on these side chains and are, therefore, different between the various types of amino acid.\n\nSeveral methods exist to synthesize amino acids. One of the oldest methods begins with the bromination at the α-carbon of a carboxylic acid. Nucleophilic substitution with ammonia then converts the alkyl bromide to the amino acid. In alternative fashion, the Strecker amino acid synthesis involves the treatment of an aldehyde with potassium cyanide and ammonia, this produces an α-amino nitrile as an intermediate. Hydrolysis of the nitrile in acid then yields an α-amino acid. Using ammonia or ammonium salts in this reaction gives unsubstituted amino acids, whereas substituting primary and secondary amines will yield substituted amino acids. Likewise, using ketones, instead of aldehydes, gives α,α-disubstituted amino acids. The classical synthesis gives racemic mixtures of α-amino acids as products, but several alternative procedures using asymmetric auxiliaries or asymmetric catalysts have been developed.\n\nAt the current time, the most-adopted method is an automated synthesis on a solid support (e.g., polystyrene beads), using protecting groups (e.g., Fmoc and t-Boc) and activating groups (e.g., DCC and DIC).\n\nAs both the amine and carboxylic acid groups of amino acids can react to form amide bonds, one amino acid molecule can react with another and become joined through an amide linkage. This polymerization of amino acids is what creates proteins. This condensation reaction yields the newly formed peptide bond and a molecule of water. In cells, this reaction does not occur directly; instead, the amino acid is first activated by attachment to a transfer RNA molecule through an ester bond. This aminoacyl-tRNA is produced in an ATP-dependent reaction carried out by an aminoacyl tRNA synthetase. This aminoacyl-tRNA is then a substrate for the ribosome, which catalyzes the attack of the amino group of the elongating protein chain on the ester bond. As a result of this mechanism, all proteins made by ribosomes are synthesized starting at their N-terminus and moving toward their C-terminus.\n\nHowever, not all peptide bonds are formed in this way. In a few cases, peptides are synthesized by specific enzymes. For example, the tripeptide glutathione is an essential part of the defenses of cells against oxidative stress. This peptide is synthesized in two steps from free amino acids. In the first step, gamma-glutamylcysteine synthetase condenses cysteine and glutamic acid through a peptide bond formed between the side chain carboxyl of the glutamate (the gamma carbon of this side chain) and the amino group of the cysteine. This dipeptide is then condensed with glycine by glutathione synthetase to form glutathione.\n\nIn chemistry, peptides are synthesized by a variety of reactions. One of the most-used in solid-phase peptide synthesis uses the aromatic oxime derivatives of amino acids as activated units. These are added in sequence onto the growing peptide chain, which is attached to a solid resin support. The ability to easily synthesize vast numbers of different peptides by varying the types and order of amino acids (using combinatorial chemistry) has made peptide synthesis particularly important in creating libraries of peptides for use in drug discovery through high-throughput screening.\n\nIn plants, nitrogen is first assimilated into organic compounds in the form of glutamate, formed from alpha-ketoglutarate and ammonia in the mitochondrion. In order to form other amino acids, the plant uses transaminases to move the amino group to another alpha-keto carboxylic acid. For example, aspartate aminotransferase converts glutamate and oxaloacetate to alpha-ketoglutarate and aspartate. Other organisms use transaminases for amino acid synthesis, too.\n\nNonstandard amino acids are usually formed through modifications to standard amino acids. For example, homocysteine is formed through the transsulfuration pathway or by the demethylation of methionine via the intermediate metabolite S-adenosyl methionine, while hydroxyproline is made by a posttranslational modification of proline.\n\nMicroorganisms and plants can synthesize many uncommon amino acids. For example, some microbes make 2-aminoisobutyric acid and lanthionine, which is a sulfide-bridged derivative of alanine. Both of these amino acids are found in peptidic lantibiotics such as alamethicin. However, in plants, 1-aminocyclopropane-1-carboxylic acid is a small disubstituted cyclic amino acid that is a key intermediate in the production of the plant hormone ethylene.\n\nAmino acids must first pass out of organelles and cells into blood circulation via amino acid transporters, since the amine and carboxylic acid groups are typically ionized. Degradation of an amino acid, occurring in the liver and kidneys, often involves deamination by moving its amino group to alpha-ketoglutarate, forming glutamate. This process involves transaminases, often the same as those used in amination during synthesis. In many vertebrates, the amino group is then removed through the urea cycle and is excreted in the form of urea. However, amino acid degradation can produce uric acid or ammonia instead. For example, serine dehydratase converts serine to pyruvate and ammonia. After removal of one or more amino groups, the remainder of the molecule can sometimes be used to synthesize new amino acids, or it can be used for energy by entering glycolysis or the citric acid cycle, as detailed in image at right.\n\nThe 20 amino acids encoded directly by the genetic code can be divided into several groups based on their properties. Important factors are charge, hydrophilicity or hydrophobicity, size, and functional groups. These properties are important for protein structure and protein–protein interactions. The water-soluble proteins tend to have their hydrophobic residues (Leu, Ile, Val, Phe, and Trp) buried in the middle of the protein, whereas hydrophilic side chains are exposed to the aqueous solvent. (Note that in biochemistry, a residue refers to a specific monomer within the polymeric chain of a polysaccharide, protein or nucleic acid.) The integral membrane proteins tend to have outer rings of exposed hydrophobic amino acids that anchor them into the lipid bilayer. In the case part-way between these two extremes, some peripheral membrane proteins have a patch of hydrophobic amino acids on their surface that locks onto the membrane. In similar fashion, proteins that have to bind to positively charged molecules have surfaces rich with negatively charged amino acids like glutamate and aspartate, while proteins binding to negatively charged molecules have surfaces rich with positively charged chains like lysine and arginine. There are different hydrophobicity scales of amino acid residues.\n\nSome amino acids have special properties such as cysteine, that can form covalent disulfide bonds to other cysteine residues, proline that forms a cycle to the polypeptide backbone, and glycine that is more flexible than other amino acids.\n\nMany proteins undergo a range of posttranslational modifications, when additional chemical groups are attached to the amino acids in proteins. Some modifications can produce hydrophobic lipoproteins, or hydrophilic glycoproteins. These type of modification allow the reversible targeting of a protein to a membrane. For example, the addition and removal of the fatty acid palmitic acid to cysteine residues in some signaling proteins causes the proteins to attach and then detach from cell membranes.\n\nTwo additional amino acids are in some species coded for by codons that are usually interpreted as stop codons:\n\nIn addition to the specific amino acid codes, placeholders are used in cases where chemical or crystallographic analysis of a peptide or protein cannot conclusively determine the identity of a residue. They are also used to summarise conserved protein sequence motifs. The use of single letters to indicate sets of similar residues is similar to the use of abbreviation codes for degenerate bases.\n\nUnk is sometimes used instead of Xaa, but is less standard.\n\nIn addition, many non-standard amino acids have a specific code. For example, several peptide drugs, such as Bortezomib and MG132, are artificially synthesized and retain their protecting groups, which have specific codes. Bortezomib is Pyz-Phe-boroLeu, and MG132 is Z-Leu-Leu-Leu-al. To aid in the analysis of protein structure, photo-reactive amino acid analogs are available. These include photoleucine (pLeu) and photomethionine (pMet).\n"}
{"id": "20144524", "url": "https://en.wikipedia.org/wiki?curid=20144524", "title": "Anaerobic corrosion", "text": "Anaerobic corrosion\n\nHydrogen corrosion is a form of metal corrosion occurring in the presence of anoxic water. Hydrogen corrosion involves a redox reaction that reduces hydrogen ions, forming molecular hydrogen. \n\nMetals enter aqueous solution and are oxidized.\n\n\"Oxidation reaction (pH independent):\"\n\n\"Reduction reaction in acid solution:\"\n\nIn an acidic solution, the water molecules are protonated and the hydronium ions (HO) are directly reduced into H.\n\n\"Reduction reaction in neutral or slightly alkaline solution:\"\n\nIn a neutral or slightly alkaline solution, the protons of water are reduced into molecular hydrogen giving rise to the production of hydroxide ions responsible of the precipitation of the slightly soluble ferrous hydroxide (Fe(OH)).\n\nThis finally leads to the global reaction of the anaerobic corrosion of iron in water:\n\nUnder anaerobic conditions, the ferrous hydroxide (Fe(OH) ) can be oxidized by the protons of water to form magnetite and molecular hydrogen.\nThis process is described by the Schikorr reaction:\n\nThe well crystallized magnetite (FeO) is thermodynamically more stable than the ferrous hydroxide (Fe(OH) ).\n\nThis process also occurs during the anaerobic corrosion of iron and steel in oxygen-free groundwater and in reducing soils below the water table.\n\n"}
{"id": "1430539", "url": "https://en.wikipedia.org/wiki?curid=1430539", "title": "Artificial photosynthesis", "text": "Artificial photosynthesis\n\nArtificial photosynthesis is a chemical process that replicates the natural process of photosynthesis, a process that converts sunlight, water, and carbon dioxide into carbohydrates and oxygen; as an imitation of a natural process it is biomimetic. The term, artificial photosynthesis, is commonly used to refer to any scheme for capturing and storing the energy from sunlight in the chemical bonds of a fuel (a solar fuel). Photocatalytic water splitting converts water into hydrogen and oxygen, and is a major research topic of artificial photosynthesis. Light-driven carbon dioxide reduction is another process studied, that replicates natural carbon fixation.\n\nResearch of this topic includes the design and assembly of devices for the direct production of solar fuels, photoelectrochemistry and its application in fuel cells, and the engineering of enzymes and photoautotrophic microorganisms for microbial biofuel and biohydrogen production from sunlight.\n\nThe photosynthetic reaction can be divided into two half-reactions of oxidation and reduction, both of which are essential to producing fuel. In plant photosynthesis, water molecules are photo-oxidized to release oxygen and protons. The second phase of plant photosynthesis (also known as the Calvin-Benson cycle) is a light-independent reaction that converts carbon dioxide into glucose (fuel). Researchers of artificial photosynthesis are developing photocatalysts that are able to perform both of these reactions. Furthermore, the protons resulting from water splitting can be used for hydrogen production. These catalysts must be able to react quickly and absorb a large percentage of the incident solar photons.\n\nWhereas photovoltaics can provide energy directly from sunlight, the inefficiency of fuel production from photovoltaic electricity (indirect process) and the fact that sunshine is not constant throughout the day sets a limit to its use. One way of using natural photosynthesis is for the production of a biofuel, which is an indirect process that suffers from low energy conversion efficiency (due to photosynthesis' own low efficiency in converting sunlight to biomass), the cost of harvesting and transporting the fuel, and conflicts due to the increasing need of land mass for food production. The purpose of artificial photosynthesis is to produce a fuel from sunlight that can be stored conveniently and used when sunlight is not available, by using direct processes, that is, to produce a solar fuel. With the development of catalysts able to reproduce the major parts of photosynthesis, water and sunlight would ultimately be the only needed sources for clean energy production. The only by-product would be oxygen, and production of a solar fuel has the potential to be cheaper than gasoline.\n\nOne process for the creation of a clean and affordable energy supply is the development of photocatalytic water splitting under solar light. This method of sustainable hydrogen production is a major objective for the development of alternative energy systems. It is also predicted to be one of the more, if not the most, efficient ways of obtaining hydrogen from water. The conversion of solar energy into hydrogen via a water-splitting process assisted by photosemiconductor catalysts is one of the most promising technologies in development. This process has the potential for large quantities of hydrogen to be generated in an ecologically sound manner. The conversion of solar energy into a clean fuel (H) under ambient conditions is one of the greatest challenges facing scientists in the twenty-first century.\n\nTwo methods are generally recognized for the construction of solar fuel cells for hydrogen production:\n\n\nAnother area of research within artificial photosynthesis is the selection and manipulation of photosynthetic microorganisms, namely green microalgae and cyanobacteria, for the production of solar fuels. Many strains are able to produce hydrogen naturally, and scientists are working to improve them. Algae biofuels such as butanol and methanol are produced both at laboratory and commercial scales. This method has benefited from the development of synthetic biology, which is also being explored by the J. Craig Venter Institute to produce a synthetic organism capable of biofuel production. In 2017, an efficient process was developed to produce acetic acid from carbon dioxide using \"cyborg bacteria\".\n\nArtificial photosynthesis was first anticipated by the Italian chemist Giacomo Ciamician during 1912. In a lecture that was it later published in Science he proposed a switch from the use of fossil fuels to radiant energy provided by the sun and captured by technical photochemistry devices. In this switch he saw a possibility to lessen the difference between the rich north of Europe and poor south and ventured a guess that this switch from coal to solar energy would \"not be harmful to the progress and to human happiness.\"\n\nDuring the late 1960s, Akira Fujishima discovered the photocatalytic properties of titanium dioxide, the so-called Honda-Fujishima effect, which could be used for hydrolysis.\n\nThe Swedish Consortium for Artificial Photosynthesis, the first of its kind, was established during 1994 as a collaboration between groups of three different universities, Lund, Uppsala and Stockholm, being presently active around Lund and the Ångström Laboratories in Uppsala. The consortium was built with a multidisciplinary approach to focus on learning from natural photosynthesis and applying this knowledge in biomimetic systems.\n\nResearch of artificial photosynthesis is experiencing a boom at the beginning of the 21st century. During 2000, Commonwealth Scientific and Industrial Research Organisation (CSIRO) researchers publicized their intent to emphasize carbon dioxide capture and its conversion to hydrocarbons. In 2003, the Brookhaven National Laboratory announced the discovery of an important intermediate part of the reduction of CO to CO (the simplest possible carbon dioxide reduction reaction), which could result in better catalysts.\n\nOne of the disadvantages of artificial systems for water-splitting catalysts is their general reliance on scarce, expensive elements, such as ruthenium or rhenium. During 2008, with the funding of the United States Air Force Office of Scientific Research, MIT chemist and director of the Solar Revolution Project Daniel G. Nocera and postdoctoral fellow Matthew Kanan attempted to circumvent this problem by using a catalyst containing the cheaper and more abundant elements cobalt and phosphate. The catalyst was able to split water into oxygen and protons using sunlight, and could potentially be coupled to a hydrogen gas producing catalyst such as platinum. Furthermore, while the catalyst broke down during catalysis, it could self-repair. This experimental catalyst design was considered a major improvement by many researchers.\n\nWhereas CO is the prime reduction product of CO, more complex carbon compounds are usually desired. During 2008, Andrew B. Bocarsly reported the direct conversion of carbon dioxide and water to methanol using solar energy in a very efficient photochemical cell.\n\nWhile Nocera and coworkers had accomplished water splitting to oxygen and protons, a light-driven process to produce hydrogen is desirable. During 2009, the Leibniz Institute for Catalysis reported inexpensive iron carbonyl complexes able to do just that. During the same year, researchers at the University of East Anglia also used iron carbonyl compounds to achieve photoelectrochemical hydrogen production with 60% efficiency, this time using a gold electrode covered with layers of indium phosphide to which the iron complexes were linked. Both of these processes used a molecular approach, where discrete nanoparticles are responsible for catalysis.\n\nVisible light water splitting with a one piece multijunction cell was first demonstrated and patented by William Ayers at Energy Conversion Devices during 1983. This group demonstrated water photolysis into hydrogen and oxygen, now referred to as an \"artificial leaf\" or \"wireless solar water splitting\" with a low cost, thin film amorphous silicon multijunction cell immersed directly in water. Hydrogen evolved on the front amorphous silicon surface decorated with various catalysts while oxygen evolved from the back metal substrate which also eliminated the hazard of mixed hydrogen/oxygen gas evolution. A Nafion membrane above the immersed cell provided a path for proton transport. The higher photovoltage available from the multijuction thin film cell with visible light was a major advance over previous photolysis attempts with UV sensitive single junction cells. The group's patent also lists several other semiconductor multijunction compositions in addition to amorphous silicon.\n\nDuring 2009, F. del Valle and K. Domen showed the effect of the thermal treatment in a closed atmosphere using photocatalysts. solid solution reports high activity in hydrogen production from water splitting under sunlight irradiation. A mixed heterogeneous/molecular approach by researchers at the University of California, Santa Cruz, during 2010, using both nitrogen-doped and cadmium selenide quantum dots-sensitized titanium dioxide nanoparticles and nanowires, also yielded photoproduced hydrogen.\n\nArtificial photosynthesis remained an academic field for many years. However, in the beginning of 2009, Mitsubishi Chemical Holdings was reported to be developing its own artificial photosynthesis research by using sunlight, water and carbon dioxide to \"create the carbon building blocks from which resins, plastics and fibers can be synthesized.\" This was confirmed with the establishment of the KAITEKI Institute later that year, with carbon dioxide reduction through artificial photosynthesis as one of the main goals.\n\nDuring 2010, the United States Department of Energy established, as one of its Energy Innovation Hubs, the Joint Center for Artificial Photosynthesis. The mission of JCAP is to find a cost-effective method to produce fuels using only sunlight, water, and carbon-dioxide as inputs.  JCAP is managed by a team from Caltech, directed by Professor Nathan Lewis and brings together more than 120 scientists and engineers from California Institute of Technology and its main partner, Lawrence Berkeley National Laboratory. JCAP also draws on the expertise and capabilities of key partners from Stanford University, the University of California at Berkeley, UCSB, University of California, Irvine, and University of California at San Diego, and the Stanford Linear Accelerator.  Additionally, JCAP serves as a central hub for other solar fuels research teams across the United States, including 20 DOE Energy Frontier Research Center.  The program has a budget of $122M over five years, subject to Congressional appropriation\n\nAlso during 2010, a team directed by professor David Wendell at the University of Cincinnati successfully demonstrated photosynthesis in an artificial construct consisting of enzymes suspended in a foam housing.\n\nDuring 2011, Daniel Nocera and his research team announced the creation of the first practical artificial leaf. In a speech at the 241st National Meeting of the American Chemical Society, Nocera described an advanced solar cell the size of a poker card capable of splitting water into oxygen and hydrogen, approximately ten times more efficient than natural photosynthesis. The cell is mostly made of inexpensive materials that are widely available, works under simple conditions, and shows increased stability over previous catalysts: in laboratory studies, the authors demonstrated that an artificial leaf prototype could operate continuously for at least forty-five hours without a drop in activity. In May 2012, Sun Catalytix, the startup based on Nocera's research, stated that it will not be scaling up the prototype as the device offers few savings over other ways to make hydrogen from sunlight. Leading experts in the field have supported a proposal for a Global Project on Artificial Photosynthesis as a combined energy security and climate change solution. Conferences on this theme have been held at Lord Howe Island during 2011, at Chicheley Hall in the UK in 2014 and at Canberra and Lord Howe island during 2016.\n\nIn energy terms, natural photosynthesis can be divided in three steps:\n\nUsing biomimetic approaches, artificial photosynthesis tries to construct systems doing the same type of processes. Ideally, a triad assembly could oxidize water with one catalyst, reduce protons with another and have a photosensitizer molecule to power the whole system. One of the simplest designs is where the photosensitizer is linked in tandem between a water oxidation catalyst and a hydrogen evolving catalyst:\nThe state of the triad with one catalyst oxidized on one end and the second one reduced on the other end of the triad is referred to as a charge separation, and is a driving force for further electron transfer, and consequently catalysis, to occur. The different components may be assembled in diverse ways, such as supramolecular complexes, compartmentalized cells, or linearly, covalently linked molecules.\n\nResearch into finding catalysts that can convert water, carbon dioxide, and sunlight to carbohydrates or hydrogen is a current, active field. By studying the natural oxygen-evolving complex (OEC), researchers have developed catalysts such as the \"blue dimer\" to mimic its function or inorganic-based materials such as Birnessite with the similar building block as the OEC. \nPhotoelectrochemical cells that reduce carbon dioxide into carbon monoxide (CO), formic acid (HCOOH) and methanol (CHOH) are under development. However, these catalysts are still very inefficient.\n\nHydrogen is the simplest solar fuel to synthesize, since it involves only the transference of two electrons to two protons. It must, however, be done stepwise, with formation of an intermediate hydride anion:\nThe proton-to-hydrogen converting catalysts present in nature are hydrogenases. These are enzymes that can either reduce protons to molecular hydrogen or oxidize hydrogen to protons and electrons. Spectroscopic and crystallographic studies spanning several decades have resulted in a good understanding of both the structure and mechanism of hydrogenase catalysis. Using this information, several molecules mimicking the structure of the active site of both nickel-iron and iron-iron hydrogenases have been synthesized. Other catalysts are not structural mimics of hydrogenase but rather functional ones. Synthesized catalysts include structural H-cluster models, a dirhodium photocatalyst, and cobalt catalysts.\n\nWater oxidation is a more complex chemical reaction than proton reduction. In nature, the oxygen-evolving complex performs this reaction by accumulating reducing equivalents (electrons) in a manganese-calcium cluster within photosystem II (PS II), then delivering them to water molecules, with the resulting production of molecular oxygen and protons:\n\nWithout a catalyst (natural or artificial), this reaction is very endothermic, requiring high temperatures (at least 2500 K).\n\nThe exact structure of the oxygen-evolving complex has been hard to determine experimentally. As of 2011, the most detailed model was from a 1.9 Å resolution crystal structure of photosystem II. The complex is a cluster containing four manganese and one calcium ions, but the exact location and mechanism of water oxidation within the cluster is unknown. Nevertheless, bio-inspired manganese and manganese-calcium complexes have been synthesized, such as [MnO] cubane-type clusters, some with catalytic activity.\n\nSome ruthenium complexes, such as the dinuclear µ-oxo-bridged \"blue dimer\" (the first of its kind to be synthesized), are capable of light-driven water oxidation, thanks to being able to form high valence states. In this case, the ruthenium complex acts as both photosensitizer and catalyst.\n\nMany metal oxides have been found to have water oxidation catalytic activity, including ruthenium(IV) oxide (RuO), iridium(IV) oxide (IrO), cobalt oxides (including nickel-doped CoO), manganese oxide (including layered MnO (birnessite), MnO), and a mix of MnO with CaMnO. Oxides are easier to obtain than molecular catalysts, especially those from relatively abundant transition metals (cobalt and manganese), but suffer from low turnover frequency and slow electron transfer properties, and their mechanism of action is hard to decipher and, therefore, to adjust.\n\nRecently Metal-Organic Framework (MOF)-based materials have been shown to be a highly promising candidate for water oxidation with first row transition metals. The stability and tunability of this system is projected to be highly beneficial for future development.\n\nNature uses pigments, mainly chlorophylls, to absorb a broad part of the visible spectrum. Artificial systems can use either one type of pigment with a broad absorption range or combine several pigments for the same purpose.\n\nRuthenium polypyridine complexes, in particular tris(bipyridine)ruthenium(II) and its derivatives, have been extensively used in hydrogen photoproduction due to their efficient visible light absorption and long-lived consequent metal-to-ligand charge transfer excited state, which makes the complexes strong reducing agents. Other noble metal-containing complexes used include ones with platinum, rhodium and iridium.\n\nMetal-free organic complexes have also been successfully employed as photosensitizers. Examples include eosin Y and rose bengal. Pyrrole rings such as porphyrins have also been used in coating nanomaterials or semiconductors for both homogeneous and heterogeneous catalysis.\n\nAs part of current research efforts artificial photonic antenna systems are being studied to determine efficient and sustainable ways to collect light for artificial photosynthesis. Gion Calzaferri (2009) describes one such antenna that uses zeolite L as a host for organic dyes, to mimic plant's light collecting systems. The antenna is fabricated by inserting dye molecules into the channels of zeolite L. The insertion process, which takes place under vacuum and at high temperature conditions, is made possible by the cooperative vibrational motion of the zeolite framework and of the dye molecules. The resulting material may be interfaced to an external device via a stopcock intermediate.\n\nIn nature, carbon fixation is done by green plants using the enzyme RuBisCO as a part of the Calvin cycle. RuBisCO is a rather slow catalyst compared to the vast majority of other enzymes, incorporating only a few molecules of carbon dioxide into ribulose-1,5-bisphosphate per minute, but does so at atmospheric pressure and in mild, biological conditions. The resulting product is further reduced and eventually used in the synthesis of glucose, which in turn is a precursor to more complex carbohydrates, such as cellulose and starch. The process consumes energy in the form of ATP and NADPH.\n\nArtificial CO reduction for fuel production aims mostly at producing reduced carbon compounds from atmospheric CO. Some transition metal polyphosphine complexes have been developed for this end; however, they usually require previous concentration of CO before use, and carriers (molecules that would fixate CO) that are both stable in aerobic conditions and able to concentrate CO at atmospheric concentrations haven't been yet developed. The simplest product from CO reduction is carbon monoxide (CO), but for fuel development, further reduction is needed, and a key step also needing further development is the transfer of hydride anions to CO.\n\nCharge separation is a major property of dyad and triad assemblies. Some nanomaterials employed are fullerenes (such as carbon nanotubes), a strategy that explores the pi-bonding properties of these materials. Diverse modifications (covalent and non-covalent) of carbon nanotubes have been attempted to increase the efficiency of charge separation, including the addition of ferrocene and pyrrole-like molecules such as porphyrins and phthalocyanines.\n\nSince photodamage is usually a consequence in many of the tested systems after a period of exposure to light, bio-inspired photoprotectants have been tested, such as carotenoids (which are used in photosynthesis as natural protectants).\n\nPhotoelectrochemical cells are a heterogeneous system that use light to produce either electricity or hydrogen. The vast majority of photoelectrochemical cells use semiconductors as catalysts. There have been attempts to use synthetic manganese complex-impregnated Nafion as a working electrode, but it has been since shown that the catalytically active species is actually the broken-down complex.\n\nA promising, emerging type of solar cell is the dye-sensitized solar cell. This type of cell still depends on a semiconductor (such as TiO) for current conduction on one electrode, but with a coating of an organic or inorganic dye that acts as a photosensitizer; the counter electrode is a platinum catalyst for H production. These cells have a self-repair mechanism and solar-to-electricity conversion efficiencies rivaling those of solid-state semiconductor ones.\n\nDirect water oxidation by photocatalysts is a more efficient usage of solar energy than photoelectrochemical water splitting because it avoids an intermediate thermal or electrical energy conversion step.\n\nBio-inspired manganese clusters have been shown to possess water oxidation activity when adsorbed on clays together with ruthenium photosensitizers, although with low turnover numbers.\n\nAs mentioned above, some ruthenium complexes are able to oxidize water under solar light irradiation. Although their photostability is still an issue, many can be reactivated by a simple adjustment of the conditions in which they work. Improvement of catalyst stability has been tried resorting to polyoxometalates, in particular ruthenium-based ones.\n\nWhereas a fully functional artificial system is usually intended when constructing a water splitting device, some mixed methods have been tried. One of these involve the use of a gold electrode to which photosystem II is linked; an electric current is detected upon illumination.\n\nThe simplest photocatalytic hydrogen production unit consists of a hydrogen-evolving catalyst linked to a photosensitizer. In this dyad assembly, a so-called sacrificial donor for the photosensitizer is needed, that is, one that is externally supplied and replenished; the photosensitizer donates the necessary reducing equivalents to the hydrogen-evolving catalyst, which uses protons from a solution where it is immersed or dissolved in. Cobalt compounds such as cobaloximes are some of the best hydrogen catalysts, having been coupled to both metal-containing and metal-free photosensitizers. The first H-cluster models linked to photosensitizers (mostly ruthenium photosensitizers, but also porphyrin-derived ones) were prepared during the early 2000s. Both types of assembly are under development to improve their stability and increase their turnover numbers, both necessary for constructing a sturdy, long-lived solar fuel cell.\n\nAs with water oxidation catalysis, not only fully artificial systems have been idealized: hydrogenase enzymes themselves have been engineered for photoproduction of hydrogen, by coupling the enzyme to an artificial photosensitizer, such as [Ru(bipy)] or even photosystem I.\n\nIn natural photosynthesis, the NADP coenzyme is reducible to NADPH through binding of a proton and two electrons. This reduced form can then deliver the proton and electrons, potentially as a hydride, to reactions that culminate in the production of carbohydrates (the Calvin cycle). The coenzyme is recyclable in a natural photosynthetic cycle, but this process is yet to be artificially replicated.\n\nA current goal is to obtain an NADPH-inspired catalyst capable of recreating the natural cyclic process. Utilizing light, hydride donors would be regenerated and produced where the molecules are continuously used in a closed cycle. Brookhaven chemists are now using a ruthenium-based complex to serve as the acting model. The complex is proven to perform correspondingly with NADP+/NADPH, behaving as the foundation for the proton and two electrons needed to convert acetone to isopropanol.\n\nCurrently, Brookhaven researchers are aiming to find ways for light to generate the hydride donors. The general idea is to use this process to produce fuels from carbon dioxide.\n\nSome photoautotrophic microorganisms can, under certain conditions, produce hydrogen. Nitrogen-fixing microorganisms, such as filamentous cyanobacteria, possess the enzyme nitrogenase, responsible for conversion of atmospheric N into ammonia; molecular hydrogen is a byproduct of this reaction, and is many times not released by the microorganism, but rather taken up by a hydrogen-oxidizing (uptake) hydrogenase. One way of forcing these organisms to produce hydrogen is then to annihilate uptake hydrogenase activity. This has been done on a strain of \"Nostoc punctiforme\": one of the structural genes of the NiFe uptake hydrogenase was inactivated by insertional mutagenesis, and the mutant strain showed hydrogen evolution under illumination.\n\nMany of these photoautotrophs also have bidirectional hydrogenases, which can produce hydrogen under certain conditions. However, other energy-demanding metabolic pathways can compete with the necessary electrons for proton reduction, decreasing the efficiency of the overall process; also, these hydrogenases are very sensitive to oxygen.\n\nSeveral carbon-based biofuels have also been produced using cyanobacteria, such as 1-butanol.\n\nSynthetic biology techniques are predicted to be useful for this topic. Microbiological and enzymatic engineering have the potential of improving enzyme efficiency and robustness, as well as constructing new biofuel-producing metabolic pathways in photoautotrophs that previously lack them, or improving on the existing ones. Another topic being developed is the optimization of photobioreactors for commercial application.\n\nResearch in artificial photosynthesis is necessarily a multidisciplinary topic, requiring a multitude of different expertise. Some techniques employed in making and investigating catalysts and solar cells include:\n\nAdvantages of solar fuel production through artificial photosynthesis include:\n\nDisadvantages include:\n\nA concern usually addressed in catalyst design is efficiency, in particular how much of the incident light can be used in a system in practice. This is comparable with photosynthetic efficiency, where light-to-chemical-energy conversion is measured. Photosynthetic organisms are able to collect about 50% of incident solar radiation, however the theoretical limit of photosynthetic efficiency is 4.6 and 6.0% for C3 and C4 plants respectively. In reality, the efficiency of photosynthesis is much lower and is usually below 1%, with some exceptions such as sugarcane in tropical climate. In contrast, the highest reported efficiency for artificial photosynthesis lab prototypes is 22.4%. However, plants are efficient in using CO at atmospheric concentrations, something that artificial catalysts still cannot perform.\n\n"}
{"id": "2105751", "url": "https://en.wikipedia.org/wiki?curid=2105751", "title": "Barouche", "text": "Barouche\n\nA barouche is a large, open, four-wheeled carriage, both heavy and luxurious, drawn by two horses. It was fashionable throughout the 19th century. Its body provides seats for four passengers, two back-seat passengers vis-à-vis two behind the coachman's high box-seat. A leather roof can be raised to give back-seat passengers some protection from the weather.\n\n\"Barouche\" is an anglicisation of the German word \"barutsche\", via the Italian \"baroccio\" or \"biroccio\" and ultimately from the ancient Roman Empire's Latin \"birotus\", \"two-wheeled\". The name thus became a misnomer, as the later form of the carriage had four wheels. This is an etymological technicality; over a few thousand years, words change.\n\nThe barouche was based on an earlier style of carriage, the \"calash\" or \"calèche\": this was a light carriage with small wheels, inside seats for four passengers, a separate driver's seat and a folding top. A folding calash top was a feature of two other types: the chaise, a two-wheeled carriage for one or two persons, a body hung on leather straps or thorough-braces, usually drawn by one horse; and a victoria, a low four-wheeled pleasure carriage for two with a raised seat in front for the driver. A victoria is distinguished from a barouche by having fold-down occasional seating for the rear-facing passengers, instead of permanent front seats.\n\nIn Quebec, Canada, \"calèche\" refers to a two-wheeled horse-drawn vehicle with or without a folding top and with a driver's seat on the splashboard.\n\nIn the Philippines, the \"kalesa\" is a one-horse descendant of Spanish Colonial calashes, and is a common sight in older cities such as Manila and Vigan.\n\nA barouche was an expensive four-wheeled, shallow vehicle used in the 19th century with two double seats inside, arranged \"vis-à-vis\", so that the sitters on the front seat face those on the back seat. It has a soft collapsible half-hood folding like a bellows over the back seat and a high outside box seat in front for the driver. The entire carriage is suspended on C springs and leather straps and more recently additional elliptical springs.\nIt is drawn by a pair of horses and was used in the 19th century for display and summer leisure driving. Designed to give a powerful impression of luxury and elegance, the structure of the carriage is heavier than it looks because of the lack of a rigid roof structure.\n\nA light barouche was a \"barouchet\" or \"barouchette\". A barouche-sociable was described as a cross between a barouche and a victoria. \n\nA barouche-landau is mentioned in \"Emma\", published in 1816 by Jane Austen. It \"combines the best features of a barouche and a landau\". An illustration of the expensive and more rarely seen vehicle, on account of the expense, is shown in a paper by Ed Ratcliffe, citing editor R. W. Chapman's collection of the works of Jane Austen, in the volume Minor Works, as noted in Ratcliffe's sources.\nf\n\nIn the 1994 novel, \"The Alienist\", which is set in 1896, by Caleb Carr a frequently used mode of transportation for the characters is a caleche.\n\nIn the novels by Jane Austen, \"Lady Dalrymple, Mr. and Mrs. Palmer, and Henry Crawford owned barouches\" in which other characters rode, and Jane Austen herself on at least one occasion in 1813 rode in a barouche. Henry Crawford was a character in \"Mansfield Park\" and his barouche was the topic of two important scenes of the novel; Lady Dalrymple was in \"Persuasion\", while Mr and Mrs Palmer were characters in \"Sense and Sensibility\".\n\nBarouche driving is mentioned as a fashionable pastime in Nice, Italy, in chapter 37 of \"Little Women\" by Louisa May Alcott.\n\n"}
{"id": "8759620", "url": "https://en.wikipedia.org/wiki?curid=8759620", "title": "Bilepton", "text": "Bilepton\n\nA bilepton is a hypothetical particle predicted by the minimal 331 model. It is a spin one gauge boson which appears with single and double electric charge and with lepton number L=+2 and L=-2. It can mediate exotic processes such as V+A muon decay and muonium-antimuonium conversion. It can also give rise to exotic scattering processes such as electron plus electron goes to muon plus muon which is forbidden in the Standard Model. Detailed measurements made at the Paul Scherrer Institute (PSI) have searched for the exotic muon decay and placed a lower bound on the bilepton mass of about 1 TeV. Studies of muonium-antimuonium conversion, also at PSI, have imposed a similar bound. Higher statistics experiments are planned. The bilepton could alternatively be discovered in electron-electron collisions or in multi TeV proton-proton collisions at the Large Hadron Collider.\n\nThis particle was at first misnamed \"dilepton\", with a different pre-existing usage; the present name was introduced in 1996.\n"}
{"id": "51505937", "url": "https://en.wikipedia.org/wiki?curid=51505937", "title": "Blackout (Elsberg novel)", "text": "Blackout (Elsberg novel)\n\nBlackout: Tomorrow Will Be Too Late is a disaster thriller book by the Austrian author Marc Elsberg, described by Penguin Books as \"a 21st-century high-concept disaster thriller\".\n\nPublished in German in 2012, it had been translated into fifteen languages and sold a million copies worldwide. The English version was published in 2017.\n\nThe novel is about a European power outage due to a cyberattack. For realism the book is written on the basis of interviews with intelligence and computer security officials.\n\nThe novel starts with a collapse of electrical grids across Europe, plunging the population into darkness and disaster. The prolonged electricity cut causes major problems: no more petrol, no telephone, no food in supermarkets, no cash machines working, nuclear disasters, etc. A former computer hacker and IT professional tries to find out the root cause for this. While doing so he himself becomes a hunted person as officials find suspicious e-mails sent from his laptop and think that he is involved.\n\n\n"}
{"id": "35589910", "url": "https://en.wikipedia.org/wiki?curid=35589910", "title": "Boggomoss", "text": "Boggomoss\n\nBoggomoss is a local term used to describe a mound spring in the Taroom district in the Dawson River valley of Queensland, Australia. The accepted plural is boggomossi, however boggomosses and boggomoss are the more commonly used plural forms.\n\nThe origin of the term is not known, but is most likely a compound of the words bog and moss. Boggomoss creek in the Parish of Fernyside appears on very early maps.\n\nBoggomosses range in form from small muddy swamps to elevated peat bogs or swamps, up to 150 meters across scattered among dry woodland communities, which form part of the Springsure Group of Great Artesian Basin springs. They are rich in invertebrates and form a vital chain of permanently moist oases in an otherwise dry environment.\n\nBoggomosses are prevalent east of Taroom along the Dawson River, near the Glebe Weir and south-east to Cockatoo Creek.\n\nThe spring flow rate is usually in the range of 0.5 to 2.0 litres per second (8th magnitude spring), however some large boggomosses have a flow rate of up to 10.0 litres per second (7th magnitude spring).\n\nA report commissioned by the Queensland Department of Environment defined four boggomoss vegetation types with distinct environmental relations:\n\n"}
{"id": "23787751", "url": "https://en.wikipedia.org/wiki?curid=23787751", "title": "Borsdane Wood", "text": "Borsdane Wood\n\nBorsdane Wood is an Ancient Semi Natural Woodland in the Metropolitan Borough of Wigan and Metropolitan Borough of Bolton, Greater Manchester, England. It is believed to have been continuous woodland cover since before 1600 AD and is composed of native tree species that have not obviously been planted. Borsdane Wood was designated a Local Nature Reserve in 1986. \n\nThe wood consists of approximately 34 hectares (85 acres) of mixed broadleaf trees including species such as oak, ash, birch, cherry, hazel, hawthorn, blackthorn and dog rose, as well as areas of open ground. With trees many hundreds of years old the wood has remained relatively unchanged for centuries and is home to a wide variety of wildlife. The wood follows the course of the Borsdane Brook which is the boundary between Hindley and Westhoughton and the Metropolitan Boroughs of Wigan and Bolton. The brook flows through Wigan, Pennington Flash and into the Glaze Brook.\n\nFrom Hindley, Greater Manchester there are several entrances to the wood, one of which can be accessed from Hindley centre through Raynor Park or through the cemetery. A tunnel under the railway leads into woodland little changed in centuries. A path leads to Aspull, Greater Manchester where the access is located near the Gerrard Arms Public House. \n\nThe town of Westhoughton is adjacent to the wood.\n\n\n"}
{"id": "56981277", "url": "https://en.wikipedia.org/wiki?curid=56981277", "title": "Bundwerk", "text": "Bundwerk\n\nBundwerk is a carpentry and rural architectural term from the 19th century for a method of building with timber that was used especially in Austria, South Tyrol and Bavaria. After log construction and timber framing, \"bundwerk\" is one of the most widespread forms of timber building techniques. It involved used wooden beams that were arranged partly in a lattice or diagonally over a cross. It often decorated the front and gable sides of agricultural buildings, frequently the grain barn or \"Stadel\" of quadrangular farms (\"Vierseithöfen\").\n\nIn northeastern Upper Bavaria \"bundwerk\" is especially varied and colourful. By contrast, in the Werdenfelser Land and the region around Innsbruck only a few places exhibit this type of timber building throughout.\n\n\"Bundwerk\" had its heyday between 1830 and 1860 when artists and woodcarvers, as well as carpenters, decorated the \"bundwerk\" with paintings and carvings, often with mythical creatures or Christian symbols.\n\n"}
{"id": "436418", "url": "https://en.wikipedia.org/wiki?curid=436418", "title": "Compressed air energy storage", "text": "Compressed air energy storage\n\nCompressed air energy storage (CAES) is a way to store energy generated at one time for use at another time using compressed air. At utility scale, energy generated during periods of low energy demand (off-peak) can be released to meet higher demand (peak load) periods. Small scale systems have long been used in such applications as propulsion of mine locomotives. Large scale applications must conserve the heat energy associated with compressing air; dissipating heat lowers the energy efficiency of the storage system.\n\nCompression of air creates heat; the air is warmer after compression. Expansion removes heat. If no extra heat is added, the air will be much colder after expansion. If the heat generated during compression can be stored and used during expansion, the efficiency of the storage improves considerably. There are three ways in which a CAES system can deal with the heat. Air storage can be adiabatic, diabatic, or isothermal.\n\nAdiabatic storage continues to keep the heat produced by compression and returns it to the air as it is expanded to generate power. This is a subject of ongoing study, with no utility scale plants as of 2015, but a German project ADELE is planning to bring a demonstration plant (360 MWh storage capacity) into service in 2016. The theoretical efficiency of adiabatic storage approaches 100% with perfect insulation, but in practice round trip efficiency is expected to be 70%. Heat can be stored in a solid such as concrete or stone, or more likely in a fluid such as hot oil (up to 300 °C) or molten salt solutions (600 °C).\n\nDiabatic storage dissipates much of the heat of compression with intercoolers (thus approaching isothermal compression) into the atmosphere as waste; essentially wasting, thereby, the renewable energy used to perform the work of compression. Upon removal from storage, the temperature of this compressed air is \"the one indicator\" of the amount of stored energy that remains in this air. Consequently, if the air temperature is low for the energy recovery process, the air must be substantially re-heated prior to expansion in the turbine to power a generator. This reheating can be accomplished with a natural gas fired burner for utility grade storage or with a heated metal mass. As recovery is often most needed when renewable sources are quiescent, fuel must be burned to make up for the \"wasted\" heat. This degrades the efficiency of the storage-recovery cycle; and while this approach is relatively simple, the burning of fuel adds to the cost of the recovered electrical energy and compromises the ecological benefits associated with most renewable energy sources. Nevertheless, this is thus far the only system which has been implemented commercially.\n\nThe McIntosh, Alabama CAES plant requires 2.5 MJ of electricity and 1.2 MJ lower heating value (LHV) of gas for each MJ of energy output, corresponding to an energy recovery efficiency of about 27%. A General Electric 7FA 2x1 combined cycle plant, one of the most efficient natural gas plants in operation, uses 1.85 MJ (LHV) of gas per MJ generated, a 54% thermal efficiency.\n\nIsothermal compression and expansion approaches attempt to maintain operating temperature by constant heat exchange to the environment. They are only practical for low power levels, without very effective heat exchangers. The theoretical efficiency of isothermal energy storage approaches 100% for perfect heat transfer to the environment. In practice neither of these perfect thermodynamic cycles is obtainable, as some heat losses are unavoidable.\n\nNear isothermal compression (and expansion) is a process in which a gas is compressed in very close proximity to a large incompressible thermal mass such as a heat absorbing and releasing structure (HARS) or a water spray. A HARS is usually made up of a series of parallel fins. As the gas is compressed the heat of compression is rapidly transferred to the thermal mass, so the gas temperature is stabilised. An external cooling circuit is then used to maintain the temperature of the thermal mass. \nThe isothermal efficiency (Z) is a measure of where the process lies between an adiabatic and isothermal process. If the efficiency is 0%, then it is totally adiabatic; with an efficiency of 100%, it is totally isothermal. Typically with a near isothermal process an efficiency of 90-95% can be expected.\n\nOne implementation of isothermal CAES uses high, medium and low pressure pistons in series, with each stage followed by an airblast venturi pump that draws ambient air over an air-to-air (or air-to-seawater) heat exchanger between each expansion stage. Early compressed air torpedo designs used a similar approach, substituting seawater for air. The venturi warms the exhaust of the preceding stage and admits this preheated air to the following stage. This approach was widely adopted in various compressed air vehicles such as H. K. Porter, Inc.'s mining locomotives and trams. Here the heat of compression is effectively stored in the atmosphere (or sea) and returned later on.\n\nCompression can be done with electrically powered turbo-compressors and expansion with turbo 'expanders' or air engines driving electrical generators to produce electricity.\n\nThe storage system of a CAES (Compressed Air Energy Storage) is one of the most interesting characteristics of this technology, and it is strictly related to its economic feasibility, energy density and flexibility. There are a few categories of air storage vessels, based on the thermodynamic conditions of the storage, and on the technology chosen:\n\n\nThis storage system uses a chamber with specific boundaries to store large amounts of air. This means from a thermodynamic point of view, that this system is a Constant Volume and Variable Pressure system. This causes some operational problems to the compressors and turbines operating on them, so the pressure variations have to be kept below a certain limit, as do the stresses induced on the storage vessels.\n\nThe storage vessel is often an underground cavern created by solution mining (salt is dissolved in water for extraction) or by utilizing an abandoned mine; use of porous rock formations (rocks which have holes through which liquid or air can pass), such as those in which reservoirs of natural gas are found, has also been studied.\n\nIn some cases also an above ground pipeline was tested as a storage system, giving some good results. Obviously the cost of the system is higher, but it can be placed wherever the designer chooses, while an underground system needs some particular geologic formations (salt domes, aquifers, depleted gas mines, etc.).\n\nIn this case the storage vessel is kept at a constant pressure, while the gas is contained in a variable volume vessel. Many types of storage vessel have been proposed, but the operating conditions follow the same principle: The storage vessel is positioned hundreds of meters underwater and the hydrostatic pressure of the water column above the storage vessel allows maintaining the pressure at the desired level.\n\nThis configuration allows:\nOn the other hand, the cost of this storage system is higher, due to the need of positioning the storage vessel on the bottom of the chosen water reservoir (often the sea or the ocean) and due to the cost of the vessel itself.\n\nPlants operate on a daily cycle, charging at night and discharging during the day. Heating of the compressed air using natural gas or geothermal heat to increase the amount of energy being extracted has been studied by the Pacific Northwest National Laboratory\n\nCompressed air energy storage can also be employed on a smaller scale such as exploited by air cars and air-driven locomotives, and can use high-strength carbon-fiber air storage tanks. In order to retain the energy stored in compressed air, this tank should be thermally isolated from the environment; else, the energy stored will escape under the form of heat since compressing air raises its temperature.\n\nCitywide compressed air energy systems have been built since 1870. Cities such as Paris, France; Birmingham, England; Dresden, Rixdorf and Offenbach, Germany and Buenos Aires, Argentina installed such systems. Victor Popp constructed the first systems to power clocks by sending a pulse of air every minute to change their pointer arms. They quickly evolved to deliver power to homes and industry. As of 1896, the Paris system had 2.2 MW of generation distributed at 550 kPa in 50 km of air pipes for motors in light and heavy industry. Usage was measured by cubic meters. The systems were the main source of house-delivered energy in those days and also powered the machines of dentists, seamstresses, printing facilities and bakeries.\n\n\n\nIn order to achieve a near thermodynamic reversible process so that most of the energy is saved in the system and can be retrieved, and losses are kept negligible, a near reversible isothermal process or an isentropic process is desired.\n\nIn an isothermal compression process, the gas in the system is kept at a constant temperature throughout. This necessarily requires exchange of heat with the gas, otherwise the temperature would rise during charging and drop during discharge. This heat exchange can be achieved by heat exchangers (intercooling) between subsequent stages in the compressor, regulator and tank. To avoid wasted energy, the intercoolers must be optimised for high heat transfer and low pressure drop. Smaller compressors can approximate isothermal compression even without intercooling, due to the relatively high ratio of surface area to volume of the compression chamber and the resulting improvement in heat dissipation from the compressor body itself.\n\nWhen one obtains perfect isothermal storage (and discharge), the process is said to be \"reversible\". This requires that the heat transfer between the surroundings and the gas occur over an infinitesimally small temperature difference. In that case, there is no exergy loss in the heat transfer process, and so the compression work can be completely recovered as expansion work: 100% storage efficiency. However, in practice, there is always a temperature difference in any heat transfer process, and so all practical energy storage obtains efficiencies lower than 100%.\n\nTo estimate the compression/expansion work in an isothermal process, it may be assumed that the compressed air obeys the ideal gas law,\nFrom a process from an initial state \"A\" to a final state \"B\", with absolute temperature formula_2 constant, one finds the work required for compression (negative) or done by the expansion (positive), to be\nwhere formula_4, and so, formula_5. Here, formula_6 is the absolute pressure,\nformula_7 is the volume of the vessel, formula_8 is the amount of substance of gas (mol) and formula_9 is the ideal gas constant.\n\nIf there is a constant pressure outside of the vessel which is equal to the starting pressure formula_10, the positive work of the outer pressure reduces the exploitable energy (negative value). This adds a term to the equation above:\n\"Example\"\n\nHow much energy can be stored in a 1 m storage vessel at a pressure of , if the ambient pressure is . In this case, the process work is\n\nThe negative sign means that work is done on the gas by the surroundings. Process irreversibilities (such as in heat transfer) will result in less energy being recovered from the expansion process than is required for the compression process. If the environment is at a constant temperature, for example, the thermal resistance in the intercoolers will mean that the compression occurs at a temperature somewhat higher than the ambient temperature, and the expansion will occur at a temperature somewhat lower than ambient temperature. So a perfect isothermal storage system is impossible to achieve.\n\nAn adiabatic process is one where there is no heat transfer between the fluid and the surroundings: the system is insulated against heat transfer. If the process is furthermore internally reversible (smooth, slow and frictionless, to the ideal limit) then it will additionally be isentropic.\n\nAn adiabatic storage system does away with the intercooling during the compression process, and simply allows the gas to heat up during compression, and likewise to cool down during expansion. This is attractive, since the energy losses associated with the heat transfer are avoided, but the downside is that the storage vessel must be insulated against heat loss. It should also be mentioned that real compressors and turbines are not isentropic, but instead have an isentropic efficiency of around 85%, with the result that round-trip storage efficiency for adiabatic systems is also considerably less than perfect.\n\nEnergy storage systems often use large underground caverns. This is the preferred system design, due to the very large volume, and thus the large quantity of energy that can be stored with only a small pressure change. The cavern space can be easily insulated, compressed adiabatically with little temperature change (approaching a reversible isothermal system) and heat loss (approaching an isentropic system). This advantage is in addition to the low cost of constructing the gas storage system, using the underground walls to assist in containing the pressure.\n\nRecently there have been developed undersea insulated air bags, with similar thermodynamic properties to large underground cavern storage.\n\nIn order to use air storage in vehicles or aircraft for practical land or air transportation, the energy storage system must be compact and lightweight. Energy density and specific energy are the engineering terms that define these desired qualities.\n\nAs explained in the thermodynamics of gas storage section above, compressing air heats it and expanding it cools it. Therefore, practical air engines require heat exchangers in order to avoid excessively high or low temperatures and even so don't reach ideal constant temperature conditions, or ideal thermal insulation.\n\nNevertheless, as stated above, it is useful to describe the maximum energy storable using the isothermal case, which works out to about 100 kJ/m [ ln(\"P\"/\"P\")].\n\nThus if 1.0 m of air from the atmosphere is very slowly compressed into a 5 L bottle at , the potential energy stored is 530 kJ. A highly efficient air motor can transfer this into kinetic energy if it runs very slowly and manages to expand the air from its initial 20 MPa pressure down to 100 kPa (bottle completely \"empty\" at atmospheric pressure). Achieving high efficiency is a technical challenge both due to heat loss to the ambient and to unrecoverable internal gas heat. If the bottle above is emptied to 1 MPa, the extractable energy is about 300 kJ at the motor shaft.\n\nA standard 20 MPa, 5 L steel bottle has a mass of 7.5 kg, a superior one 5 kg. High-tensile strength fibers such as carbon-fiber or Kevlar can weigh below 2 kg in this size, consistent with the legal safety codes. One cubic meter of air at 20 °C has a mass of 1.204 kg at standard temperature and pressure. Thus, theoretical specific energies are from roughly 70 kJ/kg at the motor shaft for a plain steel bottle to 180 kJ/kg for an advanced fiber-wound one, whereas practical achievable specific energies for the same containers would be from 40 to 100 kJ/kg.\n\nAdvanced fiber-reinforced bottles are comparable to the rechargeable lead-acid battery in terms of energy density. Batteries provide nearly constant voltage over their entire charge level, whereas the pressure varies greatly while using a pressure vessel from full to empty. It is technically challenging to design air engines to maintain high efficiency and sufficient power over a wide range of pressures. Compressed air can transfer power at very high flux rates, which meets the principal acceleration and deceleration objectives of transportation systems, particularly for hybrid vehicles.\n\nCompressed air systems have advantages over conventional batteries including longer lifetimes of pressure vessels and lower material toxicity. Newer battery designs such as those based on Lithium Iron Phosphate chemistry suffer from neither of these problems. Compressed air costs are potentially lower; however advanced pressure vessels are costly to develop and safety-test and at present are more expensive than mass-produced batteries.\n\nAs with electric storage technology, compressed air is only as \"clean\" as the source of the energy that it stores. Life cycle assessment addresses the question of overall emissions from a given energy storage technology combined with a given mix of generation on a power grid.\n\nAs with most technologies, compressed air has safety concerns, mainly catastrophic tank rupture. Safety regulations make this a rare occurrence at the cost of higher weight and additional safety features such as pressure relief valves. Regulations may limit the legal working pressure to less than 40% of the rupture pressure for steel bottles (safety factor of 2.5), and less than 20% for fiber-wound bottles (safety factor of 5). Commercial designs adopt the ISO 11439 standard. High pressure bottles are fairly strong so that they generally do not rupture in vehicle crashes.\n\nAir engines have been used since the 19th century to power mine locomotives, pumps, drills and trams, via centralized, city-level, distribution. Racecars use compressed air to start their internal combustion engine (ICE), and large Diesel engines may have starting pneumatic motors.\n\nA compressed air engine uses the expansion of compressed air to drive the pistons of an engine, turn the axle, or to drive a turbine.\n\nThe following methods can increase efficiency:\n\nA highly efficient arrangement uses high, medium and low pressure pistons in series, with each stage followed by an airblast venturi that draws ambient air over an air-to-air heat exchanger. This warms the exhaust of the preceding stage and admits this preheated air to the following stage. The only exhaust gas from each stage is cold air which can be as cold as ; the cold air may be used for air conditioning in a car.\n\nAdditional heat can be supplied by burning fuel as in 1904 for Whitehead's torpedoes. This improves the range and speed available for a given tank volume at the cost of the additional fuel.\n\nSince about 1990 several companies have claimed to be developing compressed air cars, but none is available. Typically the main claimed advantages are: no roadside pollution, low cost, use of cooking oil for lubrication, and integrated air conditioning.\n\nThe time required to refill a depleted tank is important for vehicle applications. \"Volume transfer\" moves pre-compressed air from a stationary tank to the vehicle tank almost instantaneously. Alternatively, a stationary or on-board compressor can compress air on demand, possibly requiring several hours.\n\nLarge marine diesel engines are started using compressed air, typically between 20 and 30 bar and stored in two or more large bottles, acting directly on the pistons via special starting valves to turn the crankshaft prior to beginning fuel injection. This arrangement is more compact and cheaper than an electric starter motor would be at such scales, and able to supply the necessary burst of extremely high power without placing a prohibitive load on the ship's electrical generators and distribution system. Compressed air is commonly also used, at lower pressures, to control the engine and act as the spring force acting on the cylinder exhaust valves, and to operate other auxiliary systems and power tools on board, sometimes including pneumatic PID controllers. One advantage of this approach is that in the event of an electrical blackout, ship systems powered by stored compressed air can continue functioning uninterrupted, and generators can be restarted without an electrical supply. Another is that pneumatic tools can be used in commonly wet environments without risk of electric shock.\n\nWhile the air storage system offers a relatively low power density and vehicle range, its high efficiency is attractive for hybrid vehicles that use a conventional internal combustion engine as a main power source. The air storage can be used for regenerative braking and to optimize the cycle of the piston engine which is not equally efficient at all power/RPM levels.\n\nBosch and PSA Peugeot Citroën have developed a hybrid system that use hydraulics as a way to transfer energy to and from a compressed nitrogen tank. An up to 45% reduction in fuel consumption is claimed, corresponding to 2.9l/100 km (81 mpg, 69 g CO2/km) on the NEDC cycle for a compact frame like Peugeot 208. The system is claimed to be much more affordable than competing electric and flywheel KERS systems and is expected on road cars by 2016.\n\nBrayton cycle engines compress and heat air with a fuel suitable for an internal combustion engine. For example, natural gas or biogas heat compressed air, and then a conventional gas turbine engine or the rear portion of a jet engine expands it to produce work.\n\nCompressed air engines can recharge an electric battery. The apparently defunct Energine promoted its Pne-PHEV or Pneumatic Plug-in Hybrid Electric Vehicle-system.\n\nHuntorf, Germany in 1978, and McIntosh, Alabama, U.S. in 1991 commissioned hybrid power plants.<ref name='new.scientist.com/vol.195 no.2623 p. 45'></ref> Both systems use off-peak energy for air compression and burn natural gas in the compressed air during the power generating phase.\n\nThe Iowa Stored Energy Park (ISEP) will use aquifer storage rather than cavern storage. The displacement of water in the aquifer results in regulation of the air pressure by the constant hydrostatic pressure of the water. A spokesperson for ISEP claims, \"you can optimize your equipment for better efficiency if you have a constant pressure.\" Power output of the McIntosh and Iowa systems is in the range of 2–300 MW.\n\nAdditional facilities are under development in Norton, Ohio. FirstEnergy, an Akron, Ohio electric utility obtained development rights to the 2,700 MW Norton project in November, 2009.\n\nThe RICAS2020 project attempts to use an abandoned mine for adiabatic CAES with heat recovery. The compression heat is stored in a tunnel section filled with loose stones, so the compressed air is nearly cool when entering the main pressure storage chamber. The cool compressed air regains the heat stored in the stones when released back through a surface turbine, leading to a higher overall efficiency.\n\nDeep water in lakes and the ocean can provide pressure without requiring high-pressure vessels or drilling into salt caverns or aquifers. The air goes into inexpensive, flexible containers such as plastic bags below in deep lakes or off sea coasts with steep drop-offs. Obstacles include the limited number of suitable locations and the need for high-pressure pipelines between the surface and the containers. Since the containers would be very inexpensive, the need for great pressure (and great depth) may not be as important. A key benefit of systems built on this concept is that charge and discharge pressures are a constant function of depth. Carnot inefficiencies can thereby be reduced in the power plant. Carnot efficiency can be increased by using multiple charge and discharge stages and using inexpensive heat sources and sinks such as cold water from rivers or hot water from solar ponds. Ideally, the system must be very clever—for example, by cooling air before pumping on summer days. It must be engineered to avoid inefficiency, such as wasteful pressure changes caused by inadequate piping diameter.\n\nA nearly isobaric solution is possible if the compressed gas is used to drive a hydroelectric system. However, this solution requires large pressure tanks located on land (as well as the underwater air bags). Also, hydrogen gas is the preferred fluid, since other gases suffer from substantial hydrostatic pressures at even relatively modest depths (such as 500 meters).\n\nE.ON, one of Europe's leading power and gas companies, has provided €1.4 million (£1.1 million) in funding to develop undersea air storage bags. Hydrostor in Canada is developing a commercial system of underwater storage \"accumulators\" for compressed air energy storage, starting at the 1 to 4 MW scale.\nThere is a plan for some type of compressed air energy storage in undersea caves by Northern Ireland.\n\nA number of methods of near isothermal compression are being developed. Fluid Mechanics has a system with a heat absorbing and releasing structure (HARS) attached to a reciprocating piston. Light Sail inject a water spray into a reciprocating cylinder. SustainX use an air water foam mix inside a compressor. All these systems ensure that the air is compressed with high thermal diffusivity compared to the speed of compression. Typically these compressors can run at speeds up to 1000 rpm. To ensure high thermal diffusivity the average distance a gas molecule is from a heat absorbing surface is about 0.5mm. These near isothermal compressors can also be used as near isothermal expanders and are being developed to improve the round trip efficiency of CASE.\n\n\n"}
{"id": "860861", "url": "https://en.wikipedia.org/wiki?curid=860861", "title": "Doping (semiconductor)", "text": "Doping (semiconductor)\n\nIn semiconductor production, doping is the intentional introduction of impurities into an intrinsic semiconductor for the purpose of modulating its electrical, optical and structural properties. The doped material is referred to as an extrinsic semiconductor. A semiconductor doped to such high levels that it acts more like a conductor than a semiconductor is referred to as a degenerate semiconductor.\n\nIn the context of phosphors and scintillators, doping is better known as activation. Doping is also used to control the color in some pigments.\n\nThe effects of semiconductor doping were long known empirically in such devices as crystal radio detectors and selenium rectifiers. For instance, in 1885 Shelford Bidwell, and in 1930 the German scientist Bernhard Gudden, each independently reported that the properties of semiconductors were due to the impurities contained within them. \nThe doping process was formally first developed by John Robert Woodyard working at Sperry Gyroscope Company during World War II, with a US Patent issued in 1950. The demands of his work on radar denied Woodyard the opportunity to pursue research on semiconductor doping.\n\nSimilar work was performed at Bell Labs by Gordon K. Teal and Morgan Sparks, with a US Patent issued in 1953.\n\nWoodyard's prior patent proved to be the grounds of extensive litigation by Sperry Rand .\n\nThe concentration of dopant affects many electrical properties. Most important is the material's charge carrier concentration. In an intrinsic semiconductor under thermal equilibrium, the concentrations of electrons and holes are equivalent. That is,\n\nIn a non-intrinsic semiconductor under thermal equilibrium, the relation becomes (for low doping):\n\nwhere \"n\" is the concentration of conducting electrons, \"p\" is the electron hole concentration, and \"n\" is the material's intrinsic carrier concentration. The intrinsic carrier concentration varies between materials and is dependent on temperature. Silicon's \"n\", for example, is roughly 1.08×10 cm at 300 kelvins, about room temperature.\n\nIn general, increased doping leads to increased conductivity due to the higher concentration of carriers. Degenerate (very highly doped) semiconductors have conductivity levels comparable to metals and are often used in integrated circuits as a replacement for metal. Often superscript plus and minus symbols are used to denote relative doping concentration in semiconductors. For example, \"n\" denotes an n-type semiconductor with a high, often degenerate, doping concentration. Similarly, \"p\" would indicate a very lightly doped p-type material. Even degenerate levels of doping imply low concentrations of impurities with respect to the base semiconductor. In intrinsic crystalline silicon, there are approximately 5×10 atoms/cm. Doping concentration for silicon semiconductors may range anywhere from 10 cm to 10 cm. Doping concentration above about 10 cm is considered degenerate at room temperature. Degenerately doped silicon contains a proportion of impurity to silicon on the order of parts per thousand. This proportion may be reduced to parts per billion in very lightly doped silicon. Typical concentration values fall somewhere in this range and are tailored to produce the desired properties in the device that the semiconductor is intended for.\n\nDoping a semiconductor in a good crystal introduces allowed energy states within the band gap, but very close to the energy band that corresponds to the dopant type. In other words, electron donor impurities create states near the conduction band while electron acceptor impurities create states near the valence band. The gap between these energy states and the nearest energy band is usually referred to as dopant-site bonding energy or \"E\" and is relatively small. For example, the \"E\" for boron in silicon bulk is 0.045 eV, compared with silicon's band gap of about 1.12 eV. Because \"E\" is so small, room temperature is hot enough to thermally ionize practically all of the dopant atoms and create free charge carriers in the conduction or valence bands.\n\nDopants also have the important effect of shifting the energy bands relative to the Fermi level. The energy band that corresponds with the dopant with the greatest concentration ends up closer to the Fermi level. Since the Fermi level must remain constant in a system in thermodynamic equilibrium, stacking layers of materials with different properties leads to many useful electrical properties induced by band bending, if the interfaces can be made cleanly enough. For example, the p-n junction's properties are due to the band bending that happens as a result of the necessity to line up the bands in contacting regions of p-type and n-type material.\nThis effect is shown in a band diagram. The band diagram typically indicates the variation in the valence band and conduction band edges versus some spatial dimension, often denoted \"x\". The Fermi level is also usually indicated in the diagram. Sometimes the \"intrinsic Fermi level\", \"E\", which is the Fermi level in the absence of doping, is shown. These diagrams are useful in explaining the operation of many kinds of semiconductor devices.\n\nFor low levels of doping, the relevant energy states are populated sparsely by electrons (conduction band) or holes (valence band). It is possible to write simple expressions for the electron and hole carrier concentrations, by ignoring Pauli exclusion (via Maxwell–Boltzmann statistics):\nwhere is the Fermi level, is the minimum energy of the conduction band, and is the maximum energy of the valence band. These are related to the value of the intrinsic concentration via\nan expression which is independent of the doping level, since (the band gap) does not change with doping.\n\nThe concentration factors and are given by\nwhere and are the density of states effective masses of electrons and holes, respectively, quantities that are roughly constant over temperature.\n\nThe synthesis of n-type semiconductors may involve the use of vapor-phase epitaxy. In vapor-phase epitaxy, a gas containing the negative dopant is passed over the substrate wafer. In the case of n-type GaAs doping, hydrogen sulfide is passed over the gallium arsenide, and sulfur is incorporated into the structure. This process is characterized by a constant concentration of sulfur on the surface. In the case of semiconductors in general, only a very thin layer of the wafer needs to be doped in order to obtain the desired electronic properties. The reaction conditions typically range from 600 to 800 °C for the n-doping with group VI elements, and the time is typically 6–12 hours depending on the temperature.\n\nSome dopants are added as the (usually silicon) boule is grown, giving each wafer an almost uniform initial doping. To define circuit elements, selected areas — typically controlled by photolithography — are further doped by such processes as diffusion and ion implantation, the latter method being more popular in large production runs because of increased controllability.\n\nSmall numbers of dopant atoms can change the ability of a semiconductor to conduct electricity. When on the order of one dopant atom is added per 100 million atoms, the doping is said to be \"low\" or \"light\". When many more dopant atoms are added, on the order of one per ten thousand atoms, the doping is referred to as \"high\" or \"heavy\". This is often shown as \"n+\" for n-type doping or \"p+\" for p-type doping. (\"See the article on semiconductors for a more detailed description of the doping mechanism.\")\n\nFor the Group IV semiconductors such as diamond, silicon, germanium, silicon carbide, and silicon germanium, the most common dopants are acceptors from Group III or donors from Group V elements. Boron, arsenic, phosphorus, and occasionally gallium are used to dope silicon. Boron is the p-type dopant of choice for silicon integrated circuit production because it diffuses at a rate that makes junction depths easily controllable. Phosphorus is typically used for bulk-doping of silicon wafers, while arsenic is used to diffuse junctions, because it diffuses more slowly than phosphorus and is thus more controllable.\n\nBy doping pure silicon with Group V elements such as phosphorus, extra valence electrons are added that become unbonded from individual atoms and allow the compound to be an electrically conductive n-type semiconductor. Doping with Group III elements, which are missing the fourth valence electron, creates \"broken bonds\" (holes) in the silicon lattice that are free to move. The result is an electrically conductive p-type semiconductor. In this context, a Group V element is said to behave as an electron donor, and a group III element as an acceptor. This is a key concept in the physics of a diode.\n\nA very heavily doped semiconductor behaves more like a good conductor (metal) and thus exhibits more linear positive thermal coefficient. Such effect is used for instance in sensistors. Lower dosage of doping is used in other types (NTC or PTC) thermistors.\n\n\n\nIn most cases many types of impurities will be present in the resultant doped semiconductor. If an equal number of donors and acceptors are present in the semiconductor, the extra core electrons provided by the former will be used to satisfy the broken bonds due to the latter, so that doping produces no free carriers of either type. This phenomenon is known as \"compensation\", and occurs at the p-n junction in the vast majority of semiconductor devices. Partial compensation, where donors outnumber acceptors or vice versa, allows device makers to repeatedly reverse (invert) the type of a given portion of the material by applying successively higher doses of dopants, so-called counterdoping. Most modern semiconductors are made by successive selective counterdoping steps to create the necessary P and N type areas.\n\nAlthough compensation can be used to increase or decrease the number of donors or acceptors, the electron and hole mobility is always decreased by compensation because mobility is affected by the sum of the donor and acceptor ions.\n\nConductive polymers can be doped by adding chemical reactants to oxidize, or sometimes reduce, the system so that electrons are pushed into the conducting orbitals within the already potentially conducting system. There are two primary methods of doping a conductive polymer, both of which use an oxidation-reduction (i.e., redox) process.\n\nN-doping is much less common because the Earth's atmosphere is oxygen-rich, thus creating an oxidizing environment. An electron-rich, n-doped polymer will react immediately with elemental oxygen to \"de-dope\" (i.e., reoxidize to the neutral state) the polymer. Thus, chemical n-doping must be performed in an environment of inert gas (e.g., argon). Electrochemical n-doping is far more common in research, because it is easier to exclude oxygen from a solvent in a sealed flask. However, it is unlikely that n-doped conductive polymers are available commercially.\n\nMolecular dopants are preferred in doping molecular semiconductors due to their compatibilities of processing with the host, that is, similar evaporation temperatures or controllable solubility. Besides, molecular dopants are also benefitted from their relatively large size comparing metal ion dopants (such as Li and Mo), which makes them have excellent spatial confinement in order to be used in multilayer structure, such as OLEDs and Organic solar cells. The typical p-type dopants are F4-TCNQ and Mo(tfd). However, as the similar problem encountered in doping conductive polymers, air-stable n-dopants suitable for materials with low electron affinity(EA) are still elusive. Recently, photo-activation with the combination of cleavable dimeric dopants, such as [RuCpMes], suggest a new path to realize effective n-doping in those low-EA materials.\n\nResearch on magnetic doping has shown that considerable alteration of certain properties such as specific heat may be affected by small concentrations of an impurity; for example, dopant impurities in semiconducting ferromagnetic alloys can generate different properties as first predicted by White, Hogan, Suhl and Nakamura.\nThe inclusion of dopant elements to impart dilute magnetism is of growing significance in the field of Magnetic semiconductors. The presence of disperse ferromagnetic species is key to the functionality of emerging Spintronics, a class of systems that utilise electron spin in addition to charge. Using Density functional theory(DFT) the temperature dependent magnetic behaviour of dopants within a given lattice can be modeled to identify candidate semiconductor systems.\n\nThe sensitive dependence of a semiconductor's properties on dopants has provided an extensive range of tunable phenomena to explore and apply to devices. It is possible to identify the effects of a solitary dopant on commercial device performance as well as on the fundamental properties of a semiconductor material. New applications have become available that require the discrete character of a single dopant, such as single-spin devices in the area of quantum information or single-dopant transistors. Dramatic advances in the past decade towards observing, controllably creating and manipulating single dopants, as well as their application in novel devices have allowed opening the new field of solotronics (solitary dopant optoelectronics).\n\nNeutron transmutation doping (NTD) is an unusual doping method for special applications. Most commonly, it is used to dope silicon n-type in high-power electronics. It is based on the conversion of the Si-30 isotope into phosphorus atom by neutron absorption as follows:\n\nIn practice, the silicon is typically placed near a nuclear reactor to receive the neutrons. As neutrons continue to pass through the silicon, more and more phosphorus atoms are produced by transmutation, and therefore the doping becomes more and more strongly n-type. NTD is a far less common doping method than diffusion or ion implantation, but it has the advantage of creating an extremely uniform dopant distribution.\n\nModulation doping is a synthesis technique in which the dopants are spatially separated from the carriers. In this way, carrier-donor scattering is suppressed, allowing very high mobility to be attained.\n\n"}
{"id": "14949005", "url": "https://en.wikipedia.org/wiki?curid=14949005", "title": "Dry lubricant", "text": "Dry lubricant\n\nDry lubricants or solid lubricants are materials that, despite being in the solid phase, are able to reduce friction between two surfaces sliding against each other without the need for a liquid oil medium.\n\nThe two main dry lubricants are graphite and molybdenum disulfide. They offer lubrication at temperatures higher than liquid and oil-based lubricants operate. Dry lubricants are often used in applications such as locks or dry lubricated bearings. Such materials can operate up to 350 °C (662 °F) in oxidizing environments and even higher in reducing / non-oxidizing environments (molybdenum disulfide up to 1100 °C, 2012 °F). The low-friction characteristics of most dry lubricants are attributed to a layered structure on the molecular level with weak bonding between layers. Such layers are able to slide relative to each other with minimal applied force, thus giving them their low friction properties.\n\nHowever, a layered crystal structure alone is not necessarily sufficient for lubrication. In fact, there are some solids with non-lamellar structures that function well as dry lubricants in some applications. These include certain soft metals (indium, lead, silver, tin), polytetrafluroethylene, some solid oxides, rare-earth fluorides, and even diamond.\n\nLimited interest has been shown in low friction properties of compacted oxide glaze layers formed at several hundred degrees Celsius in metallic sliding systems. However, practical use is still many years away due to their physically unstable nature.\n\nThe four most commonly used solid lubricants are:\n\nGraphite and molybdenum disulfide are the predominant materials used as dry lubricants.\n\nThe lubricity of many solids is attributable to a lamellar structure. The lamellae orient parallel to the surface in the direction of motion and slide easily over each other resulting in low friction and preventing contact between sliding components even under high loads. Large particles perform best on rough surfaces at low speed, finer particles on smoother surfaces and at higher speeds. These materials may be added in the form of dry powder to liquid lubricants to modify or enhance their properties.\n\nOther components that are useful solid lubricants include boron nitride, polytetrafluorethylene (PTFE), talc, calcium fluoride, cerium fluoride, and tungsten disulfide.\n\nSolid lubricants are useful for conditions when conventional lubricants are inadequate, such as:\n\nGraphite is structurally composed of planes of polycyclic carbon atoms that are hexagonal in orientation. The distance of carbon atoms between planes is longer and, therefore, the bonding is weaker.\n\nGraphite is best suited for lubrication in air. Water vapor is a necessary component for graphite lubrication. The adsorption of water reduces the bonding energy between the hexagonal planes of the graphite to a lower level than the adhesion energy between a substrate and the graphite. Because water vapor is a requirement for lubrication, graphite is not effective in vacuum. Because it is electrically conductive, graphite can promote galvanic corrosion. In an oxidative atmosphere, graphite is effective at high temperatures up to 450 °C continuously and can withstand much higher temperature peaks.\n\nGraphite is characterized by two main groups: natural and synthetic.\n\nFor applications where only a minor lubricity is needed and a more thermally insulating coating is required, then amorphous graphite would be chosen (80% carbon).\n\nMoS is mined from some sulfide-rich deposits and refined to achieve a purity suitable for lubricants. Like graphite, MoS has a hexagonal crystal structure with the intrinsic property of easy shear. MoS lubrication performance often exceeds that of graphite and is effective in vacuum as well, whereas graphite is not. The temperature limitation of MoS at 400 °C is restricted by oxidation. Particle size and film thickness are important parameters that should be matched to the surface roughness of the substrate. Large particles may result in excessive wear by abrasion caused by impurities in the MoS, and small particles may result in accelerated oxidation.\n\nHexagonal boron nitride is a ceramic powder lubricant. The most interesting lubricant feature is its high temperature resistance of 1200 °C service temperature in an oxidizing atmosphere. Furthermore, boron nitride has a high thermal conductivity. (Cubic boron nitride is very hard and used as an abrasive and cutting tool component.)\n\nPolytetrafluorethylene (PTFE) is widely used as an additive in lubricating oils and greases. Due to the low surface energy of PTFE, stable unflocculated dispersions of PTFE in oil or water can be produced. Contrary to the other solid lubricants discussed, PTFE does not have a layered structure. The macro molecules of PTFE slip easily along each other, similar to lamellar structures. PTFE shows one of the smallest coefficients of static and dynamic friction, down to 0.04. Operating temperatures are limited to about 260 °C.\n\nDispersion of solid lubricant as an additive in oil, water, or grease is most commonly used. For parts that are inaccessible for lubrication after assembly, a dry film lubricant can be sprayed. After the solvent evaporates, the coating cures at room temperature to form a solid lubricant. Pastes are grease-like lubricants containing a high percentage of solid lubricants used for assembly and lubrication of highly loaded, slow-moving parts. Black pastes generally contain MoS. For high temperatures above 500 °C, pastes are composed on the basis of metal powders to protect metal parts from oxidation necessary to facilitate disassembly of threaded connections and other assemblies.\n\nDry-powder tumbling is an effective application method. The bonding can be improved by prior phosphating of the substrate. Use of free powders has its limitations, since adhesion of the solid particles to the substrate is usually insufficient to provide any service life in continuous applications. However, to improve running-in conditions or in metal-forming processes, a short duration of the improved slide conditions may suffice.\n\nAnti-friction (AF) coatings are \"lubricating paints\" consisting of fine particles of lubricating pigments, such as molydisulfide, PTFE or graphite, blended with a binder. After application and proper curing, these \"slippery\" or dry lubricants bond to the metal surface and form a dark gray solid film. Many dry film lubricants contain special rust inhibitors which offer exceptional corrosion protection. Most long-wearing films are of the bonded type but are still restricted to applications where sliding distances are not too long. AF coatings are applied where fretting and galling is a problem (such as splines, universal joints and keyed bearings), where operating pressures exceed the load-bearing capacities of ordinary oils and greases, where smooth running in is desired (piston, camshaft), where clean operation is desired (AF coatings will not collect dirt and debris like greases and oils), and where parts may be stored for long periods.\n\nSelf-lubricating composites: Solid lubricants such as PTFE, graphite, MoS and some other anti-friction and anti-wear additives are often compounded in polymers and all kind of sintered materials. MoS, for example, is compounded in materials for sleeve bearings, elastomer O-rings, carbon brushes, etc. Solid lubricants are compounded in plastics to form a \"self-lubricating\" or \"internally lubricated\" thermoplastic composite. For example, PTFE particles compounded in the plastic form a PTFE film over the mating surface, resulting in a reduction of friction and wear. MoS compounded in nylon reduces wear, friction and stick-slip. Furthermore, it acts as a nucleating agent effecting in a very fine crystalline structure. The primary use of graphite lubricated thermoplastics is in applications operating in aqueous environments.\n\n"}
{"id": "9280142", "url": "https://en.wikipedia.org/wiki?curid=9280142", "title": "Dyewoods", "text": "Dyewoods\n\nA dyewood is any of a number of varieties of wood which provide dyes for textiles and other purposes. Among the more important are:\n\n"}
{"id": "18633180", "url": "https://en.wikipedia.org/wiki?curid=18633180", "title": "Ecosecurities", "text": "Ecosecurities\n\nEcoSecurities is a business that sources and develops carbon credits and carbon offsets from greenhouse gas emission reduction projects throughout the world.\n\nThe company has offices in Europe, Middle East, Americas and Asia and has a large portfolio of green house gas emission reduction projects covering numerous countries and technologies.\n\nEcoSecurities was created in January 1997, 12 months before the Kyoto conference. After a period of organic growth, the company floated on the Alternative Investment Market of the London Stock Exchange in December 2005, allowing it to grow its geographic presence by placing over 300 staff in 30 countries. Its portfolio grew to over 150 million CERs - the largest in the world at the time. In June 2007, the company raised an additional €100 million to expand into new markets, with Credit Suisse becoming a major investor. It trades in the international carbon market which was valued at €38 billion in the first six months of 2008.\n\nThe former President and co-founder of EcoSecurities was Pedro Moura-Costa and the former CEO was Bruce Usher.\n\nAt the end of 2009 the firm was acquired by JP Morgan and is now an indirect wholly owned subsidiarly of JPMorgan Chase and Co. and can provide its clients with access to the scope of services provided by the bank's Environmental Markets business in addition to those activities of sourcing and developing greenhouse gas abatement projects outlined above. Following on from acquisition of EcoSecurities the company appointed Paul Mark Kelly as Chief Executive Officer of the company in December 2009, when the company subsequently delisted from the Alternative Investment Market of the London Stock Exchange.\n\nEcoSecurities operates in both the compliance and voluntary carbon markets. In the compliance market, EcoSecurities implements projects in the developing world under the Kyoto Protocol's Clean Development Mechanism to generate Certified Emission Reductions which are used by governments and organisations under the Kyoto Protocol and European Union Emissions Trading Scheme.\n\nClean Development Mechanism projects that EcoSecurities has helped develop include:\n\nIn the voluntary market EcoSecurities develops carbon projects to sell on different markets, including the Voluntary Carbon Standard, the California Climate Action Registry standard and The Gold Standard's Voluntary Emission Reduction standard. These are sold to organisations that have no regulatory obligation to offset their carbon emissions, but wish to do so on a voluntary basis, often as part of a carbon neutrality or corporate social responsibility programme.\n\nSome of EcoSecurities' current and historic clients include Yahoo!, Fortune Conferences and NetJets Europe.\n\nVoluntary emission reduction projects developed through EcoSecurities include\nmethane capture for dairies in the United States and SPM Group pig farms in Thailand.\n\nClients include Yahoo!, Fortune Conferences and NetJets Europe.[9]\nClimate Neutral Network. 2008. “EcoSecurities,” United Nations Environment Programme, http://www.climateneutral.unep.org/cnn_contentdetail.aspx?m=336&amid=1870.\n\n"}
{"id": "23242297", "url": "https://en.wikipedia.org/wiki?curid=23242297", "title": "Equivalent oxide thickness", "text": "Equivalent oxide thickness\n\nAn Equivalent oxide thickness is a distance, usually given in nanometers (nm), which indicates how thick a silicon oxide film would need to be to produce the same effect as the high-κ material being used. \n\nThe term is often used when describing field effect transistors which rely on an electrically insulating pad of material between a gate and a doped semiconducting region. Device performance has typically been improved by reducing the thickness of a silicon oxide insulating pad. As the thickness approached 5–10 nm, leakage became a problem and alternate materials were necessary to increase the thickness while retaining the switching speed. Materials having larger dielectric constants enable thicker films to be used for this purpose while retaining fast reaction of the transistor. For example, a high-κ material with dielectric constant of 39 (compared to 3.9 for silicon oxide) can be made ten times thicker than silicon oxide which helps to reduce the leakage of electrons across the dielectric pad, while achieving the same capacitance. Commonly used high-κ gate dielectrics include hafnium oxide and more recently aluminum oxide for gate-all-around devices. \n\nThe EOT definition is useful to quickly compare different dielectric materials to the industry standard silicon oxide dielectric, as:\n"}
{"id": "1048452", "url": "https://en.wikipedia.org/wiki?curid=1048452", "title": "Flash powder", "text": "Flash powder\n\nFlash powder is a pyrotechnic composition, a mixture of oxidizer and metallic fuel, which burns quickly and if confined produces a loud report. It is widely used in theatrical pyrotechnics and fireworks (namely salutes, e.g., cherry bombs, M-80s, firexploderss, and cap gun shots) and was once used for flashes in photography.\n\nDifferent varieties of flash powder are made from different compositions; most common are potassium perchlorate and aluminium powder. Sometimes, sulfur is included in the mixture to increase the sensitivity. Early formulations used potassium chlorate instead of potassium perchlorate.\n\nFlash powder compositions are also used in military pyrotechnics when production of large amount of noise, light, or infrared radiation is required, e.g., missile decoy flares and stun grenades.\n\nLycopodium powder is a yellow-tan dust-like powder historically used as a flash powder. Today, the principal use of the powder is to create flashes or flames that are large and impressive but relatively easy to manage safely in magic acts and for cinema and theatrical special effects.\n\nNormally, flash powder mixtures are compounded to achieve a particular purpose. These mixtures range from extremely fast-burning mixtures designed to produce a maximum audio report, to mixtures designed to burn slowly and provide large amounts of illumination, to mixtures that were formerly used in photography.\n\nBecause of the above-mentioned instability, the combination of aluminium powder and potassium chlorate is a poor choice for flash powder that is to be stored for more than a very short period. For that reason, it has been largely replaced by the potassium perchlorate mixtures. Chlorate mixes are used when cost is the overriding concern because potassium chlorate is less expensive than perchlorate. It is critically important to exclude sulfur and any acidic components from these mixtures. Sometimes a few percent of bicarbonate or carbonate buffer is added to the mixture to ensure the absence of acidic impurities.\n\nThe composition is approximately 70% KClO : 30% Al by weight for the reactants of the above stoichiometrically balanced equation.\n\nThis composition, usually in a ratio of 5 parts potassium nitrate, to 3 parts aluminium powder, to 2 parts sulfur, is especially popular with hobbyists. It is not very quick-burning, unless exceptionally fine ingredients are used. Although it incorporates sulfur, it is in fact fairly stable, sustaining multiple hits from a hammer onto a hard surface. Adding 2% of its weight with boric acid is reputed to significantly increase stability and shelf life, through resistance to dampening through ambient humidity. Other ratios such as 6 KNO3/3 Al/2 S and 5 KNO3/2 Al/3 S also exist and work. All ratios have similar burn times and strength, although 5 KNO3/3 Al/2 S seems to be dominant.\n\nThe composition is approximately 59% KNO : 31.6% Al : 9.4% S by weight for the reactants of the above stoichiometrically balanced equation.\n\nFor best results, \"German Dark\" aluminium should be used, with airfloat sulfur, and finely ball milled pure potassium nitrate. The finished mixture should never be ball milled together.\n\nAluminium powder and potassium perchlorate are the only two components of the pyrotechnic industry standard flash powder. It provides a great balance of stability and power, and is the composition used in most commercial exploding fireworks.\n\naka A B mixture\n\nThe balanced equation for the reaction is:\n\nA ratio of seven parts potassium perchlorate to three parts dark pyro aluminium is the composition used by most pyrotechnicians. The stoichiometric ratio is 34.2% aluminum by mass to 65.8% perchlorate by mass. \n\nFor best results, the aluminium powder should be \"Dark Pyro\" grade, with a flake particle shape, and a particle size of fewer than 10 micrometres. The KClO should be in powder form, free from clumps. It can be sieved through a screen, if necessary, to remove any clumps prior to use. The particle size of the perchlorate is not as critical as that of the aluminium component, as much less energy is required to decompose the KClO than is needed to melt the aluminium into the liquid state required for the reaction.\n\nAlthough this composition is fairly insensitive, it should be treated with care and respect. Hobbyist pyrotechnicians usually use a method called \"diapering\", in which the materials are poured separately onto a large piece of paper, which is then alternately lifted at each corner to roll the composition over itself and mix the components. Some amateur pyrotechnicians choose to mix the composition by shaking in a closed paper container, as this is much quicker and more effective than diapering. One method of mixing flash is to put the components in the final device and handling the device will mix the flash powder. Paper/cardboard is chosen over other materials such as plastic as a result of its favorable triboelectric properties.\n\nLarge quantities should never be mixed in a single batch. Large quantities are not only more difficult to handle safely, but they place innocent bystanders within the area at risk. In the event of accidental ignition, debris from a multiple-pound flash powder explosion can be thrown hundreds of feet with sufficient force to kill or injure. (Note: 25 grams of mixture is enough to explode in open air without constraint other than air pressure.)\n\nNo matter the quantity, care must always be taken to prevent any electrostatic discharge or friction during mixing or handling, as these may cause accidental ignition.\n\nAnother flash composition common among amateurs consists of magnesium powder and potassium nitrate. Other metal nitrates have been used, including barium and strontium nitrates. Compositions using nitrate and magnesium metal have been used as photographic flash powders almost since the invention of photography. Potassium nitrate/magnesium flash powder should be mixed and used immediately and not stored due to its tendency of self-ignition.\n\nIf magnesium isn't a very fine powder it can be passivated with linseed oil or potassium dichromate.The passivated magnesium flash powder is stable and safe to store.\n\nThe composition is approximately 50% KNO3 : 50% Mg by weight for the reactants of the above stoichiometrically balanced equation. Below is the same reaction but invlolving barium nitrate.\n\nMixtures designed to make reports are substantially different from mixtures designed for illumination. A stoichiometric ratio of three parts KNO to two parts Mg is close to ideal and provides the most rapid burn. The magnesium powder should be smaller than 200 mesh, though up to 100 mesh will work. The potassium nitrate should be impalpable dust. This mixture is popular in amateur pyrotechnics because it is insensitive and relatively safe as such things go.\n\nFor photographic use, mixtures containing magnesium and nitrates are made much more fuel rich. The excess magnesium is volatilized by the reaction and burns in air providing additional light. In addition, the higher concentration of fuel results in a slower burn, providing more of a \"poof\" and less of a \"bang\" when ignited. A formula from 1917 specifies 5 parts of magnesium to 6 parts of barium nitrate for a stoichiometry of nine parts fuel to one part oxidizer. Modern recreations of photographic flash powders may avoid the use of barium salts because of their toxic nature. A mixture of five parts 80 mesh magnesium to one part of potassium nitrate provides a good white flash without being too violent. Fuel rich flash powders are also used in theatrical flash pots.\n\nMagnesium based compositions degrade over long periods, as magnesium does not form a passivating oxide coating, meaning the metallic Mg will slowly react with atmospheric oxygen and moisture. As written above,Mg can be passivated. In military pyrotechnics involving magnesium fuels, external oxygen can be excluded by using hermetically sealed canisters. Commercial photographic flash powders are sold as two-part mixtures, to be combined immediately before use.\n\nA flash composition designed specifically to generate flares that are exceptionally bright in the infrared portion of the spectrum use a mixture of pyro-grade magnesium and powdered polytetrafluoroethylene. These flares are used as decoys from aircraft that might be subject to heat-seeking missile fire.\n\nThis mixture, and similar mixtures sometimes containing pyro aluminium have been used since the early 1900s for small \"Black Cat\" style paper firecrackers. Its extremely low cost makes it popular among manufacturers of low-grade fireworks in China. Like all mixtures containing chlorates, it is extremely sensitive to friction, impact and ESD, and is considered unsafe in pyrotechnic devices that contain more than a few tens of milligrams of the mixture.\n\nThis mixture is not highly energetic, and in at least some parts of the United States, firecrackers containing 50 mg or less of this mixture are legal as consumer fireworks.\n\nFlash powders even within intended usages often release explosive force of deadly capacity. Nearly all widely used flash powder mixtures are sensitive to shock, friction and electrostatic discharge. In certain mixtures, it is not uncommon for this sensitivity to spontaneously change over time, or due to change in the environment, or to other unknowable factors in either the original manufacturing or in real-world storage. Additionally, accidental contaminants such as strong acids or sulfur compounds can sensitise them even more. Because flash powder mixtures are so easy to initiate, there is potentially a high risk of accidental explosions which can inflict severe blast/fragmentation injuries, e.g. blindness, explosive amputation, permanent maiming, or disfigurement. Fatalities have occurred. The various flash powder compositions should therefore not be handled by anyone who is unfamiliar with their properties, or the handling techniques required to maintain safety. Flash powder and flash powder devices pose exceptionally high risks to children, who typically cannot understand the danger and may be less adept with safe handling techniques. As a result, children tend to suffer more severe injuries than adults.\n\nFlash powders—especially those that use chlorate—are often highly sensitive to friction, heat/flame and static electricity. A spark of as little as 0.1–10 millijoules can set off certain mixtures. Certain formulations prominent in the underground press contain both sulfur and potassium chlorate. These mixtures are especially shock and friction sensitive and in many applications should be considered unpredictable. Modern pyrotechnic practices call for never using sulfur in a mix containing chlorate salts.\n\nSome flash powder formulations (those that use single-digit micrometre flake aluminium powder or fine magnesium powder as their fuel) can self-confine and explode in small quantities. This makes flash powder dangerous to handle, as it can cause severe hearing damage and amputation injury even when sitting in the open. Self-confinement occurs when the mass of the pile provides sufficient inertia to allow high pressure to build within it as the mixture reacts. This is referred to as 'inertial confinement', and it is not to be confused with a detonation.\n\nFlash powder of any formulation should not be mixed in large quantities by amateur pyrotechnicians. Beginners should start with sub-gram quantities, and refrain from making large devices. Flash powder should only be made at the site at which it will be used. Additionally, the mixture should be made immediately before use. When mixed, the transportation, storage, usage, various possession, and illegal \"firearms\" laws (including felonies) may come into effect that do not apply to the unmixed or pre-assembled components.\n\n\n"}
{"id": "1650455", "url": "https://en.wikipedia.org/wiki?curid=1650455", "title": "Galvanoluminescence", "text": "Galvanoluminescence\n\nGalvanoluminescence Is the emission of light produced by the passage of an electric current through an appropriate electrolyte in which an electrode, made of certain metals such as aluminium or tantalum, has been immersed. An example being the electrolysis of sodium bromide (NaBr).\n"}
{"id": "24405589", "url": "https://en.wikipedia.org/wiki?curid=24405589", "title": "Hardstone", "text": "Hardstone\n\nHardstone is an unscientific term, mostly encountered in the decorative arts or archaeology, that has a similar meaning to semi-precious stones, or gemstones. Very hard building stones, such as granite, are not included in the term in this sense; only stones which are fairly hard, and regarded as attractive - in effect ones which could be used in jewellery. Hardstone carving is the three-dimensional carving for artistic purposes of semi-precious stones such as jade, agate, onyx, rock crystal, sard or carnelian, and a general term for an object made in this way. Two-dimensional inlay techniques for floors, furniture and walls include pietre dure, opus sectile (Ancient Roman), and medieval Cosmatesque work - these typically inlay hardstone pieces into a background of marble or some other building stone. \n\nThe definition of \"hardstone\" is not very rigid, but excludes \"soft\" stones such as soapstone (steatite) and minerals such as alabaster, both widely used for carving. Hard organic minerals such as amber and jet are included, as well as the mineraloid obsidian. Geologically speaking, most of the gemstones carved in the West are varieties of quartz, including: chalcedony, agate, amethyst, sard, onyx, carnelion, heliotrope, jasper and quartz in its uncoloured form, known as rock crystal. On the Mohs scale of mineral hardness, quartz rates no higher than a 7, less for types with impurities. Stones typically used for buildings and large sculpture are not often used for small objects such as vessels, although this does occur.\n\nThe term is derived as a literal translation of the Italian plural pietre dure= \"hard stones\", which in Italian covers all hardstone carving. The semi-anglicized singular pietra dura is used in English for multi-coloured stone inlay work, in fact using marbles as much as semi-precious stones, which was a Florentine speciality from the Renaissance onwards. \"Pietre dure\" is sometimes used (normally italicized) for Italian or European vessels and small sculptures of the same period. \n"}
{"id": "1867864", "url": "https://en.wikipedia.org/wiki?curid=1867864", "title": "Heating degree day", "text": "Heating degree day\n\nHeating degree day (HDD) is a measurement designed to quantify the demand for energy needed to heat a building. HDD is derived from measurements of outside air temperature. The heating requirements for a given building at a specific location are considered to be directly proportional to the number of HDD at that location. A similar measurement, cooling degree day (CDD), reflects the amount of energy used to cool a home or business.\n\nHeating degree days are defined relative to a base temperature—the outside temperature above which a building needs no heating. Base temperatures may be defined for a particular building as a function of the temperature that the building is heated to, or it may be defined for a country or region for example. In the latter case, building standards or conventions may exist for the temperature threshold. These include:\n\nThe base temperature does not necessarily correspond to the building mean internal temperature, as standards may consider mean building insulation levels and internal gains to determine an average external temperature at which heating will be required. Base temperatures of 16°C, 18°C, and 19°C (61, 65, 66 °F) are also used. The variation in choice of base temperature implies that HDD values cannot always be compared - care must be take to ensure that only HDDs with equal base temperatures are compared.\n\nThere are a number of ways in which HDD can be calculated: the more detailed a record of temperature data, the more accurate the HDD that can be calculated. HDD are often calculated using simple approximation methods that use daily temperature readings instead of more detailed temperature records such as half-hourly readings. One popular approximation method is to take the average temperature on any given day, and subtract it from the base temperature. If the value is less than or equal to zero, that day has zero HDD. But if the value is positive, that number represents the number of HDD on that day. This method works satisfactorily if the outside air temperature does not exceed the base temperature. In climates where this is likely to occur from time to time, there are refinements to the simple calculation which allow some 'credit' for the period of the day when the air is warm enough for heating to be unnecessary. This more accurate algorithm enables results to be computed in temperate climates (maritime as well as continental) throughout the year (not just during a defined heating season) and on a weekly as well as monthly basis.\n\nHDD can be added over periods of time to provide a rough estimate of seasonal heating requirements. In the course of a heating season, for example, the number of HDD for New York City is 5,050 whereas that for Barrow, Alaska is 19,990. Thus, one can say that, for a given home of similar structure and insulation, around four times the energy would be required to heat the home in Barrow than in New York. Likewise, a similar home in Miami, Florida, whose heating degree days for the heating season is 500, would require around one tenth of the energy required to heat the house in New York City.\n\nHowever, this is a theoretical approach as the level of insulation of a building affects the demand for heating. For example, temperatures often drop below the base temperature during night (daily low temperature in diurnal variation), but because of insulation, heating is unnecessary. In the end of spring and in the beginning of fall or in the winter depending on the climate, sufficient insulation keeps the indoor temperature higher than the outdoor temperature with little or no heating. For example in southern California, during winter heating is not necessary in Los Angeles and San Diego if the insulation is sufficient to take into account the colder night temperatures. Also, buildings include thermal mass such as concrete, that is able to store energy of the sun absorbed in daytime. Thus, even if the heating degree days indicate a demand for heating sufficient insulation of a building can make heating unnecessary.\n\nHDD provides a simple metric for quantifying the amount of heating that buildings in a particular location need over a certain period (e.g. a particular month or year). In conjunction with the average U-value for a building they provide a means of roughly estimating the amount of energy required to heat the building over that period.\n\nOne HDD means that the temperature conditions outside the building were equivalent to being below a defined threshold comfort temperature inside the building by one degree for one day. Thus heat has to be provided inside the building to maintain thermal comfort.\nSay we are given the number of heating degree days D in one year and we wish to calculate the energy required by a building. We know that heat needs to be provided at the rate at which it is being lost to the environment. This can be calculated as the sum of the heat losses per degree of each element of the buildings' thermal envelope (such as windows, walls, and roof) or as the average U-value of the building multiplied by the area of the thermal envelope of the building, or quoted directly for the whole building. This gives the buildings' specific heat loss rate P, generally given in Watts per Kelvin (W/K). Total energy in kWh is then given by:\n\nNote that as total energy consumption is in kWh and heating degree days are [no. days×degrees] we must convert W/K into kWh per degree per day by dividing by 1000 (to convert W to kW), and multiplying by 24 hours in a day (1 kW = 1kWh per hour). Since a one degree Celsius temperature change and a one kelvin change in absolute temperature are the same, these cancel and no conversion is required.\n\nExample: For a typical New York City winter day with high of 40 °F and low of 30 °F, the average temperature is likely to be around 35 °F. For such a day we can approximate the HDD as (65 - 35) = 30. A month of thirty similar days might accumulate 900 HDD. A year (including summer average temperatures above 70 °F) might accumulate an annual 5000 HDD.\n\nCalculations using HDD have several problems. Heat requirements are not linear with temperature, and heavily insulated buildings have a lower \"balance point\". The amount of heating and cooling required depends on several factors besides outdoor temperature: How well insulated a particular building is, the amount of solar radiation reaching the interior of a house, the number of electrical appliances running (e.g. computers raise their surrounding temperature) the amount of wind outside, and what temperature the occupants find comfortable. Another important factor is the amount of relative humidity indoors; this is important in determining how comfortable an individual will be. Other variables such as precipitation, cloud cover, heat index, building albedo, and snow cover can also alter a building's thermal response.\n\nAnother problem with HDD is that care needs to be taken if they are to be used to compare climates internationally, because of the different baseline temperatures used as standard in different countries and the use of the Fahrenheit scale in the US and the Celsius scale almost everywhere else. This is further compounded by the use of different approximation methods in different countries.\n\nTo convert °F HDD to °C HDD:\n\nTo convert °C HDD to °F HDD:\n\nNote that, because HDD are relative to a base temperature (as opposed to being relative to zero), it is incorrect to add or subtract 32 when converting degree days from Celsius to Fahrenheit or vice versa.\n\n\n\n"}
{"id": "20612358", "url": "https://en.wikipedia.org/wiki?curid=20612358", "title": "Hydrogen silsesquioxane", "text": "Hydrogen silsesquioxane\n\nHydrogen silsesquioxane (HSQ) is class of inorganic compounds with the chemical formula [HSiO]. Such clusters are specific representatives of the family of silsesquioxanes with the formula [RSiO] (R = alkyl, halide, alkoxide, etc.). The most widely studied member of the hydrogen silsesquioxanes is the cubic cluster HSiO.\n\nHSQ has been used in photolithography and Electron-beam lithography due to the fine resolution achievable (~10 nm). Thickness of the coated resist has been reported to play a major role in the achievable resolution.\n\nCross-linking of the HSQ can is achieved through exposure to e-beam or EUV radiation with wavelengths shorter than 157 nm.\n\nA collection of practical knowledge for using HSQ is provided by Georgia Tech.\n"}
{"id": "2644317", "url": "https://en.wikipedia.org/wiki?curid=2644317", "title": "Icing conditions", "text": "Icing conditions\n\nIn aviation, icing conditions are those atmospheric conditions that can lead to the formation of water ice on the surfaces of an aircraft, or within the engine as carburetor icing. Inlet icing is another engine-related danger, often occurring in jet aircraft. These icing phenomena do not necessarily occur together. Many aircraft, especially general aviation aircraft, are not certified for flight into \"known icing\"—icing conditions certain or likely to exist, based on pilot reports, observations, and forecasts.\n\nIcing conditions exist when the air contains droplets of supercooled liquid water; icing conditions are characterized quantitatively by the average droplet size, the liquid water content and the air temperature. These parameters affect the extent and speed that characterize the formation of ice on an aircraft. Federal Aviation Regulations contain a definition of icing conditions that some aircraft are certified to fly into. So-called SLD, or supercooled large droplet, conditions are those that exceed that specification and represent a particular hazard to aircraft.\n\nQualitatively, pilot reports indicate icing conditions in terms of their effect upon the aircraft, and will be dependent upon the capabilities of the aircraft. Different aircraft may report the same quantitative conditions as different levels of icing as a result.\n\nThe wing will ordinarily stall at a lower angle of attack, and thus a higher airspeed, when contaminated with ice. Even small amounts of ice will have an effect, and if the ice is rough, it can be a large effect. Thus an increase in approach speed is advisable if ice remains on the wings. How much of an increase depends on both the aircraft type and amount of ice. Stall characteristics of an aircraft with ice contaminated wings will be degraded, and serious roll control problems are not unusual. The ice accretion may be asymmetric between the two wings. Also, the outer part of a wing, which is ordinarily thinner and thus a better collector of ice, may stall first rather than last.\n\nSeveral methods exist to reduce the dangers of icing. The first, and simplest, is to avoid icing conditions altogether, but for many flights this is not practical.\n\nIf ice (or other contaminants) are present on an aircraft prior to takeoff, they must be removed from critical surfaces. Removal can take many forms:\nAll of these methods remove existing contamination, but provide no practical protection in icing conditions. If icing conditions exist, or are expected before takeoff, then anti-icing fluids are used. These are thicker than deicing fluids and resist the effects of snow and rain for some time. They are intended to shear off the aircraft during takeoff and provide no inflight protection.\n\nTo protect an aircraft against icing in-flight, various forms of anti-icing or deicing are used:\nIn all these cases usually only critical aircraft surfaces and components are protected. In particular only the leading edge of a wing is usually protected.\n\nCarburetor heat is applied to carbureted engines to prevent and clear icing. Fuel-injected engines are not susceptible to carburetor icing but can suffer from blocked inlets. In these engines an alternate air source is often available.\n\nNote there is a difference between deicing and anti-icing. Deicing refers to the removal of ice from the airframe; anti-icing refers to the prevention of ice accumulating on the airframe.\n\n\n"}
{"id": "14750", "url": "https://en.wikipedia.org/wiki?curid=14750", "title": "Iodine", "text": "Iodine\n\nIodine is a chemical element with symbol I and atomic number 53. The heaviest of the stable halogens, it exists as a lustrous, purple-black non-metallic solid at standard conditions that sublimes readily to form a violet gas. The elemental form was discovered by the French chemist Bernard Courtois in 1811. It was named two years later by Joseph Louis Gay-Lussac from this property, after the Greek \"ἰώδης\" \"violet-coloured\".\n\nIodine occurs in many oxidation states, including iodide (I), iodate (), and the various periodate anions. It is the least abundant of the stable halogens, being the sixty-first most abundant element. It is even less abundant than the so-called rare earths. It is the heaviest essential mineral nutrient. Iodine is essential in the synthesis of thyroid hormones. Iodine deficiency affects about two billion people and is the leading preventable cause of intellectual disabilities.\n\nThe dominant producers of iodine today are Chile and Japan. Iodine and its compounds are primarily used in nutrition. Due to its high atomic number and ease of attachment to organic compounds, it has also found favour as a non-toxic radiocontrast material. Because of the specificity of its uptake by the human body, radioactive isotopes of iodine can also be used to treat thyroid cancer. Iodine is also used as a catalyst in the industrial production of acetic acid and some polymers.\n\nIn 1811, iodine was discovered by French chemist Bernard Courtois, who was born to a manufacturer of saltpeter (an essential component of gunpowder). At the time of the Napoleonic Wars, saltpeter was in great demand in France. Saltpeter produced from French nitre beds required sodium carbonate, which could be isolated from seaweed collected on the coasts of Normandy and Brittany. To isolate the sodium carbonate, seaweed was burned and the ash washed with water. The remaining waste was destroyed by adding sulfuric acid. Courtois once added excessive sulfuric acid and a cloud of purple vapour rose. He noted that the vapour crystallised on cold surfaces, making dark crystals. Courtois suspected that this material was a new element but lacked funding to pursue it further.\n\nCourtois gave samples to his friends, Charles Bernard Desormes (1777–1838) and Nicolas Clément (1779–1841), to continue research. He also gave some of the substance to chemist Joseph Louis Gay-Lussac (1778–1850), and to physicist André-Marie Ampère (1775–1836). On 29 November 1813, Desormes and Clément made Courtois' discovery public. They described the substance to a meeting of the Imperial Institute of France. On 6 December, Gay-Lussac announced that the new substance was either an element or a compound of oxygen. It was Gay-Lussac who suggested the name \"iode\", from the Greek word (\"ioeidēs\") for violet (because of the colour of iodine vapor). Ampère had given some of his sample to English chemist Humphry Davy (1778–1829), who experimented on the substance and noted its similarity to chlorine. Davy sent a letter dated 10 December to the Royal Society of London stating that he had identified a new element. Arguments erupted between Davy and Gay-Lussac over who identified iodine first, but both scientists acknowledged Courtois as the first to isolate the element.\n\nIodine is the fourth halogen, being a member of group 17 in the periodic table, below fluorine, chlorine, and bromine; it is the heaviest stable member of its group. (The scarce and fugitive fifth halogen, the radioactive astatine, is not well-studied due to its expense and inaccessibility in large quantities, but appears to show various unusual properties due to relativistic effects.) Iodine has an electron configuration of [Kr]4d5s5p, with the seven electrons in the fifth and outermost shell being its valence electrons. Like the other halogens, it is one electron short of a full octet and is hence a strong oxidising agent, reacting with many elements in order to complete its outer shell, although in keeping with periodic trends, it is the weakest oxidising agent among the stable halogens: it has the lowest electronegativity among them, just 2.66 on the Pauling scale (compare fluorine, chlorine, and bromine at 3.98, 3.16, and 2.96 respectively; astatine continues the trend with an electronegativity of 2.2). Elemental iodine hence forms diatomic molecules with chemical formula I, where two iodine atoms share a pair of electrons in order to each achieve a stable octet for themselves; at high temperatures, these diatomic molecules reversibly dissociate a pair of iodine atoms. Similarly, the iodide anion, I, is the strongest reducing agent among the stable halogens, being the most easily oxidised back to diatomic I. (Astatine goes further, being indeed unstable as At and readily oxidised to At or At, although the existence of At is not settled.)\n\nThe halogens darken in colour as the group is descended: fluorine is a very pale yellow gas, chlorine is greenish-yellow, and bromine is a reddish-brown volatile liquid. Iodine conforms to the prevailing trend, being a shiny black crystalline solid that melts at 114 °C and boils at 183 °C to form a violet gas. This trend occurs because the wavelengths of visible light absorbed by the halogens increase down the group (though astatine may not conform to it, depending on how metallic it turns out to be). Specifically, the violet colour of iodine gas results from the electron transition between the highest occupied antibonding \"π\" molecular orbital and the lowest vacant antibonding \"σ\" molecular orbital.\n\nElemental iodine is slightly soluble in water, with one gram dissolving in 3450 ml at 20 °C and 1280 ml at 50 °C; potassium iodide may be added to increase solubility via formation of triiodide ions, among other polyiodides. Nonpolar solvents such as hexane and carbon tetrachloride provide a higher solubility. Polar solutions, such as aqueous solutions, are brown, reflecting the role of these solvents as Lewis bases; on the other hand, nonpolar solutions are violet, the color of iodine vapour. Charge-transfer complexes form when iodine is dissolved in polar solvents, hence changing the colour. Iodine is violet when dissolved in carbon tetrachloride and saturated hydrocarbons but deep brown in alcohols and amines, solvents that form charge-transfer adducts.\n\nThe melting and boiling points of iodine are the highest among the halogens, conforming to the increasing trend down the group, since iodine has the largest electron cloud among them that is the most easily polarised, resulting in its molecules having the strongest van der Waals interactions among the halogens. Similarly, iodine is the least volatile of the halogens. Because it has the largest atomic radius among the halogens, iodine has the lowest first ionisation energy, lowest electron affinity, lowest electronegativity and lowest reactivity of the halogens.\nThe interhalogen bond in diiodine is the weakest of all the halogens. As such, 1% of a sample of gaseous iodine at atmospheric pressure is dissociated into iodine atoms at 575 °C. Temperatures greater than 750 °C are required for fluorine, chlorine, and bromine to dissociate to a similar extent. Most bonds to iodine are weaker than the analogous bonds to the lighter halogens. Gaseous iodine is composed of I molecules with an I–I bond length of 266.6 pm. The I–I bond is one of the longest single bonds known. It is even longer (271.5 pm) in solid orthorhombic crystalline iodine, which has the same crystal structure as chlorine and bromine. (The record is held by iodine's neighbour xenon: the Xe–Xe bond length is 308.71 pm.) As such, within the iodine molecule, significant electronic interactions occur with the two next-nearest neighbours of each atom, and these interactions give rise, in bulk iodine, to a shiny appearance and semiconducting properties. Iodine is a two-dimensional semiconductor with a band gap of 1.3 eV (125 kJ/mol): it is a semiconductor in the plane of its crystalline layers and an insulator in the perpendicular direction.\n\nOf the thirty-seven known isotopes of iodine, only one occurs in nature, iodine-127. The others are radioactive and have half-lives too short to be primordial. As such, iodine is monoisotopic and its atomic weight is known to great precision, as it is a constant of nature.\n\nThe longest-lived of the radioactive isotopes of iodine is iodine-129, which has a half-life of 15.7 million years, decaying via beta decay to stable xenon-129. Some iodine-129 was formed along with iodine-127 before the formation of the Solar System, but it has by now completely decayed away, making it an extinct radionuclide that is nevertheless still useful in dating the history of the early Solar System or very old groundwaters, due to its mobility in the environment. Its former presence may be determined from an excess of its daughter xenon-129. Traces of iodine-129 still exist today, as it is also a cosmogenic nuclide, formed from cosmic ray spallation of atmospheric xenon: these traces make up 10 to 10 of all terrestrial iodine. It also occurs from open-air nuclear testing, and is not hazardous because of its incredibly long half-life, the longest of all fission products. At the peak of thermonuclear testing in the 1960s and 1970s, iodine-129 still made up only about 10 of all terrestrial iodine. Excited states of iodine-127 and iodine-129 are often used in Mössbauer spectroscopy.\n\nThe other iodine radioisotopes have much shorter half-lives, no longer than days. Some of them have medical applications involving the thyroid gland, where the iodine that enters the body is stored and concentrated. Iodine-123 has a half-life of thirteen hours and decays by electron capture to tellurium-123, emitting gamma radiation; it is used in nuclear medicine imaging, including single photon emission computed tomography (SPECT) and X-ray computed tomography (X-Ray CT) scans. Iodine-125 has a half-life of fifty-nine days, decaying by electron capture to tellurium-125 and emitting low-energy gamma radiation; the second-longest-lived iodine radioisotope, it has uses in biological assays, nuclear medicine imaging and in radiation therapy as brachytherapy to treat a number of conditions, including prostate cancer, uveal melanomas, and brain tumours. Finally, iodine-131, with a half-life of eight days, beta decays to an excited state of stable xenon-131 that then converts to the ground state by emitting gamma radiation. It is a common fission product and thus is present in high levels in radioactive fallout. It may then be absorbed through contaminated food, and will also accumulate in the thyroid. As it decays, it may cause damage to the thyroid. The primary risk from exposure to high levels of iodine-131 is the chance occurrence of radiogenic thyroid cancer in later life. Other risks include the possibility of non-cancerous growths and thyroiditis.\n\nThe usual means of protection against the negative effects of iodine-131 is by saturating the thyroid gland with stable iodine-127 in the form of potassium iodide tablets, taken daily for optimal prophylaxis. However, iodine-131 may also be used for medicinal purposes in radiation therapy for this very reason, when tissue destruction is desired after iodine uptake by the tissue. Iodine-131 is also used as a radioactive tracer.\n\nThough it is the least reactive of the halogens, iodine is still one of the more reactive elements. For example, while chlorine gas will halogenate carbon monoxide, nitric oxide, and sulfur dioxide (to phosgene, nitrosyl chloride, and sulfuryl chloride respectively), iodine will not do so. Furthermore, iodination of metals tends to result in lower oxidation states than chlorination or bromination; for example, rhenium metal reacts with chlorine to form rhenium hexachloride, but with bromine it forms only rhenium pentabromide and iodine can achieve only rhenium tetraiodide. By the same token, however, since iodine has the lowest ionisation energy among the halogens and is the most easily oxidised of them, it has a more significant cationic chemistry and its higher oxidation states are rather more stable than those of bromine and chlorine, for example in iodine heptafluoride.\n\nThe simplest compound of iodine is hydrogen iodide, HI. It is a colourless gas that reacts with oxygen to give water and iodine. Although it is useful in iodination reactions in the laboratory, it does not have large-scale industrial uses, unlike the other hydrogen halides. Commercially, it is usually made by reacting iodine with hydrogen sulfide or hydrazine:\nAt room temperature, it is a colourless gas, like all of the hydrogen halides except hydrogen fluoride, since hydrogen cannot form strong hydrogen bonds to the large and only mildly electronegative iodine atom. It melts at −51.0 °C and boils at −35.1 °C. It is an endothermic compound that can exothermically dissociate at room temperature, although the process is very slow unless a catalyst is present: the reaction between hydrogen and iodine at room temperature to give hydrogen iodide does not proceed to completion. The H–I bond dissociation energy is likewise the smallest of the hydrogen halides, at 295 kJ/mol.\n\nAqueous hydrogen iodide is known as hydroiodic acid, which is a strong acid. Hydrogen iodide is exceptionally soluble in water: one litre of water will dissolve 425 litres of hydrogen iodide, and the saturated solution has only four water molecules per molecule of hydrogen iodide. Commercial so-called \"concentrated\" hydroiodic acid usually contains 48–57% HI by mass; the solution forms an azeotrope with boiling point 126.7 °C at 56.7 g HI per 100 g solution. Hence hydroiodic acid cannot be concentrated past this point by evaporation of water.\n\nUnlike hydrogen fluoride, anhydrous liquid hydrogen iodide is difficult to work with as a solvent, because its boiling point is low, it has a small liquid range, its dielectric constant is low and it does not dissociate appreciably into HI and ions – the latter, in any case, are much less stable than the bifluoride ions () due to the very weak hydrogen bonding between hydrogen and iodine, though its salts with very large and weakly polarising cations such as Cs and (R = Me, Et, Bu) may still be isolated. Anhydrous hydrogen iodide is a poor solvent, able to dissolve only small molecular compounds such as nitrosyl chloride and phenol, or salts with very low lattice energies such as tetraalkylammonium halides.\n\nNearly all elements in the periodic table form binary iodides. The exceptions are decidedly in the minority and stem in each case from one of three causes: extreme inertness and reluctance to participate in chemical reactions (the noble gases); extreme nuclear instability hampering chemical investigation before decay and transmutation (many of the heaviest elements beyond bismuth); and having an electronegativity higher than iodine's (oxygen, nitrogen, and the first three halogens), so that the resultant binary compounds are formally not iodides but rather oxides, nitrides, or halides of iodine. (Nonetheless, nitrogen triiodide is named as an iodide as it is analogous to the other nitrogen trihalides.)\n\nGiven the large size of the iodide anion and iodine's weak oxidising power, high oxidation states are difficult to achieve in binary iodides, the maximum known being in the pentaiodides of niobium, tantalum, and protactinium. Iodides can be made by reaction of an element or its oxide, hydroxide, or carbonate with hydroiodic acid, and then dehydrated by mildly high temperatures combined with either low pressure or anhydrous hydrogen iodide gas. These methods work best when the iodide product is stable to hydrolysis; otherwise, the possibilities include high-temperature oxidative iodination of the element with iodine or hydrogen iodide, high-temperature iodination of a metal oxide or other halide by iodine, a volatile metal halide, carbon tetraiodide, or an organic iodide. For example, molybdenum(IV) oxide reacts with aluminium(III) iodide at 230 °C to give molybdenum(II) iodide. An example involving halogen exchange is given below, involving the reaction of tantalum(V) chloride with excess aluminium(III) iodide at 400 °C to give tantalum(V) iodide:\n\nLower iodides may be produced either through thermal decomposition or disproportionation, or by reducing the higher iodide with hydrogen or a metal, for example:\n\nMost of the iodides of the pre-transition metals (groups 1, 2, and 3, along with the lanthanides and actinides in the +2 and +3 oxidation states) are mostly ionic, while nonmetals tend to form covalent molecular iodides, as do metals in high oxidation states from +3 and above. Ionic iodides MI tend to have the lowest melting and boiling points among the halides MX of the same element, because the electrostatic forces of attraction between the cations and anions are weakest for the large iodide anion. In contrast, covalent iodides tend to instead have the highest melting and boiling points among the halides of the same element, since iodine is the most polarisable of the halogens and, having the most electrons among them, can contribute the most to van der Waals forces. Naturally, exceptions abound in intermediate iodides where one trend gives way to the other. Similarly, solubilities in water of predominantly ionic iodides (e.g. potassium and calcium) are the greatest among ionic halides of that element, while those of covalent iodides (e.g. silver) are the lowest of that element. In particular, silver iodide is very insoluble in water and its formation is often used as a qualitative test for iodine.\n\nThe halogens form many binary, diamagnetic interhalogen compounds with stoichiometries XY, XY, XY, and XY (where X is heavier than Y), and iodine is no exception. Iodine forms all three possible diatomic interhalogens, a trifluoride and trichloride, as well as a pentafluoride and, exceptionally among the halogens, a heptafluoride. Numerous cationic and anionic derivatives are also characterised, such as the wine-red or bright orange compounds of and the dark brown or purplish black compounds of ICl. Apart from these, some pseudohalides are also known, such as cyanogen iodide (ICN), iodine thiocyanate (ISCN), and iodine azide (IN).\nIodine monofluoride (IF) is unstable at room temperature and disproportionates very readily and irreversibly to iodine and iodine pentafluoride, and thus cannot be obtained pure. It can be synthesised from the reaction of iodine with fluorine gas in trichlorofluoromethane at −45 °C, with iodine trifluoride in trichlorofluoromethane at −78 °C, or with silver(I) fluoride at 0 °C. Iodine monochloride (ICl) and iodine monobromide (IBr), on the other hand, are moderately stable. The former, a volatile red-brown compound, was discovered independently by Joseph Louis Gay-Lussac and Humphry Davy in 1813–4 not long after the discoveries of chlorine and iodine, and it mimics the intermediate halogen bromine so well that Justus von Liebig was misled into mistaking bromine (which he had found) for iodine monochloride. Iodine monochloride and iodine monobromide may be prepared simply by reacting iodine with chlorine or bromine at room temperature and purified by fractional crystallisation. Both are quite reactive and attack even platinum and gold, though not boron, carbon, cadmium, lead, zirconium, niobium, molybdenum, and tungsten. Their reaction with organic compounds depends on conditions. Iodine chloride vapour tends to chlorinate phenol and salicyclic acid, since when iodine chloride undergoes homolytic dissociation, chlorine and iodine are produced and the former is more reactive. However, iodine chloride in tetrachloromethane solution results in iodination being the main reaction, since now heterolytic fission of the I–Cl bond occurs and I attacks phenol as an electrophile. However, iodine monobromide tends to brominate phenol even in tetrachloromethane solution because it tends to dissociate into its elements in solution, and bromine is more reactive than iodine. When liquid, iodine monochloride and iodine monobromide dissociate into and anions (X = Cl, Br); thus they are significant conductors of electricity and can be used as ionising solvents.\n\nIodine trifluoride (IF) is an unstable yellow solid that decomposes above −28 °C. It is thus little-known. It is difficult to produce because fluorine gas would tend to oxidise iodine all the way to the pentafluoride; reaction at low temperature with xenon difluoride is necessary. Iodine trichloride, which exists in the solid state as the planar dimer ICl, is a bright yellow solid, synthesised by reacting iodine with liquid chlorine at −80 °C; caution is necessary during purification because it easily dissociates to iodine monochloride and chlorine and hence can act as a strong chlorinating agent. Liquid iodine trichloride conducts electricity, possibly indicating dissociation to and ions.\n\nIodine pentafluoride (IF), a colourless, volatile liquid, is the most thermodynamically stable iodine fluoride, and can be made by reacting iodine with fluorine gas at room temperature. It is a fluorinating agent, but is mild enough to store in glass apparatus. Again, slight electrical conductivity is present in the liquid state because of dissociation to and . The pentagonal bipyramidal iodine heptafluoride (IF) is an extremely powerful fluorinating agent, behind only chlorine trifluoride, chlorine pentafluoride, and bromine pentafluoride among the interhalogens: it reacts with almost all the elements even at low temperatures, fluorinates Pyrex glass to form iodine(VII) oxyfluoride (IOF), and sets carbon monoxide on fire.\n\nIodine oxides are the most stable of all the halogen oxides, because of the strong I–O bonds resulting from the large electronegativity difference between iodine and oxygen, and they have been known for the longest time. The stable, white, hygroscopic iodine pentoxide (IO) has been known since its formation in 1813 by Gay-Lussac and Davy. It is most easily made by the dehydration of iodic acid (HIO), of which it is the anhydride. It will quickly oxidise carbon monoxide completely to carbon dioxide at room temperature, and is thus a useful reagent in determining carbon monoxide concentration. It also oxidises nitrogen oxide, ethylene, and hydrogen sulfide. It reacts with sulfur trioxide and peroxydisulfuryl difluoride (SOF) to form salts of the iodyl cation, [IO], and is reduced by concentrated sulfuric acids to iodosyl salts involving [IO]. It may be fluorinated by fluorine, bromine trifluoride, sulfur tetrafluoride, or chloryl fluoride, resulting iodine pentafluoride, which also reacts with iodine pentoxide, giving iodine(V) oxyfluoride, IOF. A few other less stable oxides are known, notably IO and IO; their structures have not been determined, but reasonable guesses are I(IO) and [IO][IO] respectively.\nMore important are the four oxoacids: hypoiodous acid (HIO), iodous acid (HIO), iodic acid (HIO), and periodic acid (HIO or HIO). When iodine dissolves in aqueous solution, the following reactions occur:\n\nHypoiodous acid is unstable to disproportionation. The hypoiodite ions thus formed disproportionate immediately to give iodide and iodate:\n\nIodous acid and iodite are even less stable and exist only as a fleeting intermediate in the oxidation of iodide to iodate, if at all. Iodates are by far the most important of these compounds, which can be made by oxidising alkali metal iodides with oxygen at 600 °C and high pressure, or by oxidising iodine with chlorates. Unlike chlorates, which disproportionate very slowly to form chloride and perchlorate, iodates are stable to disproportionation in both acidic and alkaline solutions. From these, salts of most metals can be obtained. Iodic acid is most easily made by oxidation of an aqueous iodine suspension by electrolysis or fuming nitric acid. Iodate has the weakest oxidising power of the halates, but reacts the quickest.\n\nMany periodates are known, including not only the expected tetrahedral , but also square-pyramidal , octahedral orthoperiodate , [IO(OH)], [IO(OH)], and . They are usually made by oxidising alkaline sodium iodate electrochemically (with lead(IV) oxide as the anode) or by chlorine gas:\n\nThey are thermodymically and kinetically powerful oxidising agents, quickly oxidising Mn to , and cleaving glycols, α-diketones, α-ketols, α-aminoalcohols, and α-diamines. Orthoperiodate especially stabilises high oxidation states among metals because of its very high negative charge of −5. Orthoperiodic acid, HIO, is stable, and dehydrates at 100 °C in a vacuum to metaperiodic acid, HIO. Attempting to go further does not result in the nonexistent iodine heptoxide (IO), but rather iodine pentoxide and oxygen. Periodic acid may be protonated by sulfuric acid to give the cation, isoelectronic to Te(OH) and , and giving salts with bisulfate and sulfate.\n\nWhen iodine dissolves in strong acids, such as fuming sulfuric acid, a bright blue paramagnetic solution including cations is formed. A solid salt of the diiodine cation may be obtained by oxidising iodine with antimony pentafluoride:\nThe salt ISbF is dark blue, and the blue tantalum analogue ITaF is also known. Whereas the I–I bond length in I is 267 pm, that in is only 256 pm as the missing electron in the latter has been removed from an antibonding orbital, making the bond stronger and hence shorter. In fluorosulfuric acid solution, deep-blue reversibly dimerises below −60 °C, forming red rectangular diamagnetic . Other polyiodine cations are not as well-characterised, including bent dark-brown or black and centrosymmetric \"C\" green or black , known in the and salts among others.\n\nThe only important polyiodide anion in aqueous solution is linear triiodide, . Its formation explains why the solubility of iodine in water may be increased by the addition of potassium iodide solution:\nMany other polyiodides may be found when solutions containing iodine and iodide crystallise, such as , , , and , whose salts with large, weakly polarising cations such as Cs may be isolated.\n\nOrganoiodine compounds have been fundamental in the development of organic synthesis, such as in the Hofmann elimination of amines, the Williamson ether synthesis, the Wurtz coupling reaction, and in Grignard reagents.\n\nThe carbon–iodine bond is a common functional group that forms part of core organic chemistry; formally, these compounds may be thought of as organic derivatives of the iodide anion. The simplest organoiodine compounds, alkyl iodides, may be synthesised by the reaction of alcohols with phosphorus triiodide; these may then be used in nucleophilic substitution reactions, or for preparing Grignard reagents. The C–I bond is the weakest of all the carbon–halogen bonds due to the minuscule difference in electronegativity between carbon (2.55) and iodine (2.66). As such, iodide is the best leaving group among the halogens, to such an extent that many organoiodine compounds turn yellow when stored over time due to decomposition into elemental iodine; as such, they are commonly used in organic synthesis, because of the easy formation and cleavage of the C–I bond. They are also significantly denser than the other organohalogen compounds thanks to the high atomic weight of iodine. A few organic oxidising agents like the iodanes contain iodine in a higher oxidation state than −1, such as 2-iodoxybenzoic acid, a common reagent for the oxidation of alcohols to aldehydes, and iodobenzene dichloride (PhICl), used for the selective chlorination of alkenes and alkynes. One of the more well-known uses of organoiodine compounds is the so-called iodoform test, where iodoform (CHI) is produced by the exhaustive iodination of a methyl ketone (or another compound capable of being oxidised to a methyl ketone), as follows:\n\nSome drawbacks of using organoiodine compounds as compared to organochlorine or organobromine compounds is the greater expense and toxicity of the iodine derivatives, since iodine is expensive and organoiodine compounds are stronger alkylating agents. For example, iodoacetamide and iodoacetic acid denature proteins by irreversibly alkylating cysteine residues and preventing the reformation of disulfide linkages.\n\nHalogen exchange to produce iodoalkanes by the Finkelstein reaction is slightly complicated by the fact that iodide is a better leaving group than chloride or bromide. The difference is nevertheless small enough that the reaction can be driven to completion by exploiting the differential solubility of halide salts, or by using a large excess of the halide salt. In the classic Finkelstein reaction, an alkyl chloride or an alkyl bromide is converted to an alkyl iodide by treatment with a solution of sodium iodide in acetone. Sodium iodide is soluble in acetone and sodium chloride and sodium bromide are not. The reaction is driven toward products by mass action due to the precipitation of the insoluble salt.\n\nIodine is the least abundant of the stable halogens, comprising only 0.46 parts per million of Earth's crustal rocks (compare: fluorine 544 ppm, chlorine 126 ppm, bromine 2.5 ppm). Among the eighty-four elements which occur in significant quantities (elements 1–42, 44–60, 62–83, and 90–92), it ranks sixty-first in abundance. Iodide minerals are rare, and most deposits that are concentrated enough for economical extraction are iodate minerals instead. Examples include lautarite, Ca(IO), and dietzeite, 7Ca(IO)·8CaCrO. These are the minerals that occur as trace impurities in the caliche, found in Chile, whose main product is sodium nitrate. In total, they can contain at least 0.02% and at most 1% iodine by weight. Sodium iodate is extracted from the caliche and reduced to iodide by sodium bisulfite. This solution is then reacted with freshly extracted iodate, resulting in comproportionation to iodine, which may be filtered off.\n\nThe caliche was the main source of iodine in the 19th century and continues to be important today, replacing kelp (which is no longer an economically viable source), but in the late 20th century brines emerged as a comparable source. The Japanese Minami Kanto gas field east of Tokyo and the American Anadarko Basin gas field in northwest Oklahoma are the two largest such sources. The brine is hotter than 60 °C from the depth of the source. The brine is first purified and acidified using sulfuric acid, then the iodide present is oxidised to iodine with chlorine. An iodine solution is produced, but is dilute and must be concentrated. Air is blown into the solution to evaporate the iodine, which is passed into an absorbing tower where sulfur dioxide reduces the iodine. The hydrogen iodide (HI) is reacted with chlorine to precipitate the iodine. After filtering and purification the iodine is packed.\n\nThese sources ensure that Chile and Japan are the largest producers of iodine today. Alternatively, the brine may be treated with silver nitrate to precipitate out iodine as silver iodide, which is then decomposed by reaction with iron to form metallic silver and a solution of iron(II) iodide. The iodine may then be liberated by displacement with chlorine.\n\nUnlike chlorine and bromine, which have one significant main use dwarfing all others, iodine is used in many applications of varying importance. About half of all produced iodine goes into various organoiodine compounds; another 15% remains as the pure element, another 15% is used to form potassium iodide, and another 15% for other inorganic iodine compounds. The remaining 5% is for minor uses. Among the major uses of iodine compounds are catalysts, animal feed supplements, stabilisers, dyes, colourants and pigments, pharmaceutical, sanitation (from tincture of iodine), and photography; minor uses include smog inhibition, cloud seeding, and various uses in analytical chemistry.\n\nPotassium tetraiodomercurate(II), KHgI, is also known as Nessler's reagent. It is often used as a sensitive spot test for ammonia. Similarly, CuHgI is used as a precipitating reagent to test for alkaloids. The iodide and iodate anions are often used for quantitative volumetric analysis, for example in iodometry and the iodine clock reaction (in which iodine also serves as a test for starch, forming a dark blue complex), and aqueous alkaline iodine solution is used in the iodoform test for methyl ketones. The iodine test for starch is still used to detect counterfeit banknotes printed on starch-containing paper.\n\nThe spectra of the iodine molecule, I, consists of (not exclusively) tens of thousands of sharp spectral lines in the wavelength range 500-700 nm. Which is why it is such a commonly used wavelength reference (secondary standard). By measuring with a spectroscopic Doppler-free technique while focusing on one of these lines, the hyperfine structure of the iodine molecule reveals itself. A line is now resolved such that either 15 components, (from even rotational quantum numbers, J), or 21 components (from odd rotational quantum numbers, J) are measurable.\n\nElemental iodine is used as a disinfectant either as the element, or as the water-soluble triiodide anion I generated \"in situ\" by adding iodide to poorly water-soluble elemental iodine (the reverse chemical reaction makes some free elemental iodine available for antisepsis). Elemental iodine may also be used to treat iodine deficiency.\n\nIn the alternative, iodine may be produced from iodophors, which contain iodine complexed with a solubilizing agent (iodide ion may be thought of loosely as the iodophor in triiodide water solutions). Examples of such preparations include:\n\nThe antimicrobial action of iodine is quick and works at low concentrations, and thus it is used in operating theatres. Its specific mode of action is unknown. It penetrates into microorganisms and attacks particular amino acids (such as cysteine and methionine), nucleotides, and fatty acids, ultimately resulting in cell death. It also has an antiviral action, but nonlipid viruses and parvoviruses are less sensitive than lipid enveloped viruses. Iodine probably attacks surface proteins of enveloped viruses, and it may also destabilise membrane fatty acids by reacting with unsaturated carbon bonds.\n\nIn medicine, a saturated solution of potassium iodide is used to treat acute thyrotoxicosis. It is also used to block uptake of iodine-131 in the thyroid gland (see isotopes section above), when this isotope is used as part of radiopharmaceuticals (such as iobenguane) that are not targeted to the thyroid or thyroid-type tissues.\n\nIodine-131 (usually as iodide) is a component of nuclear fallout, and is particularly dangerous owing to the thyroid gland's propensity to concentrate ingested iodine and retain it for periods longer than this isotope's radiological half-life of eight days. For this reason, people at risk of exposure to environmental radioactive iodine (iodine-131) in fallout may be instructed to take non-radioactive potassium iodide tablets. The typical adult dose is one 130 mg tablet per 24 hours, supplying 100 mg (100,000 micrograms) of ionic iodine. (The typical daily dose of iodine for normal health is of order 100 micrograms; see \"Dietary Intake\" below.) Ingestion of this large dose of non-radioactive iodine minimises the uptake of radioactive iodine by the thyroid gland.\nAs an element with high electron density and atomic number, iodine absorbs X-rays weaker than 33.3 keV due to the photoelectric effect of the innermost electrons. Organoiodine compounds are used with intravenous injection as X-ray radiocontrast agents. This application is often in conjunction with advanced X-ray techniques such as angiography and CT scanning. At present, all water-soluble radiocontrast agents rely on iodine.\n\nThe production of ethylenediamine dihydroiodide, provided as a nutritional supplement for livestock, consumes a large portion of available iodine. Another significant use is a catalyst for the production of acetic acid by the Monsanto and Cativa processes. In these technologies, which support the world's demand for acetic acid, hydroiodic acid converts the methanol feedstock into methyl iodide, which undergoes carbonylation. Hydrolysis of the resulting acetyl iodide regenerates hydroiodic acid and gives acetic acid.\n\nInorganic iodides find specialised uses. Titanium, zirconium, hafnium, and thorium are purified by the van Arkel process, which involves the reversible formation of the tetraiodides of these elements. Silver iodide is a major ingredient to traditional photographic film. Thousands of kilograms of silver iodide are used annually for cloud seeding to induce rain.\n\nThe organoiodine compound erythrosine is an important food coloring agent. Perfluoroalkyl iodides are precursors to important surfactants, such as perfluorooctanesulfonic acid.\n\nIodine is an essential element for life and, at atomic number \"Z\" = 53, is the heaviest element commonly needed by living organisms. (Lanthanum and the other lanthanides, as well as tungsten with \"Z\" = 74, are used by a few microorganisms.) It is required for the synthesis of the growth-regulating thyroid hormones thyroxine and triiodothyronine (T and T respectively, named after their number of iodine atoms). A deficiency of iodine leads to decreased production of T and T and a concomitant enlargement of the thyroid tissue in an attempt to obtain more iodine, causing the disease known as simple goitre. The major form of thyroid hormone in the blood is thyroxine (T), which has a longer half-life than T. In humans, the ratio of T to T released into the blood is between 14:1 and 20:1. T is converted to the active T (three to four times more potent than T) within cells by deiodinases (5'-iodinase). These are further processed by decarboxylation and deiodination to produce iodothyronamine (Ta) and thyronamine (Ta'). All three isoforms of the deiodinases are selenium-containing enzymes; thus dietary selenium is essential for T production.\n\nIodine accounts for 65% of the molecular weight of T and 59% of T. Fifteen to 20 mg of iodine is concentrated in thyroid tissue and hormones, but 70% of all iodine in the body is found in other tissues, including mammary glands, eyes, gastric mucosa, fetal thymus, cerebro-spinal fluid and choroid plexus, arterial walls, the cervix, and salivary glands. In the cells of those tissues, iodide enters directly by sodium-iodide symporter (NIS). The action of iodine in mammary tissue is related to fetal and neonatal development, but in the other tissues, it is (at least) partially unknown.\n\nRecommendations by the United States Institute of Medicine are between 110 and 130 µg for infants up to 12 months, 90 µg for children up to eight years, 130 µg for children up to 13 years, 150 µg for adults, 220 µg for pregnant women and 290 µg for lactation. The Tolerable Upper Intake Level (UL) for adults is 1,100 μg/day. This upper limit was assessed by analyzing the effect of supplementation on thyroid-stimulating hormone.\n\nThe thyroid gland needs no more than 70 μg/day to synthesise the requisite daily amounts of T4 and T3. The higher recommended daily allowance levels of iodine seem necessary for optimal function of a number of body systems, including lactating breast, gastric mucosa, salivary glands, brain cells, choroid plexus, thymus, and arterial walls.\n\nNatural sources of dietary iodine include seafood, such as fish, seaweeds (such as kelp) and shellfish, dairy products and eggs so long as the animals received enough iodine, and plants grown on iodine-rich soil. Iodised salt is fortified with iodine in the form of sodium iodide.\n\nAs of 2000, the median intake of iodine from food in the United States was 240 to 300 μg/day for men and 190 to 210 μg/day for women. The general US population has adequate iodine nutrition, with women of childbearing age and pregnant women having a possible mild risk of deficiency. In Japan, consumption was considered much higher, ranging between 5,280 μg/day to 13,800 μg/day from dietary seaweed or kombu kelp, often in the form of Kombu Umami extracts for soup stock and potato chips. However, new studies suggest that Japan's consumption is closer to 1,000–3,000 μg/day. The adult UL in Japan was last revised to 3,000 µg/day in 2015.\n\nAfter iodine fortification programs such as iodisation of salt have been implemented, some cases of iodine-induced hyperthyroidism have been observed (so-called Jod-Basedow phenomenon). The condition seems to occur mainly in people over forty, and the risk appears higher when iodine deficiency is severe and the initial rise in iodine intake is high.\n\nIn areas where there is little iodine in the diet, typically remote inland areas and semi-arid equatorial climates where no marine foods are eaten, iodine deficiency gives rise to hypothyroidism, symptoms of which are extreme fatigue, goitre, mental slowing, depression, weight gain, and low basal body temperatures. Iodine deficiency is the leading cause of preventable intellectual disability, a result that occurs primarily when babies or small children are rendered hypothyroidic by a lack of the element. The addition of iodine to table salt has largely eliminated this problem in the wealthier nations, but iodine deficiency remains a serious public health problem today in the developing world. Iodine deficiency is also a problem in certain areas of Europe. Information processing, fine motor skills, and visual problem solving are improved by iodine repletion in moderately iodine-deficient children.\n\nElemental iodine (I) is toxic if taken orally undiluted. The lethal dose for an adult human is 30 mg/kg, which is about 2.1–2.4 grams for a human weighing 70 to 80 kg (even if experiments on rats demonstrated that these animals could survive after eating a 14000 mg/kg dose). Excess iodine can be more cytotoxic in the presence of selenium deficiency. Iodine supplementation in selenium-deficient populations is, in theory, problematic, partly for this reason. The toxicity derives from its oxidizing properties, through which it denaturates proteins (including enzymes).\n\nElemental iodine is also a skin irritant, and direct contact with skin can cause damage and solid iodine crystals should be handled with care. Solutions with high elemental iodine concentration, such as tincture of iodine and Lugol's solution, are capable of causing tissue damage if used in prolonged cleaning or antisepsis; similarly, liquid Povidone-iodine (Betadine) trapped against the skin resulted in chemical burns in some reported cases.\n\nPeople can be exposed to iodine in the workplace by inhalation, ingestion, skin contact, and eye contact. The Occupational Safety and Health Administration (OSHA) has set the legal limit (Permissible exposure limit) for iodine exposure in the workplace at 0.1 ppm (1 mg/m) during an 8-hour workday. The National Institute for Occupational Safety and Health (NIOSH) has set a Recommended exposure limit (REL) of 0.1 ppm (1 mg/m) during an 8-hour workday. At levels of 2 ppm, iodine is immediately dangerous to life and health.\n\nSome people develop a hypersensitivity to products and foods containing iodine. Applications of tincture of iodine or Betadine can cause rashes, sometimes severe. Parenteral use of iodine-based contrast agents (see above) can cause reactions ranging from a mild rash to fatal anaphylaxis. Such reactions have led to the misconception (widely held, even among physicians) that some people are allergic to iodine itself; even allergies to iodine-rich seafood have been so construed. In fact, there has never been a confirmed report of a true iodine allergy, and an allergy to elemental iodine or simple iodide salts is theoretically impossible. Hypersensitivity reactions to products and foods containing iodine are apparently related to their other molecular components; thus, a person who has demonstrated an allergy to one food or product containing iodine may not have an allergic reaction to another. Patients with various food allergies (shellfish, egg, milk, etc.) or asthma are more likely to suffer reactions to contrast media containing iodine. As with all medications, the patient's allergy history should be questioned and consulted before any containing iodine are administered.\n"}
{"id": "23098762", "url": "https://en.wikipedia.org/wiki?curid=23098762", "title": "Jacobi coordinates", "text": "Jacobi coordinates\n\nIn the theory of many-particle systems, Jacobi coordinates often are used to simplify the mathematical formulation. These coordinates are particularly common in treating polyatomic molecules and chemical reactions, and in celestial mechanics. An algorithm for generating the Jacobi coordinates for \"N\" bodies may be based upon binary trees. In words, the algorithm is described as follows:\nLet \"m\" and \"m\" be the masses of two bodies that are replaced by a new body of virtual mass \"M\" = \"m\" + \"m\". The position coordinates x and x are replaced by their relative position r = x − x and by the vector to their center of mass R = (\"m\" \"q\" + \"m\"\"q\")/(\"m\" + \"m\"). The node in the binary tree corresponding to the virtual body has \"m\" as its right child and \"m\" as its left child. The order of children indicates the relative coordinate points from x to x. Repeat the above step for \"N\" − 1 bodies, that is, the \"N\" − 2 original bodies plus the new virtual body. \n\nFor the \"N\"-body problem the result is:\n\nwith\n\nThe vector formula_4 is the center of mass of all the bodies:\n\nThe result one is left with is thus a system of \"N\"-1 translationally invariant coordinates formula_5 and a center of mass coordinate formula_6, from iteratively reducing two-body systems within the many-body system.\n"}
{"id": "11002752", "url": "https://en.wikipedia.org/wiki?curid=11002752", "title": "Krogmann's salt", "text": "Krogmann's salt\n\nKrogmann's salt is a mixed-valence square planar coordination complex of platinum and cyanide bonded through linear platinum metal chains, sometimes described as molecular wires.\n\nAlthough the term Krogmann’s salt most commonly refers to a platinum metal complex of the formula K[Pt(CN)X] where X is usually bromine (or sometimes chlorine), a number of non-stoichiometric metal salts containing the anionic complex [Pt(CN)] can also be characterized under the blanket term “Krogmann’s salts.”\n\nModeled as an infinite one-dimensional molecular chain of platinum atoms, the high anisotropy and restricted dimensionality of Krogmann’s salt and related compounds are becoming increasingly attractive properties for many facets of nanotechnology.\n\nKrogmann's salt was first synthesized by Klaus Krogmann in the late 1960s at the University of Stuttgart in Germany. Krogmann published the original journal article documenting the synthesis and characterization of the salt in 1969.\n\nKrogmann’s salt is a series of partially oxidized tetracyanoplatinate complexes linked by the platinum-platinum bonds on the top and bottom faces of the planar [Pt(CN)] anions. This salt forms infinite stacks in the solid state based on the overlap of the d orbitals.\n\nKrogmann’s salt has a tetragonal crystal structure with a Pt-Pt distance of 2.880 angstroms, which is much shorter than the metal-metal bond distances in other planar platinum complexes such as Ca[Pt(CN)]·5HO (3.36 angstroms), Sr[Pt(CN)]·5HO (3.58 angstroms), and Mg[Pt(CN)]·7HO (3.16 angstroms). The Pt-Pt distance in Krogmann's salt is only 0.1 angstroms longer than in platinum metal.\n\nEach unit cell contains a site for Cl, corresponding to 0.5 Cl per Pt. However, this site is only filled 64% of the time, giving 0.32 Cl per Pt in the actual compound. Because of this, the oxidation number of Pt does not rise above +2.32.\n\nKrogmann’s salt has no recognizable phase range and is characterized by broad and intense intervalence bands in its electronic spectra.\n\nOne of the most widely researched properties of Krogmann’s salt is its unusual electric conductance. Because of its linear chain structure and overlap of the platinum formula_1 orbitals, Krogmann’s salt is an excellent conductor of electricity. This property makes it an attractive material for nanotechnology.\n\nThe usual preparation of Krogmann's salt involves the evaporation of a 5:1 molar ratio mixture of the salts K[Pt(CN)] and K[Pt(CN)Br] in water to give copper-colored needles of K[Pt(CN)]Br·2.6 HO.\n\nBecause excess Pt or Pt complex crystallizes out with the product when the reactant ratio is changed, the product is therefore well defined, although non-stoichiometric.\n\nAlthough there was a large body of research and literature generated on molecular wire-type metal complexes through the mid-1980s, interest in stacked metal-metal bonds saw a decline until only very recently. Due to the explosion of nanotechnology in the 2010s, many researchers have taken a renewed interest in Krogmann's salt and its related compounds due to their high anisotropy, restricted dimensionality, and unique conductance properties.\n\nA new group of platinum chains based on alternating cations and anions of [Pt(CNR)] (R = \"i\"Pr, \"c-\"CH, \"p-\"(CH)CH) and [Pt(CN)] is undergoing current research. These may be able to be used as vapochromic sensor materials, or materials which change color when exposed to different vapors.\n\nSimilar to Krogmann’s platinum salt, it has been shown that it is possible to stabilize metal chains with only unsaturated hydrocarbons, or olefins. Current research indicates that mononuclear Pd and Pd react with conjugated polyenes to give linear chains of Pd-Pd bonds protected by a “π-electron sheath.”\n\nNot only do these olefin-stabilized metal chains constitute a significant contribution to the field of organometallic chemistry, both the complex’s metal atom structures and the olefin ligands themselves can conduct a current. The prospect of creating molecular wires of conducting organic and inorganic constituents has intriguing possibilities for future research, especially in microbiology, nanotechnology, and organic circuitry.\n"}
{"id": "17634043", "url": "https://en.wikipedia.org/wiki?curid=17634043", "title": "Kuznetsov NK-14", "text": "Kuznetsov NK-14\n\nThe Kuznetsov NK-14A was an onboard nuclear-powered engine which was made to be used on the Tupolev Tu-119 nuclear-powered aircraft, designed and built by the Soviet Kuznetsov Design Bureau. The design of the plane was based on a modified Tupolev Tu-95 and would be fitted with two Kuznetsov NK-14A nuclear-fuelled engines inboard fed with heat from a fuselage mounted reactor and two Kerosene-fed Kuznetsov NK-12 turboprops outboard.\n\nDevelopment was suspended with the cancellation of the Tu-119, but flight trials would have been made initially fitted to the inboard nacelles of the Tu-119 prototype. \n"}
{"id": "158830", "url": "https://en.wikipedia.org/wiki?curid=158830", "title": "List of minerals", "text": "List of minerals\n\nThis is a list of minerals \"for which there are articles on Wikipedia\". \n\nMinerals are distinguished by various chemical and physical properties. Differences in chemical composition and crystal structure distinguish the various \"species\". Within a mineral species there may be variation in physical properties or minor amounts of impurities that are recognized by mineralogists or wider society as a mineral \"variety\". \n\nMineral variety names and mineraloids are to be listed after the valid minerals for each letter.\n\nFor a complete listing (about 5,000) of all mineral names, see List of minerals (complete).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "48176889", "url": "https://en.wikipedia.org/wiki?curid=48176889", "title": "List of protected areas of Armenia", "text": "List of protected areas of Armenia\n\nThis is a list of protected areas in Armenia that are categorized as follows: 4 national parks, 3 state reserves, 27 state sanctuaries and 5 botanical gardens.\n"}
{"id": "35413022", "url": "https://en.wikipedia.org/wiki?curid=35413022", "title": "Lyddane–Sachs–Teller relation", "text": "Lyddane–Sachs–Teller relation\n\nIn condensed matter physics, the Lyddane–Sachs–Teller relation (or LST relation) determines the ratio of the natural frequency of longitudinal optic lattice vibrations (phonons) (formula_1) of an ion crystal to the natural frequency of the transverse optical lattice vibration (formula_2) for long wavelengths (zero wavevector). The ratio is equal to the ratio of the static permittivity formula_3 (Often also formula_4) to the permittivity for frequencies in the visible range formula_5 (often also formula_6). \nThe Lyddane–Sachs–Teller relation is named after the physicists R. H. Lyddane, Edward Teller, and R. G. Sachs.\n\nThe Lyddane–Sachs–Teller relation applies to optical lattice vibrations that have an associated net polarization density, so that they can produce long ranged electromagnetic fields (over ranges much longer than the inter-atom distances). The relation assumes an idealized polar (\"infrared active\") optical lattice vibration that gives a contribution to the frequency-dependent permittivity described by a lossless Lorentzian oscillator:\nwhere formula_8 is the permittivity at high frequencies, formula_9 is the static polarizability of the optical lattice mode, and formula_2 is the \"natural\" oscillation frequency of the lattice vibration taking into account only the short-ranged (microscopic) restoring forces.\nThe above equation can be plugged into Maxwell's equations to find the complete set of normal modes including all restoring forces (short-ranged and long-ranged), which are sometimes called phonon polaritons. In Maxwell's equations, an electric longitudinal wave occurs when the permittivity passes through zero, i.e.\nFor the Lorentzian resonance described above, this longitudinal mode frequency is given by the Lyddane–Sachs–Teller relation. A transverse resonance at formula_2 is also present although it does not extend all the way to k=0 since for small, lightlike wavevectors, the magnetic fields generated by the transverse polarization currents cannot be neglected and lead to hybridizing of the lattice vibration with light (see figure).\n\nSince the Lyddane–Sachs–Teller relation is derived from the lossless Lorentzian oscillator, it may break down in realistic materials where the permittivity function is more complicated for various reasons:\nIn the case of multiple, lossy Lorentzian oscillators, there are generalized Lyddane–Sachs–Teller relations available.\nMost generally, the permittivity cannot be described as a combination of Lorentizan oscillators, and the longitudinal mode frequency can only be found as a complex zero in the permittivity function.\n"}
{"id": "27107499", "url": "https://en.wikipedia.org/wiki?curid=27107499", "title": "Mundy Hepburn", "text": "Mundy Hepburn\n\nMundy Hepburn is an American artist who designs and builds glass sculptures filled with luminous electrified inert gases—the same technology used in neon signs. Hepburn developed many of the glass and lighting techniques he uses in his sculptures himself. \n\nHepburn lives in Old Saybrook, Connecticut. He is the nephew of actress Katharine Hepburn.\n\n"}
{"id": "30373641", "url": "https://en.wikipedia.org/wiki?curid=30373641", "title": "Oscar Rivas (environmentalist)", "text": "Oscar Rivas (environmentalist)\n\nOscar Rivas is a Paraguayan environmentalist. He was awarded the Goldman Environmental Prize in 1992, jointly with Elias Diaz Peña, for their efforts to protect the ecosystems of the Paraná River and the Paraguay River, in particular consequences from the Yacyretá Dam project.\n"}
{"id": "6863915", "url": "https://en.wikipedia.org/wiki?curid=6863915", "title": "PICO", "text": "PICO\n\nPICO is an experiment searching for direct evidence of dark matter using a bubble chamber of chlorofluorocarbon (Freon) as the active mass. It is located at SNOLAB in Canada.\n\nIt was formed in 2013 from the merger of two similar experiments, PICASSO and COUPP.\n\nPICASSO (Project In CAnada to Search for Supersymmetric Objects, or Projet d'Identification de CAndidats Supersymétriques SOmbres in French) was an international collaboration with members from the Université de Montréal, Queen's University, Indiana University South Bend and Czech Technical University in Prague, University of Alberta, Laurentian University and BTI, Chalk River, Ontario. PICASSO is predominantly sensitive to spin-dependent interactions of Weakly Interacting Massive Particles (WIMPs) with fluorine atoms.\n\nCOUPP (Chicagoland Observatory for Underground Particle Physics) was a similar project with members from Fermilab, University of Chicago, and Indiana University. Prototypes were tested in the MINOS experiment far hall, with a scaled-up experiment also operating at SNOLAB. It used trifluoroiodomethane (CFI) as the medium.\n\nA bubble detector is a radiation sensitive device that uses small droplets of superheated liquid that are suspended in a gel matrix. It uses the principle of a bubble chamber but since only the small droplets can undergo a phase transition at a time, the detector can stay active for much longer periods than a classic bubble chamber. When enough energy is deposited in a droplet by ionizing radiation the superheated droplet undergoes a phase transition and becomes a gas bubble. The PICASSO detectors contain Freon droplets with an average diameter of . The bubble development in the detector is accompanied by an acoustic shock wave that is picked up by piezo-electric sensors. The main advantage of the bubble detector technique is that the detector is almost insensitive to background radiation. The detector sensitivity can be adjusted by changing the temperature of the droplets. Freon-loaded detectors are typically operated at temperatures between .\n\nThe validity of the bubble detector concept has been shown in several publications. There is another similar experiment using this technique in Europe called SIMPLE.\n\nPICASSO reports results (November 2009) for spin-dependent WIMP interactions on F. No dark matter signal has been found, but for WIMP masses of new stringent limits have been obtained on the spin-dependent cross section for WIMP scattering on F of (90% CL). This result has been converted into a cross section limit for WIMP interactions on protons of (90% CL). The obtained limits restrict recent interpretations of the DAMA/LIBRA annual modulation effect in terms of spin dependent interactions.\n\nNew results were published in May 2012, using 10 detectors with total exposure , to constrain low-mass WIMP interaction on F. The best spin-dependent limits were obtained for a 20 GeV/c WIMP mass: 0.032 pb (90% C.L.) for proton cross section. For the Spin-independent near 7 GeV low mass region cross section: upper limit (90% C.L.)\n\n"}
{"id": "50317869", "url": "https://en.wikipedia.org/wiki?curid=50317869", "title": "Panaviska Recreation Area", "text": "Panaviska Recreation Area\n\nThe Panaviska Recreation Area is located on the bench of Panasivka mountain stream between villages of Korostiv and Kozyova in Skole Raion. This is on the 666-kilometer along the highway () Kyiv-Chop.\n\nIt is a favorite spot as residents of the area and the car tourists. Back in the 1880 there were building recreation centers for children. On the territory of rest has gazebos, tables with benches, parking.\n\nAlso, there had once been famous \"Guta Korostivska\", wherein worked 140 people. \"Guta Korostivska\" was once famous for its of glass products throughout Europe.\n\n"}
{"id": "414352", "url": "https://en.wikipedia.org/wiki?curid=414352", "title": "Pumpjack", "text": "Pumpjack\n\nA pumpjack is the overground drive for a reciprocating piston pump in an oil well.\n\nIt is used to mechanically lift liquid out of the well if not enough bottom hole pressure exists for the liquid to flow all the way to the surface. The arrangement is commonly used for onshore wells producing little oil. Pumpjacks are common in oil-rich areas.\n\nDepending on the size of the pump, it generally produces of liquid at each stroke. Often this is an emulsion of crude oil and water. Pump size is also determined by the depth and weight of the oil to remove, with deeper extraction requiring more power to move the increased weight of the discharge column (discharge head).\n\nA pumpjack converts the rotary motion of the motor to a vertical reciprocating motion to drive the pump shaft, which is exhibited in the characteristic nodding motion. The engineering term for this type of mechanism is a walking beam. It was often employed in stationary and marine steam engine designs in the 18th and 19th centuries.\n\nIn the early days, pumpjacks worked by rod lines running horizontally above the ground to a wheel on a rotating eccentric in a mechanism known as a central power. The central power, which might operate a dozen or more pumpjacks, would be powered by a steam or internal combustion engine or by an electric motor. Among the advantages of this scheme was only having one motor to power all the pumpjacks rather than individual motors for each. However, among the many difficulties was maintaining system balance as individual well loads changed.\n\nModern pumpjacks are powered by a prime mover. This is commonly an electric motor, but internal combustion engines are used in isolated locations without access to electricity, or, in the cases of water pumpjacks, where three-phase power is not available (while single phase motors exist at least up to 60 hp, providing power to single-phase motors above 10 horsepower can cause powerline problems, and many pumps require more than 10 horsepower). Common off-grid pumpjack engines run on natural gas, often casing gas produced from the well, but pumpjacks have been run on many types of fuel, such as propane and diesel fuel. In harsh climates, such motors and engines may be housed in a shack for protection from the elements. Engines that power water pumpjacks often receive natural gas from the nearest available gas grid.\n\nThe prime mover runs a set of pulleys to the transmission, often a double-reduction gearbox, which drives a pair of cranks, generally with counterweights installed on them to assist the motor in lifting the heavy rod assembly. The cranks raise and lower one end of an I-beam which is free to move on an A-frame. On the other end of the beam is a curved metal box called a horse head or donkey head, so named due to its appearance. A cable made of steel—occasionally, fibreglass—called a bridle, connects the horse head to the polished rod, a piston that passes through the stuffing box.\n\nThe cranks themselves also produce counterbalance due to their weight, so on pumpjacks that do not carry very heavy loads, the weight of the cranks themselves may be enough to balance the well load.\n\nSometimes, however, crank-balanced units can become prohibitively heavy due to the need for counterweights. Currently, Lufkin Industries offer \"air-balanced\" units, where counterbalance is provided by a pneumatic cylinder charged with air from a compressor, eliminating the need for counterweights.\n\nThe polished rod has a close fit to the stuffing box, letting it move in and out of the tubing without fluid escaping. (The tubing is a pipe that runs to the bottom of the well through which the liquid is produced.) The bridle follows the curve of the horse head as it lowers and raises to create a vertical or nearly-vertical stroke. The polished rod is connected to a long string of rods called sucker rods, which run through the tubing to the down-hole pump, usually positioned near the bottom of the well.\n\nAt the bottom of the tubing is the down-hole pump. This pump has two ball check valves: a stationary valve at bottom called the standing valve, and a valve on the piston connected to the bottom of the sucker rods that travels up and down as the rods reciprocate, known as the traveling valve. Reservoir fluid enters from the formation into the bottom of the borehole through perforations that have been made through the casing and cement (the casing is a larger metal pipe that runs the length of the well, which has cement placed between it and the earth; the tubing, pump, and sucker rod are all inside the casing).\n\nWhen the rods at the pump end are travelling up, the traveling valve is closed and the standing valve is open (due to the drop in pressure in the pump barrel). Consequently, the pump barrel fills with the fluid from the formation as the traveling piston lifts the previous contents of the barrel upwards. When the rods begin pushing down, the traveling valve opens and the standing valve closes (due to an increase in pressure in the pump barrel). The traveling valve drops through the fluid in the barrel (which had been sucked in during the upstroke). The piston then reaches the end of its stroke and begins its path upwards again, repeating the process.\n\nOften, gas is produced through the same perforations as the oil. This can be problematic if gas enters the pump, because it can result in what is known as gas locking, where insufficient pressure builds up in the pump barrel to open the valves (due to compression of the gas) and little or nothing is pumped. To preclude this, the inlet for the pump can be placed below the perforations. As the gas-laden fluid enters the well bore through the perforations, the gas bubbles up the annulus (the space between the casing and the tubing) while the liquid moves down to the standing valve inlet. Once at the surface, the gas is collected through piping connected to the annulus.\n\nPumpjacks can also be used to drive what would now be considered old-fashioned hand-pumped water wells. The scale of the technology is frequently smaller than for an oil well, and can typically fit on top of an existing hand-pumped well head. The technology is simple, typically using a parallel-bar double-cam lift driven from a low-power electric motor, although the number of pumpjacks with stroke lengths 54 inches (137 cm) and longer being used as water pumps is increasing. A short video recording of such a pump in action can be viewed on YouTube.\n\nAlthough the flow rate for a water well pumpjack is lower than that from a jet pump and the lifted water is not pressurised, the beam pumping unit has the option of hand pumping in an emergency, by hand-rotating the pumpjack cam to its lowest position, and attaching a manual handle to the top of the wellhead rod. In larger pumpjacks powered by engines, the engine can run off fuel stored in a reservoir or from natural gas delivered from the nearest gas grid. In some cases, this type of pump consumes less power than a jet pump and is, therefore, cheaper to run.\n\n\n"}
{"id": "2044964", "url": "https://en.wikipedia.org/wiki?curid=2044964", "title": "Rainmaking", "text": "Rainmaking\n\nRainmaking, also known as artificial precipitation, artificial rainfall and pluviculture, is the act of attempting to artificially induce or increase precipitation, usually to stave off drought. According to the clouds' different physical properties, this can be done using airplanes or rockets to sow to the clouds with catalysts such as dry ice, silver iodide and salt powder, to make clouds rain or increase precipitation, to remove or mitigate farmland drought, to increase reservoir irrigation water or water supply capacity, or to increase water levels for power generation. \n\nIn the United States, rainmaking was attempted by traveling showmen. It was practiced in the old west, but may have reached a peak during the dust bowl drought of the American West and Midwest in the 1930s. The practice was depicted in the 1956 film \"The Rainmaker\". Attempts to bring rain directly have waned with development of the science of meteorology, the advent of laws against fraud and increased communication technology, with some exceptions such as cloud seeding and forms of prayer including rain dances, which are still practiced today. Prayer for more rain is also a cultural practice for Christians and Muslims in areas which people keep \"traditional\" non-scriptural religions. In the Christian areas the Defteras (learned clerics of the Orthodox Christian Church) believed to have the wisdom to arrest the rain, to bring hail to farms of individuals who refuse to comply with religious rules as well as to bring more rains when the rainy season fell short of giving the usual amount of rain needed for growing cereals. \n\nThe term is also used metaphorically to describe the process of bringing new clients into a professional practice, such as law, architecture, consulting, advertising, or investment banking—in general, processes that bring money into a company.\n\nIt is also used to describe a confidence trick where the scammer takes money from the victim to influence a system over which they have no real control, but a random chance of the outcome happening anyway.\n\nSince the 1940s, cloud seeding has been used to change the structure of clouds by dispersing substances into the air, potentially increasing or altering rainfall. In spite of experiments dating back to at least the start of the 20th century, however, there is much controversy surrounding the efficacy of cloud seeding, and evidence that cloud seeding leads to increased precipitation on the ground is highly equivocal. One difficulty is knowing how much precipitation might have fallen had any particular cloud not been seeded. Operation Popeye was a US military rainmaking operation to increase rains over Vietnam during the Vietnam War in order to slow Vietnamese military truck activity in the region. Rainmaking is not climate engineering, which seeks to alter climate, but a form of weather modification, as it seeks only to change local weather.\n\nAustrian-American psychoanalyst Wilhelm Reich designed a \"cloudbuster\" in the United States with which he said he could manipulate streams of \"orgone energy\" (which he claimed was a primordial cosmic energy) in the atmosphere to induce rain by forcing clouds to form and disperse. It was a set of hollow metal pipes and cables inserted into water, which Reich argued created a stronger orgone energy field than was in the atmosphere, the water drawing the atmospheric orgone through the pipes. Reich called his research \"Cosmic Orgone Engineering\".\n\nIn many societies around the world, rain dances and other rituals have been used to attempt to increase rainfall. Some Native Americans used rain dances extensively. European examples include the Romanian ceremonies known as paparuda and caloian. Some United States farmers also attempt to bring rain during droughts through prayer. These rituals differ greatly in their specifics, but share a common concern with bringing rain through ritual and/or spiritual means. Typical of these ceremonies was then-governor of Georgia Sonny Perdue's public prayer service for rain, in 2007.\n\nContemporary Jewish liturgy includes prayers for rain, seasonally, as a part of the morning, afernoon, and evening, daily amidah prayer, during mid-Autumn to mid-Spring. During Summer, this prayer is changed from the prayer for rain, to a prayer for dew.\n\n\n\n"}
{"id": "26587782", "url": "https://en.wikipedia.org/wiki?curid=26587782", "title": "Scotophor", "text": "Scotophor\n\nA scotophor is a material showing reversible darkening and bleaching when subjected to certain types of radiation. The name means \"dark bearer\", in contrast to phosphor, which means \"light bearer\". Scotophors show tenebrescence (reversible photochromism) and darken when subjected to an intense radiation such as sunlight. Minerals showing such behavior include hackmanite sodalite, spodumene and tugtupite. Some pure alkali halides also show such behavior.\n\nScotophors can be sensitive to light, particle radiation (e.g. electron beam – see cathodochromism), X-rays, or other stimuli. The induced absorption bands in the material, caused by F-centers created by electron bombardment, can be returned to their non-absorbing state, usually by light and/or heating.\n\nScotophors sensitive to electron beam radiation can be used instead of phosphors in cathode ray tubes, for creating a light absorbing instead of light emitting image. Such displays are viewable in bright light and the image is persistent, until erased.\n\nThe image would be retained until erased by flooding the scotophor with a high-intensity infrared light or by electro-thermal heating. Using conventional deflection and raster formation circuity, a bi-level image could be created on the membrane and retained even when power was removed from the CRT.\n\nIn Germany, scotophor tubes were developed by Telefunken as blauschrift-röhre (\"dark-trace tube\"). The heating mechanism was a layer of mica with transparent thin film of tungsten. When the image was to be erased, current was applied to the tungsten layer; even very dark images could be erased in 5–10 seconds.\n\nScotophors typically require a higher-intensity electron beam to change color than phosphors need to emit light. Screens with layers of a scotophor and a phosphor are therefore possible, where the phosphor, flooded with a dedicated wide-beam low-intensity electron gun, produces backlight for the scotophor, and optionally highlights selected areas of the screen if bombarded with electrons with higher energy but still insufficient to penetrate the phosphor and change the scotophor state.\n\nThe main application of scotophors was in plan position indicators, specialized military radar displays. The achievable brightness allowed projecting the image to a larger surface. The ability to quickly record a persistent trace found its use in some oscilloscopes.\n\nPotassium chloride is used as a scotophor with designation P10 in dark-trace CRTs (also called \"dark trace tubes\", \"color center tubes\", \"cathodochromic displays\" or \"scotophor tubes\"), e.g. in the Skiatron. This CRT replaced the conventional light-emitting \"phosphor\" layer on the face of the tube screen with a \"scotophor\" such as potassium chloride (KCl). Potassium chloride has the property that when a crystal is struck by an electron beam, that spot would change from translucent white to a dark magenta color. By backlighting such a CRT with a white or green circular fluorescent lamp, the resulting image would appear as black information against a green background or as magenta information against a white background. A benefit, aside from the semi-permanent storage of the displayed image, is that the brightness of the resultant display is only limited by the illumination source and optics. The F-centers, however, have tendency to aggregate, and the screen needs to be heated to fully erase the image.\n\nThe image on KCl can be formed by depositing a charge of over 0.3 microcoulomb per square centimeter, by an electron beam with energy typically at 8–10 keV. The erasure can be achieved in less than a second by heating the scotophor at 150 °C.\n\nKCl was the most common scotophor used. Other halides show the same property; potassium bromide absorbs in bluish end of the spectrum, resulting in a brown trace, sodium chloride produces a trace that is colored more towards orange.\n\nAnother scotophor used in dark-trace CRTs is a modified sodalite, fired in reducing atmosphere or having some chlorides substituted with sulfate ions. Its advantage against KCl is its higher writing speed, less fatigue, and the F-centers do not aggregate, therefore it is possible to substantially erase the screen with light only, without heating.\n\n"}
{"id": "3620291", "url": "https://en.wikipedia.org/wiki?curid=3620291", "title": "Sfermion", "text": "Sfermion\n\nIn supersymmetric extension to the Standard Model of physics, a sfermion is a hypothetical spin-0 superpartner particle (sparticle) of its associated fermion. Each particle has a superpartner with spin that differs by . Fermions in the SM have spin- and therefore sfermions have spin 0.\n\nThe name 'sfermion' was formed by the general rule of prefixing an 's' to the name of its superpartner, denoting that it is a scalar particle with spin 0. For instance, the electron's superpartner is the selectron and the top quark's superpartner is the stop squark.\n\nOne corollary from supersymmetry is that sparticles have the same gauge numbers as their SM partners. This means that sparticle–particle pairs have the same color charge, weak isospin charge, and hypercharge (and consequently electric charge). Unbroken supersymmetry also implies that sparticle–particle pairs have the same mass. This is evidently not the case, since these sparticles would have already been detected. Thus, sparticles must have different masses from the particle partners and supersymmetry is said to be broken.\n\nSquarks are the superpartners of quarks. These include the sup squark, sdown squark, scharm squark, sstrange squark, stop squark, and sbottom squark.\nSleptons are the superpartners of leptons. These include the selectron, smuon, stau, and the sneutrinos.\n\n"}
{"id": "1150377", "url": "https://en.wikipedia.org/wiki?curid=1150377", "title": "Slurry", "text": "Slurry\n\nA slurry is a thin and viscous fluid mixture composed of a pulverized solid and a liquid. Slurries flow under gravity, can be pumped if not too thick, and are often used as a convenient way of handling solids in bulk. \n\nExamples of slurries include:\n\nTo determine the percent solids (or solids fraction) of a slurry from the density of the slurry, solids and liquid\nwhere\n\nIn aqueous slurries, as is common in mineral processing, the specific gravity of the species is typically used, and since formula_6 is taken to be 1, this relation is typically written:\neven though specific gravity with units tonnes/m^3 (t/m^3) is used instead of the SI density unit, kg/m^3.\n\nTo determine the mass of liquid in a sample given the mass of solids and the mass fraction:\nBy definition\ntherefore\nand\nthen\nand therefore\nwhere\n\nEquivalently\nand in a minerals processing context where the specific gravity of the liquid (water) is taken to be one:\nSo\nand\nThen combining with the first equation:\nSo\nThen since\nwe conclude that\n\nwhere\n\n\n"}
{"id": "14522694", "url": "https://en.wikipedia.org/wiki?curid=14522694", "title": "Stone of Tmutarakan", "text": "Stone of Tmutarakan\n\nThe Stone of Tmutarakan () is a marble slab engraved with the words \"In the year 6576 [ A.M., 1068 A.D] the sixth of the Indiction, Prince Gleb measured across the sea on the ice from Tmutarakan to Kerch 14,000 sazhen\" («В лето 6576 индикта 6 Глеб князь мерил море по леду от Тмутороканя до Корчева 14000 сажен»).\n\nA sazhen, an old Rus unit of length, was equal to seven feet (or corresponded roughly to a fathom); thus the Kerch Straits, according to the stone, were 88,000 feet or 18.5 miles across (that is, from Kerch to Tmutarakan — the straits themselves are only 4.5 miles wide at their narrowest point, but the distance from the site of Tmutarakan to modern-day Kerch is about 15 miles.) The tenth-century Byzantine Emperor Constantine Porphyrogenitus wrote that the straits were the equivalent of 18 miles across, and this might explain why that measurement appears on the stone, although it is unclear if an eleventh-century prince in Rus would have had access to that information; this uncertainty calls the stone's authenticity into question.\n\nThe Prince Gleb referred to in the inscription was Gleb Svyatoslavich, then prince of Tmutarakan. Gleb was later Prince of Novgorod the Great, where he saved Bishop Fedor's life by chopping a sorcerer in half who led a pagan uprising against the bishop. Gleb was eventually killed fighting pagan Finnic tribes in the northern Novgorodian Lands (\"the Zavoloch'e\" or \"Za Volokom\", \"the Land beyond the Portages\") on May 30, 1079.\n\nThe stone was discovered on the Taman Peninsula just east of Crimea in 1792 and the inscription was first published in 1794 by Aleksei Musin-Pushkin. The study of the inscription is said to be the first epigraphic study in Russian history. In spite of its importance in the history of Russian epigraphy, a number of scholars have called the stone's provenance into question and consider the stone an eighteenth-century forgery, perhaps done by Romanticists enamored of ancient culture or even as an effort to find precedent for Russian involvement in the Caucasus. The stone is currently housed in the State Hermitage Museum in Saint Petersburg. \n"}
{"id": "2518458", "url": "https://en.wikipedia.org/wiki?curid=2518458", "title": "Thickening agent", "text": "Thickening agent\n\nA thickening agent or thickener is a substance which can increase the viscosity of a liquid without substantially changing its other properties. Edible thickeners are commonly used to thicken sauces, soups, and puddings without altering their taste; thickeners are also used in paints, inks, explosives, and cosmetics.\n\nThickeners may also improve the suspension of other ingredients or emulsions which increases the stability of the product. Thickening agents are often regulated as food additives and as cosmetics and personal hygiene product ingredients. Some thickening agents are gelling agents (gellants), forming a gel, dissolving in the liquid phase as a colloid mixture that forms a weakly cohesive internal structure. Others act as mechanical thixotropic additives with discrete particles adhering or interlocking to resist strain.\n\nThickening agents can also be used when a medical condition such as dysphagia causes difficulty in swallowing. Thickened liquids play a vital role in reducing risk of aspiration for dysphagia patients.\n\nFood thickeners frequently are based on either polysaccharides (starches, vegetable gums, and pectin), or proteins. A flavorless powdered starch used for this purpose is a fecula (from the Latin \"faecula\", diminutive of \"faex,\" \"dregs\"). This category includes starches as arrowroot, cornstarch, katakuri starch, potato starch, sago, tapioca and their starch derivatives. Microbial and Vegetable gums used as food thickeners include alginin, guar gum, locust bean gum, and xanthan gum. Proteins used as food thickeners include collagen, egg whites, and gelatin. Sugar polymers include agar, carboxymethyl cellulose, pectin and carrageenan. Other thickening agents act on the proteins already present in a food. One example is sodium pyrophosphate, which acts on casein in milk during the preparation of instant pudding.\n\nDifferent thickeners may be more or less suitable in a given application, due to differences in taste, clarity, and their responses to chemical and physical conditions. For example, for acidic foods, arrowroot is a better choice than cornstarch, which loses thickening potency in acidic mixtures. At (acidic) pH levels below 4.5, guar gum has sharply reduced aqueous solubility, thus also reducing its thickening capability. If the food is to be frozen, tapioca or arrowroot are preferable over cornstarch, which becomes spongy when frozen.\n\nMany other food ingredients are used as thickeners, usually in the final stages of preparation of specific foods. These thickeners have a flavor and are not markedly stable, thus are not suitable for general use. However, they are very convenient and effective, and hence are widely used.\n\nFunctional flours are produced from specific cereal variety (wheat, maize, rice or other) conjugated to specific heat treatment able to increase stability, consistency and general functionalities. These functional flours are resistant to industrial stresses such as acidic pH, sterilisation, freeze conditions, and can help food industries to formulate with natural ingredients. For the final consumer, these ingredients are more accepted because they are shown as \"flour\" in the ingredient list.\n\nFlour is often used for thickening gravies, gumbos, and stews. It must be cooked in thoroughly to avoid the taste of uncooked flour. Roux, a mixture of flour and fat (usually butter) cooked into a paste, is used for gravies, sauces and stews. Cereal grains (oatmeal, couscous, farina, etc.) are used to thicken soups. Yogurt is popular in Eastern Europe and Middle East for thickening soups. Soups can also be thickened by adding grated starchy vegetables before cooking, though these will add their own flavour. Tomato puree also adds thickness as well as flavour. Egg yolks are a traditional sauce thickener in professional cooking; they have rich flavor and offer a velvety smooth texture but achieve the desired thickening effect only in a narrow temperature range. Overheating easily ruins such a sauce, which can make egg yolk difficult to use as a thickener for amateur cooks. Other thickeners used by cooks are nuts (including rehan) or glaces made of meat or fish.\n\nMany thickening agents require extra care in cooking. Some starches lose their thickening quality when cooked for too long or at too high a temperature; on the other hand, cooking starches too short or not hot enough might lead to an unpleasant starchy taste or cause water to seep out of the finished product after cooling. Also, higher viscosity causes foods to burn more easily during cooking. As an alternative to adding more thickener, recipes may call for reduction of the food's water content by lengthy simmering. When cooking, it is generally better to add thickener cautiously; if over-thickened, more water may be added but loss of flavour and texture may result.\n\nGelling agents are food additives used to thicken and stabilize various foods, like jellies, desserts and candies. The agents provide the foods with texture through formation of a gel. Some stabilizers and thickening agents are gelling agents.\n\nTypical gelling agents include natural gums, starches, pectins, agar-agar and gelatin. Often they are based on polysaccharides or proteins.\n\nExamples are:\n\nCommercial jellies used in East Asian cuisines include the glucomannan polysaccharide gum used to make \"lychee cups\" from the konjac plants, and aiyu or ice jelly from the \"Ficus pumila\" climbing fig plant.\n\nFood thickening can be important for people facing medical issues with chewing or swallowing, as foods with a thicker consistency can reduce the chances of choking, or of inhalation of liquids or food particles, which can lead to aspiration pneumonia.\n\nFumed silica and similar products form stiff microscopic chains or fibers which interlock or agglomerate into a mass, holding the associated liquid by surface tension, but which can separate or slide when sufficient force is applied. This causes the thixotropic or shear-thinning property (also frequently exhibited by gels), where the viscosity is non-newtonian and becomes lower as the shearing force or time increases; their usefulness is primarily that the resulting increase in viscosity is large compared to the quantity of silica added. Fumed silica is generally accepted as safe as a food additive and is frequently used in cosmetics. Additives such as precipitated silica, fine talc, or chalk also meet the definition of \"thickening agent\" in that they increase viscosity and body while not affecting the target property of a mixture.\n\nThickening agents used in cosmetics or personal hygiene products include viscous liquids such as polyethylene glycol, synthetic polymers such as carbomer (a trade name for polyacrylic acid) and vegetable gums. Some thickening agents may also function as stabilizers when they are used to maintain the stability of an emulsion. Some emollients, such as petroleum jelly and various waxes may also function as thickening agents in an emulsion.\n\nOne of the main use of thickeners is in the paint and printing industries, which depend heavily on rheology modifiers, to prevent pigments settling to the bottom of the can, yielding inconsistent results. Water based formulas would be nearly impossible with the exception of India ink and the few other water-soluble pigments, but these would have very little coverage and at best would stain wood slightly. All modern paints and inks will have some pigment added at the factory for opacity and to control the specularity of the finish, from matte to high gloss, dependent on thickener used, but more so on the size of the particles added as opacity modifier. One-micrometer (µm) particle sizes and below will be the limit of high gloss, probably confined to luxury automotive coatings, and about 100 µm particulates will make a bumpy surface on the microscopic scale, which scatters light and makes the surface appear matte.\n\nRheology modifiers in common use:\n\nAll of the above rheology modifiers are used in the 0.2% to 2.0% range\n\nIn petrochemistry, gelling agents, also called solidifiers, are chemicals capable of reacting with oil spills and forming rubber-like solids. The gelled coagulated oil then can be removed from the water surface by skimming, suction devices, or nets. Calm or only moderately rough sea is required.\n\nVarious materials are used to convert liquid explosives to a gel form. Nitrocellulose and other nitro esters are often used. Other possibilities include nitrated guar gum.\n\nMany fuels used in incendiary devices require thickening for increased performance. Aluminium salts of fatty acids are frequently used. Some formulations (e.g. Napalm-B) use polymeric thickeners, namely polystyrene. Hydroxyl aluminium bis(2-ethylhexanoate) is also used. Thickened pyrophoric agent, a pyrophoric replacement of napalm, is a triethylaluminium thickened with polyisobutylene.\n\n\n"}
{"id": "42000479", "url": "https://en.wikipedia.org/wiki?curid=42000479", "title": "Valleytronics", "text": "Valleytronics\n\nValleytronics is a \"portmanteau\" combining the terms \"valley\" and \"electronics\". Certain semiconductors present multiple \"valleys\" in the first Brillouin zone, and are known as multivalley semiconductors. The term refers to the technology of control over the valley degree of freedom, a local maximum/minimum on the valence/conduction band, of such multivalley semiconductors. \n\nThe term was coined in analogy to the blooming field of spintronics. While in spintronics the internal degree of freedom of spin is harnessed to store, manipulate and read out bits of information, the proposal for valleytronics is to perform similar tasks using the multiple extrema of the band structure, so that the information of 0s and 1s would be stored as different discrete values of the crystal momentum.\n\nThe term is often used as an umbrella term to other forms of quantum manipulation of valleys in semiconductors, including quantum computation with valley-based qubits, valley blockade and other forms of quantum electronics. First experimental evidence of valley blockade predicted in Ref. (which completes the set of Coulomb charge blockade and Pauli spin blockade) has been observed in a single atom doped silicon transistor. \n\nSeveral theoretical proposals and experiments were performed in a variety of systems, such as graphene, few-layer phosphorene, some transition metal dichalcogenide monolayers, diamond, bismuth, silicon, carbon nanotubes, aluminium arsenide and silicene.\n\n"}
{"id": "44884825", "url": "https://en.wikipedia.org/wiki?curid=44884825", "title": "Vetigel", "text": "Vetigel\n\nVeti-gel is a veterinary product, a plant-derived injectable gel that is claimed to quickly stop traumatic bleeding on external and internal wounds. Its name is coined from Medi-Gel, from the video game series Mass Effect. It uses a plant-based haemophilic polymer made from polysaccharides that forms a mesh which seals the wound. It is manufactured by Suneris Inc, an American biotechnology company, which is also exploring human products derived from its technology, slated to launch as early as 2016. The company plans on releasing a product for the military and the emergency medicine market first, followed by a product for the human surgical market when FDA approval is granted.\nSuneris, Inc. is headquartered in Brooklyn, New York City, United States. The company was founded in 2010 by Joe Landolina and Isaac Miller, while they were students at NYU Poly. Suneris focuses on wound care products, specifically those in the field of hemostasis. The company operates out of a 2500 sq. ft. animal health manufacturing facility located in Park Slope, Brooklyn.\n\n\n"}
{"id": "24295042", "url": "https://en.wikipedia.org/wiki?curid=24295042", "title": "Winding factor", "text": "Winding factor\n\nIn power engineering, winding factor is a technique that is employed to improve the rms generated voltage or electromotive force (EMF) in a three-phase AC electrical machine so that the output voltage and hence torque does not have any harmonics in it which may reduce efficiency. This is because the armature winding of each phase is distributed in a number of slots. Since the EMF induced in different slots are not in phase, their phasor sum is less than their numerical sum. This reduction factor is called distribution factor K. Another factor that can reduce the winding factor is when the slot pitch is smaller than the pole pitch, called pitch factor K. \n\nThe winding factor can be calculated as \nK = K * K \n\nTo calculate the winding factor, \nK=K X K ----(1) \nWhere \nK=~1 (point here to keep in mind is that all(three phase) motors are balanced systems. \nK=Cos(a/2) ----(2) \nwhere a is the value of by which the coil is short-pitched \n\nTo calculate a, one needs to know and hence calculate the coil pitch and pole pitch of the motor. \n\nCoil pitch is 2pi/number of slots. It needs to be in electrical degrees so the answer has to be divided by 2. \n\nPole pitch is 2pi/number of poles. Again, divide the answer by 2 to get elec deg.\n\nThen a=pole pitch - coil pitch. \nAnd one can say that the winding is short pitched by a.\n\nPutting the value of a in (2) gives the K which will go in (1) to get the winding distribution factor. \n\nExample: \nFor a 3-phase 6 slot 4 pole non-overlapping winding motor: \nCoil pitch=2pi/6 = pi/3 (mech) and 2pi/3 (elec)g\nPole pitch =2pi/\n4=pi/2(mech) & pi (elec)\n\nMost of 3phase motors have winding factor values between 0.85 and 0.95.\n\nThe winding factor (along with some other factors like winding skew) can help to improve the Harmonic Contents in the generated EMF of machine.\n\n\n"}
