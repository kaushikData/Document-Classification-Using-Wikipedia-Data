{"id": "1923", "url": "https://en.wikipedia.org/wiki?curid=1923", "title": "Alessandro Volta", "text": "Alessandro Volta\n\nAlessandro Giuseppe Antonio Anastasio Volta (; 18 February 1745 – 5 March 1827) was an Italian physicist, chemist, and a pioneer of electricity and power, who is credited as the inventor of the electric battery and the discoverer of methane. He invented the Voltaic pile in 1799, and reported the results of his experiments in 1800 in a two-part letter to the President of the Royal Society. With this invention Volta proved that electricity could be generated chemically and debunked the prevalent theory that electricity was generated solely by living beings. Volta's invention sparked a great amount of scientific excitement and led others to conduct similar experiments which eventually led to the development of the field of electrochemistry.\n\nAlessandro Volta also drew admiration from Napoleon Bonaparte for his invention, and was invited to the Institute of France to demonstrate his invention to the members of the Institute. Volta enjoyed a certain amount of closeness with the Emperor throughout his life and he was conferred numerous honours by him. Alessandro Volta held the chair of experimental physics at the University of Pavia for nearly 40 years and was widely idolised by his students.\n\nDespite his professional success, Volta tended to be a person inclined towards domestic life and this was more apparent in his later years. At this time he tended to live secluded from public life and more for the sake of his family until his eventual death in 1827 from a series of illnesses which began in 1823. The SI unit of electric potential is named in his honour as the volt.\n\nVolta was born in Como, a town in present-day northern Italy, on 18 February 1745. In 1794, Volta married an aristocratic lady also from Como, Teresa Peregrini, with whom he raised three sons: Zanino, Flaminio, and Luigi. His father, Filippo Volta, was of noble lineage. His mother, Donna Maddalena, came from the family of the Inzaghis.\n\nIn 1774, he became a professor of physics at the Royal School in Como. A year later, he improved and popularised the electrophorus, a device that produced static electricity. His promotion of it was so extensive that he is often credited with its invention, even though a machine operating on the same principle was described in 1762 by the Swedish experimenter Johan Wilcke. In 1777, he travelled through Switzerland. There he befriended H. B. de Saussure.\n\nIn the years between 1776 and 1778, Volta studied the chemistry of gases. He researched and discovered methane after reading a paper by Benjamin Franklin of the United States on \"flammable air\". In November 1776, he found methane at Lake Maggiore, and by 1778 he managed to isolate methane. He devised experiments such as the ignition of methane by an electric spark in a closed vessel.\n\nVolta also studied what we now call electrical capacitance, developing separate means to study both electrical potential (\"V\" ) and charge (\"Q\" ), and discovering that for a given object, they are proportional. This is called Volta's Law of Capacitance, and it was for this work the unit of electrical potential has been named the volt.\n\nIn 1779 he became a professor of experimental physics at the University of Pavia, a chair that he occupied for almost 40 years.\n\nLuigi Galvani, an Italian physicist, discovered something he named, \"animal electricity\" when two different metals were connected in series with a frog's leg and to one another. Volta realised that the frog's leg served as both a conductor of electricity (what we would now call an electrolyte) and as a detector of electricity. He replaced the frog's leg with brine-soaked paper, and detected the flow of electricity by other means familiar to him from his previous studies.\n\nIn this way he discovered the electrochemical series, and the law that the electromotive force (emf) of a galvanic cell, consisting of a pair of metal electrodes separated by electrolyte, is the difference between their two electrode potentials (thus, two identical electrodes and a common electrolyte give zero net emf). This may be called Volta's Law of the electrochemical series.\n\nIn 1800, as the result of a professional disagreement over the galvanic response advocated by Galvani, Volta invented the voltaic pile, an early electric battery, which produced a steady electric current. Volta had determined that the most effective pair of dissimilar metals to produce electricity was zinc and copper. Initially he experimented with individual cells in series, each cell being a wine goblet filled with brine into which the two dissimilar electrodes were dipped. The voltaic pile replaced the goblets with cardboard soaked in brine.\n\nIn announcing his discovery of the voltaic pile, Volta paid tribute to the influences of William Nicholson, Tiberius Cavallo, and Abraham Bennet.\n\nThe battery made by Volta is credited as one of the first electrochemical cells. It consists of two electrodes: one made of zinc, the other of copper. The electrolyte is either sulfuric acid mixed with water or a form of saltwater brine. The electrolyte exists in the form 2H and SO. The zinc, which is higher in the electrochemical series than both copper and hydrogen, reacts with the negatively charged sulfate (SO). The positively charged hydrogen ions (protons) capture electrons from the copper, forming bubbles of hydrogen gas, H. This makes the zinc rod the negative electrode and the copper rod the positive electrode.\n\nThus, there are two terminals, and an electric current will flow if they are connected. The chemical reactions in this voltaic cell are as follows:\n\nThe copper does not react, but rather it functions as an electrode for the electric current.\n\nHowever, this cell also has some disadvantages. It is unsafe to handle, since sulfuric acid, even if diluted, can be hazardous. Also, the power of the cell diminishes over time because the hydrogen gas is not released. Instead, it accumulates on the surface of the copper electrode and forms a barrier between the metal and the electrolyte solution.\n\nIn 1809 Volta became associated member of the Royal Institute of the Netherlands. In honour of his work, Volta was made a count by Napoleon Bonaparte in 1810.\n\nVolta retired in 1819 to his estate in Camnago, a frazione of Como, Italy, now named \"Camnago Volta\" in his honour. He died there on 5 March 1827, just after his 82nd birthday. Volta's remains were buried in Camnago Volta.\n\nVolta's legacy is celebrated by the Tempio Voltiano memorial located in the public gardens by the lake. There is also a museum which has been built in his honour, which exhibits some of the equipment that Volta used to conduct experiments. Nearby stands the Villa Olmo, which houses the Voltian Foundation, an organization promoting scientific activities. Volta carried out his experimental studies and produced his first inventions near Como.\n\nHis image was depicted on the Italian 10,000 lira note (1990-1997) along with a sketch of his voltaic pile.\n\nVolta was raised as a Catholic and for all of his life continued to maintain his belief. Because he was not ordained a clergyman as his family expected, he was sometimes accused of being irreligious and some people have speculated about his possible unbelief, stressing that \"he did not join the Church\", or that he virtually \"ignored the church's call\". Nevertheless, he cast out doubts in a declaration of faith in which he said:\n\nI do not understand how anyone can doubt the sincerity and constancy of my attachment to the religion which I profess, the Roman, Catholic and Apostolic religion in which I was born and brought up, and of which I have always made confession, externally and internally. I have, indeed, and only too often, failed in the performance of those good works which are the mark of a Catholic Christian, and I have been guilty of many sins: but through the special mercy of God I have never, as far as I know, wavered in my faith... In this faith I recognise a pure gift of God, a supernatural grace; but I have not neglected those human means which confirm belief, and overthrow the doubts which at times arise. I studied attentively the grounds and basis of religion, the works of apologists and assailants, the reasons for and against, and I can say that the result of such study is to clothe religion with such a degree of probability, even for the merely natural reason, that every spirit unperverted by sin and passion, every naturally noble spirit must love and accept it. May this confession which has been asked from me and which I willingly give, written and subscribed by my own hand, with authority to show it to whomsoever you will, for I am not ashamed of the Gospel, may it produce some good fruit!\n\n\n"}
{"id": "38103559", "url": "https://en.wikipedia.org/wiki?curid=38103559", "title": "Amla Ruia", "text": "Amla Ruia\n\nAmla Ruia is an Indian social activist \n\nAmla Ruia is known for her work in water harvesting. She is the founder of the Aakar Charitable Trust(ACT), an organization that partners with villages to build check dams that provide water security. In 2011, she was awarded a Lakshmipat Singhania - IIM, Lucknow National Leadership Award in the category of Community Service and Social Upliftment.\n\nHer first check dam project was in Mandawar village which was a success. Farmers managed to earn up to Rs. 12 crores in a year via the check dams built by ACT. ACT has built 200 check dams in 100 villages in Rajasthan. 2 lakh villagers were able to generate an income of Rs. 300 crore per annum. Due to her intervention, dry villages in Rajasthan have had their bore wells and handpumps recharged. She is popularly referred to as 'Water Mother'.\n\nAmla and her team have extended their efforts in other states such as Madhya pradesh and Maharashtra. ACT is now working in the Dantewada district in Chhattisgarh.\n\nAmla Ruia was born in Uttar Pradesh. She currently lives in Malabar Hill, Mumbai, Maharashtra.\n"}
{"id": "1478918", "url": "https://en.wikipedia.org/wiki?curid=1478918", "title": "Britta Thomsen", "text": "Britta Thomsen\n\nBritta Thomsen (born 23 January 1954 in Aalborg) is a Danish politician and Member of the European Parliament. She is a member of the Social Democrats, which is part of the Party of European Socialists, and is vice-chair of the European Parliament's Committee on Industry, Research and Energy.\n\nShe is also a member of the Committee on Women's Rights and Gender Equality, a substitute for the Committee on Development, and vice-chair of the delegation for relations with South Africa. She recently backed up the Manifesto of the Spinelli Group.\n\n\n"}
{"id": "175841", "url": "https://en.wikipedia.org/wiki?curid=175841", "title": "Bumper cars", "text": "Bumper cars\n\nBumper cars or dodgems is the generic name for a type of flat ride consisting of several small electrically powered cars which draw power from the floor and/or ceiling, and which are turned on and off remotely by an operator. They are also known as bumping cars, dodging cars and dashing cars.\n\nPower is commonly supplied by one of three methods:\n\nThe metal floor is usually set up as a rectangular or oval track, and graphite is sprinkled on the floor to decrease friction. A rubber bumper surrounds each vehicle, and drivers either ram or dodge each other as they travel. The controls are usually an accelerator and a steering wheel. The cars can be made to go backwards by turning the steering wheel far enough in either direction, necessary in the frequent pile-ups that occur.\n\nAlthough the idea of the ride is to bump other cars, safety-conscious (or at least litigation-conscious) owners sometimes put up signs reading \"This way around\" and \"No (head on) bumping.\" Depending on the level of enforcement by operators, these rules are often ignored by bumper car riders, especially younger children and teenagers. \n\nDuring their heyday, from the late 1920s to 1950s, two major US bumper cars brands were Dodgem by Max and Harold Stoehrer and the Lusse Brothers' Auto-Skooter by Joseph and Robert 'Ray' Lusse. Lusse Brothers built the first fiberglass body in 1959, in part due to the survival of Chevrolet Corvette bodies over the previous six years. After getting permission from Chevrolet, then subsequently buying the actual Corvette chevrons from local Philadelphia dealers, those were attached to the nose of their product for 1959. In the mid-1960s, Disneyland introduced hovercraft-based bumper cars called Flying Saucers, which worked on the same principle as an air hockey game; however, the ride was a mechanical failure and closed after a few years.\n\nThe current largest operating bumper car floor is located at Six Flags Great America in Gurnee, Illinois, and is called the Rue Le Dodge (Rue Le Morgue during Fright Fest in the fall). The ride is by or a total of . A replica of the ride was built at California's Great America in Santa Clara; in 2005, however, a concrete island was added to the middle of the floor to promote one-way traffic, reducing the floor area. \nSix Flags Great Adventure's Autobahn is the largest bumper car floor, but it has not operated since 2008.\n\n\n"}
{"id": "19440608", "url": "https://en.wikipedia.org/wiki?curid=19440608", "title": "Butterfly (2000 film)", "text": "Butterfly (2000 film)\n\nButterfly, which first aired on public television in 2000, is a documentary film directed by Doug Wolens about the environmental heroine and tree sitter Julia Butterfly Hill who gained the attention of the world for her two-year vigil 180 feet atop Luna, an ancient redwood tree preventing it from being clear-cut.\n\n\n\n"}
{"id": "37962744", "url": "https://en.wikipedia.org/wiki?curid=37962744", "title": "Cabinetry", "text": "Cabinetry\n\nA cabinet is a box-shaped piece of furniture with doors and/or drawers for storing miscellaneous items. Some cabinets stand alone while others are built in to a wall or are attached to it like a medicine cabinet. Cabinets are typically made of wood (solid or with veneers or artificial surfaces), coated steel (common for medicine cabinets), or synthetic materials. Commercial grade cabinets, which differ in the materials used, are called casework, casegoods, or case furniture.\n\nCabinets usually have one or more doors on the front, which are mounted with door hardware, and occasionally a lock. Cabinets may have one or more doors, drawers, and/or shelves. Short cabinets often have a finished surface on top that can be used for display, or as a working surface, such as the countertops found in kitchens.\n\nA cabinet intended to be used in a bedroom and with several drawers typically placed one above another in one or more columns intended for clothing and small articles is called a chest of drawers. A small bedside cabinet is more frequently called a nightstand or night table. A tall cabinet intended for clothing storage including hanging of clothes is called a \"wardrobe\" or an \"armoire\", or (in some countries) a \"closet\" if built-in.\n\nBefore the advent of industrial design, cabinet makers were responsible for the conception and the production of any piece of furniture. In the last half of the 18th century, cabinet makers, such as Thomas Sheraton, Thomas Chippendale, Shaver and Wormley Bros. Cabinet Constructors, and George Hepplewhite, also published books of furniture forms. These books were compendiums of their designs and those of other cabinet makers. The most famous cabinetmaker before the advent of industrial design is probably André-Charles Boulle (11 November 1642 – 29 February 1732) and his legacy is known as \"Boulle Work\" and the École Boulle, a college of fine arts and crafts and applied arts in Paris, today bears testimony to his Art.\n\nWith the industrial revolution and the application of steam power to cabinet making tools, mass production techniques were gradually applied to nearly all aspects of cabinet making, and the traditional cabinet shop ceased to be the main source of furniture, domestic or commercial. In parallel to this evolution there came a growing demand by the rising middle class in most industrialised countries for finely made furniture. This eventually resulted in a growth in the total number of traditional cabinet makers.\n\nBefore 1650, fine furniture was a rarity in Western Europe and North America. Generally, people did not need it and for the most part could not afford it. They made do with simple but serviceable pieces.\n\nThe arts and craft movement which started in the United Kingdom in the middle of the 19th century spurred a market for traditional cabinet making, and other craft goods. It rapidly spread to the United States and to all the countries in the British Empire. This movement exemplified the reaction to the eclectic historicism of the Victorian era and to the 'soulless' machine-made production which was starting to become widespread.\n\nAfter World War II woodworking became a popular hobby among the middle classes. The more serious and skilled amateurs in this field now turn out pieces of furniture which rival the work of professional cabinet makers. Together, their work now represents but a small percentage of furniture production in any industrial country, but their numbers are vastly greater than those of their counterparts in the 18th century and before.\n\nThis style of design is typified by clean horizontal and vertical lines. Compared to other designs there is a distinct absence of ornamentation. While Scandinavian design is easy to identify, it is much more about the materials than the design.\n\nThis style of design is very ornate. French Provincial objects are often stained or painted, leaving the wood concealed. Corners and bevels are often decorated with gold leaf or given some other kind of gilding. Flat surfaces often have artwork such as landscapes painted directly on them. The wood used in French provincial varied, but was often originally beech.\n\nThis design emphasises both form and materials. Early American chairs and tables are often constructed with turned spindles and chair backs often constructed using steaming to bend the wood. Wood choices tend to be deciduous hardwoods with a particular emphasis on the wood of edible or fruit-bearing trees such as cherry or walnut.\n\nThe rustic style of design sometimes called \"log furniture\" or \"log cabin\" is the least finished. Design is very utilitarian yet seeks to feature not only the materials used but in, as much as possible, how they existed in their natural state. For example, a table top may have what is considered a \"live edge\" that allows you to see the original contours of the tree that it came from. It also often uses whole logs or branches including the bark of the tree. Rustic furniture is often made from pine, cedar, fir and spruce. Also see Adirondack Architecture.\n\nMission Design is characterized by straight, thick horizontal and vertical lines and flat panels. The most common material used in Mission furniture is oak. For early mission cabinetmakers, the material of choice was white oak, which they often darkened through a process known as \"fuming\". Hardware is often visible on the outside of the pieces and made of black iron. It is a style that became popular in the early 20th century; popularized by designers in the Arts and Crafts and Art Nouveaux movements.\n\nAlso known as Asian Design, this style of furniture is characterized by its use of materials such as bamboo and rattan. Red is a frequent color choice along with landscape art and Chinese or other Asian language characters on the pieces.\n\nShaker furniture design is focused on function and symmetry. Because it is so influenced by an egalitarian religious community and tradition it is rooted in the needs of the community versus the creative expression of the designer. Like Early American and Colonial design, Shaker craftsmen often chose fruit woods for their designs. Pieces reflect a very efficient use of materials.\n\nThe fundamental focus of the cabinet maker is the production of cabinetry. Although the cabinet maker may also be required to produce items that would not be recognized as cabinets, the same skills and techniques apply.\n\nA cabinet may be built-in or free-standing. A built-in cabinet is usually custom made for a particular situation and it is fixed into position, on a floor, against a wall, or framed in an opening. For example, modern kitchens are examples of built-in cabinetry. Free-standing cabinets are more commonly available as off-the-shelf items and can be moved from place to place if required. Cabinets may be wall hung or suspended from the ceiling. Cabinet doors may be hinged or sliding and may have mirrors on the inner or outer surface.\n\nCabinets may have a face frame or may be of frameless construction (also known as \"European\" or \"euro-style\"). Face frame cabinets have a supporting frame attached to the front of the cabinet box. This face frame is usually in width. Mounted on the cabinet frame is the cabinet door. In contrast, frameless cabinet have no such supporting front face frame, the cabinet doors attach directly to the sides of the cabinet box. The box's side, bottom and top panels are usually thick, with the door overlaying all but of the box edge.\nModern cabinetry is often frameless and is typically constructed from man-made sheet materials, such as plywood, chipboard or medium-density fibreboard (MDF). The visible surfaces of these materials are usually clad in a timber veneer, plastic laminate, or other material. They may also be painted.\n\nCabinets which rest on the floor are supported by some base. This base could be a fully enclosed base (i.e. a plinth), a scrolled based, bracket feet or it could be a set of legs.\n\nA type of adjustable leg has been adopted from the European cabinet system which offers several advantages. First off, in making base cabinets for kitchens, the cabinet sides would be cut to 34½ inches, yielding four cabinet side blanks per 4 foot by 8 foot sheet. Using the adjustable feet, the side blanks are cut to 30 inches, thus yielding six cabinet side per sheet.\n\nThese feet can be secured to the bottom of the cabinet by having the leg base screwed onto the cabinet bottom. They can also be attached by means of a hole drilled through the cabinet bottom at specific locations. The legs are then attached to the cabinet bottom by a slotted, hollow machine screw. The height of the cabinet can be adjusted from inside the cabinet, simply by inserting a screwdriver into the slot and turning to raise or lower the cabinet. The holes in the cabinet are capped by plastic inserts, making the appearance more acceptable for residential cabinets. Using these feet, the cabinets need not be shimmed or scribed to the floor for leveling. The toe kick board is attached to the cabinet by means of a clip, which is either screwed onto the back side of the kick board, or a barbed plastic clip is inserted into a saw kerf, also made on the back side of the kick board. This toe kick board can be made to fit each base cabinet, or made to fit a run of cabinets.\n\nKitchen cabinets, or any cabinet generally at which a person may stand, usually have a fully enclosed base in which the front edge has been set back 75 mm or so to provide room for toes, known as the kick space. A scrolled base is similar to the fully enclosed base but it has areas of the base material removed, often with a decorative pattern, leaving feet on which the cabinet stands. Bracket feet are separate feet, usually attached in each corner and occasionally for larger pieces in the middle of the cabinet.\n\nA cabinet usually has at least one compartment. Compartments may be open, as in open shelving; they may be enclosed by one or more doors; or they may contain one or more drawers. Some cabinets contain secret compartments, access to which is generally not obvious.\n\nModern cabinets employ many more complicated means (relative to a simple shelf) of making browsing lower cabinets more efficient and comfortable. One example is the lazy susan, a shelf which rotates around a central axis, allowing items stored at the back of the cabinet to be brought to the front by rotating the shelf. These are usually used in corner cabinets, which are larger and deeper and have a greater \"dead space\" at the back than other cabinets.\n\nAn alternative to the lazy susan, particularly in base cabinets, is the blind corner cabinet pull out unit. These pull out and turn, making the attached shelving unit slide into the open area of the cabinet door, thus making the shelves accessible to the user. These units make usable what was once dead space.\n\nOther insert hardware includes such items as mixer shelves that pull out of a base cabinet and spring into a locked position at counter height. This hardware aids in lifting these somewhat heavy mixers and assists with positioning the unit for use. More and more components are being designed to enable specialized hardware to be used in standard cabinet carcasses.\n\nMost cabinets incorporate a top of some sort. In many cases, the top is merely to enclose the compartments within and serves no other purpose—as in a wall hung cupboard for example. In other cabinets, the top also serves as a work surface—a kitchen countertop for example.\n\n\n\n\n"}
{"id": "349627", "url": "https://en.wikipedia.org/wiki?curid=349627", "title": "Calcium chloride", "text": "Calcium chloride\n\nCalcium chloride is an inorganic compound, a salt with the chemical formula CaCl. It is a colorless crystalline solid at room temperature, highly soluble in water.\n\nCalcium chloride is commonly encountered as a hydrated solid with generic formula CaCl(HO), where \"x\" = 0, 1, 2, 4, and 6. These compounds are mainly used for de-icing and dust control. Because the anhydrous salt is hygroscopic, it is used as a desiccant.\n\nBy depressing the freezing point of water, calcium chloride is used to prevent ice formation and is used to de-ice. This application consumes the greatest amount of calcium chloride. Calcium chloride is relatively harmless to plants and soil. As a de-icing agent, it is much more effective at lower temperatures than sodium chloride. When distributed for this use, it usually takes the form of small, white spheres a few millimeters in diameter, called prills. Solutions of calcium chloride can prevent freezing at temperature as low as −52 °C (−62 °F), making it ideal for filling agricultural implement tires as a liquid ballast, aiding traction in cold climates.\n\nIt is also used in domestic and industrial chemical air dehumidifiers.\n\nThe second largest application of calcium chloride exploits hygroscopic properties and the tackiness of its hydrates. A concentrated solution keeps a liquid layer on the surface of dirt roads, which suppresses formation of dust. It keeps the finer dust particles on the road, providing a cushioning layer. If these are allowed to blow away, the large aggregate begins to shift around and the road breaks down. Using calcium chloride reduces the need for grading by as much as 50% and the need for fill-in materials as much as 80%.\n\nCalcium chloride is used to increase the water hardness in swimming pools. This process reduces the erosion of the concrete in the pool. By Le Chatelier's principle and the common-ion effect, increasing the concentration of calcium in the water reduces the dissolution of calcium compounds essential to the structure of concrete.\n\nThe average intake of calcium chloride as food additives has been estimated to be 160–345 mg/day. Calcium chloride is permitted as a food additive in the European Union for use as a sequestrant and firming agent with the E number E509. It is considered as generally recognized as safe (GRAS) by the U.S. Food and Drug Administration. Its use in organic crop production is generally prohibited under the US National Organic Program.\n\nIn marine aquariums, calcium chloride is one way to introduce bioavailable calcium for calcium carbonate-shelled animals such as mollusks and some cnidarians. Calcium hydroxide (kalkwasser mix) or a calcium reactor can also be used.\n\nAs a firming agent, calcium chloride is used in canned vegetables, in firming soybean curds into tofu and in producing a caviar substitute from vegetable or fruit juices. It is commonly used as an electrolyte in sports drinks and other beverages, including bottled water. The extremely salty taste of calcium chloride is used to flavor pickles without increasing the food's sodium content. Calcium chloride's freezing-point depression properties are used to slow the freezing of the caramel in caramel-filled chocolate bars. Also, it is frequently added to sliced apples to maintain texture.\n\nIn brewing beer, calcium chloride is sometimes used to correct mineral deficiencies in the brewing water. It affects flavor and chemical reactions during the brewing process, and can also affect yeast function during fermentation.\n\nIn cheesemaking, calcium chloride is sometimes added to processed (pasteurized/homogenized) milk to restore the natural balance between calcium and protein in casein. It is added before the coagulant.\n\nCalcium chloride is used to prevent cork spot and bitter pit on apples by spraying on the tree during the late growing season.\n\nDrying tubes are frequently packed with calcium chloride. Kelp is dried with calcium chloride for use in producing sodium carbonate. Anhydrous calcium chloride has been approved by the FDA as a packaging aid to ensure dryness (CPG 7117.02).\n\nIt is injected to treat internal hydrofluoric acid burns. It can be used to treat magnesium intoxication. Calcium chloride injection may improve the appearance of an electrocardiogram. It can help to protect the myocardium from dangerously high levels of serum potassium in hyperkalemia. Calcium chloride can be used to quickly treat calcium channel blocker toxicity, from the side effects of drugs such as diltiazem.\n\nWhile intravenous calcium has been used to treat cardiac arrest, its general use is not recommended. Cases of cardiac arrest in which it is still recommended include high blood potassium, low blood calcium such as may occur following blood transfusions, and calcium channel blocker overdose. There is the potential that general use could worsen outcomes. If calcium is used, calcium chloride is generally the recommended form.\n\nAqueous calcium chloride is used in genetic transformation of cells by increasing the cell membrane permeability, inducing competence for DNA uptake (allowing DNA fragments to enter the cell more readily).\n\nCalcium chloride is used in concrete mixes to accelerate the initial setting, but chloride ions lead to corrosion of steel rebar, so it should not be used in reinforced concrete. The anhydrous form of calcium chloride may also be used for this purpose and can provide a measure of the moisture in concrete.\n\nCalcium chloride is included as an additive in plastics and in fire extinguishers, in wastewater treatment as a drainage aid, in blast furnaces as an additive to control scaffolding (clumping and adhesion of materials that prevent the furnace charge from descending), and in fabric softener as a thinner.\n\nThe exothermic dissolution of calcium chloride is used in self-heating cans and heating pads.\n\nIn the oil industry, calcium chloride is used to increase the density of solids-free brines. It is also used to provide inhibition of swelling clays in the water phase of invert emulsion drilling fluids.\n\nCaCl acts as flux material (decreasing melting point) in the Davy process for the industrial production of sodium metal, through the electrolysis of molten NaCl.\n\nSimilarly, CaCl is used as a flux and electrolyte in the FFC Cambridge process for titanium production, where it ensures the proper exchange of calcium and oxygen ions between the electrodes.\n\nCalcium chloride is also used in the production of activated charcoal.\n\nCalcium chloride is also an ingredient used in ceramic slipware. It suspends clay particles so that they float within the solution making it easier to use in a variety of slipcasting techniques.\n\nCalcium chloride dihydrate (20% by weight) dissolved in ethanol (95% ABV) has been used as a sterilant for male animals. The solution is injected into the testes of the animal. Within 1 month, necrosis of testicular tissue results in sterilization.\n\nCalcium chloride can act as an irritant by desiccating moist skin. Solid calcium chloride dissolves exothermically, and burns can result in the mouth and esophagus if it is ingested. Ingestion of concentrated solutions or solid products may cause gastrointestinal irritation or ulceration.\n\nConsumption of calcium chloride can lead to hypercalcemia.\n\nCalcium chloride dissolves in water, producing chloride and the aquo complex [Ca(HO)]. In this way, these solutions are sources of \"free\" calcium and free chloride ions. This description is illustrated by the fact that these solutions react with phosphate sources to give a solid precipitate of calcium phosphate:\nCalcium chloride has a very high enthalpy change of solution, indicated by considerable temperature rise accompanying dissolution of the anhydrous salt in water. This property is the basis for its largest-scale application.\n\nMolten calcium chloride can be electrolysed to give calcium metal and chlorine gas:\n\nIn much of the world, calcium chloride is derived from limestone as a by-product of the Solvay process: North American consumption in 2002 was 1,529,000 tonnes (3.37 billion pounds).\n\nIn the US, most of calcium chloride is obtained by purification from brine. A Dow Chemical Company manufacturing facility in Michigan houses about 35% of the total U.S. production capacity for calcium chloride.\n\nAs with most bulk commodity salt products, trace amounts of other cations from the alkali metals and alkaline earth metals (groups 1 and 2) and other anions from the halogens (group 17) typically occur, but the concentrations are trifling.\n\nCalcium chloride occurs as the rare evaporite minerals sinjarite (dihydrate) and antarcticite (hexahydrate). The related minerals chlorocalcite (potassium calcium chloride, KCaCl) and tachyhydrite (calcium magnesium chloride, CaMgCl·12HO) are also very rare.\n\n\n"}
{"id": "56926930", "url": "https://en.wikipedia.org/wiki?curid=56926930", "title": "Carbon Offsetting and Reduction Scheme for International Aviation", "text": "Carbon Offsetting and Reduction Scheme for International Aviation\n\nCarbon Offsetting and Reduction Scheme for International Aviation, or CORSIA, is an emission mitigation approach for the global airline industry, developed by the International Civil Aviation Organization (ICAO). CORSIA addresses emissions from international air travel. The proposal has been described as \"a delicate compromise between all involved in its elaboration.\" \n\nAs per an IPCC report published in 1999, aviation accounted for approximately two percent of global carbon emissions due to human activity, as of 1992 . Per capita emissions from air travel is one of the highest in comparison to various other modes of transportation . The industry is expecting a growth in air travel in various regions. Through CORSIA, the aviation industry is aiming for a carbon neutral growth from 2020.\n\nMany airlines offers an option for airline passengers, for an additional payment, to offset their emissions associated with their air travel. Few airlines such as NatureAir and Harbour Air have been carbon-neutral. There are start-up aviation ventures such as FlyPOP that are aiming to be carbon neutral .\n\nCORSIA has three implementation phases, beginning 2021. Participation of countries till 2026 is voluntary.\n\nAs of January 2018, more than 70 countries representing more than 85% of international aviation activity have volunteered to participate. India and Russia are yet to join CORSIA. India, which has four of the five carbon-neutral airports in the Asia-Pacific region and the world's first fully solar powered airport, has drawn attention to \"differentiated responsibilities\" and the \"need to ensure the transfer of financial resources, technology transfer and deployment and capacity building support to developing countries for enabling them to voluntarily undertake action plans.\".\n\nLeast Developed Countries, Small Island Developing States and Landlocked Developing Countries can volunteer to participate in CORSIA, while it is not mandated on them. However, all ICAO member states \"with aeroplane operators conducting international flights are required to monitor, report and verify (MRV) CO2 emissions from these flights every year from 2019\". All aeroplane operators with CO2 emissions less than or equal to 10,000 tonnes are exempted from the CORSIA reporting requirements.\n\nCORSIA is a market based mechanism focusing on offsetting emissions through the process of an airline purchasing emission units equivalent to its offsetting requirements. Offsetting processes may tend to divert the focus from reducing emissions, to trading on emissions . \nIt is also said that CORSIA is not as stringent as EU ETS .\nWhile the scheme may address a significant percentage of emissions from international aviation, it may not fully contribute to a 'carbon neutral growth'. CORSIA has also not stated any upper limit to the aviation related emissions that may be produced by an airline operator or a country.\n\nEmissions from domestic air travel is not included in CORSIA. ICAO states that \"Emissions from domestic aviation, as other domestic sources, are addressed under the UNFCCC and calculated as part of the national GHG inventories and are included in national totals (part of the Nationally Determined Contributions (NDCs))...\" \n\n\n"}
{"id": "8569148", "url": "https://en.wikipedia.org/wiki?curid=8569148", "title": "Cascade filling system", "text": "Cascade filling system\n\nA cascade filling system is a high pressure gas cylinder storage system which is used for the refilling of smaller compressed gas cylinders. In some applications, each of the large cylinders is filled by a compressor, otherwise they may be filled remotely and replaced when the pressure is too low for effective transfer. The cascade system allows small cylinders to be filled without a compressor. In addition, a cascade system is useful as a reservoir to allow a low-capacity compressor to meet the demand of filling several small cylinders in close succession, with longer intermediate periods during which the storage cylinders can be recharged.\n\nWhen gas contained in a cylinder at high pressure is allowed to flow to another cylinder containing gas at a lower pressure, the pressures will equalise to a value somewhere between the two initial pressures. The equilibrium pressure is affected by transfer rate as it will be influenced by temperature, but at constant temperature the equilibrium pressure is described by Dalton's law of partial pressures and Boyle's law for ideal gases.\n\nThe formula for the equilibrium pressure is:\n\nAn example could be a 100-litre (internal volume) cylinder (V) pressurised to 200 bar (P) filling a 10-litre (internal volume) cylinder (V) which was unpressurised (P = 1 bar) (resulting in both cylinder equalising to approximately 180 bar (P). If another 100 litre cylinder pressurised this time to 250 bar were then used to \"top-up\" the 10 litre cylinder, both of these cylinders would equalise to about 240 bar. However, if the higher pressure 100 litre cylinder were used first, the 10 litre cylinder would equalise to about 225 bar and the lower pressure 100 litre cylinder could not be used to top it up. In a cascade storage system, several large cylinders are used to bring a small cylinder up to a desired pressure, by always using the supply cylinder with the lowest usable pressure first, then the cylinder with the next lowest pressure, and so on.\n\nIn practice the theoretical transfers can only be achieved if the gases are allowed to reach a temperature equilibrium before disconnection. This requires significant time, and a lower efficiency may be accepted to save time. Actual transfer can be calculated using the general gas equation of state if the temperature of the gas in the cylinder is accurately measured.\n\nA breathing set cylinder may be filled to its working pressure by decanting from larger (often 50 litre) cylinders. (To make this easy the neck of the cylinder of the Siebe Gorman Salvus rebreather had the same thread as an oxygen storage cylinder, but the opposite gender, for direct decanting.) The storage cylinders are available in a variety of sizes, typically from 50 litre internal capacity to well over 100 litres.\n\nIn the more general case a high pressure hose known as a filling whip is used to connect the filling panel or storage cylinder to the receiving cylinder.\n\nCascade filling is often used for partial pressure blending of breathing gas mixtures for diving, to economize on the relatively expensive oxygen, for nitrox, and the even more expensive helium in trimix or heliox mixtures.\n\nCascade storage is used at compressed natural gas (CNG) fueling stations. Typically three CNG tanks will be used, and a vehicle will first be fueled from one of them, which will result an incomplete fill, perhaps to 2000 PSI for a 3000 PSI tank. The second and third tanks will bring the vehicle's tank closer to 3000 PSI. The station normally has a compressor, which refills the station's tanks, using natural gas from a utility line. This prevents accidentally overfilling the tank, which could happen with a system using a single fueling tank at a higher pressure than the target pressure for the vehicle.\n\nIn cascade storage systems for hydrogen storage, as for example at hydrogen stations, fuel dispenser A draws hydrogen from tank A, while dispenser B draws fuel from hydrogen tank B. If dispenser A is over-utilized, tank A will become depleted before tank B. At this point the dispenser A is switched to tank C. Tank C will then supply dispenser A and B and tank A until tank A is filled to the same pressure as tank B and the dispensers are disconnected, after which the control system will close the control valves to switch to its former state.\n\nThe storage cylinders may be used independently in sequence using a portable transfer whip with a pressure gauge and manual bleed valve, to transfill the receiving cylinder until the appropriate fill pressure has been reached, or the storage cylinders may be connected to a manifold system and a filling control panel with one or more filling whips. \n\nIdeally each storage cylinder has an independent connection to the filling panel with a contents pressure gauge and supply valve dedicated to that cylinder, and a filling gauge connected to the filling whip, so the operator can see at a glance the next higher storage cylinder pressure compared to the receiving cylinder pressure.\n\nThe storage cylinders may be filled remotely and connected to the manifold by a flexible hose when in use, or may be permanently connected and refilled by a compressor through a dedicated filling system, which may be automated or manually controlled.\n\nAn over-pressure safety valve is usually installed inline between the compressor and the storage units to protect the cylinders from overfilling, and each cylinder may also be protected by a rupture disc.\n"}
{"id": "263660", "url": "https://en.wikipedia.org/wiki?curid=263660", "title": "Circle of forces", "text": "Circle of forces\n\nThe circle of forces, traction circle, friction circle, or friction ellipse is a useful way to think about the dynamic interaction between a vehicle's tire and the road surface. The diagram below shows the tire from above, so that the road surface lies in the \"x\"-\"y\" plane. The vehicle to which the tire is attached is moving in the positive \"y\" direction.\n\nIn this example, the vehicle would be cornering to the right (i.e. the positive \"x\" direction points to the center of the corner). Note that the plane of rotation of the tire is at an angle to the actual direction that the tire is moving (the positive \"y\" direction). This is the slip angle.\n\nA tire can generate horizontal force where it meets the road surface by the mechanism of slip. That force is represented in the diagram by the vector F. Note that in this example F is perpendicular to the plane of the tire. That is because the tire is rolling freely, with no torque applied to it by the vehicle's brakes or drive train. However, that is not always the case.\n\nThe magnitude of F is limited by the dashed circle, but it can be any combination of the components F and F that does not extend beyond the dashed circle. (For a real-world tire, the circle is likely to be closer to an ellipse, with the \"y\" axis slightly longer than the \"x\" axis.)\n\nIn the example, the tire is generating a component of force in the \"x\" direction (F) which, when transferred to the vehicle's chassis via the suspension system in combination with similar forces from the other tires, will cause the vehicle to turn to the right. Note that there is also a small component of force in the negative \"y\" direction (F). This represents drag that will, if not countered by some other force, cause the vehicle to decelerate. Drag of this kind is an unavoidable consequence of the mechanism of slip, by which the tire generates lateral force.\n\nThe diameter of the circle of forces, and therefore the maximum horizontal force that the tire can generate, depends upon many factors, including the design of the tire and its condition (age and temperature, for example), the qualities of the road surface, and the vertical load on the tire.\n\n"}
{"id": "8084965", "url": "https://en.wikipedia.org/wiki?curid=8084965", "title": "Coffee wastewater", "text": "Coffee wastewater\n\nCoffee wastewater, also known as coffee effluent, is a byproduct of coffee processing. Its treatment and disposal is an important environmental consideration for coffee processing as wastewater is a form of industrial water pollution.\n\nThe unpicked fruit of the coffee tree, known as the coffee cherry, undergoes a long process to make it ready for consumption. This process often entails use of large quantities of water and the production of considerable amounts of solid and liquid waste. The type of waste is a result of the type of process that the coffee cherries go through. The conversion of the cherry to \"oro\" or \"green bean\" (the dried coffee bean which is ready to be exported) is achieved through either a dry, semi-washed or fully washed process.\n\nThe coffee cherries are dried immediately after they are harvested through sun drying, solar drying or artificial drying. In sun drying, the coffee cherries are placed on a clean floor and left to dry in the open air. In solar drying, the cherries are placed in a closed cabinet, which has ventilation holes to let moisture out. Artificial drying is used mostly during the wet season, when the low level of sunlight extends the time needed for solar drying and the cherries are prone to mold growth. After being dried, the cherries are hulled. In this process the dried outer layer of the cherry, known as the pericarp, is removed mechanically.\n\nIn semi-washed processing, the cherries are de-pulped to remove the pericarp. After this the slimy mucilage layer which covers the bean is removed. This is done mechanically by feeding the beans into a cylindrical device which conveys them upward. While the friction and pressure exerted on the beans by this process is enough to remove most of the mucilage, a small amount of it will still remain in the centre-cut of the beans. This technique is used in Colombia and Mexico in order to reduce the water consumption from the long fermentation process and the extensive washing.\n\nIn order to reduce the contamination generated by the wet process of coffee fruits, scientists at Cenicafé developed a technology that avoids using water when not needed and uses the right water when needed. The technology, called \"Becolsub\" (taken from the initials of the Spanish for ecological wet coffee process with by-products handling: \"Beneficio Ecologicos Sub-productos\"), controls more than 90% of the contamination generated by its predecessor. The quality of the coffee processed this way is the same as for coffee processed by natural fermentation.\n\nThe Becolsub technology consists of pulping without water, mechanical demucilaging and mixing the by-products (fruit outer-skin and mucilage) in a screw conveyor. The technology also includes a hydromechanical device to remove floating fruits and light impurities, as well as heavy and hard objects, and a cylindrical screen to remove the fruits whose skin was not separated in the pulping machine. Scientists at Cenicafé discovered that a coffee fruit with mucilage (immature and dry fruits have no mucilage) has enough water inside for the skin and seeds to be separated in conventional pulping machines without water, that the liquid was only required as a conveying means and that pulping without water avoids 72% of the potential contamination.\n\nMucilage removal has been done through a fermenting process, which takes between 14 and 18 hours, until the mucilage is degraded and can easily be removed with water. Washing fermented mucilage requires, in the best case, 5.0 L/kg of DPC. Scientists at Cenicafé developed a machine to remove the mucilage covering the coffee seeds. This machine, called Deslim (the initial letters of the Spanish demucilager, the mechanical washer and cleaner) removes more than 98% of the total mucilage (same as a well conducted fermentation) by exerting stress and generating collisions among beans, using only 0.7 L/kg of DPC. The resulting highly concentrated mixture of water, mucilage and impurities is viscous and is added to the separated fruit skin in a screw conveyor. In the screw conveyor the retention is greater than 60%, which means a 20% additional control of potential contamination.\n\nThe two by-products are widely used as worms' substrate to produce natural fertilizers. However, the high concentration of the mucilage obtained from the demucilager provides opportunity for industrializing the by-product.\n\nThis process is mainly used when processing \"Coffea arabica\". After de-pulping, the beans are collected in fermentation tanks where bacterial removal of the mucilage takes place over 12 to 36 hours. The fermentation phase is important in the development of the flavour of the coffee, which is partially due to the microbiological processes that take place. The emergence of yeasts and moulds in acidic water can lead to off-flavors like \"sour coffee\" and \"onion-flavour\". However, wet processing is believed to yield higher quality coffee than the other processes since small amounts of off-flavors give the coffee its particular taste and \"body\".\n\nWhen fermentation is complete, the beans are washed thoroughly to remove fermentation residues and any remaining mucilage. If not removed, these cause decolouring of the parchment and make the beans susceptible to yeasts. After washing, the beans are dried. When the drying process is not rapid enough earthy and musty taints, like \"Rio-flavor\" come up.\n\nThe amount of water used in processing depends strongly on the type of processing. Wet fully washed processing of the coffee cherries requires the most fresh water, dry processing the least. Sources indicate a wide range in water use. Recycling of water in the de-pulping process can drastically reduce the amount needed. With reuse and improved washing techniques, up to 1 to 6 m³ water per tonne of fresh coffee cherry is achievable; without reuse a consumption of up to 20 m³/tonne is possible.\n\nWater used in processing coffee leaves the coffee processing unit with high levels of pollution. The main component is organic matter, stemming from de-pulping and mucilage removal. The majority of organic material in the wastewater is highly resistant and COD values, the amount oxygen required to stabilize organic matter by using a strong oxidant, make up 80% of the pollution load, with values as high as 50 g/l. The BOD, the amount of oxygen required for the biological decomposition of organic matter under aerobic conditions at a standardized temperature and time of incubation, coming from biodegradable organic material can reach values of 20 g/l.\n\nWith a (rough) screening and removal of the pulp COD and BOD values become considerably lower. Values in the range of 3 - 5 g/l for COD and 1.5– 3 g/l for BOD were found. Recorded values of 2.5 g/l for COD and 1.5 g/l for BOD.\n\nA large part of the organic matter, pectins, precipitates as mucilated solids and could be taken out of the water. When these solids are not removed and pH values rise and an increase in COD can be observed.\n\nIn order to optimize the anaerobic processing of the wastewater pH values should be between 6.5 and 7.5, instead of the generally present values of pH=4, which is highly acidic. This is obtained by adding calcium hydroxide (CaOH) to the wastewater. This resulted in a regained solubility of the pectins, raising COD from an average of 3.7 g/l to an average of 12.7 g/l.\n\nThe water is further characterised by the presence of flavonoid compounds, coming from the skin of the cherries. Flavonoid compounds result in dark colouration of the water at a pH=7 or higher, but they do not add to BOD or COD levels of the wastewater, nor have major environmental impacts. Lower levels of transparency, however, can have a negative impact on photosynthetic processes and growth and nutrient transformations by (especially) rooted water plants. Many efforts in olive and wine processing industries, with relatively large funds for research, have been trying to find a solution for this problem. Calvert mentions research done into the removal of polyphenolics and flavonoid compounds by species of wood digesting fungi (Basidiomycetes) in a submerged solution with aeration using compressed air. These complex processes seemed to be able to remove the colour compounds, but simplified, cheaper techniques using other types of fungi (i.e. \"Geotrichum\", \"Penicillium\", \"Aspergillus\") only thrived in highly diluted wastewaters.\n\nCoffee wastewater is not a constant flow of water with uniform loadings of contamination. The processing of coffee cherries is a batch process and regarding water flows, two processes can be determined: de-pulping and fermentation/washing.\n\nThe water used for de-pulping of the cherries is referred to as pulping water. It accounts for just over half of the water used in the process. According to Von Enden and Calvert, \"pulping water consists of quickly fermenting sugars from both pulp and mucilage components. Pulp and mucilage consists to a large extent of proteins, sugars and the mucilage in particular of pectins, i.e. polysaccharide carbohydrates. These sugars are fermenting using the enzymes from the bacteria on the cherries. Other components in pulping water are acids and toxic chemicals like polyphenolics (tannins) or alkaloids (caffeine).\n\nPulping water can be reused during the de-pulping of the harvest of one day. This results in an increase in organic matter and a decrease in pH. Research in Nicaragua showed COD averages rising from 5,400 mg/l up to 8,400 mg/l with most of the pulp removed. The drop in pH can be attributed to the start of fermentation of the pulping water. This drop continues until fermentation is finished and pH levels of around 4 are reached. The nutrient content of the pulping water at the maximum COD load, which was considered to reflect maximum pollution, was determined during this research. Total nitrogen (TN) concentration in the samples ranged from 50 to 110 mg/l with an average over all samples of 90 mg/l. Total phosphorus (TP) concentration in the samples ranged from 8.9 to 15.2 mg/l with an average over all samples of 12.4 mg/l.\n\nWashing of the fermented beans leads to wastewater containing mainly pectins from the mucilage, proteins and sugars. The fermentation of the sugars (disaccharide carbohydrates) into ethanol and CO leads to acid conditions in the washing water. The ethanol is converted in acetic acids after reaction with oxygen, lowering the pH to levels of around 4. The high acidity can negatively affect the treatment efficiency of treatment facilities treating the coffee wastewater like an anaerobic reactor or constructed wetlands and is considered to be detrimental for aquatic life when discharged directly into surface waters.\n\nDuring the washing process the research in Nicaragua showed a clear decrease in contamination of the wastewater. The COD values drop from an average of 7,200 mg/l to less than 50 mg/l. Despite the fact that wastewater with COD values below 200 mg/l is allowed to be discharged in the natural waterways in Nicaragua it is advisable to redirect all the wastewater to the treatment system. This is because COD levels cannot be determined onsite during the washing process and discharge of the wastewater into surface waters is based on visual inspection. When the water is \"clear\" it is considered to be clean enough but the COD values measured during the research showed that discharge generally was to soon, resulting in wastewater with higher levels of COD than permitted. Another positive effect of diverting the wastewater to a treatment system is the dilution of the wastewater which enables better treatment by anaerobic bacteria due to more favourable pH values and better post-treatment due to lower concentrations of ammonium.\n\nTN concentration in the samples of wastewater stemming from washing ranged from 40 to 150 mg/l with an average over all samples of 110 mg/l. TP concentration in the samples ranged from 7.8 to 15.8 mg/l with an average over all samples of 10.7 mg/l.\n\n"}
{"id": "886856", "url": "https://en.wikipedia.org/wiki?curid=886856", "title": "Colloidal gold", "text": "Colloidal gold\n\nColloidal gold is a sol or colloidal suspension of nanoparticles of gold in a fluid, usually water. The colloid is usually either an intense red colour (for spherical particles less than 100 nm) or blue/purple (for larger spherical particles or nanorods).\nDue to their optical, electronic, and molecular-recognition properties, gold nanoparticles are the subject of substantial research, with many potential or promised applications in a wide variety of areas, including electron microscopy, electronics, nanotechnology, materials science, and biomedicine.\n\nThe properties of colloidal gold nanoparticles, and thus their potential applications, depend strongly upon their size and shape. For example, rodlike particles have both transverse and longitudinal absorption peak, and anisotropy of the shape affects their self-assembly.\n\nUsed since ancient times as a method of staining glass colloidal gold was used in the 4th-century Lycurgus Cup, which changes color depending on the location of light source.\n\nDuring the Middle Ages, soluble gold, a solution containing gold salt, had a reputation for its curative property for various diseases. In 1618, Francis Anthony, a philosopher and member of the medical profession, published a book called \"Panacea Aurea, sive tractatus duo de ipsius Auro Potabili\" (Latin: gold potion, or two treatments of potable gold). The book introduces information on the formation of colloidal gold and its medical uses. About half a century later, English botanist Nicholas Culpepper published book in 1656, \"Treatise of Aurum Potabile\", solely discussing the medical uses of colloidal gold.\n\nIn 1676, Johann Kunckel, a German chemist, published a book on the manufacture of stained glass. In his book \"Valuable Observations or Remarks About the Fixed and Volatile Salts-Auro and Argento Potabile, Spiritu Mundi and the Like\", Kunckel assumed that the pink color of Aurum Potabile came from small particles of metallic gold, not visible to human eyes. In 1842, John Herschel invented a photographic process called chrysotype (from the Greek χρῡσός meaning \"gold\") that used colloidal gold to record images on paper.\n\nModern scientific evaluation of colloidal gold did not begin until Michael Faraday's work in the 1850s. In 1856, in a basement laboratory of Royal Institution, Faraday accidentally created a ruby red solution while mounting pieces of gold leaf onto microscope slides. Since he was already interested in the properties of light and matter, Faraday further investigated the optical properties of the colloidal gold. He prepared the first pure sample of colloidal gold, which he called 'activated gold', in 1857. He used phosphorus to reduce a solution of gold chloride. The colloidal gold Faraday made 150 years ago is still optically active. For a long time, the composition of the 'ruby' gold was unclear. Several chemists suspected it to be a gold tin compound, due to its preparation. Faraday recognized that the color was actually due to the miniature size of the gold particles. He noted the light scattering properties of suspended gold microparticles, which is now called Faraday-Tyndall effect.\n\nIn 1898, Richard Adolf Zsigmondy prepared the first colloidal gold in diluted solution. Apart from Zsigmondy, Theodor Svedberg, who invented ultracentrifugation, and Gustav Mie, who provided the theory for scattering and absorption by spherical particles, were also interested in the synthesis and properties of colloidal gold.\n\nWith advances in various analytical technologies in the 20th century, studies on gold nanoparticles has accelerated. Advanced microscopy methods, such as atomic force microscopy and electron microscopy, have contributed the most to nanoparticle research. Due to their comparably easy synthesis and high stability, various gold particles have been studied for their practical uses. Different types of gold nanoparticle are already used in many industries, such as medicine and electronics. For example, several FDA-approved nanoparticles are currently used in drug delivery.\n\nColloidal gold has been used by artists for centuries because of the nanoparticle’s interactions with visible light. Gold nanoparticles absorb and scatter light resulting in colours ranging from vibrant reds to blues to black and finally to clear and colorless, depending on particle size, shape, local refractive index, and aggregation state. These colors occur because of a phenomenon called Localized Surface Plasmon Resonance (LSPR), in which conduction electrons on the surface of the nanoparticle oscillate in resonance with incident light.\n\nAs a general rule, the wavelength of light absorbed increases as a function of increasing nano particle size. For example, pseudo-spherical gold nanoparticles with diameters ~ 30 nm have a peak LSPR absorption at ~530 nm.\n\nChanges in the apparent color of a gold nanoparticle solution can also be caused by the environment in which the colloidal gold is suspended The optical properties of gold nanoparticles depends on the refractive index near the nanoparticle surface, therefore both the molecules directly attached to the nanoparticle surface (i.e. nanoparticle ligands) and/or the nanoparticle solvent both may influence observed optical features. As the refractive index near the gold surface increases, the NP LSPR will shift to longer wavelengths In addition to solvent environment, the extinction peak can be tuned by coating the nanoparticles with non-conducting shells such as silica, bio molecules, or aluminium oxide.\n\nWhen gold nano particles aggregate, the optical properties of the particle change, because the effective particle size, shape, and dielectric environment all change.\n\nColloidal gold and various derivatives have long been among the most widely used labels for antigens in biological electron microscopy. Colloidal gold particles can be attached to many traditional biological probes such as antibodies, lectins, superantigens, glycans, nucleic acids, and receptors. Particles of different sizes are easily distinguishable in electron micrographs, allowing simultaneous multiple-labelling experiments.\n\nIn addition to biological probes, gold nanoparticles can be transferred to various mineral substrates, such as mica, single crystal silicon, and atomically flat gold(III), to be observed under atomic force microscopy (AFM).\n\nGold nanoparticles can be used to optimize the biodistribution of drugs to diseased organs, tissues or cells, in order to improve and target drug delivery.\nNanoparticle-mediated drug delivery is feasible only if the drug distribution is otherwise inadequate. These cases include drug targeting of unstable (proteins, siRNA, DNA), delivery to the difficult sites (brain, retina, tumors, intracellular organelles) and drugs with serious side effects (e.g. anti-cancer agents). The performance of the nanoparticles depends on the size and surface functionalities in the particles. Also, the drug release and particle disintegration can vary depending on the system (e.g. biodegradable polymers sensitive to pH). An optimal nanodrug delivery system ensures that the active drug is available at the site of action for the correct time and duration, and their concentration should be above the minimal effective concentration (MEC) and below the minimal toxic concentration (MTC).\n\nGold nanoparticles are being investigated as carriers for drugs such as Paclitaxel. The administration of hydrophobic drugs require molecular encapsulation and it is found that nanosized particles are particularly efficient in evading the reticuloendothelial system.\n\nIn cancer research, colloidal gold can be used to target tumors and provide detection using SERS (surface enhanced Raman spectroscopy) \"in vivo\". These gold nanoparticles are surrounded with Raman reporters, which provide light emission that is over 200 times brighter than quantum dots. It was found that the Raman reporters were stabilized when the nanoparticles were encapsulated with a thiol-modified polyethylene glycol coat. This allows for compatibility and circulation \"in vivo\". To specifically target tumor cells, the polyethylenegylated gold particles are conjugated with an antibody (or an antibody fragment such as scFv), against, e.g. epidermal growth factor receptor, which is sometimes overexpressed in cells of certain cancer types. Using SERS, these pegylated gold nanoparticles can then detect the location of the tumor.\n\nGold nanoparticles accumulate in tumors, due to the leakiness of tumor vasculature, and can be used as contrast agents for enhanced imaging in a time-resolved optical tomography system using short-pulse lasers for skin cancer detection in mouse model. It is found that intravenously administrated spherical gold nanoparticles broadened the temporal profile of reflected optical signals and enhanced the contrast between surrounding normal tissue and tumors.\nGold nanoparticles have shown potential as intracellular delivery vehicles for siRNA oligonucleotides with maximal therapeutic impact.\n\nGold nanoparticles show potential as intracellular delivery vehicles for antisense oligonucleotides (ssDNA,dsDNA) by providing protection against intracellular nucleases and ease of functionalization for selective targeting.\n\nGold nanorods are being investigated as photothermal agents for in-vivo applications. Gold nanorods are rod-shaped gold nanoparticles whose aspect ratios tune the surface plasmon resonance (SPR) band from the visible to near-infrared wavelength. The total extinction of light at the SPR is made up of both absorption and scattering. For the smaller axial diameter nanorods (~10 nm), absorption dominates, whereas for the larger axial diameter nanorods (>35 nm) scattering can dominate. As a consequence, for in-vivo studies, small diameter gold nanorods are being used as photothermal converters of near-infrared light due to their high absorption cross-sections. Since near-infrared light transmits readily through human skin and tissue, these nanorods can be used as ablation components for cancer, and other targets. When coated with polymers, gold nanorods have been observed to circulate in-vivo with half-lives longer than 6 hours, bodily residence times around 72 hours, and little to no uptake in any internal organs except the liver.\nApart from rod-like gold nanoparticles, also spherical colloidal gold nanoparticles are recently used as markers in combination with photothermal single particle microscopy.\n\nConsiderable interest has been shown in the use of gold and other heavy-atom-containing nanoparticles to enhance the dose delivered to tumors. Since the gold nanoparticles are taken up by the tumors more than the nearby healthy tissue, the dose is selectively enhanced. The biological effectiveness of this type of therapy seems to be due to the local deposition of the radiation dose near the nanoparticles. This mechanism is the same as occurs in heavy ion therapy.\n\nResearchers have developed simple inexpensive methods for on-site detection of hydrogen sulfide present in air based on the antiaggregation of gold nanoparticles (AuNPs). Dissolving into a weak alkaline buffer solution leads to the formation of HS-, which can stabilize AuNPs and ensure they maintain their red color allowing for visual detection of toxic levels of .\n\nGold nanoparticles are incorporated into biosensors to enhance its stability, sensitivity, and selectivity. Nanoparticle properties such as small size, high surface-to-volume ratio, and high surface energy allow immobilization of large range of biomolecules. Gold nanoparticle, in particular, could also act as \"electron wire\" to transport electrons and its amplification effect on electromagnetic light allows it to function as signal amplifiers. Main types of gold nanoparticle based biosensors are optical and electrochemical biosensor.\n\nGold nanoparticles improve the sensitivity of optical sensor by response to the change in local refractive index. The angle of the incidence light for surface plasmon resonance, an interaction between light wave and conducting electrons in metal, changes when other substances are bounded to the metal surface. Because gold is very sensitive to its surroundings' dielectric constant, binding of an analyte would significantly shift gold nanoparticle's SPR and therefore allow more sensitive detection. Gold nanoparticle could also amplify the SPR signal. When the plasmon wave pass through the gold nanoparticle, the charge density in the wave and the electron I the gold interacted and resulted in higher energy response, so called electron coupling. Since the analyte and bio-receptor now bind to the gold, it increases the apparent mass of the analyte and therefore amplified the signal.\nThese properties had been used to build DNA sensor with 1000-fold sensitive than without the Au NP. Humidity senor was also built by altering the atom interspacing between molecules with humidity change, the interspacing change would also result in a change of the Au NP's LSPR.\n\nElectrochemical sensor convert biological information into electrical signals that could be detected. The conductivity and biocompatibility of Au NP allow it to act as \"electron wire\". It transfers electron between the electrode and the active site of the enzyme. It could be accomplished in two ways: attach the Au NP to either the enzyme or the electrode. GNP-glucose oxidase monolayer electrode was constructed use these two methods. The Au NP allowed more freedom in the enzyme's orientation and therefore more sensitive and stable detection. Au NP also acts as immobilization platform for the enzyme. Most biomolecules denatures or lose its activity when interacted with the electrode. The biocompatibility and high surface energy of Au allow it to bind to a large amount of protein without altering its activity and results in a more sensitive sensor. Moreover, Au NP also catalyzes biological reactions. Gold nanoparticle under 2 nm has shown catalytic activity to the oxidation of styrene.\n\nGold nanoparticles capped with organic ligands, such as alkanethiol molecules, can self-assemble into large monolayers (>cmformula_1). The particles are first prepared in organic solvent, such as chloroform or toluene, and are then spread into monolayers either on a liquid surface or on a solid substrate. Such interfacial thin films of nanoparticles have close relationship with Langmuir-Blodgett monolayers made from surfactants.\n\nThe mechanical properties of nanoparticle monolayers have been studied extensively. For 5 nm spheres capped with dodecanethiol, the Young's modulus of the monolayer is on the order of GPa. The mechanics of the membranes are guided by strong interactions between ligand shells on adjacent particles. Upon fracture, the films crack perpendicular to the direction of strain at a fracture stress of 11 formula_2 2.6 MPa, comparable to that of cross-linked polymer films. Free-standing nanoparticle membranes exhibit bending rigidity on the order of 10formula_3 eV, higher than what is predicted in theory for continuum plates of the same thickness, due to nonlocal microstructural constraints such as nonlocal coupling of particle rotational degrees of freedom. On the other hand, resistance to bending is found to be greatly reduced in nanoparticle monolayers that are supported at the air/water interface, possibly due to screening of ligand interactions in a wet environment.\n\nIn many different types of colloidal gold syntheses, the interface of the nanoparticles can display widely different character – ranging from an interface similar to a self-assembled monolayer to a disordered boundary with no repeating patterns. Beyond the Au-Ligand interface, conjugation of the interfacial ligands with various functional moieties (from small organic molecules to polymers to DNA to RNA) afford colloidal gold much of its vast functionality.\n\nAfter initial nanoparticle synthesis, colloidal gold ligands are often exchanged with new ligands designed for specific applications. For example, Au NPs produced via the Turkevich-style (or Citrate Reduction) method are readily reacted via ligand exchange reactions, due to the relatively weak binding between the carboxyl groups and the surfaces of the NPs. This ligand exchange can produce conjugation with a number of biomolecules from DNA to RNA to proteins to polymers (such as PEG) to increase biocompatibility and functionality. For example, ligands have been shown to enhance catalytic activity by mediating interactions between adsorbates and the active gold surfaces for specific oxygenation reactions. Ligand exchange can also be used to promote phase transfer of the colloidal particles. Ligand exchange is also possible with alkane thiol-arrested NPs produced from the Brust-type synthesis method, although higher temperatures are needed to promote the rate of the ligand detachment. An alternative method for further functionalization is achieved through the conjugation of the ligands with other molecules, though this method can cause the colloidal stability of the Au NPs to breakdown.\n\nIn many cases, as in various high-temperature catalytic applications of Au, the removal of the capping ligands produces more desirable physicochemical properties. The removal of ligands from colloidal gold while maintaining a relatively constant number of Au atoms per Au NP can be difficult due to the tendency for these bare clusters to aggregate. The removal of ligands is partially achievable by simply washing away all excess capping ligands, though this method is ineffective in removing all capping ligand. More often ligand removal achieved under high temperature or light ablation followed by washing. Alternatively, the ligands can be electrochemically etched off.\n\nThe precise structure of the ligands on the surface of colloidal gold NPs impact the properties of the colloidal gold particles. Binding conformations and surface packing of the capping ligands at the surface of the colloidal gold NPs tend to differ greatly from bulk surface model adsorption, largely due to the high curvature observed at the nanoparticle surfaces. Thiolate-gold interfaces at the nanoscale have been well-studied and the thiolate ligands are observed to pull Au atoms off of the surface of the particles to for “staple” motifs that have significant Thiyl-Au(0) character. The citrate-gold surface, on the other hand, is relatively less-studied due to the vast number of binding conformations of the citrate to the curved gold surfaces. A study performed in 2014 identified that the most-preferred binding of the citrate involves two carboxylic acids and the hydroxyl group of the citrate binds three surface metal atoms.\n\nAs gold nanoparticles (AuNPs) are further investigated for targeted drug delivery in humans, their toxicity needs to be considered. For the most part, it is suggested that AuNPs are biocompatible, but the concentrations at which they become toxic needs to be determined, and if those concentrations fall within the range of used concentrations. Toxicity can be tested \"in vitro\" and \"in vivo\". \"In vitro\" toxicity results can vary depending on the type of the cellular growth media with different protein compositions, the method used to determine cellular toxicity (cell health, cell stress, how many cells are taken into a cell), and the capping ligands in solution. \"In vivo\" assessments can determine the general health of an organism (abnormal behavior, weight loss, average life span) as well as tissue specific toxicology (kidney, liver, blood) and inflammation and oxidative responses. \"In vitro\" experiments are more popular than \"in vivo\" experiments because \"in vitro\" experiments are more simplistic to perform than \"in vivo\" experiments.\n\nWhile AuNPs themselves appear to have low or negligible toxicity, and the literature shows that the toxicity has much more to do with the ligands rather than the particles themselves, the synthesis of them involves chemicals that are hazardous. Sodium borohydride, a harsh reagent, is used to reduce the gold ions to gold metal. The gold ions usually come from chloroauric acid, a potent acid. Because of the high toxicity and hazard of reagents used to synthesize AuNPs, the need for more “green” methods of synthesis arose.\n\nSome of the capping ligands associated with AuNPs can be toxic while others are nontoxic. In gold nanorods (AuNRs), it has been shown that a strong cytotoxicity was associated with CTAB-stabilized AuNRs at low concentration, but it is thought that free CTAB was the culprit in toxicity . Modifications that overcoat these AuNRs reduces this toxicity in human colon cancer cells (HT-29) by preventing CTAB molecules from desorbing from the AuNRs back into the solution.\nLigand toxicity can also be seen in AuNPs. Compared to the 90% toxicity of HAuCl4 at the same concentration, AuNPs with carboxylate termini were shown to be non-toxic. Large AuNPs conjugated with biotin, cysteine, citrate, and glucose were not toxic in human leukemia cells (K562) for concentrations up to 0.25 M. Also, citrate-capped gold nanospheres (AuNSs) have been proven to be compatible with human blood and did not cause platelet aggregation or an immune response. However, citrate-capped gold nanoparticles sizes 8-37 nm were found to be lethally toxic for mice, causing shorter lifespans, severe sickness, loss of appetite and weight, hair discoloration, and damage to the liver, spleen, and lungs; gold nanoparticles accumulated in the spleen and liver after traveling a section of the immune system.\nThere are mixed-views for polyethylene glycol (PEG)-modified AuNPs. These AuNPs were found to be toxic in mouse liver by injection, causing cell death and minor inflammation. However, AuNPs conjugated with PEG copolymers showed negligible toxicity towards human colon cells (Caco-2).\nAuNP toxicity also depends on the overall charge of the ligands. In certain doses, AuNSs that have positively-charged ligands are toxic in monkey kidney cells (Cos-1), human red blood cells, and E. coli because of the AuNSs interaction with the negatively-charged cell membrane; AuNSs with negatively-charged ligands have been found to be nontoxic in these species.\nIn addition to the previously mentioned \"in vivo\" and \"in vitro\" experiments, other similar experiments have been performed. Alkylthiolate-AuNPs with trimethlyammonium ligand termini mediate the translocation of DNA across mammalian cell membranes \"in vitro\" at a high level, which is detrimental to these cells. Corneal haze in rabbits have been healed \"in vivo\" by using polyethylemnimine-capped gold nanoparticles that were transfected with a gene that promotes wound healing and inhibits corneal fibrosis.\n\nToxicity in certain systems can also be dependent on the size of the nanoparticle. AuNSs size 1.4 nm were found to be toxic in human skin cancer cells (SK-Mel-28), human cervical cancer cells (HeLa), mouse fibroblast cells (L929), and mouse macrophages (J774A.1), while 0.8, 1.2, and 1.8 nm sized AuNSs were less toxic by a six-fold amount and 15 nm AuNSs were nontoxic. There is some evidence for AuNP buildup after injection in \"in vivo\" studies, but this is very size dependent. 1.8 nm AuNPs were found to be almost totally trapped in the lungs of rats. Different sized AuNPs were found to buildup in the blood, brain, stomach, pancreas, kidneys, liver, and spleen.\n\nGenerally, gold nanoparticles are produced in a liquid (\"liquid chemical methods\") by reduction of chloroauric acid (H[AuCl]). After dissolving H[AuCl], the solution is rapidly stirred while a reducing agent is added. This causes Au ions to be reduced to Auions. Then a disproportionation reaction occurs whereby 3 Au ions give rise to Au and 2 Au atoms. The Au atoms act as center of nucleation around which further Au ions gets reduced. To prevent the particles from aggregating, some sort of stabilizing agent that sticks to the nanoparticle surface is usually added. In the Turkevich method of Au NP synthesis, citrate initially acts as the reducing agent and finally as the capping agent which stabilizes the Au NP through electrostatic interactions between the lone pair of electrons on the oxygen and the metal surface.\n\nThey can be functionalized with various organic ligands to create organic-inorganic hybrids with advanced functionality.\n\nThe method pioneered by J. Turkevich et al. in 1951 and refined by G. Frens in the 1970s, is the simplest one available. In general, it is used to produce modestly monodisperse spherical gold nanoparticles suspended in water of around 10–20 nm in diameter. Larger particles can be produced, but this comes at the cost of monodispersity and shape. It involves the reaction of small amounts of hot chloroauric acid with small amounts of sodium citrate solution. The colloidal gold will form because the citrate ions act as both a reducing agent and a capping agent. A capping agent is used in nanoparticle synthesis to stop particle growth and aggregation. A good capping agent has a high affinity for the new nuclei so it will bind to surface atoms which stabilizes the surface energy of the new nuclei and makes so that they cannot bind to other nuclei.\n\nRecently, the evolution of the spherical gold nanoparticles in the Turkevich reaction has been elucidated. Extensive networks of gold nanowires are formed as a transient intermediate. These gold nanowires are responsible for the dark appearance of the reaction solution before it turns ruby-red.\n\nTo produce larger particles, less sodium citrate should be added (possibly down to 0.05%, after which there simply would not be enough to reduce all the gold). The reduction in the amount of sodium citrate will reduce the amount of the citrate ions available for stabilizing the particles, and this will cause the small particles to aggregate into bigger ones (until the total surface area of all particles becomes small enough to be covered by the existing citrate ions).\n\nThis method was discovered by Brust and Schiffrin in the early 1990s, and can be used to produce gold nanoparticles in organic liquids that are normally not miscible with water (like toluene). It involves the reaction of a chlorauric acid solution with tetraoctylammonium bromide (TOAB) solution in toluene and sodium borohydride as an anti-coagulant and a reducing agent, respectively.\n\nHere, the gold nanoparticles will be around 5–6 nm. NaBH is the reducing agent, and TOAB is both the phase transfer catalyst and the stabilizing agent.\n\nTOAB does not bind to the gold nanoparticles particularly strongly, so the solution will aggregate gradually over the course of approximately two weeks. To prevent this, one can add a stronger binding agent, like a thiol (in particular, alkanethiols), which will bind to gold, producing a near-permanent solution. Alkanethiol protected gold nanoparticles can be precipitated and then redissolved. Thiols are better binding agents because there is a strong affinity for the gold-sulfur bonds that form when the two substances react with each other. Tetra-dodecanthiol is a commonly used strong binding agent to synthesize smaller particles.\nSome of the phase transfer agent may remain bound to the purified nanoparticles, this may affect physical properties such as solubility. In order to remove as much of this agent as possible, the nanoparticles must be further purified by soxhlet extraction.\n\nThis approach, discovered by Perrault and Chan in 2009, uses hydroquinone to reduce HAuCl in an aqueous solution that contains 15 nm gold nanoparticle seeds. This seed-based method of synthesis is similar to that used in photographic film development, in which silver grains within the film grow through addition of reduced silver onto their surface. Likewise, gold nanoparticles can act in conjunction with hydroquinone to catalyze reduction of ionic gold onto their surface. The presence of a stabilizer such as citrate results in controlled deposition of gold atoms onto the particles, and growth. Typically, the nanoparticle seeds are produced using the citrate method. The hydroquinone method complements that of Frens, as it extends the range of monodispersed spherical particle sizes that can be produced. Whereas the Frens method is ideal for particles of 12–20 nm, the hydroquinone method can produce particles of at least 30–300 nm.\n\nThis simple method, discovered by Martin and Eah in 2010, generates nearly monodisperse \"naked\" gold nanoparticles in water. Precisely controlling the reduction stoichiometry by adjusting the ratio of NaBH-NaOH ions to HAuCl-HCl ions within the \"sweet zone,\" along with heating, enables reproducible diameter tuning between 3–6 nm. The aqueous particles are colloidally stable due to their high charge from the excess ions in solution. These particles can be coated with various hydrophilic functionalities, or mixed with hydrophobic ligands for applications in non-polar solvents. In non-polar solvents the nanoparticles remain highly charged, and self-assemble on liquid droplets to form 2D monolayer films of monodisperse nanoparticles.\n\n\"Bacillus licheniformis\" can be used in synthesis of gold nanocubes with sizes between 10 and 100 nanometres. Gold nanoparticles are usually synthesized at high temperatures in organic solvents or using toxic reagents. The bacteria produce them in much milder conditions.\n\nFor particles larger than 30 nm, control of particle size with a low polydispersity of spherical gold nanoparticles remains challenging. In order to provide maximum control on the NP structure, Navarro and co-workers used a modified Turkevitch-Frens procedure using sodium acetylacetonate Na(acac) as the reducing agent and sodium citrate as the stabilizer.\n\nAnother method for the experimental generation of gold particles is by sonolysis. The first method of this type was invented by Baigent and Müller. This work pioneered the use of ultrasound to provide the energy for the processes involved and allowed the creation of gold particles with a diameter of under 10 nm. In another method using ultrasound, the reaction of an aqueous solution of HAuCl with glucose, the reducing agents are hydroxyl radicals and sugar pyrolysis radicals (forming at the interfacial region between the collapsing cavities and the bulk water) and the morphology obtained is that of nanoribbons with width 30–50 nm and length of several micrometers. These ribbons are very flexible and can bend with angles larger than 90°. When glucose is replaced by cyclodextrin (a glucose oligomer), only spherical gold particles are obtained, suggesting that glucose is essential in directing the morphology toward a ribbon.\n\nAn economical, environmentally benign and fast synthesis methodology for gold nanoparticles using block copolymer has been developed by Sakai et al. In this synthesis methodology, block copolymer plays the dual role of a reducing agent as well as a stabilizing agent. The formation of gold nanoparticles comprises three main steps: reduction of gold salt ion by block copolymers in the solution and formation of gold clusters, adsorption of block copolymers on gold clusters and further reduction of gold salt ions on the surfaces of these gold clusters for the growth of gold particles in steps, and finally its stabilization by block copolymers. But this method usually has a limited-yield (nanoparticle concentration), which does not increase with the increase in the gold salt concentration. Recently, Ray et al. demonstrated that the presence of an additional reductant (trisodium citrate) in 1:1 mole ratio with gold salt enhances the yield by manyfold at ambient conditions and room temperature.\n\n\n"}
{"id": "8309686", "url": "https://en.wikipedia.org/wiki?curid=8309686", "title": "Coordination number", "text": "Coordination number\n\nIn chemistry, crystallography, and materials science the coordination number, also called ligancy, of a central atom in a molecule or crystal is the number of atoms, molecules or ions bonded to it. The ion/molecule/atom surrounding the central ion/molecule/atom is called a ligand. This number is determined somewhat differently for molecules than for crystals.\n\nFor molecules and polyatomic ions the coordination number of an atom is determined by simply counting the other atoms to which it is bonded (by either single or multiple bonds). For example, [Cr(NH)ClBr] has Cr as its central cation, and has a coordination number of 6.\n\nHowever the solid-state structures of crystals often have less clearly defined bonds, and in these cases a count of neighboring atoms is employed. The simplest method is one used in materials science. The usual value of the coordination number for a given structure refers to an atom in the interior of a crystal lattice with neighbors in all directions. In contexts where crystal surfaces are important, such as materials science and heterogeneous catalysis, the value for an interior atom is the bulk coordination number, while the value for an atom at a surface of the crystal is the surface coordination number.\n\nIn chemistry, coordination number (C.N.), defined originally in 1893 by Alfred Werner, is the total number of neighbors of a central atom in a molecule or ion. Although a carbon atom has four chemical bonds in most stable molecules, the coordination number of each carbon is four in methane (CH), three in ethylene (HC=CH, each C is bonded to 2H + 1C = 3 atoms), and two in acetylene (HC≡CH). In effect we count the first bond (or sigma bond) to each neighboring atom, but not the other bonds (pi bonds).\n\nIn coordination complexes, only the first or sigma bond between each ligand and the central atom counts, but not any pi bonds to the same ligands. In tungsten hexacarbonyl, W(CO), the coordination number of tungsten (W) is counted as six although pi as well as sigma bonding is important in such metal carbonyls.\n\nThe most common coordination number for \"d-\"block transition metal complexes is 6, with an octahedral geometry. The observed range is 2 (e.g., Au in PhPAuCl) to 9 (e.g., Re in [ReH]). Metals in the \"f\"-block (the lanthanoids and actinoids) can accommodate higher coordination number due to their greater ionic radii and availability of more orbitals for bonding. Coordination numbers of 8 to 12 are commonly observed for \"f\"-block elements. For example, with bidentate nitrate ions as ligands, Ce and Th form the 12-coordinate ions [Ce(NO)] (ceric ammonium nitrate) and [Th(NO)]. When the surrounding ligands are much smaller than the central atom, even higher coordination numbers may be possible. One computational chemistry study predicted a particularly stable ion composed of a central lead ion coordinated with no fewer than 15 helium atoms. At the opposite extreme, steric shielding can give rise to unusually low coordination numbers. An extremely rare instance of a metal adopting a coordination number of 1 occurs in the terphenyl-based arylthallium(I) complex 2,6-TippCHTl, where Tipp is the 2,4,6-triisopropylphenyl group.\n\nFor π-electron ligands such as the cyclopentadienide ion [CH], alkenes and the cyclooctatetraenide ion [CH], the number of atoms in the π-electron system that bind to the central atom is termed the hapticity. In ferrocene the hapticity, \"η\", of each cyclopentadienide anion is five, Fe(\"η\"-CH). There are various ways of assigning the contribution made to the coordination number of the central iron atom by each cyclopentadienide ligand. The contribution could be assigned as one since there is one ligand, or as five since there are five neighbouring atoms, or as three since there are three electron pairs involved. Normally the count of electron pairs is taken.\n\nIn order to determine the coordination number of an atom in a crystal, the crystal structure has first to be determined. This is achieved using X-ray, neutron or electron diffraction. Once the positions of the atoms within the unit cell of the crystal are known the coordination number of an atom can be determined. For molecular solids or coordination complexes the units of the polyatomic species can be detected and a count of the bonds can be performed. Solids with lattice structures which includes metals and many inorganic solids can have regular structures where coordinating atoms are all at the same distance and they form the vertices of a coordination polyhedron. However, there are also many such solids where the structures are irregular. In materials science, the bulk coordination number of a given atom in the interior of a crystal lattice is the number of nearest neighbours to a given atom. For an atom at a surface of a crystal, the surface coordination number is always less than the bulk coordination number. The surface coordination number is dependent on the Miller indices of the surface. In a body-centered cubic (BCC) crystal, the bulk coordination number is 8, whereas, for the (100) surface, the surface coordination number is 4.\n\nα-Aluminium has a regular cubic close packed structure, fcc, where each aluminium atom has 12 nearest neighbors, 6 in the same plane and 3 above and below and the coordination polyhedron is a cuboctahedron. α-Iron has a body centered cubic structure where each iron atom has 8 nearest neighbors situated at the corners of a cube. \nThe two most common allotropes of carbon have different coordination numbers. In diamond, each carbon atom is at the centre of a regular tetrahedron formed by four other carbon atoms, the coordination number is four, as for methane. Graphite is made of two-dimensional layers in which each carbon is covalently bonded to three other carbons; atoms in other layers are further away and are not nearest neighbours, giving a coordination number of 3.\n\nFor chemical compounds with regular lattices such as sodium chloride and caesium chloride, a count of the nearest neighbors gives a good picture of the environment of the ions. In sodium chloride each sodium ion has 6 chloride ions as nearest neighbours (at 276 pm) at the corners of an octahedron and each chloride ion has 6 sodium atoms (also at 276 pm) at the corners of an octahedron. In caesium chloride each caesium has 8 chloride ions (at 356 pm) situated at the corners of a cube and each chloride has eight caesium ions (also at 356 pm) at the corners of a cube.\n\nHowever, not all crystal structures are regular and the neighboring atoms may not all be at the same distance. In these cases a different definition of coordination number is used that includes atoms at a greater distance than the nearest neighbours. The very broad definition adopted by the International Union of Crystallography, IUCR, states that the coordination number of an atom in a crystalline solid depends on the chemical bonding model and the way in which the coordination number is calculated.\n\nSome metals have irregular structures. For example, zinc has a distorted hexagonal close packed structure. Regular hexagonal close packing of spheres would predict that each atom has 12 nearest neighbours and a triangular orthobicupola (also called an anticuboctahedron or twinned cuboctahedron) coordination polyhedron. In zinc there are only 6 nearest neighbours at 266 pm in the same close packed plane with six other, next-nearest neighbours, equidistant, three in each of the close packed planes above and below at 291 pm. It is considered to be reasonable to describe the coordination number as 12 rather than 6. Similar considerations can be applied to the regular body centred cube structure where in addition to the 8 nearest neighbors there 6 more, approximately 15% more distant, and in this case the coordination number is often considered to be 14.\nMany chemical compounds have distorted structures. Nickel arsenide, NiAs has a structure where nickel and arsenic atoms are 6-coordinate. Unlike sodium chloride where the chloride ions are cubic close packed, the arsenic anions are hexagonal close packed. The nickel ions are 6-coordinate with a distorted octahedral coordination polyhedron where columns of octahedra share opposite faces. The arsenic ions are not octahedrally coordinated but have a trigonal prismatic coordination polyhedron. A consequence of this arrangement is that the nickel atoms are rather close to each other. Other compounds that share this structure, or a closely related one are some transition metal sulfides such as FeS and CoS, as well as some intermetallics. In cobalt(II) telluride, CoTe, the six tellurium and two cobalt atoms are all equidistant from the central Co atom.\nTwo other examples of commonly-encountered chemicals are FeO and TiO. FeO has a crystal structure that can be described as having a near close packed array of oxygen atoms with iron atoms filling two thirds of the octahedral holes. However each iron atom has 3 nearest neighbors and 3 others a little further away. The structure is quite complex, the oxygen atoms are coordinated to four iron atoms and the iron atoms in turn share vertices, edges and faces of the distorted octahedra. TiO has the rutile structure. The titanium atoms 6-coordinate, 2 atoms at 198.3 pm and 4 at 194.6 pm, in a slightly distorted octahedron. The octahedra around the titanium atoms share edges and vertices to form a 3-D network. The oxide ions are 3-coordinate in a trigonal planar configuration.\n\nThe coordination number of systems with disorder cannot be precisely defined.\n\nThe first coordination number can be defined using the radial distribution function \"g\"(\"r\"):\nwhere \"r\" is the rightmost position starting from \"r\" = 0 whereon \"g\"(\"r\") is approximately zero, \"r\" is the first minimum. Therefore, it is the area under the first peak of \"g\"(\"r\").\n\nThe second coordination number is defined similarly:\n\nAlternative definitions for the coordination number can be found in literature, but in essence the main idea is the same. One of those definition are as follows: Denoting the position of the first peak as \"r\",\n\nThe first coordination shell is the spherical shell with radius between \"r\" and \"r\" around the central particle under investigation.\n\n"}
{"id": "859284", "url": "https://en.wikipedia.org/wiki?curid=859284", "title": "Cylinder (engine)", "text": "Cylinder (engine)\n\nA cylinder is the central working part of a reciprocating engine or pump, the space in which a piston travels. Multiple cylinders are commonly arranged side by side in a bank, or engine block, which is typically cast from aluminum or cast iron before receiving precision machine work. Cylinders may be sleeved (\"lined\" with a harder metal) or sleeveless (with a wear-resistant coating such as Nikasil). A sleeveless engine may also be referred to as a \"parent-bore engine\".\n\nA cylinder's displacement, or swept volume, can be calculated by multiplying its cross-sectional area (the square of half the bore by pi) by the distance the piston travels within the cylinder (the stroke). The engine displacement can be calculated by multiplying the swept volume of one cylinder by the number of cylinders.\n\nPresented symbolically,\n\nA piston is seated inside each cylinder by several metal piston rings fitted around its outside surface in machined grooves; typically two for compressional sealing and one to seal the oil. The rings make near contact with the cylinder walls (sleeved or sleeveless), riding on a thin layer of lubricating oil; essential to keep the engine from seizing and necessitating a cylinder wall's durable surface. \n\nDuring the earliest stage of an engine's life, its initial \"breaking-in\" or \"running-in\" period, small irregularities in the metals are encouraged to gradually form congruent grooves by avoiding extreme operating conditions. Later in its life, after mechanical wear has increased the spacing between the piston and the cylinder (with a consequent decrease in power output) the cylinders may be machined to a slightly larger diameter to receive new sleeves (where applicable) and piston rings, a process sometimes known as \"reboring\".\n\nHeat engines, including Stirling engines, are sealed machines using pistons within cylinders to transfer energy from a heat source to a colder reservoir, often using steam or another gas as the working substance. (See Carnot cycle.) The first illustration depicts a longitudinal section of a cylinder in a steam engine. The sliding part at the bottom is the piston, and the upper sliding part is a distribution valve (in this case of the D slide valve type) that directs steam alternately into either end of the cylinder. Refrigerator and air conditioner compressors are heat engines driven in reverse cycle as pumps.\n\nInternal combustion engines operate on the inherent volume change accompanying oxidation of gasoline (petrol), diesel fuel (or some other hydrocarbon) or ethanol, an expansion which is greatly enhanced by the heat produced. They are not classical heat engines since they expel the working substance, which is also the combustion product, into the surroundings.\n\nThe reciprocating motion of the pistons is translated into crankshaft rotation via connecting rods. As a piston moves back and forth, a connecting rod changes its angle; its distal end has a rotating link to the crankshaft. A typical four-cylinder automobile engine has a single row of water-cooled cylinders. V engines (V6 or V8) use two angled cylinder banks. The \"V\" configuration is utilized to create a more compact configuration relative to the number of cylinders. Many other engine configurations exist.\n\nFor example, there are also rotary turbines. The Wankel engine is a rotary adaptation of the cylinder-piston concept which has been used by Mazda and NSU in automobiles. Rotary engines are relatively quiet because they lack the clatter of reciprocating motion.\n\nAir-cooled engines generally use individual cases for the cylinders to facilitate cooling. Inline motorcycle engines are an exception, having two-, three-, four-, or even six-cylinder air-cooled units in a common block. Water-cooled engines with only a few cylinders may also use individual cylinder cases, though this makes the cooling system more complex. The Ducati motorcycle company, which for years used air-cooled motors with individual cylinder cases, retained the basic design of their V-twin engine while adapting it to water-cooling.\n\nIn some engines, especially French designs, the cylinders have \"wet liners\". They are formed separately from the main casting so that liquid coolant is free to flow around their outsides. Wet-lined cylinders have better cooling and a more even temperature distribution, but this design makes the engine as a whole somewhat less rigid.\n\nDuring use, the cylinder is subject to wear from the rubbing action of the piston rings and piston skirt. This is minimized by the thin oil film which coats the cylinder walls and also by a layer of glaze which naturally forms as the engine is run-in, but eventually the cylinder becomes worn and slightly oval in shape, usually necessitating a rebore to an oversize diameter and the fitting of new, oversize pistons. The cylinder does not wear above the highest point reached by the top compression ring of the piston, which can result in a detectable ridge. If an engine is only operated at low rpm for its early life (e.g. in a gently driven automobile) then abruptly used in the higher rpm range (e.g. by a new owner), the slight stretching of the connecting rods at high speed can enable the top compression ring to contact the wear ridge, breaking the ring. For this reason it is important that all engines, once initially run-in, are occasionally \"exercised\" through their full speed range to develop a tapered wear profile rather than a sharp ridge.\n\nCylinder walls can become very worn or damaged from use. If the engine is not equipped with replaceable sleeves, there is a limit to how far the cylinder walls can be bored or worn before the block must be sleeved or replaced. In such cases, the use of a sleeve or liner can restore proper clearances to an engine. Sleeves are made out of iron alloys and are very reliable. A sleeve is installed by a machinist at a machine shop. The engine block is mounted on a precision boring machine, where the cylinder is then bored to a size much larger than normal and a new cast-iron sleeve can be inserted with an interference fit. The sleeves can be pressed into place, or they can be held in by a shrink fit. This is done by boring the cylinder (between 3 and 6 thousandths of an inch) smaller than the sleeve being installed, then heating the engine block and while hot, the cold sleeve can be inserted easily. When the engine block cools down it shrink fits around the sleeve holding it into place. Cylinder wall thickness is important to efficient thermal conductivity in the engine. When choosing sleeves, engines have specifications to how thick the cylinder walls should be to prevent overworking the coolant system. Each engine's needs are different, dependent on designed work load duty cycle and energy produced. After selecting and installing the sleeve, the cylinder needs to be finish bored and honed to match the piston. Care needs to be given to the finish of the cylinder walls to prevent improper ring seating at break in.\n\nFailed lubrication can cause the pistons or piston rings to seize to the cylinder walls. Seizing can occur during engine use, via overheating and lack of oil, or during storage via condensation and corrosion.\n\n\n"}
{"id": "1769573", "url": "https://en.wikipedia.org/wiki?curid=1769573", "title": "Dark galaxy", "text": "Dark galaxy\n\nA dark galaxy is a hypothesized galaxy with no, or very few, stars. They received their name because they have no visible stars, but may be detectable if they contain significant amounts of gas. Astronomers have long theorized the existence of dark galaxies, but there are no confirmed examples to date. Dark galaxies are distinct from intergalactic gas clouds caused by galactic tidal interactions, since these gas clouds do not contain dark matter, so they do not technically qualify as galaxies. Distinguishing between intergalactic gas clouds and galaxies is difficult; most candidate dark galaxies turn out to be tidal gas clouds. The best candidate dark galaxies to date include HI1225+01, AGC229385, and numerous gas clouds detected in studies of quasars.\n\nOn 25 August 2016, astronomers reported that Dragonfly 44, an ultra diffuse galaxy (UDG) with the mass of the Milky Way galaxy, but with nearly no discernable stars or galactic structure, may be made almost entirely of dark matter.\n\nLarge surveys with sensitive, but low resolution radio telescopes like Arecibo or the Parkes Telescope look for 21 cm emission from atomic hydrogen in galaxies. These surveys are then matched to optical surveys to identify any objects with no optical counterpart, i.e. sources with no stars. \n\nAnother way astronomers search for dark galaxies is to look for hydrogen absorption lines in the spectra of background quasars. This technique has revealed many intergalactic clouds of hydrogen, but following up candidate dark galaxies is difficult, since these sources tend to be too far away, and are often optically drowned out by the bright light from the quasar.\n\nIn 2005, astronomers found gas cloud VIRGOHI21 and attempted to determine what it was and why it caused such a gravitational pull on galaxy NGC 4254. After years of running out of other explanations, some have concluded that VIRGOHI21 is a dark galaxy, due to the massive effect it had on NGC 4254.\n\nThe actual size of dark galaxies is unknown because they cannot be observed with normal telescopes. There have been various estimations, ranging from double the size of the Milky Way to the size of a small quasar.\n\nDark galaxies are composed of dark matter. Furthermore, dark galaxies are theoretically composed of hydrogen and dust. Some scientists support the idea that dark galaxies may contain stars. Yet the exact composition of dark galaxies is unknown because there is no conclusive way to spot them so far. However, astronomers estimate that the mass of the gas in these galaxies is approximately 1 billion times that of the Sun.\n\nDark galaxies contain no visible stars, and are not visible using optical telescopes. The Arecibo Galaxy Environment Survey (AGES) is a current study using the Arecibo radio telescope to search for dark galaxies, which are predicted to contain detectable amounts of neutral hydrogen. The Arecibo radio telescope is useful where others are not because of its ability to detect the emission from this neutral hydrogen, specifically the 21 cm line.\n\nScientists do not have much explanation for some astronomic events, so some use the idea of a dark galaxy to explain these events. Little is known about dark galaxies, and some scientists believe a dark galaxy is actually a newly forming galaxy. One such candidate is in the Virgo cluster. This candidate contains very few stars. Scientists classify this galaxy as a newly forming galaxy, rather than a dark galaxy.\nScientists say that the galaxies we see today only began to create stars after dark galaxies. Based on numerous scientific assertions, dark galaxies played a big role in many of the galaxies astronomers and scientists see today. Martin Haehnel, from Kavli Institute for Cosmology at the University of Cambridge, claims that the precursor to the Milky Way galaxy was actually a much smaller bright galaxy that had merged with dark galaxies nearby to form the Milky Way we currently see. Multiple scientists agree that dark galaxies are building blocks of modern galaxies. Sebastian Cantalupo of the University of California, Santa Cruz, agrees with this theory. He goes on to say, \"In our current theory of galaxy formation, we believe that big galaxies form from the merger of smaller galaxies. Dark galaxies bring to big galaxies a lot of gas, which then accelerates star formation in the bigger galaxies.\" Scientists have specific techniques they use to locate these dark galaxies. These techniques have the capability of teaching us more about other special events that occur in the universe; for instance, the “cosmic web”. This “web” is made of invisible filaments of gas and dark matter believed to permeate the universe, as well as “feeding and building galaxies and galaxy clusters where the filaments intersect.”\n\nHE0450-2958 is a quasar at redshift z=0.285. Hubble Space Telescope images showed that the quasar is located at the edge of a large cloud of gas, but no host galaxy was detected for the quasar. The authors of the Hubble study suggested that one possible scenario was that the quasar is located in a dark galaxy. However, subsequent analysis by other groups found no evidence that the host galaxy is anomalously dark, and demonstrated that a normal host galaxy is probably present, so the observations do not support the dark galaxy interpretation.\n\nHVC 127-41-330 is a cloud rotating at high speed between Andromeda and the Triangulum Galaxy. Astronomer Josh Simon considers this cloud to be a dark galaxy because of the speed of its rotation and its predicted mass.\n\nSmith's Cloud is a candidate to be a dark galaxy, due to its projected mass and survival of encounters with the Milky Way.\n\nInitially discovered in 2000, VIRGOHI21 was announced in February 2005 as a good candidate to be a true dark galaxy. It was detected in 21-cm surveys, and was suspected to be a possible cosmic partner to the galaxy NGC 4254. This unusual-looking galaxy appears to be one partner in a cosmic collision, and appeared to show dynamics consistent with a dark galaxy (and apparently inconsistent with the predictions of the Modified Newtonian Dynamics (MOND) theory). However, further observations revealed that VIRGOHI21 was an intergalactic gas cloud, stripped from NGC4254 by a high speed collision. The high speed interaction was caused by infall into the Virgo cluster.\n\n"}
{"id": "2349173", "url": "https://en.wikipedia.org/wiki?curid=2349173", "title": "Decanoic acid", "text": "Decanoic acid\n\nDecanoic acid, also known as capric acid (C10:0) or decylic acid, is a saturated fatty acid. Its formula is CH(CH)COOH. Salts and esters of decanoic acid are called decanoates or caprates. The term capric acid is derived from the Latin \"caper / capra\" (goat) because the sweaty, unpleasant smell of the compound is reminiscent of goats.\n\nCapric acid occurs naturally in coconut oil (about 10%) and palm kernel oil (about 4%), otherwise it is uncommon in typical seed oils. It is found in the milk of various mammals and to a lesser extent in other animal fats. It also comprises 1.62% of the fats from the fruit of the durian species \"Durio graveolens\".\n\nTwo other acids are named after goats: caproic (a C6:0 fatty acid) and caprylic (a C8:0 fatty acid). Along with decanoic acid, these total 15% in goat milk fat.\n\nDecanoic acid can be prepared from oxidation of primary alcohol decanol by using chromium trioxide (CrO) oxidant under acidic conditions.\n\nNeutralization of decanoic acid or saponification of its esters, typically triglycerides, with sodium hydroxide will give sodium decanoate. This salt (CH(CH)COONa) is a component of some types of soap.\n\nDecanoic acid is used in the manufacture of esters for artificial fruit flavors and perfumes. It is also used as an intermediate in chemical syntheses. It is used in organic synthesis and industrially in the manufacture of perfumes, lubricants, greases, rubber, dyes, plastics, food additives and pharmaceuticals.\n\nDecanoate ester prodrugs of various pharmaceuticals are available. Since decanoic acid is a fatty acid, forming a salt or ester with a drug will increase its lipophilicity and its affinity for adipose tissue. Since distribution of a drug from fatty tissue is usually slow, one may develop a long-acting injectable form of a drug (called a Depot injection) by using its decanoate form. Some examples of drugs available as a decanoate ester include nandrolone, fluphenazine, bromperidol, and haloperidol.\n\nDecanoic acid acts as a non-competitive AMPA receptor antagonist at therapeutically relevant concentrations, in a voltage- and subunit-dependent manner, and this is sufficient to explain its antiseizure effects. This direct inhibition of excitatory neurotransmission by decanoic acid in the brain contributes to the anticonvulsant effect of the MCT ketogenic diet. Decanoic acid and the AMPA receptor antagonist drug perampanel act at separate sites on the AMPA receptor, and so it is possible that they have a cooperative effect at the AMPA receptor, suggesting that perampanel and the ketogenic diet could be synergistic.\n\nDecanoic acid may be responsible for the mitochondrial proliferation associated with the ketogenic diet, and that this may occur via PPARγ receptor agonism and its target genes involved in mitochondrial biogenesis.\nComplex I activity of the electron transport chain is substantially elevated by decanoic acid treatment.\n\nIt should however be noted that orally ingested medium chain fatty acids would be very rapidly degraded by first-pass metabolism by being taken up in the liver via the portal vein, and are quickly metabolized via coenzyme A intermediates through β-oxidation and the citric acid cycle to produce carbon dioxide, acetate and ketone bodies. Whether the ketones β-hydroxybutryate and acetone have direct antiseizure activity is unclear\n\n"}
{"id": "3442941", "url": "https://en.wikipedia.org/wiki?curid=3442941", "title": "December 2005 North American ice storm", "text": "December 2005 North American ice storm\n\nThe December 2005 North American ice storm was a damaging winter storm that produced extensive ice damage in a large portion of the Southern United States from December 14–16, 2005, while extensive snowfall was reported across portions of the Canadian provinces of Ontario and Quebec. The ice storm led to enormous and widespread power outages, and at least 7 deaths.\n\nThe storm was triggered by a deep low pressure system formed over the Gulf of Mexico on 14 December 2005, which began moving northward. At the same time, cold arctic air from northern Canada penetrated deep into the central United States and lowered the temperatures at the surface while warm air from the Gulf Stream remained at the coast. A second Alberta clipper farther north also added additional energy to the system.\n\nThe precipitation remained as rain in the coastal areas, including the large cities from Boston to Washington, D.C.. However, freezing rain was extensive in the inland areas, including around Atlanta, where the temperatures remained just below freezing for extended periods. The freezing rain persisted for many hours, leading to extensive ice damage.\n\nTrees and power lines, along with numerous other lightweight structures, came down in many areas from Georgia northward, and highways (including several Interstate Highways) were closed and impassable. The heaviest ice accretions were in southwestern North Carolina, where ice over 3/4 inch (20 mm) thick was reported and Charlottesville, Virginia with 1 inch (25,4 mm). At the higher elevations, and farther north across the Great Lakes region and into northern New England, the storm produced heavy snow with amounts varying between 7 inches to as high as 26 inches (57 cm).\n\nIn Canada, 41 centimetres of snow fell in Montreal in about 12 hours, with snowfall rates as high as 30 centimetres in 4 hours, and 11 centimetres in a one-hour period during the morning rush hour on December 16. This is the second worst storm on record, after a storm on March 4, 1971 dumped 47 centimetres and the worst fall snowstorm to hit the area since records were kept. In Ottawa, between 20 and 35 centimetres fell in a short period of time causing several OC Transpo buses to become stuck on the city's transitway and several of their articulated buses to become jackknifed at a busy intersection in the suburb of Gatineau, Quebec.\n\nIn addition, at least seven deaths were blamed on the weather, one of them directly related to weather conditions. One of the deaths was as a result of a tree that fell into a home and crashed into a man in Kannapolis, North Carolina, one as a result of a faulty generator in a house without power, and the other five as a result of traffic accidents.\n\nThe ice storm left more than a million people without power in and near the Appalachians, including 630,000 customers in Georgia, 358,000 in South Carolina, 328,000 in North Carolina and 13,000 in Virginia. It took over a week to restore power. Several emergency shelters also were opened. Electricity was not restored in many places until 20 December 2005, by which time one death was blamed on the outage.\n\n\n"}
{"id": "8177392", "url": "https://en.wikipedia.org/wiki?curid=8177392", "title": "Edward A. Guggenheim", "text": "Edward A. Guggenheim\n\nEdward Armand Guggenheim FRS (11 August 1901 in Manchester – 9 August 1970) was an English physical chemist, noted for his contributions to thermodynamics.\n\nGuggenheim was born in Manchester 11 August 1901, the son of Armand Guggenheim and Marguerite Bertha Simon. His father was Swiss, a naturalised British citizen. Guggenheim married Simone Ganzin (died 1954), in 1934 and Ruth Helen Aitkin, born Clarke, widow, in 1955. They had no children. He died in Reading, Berkshire 9 August 1970.\n\nGuggenheim was educated at Terra Nova School, Southport, Charterhouse School and Gonville and Caius College, Cambridge where he obtained firsts in both the mathematics part 1 and chemistry part 2 triposes. Unable to gain a fellowship at the college, he went to Denmark where he studied under J. N. Brønsted at the University of Copenhagen.\n\nReturning to England, he found a place at University College, London where he wrote his first book, \"Modern Thermodynamics by the Methods of Willard Gibbs\" (1933), which \"established his reputation and revolutionized the teaching of the subject\". He was also a visiting professor of chemistry at Stanford University, and later became a reader in the chemical engineering department at Imperial College London. During World War II he worked on defence matters for the navy. In 1946 he was appointed professor of chemistry and head of department at Reading University, where he stayed until his retirement in 1966.\n\nGuggenheim produced eleven books and more than 100 papers. His first book,\"Modern Thermodynamics by the Methods of Willard Gibbs\" (1933), was a 206-page, detailed study, with text, figures, index, and preface by F. G. Donnan, showing how the analytical thermodynamic methods developed by Willard Gibbs leads in a straightforward manner to relations such as phases, constants, solution, systems, and laws, that are unambiguous and exact. This book, together with Gilbert N. Lewis and Merle Randall's 1923 textbook \"Thermodynamics and the Free Energy of Chemical Substances\", are said to be responsible for the inception of the modern science of chemical thermodynamics.\n\nOther books included \"Statistical Thermodynamics\" with Ralph Fowler (1939), and \"Thermodynamics – an Advanced Treatment for Chemists and Physicists\" . In the preface to this book, he states that no thermodynamics book written before 1929 even attempts an account of any of the following matters:\n\n\nGuggenheim was elected a Fellow of the Royal Society in 1946. His nomination reads\nIn 1972, the E. A. Guggenheim Memorial Fund was established by friends and colleagues. The income from the fund is used to (a) award an annual prize and (b) to provide a biennial or triennial memorial lecture on some topic of chemistry or physics appropriate to the interests of Guggenheim.\n\nThe Guggenheim Medal was introduced in 2014 by the Institution of Chemical Engineers for significant contributions to research in thermodynamics and / or complex fluids. The first recipient (in 2015) was Professor George Jackson of Imperial College London.\n"}
{"id": "37997021", "url": "https://en.wikipedia.org/wiki?curid=37997021", "title": "Egg lecithin", "text": "Egg lecithin\n\nEgg lecithin is a type of lecithin, a group of compounds primarily containing phospholipids, that is derived from eggs.\n\nThe key companies operating in the global egg lecithin market include Lipoid GmbH, Cargill Incorporated, Kewpie Corporation and VAV Life Sciences Pvt. Ltd.\n\nEgg lecithin was first isolated in 1846 by the French chemist and pharmacist Theodore Gobley. Gobley originally isolated lecithin from egg yolk—λέκιθος (\"lekithos\") is 'egg yolk' in ancient Greek—and established the complete chemical formula of phosphatidylcholine in 1874.\n\nPhosphatidylcholine a major component of egg lecithin, occurs in all cellular organisms, being one of the important components of the phospholipid portion of the cell membrane. Other components include phosphatidylethanolamine and sphingomyelin.\n\nEgg lecithin is usually extracted chemically using ethanol, acetone, petroleum ether but not benzene or hexane due to restrictions on residual solvents by the pharmaceutical regulations. It is an emulsifier, especially for parenteral use since it does not need to be metabolized. In aqueous solution, its phospholipids can form either liposomes, bilayer sheets, micelles, or lamellar structures, depending on hydration and temperature. This results in a type of surfactant that is usually classified as amphipathic.\n\nCommercial egg lecithin, specified in the United States National Formulatory (USP/NF) as used by pharmaceutical companies, is a highly purified mixture of phospholipids, devoid of triglycerides, cholesterol, or proteins.\n\nEgg lecithin has emulsification and lubricant properties, and is a surfactant. It can be totally integrated into the cell membrane in humans, so does not need to be metabolized and is well tolerated by humans and nontoxic when ingested; some synthetic emulsifiers can only be excreted via the kidneys.\n\nApplications include:\nEgg lecithin is approved by the United States Food and Drug Administration with the status \"generally recognized as safe\" and listed in the compendium.\n\nEgg-derived lecithin is not usually a concern for those allergic to eggs since commercially available food grade egg lecithin is devoid of allergy causing egg proteins. Egg lecithin is not a concern for those on low-cholesterol diets, because the lecithin found in eggs markedly inhibits the absorption of the cholesterol contained in eggs.\n\n"}
{"id": "986096", "url": "https://en.wikipedia.org/wiki?curid=986096", "title": "Fermi surface", "text": "Fermi surface\n\nIn condensed matter physics, the Fermi surface is the surface in reciprocal space which separates occupied from unoccupied electron states at zero temperature. The shape of the Fermi surface is derived from the periodicity and symmetry of the crystalline lattice and from the occupation of electronic energy bands. The existence of a Fermi surface is a direct consequence of the Pauli exclusion principle, which allows a maximum of one electron per quantum state.\n\nConsider a spin-less ideal Fermi gas of formula_1 particles. According to Fermi–Dirac statistics, the mean occupation number of a state with energy formula_2 is given by\n\nformula_3\n\nwhere,\n\n\nSuppose we consider the limit formula_12. Then we have,\n\nformula_13\n\nBy the Pauli exclusion principle, no two fermions can be in the same state. Therefore, in the state of lowest energy, the particles fill up all energy levels below the Fermi energy formula_9, which is equivalent to saying that formula_9 is the energy level below which there are exactly formula_1 states\".\"\n\nIn momentum space, these particles fill up a sphere of radius formula_17, the surface of which is called the Fermi surface.\n\nThe linear response of a metal to an electric, magnetic or thermal gradient is determined by the shape of the Fermi surface, because currents are due to changes in the occupancy of states near the Fermi energy. In reciprocal space, the Fermi surface of an ideal Fermi gas is a sphere of radius\n\nformula_18,\n\ndetermined by the valence electron concentration where formula_19 is the reduced Planck's constant. A material whose Fermi level falls in a gap between bands is an insulator or semiconductor depending on the size of the bandgap. When a material's Fermi level falls in a bandgap, there is no Fermi surface.\n\nMaterials with complex crystal structures can have quite intricate Fermi surfaces. The figure 2 illustrates the anisotropic Fermi surface of graphite, which has both electron and hole pockets in its Fermi surface due to multiple bands crossing the Fermi energy along the formula_20 direction. Often in a metal the Fermi surface radius formula_21 is larger than the size of the first Brillouin zone which results in a portion of the Fermi surface lying in the second (or higher) zones. As with the band structure itself, the Fermi surface can be displayed in an extended-zone scheme where formula_22 is allowed to have arbitrarily large values or a reduced-zone scheme where wavevectors are shown modulo formula_23 (in the 1-dimensional case) where a is the lattice constant. In the three-dimensional case the reduced zone scheme means that from any wavevector formula_22 there is an appropriate number of reciprocal lattice vectors formula_25 subtracted that the new formula_22 now is closer to the origin in formula_22-space than to any formula_25. Solids with a large density of states at the Fermi level become unstable at low temperatures and tend to form ground states where the condensation energy comes from opening a gap at the Fermi surface. Examples of such ground states are superconductors, ferromagnets, Jahn–Teller distortions and spin density waves.\n\nThe state occupancy of fermions like electrons is governed by Fermi–Dirac statistics so at finite temperatures the Fermi surface is accordingly broadened. In principle all fermion energy level populations are bound by a Fermi surface although the term is not generally used outside of condensed-matter physics.\n\nElectronic Fermi surfaces have been measured through observation of the oscillation of transport properties in magnetic fields formula_29, for example the de Haas–van Alphen effect (dHvA) and the Shubnikov–de Haas effect (SdH). The former is an oscillation in magnetic susceptibility and the latter in resistivity. The oscillations are periodic versus formula_30 and occur because of the quantization of energy levels in the plane perpendicular to a magnetic field, a phenomenon first predicted by Lev Landau. The new states are called Landau levels and are separated by an energy formula_31 where formula_32 is called the cyclotron frequency, formula_33 is the electronic charge, formula_34 is the electron effective mass and formula_35 is the speed of light. In a famous result, Lars Onsager proved that the period of oscillation formula_36 is related to the cross-section of the Fermi surface (typically given in Å) perpendicular to the magnetic field direction formula_37 by the equationformula_38. Thus the determination of the periods of oscillation for various applied field directions allows mapping of the Fermi surface.Observation of the dHvA and SdH oscillations requires magnetic fields large enough that the circumference of the cyclotron orbit is smaller than a mean free path. Therefore, dHvA and SdH experiments are usually performed at high-field facilities like the High Field Magnet Laboratory in Netherlands, Grenoble High Magnetic Field Laboratory in France, the Tsukuba Magnet Laboratory in Japan or the National High Magnetic Field Laboratory in the United States.\n\nThe most direct experimental technique to resolve the electronic structure of crystals in the momentum-energy space (see reciprocal lattice), and, consequently, the Fermi surface, is the angle resolved photoemission spectroscopy (ARPES). An example of the Fermi surface of superconducting cuprates measured by ARPES is shown in the figure 3.\n\nWith positron annihilation it is also possible to determine the Fermi surface as the annihilation process conserves the momentum of the initial particle. Since a positron in a solid will thermalize prior to annihilation, the annihilation radiation carries the information about the electron momentum. The corresponding experimental technique is called angular correlation of electron positron annihilation radiation (ACAR) as it measures the angular deviation from of both annihilation quanta. In this way it is possible to probe the electron momentum density of a solid and determine the Fermi surface. Furthermore, using spin polarized positrons, the momentum distribution for the two spin states in magnetized materials can be obtained. ACAR has many advantages and disadvantages compared to other experimental techniques: It does not rely on UHV conditions, cryogenic temperatures, high magnetic fields or fully ordered alloys. However, ACAR needs samples with a low vacancy concentration as they act as effective traps for positrons. In this way, the first determination of a \"smeared Fermi surface\" in a 30% alloy was obtained in 1978.\n\n\n"}
{"id": "14084981", "url": "https://en.wikipedia.org/wiki?curid=14084981", "title": "Gianni Silvestrini", "text": "Gianni Silvestrini\n\nGianni Silvestrini is an Italian researcher. From 1977 to 2014 he taught at the University of Palermo and at the National Research Council in the fields of solar technology and energy policy.\n\nFrom 1977 to 1982 worked for the University of Palermo in the field of simulation models for solar technologies and energy efficiency in buildings and was visiting researcher at the Lawrence Berkeley Laboratory.\nSince 1983 he continued his activity at the National Research Council Institute, focused on Energy and Buildings (Ieren).\nDuring this period he was involved in the Task 11 of the International Energy Agency “Passive and hybrid solar commercial buildings”.\nSince 1988 he enlarged his activities in the area of environmental and energy strategies, with studies at urban, regional and national level. \nA particular attention was given on the issue of global warming, with the preparation of specific studies for the Ministry of Environment. \nFrom 2004 to 2006 he was professor at the Politecnico di Milano and has launched the post graduate Master “Ridef – Energy for Kyoto” on energy efficiency and renewable energies that has continued to follow as responsible until now (www.ridef.it).\nHe is author of more than 100 scientific papers and co-authored 3 books (“Solar Architecture”, 1984, textbook at Polytechnic of Milan; “The future of sun”, 1990, “La corsa della green economy”, 2010) and is member of the advisory board of different associations and journals in the field of energy.\n\nHe was President of “Bologna-Energia 2010”, the Energy agency of the City of Bologna and acted as Advisor of the Mayor of Palermo in the area of Energy and Environment. \nFrom 1997 to 2000 he was advisor of the Minister of Environment and from 2000 to 2002 he was appointed as General Director at the Ministry of the Environment. \nDuring this period he has been involved in the policies connected with the Kyoto Protocol, particularly on energy efficiency and in the field of renewable energies. \nIn this area he coordinated the Italian “10.000 solar roofs” program and launched the \"Car Sharing\" program in Italy and the \"Car Free Sundays\"\nFrom 2007 to 2008 he was energy advisor of the Minister of Economic Development and was involved in the definitions of new laws on energy efficiency in buildings.\nFrom 2006 to 2009 he was President of the Italian Ecolabel section; \nHe has provided information and consulting services to a large variety of clients including Local Municipalities, Regions, Ministries, European Commission, IEA\n\nSince 2003 he is the scientific Director of the \"Kyoto Club\" a not-for-profit association made up of representatives of 230 Italian business, financial, local municipalities that promotes a proactive stance on the global warming issue (www.kyotoclub.org).\nSince 2002 he is the scientific Director of the bimonthly magazine \"Qualenergia\" and of the most visited Italian web site specialized on energy efficiency and on renewable energies (www.qualenergia.it). \nFrom 2014 to 2016 he has been President of Green Building Council Italy and from 2015 to 2016 President of FREE, the Coordination of 27 Italian Associations on renewable energies and energy efficiency\n\nIn 2001 he received the “European solar prize 2001” by Eurosolar, for his “extraordinary individual commitment”.\n\nFrom 2008 to date he is President of Exalto Energy&Innovation a company involved in research activities, consulting and acting as Esco in the field of renewable energies and energy efficiency (www.exaltoenergia.it).\n\n"}
{"id": "2752651", "url": "https://en.wikipedia.org/wiki?curid=2752651", "title": "History of measurement", "text": "History of measurement\n\nThe earliest recorded systems of weights and measures originate in the 3rd or 4th millennium BC. Even the very earliest civilizations needed measurement for purposes of agriculture, construction, and trade. Early standard units might only have applied to a single community or small region, with every area developing its own standards for lengths, areas, volumes and masses. Often such systems were closely tied to one field of use, so that volume measures used, for example, for dry grains were unrelated to those for liquids, with neither bearing any particular relationship to units of length used for measuring cloth or land. With development of manufacturing technologies, and the growing importance of trade between communities and ultimately across the Earth, standardized weights and measures became critical. Starting in the 18th century, modernized, simplified and uniform systems of weights and measures were developed, with the fundamental units defined by ever more precise methods in the science of metrology. The discovery and application of electricity was one factor motivating the development of standardized internationally applicable units. \n\nWeights and measures have taken a great variety of forms over the course of history, from simple informal expectations in barter transactions to elaborate state and supranational systems that integrate measures of many different kinds. Weights and measures from the oldest societies can often be inferred at least in part from archaeological specimens, often preserved in museums. The comparison of the dimensions of buildings with the descriptions of contemporary writers is another source of information. An interesting example of this is the comparison of the dimensions of the Greek Parthenon with the description given by Plutarch from which a fairly accurate idea of the size of the Attic foot is obtained. Because of the comparative volume of artifacts and documentation, we know much more about the state-sanctioned measures of large, advanced societies than we do about those of smaller societies or about the informal measures that often coexisted with official ones throughout history. In some cases, we have only plausible theories and we must sometimes select the interpretation to be given to the evidence.\n\nBy studying the evidence given by all available sources, and by correlating the relevant facts, we obtain some idea of the origin and development of the units. We find that they have changed more or less gradually with the passing of time in a complex manner because of a great variety of modifying influences. It is possible to group official measurement systems for large societies into historical systems that are relatively stable over time, including: the Babylonian system, the Egyptian system, the Phileterian system of the Ptolemaic age, the Olympic system of Greece, the Roman system, the British system, and the metric system.\n\nThe earliest known uniform systems of weights and measures seem all to have been created at some time in the 4th and 3rd millennia BC among the ancient peoples of Egypt, Mesopotamia and the Indus Valley, and perhaps also Elam (in Iran) as well.\n\nEarly Babylonian and Egyptian records and the Hebrew Bible indicate that length was first measured with the forearm, hand, or finger and that time was measured by the periods of the sun, moon, and other heavenly bodies. When it was necessary to compare the capacities of containers such as gourds or clay or metal vessels, they were filled with plant seeds which were then counted to measure the volumes. When means for weighing were invented, seeds and stones served as standards. For instance, the carat, still used as a unit for gems, was derived from the carob seed.\n\nThe Egyptian cubit, the Indus Valley units of length referred to above and the Mesopotamian cubit were used in the 3rd millennium BC and are the earliest known units used by ancient peoples to measure length. The units of length used in ancient India included the dhanus, or dhanush (bow), the krosa (cry, or cow-call) and the yojana (stage).\n\nThe common cubit was the length of the forearm from the elbow to the tip of the middle finger. It was divided into the span of the hand or the length between the tip of little finger to the tip of the thumb (one-half cubit), the palm or width of the hand (one sixth), and the digit or width of the middle finger (one twenty-fourth). The Royal Cubit, which was a standard cubit enhanced by an extra palm—thus 7 palms or 28 digits long—was used in constructing buildings and monuments and in surveying in ancient Egypt. The inch, foot, and yard evolved from these units through a complicated transformation not yet fully understood. Some believe they evolved from cubic measures; others believe they were simple proportions or multiples of the cubit. In whichever case, the Greeks and Romans inherited the foot from the Egyptians. The Roman foot (~296 mm) was divided into both 12 \"unciae\" (inches) (~24.7 mm) and 16 digits (~18.5 mm). The Romans also introduced the \"mille passus\" (1000 paces) or double steps, the pace being equal to five Roman feet (~1480 mm). The Roman mile of 5000 feet (1480 m) was introduced into England during the occupation. Queen Elizabeth I (reigned from 1558 to 1603) changed, by statute, the mile to 5280 feet (~1609 m) or 8 furlongs, a furlong being 40 rod (unit)s (~201 m) of 5.5 yards (~5.03 m) each.\n\nThe introduction of the yard (0.9144 m) as a unit of length came later, but its origin is not definitely known. Some believe the origin was the double cubit, others believe that it originated from cubic measure. Whatever its origin, the early yard was divided by the binary method into 2, 4, 8, and 16 parts called the half-yard, span, finger, and nail. The association of the yard with the \"gird\" or circumference of a person's waist or with the distance from the tip of the nose to the end of the thumb of King Henry I (reigned 1100–1135) are probably standardizing actions, since several yards were in use in Britain.\nThere were also Rods, Poles and Perches for measurements of length. The following table lists the equivalents.\n\nThe grain was the earliest unit of mass and is the smallest unit in the apothecary, avoirdupois, Tower, and troy systems. The early unit was a grain of wheat or barleycorn used to weigh the precious metals silver and gold. Larger units preserved in stone standards were developed that were used as both units of mass and of monetary currency. The pound was derived from the mina used by ancient civilizations. A smaller unit was the shekel, and a larger unit was the talent. The magnitude of these units varied from place to place. The Babylonians and Sumerians had a system in which there were 60 shekels in a mina and 60 minas in a talent. The Roman talent consisted of 100 libra (pound) which were smaller in magnitude than the mina. The troy pound (~373.2 g) used in England and the United States for monetary purposes, like the Roman pound, was divided into 12 ounces, but the Roman uncia (ounce) was smaller. The carat is a unit for measuring gemstones that had its origin in the carob seed, which later was standardized at 1/144 ounce and then 0.2 gram. \n\nGoods of commerce were originally traded by number or volume. When weighing of goods began, units of mass based on a volume of grain or water were developed. The diverse magnitudes of units having the same name, which still appear today in our dry and liquid measures, could have arisen from the various commodities traded. The larger avoirdupois pound for goods of commerce might have been based on volume of water which has a higher bulk density than grain.\n\nThe stone, quarter, hundredweight, and ton were larger units of mass used in Britain. Today only the stone continues in customary use for measuring personal body weight. The present stone is 14 pounds (~6.35 kg), but an earlier unit appears to have been 16 pounds (~7.25 kg). The other units were multiples of 2, 8, and 160 times the stone, or 28, 112, and 2240 pounds (~12.7 kg, 50.8 kg, 1016 kg), respectively. The hundredweight was approximately equal to two talents. The ton of 2240 pounds is called the \"long ton\". The \"short ton\" is equal to 2000 pounds (~907 kg). A tonne (t) is equal to 1000 kg.\n\nThe division of the circle into 360 degrees and the day into hours, minutes, and seconds can be traced to the Babylonians who had sexagesimal system of numbers. The 360 degrees may have been related to a year of 360 days. Many other systems of measurement divided the day differently -- counting hours, decimal time, etc. Other calendars divided the year differently.\n\nDecimal numbers are an essential part of the metric system, with only one base unit and multiples created on the decimal base, the figures remain the same. This simplifies calculations. Although the Indians used decimal numbers for mathematical computations, it was Simon Stevin who in 1585 first advocated the use of decimal numbers for everyday purposes in his booklet \"De Thiende\" (old Dutch for 'the tenth'). He also declared that it would only be a matter of time before decimal numbers were used for currencies and measurements. His notation for decimal fractions was clumsy, but this was overcome with the introduction of the decimal point, generally attributed to Bartholomaeus Pitiscus who used this notation in his trigonometrical tables (1595).\n\nIn his \"Essay towards a Real Character and a Philosophical Language\", published in 1668, John Wilkins proposed a system of measurement that was very similar in concept to today's metric system. He proposed retaining the second as the basic unit of time and proposed that the length of a pendulum which crossed the zero position once a second (i.e. had a period of two seconds) should be the base unit of length. This length, for which he proposed the name \"standard\", would have been 994 mm. His base unit of mass, which he proposed calling a \"hundred\", would have been the mass of a cubic standard of distilled rainwater. The names that he proposed for decimal multiples and subunits of his base units of measure were the names of units of measure that were in use at the time.\n\nIn 1670, Gabriel Mouton published a proposal that was in essence similar to Wilkins' proposal, except that his base unit of length would have been 1/1000 of a minute of arc (about 1.852 m) of geographical latitude. He proposed calling this unit the virga. Rather than using different names for each unit of length, he proposed a series of names that had prefixes, rather like the prefixes found in SI.\n\nIn 1790, Thomas Jefferson submitted a report to the United States Congress in which he proposed the adoption of a decimal system of coinage and of weights and measures. He proposed calling his base unit of length a \"foot\" which he suggested should be either or of the length of a pendulum that had a period of one second – that is or of the \"standard\" proposed by Wilkins over a century previously. This would have equated to 11.755 English inches (29.8 cm) or 13.06 English inches (33.1 cm). Like Wilkins, the names that he proposed for multiples and subunits of his base units of measure were the names of units of measure that were in use at the time. The great interest in geodesy during this era, and the measurement system ideas that developed, influenced how the continental US was surveyed and parceled. The story of how Jefferson's full vision for the new measurement system came close to displacing the Gunter chain and the traditional acre, but ended up not doing so, is explored in Andro Linklater's \"Measuring America\".\n\nThe metric system was first described in 1668 and officially adopted by France in 1799. Over nineteenth and twentieth centuries, it became the dominant system worldwide, although several countries, including the United States and China, continue to use their customary units. Among the numerous customary systems, many have been adapted to become an integer multiple of a related metric unit: The Scandinavian mile is now defined as 10 km, the Chinese jin is now defined as 0.5 kg, and the Dutch ons is now defined as 100 g. The American system is unusual in that its units have not been adapted in such a manner.\n\n"}
{"id": "3539535", "url": "https://en.wikipedia.org/wiki?curid=3539535", "title": "Homasote", "text": "Homasote\n\nHomasote is a brand name associated with the product generically known as cellulose based fiber wall board, which is similar in composition to papier-mâché, made from recycled paper that is compressed under high temperature and pressure and held together with an adhesive. It is thick and comes in sheets . The Homasote Company operates a factory in the West Trenton section of Ewing Township, New Jersey.\n\nThe Agasote Millboard Company was founded as a division of the Bermuda Trading Company in 1909 by Eugenius Harvey Outerbridge. Outerbridge brought the process to the United States from England.\n\nThe first commercial use of the panels were for lining and insides of railroad cars. In 1915, the company won a contract to use the panels as automobile tops. From 1915 to 1925 they supplied board for the tops of Ford Motor Company, Buick, Nash Motors, Studebaker, and Dodge. They also manufactured a larger panel, sold as \"vehisote\" for truck panels. The panels were used for the exterior of field hospitals and military housing in France during World War I. By 1925 car manufacturers switched to canvas tops and Agasote lost sales, so the company heavily promoted Homasote for its versatility and insulation properties. The company then changed its name to Homasote after its now largest product. The company makes a version called \"440 SoundBarrier\".\n\nHomasote is frequently used by model railroading for the sub-roadbed or roadbed, because of its noise-deadening qualities, ease of forming into shapes used as roadbed for tracks, ease of driving nails to hold track sections to the bed, light weight and retention of form under plaster scenery. Cork, plywood, hardboard, drywall, and foam insulation are common alternatives to Homasote.\n\nHomasote has also found its way into numerous church basements and fellowship halls as the material used by church dartball leagues for the backboard of their baseball field shaped dartboards. It has proven to be a durable material that darts can be thrown into repeatedly without damaging their metal tips.\n\nHomasote has also been used to cover ice rinks in multipurpose arenas for basketball and other events. It keeps ice cold and is durable enough to walk on.\n\nHomasote was also widely used as wall sheeting from the 1940s into the 1970s; however, due to the development of the more fire-resistant gypsum board it has decreased in popularity as a wall sheeting.\n\nHomasote is also commonly found in studio spaces and featured in many art institutions as a wall covering and doubling as a type of cork board. It often receives hundreds of coats of paint over the years due to the strength of the product.\n\nHomasote is also used for blocking knit or crochet pieces. \"Homosote is sturdy, and incredibly absorbent. It will wick water away from your garment so it dries more quickly. And it's like a bulletin board -- you can stick pins in it easily.\"\n\nAt Northwest Folklife the dance floor in the Fisher Pavilion is built each year from two layers of Homasote overlaid with a layer of painted Masonite. The Homasote base reduces the incidence of impact injuries such as shin splints caused by dancing on the concrete floor.\n\nHomasote is also widely used in theatrical sets as a noise deadening layer for stage platforms; which consist of a plywood sublayer, a Homasote layer, and a Masonite top layer.\n\n"}
{"id": "744460", "url": "https://en.wikipedia.org/wiki?curid=744460", "title": "Jumbotron", "text": "Jumbotron\n\nA jumbotron, sometimes referred to as Jumbovision, is a video display using large-screen television technology. The original JumboTron was developed by Sony, and is typically used in sports stadiums and concert venues to show close up shots of an event. \n\nThe JumboTron was originally manufactured by Sony, and is recognized as one of the largest non-projection video displays ever manufactured. Sony creative director Yasuo Kuroki is credited with the development of the JumboTron.\n\nWhile the JumboTron and similar large-screen displays are physically large, they were often low in display resolution. The JumboTron at the now-demolished Tampa Stadium in Tampa, Florida, measured 30 ft (9 m) diagonally with a resolution of only 240×192 pixels, below VHS resolution. Screen size since then varies depending on the venue. The display introduced in 1985 was 40 meters wide by 25 meters tall. Newer, LED-based large screens have an order of magnitude greater than the early JumboTron resolution at a fraction of the cost. For example, the much publicized center-hung video board in the Dallas Cowboys' AT&T Stadium is 72 feet tall and 160 feet wide (22 m x 49 m), displaying HDTV at 1920 x 1080 resolution, 45 times more pixels.\n\nThe largest JumboTron in use was located at SkyDome (now Rogers Centre) in Toronto, Ontario, and measured 10 m tall by 33.5 m wide (33 ft × 110 ft) at a cost of US$17 million. By comparison, a similar-sized LED system sold today would cost around $3 million. The Rogers Centre JumboTron was replaced in 2005 by a Daktronics ProStar as part of a stadium revitalization project.\n\nOriginally, the JumboTron was not an LED (light-emitting diode) display since blue LEDs were unavailable at the time, and the only green LEDs available were of the traditional yellow-green variety, which were unsuitable for an RGB display. Each display consisted of multiple modules composed of 16 or more small flood-beam CRTs (cathode ray tubes), each of which included from 2 to 16 pixels composed of red, green, and blue phosphors. Sony displayed one of the earliest versions at the Expo '85 World's Fair in Tsukuba.\nEventually, JumboTron systems adopted LED technology as blue and pure green LEDs were developed. LED-based systems have about ten times the lifespan of CRT-based systems, a key reason for the change.\n\nAlthough \"JumboTron\" is a registered trademark owned by the Sony Corporation, Sony stopped manufacturing the devices under that name in 2001 and the word \"jumbotron\" has since become a genericized trademark.\n\n\n"}
{"id": "25227555", "url": "https://en.wikipedia.org/wiki?curid=25227555", "title": "Kytoon", "text": "Kytoon\n\nA kytoon or kite balloon is a tethered aircraft which obtains some of its lift dynamically as a heavier-than-air kite and the rest aerostatically as a lighter-than-air balloon. The word is a portmanteau of kite and balloon.\n\nThe primary advantage of a kytoon is that it remains in a reasonably stable position above the tether point, irrespective of the strength of wind, whereas ordinary balloons and kites are less stable.\n\nThe kytoon has been used for many purposes both civil and military.\n\nIn 1919, a handbook was published giving extensive details to support the kite balloon crafts being used in the military. Described is the first kite balloon made in 1893 by Captains Parseval and Sigsfeld at the Berlin works of the Prussian Balloon Battalion; theirs was the \"predecessor of the \"Drachen\" balloon.\" \"This was the first real kite balloon flying like a kite with a fairly constant angle and direction relative to the wind and remained practically unchanged until the beginning of the war in 1914-1918.\"\nA hybrid kite-balloon was patented by Domina Jalbert in 1944 as patent US2431938. and later became known as the kytoon. Jalbert furthered his attention on kite balloons also with another patent filed on August 31, 1945 titled \"Kite Balloon\"\n\nThe Allsopp Helikite is a modern helium-filled example.\n\nA captive balloon tends to drift down the wind and the harder the wind blows, the further the balloon drifts. This leans the tether over at an angle, pulling the balloon lower. On a kytoon, the kite action lifts the balloon, counteracting this pull and holding the kytoon in position. As the wind blows harder, the kite action lifts harder. This can provide good stability even in strong winds.\n\nIn low or gusty winds a kite can nose-dive, losing a large amount of height even if it recovers. Because a kytoon is buoyant it does not nose-dive and remains in position even in relatively still air.\n\nApplications of the kytoon have included:\n\n\n\n"}
{"id": "39061016", "url": "https://en.wikipedia.org/wiki?curid=39061016", "title": "Laser Weapon System", "text": "Laser Weapon System\n\nThe AN/SEQ-3 Laser Weapon System or XN-1 LaWS is a directed-energy weapon developed by the United States Navy. The weapon was installed on for field testing in 2014. In December 2014, the United States Navy reported that the LaWS system worked perfectly against low-end asymmetric threats, and that the commander of the \"Ponce\" is authorized to use the system as a defensive weapon.\n\nThe LaWS is a ship-defense system that has so far publicly engaged an unmanned aerial vehicle (UAV or drone) and a simulated small-boat attacker. LaWS uses an infrared beam from a solid-state laser array which can be tuned to high output to destroy the target or low output to warn or cripple the sensors of a target. Among the advantages of this device versus projectile weapons is the low cost per shot, as each firing of the weapon requires only the minimal cost of generating the energetic pulse; by contrast ordnance for projectile weapons must be designed, manufactured, handled, transported and maintained, and takes up storage space.\n\nThe LaWS is designed to be used against low-end asymmetric threats. Scalable power levels allow it to be used on low-power to dazzle a person's eye to non-lethally make them turn away from a threatening posture, and increase to 30 thousand watts (30 kW) to fry sensors, burn out motors, and detonate explosive materials. Against a vital point on small UAVs, one can be shot down in as little as two seconds. When facing small boats, the laser would target a craft's motor to disable it and make it \"dead in the water,\" then repeating this against others in rapid succession, requiring only a few seconds of firing per boat. Targeting the platform is more effective than individual crewmembers, although the LaWS is accurate enough to target explosive rockets if on board, whose detonations could kill the operators. Against a larger aircraft like a helicopter, LaWS can burn through some vital components to cause it to fall and crash.\n\nIn 2010, Kratos Defense & Security Solutions was awarded an $11 million contract to support the Naval Surface Warfare Center (NSWC) in the development of LaWS for the U.S. Navy's Directed Energy and Electric Weapon Systems (DE&EWS) program. The May 2012 NSWC test used a close-in weapon system control system to enable the beam director to track an unmanned aerial vehicle target.\n\nThe LaWS was to be installed on USS \"Ponce\" in summer 2014 for a 12-month trial deployment. The Navy spent about $40 million over the past six years on research, development, and testing of the laser weapon. It will be directed to targets by the Phalanx CIWS radar. If tests go well, the Navy could deploy a laser weapon operationally between 2017 and 2021 with an effective range of . The exact level of power the LaWS will use is unknown but estimated between 15–50 kW for engaging small aircraft and high-speed boats. Directed-energy weapons are being pursued for economic reasons, as they can be fired for as little as one dollar per shot, while conventional gun rounds and missiles can cost thousands of dollars each. The Navy has a history of testing energy weapons, including megawatt chemical lasers in the 1980s. Their chemicals were found to be too hazardous for shipboard use, so they turned to less powerful fiber solid-state lasers. Other types can include slab solid state and free electron lasers. The LaWS benefitted from commercial laser developments, with the system basically being six welding lasers \"strapped together\" that, although they don't become a single beam, all converge on the target at the same time. It generates 33 kW in testing, with follow-on deployable weapons generating 60–100 kW mounted on a Littoral Combat Ship or to destroy fast-attack boats, drones, manned aircraft, and anti-ship cruise missiles out to a few miles. In the short term, the LaWS will act as a short-range, self-defense system against drones and boats, while more powerful lasers in the future should have enough power to destroy anti-ship missiles; Navy slab lasers have been tested at 105 kW with increases to 300 kW planned. Laser weapons like the LaWS are meant to complement other missile and gun-based defense systems rather than replace them. While lasers are significantly cheaper and have virtually unlimited magazines, their beams can be disrupted by atmospheric and weather conditions (especially when operating at the ocean's surface) and are restricted to line-of-sight firing to continuously keep the beam on target. More conventional systems will remain in place for larger and longer-range targets that require the use of kinetic defense.\n\nThe LaWS was deployed on the \"Ponce\" in late August 2014 to the Persian Gulf with the U.S. 5th Fleet. The deployment is to test the feasibility of a laser weapon in a maritime environment against heat, humidity, dust, and salt water and to see how much power is used. The system has scalable power levels to be able to fire a non-lethal beam to dazzle a suspect vessel, and fire stronger beams to physically destroy a target; range is classified. Although neighboring Iran has threatened to block the Strait of Hormuz out of the Gulf using small boat swarms that the LaWS is able to counter, it was not designed or deployed specifically to be used against any one particular country.\n\nIn September 2014, the LaWS was declared an operational asset, so the ship commander has permission to use it for self-defense. Humans are not a target of the weapon under stipulations of the Convention on Certain Conventional Weapons, but targets do include UAVs, helicopters, and fast patrol craft. Rules of engagement have been developed for its use, but details have not been released. The Navy has released video of the LaWS on deployment disabling a ScanEagle UAV, detonating a rocket propelled grenade, and burning out the engine of a rigid hull inflatable boat. Officials said it is working beyond expectations. Compared to hundreds of thousands or millions of dollars for a missile, one laser shot costs only 59 cents. Composed of commercial laser components and proprietary Navy software, it is powered and cooled by a \"skid\" through a diesel generator, separate from the ship's electrical systems, giving greater efficiency relative to power provided of 35 percent. Mounted on the \"Ponce\"'s superstructure above the bridge, its powerful optics are also useful as a surveillance tool that can detect objects at unspecified but \"tactically significant ranges\"; sailors have equated its surveillance abilities to having the Hubble telescope at sea. Sailors are using it for targeting and training daily, whether to disable or destroy test targets or for potential target identification. The system is operated through a flat screen monitor and a gaming system-like controller integrated into the ship's combat system, so anyone with experience playing common video games can operate the weapon. It has functioned well against adverse weather, able to work in high humidity and after a dust storm. However, the system is not expected to work during harsh sandstorms and has not been tested in such conditions because \"it didn't make much sense to\", but threats would also not be expected to be operational under the same conditions. Deployments on other ships are being examined and although the LaWS was planned to remain deployed for one year, it performed so well that fleet leadership decided to keep it on the \"Ponce\" as long as it was at sea.\n\nFollowing a review of several ship classes to determine which had available space, power, and cooling, it was decided that after the \"Ponce's\" planned decommissioning in 2018, the LaWS will be moved to the new amphibious transport dock ship for indefinite testing, it will utilise the space and power connections reserved for VLS to house the LaWS power and control modules while the laser itself will be bolted to the deck. Because the installation will be only a trial, LaWS will not be integrated into the ship's warfare system.\n\nIn January 2018, the Navy announced a $150 million contract with Lockheed Martin for the production of two more LaWS units to be delivered in 2020; one will be fitted to while the other will be used for land-based testing. Further contract options could bring its value to $942.8 million.\n\n"}
{"id": "14087432", "url": "https://en.wikipedia.org/wiki?curid=14087432", "title": "Lofotkraft", "text": "Lofotkraft\n\nLofotkraft is a power company that operates the power grid in Lofoten, Norway as well as ten hydroelectric power plants through the subsidiary Lofotkraft Produksjon. Since 1998 retailing of power has been managed by Kraftinor, a joint venture with Narvik Energi. It also owns half of Lofotkraft Vind, along with Narvik Energi.\n\nThe company is owned by the six municipalities it operates the power grid in, Vestvågøy (41%), Vågan (41%), Flakstad (6.5%), Moskenes (6.5%), Værøy (3%) and Røst (2%).\n"}
{"id": "6001495", "url": "https://en.wikipedia.org/wiki?curid=6001495", "title": "MSW/LFG", "text": "MSW/LFG\n\nMSW/LFG stands for municipal solid waste and landfill gas. The United States Environmental Protection Agency has several standards required for MSW landfills to help ensure public and environmental safety.\n\n\n"}
{"id": "23937653", "url": "https://en.wikipedia.org/wiki?curid=23937653", "title": "Mammoths, Sabertooths, and Hominids", "text": "Mammoths, Sabertooths, and Hominids\n\nMammoths, Sabertooths, and Hominids: 65 Million Years of Mammalian Evolution in Europe is a book written by Jordi Agustí and illustrated by Mauricio Antón. It was first published in 2002 by Columbia University Press.\n\nThe book is a journey through of palaeontological records, from the extinction of the dinosaurs to just before present. Notwithstanding the title, the book includes the complex evolutionary records of most continents within its pages – thoroughfully described by Agustí and breathtakingly illustrated by Antón.\n\nAgustí and Antón provide a broad overview of the Tertiary history of mammals in Europe: the evolutionary changes within the European fauna as well as the fates of immigrant taxa that arrived from other continents.\n\n"}
{"id": "22901829", "url": "https://en.wikipedia.org/wiki?curid=22901829", "title": "Marches Energy Agency", "text": "Marches Energy Agency\n\nMarches Energy Agency (MEA) is an energy agency in the United Kingdom, operating on a not-for-profit basis. The agency was formed by Shropshire County Council in 1995 to promote the use of sustainable energy in the area. Richard Davies was the director from 1998 to 2014, having previously worked as a chemical engineer. Much of their work is conduction in partnership with local authorities, and focuses on helping communities cut their carbon emissions, especially in rural areas.\n\nAlthough MEA initially operated on the English side of the Welsh Marches, it has since expanded its work through service level agreements with Staffordshire Moorlands District Council, the entire Shropshire Council area, and in 2009 to Nottinghamshire and Derbyshire through an agreement with the Local Authority Energy Partnership.\n\nIn 2009 MEA won an Ashden Award for their work to create Low Carbon Communities.\n\n\n"}
{"id": "518082", "url": "https://en.wikipedia.org/wiki?curid=518082", "title": "Monolithic church", "text": "Monolithic church\n\nA monolithic church or rock-hewn church is a church made from a single block of stone. Because freestanding rocks of sufficient size are rare, such edifices are usually hewn into the ground or into the side of a hill or mountain. They can be of comparable architectural complexity to constructed buildings.\n\nThe term \"monolithic church\" is most often used to refer to the complex of eleven churches in Lalibela, Ethiopia, believed to have been created in the 12th century. \n\nA series of eleven monolithic churches in Lalibela are the Church of the Redeemer, of Saint Mary, of Mount Sinai, of Golgotha, of the House of the Cross, of the House of the Virgins, of Saint Gabriel, of Abba Matta, of Saint Mercurius, and of Immanuel. The most famous of the edifices is the cross-shaped Church of St. George (Bete Giyorgis). Tradition credits its construction to the Zagwe dynasty King Gebre Mesqel Lalibela, who was a devout Orthodox Tewahedo Christian. The medieval monolithic churches of this 12th-century 'New Jerusalem' are situated in a mountainous region in the heart of Ethiopia near a traditional village. Lalibela is an important center of Ethiopian Christianity, and even today is a place of pilgrimage and devotion. Lalibela is one of the world's heritage sites registered by UNESCO.\n\nMany other churches were hewn from rock in Ethiopia, outside of Lalibela. This practice was very common in Tigray, where the outside world knew of only a few such churches until the Catholic priest Abba Tewelde Medhin Josief presented a paper to the Third International Conference of Ethiopian Studies in which he announced the existence of over 120 churches, 90 of which were still in use. Despite Dr. Josief's death soon after his presentation, research over the next few years raised the total number of these rock-hewn churches to 153.\n\nThere are a number of monolithic churches elsewhere in the world. However, none have the free-standing external walls of the Lalibela churches. They instead more closely resemble cave monasteries in that they consist of tunnels converging into a single rock. Examples include:\n\n\n\n"}
{"id": "34740010", "url": "https://en.wikipedia.org/wiki?curid=34740010", "title": "NACA Report No. 736", "text": "NACA Report No. 736\n\nNACA Report No. 736 - Nonstationary Flow about a Wing-aileron-tab Combination Including Aerodynamic Balance was issued by the United States National Advisory Committee for Aeronautics in 1942. It analyzes the oscillating air forces on an airfoil that is equipped with various control or lift-augmenting devices.\n\nNACA Report No. 736 presents a continuation of the flutter analysis published in NACA Report No. 496. The results of that paper have been extended to include the effect of aerodynamic balance and the effect of adding a control tab to the aileron. The aerodynamic coefficients are presented in a form usable for application to the flutter problem.\n\nNACA Report 736 presents theoretical expressions for the forces and the moments in a uniform\nhorizontal air stream on a plane airfoil performing small sinusoidal motions in several degrees of freedom: verticaI motion, torsional movement about an arbitrary spanwise axis, aileron movement about a hinge axis not necessarily located at the Ieading edge of the aileron, and tab movement similar to the aileron movement. The solution of this problem has direct application to the larger problem of flutter involving these various degrees of freedom and, in particular, to flutter of tails with control surfaces, including servocontrols. The development of the theory is analogous with\nthat of Report No. 496, which treats the case of three degreea of freedom: vertical motion, torsional movement about an arbitrary spanwise axis, and an aileron movement about a hinge axis located at the aileron's leading edge.\n\nThe material presented in NACA Report No. 736 represents an extension of the work in NACA Report No. 496, which has been expanded to include tab functions, and the effect of aerodynamic balance. Inasmuch as this addition fits in with the general arrangement of the earlier report, reference should be made to that report, and also to NACA Report No. 685, for application to the flutter problem.\n\n"}
{"id": "45160933", "url": "https://en.wikipedia.org/wiki?curid=45160933", "title": "Nanocomposite hydrogels", "text": "Nanocomposite hydrogels\n\nNanocomposite hydrogels (NC gels) are nanomaterial-filled, hydrated, polymeric networks that exhibit higher elasticity and strength relative to traditionally made hydrogels. A range of natural and synthetic polymers are used to design nanocomposite network. By controlling the interactions between nanoparticles and polymer chains, a range of physical, chemical, and biological properties can be engineered. The combination of organic (polymer) and inorganic (clay) structure gives these hydrogels improved physical, chemical, electrical, biological, and swelling/de-swelling properties that cannot be achieved by either material alone. Inspired by flexible biological tissues, researchers incorporate carbon-based, polymeric, ceramic and/or metallic nanomaterials to give these hydrogels superior characteristics like optical properties and stimulus-sensitivity which can potentially be very helpful to medical (especially drug delivery and stem cell engineering) and mechanical fields.\n\nNanocomposite hydrogels are not to be confused with \"nanogel\", a nanoparticle composed of a hydrogel.\n\nThe synthesis of nanocomposite hydrogels is a process that requires specific material and method. These polymers need to be made up of equally spaced out, 30 nm in diameter, clay platelets that can swell and exfoliate in the presence of water. The platelets act as cross-links to modify molecular functions to enable the hydrogels to have superior elasticity and toughness that resembles closely that of biological tissue. Using clay platelets that do not swell or exfoliate in water, using an organic cross-linker such as N,N-methylenebisacrylamide(BIS), mixing of clay and BIS, or preparing nanocomposite hydrogels in a method other than cross-link, will be unsuccessful.\n\nDespite all the specifications, the process of synthesizing nanocomposite hydrogels is simple and because of the flexible nature of the material, these hydrogels can be easily made to come in different shapes such as huge blocks, sheets, thin films, rods, hollow tubes, spheres, bellows and uneven sheets.\n\nNanocomposite hydrogels are tough, and can withstand stretching, bending, knotting, crushing, and other modifications.\n\nTensile testings were performed on nanocomposite hydrogels to measure the stress and strain it experiences when elongated under room temperature. The results show that this material can be stretched up to 1000% of its original length.\n\nHysteresis is used to measure the compression properties of nanocomposite hydrogels, which shows that this material can withstand around 90% compression. This data shows that nanocomposite hydrogels exhibit superior strength relative to conventionally-made hydrogels, which would have broken down under less compression.\n\nThe porous network of clay particles enable nanocomposite hydrogels to swell in the presence of water. Swelling (and de-swelling) distinguishes NC gels from conventionally-made hydrogels (OR gels) as it is a property that OR gels lack. The swelling property of NC gels allows them to collect the surrounding aqueous solution instead of being dissolved by it, which helps make them good candidates for drug delivery carriers.\n\nNanocomposite hydrogels are observed to be temperature sensitive and will change temperature when their surrounding is altered. Inorganic salts, when absorbed, will result in changing the hydrogels to a lower temperature whereas cat-ionic surfactant will shift the temperature the other way. The temperature of these hydrogels are around 40 degrees Celsius, making it a possible candidate for use as biomaterial. The stimulus-sensitivity of hydrogels allow for a responsive release system where the hydrogels can be designed to deliver the drug in response to changes in condition of the body.\n\nNanocomposite hydrogels that are enforced with carbon-based nanomaterials are mechanically tough and electrically conducive, which make them suitable for use in biomedicine, tissue engineering, drug delivery, biosensing, etc. The electrical conducting property of these hydrogels allow them to mimic the characteristic of nerve, muscle, and cardiac tissues. However, even though these nanocomposite hydrogels demonstrate some functions of human tissue in lab environments, more research is needed to ensure their utility as tissue replacement.\n\nNanocomposite hydrogels incorporated with polymeric nanoparticles are tailored for drug delivery and tissue engineering. The addition of polymeric nanoparticles gives these hydrogels a reinforced polymeric network that is more stiff and has the ability to enclose hydrophilic and hydrophobic drugs along with genes and proteins. The high stress-absorbing property makes them a potential candidate for cartilage tissue engineering.\n\nMost inorganic nanoparticles used for nanocomposite hydrogels are already present in and necessary for the body, and thus present no negative impacts on the body. Some of them, like calcium and silicon, help with preventing bone loss and skeletal development. Others, like nanoclays, improve the structural formation and characteristics of hydrogels where they acquire self-healing properties, flame retardant structures, elasticity, super gas-barrier membrane, oil-repellence, etc. The unique properties obtained by incorporating nanocomposite hydrogels with inorganic nanoparticles will let researchers work on improving bone-related tissue engineering.\n\nThe electrical and thermal conductivity and magnetic property of metals enhance the electrical conductivity and antibacterial property of nanocomposite hydrogels when incorporated. The electrical conducting property is necessary for the hydrogels to start forming functional tissues and be used as imaging agents, drug delivery systems, conductive scaffolds, switchable electronics, actuators, and sensors.\n\nResearchers have been looking for a material that can mimic tissue properties to make the tissue engineering process more effective and less invasive to the human body. The porous, interconnecting network of nanocomposite hydrogels, created through cross-link, enable wastes and nutrients to easily enter and exit the structure, and their elastomeric properties let them acquire the desired anatomical shape without needing prior molding. The porous structure of this material would also make the process of drug delivery easier where the pharmaceutical compounds present in the hydrogel can easily escape and be absorbed by the body. Aside from that, researchers are also looking into incorporating nanocomposite hydrogels with silver nanoparticles for antibacterial applications and microorganism elimination in medical and food packing and water treatment.Hydrogels infused with nanoparticles have a number of biological applications, including: tissue engineering, chemical and biological sensing and drug and gene delivery.\n\nAs tissue replacements, nanocomposite hydrogels need to interact with cells and form functional tissues. With the incorporated nanoparticles and nanomaterials, these hydrogels can mimic the physical, chemical, electrical, and biological properties of most native tissue. Each type of nanocomposite hydrogels has its own unique properties that let it mimic certain types of animal tissue.\n\nThe emergence of nanocomposite hydrogels allow for more site-specific and time-controlled delivery of drugs of different sizes at improved safety and specificity. Depending on the method of inserting drugs into the material, for example, dissolved, encased, or attached, the drug carrier will be named differently: nanoparticles, nanospheres (where the drug is evenly dispersed throughout the polymeric network), or nanocapsules (where the drug is surrounded by a polymer shell structure). The elastomeric nature of this material allows the hydrogels to obtain the shape of the targeted site and thus the hydrogels can be manufactured identically and used on all patients.\n\nHydrogels are controlled drug delivery agents that can be engineered to have desired properties. Specifically, hydrogels can be designed to release drugs or other agents in response to physical characteristics of the environment like temperature and pH. The responsiveness of hydrogels is a result of their molecular structure and polymer networks.\n\nHydrogel nanoparticles have a promising future in the drug delivery field. Ideally, drug delivery systems should, “…maximize the efficacy and the safety of the therapeutic agent, delivering an appropriate amount at a suitable rate and to the most appropriate site in the body”. Nanotechnology incorporated within hydrogels has the potential to meet all the requirements of an ideal drug delivery system. Hydrogels have been studied with a variety of nanocomposites including: clay, gold, silver, iron oxide, carbon nanotubes, hydroxyapatite, and tricalcium phosphate.\n\nNanoparticles, largely due to their size related physical properties, are highly useful as drug delivery agents. They can overcome physiological barriers and reach specific targets. Nanoparticles’ size, surface charge and properties enable them to penetrate biological barriers that most other drug carriers cannot. To become even more specified, nanoparticles can be coated with targeting ligands. The ability of nanoparticles to deliver drugs to specific targets suggests the potential to limit systemic side-effects and immune responses.\n\nThe ability of nanoparticles to carry and release drugs is also largely dependent on characteristics which result from the small size and unique surface area to volume ratio of nanoparticles. Nanoparticles can generally carry drugs in two ways: drugs can either be bound to the outside of the nanoparticles or packed within the polymeric matrix of the nanoparticles. Smaller nanoparticles have higher surface area ratios and can thus bind a high quantity of drug, while larger nanoparticles can encapsulate more of the drug within its core. The best method of drug loading is dependent on the structures of the drug to be bound. Also, drug loading can occur as the nanoparticles are produced, or the drugs can be added to pre-existing nanoparticles. The release of drugs, depends largely on the size of the nanoparticle carrying it. Because nanoparticles can be bound to the surface of nanoparticles, which is large relative to the volume of the particles, drugs can be released quickly. In contrast, drugs that are loaded within nanoparticles are released more slowly.\n\nSilver nanoparticles are inserted into the 3D polymeric networks of nanocomposite hydrogels for applications in antibacterial activity and improvement in electrical conductance. The presence of silver ions either stop the respiratory enzyme from transferring electrons to oxygen molecules during respiration or prevent proteins from reacting with thiol groups (-SH) on bacteria membrane, both result in the death of bacteria and microorganism without damaging mammal cells. The size of these silver nanoparticles need to be small enough to pass through the cell membrane and thus further research is required to manufacture them into appropriate sizes.\n\nSome concerns relating to hydrogels infused with nanoparticles are the chances of either bursting, or of incomplete release of drugs. Although hydrogels infused with nanoparticles are speculated to be quite promising methods of drug, protein, peptide, oligosaccharide, vaccine, and nucleic acid delivery, more studies regarding nanotoxicology and safety are required before clinical applications can be pursued. Further, to avoid accumulation, biodegradable gels and nanoparticles are highly desirable.\n\nGel\nNanomaterials\nCross-link\n"}
{"id": "10092249", "url": "https://en.wikipedia.org/wiki?curid=10092249", "title": "New World Agriculture and Ecology Group", "text": "New World Agriculture and Ecology Group\n\nThe New World Agriculture and Ecology Group (NWAEG) is an organization focused on sustainable agriculture, conservation biology and social justice.\n\nOriginally known as the New World Agriculture Group, NWAEG (pronounced \"new-ag\") became active in the 1980s. NWAEG drew inspiration from the 1970s-1980s Science for the People movement, and many of its founding members were active in Science for the People.\n\nNWAEG's best-known project was an intensive effort to provide agricultural research and extension services to the Nicaraguan people during the Sandinista era. Cuba and Chiapas, Mexico are locations of other NWAEG projects, exemplifying the group's informal focus on Latin America.\n\n"}
{"id": "14659505", "url": "https://en.wikipedia.org/wiki?curid=14659505", "title": "Oil megaprojects (2008)", "text": "Oil megaprojects (2008)\n\nThis page summarizes projects that brought more than of new liquid fuel capacity to market with the first production of fuel beginning in 2008. This is part of the Wikipedia summary of Oil Megaprojects.\n\nTerminology\n"}
{"id": "17531494", "url": "https://en.wikipedia.org/wiki?curid=17531494", "title": "Overlogging", "text": "Overlogging\n\nOverlogging is a kind of overexploitation caused by legal or illegal logging activities that lead to unsustainable or irrecoverable deforestation and permanent habitat destruction for forest wildlife. Overlogging is often associated with attempts at reducing the \"third world debt\" but is not restricted to developing countries. With the developed world's growing demand for pulp and paper, especially - but not restricted to - for disposable tissues, overlogging is an imminent threat to Earth's forests everywhere.\n"}
{"id": "37528203", "url": "https://en.wikipedia.org/wiki?curid=37528203", "title": "Plug-in electric vehicles in Germany", "text": "Plug-in electric vehicles in Germany\n\nThe adoption of plug-in electric vehicles in Germany is actively supported by the German Federal Government. Under its National Plattform for Electric Mobility, Chancellor Angela Merkel set the goal in 2010 to deploy one million electric vehicles on German roads by 2020. , a total of 129,246 plug-in electric cars have been registered in Germany since 2010. The country is the largest passenger car market in Europe, however, , ranked as the eighth largest plug-in market in the world and the fifth largest in Europe. About 80% of the plug-in electric cars registered in the country through September 2016 were registered since January 2014. , the country had 4,800 public charging stations.\n\nThe market share of plug-in electric passenger cars increased from 0.12% of new car sales in 2012, to 0.25% in 2013, and reached 0.40% in 2014. As plug-in car sales surge in 2015, the segment's market share increased to 0.7% of new car sales. The German monthly plug-in market share passed the 1% mark for the first time ever in December 2015, with an all-time record market share of 1.28% of new car registrations that month. After the introduction of the purchase subsidy in mid-2016, the plug-in segment achieved a market share of 1.1% in September, the highest during the first nine months of 2016. The market share remained at 0.7% in 2016, but rose to 1.6% in 2017, with sales up 217% from 2016.\n\nInitially, the government announced that it would not provide subsidies to promote sales of plug-in electric vehicles, however, by the end of 2014 it was recognized that the country was well behind the set sales targets. As a result, an incentive scheme to promote plug-in electric vehicle adoption was approved in April 2016 with a budget of (), of which, a total of () is reserved for purchase subsidies, which are expected to run until all the money is disbursed, estimated to last until 2019 at the latest. Electric car buyers get a () discount while buyers of plug-in hybrid vehicles get a discount of (). Premium cars are not eligible to the incentive. Only electric vehicles purchased after 18 May 2016 are eligible for the bonus.\n\nIn May 2010, under its National Platform for Electric Mobility, Chancellor Angela Merkel set the goal to bring one million electric vehicles on German roads by 2020. However, the government also announced that it would not provide subsidies to the sales of plug-in electric cars but instead it would only fund research in the area of electric mobility. , electric vehicles and plug-ins in Germany are exempt from the annual circulation tax for a period of five years from the date of their first registration. In 2016, the annual circulation tax exemption was extended from five to ten years, backdated to 1 January 2016.\n\nThe private use of a company car is treated as taxable income in Germany and measured at a flat monthly rate of 1% of the vehicle's gross list price. So plug-in electric cars have been at a disadvantage since their price tag can be as much as double that of a car using a conventional internal combustion engine due to the high cost of the battery. In June 2013 German legislators approved a law that ends the tax disadvantage for corporate plug-in electric cars. The law, backdated to 1 January 2013, allows private users to offset the list price with per unit of battery size, expressed in kilowatt hours (kWh). The maximum offset was set at corresponding to a 20 kWh battery. the amount one can offset will sink annually by per kilowatt hour. As part of the package of financial incentives approved in 2016, private owners of plug-in electric vehicles that charge their cars in their employer premises are exempted from declaring this perk as a cash benefit in their income tax return. Employers who provide this perk are allowed to discount from their income tax a 25% of the lump sum value of the cash benefit. These two fiscal benefits apply only from 1 January 2017 until the end of 2020.\n\nIn August 2014, the federal government announced its plan to introduce non-monetary incentives through new legislation to be effective by early 2015. The proposed user benefits include measures to privilege battery-powered cars, fuel cell vehicles and some plug-in hybrids, just like Norway does, by granting local governments the authority to allow these vehicles into bus lanes, and to offer free parking and reserved parking spaces in locations with charging points. Not all plug-in hybrids will qualify for the benefits, only those with emissions of no more than 50 g/km or an all-electric range of over are eligible. The range criteria will rise to starting in 2018. The Bundestag passed the Electric Mobility Act in March 2015 authorizing local government to grant these non-monetary incentives, which are not mandatory. The law also provides issuing special license plates for electric vehicles to allow proper identification to avoid abuses of these privileges. , just 12 municipalities are considering to allow electric vehicles in the bus lanes in their jurisdiction. Most cities, including Hamburg and Munich, are not willing to allow electric cars in their bus lanes. \n\nThe special license plate authorized by the 2015 Electric Mobility Act adds the letter \"E\" at the end of the license number. Owners of all-electric cars and plug-in hybrids with a minimum all-electric range of can apply for the special license. The minimum range for eligible plug-in hybrids goes up to from January 1, 2018.\n\nAccording to the fourth progress report of the German National Platform for Electric Mobility, only about 24,000 plug-in electric cars are on German roads by the end of November 2014, well behind the target of 100,000 unit goal set for 2014. As a result, Chancellor Angela Merkel recognized in December 2014 that the government has to provide more incentives to meet the goal of having one million electric cars on the country's roads by 2020. Among others, the federal government is considering to offer a tax break for zero-emission company cars, more subsidies to expand charging infrastructure, particularly to deploy more public fast chargers, and more public funding for research and development of the next generation of rechargeable batteries.\n\nAt the beginning of 2016, German politicians from the three parties in Mrs. Merkel's ruling coalition and auto executives began talks to introduce a subsidy for green car buyers worth up to () to boost sales of electric and plug-in hybrid cars. , the German government proposal is for the auto industry to cover 40% of the cost of the purchase subsidy. Private buyers would get the full subsidy, while corporate buyers would receive for each electric car, and the program is expected to run until 2020, the deadline set to achieve the goal of 1 million electric cars on German roads. Incentives will fall by each year. In March 2016, Nissan Europe announced its support to the green car incentive and its commintment to double the government's E-premium incentive when buying a Nissan electric car, with a reduction of the purchase price of the same amount of the subsidy. Nissan Center Europe CEO said \"\"we remain convinced that the goal of one million electric cars by 2020 is still achievable\".\" According to Nissan if from now on electric car sales double every year until 2020, it is still possible to achieve the government goal.\n\nAn incentive scheme to promote plug-in electric vehicle adoption was approved in April 2016 with a budget of (). A total of () is reserved for the purchase subsidies, which are expected to run until all the money is disbursed, estimated to last until 2019 at the latest. Another () are budgeted to finance the deployment of charging stations in cities and on autobahn highway stops. And another () would go toward purchasing electric cars for federal government fleets. The program is aimed to promote the sale of 400,000 electric vehicles. The cost of the purchase incentive is shared equally between the government and automakers. Electric car buyers get a () discount while buyers of plug-in hybrid vehicles get a discount of (). Premium cars, such as the Tesla Model S and BMW i8, are not eligible to the incentive because there is a cap of () for the purchase price. Only electric vehicles purchased after 18 May 2016 are eligible for the bonus and the owner must keep the new electric car at least nine months. The same rule applies for leasing.\n\n, BMW, Citroën, Daimler, Ford, Hyundai, Kia, Mitsubishi, Nissan, Peugeot, Renault, Toyota, Volkswagen, and Volvo had signed up to participate in the scheme. In May 2016, Nissan announced the company decided to raise the bonus with an additional () to () for customers of its all-electric Leaf car and e-NV200 utility van. The online application system to claim the bonus went into effect on 2 July 2016. , a total of 26 plug-in electric cars and vans are eligible for the purchase bonus. According to the Federal Office of Economics and Export Control (BAFA), a total of 4,451 applications have been made for the government subsidy for the purchase of a plug-in electric model , consisting of 2,650 all-electrics and 1,801 plug-in hybrids. , the federal states with the most claims are Bayern (1,130), Baden-Württemberg (873), and Nordrhein-Westfalen (726).\n\n, the following 26 plug-in electric cars and vans are eligible for the purchase bonus: Audi A3 e-tron, BMW 225xe, BMW 330e, BMW i3, Citroën Berlingo Electric, Citroën C-Zero, Ford Focus Electric, Kia Soul EV, Mercedes-Benz B-Class Electric Drive (B 250e), Mercedes-Benz C350 e, Mitsubishi i-MiEV, Mitsubishi Outlander P-HEV, Nissan e-NV200 5- and 7-seater Combi, Nissan Leaf, Peugeot iOn, Peugeot Partner Electric, Renault Kangoo Z.E., Renault Zoe, Smart Fortwo electric drive, Toyota Prius Plug-in Hybrid, Volkswagen e-Golf, Volkswagen e-Up!, Volkswagen Golf GTE, Volkswagen Passat GTE, and Volvo V60 Plug-in Hybrid. , the models with the most applications are the Renault Zoe (876), BMW i3 (766), Audi A3 e-tron (462), BMW 225xe (440), and Mitsubishi Outlander P-HEV (353).\n\nAccording to \"Der Spiegel\", by the early fourth quarter of 2015 the Kia Soul EV ranked as the top selling plug-in electric car in Germany during 2015 with 2,459 units sold, with almost 1,000 registered in October, nevertheless, there were actually only a few of them on German roads. At the time, about 1,400 Soul EVs had been shipped to Norway and sold as used cars, where availability of new Soul EVs was limited. According to the magazine, Kia Motors is registering the electric cars in Germany and then shipping them to Norway, which does not belong to the European Union, as a strategy to reduce the average fleet emissions of the entire Hyundai-Kia Group. This strategy allows the carmaker to comply with European Union regulations that mandate 130 grams of emission per km in 2015, and so they avoid to pay a fine of per year for each gram above the established average limit. According to German authorities this loophole is legal. A total of 2,044 Kia Soul EVs were imported to Norway as used cars during 2015.\n\nThe German National Platform for Electric Mobility (\"Nationale Plattform Elektromobilität\") is an advisory council of the German Federal Government for electric vehicle introduction. It consists of the top representatives of industry (10 Members), politics (6), science (3), associations (3) and unions (1). It was officially established on 3 May 2010 during a meeting with German chancellor Angela Merkel. Its task is to push on the National Development Plan for Electric Mobility (\"Nationaler Entwicklungsplan Elektromobilität\"). The goal for 2020 of the NPE is to develop Germany to the leading supplier and lead market for electric mobility and to gain employment in the country.\n\n, the country had 4,800 public charging stations. Several pilot projects have been implemented based on partnerships of carmakers and utility companies.\n\nDaimler AG and utility RWE AG run a joint electric car and charging station test project in the German capital, Berlin, called \"E-Mobility Berlin.\" They have set up 60 charging stations in Berlin (September 2009) and planned to expand the system to include 500 charging stations. Daimler has provided for 100 Smart electric drive cars to the project. The second phase started in November 2010. The RWE subsidiary \"RWE Mobility\" created cooperations with the automobilist club ADAC, car rental service Sixt and car park provider APCOA to equip all locations with charging stations. since mid of 2009. Renault joined the RWE Mobility program in September 2009 whereby the project goals of erecting charging stations were enlarged to mid of 2011 Renault's partner Nissan has joined the RWE-mobility program in June 2010 announcing that RWE will create a network of 1,000 charging stations until the end of the year 2010 focusing on the Berlin and Rhein-Ruhr region. In August 2010 a cooperation with fuel retailer PKN Orlen was announced – they planned to equip 30 gas stations in Hamburg with charging points for electric vehicles.\n\nCarmaker BMW and utility Vattenfall run a joint electric car and charging test project with Mini E electric cars.\nA total of 100 trial vehicles were assigned. Testing in Berlin began in June 2009, and for the second phase, a total of 70 vehicles were delivered in March 2011 to private customers and fleet users. Field testing began in Munich in September 2010, for a leasing fee of (approx. ) per month. Up to June 2011 there were 42 public charge points by Vattenfall in Berlin and the company is in the process of building 50 public charge points in Hamburg.\n\nCarmaker VW and utility E.ON run a joint electric car and charging station test project in the German capital, Berlin and in Wolfsburg. The \"Electric Mobility Fleet Test\" was started as a research project with mostly partners in German universities using the VW hybrid cars (to be tested in 2010). E.ON has later joined also in the MINI E project providing the infrastructure in Munich which was started in July 2009. erecting an initial series of 11 charging stations (May 2010) enlarging it continuously (21 locations in December 2010). The region test in Munich has been extended with BMW i prototypes (BMW i3 and BMW i8) as well as Audi e-tron models (project eflott) in 2011. E.ON has announced to provide the eflott project with 200 public charging stations the Munich region.\n\nCarmaker Daimler, the utility EnBW and the government of Baden-Württemberg announced in June 2010 to expand the \"Landesinitiative Elektromobilität\" program with the \"e-mobility Baden-Württemberg\" project that includes erecting 700 charging stations in the state until the end of 2011. Additionally there will be 200 electric vehicles added to the test including some electric trucks. The government of Baden-Württemberg has assigned to support EV research up to 2014. Meanwhile, EnBW has sponsored 500 E-Bikes in the Elektronauten project in 2010 which can use 13 charging stations in the Stuttgart region. EnBW has claimed to offer 250 charging stations for the Elektronauten 500 project in May 2011 although the map has not been updated. Bosch has developed a new charging station type for EnBW that is capable for 63A – the station was certified on 11. April 2011 by DEKRA and EnBW has announced to install 260 charge stations in the following weeks for MeRegioMobil project in Stuttgart and Karlsruhe. In November 2011 the Car2Go carsharing service announced plans to operate in Stuttgart in 2012 – EnBW reassured to have 500 charging spots ready in time with the roll out of the Car2Go vehicles in the second half of 2012.\n\n, a total of 129,246 plug-in electric cars have been registered in Germany since 2010. The country is the largest passenger car market in Europe, however, , ranked as the eighth largest plug-in market in the world and the fifth largest in Europe. About 80% of the new plug-in cars registered in the country through 30 September 2016 were registered since January 2014, with 13,049 units registered in 2014, 23,464 registered in 2015, and 17,074 during the first nine months of 2016. The official German definition of electric vehicles changed at the beginning of 2013, before that, official statistics only registered all-electric vehicles because plug-in hybrids were accounted together with conventional hybrids. As a result, the registrations figures for 2012 and older do not account for total new plug-in electric car registrations.\n\nThe fleet of electric car registered in the country increased from 1,558 units in 2009 to 2,307 in 2010. The electric car stock in 2011 increased 96.8% from 2010 to 4,541 units registered, and up 56.7% from 2011 to 7,114 units in 2012, reaching 12,156 registered cars on 1 January 2014. At the beginning of 2014 registrations of plug-in electric vehicles represented a 0.028% market share of all passenger vehicles registered in Germany. Most of the plug-in stock in the country was registered by corporate buyers. The plug-in hybrid segment in the German market in 2014 experienced an explosive growth of 226.9% year-over-year, and the overall plug-in segment increased 75.5% from a year earlier. The surge in sales continued in 2015, the plug-in hybrid segment grew 125.1% year-over-year, while the all-electric segment climbed 91.2% from the previous year.\n\nDuring 2011, a total of 2,154 pure electric cars were registered in the country, up from 541 units in 2010. All-electric car sales for 2011 were led by the Mitsubishi i-MiEV family with 683 i-MiEVs, 208 Peugeot iOns and 200 Citroën C-Zeros, representing 50.6% of all electric car registrations in 2011. Plug-in hybrid registrations totaled 266 units in 2011, 241 Opel Amperas and 25 Chevrolet Volts, for a total of 2,420 plug-in electric vehicles registered in 2011.\n\nA total of 2,956 all-electric vehicles were registered in Germany during 2012, a 37.2% increase over 2011. When 901 registered plug-in hybrids are accounted for, 2012 registrations climb to 3,857 units, and sales of plug-in electric car represented a 0.12% market share of new passenger vehicles sold in the country in 2012. Most sales in the country were made by corporate and fleet customers and 1,493 all-electric vehicles were registered by the automobile industry, as demonstration or research vehicles. Registrations of plug-in electric-drive vehicles were led by the Opel Ampera extended-range electric car with 828 units, followed by the Smart electric drive with 734 units. In addition, a total of 2,413 Renault Twizys were sold during 2012, making Germany the top selling European market for the electric quadricycle.\n\nA total of 7,436 new plug-in electric cars were registered in Germany in 2013, consisting of 6,051 all-electric cars and 1,385 plug-in hybrids. Total registrations at the end of 2013 reached 12,156 units. The market share of plug-in electric passenger cars increased to 0.25% in 2013 from 0.12% in 2012. The Smart electric drive led new plug-in car registrations in 2013 with 2,146 units, followed by Renault Zoe with 1,019, the Nissan Leaf with 855 units, and the BMW i3 with 559. \n\nRegistrations of plug-in electric cars totaled 13,049 units in 2014, consisting of 8,522 all-electric cars and 4,527 plug-in hybrids. The plug-in segment achieved a market share of 0.4% of new car sales that year. The BMW i3 ended 2014 as the top selling plug-in electric car with 2,233 units registered, followed by the Smart Fortwo ED with 1,589, and the Renault Zoe with 1,498. Accounting for registrations of plug-in electric cars between January 2010 and June 2014, the leading model was the Smart electric drive with 3,959 units, with a significant number in use by carsharing services, followed by the BMW i3 with 1,937 units, Nissan Leaf with 1,693 units, Renault Zoe with 1,532, and Opel Ampera with 1,450 units.\n\nPlug-in hybrid registrations totaled 11,101 units in 2015, up 145% from 2014, and all-electric cars totaled 12,363 units registered, up 45% from 2014. Combined sales of the two segments totaled 23,464 units. The plug-in segment achieved a market share of 0.7% of new car sales that year, up from 0.4% in 2014. Registrations totaled 3,176 plug-in cars in December 2015, achieving both, the highest monthly sales volume ever and a record market share of 1.28% of new car registrations that month. The top selling models in 2015 were the Kia Soul EV with 3,839 units, followed by the BMW i3 with 2,271, the Mitsubishi Outlander P-HEV with 2,128, the Volkswagen Golf GTE with 2,109 and the Audi A3 e-tron with 1,839.\n\nThe magazine \"Der Spiegel\" questioned whether the Kia Soul EV was actually the top selling plug-in electric car in the country, as about 2,000 electric cars were registered in Germany and then imported to Norway as used cars, as part of a strategy of the Hyundai-Kia Group to comply with European Union regulations. (see Controversies section above). There were about 50,000 plug-in electric cars registered in Germany by the end of 2015.\n\nDuring the first three quarters of 2016, sales of plug-in hybrids surpassed sales of all-electric cars for the first time in the country with a total of 17,074 units were registered. The introduction of the purchase bonus did not produce immediate effect on plug-in car sales until September 2016, when registrations peaked to 3,061 units. Combined registrations of both type of plug-in accounted for 1.1% of new car registrations, allowing the German plug-in market share to pass the 1% mark for the first time during 2016. \n\nA total of 25,254 plug-in cars were registered in 2016 consisting of 13,744 plug-in hybrids and 11,410 all-electric cars, representing a market share of 0.72% of new car registrations that year. The top selling models in 2016 were the BMW i3 (2,863), Renault Zoe (2,805), Audi A3 e-tron (1,615), Tesla Model S (1,474), and Mitsubishi Outlander P-HEV (1,436).\n\nA record of 54,492 plug-in cars were registered in 2017, up 217% the previous year, and consisting of 29,436 plug-in hybrids and 25,056 all-electric cars. The top selling models in 2017 were the Audi A3 e-tron (4,454), Renault Zoe (4,322), and BMW i3 (4,319). Registrations achieved a record market share of 1.58% in 2017.\n\nThe following table presents registrations of the top selling highway-capable plug-in electric cars available for retail customers by year between 2010 and June 2014.\n\n\n"}
{"id": "7817455", "url": "https://en.wikipedia.org/wiki?curid=7817455", "title": "Pre-charge", "text": "Pre-charge\n\nPre-charge of the powerline voltages in a high voltage DC application is a preliminary mode which limits the inrush current during the power up procedure. \n\nA high-voltage system with a large capacitive load can be exposed to high electric current during initial turn-on. This current, if not limited, can cause considerable stress or damage to the system components. In some applications, the occasion to activate the system is a rare occurrence, such as in commercial utility power distribution. In other systems such as vehicle applications, pre-charge will occur with each use of the system, multiple times per day. Precharging is implemented to increase the lifespan of electronic components and increase reliability of the high voltage system.\n\nInrush currents into capacitive components are a key concern in power-up stress to components. When DC input power is applied to a capacitive load, the step response of the voltage input will cause the input capacitor to charge. The capacitor charging starts with an inrush current and ends with an exponential decay down to the steady state condition. When the magnitude of the inrush peak is very large compared to the maximum rating of the components, then component stress is to be expected. \nThe current into a capacitor is known to be formula_1: the peak inrush current will depend upon the capacitance C and the rate of change of the voltage (dV/dT). The inrush current will increase as the capacitance value increases, and the inrush current will increase as the voltage of the power source increases. This second parameter is of primary concern in high voltage power distribution systems. By their nature, high voltage power sources will deliver high voltage into the distribution system. Capacitive loads will then be subject to high inrush currents upon power-up. The stress to the components must be understood and minimized.\n\nThe objective of a pre-charge function is to limit the magnitude of the inrush current into capacitive loads during power-up. This may take several seconds depending on the system. In general, higher voltage systems benefit from longer pre-charge times during power-up.\n\nConsider an example where a high voltage source powers up a typical electronics control unit which has an internal power supply with 11000 µF input capacitance. When powered from a 28 V source, the inrush current into the electronics unit would approach 31 amperes in 10 milliseconds. If that same circuit is activated by a 610 V source, then the inrush current would approach 670 A in 10 milliseconds. It is wise not to allow unlimited inrush currents from high voltage power distribution system activation into capacitive loads: instead the inrush current should be controlled to avoid power-up stress to components.\n\nThe functional requirement of the high voltage pre-charge circuit is to minimize the peak current out from the power source by slowing down the \"dV\"/\"dT\" of the input power voltage such that a new “pre-charge mode” is created. Of course the inductive loads on the distribution system must be switched off during the precharge mode. While pre-charging, the system voltage will rise slowly and controllably with power-up current never exceeding the maximum allowed. As the circuit voltage approaches near steady state, then the pre-charge function is complete. Normal operation of a pre-charge circuit is to terminate pre-charge mode when the circuit voltage is 90% or 95% of the operating voltage. Upon completion of pre-charging, the pre-charge resistance is switched out of the power supply circuit and returns to a low impedance power source for normal mode. The high voltage loads are then powered up sequentially.\n\nThe simplest inrush-current limiting system, used in many consumer electronics devices, is a NTC resistor. When cold, its high resistance allows a small current to pre-charge the reservoir capacitor. After it warms up, its low resistance more efficiently passes the working current.\n\nMany active power factor correction systems also include soft start.\n\nIf the example circuit from before is used with a pre-charge circuit which limits the \"dV\"/\"dT\" to less than 600 volts per second, then the inrush current will be reduced from 670 amperes to 7 amperes. This is a “kinder and gentler” way to activate a high voltage DC power distribution system.\n\nThe primary benefit of avoiding component stress during power-up is to realize a long system operating life due to reliable and long lasting components. \n\nThere are additional benefits: pre-charging reduces the electrical hazards which may occur when the system integrity is compromised due to hardware damage or failure. Activating the high voltage DC system into a short circuit or a ground fault or into unsuspecting personnel and their equipment can have undesired effects. Arc flash will be minimized if a pre-charge function slows down the activation time of a high voltage power-up. A slow pre-charge will also reduce the voltage into a faulty circuit which builds up while the system diagnostics come on-line. This allows a diagnostic shut down before the fault is fully realized in worst case proportions.\n\nIn cases where unlimited inrush current is large enough to trip the source circuit breaker, a slow precharge may even be required to avoid the nuisance trip.\n\nPre-charging is commonly used in battery electric vehicle applications. The current to the motor is regulated by a \"controller\" that employs large capacitors in its input circuit. Such systems typically have \"contactors\" (a high-current relay) to disable the system during inactive periods and to act as an emergency disconnect should the motor current regulator fail in an active state. Without pre-charge the high voltage across the contactors and inrush current can cause a brief arc which will cause pitting of the contacts. Pre-charging the controller input capacitors (typically to 90 to 95 percent of applied battery voltage) eliminates the pitting problem. The current to maintain the charge is so low that some systems apply the pre-charge at all times other than when charging batteries, while more complex systems apply pre-charge as part of the starting sequence and will defer main contactor closure until the pre-charge voltage level is detected as sufficiently high.\n\n"}
{"id": "23501853", "url": "https://en.wikipedia.org/wiki?curid=23501853", "title": "Richard Hardman", "text": "Richard Hardman\n\nRichard Frederick Paynter Hardman, CBE (born 1936) is a British geologist and the doyen of applied petroleum geologists. He remained the President of the Geological Society of London, 1996 - 1998 and Chairman of the Petroleum Society of Great Britain. In a career spanning over 40 years, he has worked in oil and gas exploration as a geologist in Libya, Kuwait, Colombia, Norway, and the North Sea with companies like BP, Amoco and Amerada Hess. Working as an Oil & Gas Drilling & Exploration consultant, he has remained Executive Director, Exploration at Regal Petroleum (2005–2006), Atlantic Petroleum UK, and has been Director FX Energy, Inc., based in Salt Lake City, US, since 2003.\n\nHe was awarded the Commander of the Order of the British Empire (CBE) in New Year Honours List of 1998 for services to the oil industry and the William Smith Medal by the Geological Society in 2003.\n\nStarting his career with British Petroleum (BP), where after working for ten years, he joined Amoco in 1969, where he became known for North Sea oil and gas exploration. He worked with the company for next 11 years, before joining Superior Oil for 3 years and finally 18 years at Amerada Hess to become its Vice President, Exploration till 2001.\n\nHe is now an oil and gas exploration consultant and since 2001 as a senior consultant to various oil and gas companies including Enterprise Oil, Neptune, FX Energy Inc and Atlantic Petroleum UK Limited and Executive Director, Exploration at Regal Petroleum (2005–06).\n\nHe is the former Chairman of both the Petroleum Group of the Geological Society and the Petroleum Exploration Society of Great Britain and the former President of the Geological Society. He was formerly chairman of the Science and Innovation Strategy Board of the Natural Environment Research Council, and also a trustee of the UK based education charity Oil Depletion Analysis Centre established in 2001.\n"}
{"id": "8330816", "url": "https://en.wikipedia.org/wiki?curid=8330816", "title": "Short rotation forestry", "text": "Short rotation forestry\n\nShort Rotation Forestry (SRF) is grown as an energy crop for use in power stations, alone or in combination with other fuels such as coal. It is similar to historic fuelwood coppice systems.\n\nSRF is the practice of cultivating fast-growing trees that reach their economically optimum size between eight and 20 years old. Species used are selected on this basis and include Alder, Ash, Southern Beech, Birch, Eucalyptus, Poplar, Willow, new varieties of paulownia elongata, Paper mulberry, Australian Blackwood and Sycamore.\n\nTrees are planted at widths that allow for quick growth and easy harvesting. They are usually felled when they are around 15 cm wide at chest height, this takes from 8 to 20 years. This compares with 60 years or more for standard forestry crops. When felled, SRF trees are replaced by new planting or, more usually, allowed to regenerate from the stumps as coppice. The wood chip produced by SRF is preferred in the power industry as it does not contain bark and wood and is therefore more homogenous than wood chip provided by short rotation coppice. The profit after transport is estimated to be around €15 to €30 per tonne.\n\nDuring growth SRF will offer significant carbon sequestration. The main carbon cost is associated with haulage of the harvested trees.\n\nSome species such as Eucalyptus have a high water usage, this is especially important given the changing water patterns due to climate change. There are also potential impacts on biodiversity and the effects of large scale SRF on flora and fauna are not known.\n\n\n"}
{"id": "23592835", "url": "https://en.wikipedia.org/wiki?curid=23592835", "title": "Soap substitute", "text": "Soap substitute\n\nA soap substitute refers to detergents or cleansing creams, other than soap, for cleaning the skin, especially removing greasy films or glandular exudates. Soap contains a increasingly high amount of synthetic elements than actual ingredients intended to cleanse the skin. The use of everyday cleansing soap has correlated to several cases of topical skin problems like acne and eczema. Making the switch to soap alternatives welcomes the replacement of chemical-ridden body wash for eco-friendly products. Soap substitutes can be made from a variety of sources including plants with high saponin levels. Soap substitutes should not be confused with natural cleaning products which are cleaning agents for kitchen and house use. Plants with high saponin levels are purported to contain saponins in sufficient quantities to produce lather (when mashed plant parts are beaten in water) and can be used either as is or in soap or shampoos.\n\n\n\n"}
{"id": "4721838", "url": "https://en.wikipedia.org/wiki?curid=4721838", "title": "Steam clock", "text": "Steam clock\n\nA steam clock is a clock which is fully or partially powered by a steam engine. Only a few functioning steam clocks exist, most designed and built by Canadian horologist Raymond Saunders for display in urban public spaces. Steam clocks built by Saunders are located in Otaru, Japan; Indianapolis, United States; and the Canadian cities of Vancouver, Whistler and Port Coquitlam, all in British Columbia. Steam clocks by other makers are installed in St Helier, Jersey and at the Chelsea Farmers' Market in London, England.\n\nAlthough they are often styled to appear as 19th-century antiques, steam clocks are a more recent phenomenon inspired by the Gastown steam clock built by Saunders in 1977. One exception is the steam clock built in the 19th century by Birmingham engineer John Inshaw to demonstrate the versatility of steam power.\n\nIn 1859, the engineer and businessman John Inshaw took over the public house on the corner of Morville Street and Sherborne Street in Ladywood, Birmingham, UK. In a bid to make the establishment a talking point in the area, as well as furnishing it with various working models, Inshaw applied his interest in steam power to construct a steam-powered clock as a feature. A small boiler made steam; the steam condensed into droplets of water that fell on a plate at regular intervals, and the plate then drove the mechanism. The clock was installed above the door, and the pub became known as the Steam Clock Tavern. The establishment was sufficiently successful that it became a music hall in the 1880s.\n\nRaymond Saunders' first steam clock was built in 1977 at the corner of Cambie and Water streets in Vancouver's Gastown neighbourhood, . Although the clock is now owned by the City of Vancouver, funding for the project, estimated to be about $C58,000, was provided by contributions from local merchants, property owners, and private donors. Incorporating a steam engine and electric motors, the clock displays the time on four faces and announces the quarter hours with a whistle chime that plays the Westminster Quarters. The clock produces a puff of steam from its top on the hour. The clock is featured on the cover for the 2011 Nickelback album \"Here and Now\".\n\nThe steam engine that originally ran the clock is a Stuart #4 single expansion double acting 1\" piston engine. This engine is still visible through the glass sides of the clock. However, owing to the clock's high noise levels and inability to keep accurate time, since 1986 the clock has been powered by an electric motor that was originally intended solely as a back-up system.\n\nThe 17–foot–tall Indiana State Museum steam clock in Indianapolis, Indiana is located on the sidewalk on the north side of the museum near the canal. It has four 24\" diameter dials that are back-lit by neon. The clock’s eight brass whistles play a few notes of \"Back Home Again in Indiana\" every 15 minutes. A more complete rendition is played at the top of every hour.\n\nThe towering and quirky steam clock located at the Chelsea Farmers' Market was constructed in 1984. Although still standing, the clock is no longer in operation.\n\nThe Jersey steam clock is a full-scale replica of the centre section of a paddle steamboat named the \"Ariadne\". The clock was commissioned by the Jersey Waterfront Board in 1996, and built by Smith of Derby Group. Although once powered by steam, according to a Jersey government document \"the steam workings have been replaced with electrical fittings designed to provide the same functionality including the blowing of 'steam' at the appropriate times of the day.\"\n\nThe clock is sited on the North Quay of the harbour at St Helier, Jersey, and also incorporates a fountain. It is listed in the Guinness Book of Records as the World's largest steam clock.\n\nLocated at Berwick, Australia, . Designed and built by Peter Weare at his own expense, it is a half scale prototype for a larger clock proposed for Melbourne Australia. The clock was dismantled in December 2010 due to vandalism. In December 2011, the City Council agreed to find a new site for the clock.\n\n\n"}
{"id": "44739880", "url": "https://en.wikipedia.org/wiki?curid=44739880", "title": "Tarfaya Wind Farm", "text": "Tarfaya Wind Farm\n\nTarfaya Wind Farm is a wind farm in Morocco, located in 20 km from Tarfaya. It was developed by Tarec (Trarfaya Energy Company), a 50/50 joint venture of Nareva Holding and International Power Ltd. Tarfaya Wind Farm is owned and operated by a 50:50 joint venture between the GDF SUEZ and Nareva Holding and it is the Africa's largest capacity wind farms with 131 wind turbines, each generating 2.5 Megawatts of power, and a total installed capacity of 301 MW. It was on the list of ten “Most Outstanding African Projects in 2015”, a ranking by Jeune Afrique magazine.\nThe park was commissioned in December 2014 after two years of work and investment of 5 billion dirhams. Its constructor and operator is Tarec, which sells the power generated to the National Electricity Office.\n\n"}
{"id": "39618697", "url": "https://en.wikipedia.org/wiki?curid=39618697", "title": "The Doomsday Machine (book)", "text": "The Doomsday Machine (book)\n\nThe Doomsday Machine: The High Price of Nuclear Energy, the World's Most Dangerous Fuel is a 2012 book by Martin Cohen and Andrew McKillop which addresses a broad range of concerns regarding the nuclear industry, the economics and environmental aspects of nuclear energy, nuclear power plants, and nuclear accidents. The book has been described by \"The New York Times\" as \"a polemic on the evils of splitting the atom\".\n\n\"The usual rule of thumb for nuclear power is that about two thirds of the generation cost is accounted for by fixed costs, the main ones being the cost of paying interest on the loans and repaying the capital...\"\nAreva, the French nuclear plant operator, for example, offers that 70 percent of the cost of a kWh of nuclear electricity is accounted for by the fixed costs from the construction process. In the foreword to the book, Steve Thomas, Professor of Energy Studies at the University of Greenwich in the UK, states that \"the economic realities of rapidly escalating costs and insurmountable financing problems... will mean that the much-hyped nuclear renaissance will one day be remembered as just another 'nuclear myth'.\"\n\nIn discussions about the economics of nuclear power, the authors explain, what is often not appreciated is that the cost of equity, that is, companies using their own funds to pay for new plants, is generally higher than the cost of debt. Another advantage of borrowing may be that \"once large loans have been arranged at low interest rates—perhaps with government support—the money can then be lent out at higher rates of return\".\n\nAs Matthew Wald in the \"New York Times\" noted, despite being at its heart environmentalist, the book challenges certain Green orthodoxies, notably the idea that whatever the risks of nuclear energy, the threat from man-made climate change is greater.\n\nAs Chiara Proietti Silvestri wrote in a review for the Italian Energy journal \"Energia\" in the \"Doomsday Machine\", the authors argue that the fight against pollution from CO2 generated mostly by the human activities and described as be the primary cause of rising temperatures at the global level thus becomes a \"simple story\" told by a club dominated from Anglophone countries in an attempt to defend and promote particular national interests. Reducing huge national subsidies to domestic coal industries and promoting the lucrative market for nuclear power stations being the case in point.\n\nThe book describes how fossil fuels remain the main source of energy for the world, while nuclear power manages to meet only three percent of the needs of world energy. Which leads to the question: why is the figure so low when it is often read that nuclear is a key part of the 'world energy mix'? The authors explain that the trick is in the \"fiddling\" of statistics, writing that the \"key point about world energy is that it is almost all thermal. Whether it is created by burning coal, oil or gas, or firewood or dung, or even running nuclear power plants, the first thing produced is heat\".\n\nAlthough nuclear power is still today presented as the energy of future (\"the first myth\" ), its roots are paradoxically in the speech \"Atoms for Peace\", by President Eisenhower. The authors state that this history shows that the origins of nuclear power lie with military needs, and are anything but peaceful, and that Eisenhower's speech itself was an attempt to distract the world from the tests of the US hydrogen bomb. Scientists — from Albert Einstein and Georges Lemaître to Enrico Fermi and Robert Oppenheimer — come in for criticism for serving the same military strategies.\n\nThe history of nuclear power is also marked by the slogan — \"too cheap to meter\" — described as the foundation of another myth. The book argues that nuclear power is not and has never been cheap but rather found the resources to tap public subsidies, special systems of taxation, loans government and other beneficial guarantees. The arrival of the liberalization of the electrical market has had a strong impact on the nuclear industry, revealing the true costs (e.g. the over-run in costs on the new EPR at Olkiluoto in Finland).\n\nThis leads to \"tricks\" to manipulate figures (cost projections of construction, decommissioning and insurance schemes), the extension of the life of the reactors, the reuse of the depleted fuel) in order to conceal the fundamental non-affordability. Today, according to the authors, the nuclear lobby triumphantly describes the new order flow, especially in developing countries, where the \"environment tends to remain a 'free good'\", and there is a \"cultural indifference to public hazard and risk\" all of which, they argue, raises new concerns about the environmental protection, about technical expertise and political instability.\n\nA central theme of the book is the issue of the true, economic, cost of nuclear electricity. The preface by professor Steve Thomas indicates the information density with which the authors construct their arguments, expressed nonetheless in witty and tight language, as the Italian \"Energia\" review put it, while according to Kirukus: \"The authors deliver a convincing account of the partnership between industry and government (essential because nuclear plants require massive subsidies) to build wildly expensive generators whose electricity remains uncompetitive without more subsidies.\" In another review, science policy writer Jon Turney stated that the \"strongest suit\" of the book was \"energy economics and supply data\".\n\n\"New York Times\"' Matthew L. Wald analyzes the argument put forth in the \"Doomsday Machine\" that \"even if global warming science was not explicitly invented by the nuclear lobby, the science could hardly suit the lobby better\". He comments, \"In fact, the [nuclear] industry continues to argue that in the United States it is by far the largest source of zero-carbon energy, and recently began a campaign of upbeat ads to improve its image.\" Finding the claim that \"In almost every country — usually for reasons completely unrelated to its ability to deliver electricity — there is almost universal political support for nuclear power\" is \"probably an exaggeration\" in the case of Japan post-Fukushima and Germany, Wald agrees that \"two countries with enormous demand for electricity and not much hand-wringing over global warming, are planning huge reactor construction projects\". Wald notes that, even in Japan, the \"catastrophe plays in some quarters as a reason to build new reactors\".\n\nNew Scientist's Fred Pearce panned the book, calling it \"mendacious and frequently anti-scientific\", remarking that it \"combines hysterical opposition to all things nuclear with an equally deranged climate-change denialism\".\n\n\n"}
{"id": "55788372", "url": "https://en.wikipedia.org/wiki?curid=55788372", "title": "The Elephant's Foot", "text": "The Elephant's Foot\n\nThe Elephant’s Foot is an extremely radioactive mass of corium formed during the Chernobyl disaster in April 1986, and was discovered in December 1986. It is named for its wrinkly appearance, resembling the foot of an elephant. The corium mass is beneath Reactor N. 4 of the Chernobyl Nuclear Power Plant, in under-reactor room 217.\n\nThe Elephant's Foot is a large mass of black corium with many layers, externally resembling tree bark and glass. The mass is quite dense, unyielding to a drill but able to be damaged by a Kalashnikov rifle. By June 1998, the outer layers of the mass began to turn to dust and the entire mass was starting to crack. The mass has penetrated through at least of concrete.\n\nThe radiation level near the mass was approximately 8,000 roentgens per hour, an invariably lethal dosage of radiation.\n\nIt is composed chiefly of silicon dioxide, with traces of uranium. The mass is largely homogeneous, though the depolymerized silicate glass contains occasional crystalline grains of zircon. These grains of zircon are not elongated, suggesting a moderate crystallization rate. While uranium dioxide dendrites grew quickly at high temperatures within the lava, the zircon began crystallization during slow cooling of the lava. Despite the distribution of uranium bearing particles not being uniform, the radioactivity of the mass is evenly distributed.\n"}
{"id": "4211535", "url": "https://en.wikipedia.org/wiki?curid=4211535", "title": "UK Dark Matter Collaboration", "text": "UK Dark Matter Collaboration\n\nThe UK Dark Matter Collaboration (UKDMC) (1987–2007) was an experiment to search for Weakly interacting massive particles (WIMPs). The consortium consisted of astrophysicists and particle physicists from the United Kingdom, who conducted experiments with the ultimate goal of detecting rare scattering events which would occur if galactic dark matter consists largely of a new heavy neutral particle. Detectors were set up underground in a halite seam at the Boulby Mine in North Yorkshire.\n\nWIMPs are considered prime candidates for dark matter, which accounts for approximately nine-tenths of the mass of certain galaxies, such as the Milky Way. WIMPs are predicted by several supersymmetric theories of particle physics. The particle detectors used for this experiment are placed 1100 metres below the surface of Yorkshire's Boulby mine.\n\nUKDMC began in 1987, with principal participants from several notable institutions, including the Imperial College of Science, Technology and Medicine, the CCLRC's Rutherford Appleton Laboratory, and the University of Sheffield. Funding for the programme was provided by the Particle Physics and Astronomy Research Council (PPARC), as well as Cleveland Potash Ltd. which operates the mine where the experiments were conducted. The underground laboratory was officially opened on 18 April 2003, and the experiment ran until 2007 when collaborating institutions and scientists moved on to the related projects ZEPLIN-III and DRIFT-II.\n\nUKDMC operated multiple dark matter detectors and developed techniques for WIMP searches in crystals and xenon.\n\nIn 1996 they published limits that were obtained using room temperature crystals. NAIAD was an array of NaI(Tl) crystals that ran 2001-2003, collecting 44.9 kg×years of exposure, setting spin-independent and spin-dependent limits on WIMPs. Then the ZEPLIN series of searches were done.\n\n"}
{"id": "13751782", "url": "https://en.wikipedia.org/wiki?curid=13751782", "title": "Villagetech solutions", "text": "Villagetech solutions\n\nVillageTech Solutions \nbegan with EcoSystems (Nepal) in 1996, to improve living standards for rural people by creating affordable energy and transport products. VTS creates inexpensive technology by focusing creative talent on problems ignored by commercial investors because the solutions are intentionally easily copied, and the markets are distorted by conflicting cultures, governments, subsidies and real conflict.\n\nThe VTS mission is to apply simple, locally appropriate technology to problems in education, transportation, health and economic development.\n\nVTS is the American non-profit offshoot of EcoSystems Pvt Ltd in Nepal, founded by David and Haydi Sowerwine in 1996 to provide ‘energy and transport solutions’ in Nepal where they lived for 14 years. In that time EcoSystems built 38 “WireBridges” across Himalayan rivers to connect villagers with medical care, schools and trade. Since 1998 the bridges have moved an estimated 3.5 million passengers without harm.\n\nIn 2009 the Sowerwines launched a successor wirebridge builder in Kathmandu, the locally owned and operated: VillageSolutions Pvt. Ltd.\n\nVTS won the esteemed Tech Museum Award in 2003.\n\nVillageTech Solutions is developing Looma for use in rural schools in Nepal and elsewhere.\n\nLooma is an affordable, battery-powered audiovisual\ndevice that brings the Internet and enhanced learning\nmedia tools to village schools that have never seen electricity,\ncomputers, or in some cases, even books.\n\nLooma is a small box that contains a computer, 300 lumen\nprojector, interactive whiteboard, and audio system, all in one.\nLooma projects an image onto a facing wall, and a simple user\ninterface allows teachers who are unaccustomed to computers\nto navigate through screens intuitively using a handheld\nwand. It comes pre-loaded with government textbooks for the\ncountry of use and a rich set of multimedia enhancements.\nLooma’s web browser allows the class to explore the Internet.\n\nThe Looma prototype is being field tested in Nepal. Enhancements such as webcam, microphone,\nand dual-wand control are just a few features planned for future versions.\n\nIn 2008 David challenged a Dartmouth engineering student team to take on a social change project. The project was a system for removing arsenic from groundwater in a geology that runs from Nepal to Bangladesh. In April 2009, the team demonstrated their prototype, inexpensive household-scale system that cuts arsenic concentrations to below the World Health Organization standard of 10 ppb and in a region where villagers’ well water often ranges beyond 200 ppb.\n\nConfirming the merit of their discovery, the sponsors of the Collegiate Inventors Competition (US Patent Office, Inventors Hall of Fame, and Abbott Foundation) awarded the top prize for USA undergraduate innovators to the Thayer School of Engineering team in October 2009.\n\nThe arsenic removal system is called SafaPani.\n\nVTS supports the building of WireBridges in developing countries.\n\nThe WireBridge is a low-cost monorail-like aerial ‘roadway'. This aerial roadway offers all-weather transport, requires little maintenance, poses no threat to children, uses no fossil fuel, provides low-skill jobs, and costs less than a good conventional bridge. The WireBridge is adapted from a business “best practice\" transport system developed by the global banana industry.\n\nAt each end of a bridge stands a tall steel post. The ends of the WireBridge are at exactly the same height. A carriage, holding up to five or six people plus goods, hangs from wheels which roll on the wires suspended between the posts. Passengers and bystanders pull the carriage with a rope.\n\nBuilding a bridge first requires a site survey that includes terrain, traffic levels, other crossing options, supply of local labor and maintenance. In addition, VTS requires that there be significant and demonstrable benefits to local villages in the categories of health, education, commerce, and social integration.\n\nVillageTech Solutions works extensively with student engineering teams from universities around the world. \n\n"}
{"id": "2528589", "url": "https://en.wikipedia.org/wiki?curid=2528589", "title": "Volatility (chemistry)", "text": "Volatility (chemistry)\n\nIn chemistry and physics, volatility is quantified by the tendency of a substance to vaporize. Volatility is directly related to a substance's vapor pressure. At a given temperature, a substance with higher vapor pressure vaporizes more readily than a substance with a lower vapor pressure.\n\nThe term is primarily written to be applied to liquids; however, it may be used to describe the process of sublimation which is associated with solid substances, such as dry ice (solid carbon dioxide) and osmium tetroxide (OsO), which can change directly from the solid state to a vapor, without becoming liquid.\n\nThe vapor pressure of a substance is the pressure at which its gas phase is in equilibrium with its condensed phases (liquid or solid). It is a measure of the tendency of molecules and atoms to escape from a liquid or a solid. A liquid's atmospheric pressure boiling point corresponds to the temperature at which its vapor pressure is equal to the surrounding atmospheric pressure and it is often called the \"normal boiling point\".\n\nThe higher the vapor pressure of a liquid at a given temperature, the higher the volatility and the lower the normal boiling point of the liquid. The vapor pressure chart (right hand side) displays the vapor pressures dependency for a variety of liquids as a function of temperature.\n\nFor example, at any given temperature, chloromethane (methyl chloride) has the highest vapor pressure of any of the liquids in the chart. It also has the lowest normal boiling point (−24.2 °C), which is where the vapor pressure curve (the blue line) intersects the horizontal pressure line of one atmosphere (atm) of absolute vapor pressure.\n\n\n"}
{"id": "34795832", "url": "https://en.wikipedia.org/wiki?curid=34795832", "title": "Windfall (2010 film)", "text": "Windfall (2010 film)\n\nWindfall is a 2010 documentary film directed by Laura Israel about the reaction of residents in rural Meredith, New York (in Delaware County, New York) to a proposal to place numerous wind turbines in their community to harness wind power. It is notable that the film was made by an interested local citizen rather than an organization.\n\nThe film begins in 2004, when energy companies approached several property owners in Meredith, offering cash payments to allow the long-term placement of wind turbines standing over 400 feet tall on their land. The documentary portrays Meredith residents as deeply divided over the idea. Some believe the economic and energy benefits are worth investigating. Others are concerned about the towers being an eyesore, loss of property values, or posing a variety of hazards such as collapse, accumulation of ice which is then flung from the turbines in large chunks, or health problems attributed to low frequency noise. Residents of Lowville, New York are also interviewed, expressing regret at installing wind turbines in their community.\n\nAfter an often rancorous debate, the citizens vote out the current officials who were promoting wind energy, and were not amenable to enacting a protective wind ordinance. The newly elected officials in Meredith subsequently passed a citizen-friendly wind law, and the developer decided to leave the community.\n\nThe film is composed mostly of interviews with Meredith residents. Also included are excerpts of news broadcasts, films of town council meetings, and computer-animated segments.\n\nRoger Ebert gave the film 3 out of 4 stars, writing that the film \"left me disheartened. I thought wind energy was something I could believe in. This film suggests it's just another corporate flim-flam game.\" He notes that there is doubtless a legion of wind-power activists and lobbyists who would counter-argue the points made in \"Windfall\", but asks \"How many of them live on wind farms?\" In a review for the \"New York Times\", Andy Webster writes that Israel's film tends to \"overheat\" but raises important questions: \"The quest for energy independence comes with caveats. Developers’ motives must be weighed, as should the risks Americans are willing to take in their own backyard.\"\n\n\n"}
