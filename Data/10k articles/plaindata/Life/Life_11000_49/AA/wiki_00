{"id": "57189202", "url": "https://en.wikipedia.org/wiki?curid=57189202", "title": "27 Club graffiti in Tel Aviv", "text": "27 Club graffiti in Tel Aviv\n\nThe 27 Club graffiti is a mural that was painted by the Israeli graffiti crew, Kis-Lev, on the side wall of a large building in Tel Aviv, Israel in September 2014. The artwork is high by wide, and depicts seven artists from the \"27 Club\", a list of popular musicians, artists, or actors who died at the age of 27. The mural has received much criticism.\n\nThe work depicts, from left to right, Brian Jones, Jimi Hendrix, Janis Joplin, Jim Morrison, Jean-Michel Basquiat, Kurt Cobain, Amy Winehouse and an unknown figure, \"believed to be the artist, Jonathan Kis-Lev.\" That part of the painting was covered by pink paint, and \"there is some argument as to whether or not the pink paint over Kis-Lev's face was done by Kis-Lev himself or another artist. One rumor is that Kis-Lev was so disappointed in all that he hadn't accomplished by the age of 27, that he included paint to cover his face.\" The painting was reportedly made with the help of a crane and took two days to complete.\n\nKis-Lev first came up with the idea to depict the seven figures when he was 26 years old. \"I was about to turn 27 and, having read much about the 27 club, I was afraid I was to somehow join them. Some of my friends teased me that I would, being an artist and all... I promised myself that I would do a mural to commemorate these giants including my face in faded colors, and once I turn 28 to cover my face.\" \n\nKis-Lev and the artists Itai Froumin and Roman Kozhokin jointly created the mural.\n\nSince its creation in 2014 the mural has become one of the best-known street art works in Israel, and according to \"From the Grapevine Magazine\" \"the most popular in Florentin.\" It has become the focal point of many art tours in Tel Aviv. Some regard the work as vandalism. Others see the work as self-promotion for Kis-Lev and his crew, calling the work \"commercial.\"\n\nSome interpret the work to symbolize the price of fame, and see a morbid quality to it.\n\n"}
{"id": "4456263", "url": "https://en.wikipedia.org/wiki?curid=4456263", "title": "Abu Ishaq Shami", "text": "Abu Ishaq Shami\n\nAbu Ishaq Shami (died 940) was a Muslim scholar who is often regarded as the founder of the Sufi Chishti Order ( - \"Čištī\") ( - \"Shishti\"). He was the first in the Chishti lineage (\"silsila\") to live in Chisht and so to adopt the name \"Chishti\", so that, if the Chishti order itself dates back to him, it is one of the oldest recorded Sufi orders. His original name, Shami, implies he came from Syria (ash-Sham). He died in Damascus and lies buried on Mount Qasiyun, where later on also Ibn Arabi was buried.\n\nAbu Ishaq Shami's teacher was Shaikh Ilw Dinwari, whose own teacher was Abu Hubairah Basri, a disciple of Huzaifah Al-Mar'ashi who was in turn a disciple of Ibrahim ibn Adham. The Chishtiyyah \"silsila\" continued through Abu Ishaq Shami's disciple Abu Ahmad Abdal. In south Asia Moinuddin Chishti was the founding father and most revered saint of the Chishti order.\n\nSome of Abu Ishaq Shami's sayings are:\n\n\n\n"}
{"id": "14475062", "url": "https://en.wikipedia.org/wiki?curid=14475062", "title": "Agallis", "text": "Agallis\n\nAgallis (Gr. , fl. 2nd century BC) of Corcyra was a female grammarian who wrote about Homer, according to Athenaeus. Some scholars believe her to have belonged to the hetaerae class. She attributed the invention of ball games to Nausicaa, one of her countrywomen, and most later writers took her bias in this matter as self-evident. Her writings are no longer extant.\n\nSome have supposed from two passages in the \"Suda\" that we ought to read \"Anagallis\" in this passage of Athenaeus. The scholiast on Homer and Eustathius mention a grammarian of the name of Agallias, a pupil of Aristophanes the grammarian, also a Corcyraean and a commentator upon Homer, who may be the same as Agallis, or perhaps her father.\n\nFor a time in the 19th century, the name \"Agallis\" was thought to be the subject of one of the poems of Sappho. Later scholarship showed this to have been an erroneous interpretation of a corrupted text.\n"}
{"id": "46588607", "url": "https://en.wikipedia.org/wiki?curid=46588607", "title": "Agnesium", "text": "Agnesium\n\nAgnesium is a neologism coined to describe a powerful, efficient and effective organization as an entity. Agnesium is a combination of Agnes (Latin for powerful, efficient and effective) and the suffix -ium (indicating a biological structure).\n\nThe basic principle of Agnesium is that for complex structures/organizations there are many interdependencies between the members that are not known, understood or leveraged. By recognizing, understanding and leveraging these hidden interdependencies an organization can be proactive, adaptive and exploitive.\n"}
{"id": "972171", "url": "https://en.wikipedia.org/wiki?curid=972171", "title": "Alcestis (play)", "text": "Alcestis (play)\n\nAlcestis (; , \"Alkēstis\") is an Athenian tragedy by the ancient Greek playwright Euripides. It was first produced at the City Dionysia festival in 438 BC. Euripides presented it as the final part of a tetralogy of unconnected plays in the competition of tragedies, for which he won second prize; this arrangement was exceptional, as the fourth part was normally a satyr play. Its ambiguous, tragicomic tone—which may be \"cheerfully romantic\" or \"bitterly ironic\"—has earned it the label of a \"problem play.\" \"Alcestis\" is, possibly excepting the \"Rhesus\", the oldest surviving work by Euripides, although at the time of its first performance he had been producing plays for 17 years.\n\nLong before the start of the play, King Admetus was granted by the Fates the privilege of living past the allotted time of his death. The Fates were persuaded to allow this by the god Apollo (who got them drunk). This unusual bargain was struck after Apollo was exiled from Olympus for nine years and spent the time in the service of the Thessalian king, a man renowned for his hospitality who treated Apollo well. Apollo wishes to repay Admetus' hospitality and offers him freedom from death. The gift, however, comes with a price: Admetus must find someone to take his place when Death comes to claim him.\n\nThe time of Admetus' death comes and he still has not found a willing substitute. His father, Pheres, is unwilling to step in and thinks that it is ludicrous that he should be asked to give up the life he enjoys so much as part of this strange deal. Finally, Admetus' devoted wife Alcestis agrees to be taken in his place because she wishes not to leave her children fatherless or be bereft of her lover. At the start of the play, she is close to death.\n\nIn the play's prologue, the god Apollo comes out from Admetus' palace in Pherae (modern Velestino in Magnesia), dressed in white and carrying his golden bow, with the intention of leaving to avoid becoming stained by the imminent death of Alcestis, who is being comforted within. He offers an exposition of the events leading up to this moment. He hails the arrival of Thanatos (Death), who, dressed in black and carrying a sword, has come to the palace in his role as psychopomp to lead Alcestis to the underworld. Thanatos challenges Apollo's apparent defense of Alcestis and accuses him of \"twisting slippery tricks\" when he helped Admetus cheat death in the first place. Apollo reassures him and, in a passage of swift stichomythic banter, proposes a postponement of Alcestis' death, which is sarcastically rebuffed. Thanatos concludes, \"you may not have what is not yours.\" Defeated, Apollo leaves angrily, prophesying the arrival of a man (Heracles) who will wrestle Alcestis away from Death. Alone with the audience, Thanatos warns that \"this was a god of many words; but words / are not enough,\" before he summons the doors open with the tip of his sword and slowly enters the palace.\n\nThe entry of the chorus, or the \"parodos\" sequence, follows: a chorus of fifteen men of Pherae, led by a \"coryphaeus\" (chorus-leader), enter the orchestra of the theatre. The chorus-leader complains that they are in a state of suspense, ignorant of whether they ought to be performing mourning rituals for their queen. The chorus' lyrical ode, to which they dance as they sing, consists of two paired stanzas of strophe and antistrophe. They sing of the silence that greets their search for signs of mourning, the evidence of Alcestis' death. \"When goodness dies,\" they lament, \"all good men suffer, too.\" The chorus-leader concludes by dismissing the chorus' search for hope in the situation: \"The King has exhausted every ritual.\"\nThe first episode begins with a maidservant, who enters from the palace in tears. When the chorus-leader presses her for news, she gives a confusing response: \"She is alive. And dead.\" Alcestis stands, she explains, at this moment on the brink of life and death. The chorus-leader anxiously confirms that all of the customary preparations have been made for her proper burial. The maidservant joins the chorus-leader in praising Alcestis' virtue. She narrates a long description of Alcestis' prayers and preparations to die earlier that morning, when Alcestis cried over the bridal bed that will destroy her, embraced her sobbing children, and bade all farewell. She describes how Admetus held Alcestis weeping in his arms while her eyes clung to the sight of the last rays of sun she would see. The maidservant welcomes the chorus-leader to the palace and goes inside to inform Admetus of their arrival.\n\nAlcestis, on her death-bed, requests that in return for her sacrifice, Admetus never again marry, nor forget her or place a resentful stepmother in charge of their children. Admetus agrees to this and also promises to lead a life of solemnity in her honour, abstaining from the merrymaking that was an integral part of his household. Alcestis then dies.\n\nJust afterwards, Admetus' old friend Heracles arrives at the palace, having no idea of the sorrow that has befallen the place. Unwilling to turn a guest away, the king decides not to burden Heracles with the sad news and instructs the servants to make him welcome and to keep their mouths shut. By doing this, Admetus breaks his promise to Alcestis to abstain from merrymaking during the period that follows her death. Heracles gets drunk and begins to irritate the servants, who loved their queen and are bitter at not being allowed to mourn her properly. Finally, one of the servants snaps at the guest and tells him what has happened.\n\nHeracles is deeply embarrassed at his blunder and his bad behaviour and he decides to ambush and confront Death when the funerary sacrifices are made at Alcestis' tomb. When he returns, he brings with him a veiled woman whom he tells Admetus he has won in a competition. He asks his host to take her and look after her while Heracles is away on his labours. After much discussion, he finally forces a reluctant Admetus to take her by the hand, but when he lifts the veil, he finds that it appears to be Alcestis, back from the dead. Heracles has battled Death and forced him to give her up. She cannot speak for three days, after which she will be purified and fully restored to life.\n\nThe categorization of \"Alcestis\" has been a subject of debate among literary critics. It employs both tragic and comic elements, and (when first performed) occupied a slot that was generally reserved for satyr plays. Conacher explores how Euripides expanded the myth of Admetus and Alcestis, and added elements of comedy and folk tales. Beye also discusses legendary and fairy tale aspects of the play.\n\n\"Alcestis\" is also a popular text for women's studies. Critics have indicated that the play's central focus is Admetus rather than Alcestis. Segal, for example, has written of the play's patriarchal aspects. The nature of sacrifice, especially in ancient times, has been variously analysed by Rabinowitz, Vellacott, and Burnett, who explain that ancient Greek morality differed considerably from that of the present day. Modern interpretations of the play have been extremely varied, so much so that critics (such as Michelini and Gounaridou) have noted their failure to agree on much of anything. Gounaridou argues that Euripides meant for the play to be understood in many different ways. The psychologies and motivations of Admetus and Alcestis are especially disputed, with the question of Admetus's selfishness strongly contested.\n\nThe American theatre director Robert Wilson staged a production of the play in 1986 at the American Repertory Theater in Cambridge, Massachusetts and in 1987 at the Staatstheater in Stuttgart. The production supplemented Euripides' play with material drawn from a range of sources, united by their exploration of the themes of death and rebirth. It began with Heiner Müller's \"Explosion of a Memory (Description of a Picture)\" (1985) as a prologue; the piece is a dream narrative partly composed using automatic writing. Müller described it as a description of \"a landscape beyond death\" that is \"an overpainting of Euripides' \"Alcestis\" which quotes the Noh play \"Kumasaka\", the Eleventh Canto of the \"Odyssey\", and Hitchcock's \"The Birds.\"\" The production also utilised a Japanese kyogen play whose themes parodied those of \"Alcestis\", laser projections, a musical score by Laurie Anderson, and sound sculptures by composer Robert Rutman.\n\nT. S. Eliot's play \"The Cocktail Party\" is a modern adaption of \"Alcestis\"; it was premiered at the Edinburgh Festival on 22 August 1949, directed by E. Martin Browne.\n\n\n\n"}
{"id": "19974877", "url": "https://en.wikipedia.org/wiki?curid=19974877", "title": "Andrew Belton", "text": "Andrew Belton\n\nAndrew Belton (17 April 1882 – 1970) was a British Army officer and veteran of campaigns in South Africa and Morocco. He was an early exponent of the use of aircraft for military purposes, enrolling at the Chicago School of Aviation in April, 1911. He was an entrepreneur who registered a number of companies in the newly established Irish Free State.\n\nFollowing the deaths of two of his brothers during the Second Boer War, and though under age, Belton enlisted and saw service in Africa. On his return to England, he became aware of the developing military dispute in Morocco, subsequently known as the First Moroccan Crisis. Having apparently resigned his military commission, he assisted the pretender to the throne Abdelhafid, to overthrow his brother Abdelaziz. This is how he acquired the title 'Kaid', or Commander.\n\nDuring the Spanish Civil War, he was arrested in Seville having crossed the border from Gibraltar. He was instrumental in establishing the Independent Overseas Command of the Legion of Frontiersmen in Africa.\n\nIn 1924, Belton was at the centre of an investigation concerning allegations of Government corruption in the Irish Free State. He came to public attention when a letter from him to the Irish Postmaster General, J.J Walsh was revealed by Walsh. Accusations of impropriety centered around the business relationship between Belton and Deputy Darrell Figgis, and Wireless Broadcasting concessions, which led to Belton almost gaining control of the Irish Broadcasting Company. The ensuing scandal finished Figgis' political career.\n\n"}
{"id": "56127293", "url": "https://en.wikipedia.org/wiki?curid=56127293", "title": "Artificial intelligence arms race", "text": "Artificial intelligence arms race\n\nAn artificial intelligence arms race is a competition between two or more states to have its military forces equipped with the best \"artificial intelligence\" (AI). Since the mid-2010s, many analysts have argued that a such a global arms race for better artificial intelligence has already begun.\n\nRussian General Viktor Bondarev, commander-in-chief of the Russian air force, has stated that as early as February 2017, Russia has been working on AI-guided missiles that can decide to switch targets mid-flight. Reports by state-sponsored Russian media on potential military uses of AI increased in mid-2017. In May 2017, the CEO of Russia's Kronstadt Group, a defense contractor, stated that \"there already exist completely autonomous AI operation systems that provide the means for UAV clusters, when they fulfill missions autonomously, sharing tasks between them, and interact\", and that it is inevitable that \"swarms of drones\" will one day fly over combat zones. Russia has been testing several autonomous and semi-autonomous combat systems, such as Kalashnikov's \"neural net\" combat module, with a machine gun, a camera, and an AI that its makers claim can make its own targeting judgements without human intervention. In September 2017, during a National Knowledge Day address to over a million students in 16,000 Russian schools, Russian President Vladimir Putin stated \"Artificial intelligence is the future, not only for Russia but for all humankind... Whoever becomes the leader in this sphere will become the ruler of the world\".\n\nThe Russian government has strongly rejected any ban on lethal autonomous weapons systems, suggesting that such a ban could be ignored.\n\nAccording to Elsa Kania of the Center for a New American Security, \"China is no longer in a position of technological inferiority but rather sees itself as close to catching up with and overtaking the United States in AI. As such, the (Chinese military) intends to achieve an advantage through changing paradigms in warfare with military innovation, thus seizing the 'commanding heights'...of future military competition\". The close ties between Silicon Valley and China, and the open nature of the American research community, has made the West's most advanced AI technology easily available to China; in addition, Chinese industry has numerous home-grown AI accomplishments of its own, such as Baidu passing a notable Chinese-language speech recognition capability benchmark in 2015. As of 2017, Beijing's roadmap aims to create a $150 billion AI industry by 2030. Before 2013, Chinese defense procurement was mainly restricted to a few conglomerates; however, as of 2017, China often sources sensitive emerging technology such as drones and artificial intelligence from private start-up companies. One Chinese state has pledged to invest $5 billion in AI. Beijing has committed $2 billion to an AI development park. The \"Japan Times\" reported in 2018 that annual private Chinese investment in AI is under $7 billion per year. AI startups in China received nearly half of total global investment in AI startups in 2017; the Chinese filed for nearly five times as many AI patents as did Americans.\n\nChina published a position paper in 2016 questioning the adequacy of existing international law to address the eventuality of fully autonomous weapons, becoming the first permanent member of the U.N. Security Council to broach the issue.\n\nIn 2014, former Secretary of Defense Chuck Hagel posited the \"Third Offset Strategy\" that rapid advances in artificial intelligence will define the next generation of warfare. According to data science and analytics firm Govini, The U.S. Department of Defense increased investment in artificial intelligence, big data and cloud computing from $5.6 billion in 2011 to $7.4 billion in 2016. However, the civilian NSF budget for AI saw no increase in 2017.\n\nThe U.S. has many military AI combat programs, such as the \"Sea Hunter\" autonomous warship, which is designed to operate for extended periods at sea without a single crew member, and to even guide itself in and out of port. As of 2017, a temporary US Department of Defense directive requires a human operator to be kept in the loop when it comes to the taking of human life by autonomous weapons systems. \"Japan Times\" reported in 2018 that the United States private investment is around $70 billion per year.\n\nIn 2015, the UK government opposed a ban on lethal autonomous weapons, stating that \"international humanitarian law already provides sufficient regulation for this area\", but that all weapons employed by UK armed forces would be \"under human oversight and control\".\n\nIsrael's Harpy anti-radar \"fire and forget\" drone is designed to be launched by ground troops, and autonomously fly over an area to find and destroy radar that fits pre-determined criteria.\n\nThe South Korean Super aEgis II machine gun, unveiled in 2010, sees use both in South Korea and in the Middle East. It can identify, track, and destroy a moving target at a range of 4 km. While the technology can theoretically operate without human intervention, in practice safeguards are installed to require manual input. A South Korean manufacturer states, \"Our weapons don't sleep, like humans must. They can see in the dark, like humans can't. Our technology therefore plugs the gaps in human capability\", and they want to \"get to a place where our software can discern whether a target is friend, foe, civilian or military\".\n\nAccording to Siemens, worldwide military spending on robotics was 5.1 billion USD in 2010 and 7.5 billion USD in 2015.\n\nChina became a top player in artificial intelligence research in the 2010s. According to the \"Financial Times\", in 2016, for the first time, China published more AI papers than the entire European Union. When restricted to number of AI papers in the top 5% of cited papers, China overtook the United States in 2016 but lagged behind the European Union. 23% of the researchers presenting at the 2017 American Association for the Advancement of Artificial Intelligence (AAAI) conference were Chinese. Eric Schmidt, the former chairman of Alphabet, has predicted China will be the leading country in AI by 2025.\n\nAs early as 2007, scholars such as AI professor Noel Sharkey have warned of \"an emerging arms race among the hi-tech nations to develop autonomous submarines, fighter jets, battleships and tanks that can find their own targets and apply violent force without the involvement of meaningful human decisions\". As early as 2014, AI specialists such as Steve Omohundro have been warning that \"An autonomous weapons arms race is already taking place\". Miles Brundage of the University of Oxford has argued an AI arms race might be somewhat mitigated through diplomacy: \"We saw in the various historical arms races that collaboration and dialog can pay dividends\". Over a hundred experts signed an open letter in 2017 calling on the UN to address the issue of lethal autonomous weapons; however, at a November 2017 session of the UN Convention on Certain Conventional Weapons (CCW), diplomats could not agree even on how to define such weapons. The Indian ambassador and chair of the CCW stated that agreement on rules remained a distant prospect. As of 2017, twenty-two countries have called for a full ban on lethal autonomous weapons.\n\nMany experts believe attempts to completely ban killer robots are likely to fail. A 2017 report from Harvard's Belfer Center predicts that AI has the potential to be as transformative as nuclear weapons. The report further argues that \"Preventing expanded military use of AI is likely impossible\" and that \"the more modest goal of safe and effective technology management must be pursued\", such as banning the attaching of an AI dead man's switch to a nuclear arsenal. Part of the impracticality is that detecting treaty violations would be extremely difficult.\n\nA 2015 open letter calling for the ban of lethal automated weapons systems has been signed by tens of thousands of citizens, including scholars such as physicist Stephen Hawking, Tesla magnate Elon Musk, and Apple's Steve Wozniak.\n\nProfessor Noel Sharkey of the University of Sheffield has warned that autonomous weapons will inevitably fall into the hands of terrorist groups such as the Islamic State.\n\nMany Western tech companies are leery of being associated too closely with the U.S. military, for fear of losing access to China's market. Furthermore, some researchers, such as DeepMind's Demis Hassabis, are ideologically opposed to contributing to military work.\n\nFor example, Project Maven is a Pentagon project involving using machine learning and engineering talent to distinguish people and objects in drone videos, established in a memo by the U.S. Deputy Secretary of Defense on 26 April 2017. Also known as the Algorithmic Warfare Cross Functional Team, it is, according to Lt. Gen. of the United States Air Force Jack Shanahan in November 2017, a project \"designed to be that pilot project, that pathfinder, that spark that kindles the flame front of artificial intelligence across the rest of the [Defense] Department\". Its chief, U.S. Marine Corps Col. Drew Cukor, said: \"People and computers will work symbiotically to increase the ability of weapon systems to detect objects.\" At the second Defense One Tech Summit in July 2017, Cukor also said that the investment in a \"deliberate workflow process\" was funded by the Department [of Defense] through its \"rapid acquisition authorities\" for about \"the next 36 months\".\n\nIn June 2018, company sources at Google said that top executive Diane Greene told staff that the company would not follow-up Project Maven after the current contract expires in March 2019.\n\n"}
{"id": "48049", "url": "https://en.wikipedia.org/wiki?curid=48049", "title": "Autonomous robot", "text": "Autonomous robot\n\nAn autonomous robot is a robot that performs behaviors or tasks with a high degree of autonomy. Autonomous robotics is usually considered to be a subfield of artificial intelligence, robotics, and information engineering.\n\nAutonomous robots are particularly desirable in fields such as spaceflight, household maintenance (such as cleaning), waste water treatment, and delivering goods and services.\n\nSome modern factory robots are \"autonomous\" within the strict confines of their direct environment. It may not be that every degree of freedom exists in their surrounding environment, but the factory robot's workplace is challenging and can often contain chaotic, unpredicted variables. The exact orientation and position of the next object of work and (in the more advanced factories) even the type of object and the required task must be determined. This can vary unpredictably (at least from the robot's point of view).\n\nOne important area of robotics research is to enable the robot to cope with its environment whether this be on land, underwater, in the air, underground, or in space.\n\nA fully autonomous robot can:\n\nAn autonomous robot may also learn or gain new knowledge like adjusting for new methods of accomplishing its tasks or adapting to changing surroundings.\n\nLike other machines, autonomous robots still require regular maintenance.\n\nThe first requirement for complete physical autonomy is the ability for a robot to take care of itself. Many of the battery-powered robots on the market today can find and connect to a charging station, and some toys like Sony's \"Aibo\" are capable of self-docking to charge their batteries.\n\nSelf-maintenance is based on \"proprioception\", or sensing one's own internal status. In the battery charging example, the robot can tell proprioceptively that its batteries are low and it then seeks the charger. Another common proprioceptive sensor is for heat monitoring. Increased proprioception will be required for robots to work autonomously near people and in harsh environments. Common proprioceptive sensors include thermal, optical, and haptic sensing, as well as the Hall effect (electric).\n\nExteroception is sensing things about the environment. Autonomous robots must have a range of environmental sensors to perform their task and stay out of trouble.\n\n\nSome robotic lawn mowers will adapt their programming by detecting the speed in which grass grows as needed to maintain a perfectly cut lawn, and some vacuum cleaning robots have dirt detectors that sense how much dirt is being picked up and use this information to tell them to stay in one area longer.\n\nThe next step in autonomous behavior is to actually perform a physical task. A new area showing commercial promise is domestic robots, with a flood of small vacuuming robots beginning with iRobot and Electrolux in 2002. While the level of intelligence is not high in these systems, they navigate over wide areas and pilot in tight situations around homes using contact and non-contact sensors. Both of these robots use proprietary algorithms to increase coverage over simple random bounce.\n\nThe next level of autonomous task performance requires a robot to perform conditional tasks. For instance, security robots can be programmed to detect intruders and respond in a particular way depending upon where the intruder is.\n\nFor a robot to associate behaviors with a place (localization) requires it to know where it is and to be able to navigate point-to-point. Such navigation began with wire-guidance in the 1970s and progressed in the early 2000s to beacon-based triangulation. Current commercial robots autonomously navigate based on sensing natural features. The first commercial robots to achieve this were Pyxus' HelpMate hospital robot and the CyberMotion guard robot, both designed by robotics pioneers in the 1980s. These robots originally used manually created CAD floor plans, sonar sensing and wall-following variations to navigate buildings. The next generation, such as MobileRobots' PatrolBot and autonomous wheelchair, both introduced in 2004, have the ability to create their own laser-based maps of a building and to navigate open areas as well as corridors. Their control system changes its path on the fly if something blocks the way.\n\nAt first, autonomous navigation was based on planar sensors, such as laser range-finders, that can only sense at one level. The most advanced systems now fuse information from various sensors for both localization (position) and navigation. Systems such as Motivity can rely on different sensors in different areas, depending upon which provides the most reliable data at the time, and can re-map a building autonomously.\n\nRather than climb stairs, which requires highly specialized hardware, most indoor robots navigate handicapped-accessible areas, controlling elevators, and electronic doors. With such electronic access-control interfaces, robots can now freely navigate indoors. Autonomously climbing stairs and opening doors manually are topics of research at the current time.\n\nAs these indoor techniques continue to develop, vacuuming robots will gain the ability to clean a specific user-specified room or a whole floor. Security robots will be able to cooperatively surround intruders and cut off exits. These advances also bring concomitant protections: robots' internal maps typically permit \"forbidden areas\" to be defined to prevent robots from autonomously entering certain regions.\n\nOutdoor autonomy is most easily achieved in the air, since obstacles are rare. Cruise missiles are rather dangerous highly autonomous robots. Pilotless drone aircraft are increasingly used for reconnaissance. Some of these unmanned aerial vehicles (UAVs) are capable of flying their entire mission without any human interaction at all except possibly for the landing where a person intervenes using radio remote control. Some drones are capable of safe, automatic landings, however. An autonomous ship was announced in 2014—the Autonomous spaceport drone ship—and is scheduled to make its first operational test in December 2014.\n\nOutdoor autonomy is the most difficult for ground vehicles, due to:\nThe Seekur robot was the first commercially available robot to demonstrate MDARS-like capabilities for general use by airports, utility plants, corrections facilities and Homeland Security.\n\nThe Mars rovers MER-A and MER-B (now known as Spirit rover and Opportunity rover) can find the position of the sun and navigate their own routes to destinations on the fly by:\n\n\nThe planned ESA Rover, ExoMars Rover, is capable of vision based relative localisation and absolute localisation to autonomously navigate safe and efficient trajectories to targets by:\n\n\nDuring the final NASA Sample Return Robot Centennial Challenge in 2016, a rover, named Cataglyphis, successfully demonstrated fully autonomous navigation, decision-making, and sample detection, retrieval, and return capabilities. The rover relied on a fusion of measurements from inertial sensors, wheel encoders, Lidar, and camera for navigation and mapping, instead of using GPS or magnetometers. During the 2 hour challenge, Cataglyphis traversed over 2.6 km and returned five different samples to its starting position.\n\nThe DARPA Grand Challenge and DARPA Urban Challenge have encouraged development of even more autonomous capabilities for ground vehicles, while this has been the demonstrated goal for aerial robots since 1990 as part of the AUVSI International Aerial Robotics Competition.\n\nBetween 2013 and 2017, Total S.A. has held the ARGOS Challenge to develop the first autonomous robot for oil and gas production sites. The robots had to face adverse outdoor conditions such as rain, wind and extreme temperatures.\n\nThere are several open problems in autonomous robotics which are special to the field rather than being a part of the general pursuit of AI. According to George A. Bekey's \"Autonomous Robots: From Biological Inspiration to Implementation and Control\", problems include things such as making sure the robot is able to function correctly and not run into obstacles autonomously.\n\nResearchers concerned with creating true artificial life are concerned not only with intelligent control, but further with the capacity of the robot to find its own resources through foraging (looking for food, which includes both energy and spare parts).\n\nThis is related to autonomous foraging, a concern within the sciences of behavioral ecology, social anthropology, and human behavioral ecology; as well as robotics, artificial intelligence, and artificial life.\n\nA delivery robot is an autonomous robot used for delivering goods. As of February 2017 there were several notable companies developing delivery robots (some with pilot deliveries in progress):\n\n\nIn March 2016 a bill was introduced in Washington, D.C., allowing pilot ground robotic deliveries. The program was to take place from September 15 through the end of December 2017. The robots were limited to a weight of 50 pounds unloaded and maximum speed of 10 miles per hour. In case the robot stopped moving because of malfunction the company was required to remove it from the streets within 24 hours. There were allowed only 5 robots to be tested per company at a time. A 2017 version of the Personal Delivery Device Act bill was under review as of March 2017.\n\nIn February 2017 a bill was passed in the US state of Virginia (the House bill, HB2016, and the Senate bill, SB1207) that will allow autonomous delivery robots to travel on sidewalks and use crosswalks statewide beginning on July 1, 2017. The robots will be limited to a maximum speed of 10 mph and maximum weight of 50 pounds. In the states of Idaho and Florida there are also talks about passing similar legislature.\n\nIt has been discussed that robots with similar characteristics to invalid carriages (e.g. 10 mph maximum, limited battery life) might be a workaround for certain classes of applications. If the robot was sufficiently intelligent and able to recharge itself using existing electric vehicle (EV) charging infrastructure it would only need minimal supervision and a single arm with low dexterity might be enough to enable this function if its visual systems had enough resolution.\n\nIn November 2017, the San Francisco Board of Supervisors announced that companies would need to get a city permit in order to test these robots. In addition, sidewalk delivery robots have been banned from making non-research deliveries.\n\n"}
{"id": "3113869", "url": "https://en.wikipedia.org/wiki?curid=3113869", "title": "Barry (dog)", "text": "Barry (dog)\n\nBarry der Menschenretter (1800–1814), also known as Barry, was a dog of a breed which was later called the St. Bernard that worked as a mountain rescue dog in Switzerland and Italy for the Great St Bernard Hospice. He predates the modern St. Bernard, and was lighter built than the modern breed. He has been described as the most famous St. Bernard, as he was credited with saving more than 40 lives during his lifetime, hence his byname meaning \"people rescuer\" in German.\n\nThe legend surrounding him was that he was killed while attempting a rescue; however, this is untrue. Barry retired to Bern, Switzerland and after his death his body was passed into the care of the Natural History Museum of Bern. His skin has been preserved through taxidermy although his skull was modified in 1923 to match the Saint Bernard of that time period. His story and name have been used in literary works, and a monument to him stands in the Cimetière des Chiens near Paris. At the hospice, one dog has always been named Barry in his honor; and since 2004, the Foundation Barry du Grand Saint Bernard has been set up to take over the responsibility for breeding dogs from the hospice.\n\nThe first mention in the Great St Bernard Hospice archives of a dog was in 1707 which simply said \"A dog was buried by us.\" The dogs are thought to have been introduced to the monastery as watchdogs at some point between 1660 and 1670. Old skulls from the collection of the Natural History Museum of Bern show that at least two types of dog lived at the hospice. By 1800, the year that Barry was born, it was known that a special kind of dog was being used for rescue work in the pass. This general variety of dog was known as a Küherhund, or cowherd's dog.\n\nMeasurements of his preserved body show that Barry was smaller than the modern Saint Bernard, weighing between whereas modern Bernards weigh between . His current mounted height is approximately , but the living Barry would have been slightly smaller.\n\nDuring Barry's career, he was credited with saving the lives of more than forty people, although this number has sometimes varied over the years. Barry's most famous rescue was that of a young boy. He found the child asleep in a cavern of ice. After warming up the boy's body sufficiently by licking him, he moved the boy about and onto his back and carried the child back to the hospice. The child survived and was returned to his parents, although other sources say that the boy's mother died in the avalanche that trapped the boy. The Museum of Natural History of Bern disputes the legend, attributing it to Peter Scheitlin, an animal psychologist.\n\nThere is a plaque on a monument in the Cimetière des Chiens pet cemetery which states, \"\" (\"Barry saved the lives of forty people, but died while attempting to save his forty-first\"). The story goes that news had come that a Swiss soldier was lost in the mountains. Barry was searching for the soldier and had picked up the scent, some forty-eight hours old, and finally stopped before a large bank of ice. He dug until he reached the soldier, and then licked him as he was trained. The Swiss soldier awoke startled and mistook Barry for a wolf and fatally stabbed him with his bayonet. James Watson in his 1906 work \"The Dog Book\" attributed the rumour to fellow author Idstone, also known as Reverend Thomas Pearce.\n\nHowever, the legend of his death is untrue. After twelve years of service at the monastery, Barry was brought by a monk to Bern, Switzerland so that he could live out the rest of his life. He died at the age of 14. His body passed into the hands of the Natural History Museum of Bern. A special exhibit was held in his honor at the museum to commemorate his 200th birthday in 2000.\n\nThe Hospice has always maintained one St. Bernard named Barry in the original's honor. During Barry's lifetime, his breed did not have one specific name. By 1820, six years after his death, Barry was specifically referred to as being an Alpine Mastiff, while there was also a breed called the Alpine Spaniel which was recorded around the same time period. The English called the breed \"sacred dogs\", while the German Kynology proposed the name \"Alpendog\" in 1828. Following his death and up until 1860, the entire stock were called \"Barry hounds\" in the Canton of Bern after Barry himself. It was not until 1865 when the term \"St. Bernard\" was first used primarily for the breed. Under this name, the St. Bernard has been recognised since 1880 by the Swiss Kennel Club.\n\nBarry is described as the most famous St. Bernard by the Natural History Museum of Bern. Following his death, his skin was preserved by a taxidermist for the museum, while the rest of his body was buried. He was originally given a humble and meek pose, as the taxidermist felt that this would serve as a reminder of servitude to future generations. In 1923, his body was refurbished by Georg Ruprecht, as his coat had become brittle and had broken into more than 20 pieces. During the restoration, his body was re-posed and his skull shape was modified to match the shape of the St. Bernard of that time, in a compromise between Ruprecht and the Museum's director. His original head shape was rather flat with a moderate stop, with the modification resulting in a larger head with a more pronounced stop. A barrel was added hanging from his collar, following the popularization of the myth of the monastery's dogs using these during the rescues, which was originally introduced by Edwin Landseer's work \"Alpine Mastiffs Reanimating a Distressed Traveller\". The barrel was removed in 1978 by Professor Walter Huber, the director of the museum, although it has since been replaced. A monument to Barry is located opposite the entrance to the Cimetière des Chiens in Paris.\n\nIn literary works, Samuel Rogers' poem \"The Great Saint Bernard\" is sometimes referred to as \"Barry, The Great St. Bernard\". Henry Bordeaux praised Barry's work in his 1911 novel \"La Neige sur les pas\". Walt Disney Productions made a telemovie entitled \"Barry of the Great St. Bernard\" in 1977, and Barry's story has been featured in children's books such as \"Barry: The Bravest Saint Bernard\" published by Random House Books for Young Readers.\n\nUntil September 2004, 18 dogs still belonged to the hospice at any one time. The Foundation Barry du Grand Saint Bernard was established to create kennels in Martigny, a village further down the pass, to take over the breeding of St. Bernard puppies from the friars at the hospice. Around 20 puppies per year are born at the foundation. In 2009, the St. Bernard Dog Museum was opened at the Foundation in Martigny and – to commemorate the occasion – Barry's remains were lent from the museum in Bern. Each summer, the foundation leads dogs up the pass when it is open to the hospice, mainly for tourists, with rescue efforts on the pass now conducted by helicopters.\n\n"}
{"id": "27277176", "url": "https://en.wikipedia.org/wiki?curid=27277176", "title": "Beastmaster III: The Eye of Braxus", "text": "Beastmaster III: The Eye of Braxus\n\nBeastmaster III: The Eye of Braxus is the 1996 sequel to the 1982 film \"The Beastmaster\", starring Marc Singer.\n\nIn the third installment in the series, Dar, the Beastmaster (Marc Singer) teams up with Seth (Tony Todd) to rescue his brother King Tal (Casper Van Dien). They learn that the boy was captured by the evil Lord Agon (David Warner), who has been sacrificing young prisoners in order to magically retain his youth, and seeks to gain immortality by releasing the dark god Braxus from his prison. Along the way, the heroes are assisted by a beautiful witch named Morgana (Lesley-Anne Down), her acrobatic sidekick Bey (Keith Coulouris), and a warrior woman named Shada (Sandra Hess).\n\n\n"}
{"id": "39986764", "url": "https://en.wikipedia.org/wiki?curid=39986764", "title": "Bihar school meal poisoning incident", "text": "Bihar school meal poisoning incident\n\nOn 16 July 2013, at least 23 students died and dozens more fell ill at a primary school in the village of Dharmashati Gandaman in the Saran district of the Indian state of Bihar after eating a Midday Meal contaminated with pesticide. Angered by the deaths and illnesses, villagers took to the streets in many parts of the district in violent protest. Subsequently, the Bihar government took a series of steps to prevent any recurrence of such incidents.\n\nAcross India, the Midday Meal Scheme provides roughly 120 million children with free lunch, making it the world's largest school lunch program. In spite of corruption involved in implementing the scheme, it aims to fight widespread poverty and improve children's school attendance and health as a large number of India's children suffer from malnutrition.\n\nBihar in northern India is among the nation's poorest states. According to Mashrakh residents, students have suffered from food poisoning after eating school lunches on multiple occasions. P. K. Shahi, Bihar's education minister, said complaints about food quality were not uncommon but there had been no reported incidents of widespread food poisoning during his tenure. The nonprofit Iskcon Food Relief Foundation describes the meal programmes in Bihar and neighbouring Uttar Pradesh as \"the worst in India.\" Public health is poor in general, with most water sources contaminated, and hospitals underfunded.\n\nThe primary school in the village of Dharmashati Gandaman was established in 2010. At the time of the incident, 89 children were registered with the school. The food material for midday meals was stored at the house of the headmistress as the school did not have sufficient infrastructure.\n\nOn 16 July 2013, children aged between four and twelve years at the Dharmashati Gandaman primary school complained that their lunch, served as a part of the Midday Meal Scheme, tasted odd. Children who questioned the food were rebuked by the headmistress. Earlier, headmistress Meena Kumari had been informed by the school's cook that the new cooking oil was discoloured and smelled odd. Kumari replied that the oil was purchased at a local grocery store and safe to use. The cook, who was also hospitalized by the poisoning, later told reporters that it looked like there was \"an accumulation of residual waste at the bottom [of the oil jar]\". The meal cooked at the school that day consisted of soya beans, rice and potato curry.\n\nThirty minutes after eating the meal the children complained of stomach pain and soon after were taken ill with vomiting and diarrhoea. The number of sick children overwhelmed the school and local medical system. Some of the sick children were sent home, forcing their parents to seek help on their own. According to the official count, 23 children died as a result of the contaminated food. Parents and local villagers said at least 27 had died. Sixteen children died on site, and four others were declared dead upon arrival at the local hospital. Others died in hospital. Among the dead were two children of a female cook, Panna Devi; her third child survived. A total of 48 students fell ill from the contaminated food. Three remained in a critical condition as of 17 July. Thirty-one children were moved from the local hospital to Patna Medical College Hospital (PMCH) for further treatment.\n\nInitial indications were that the food was contaminated by an organophosphate, a class of chemicals commonly found in insecticides. A local government administrator commented \"It appears to be a case of poisoning but we will have to wait for forensic reports ... Had it been a case of natural food poisoning, so many children would not have died.\" Dr. Amar Kant Jha, superintendent of Patna Medical College and Hospital in Patna, said that the survivors were emitting toxic vapours, which led his team to suspect almost immediately that they had been poisoned by an organophosphate.\n\nLate on 17 July, officials stated that they believed the cooking oil had been placed in a container formerly used to store insecticides. According to state officials, the school’s headmistress had bought the cooking oil used in the food from a grocery store owned by her husband. On 20 July police said that a forensic report confirmed the cooking oil contained \"very toxic\" levels of monocrotophos, an agricultural pesticide.\n\nNineteen of the children's bodies were buried on or near school grounds in protest. Across Bihar, numerous students refused to eat their meals in the days following the incident. On 17 July, hundreds of Mashrakh residents took to the streets in protest. Demonstrators lit fires and burned effigies of Bihar Chief Minister Nitish Kumar. Four police vehicles were damaged by the fires. Others threw stones at the police station and chanted slogans denouncing the government. Some villagers demanded that the Midday Meal program be scrapped. Roads and rail lines were blocked by angry protesters carrying sticks and poles. Desks and chairs from the school were taken and smashed, while the kitchen area was destroyed. In nearby Chhapra, multiple arson attacks were reported, including reports that a crowd set fire to a bus, but no injuries were reported from either city.\n\nShahi commented that many people involved in the program are looking for easy money and that \"it is just not possible to taste meals in all the 73,000 schools before children eat the food.\" He also alleged that the contaminated oil had been purchased from a member of a rival political party. Opposition party members accused the ruling Janata Dal (United) party of acting too slowly and called for a general strike.\n\nThe Bihar government promised a thorough investigation and offered INR 200,000 (US$3,400) compensation for families of the dead children. Kumar called an emergency meeting and dispatched forensic experts to Mashrakh. The headmistress and her husband fled after the deaths became public knowledge and she was suspended by the administrative authority. A First Information Report was filed against the headmistress for criminal negligence, and police began searching for her. A district magistrate told the BBC that her property would be confiscated if she did not surrender to the authorities. She was apprehended by police in Chhapra on 24 July and held on suspicion of murder and criminal conspiracy. In October 2013 the headmistress and her husband were charged with murder and faced the death penalty if convicted.\n\nBihar Government has taken a number of steps to prevent the recurrence of such incidents. The Government issued a toll free number for all complaints related to Mid-Day Meal. It also ordered that the raw grain samples would be kept for three months at Godowns from where items for Midday Meal are supplied. A case was filed by Akhilanand Mishra (one of the victims). On August 29, 2016, Meena Kumari was sentenced to 17 years in prison for her role in the incident. \n"}
{"id": "39308041", "url": "https://en.wikipedia.org/wiki?curid=39308041", "title": "Bill Maris", "text": "Bill Maris\n\nBill Maris is an entrepreneur and venture capitalist focused on technology and the life sciences. He is the founder and first CEO of Google Ventures (GV). He is the creator of Google's Calico project, a company focused on the genetic basis of aging. He is the founder of early web hosting pioneer Burlee.com, now part of Web.com, and the founder of Section 32, a California-based venture fund focused on frontier technology.\n\nMaris graduated with a highest honors with a degree in neuroscience from Middlebury College.\n\nMaris' background includes research at the Duke University Medical Center Department of Neurobiology. He has a degree in neuroscience from Middlebury College. Maris began his career as a biotechnology and healthcare portfolio manager for Swedish investment firm Investor AB. In 1997, Maris founded one of the first Web hosting companies, Burlee.com, and built the company's original computing, network and technological infrastructure after teaching himself to code from books purchased at the local Barnes & Noble. Burlee was subsequently acquired by Interland, Inc. and renamed Web.com. Burlee.com was merged with Interland in 2002. Maris remained with the company until 2003. Interland subsequently changed its name to Web.com.\n\nMaris was a portfolio manager at Investor AB in 1997. It was on a business trip to Investor's headquarters in Stockholm where Maris was inspired to found Burlee.com after seeing a rack of servers in a closet which housed the company's email and web site.\n\nIn the mid 2000s, Maris partnered with entrepreneur David Green to transfer a novel hydrophobic acrylic lens to Aurolab to cure cataract blindness in the developing world, where it has been used in more than 30 million patients.\n\nMaris founded GV, formerly Google Ventures, in 2008 as the venture capital investment arm of Google Inc. He was responsible for the fund's strategy and management, and oversaw $3.0 billion in investments in technology and the life sciences.\n\nMaris was one of the first to cite the troubles with Theranos, the troubled Silicon Valley blood testing company.\n\nMaris founded Calico, a multibillion-dollar company whose mission is to understand and influence the genetic basis of aging. Google funded the company after Maris pitched the board of directors.\n\nIn a 2015 interview, Maris stated that health care breakthroughs can significantly improve the quality and duration of human lifespan across the globe, and that he is looking to invest in promising biotechnology companies.\n\nMaris was also the Vice President of Special Projects at Google, where he was heavily involved in Google X, Verily.\n\nMaris left Google Ventures on August 12, 2016, declaring \"mission accomplished.\" \n\nIn 2017 Maris founded Section 32, a California-based venture fund with approximately $400 million under management.\n\nIn 2014, Maris married singer/songwriter Tristan Prettyman at Kruger National Park in South Africa. Their son Kylo Evergreen Maris was born on August 26, 2015. He filed divorce from Prettyman on April 20, 2018.\n"}
{"id": "35439884", "url": "https://en.wikipedia.org/wiki?curid=35439884", "title": "Bio-inspired robotics", "text": "Bio-inspired robotics\n\nBio-inspired robotic locomotion is a fairly new subcategory of bio-inspired design. It is about learning concepts from nature and applying them to the design of real-world engineered systems. More specifically, this field is about making robots that are inspired by biological systems. Biomimicry and bio-inspired design are sometimes confused. Biomimicry is copying the nature while bio-inspired design is learning from nature and making a mechanism that is simpler and more effective than the system observed in nature. Biomimicry has led to the development of a different branch of robotics called soft robotics. The biological systems have been optimized for specific tasks according to their habitat. However, they are multifunctional and are not designed for only one specific functionality. Bio-inspired robotics is about studying biological systems, and look for the mechanisms that may solve a problem in the engineering field. The designer should then try to simplify and enhance that mechanism for the specific task of interest. Bio-inspired roboticists are usually interested in biosensors (e.g. eye), bioactuators (e.g. muscle), or biomaterials (e.g. spider silk). Most of the robots have some type of locomotion system. Thus, in this article different modes of animal locomotion and few examples of the corresponding bio-inspired robots are introduced.\n\nBiolocomotion or animal locomotion is usually categorized as below:\n\nLocomotion on a surface may include terrestrial locomotion and arboreal locomotion. We will specifically discuss about terrestrial locomotion in detail in the next section.\n\nLocomotion in a blood stream swimming and flying. There are many swimming and flying robots designed and built by roboticists.\n\nThere are many animal and insects moving on land with or without legs. We will discuss legged and limbless locomotion in this section as well as climbing and jumping. Anchoring the feet is fundamental to locomotion on land. The ability to increase traction is important for slip-free motion on surfaces such as smooth rock faces and ice, and is especially critical for moving uphill. Numerous biological mechanisms exist for providing purchase: claws rely upon friction-based mechanisms; gecko feet upon van der walls forces; and some insect feet upon fluid-mediated adhesive forces.\n\nLegged robots may have one, two, four, six, or many legs depending on the application. One of the main advantages of using legs instead of wheels is moving on uneven environment more effectively. Bipedal, quadrupedal, and hexapedal locomotion are among the most favorite types of legged locomotion in the field of bio-inspired robotics. Rhex, a Reliable Hexapedal robot and Cheetah are the two fastest running robots so far. iSprawl is another hexapedal robot inspired by cockroach locomotion that has been developed at Stanford University. This robot can run up to 15 body length per second and can achieve speeds of up to 2.3 m/s. The original version of this robot was pneumatically driven while the new generation uses a single electric motor for locomotion.\n\nTerrain involving topography over a range of length scales can be challenging for most organisms and biomimetic robots. Such terrain are easily passed over by limbless organisms such as snakes. Several animals and insects including worms, snails, caterpillars, and snakes are capable of limbless locomotion. A review of snake-like robots is presented by Hirose et al. These robots can be categorized as robots with passive or active wheels, robots with active treads, and undulating robots using vertical waves or linear expansions. Most snake-like robots use wheels, which are high in friction when moving side to side but low in friction when rolling forward (and can be prevented from rolling backward). The majority of snake-like robots use either lateral undulation or rectilinear locomotion and have difficulty climbing vertically. Choset has recently developed a modular robot that can mimic several snake gaits, but it cannot perform concertina motion. Researchers at Georgia Tech have recently developed two snake-like robots called Scalybot. The focus of these robots is on the role of snake ventral scales on adjusting the frictional properties in different directions. These robots can actively control their scales to modify their frictional properties and move on a variety of surfaces efficiently. Researchers at CMU have developed both scaled and conventional actuated snake-like robots.\nClimbing is an especially difficult task because mistakes made by the climber may cause the climber to lose its grip and fall. Most robots have been built around a single functionality observed in their biological counterparts. Geckobots typically use van der waals forces that work only on smooth surfaces. Stickybots, and use directional dry adhesives that works best on smooth surfaces. Spinybot and the RiSE robot are among the insect-like robots that use spines instead. Legged climbing robots have several limitations. They cannot handle large obstacles since they are not flexible and they require a wide space for moving. They usually cannot climb both smooth and rough surfaces or handle vertical to horizontal transitions as well.\n\nOne of the tasks commonly performed by a variety of living organisms is jumping. Bharal, hares, kangaroo, grasshopper, flea, and locust are among the best jumping animals. A miniature 7g jumping robot inspired by locust has been developed at EPFL that can jump up to 138 cm. The jump event is induced by releasing the tension of a spring. The highest jumping miniature robot is inspired by the locust, weighs 23 grams with its highest jump to 365 cm is \"TAUB\" (Tel-Aviv University and Braude College of engineering). It uses torsion springs as energy storage and includes a wire and latch mechanism to compress and release the springs. ETH Zurich has reported a soft jumping robot based on the combustion of methane and laughing gas. The thermal gas expansion inside the soft combustion chamber drastically increases the chamber volume. This causes the 2 kg robot to jump up to 20 cm. The soft robot inspired by a roly-poly toy then reorientates itself into an upright position after landing.\n\nIt is calculated that when swimming some fish can achieve a propulsive efficiency greater than 90%. Furthermore, they can accelerate and maneuver far better than any man-made boat or submarine, and produce less noise and water disturbance. Therefore, many researchers studying underwater robots would like to copy this type of locomotion. Notable examples are the Essex University Computer Science Robotic Fish G9, and the Robot Tuna built by the Institute of Field Robotics, to analyze and mathematically model thunniform motion. The Aqua Penguin, designed and built by Festo of Germany, copies the streamlined shape and propulsion by front \"flippers\" of penguins. Festo have also built the Aqua Ray and Aqua Jelly, which emulate the locomotion of manta ray, and jellyfish, respectively.\n\nIn 2014 \"iSplash\"-II was developed by PhD student Richard James Clapham and Prof. Huosheng Hu at Essex University. It was the first robotic fish capable of outperforming real carangiform fish in terms of average maximum velocity (measured in body lengths/ second) and endurance, the duration that top speed is maintained. This build attained swimming speeds of 11.6BL/s (i.e. 3.7 m/s). The first build, \"iSplash\"-I (2014) was the first robotic platform to apply a full-body length carangiform swimming motion which was found to increase swimming speed by 27% over the traditional approach of a posterior confined waveform.\n\nThe modular robots are typically capable of performing several tasks and are specifically useful for search and rescue or exploratory missions. Some of the featured robots in this category include a salamander inspired robot developed at EPFL that can walk and swim, a snake inspired robot developed at Carnegie-Mellon University that has four different modes of terrestrial locomotion, and a cockroach inspired robot can run and climb on a variety of complex terrain.\n\nHumanoid robots are robots that look human-like or are inspired by the human form. There are many different types of humanoid robots for applications such as personal assistance, reception, work at industries, or companionship. These type of robots are used for research purposes as well and were originally developed to build better orthosis and prosthesis for human beings. Petman is one of the first and most advanced humanoid robots developed at Boston Dynamics. Some of the humanoid robots such as Honda Asimo are over actuated. On the other hand, there are some humanoid robots like the robot developed at Cornell University that do not have any actuators and walk passively descending a shallow slope.\n\nThe collective behavior of animals has been of interest to researchers for several years. Ants can make structures like rafts to survive on the rivers. Fish can sense their environment more effectively in large groups. Swarm robotics is a fairly new field and the goal is to make robots that can work together and transfer the data, make structures as a group, etc.\n\nSoft robots are robots composed entirely of soft materials and moved through pneumatic pressure, similar to an octopus or starfish. Such robots are flexible enough to move in very limited spaces (such as in the human body). The first multigait soft robots was developed in 2011 and the first fully integrated, independent soft robot (with soft batteries and control systems) was developed in 2015.\n\n\n\n"}
{"id": "1631144", "url": "https://en.wikipedia.org/wiki?curid=1631144", "title": "Biolex", "text": "Biolex\n\nBiolex Therapeutics was a biotechnology firm in the Research Triangle of North Carolina that was founded in 1997 and raised $190 million from investors. It filed for Chapter 7 bankruptcy on July 5, 2012.\n\nThe company focused on expression of difficult-to-synthesize recombinant proteins in its LEX platform, which used \"Lemna\", a duckweed. The duckweeds are a family of small aquatic plants that can be grown in sterile culture. Biolex developed recombinant DNA technology for efficiently producing pharmaceutical proteins in Lemna. Therapeutic glycosylated proteins, including monoclonal antibodies and interferon (IFN-alpha2b) have been produced using the LEX platform.\n\nBiolex acquired Epicyte Pharmaceutical Inc. on May 6, 2004, and acquired LemnaGene SA of Lyon, France in 2005. Biolex was a privately held company, originally backed by Quaker BioVentures, The Trelys Funds, and Polaris Venture Partners. The term \"plantibody\" is trademarked by Biolex. In May 2012 Biolex announced that it sold the LEX System to Synthon, a Netherlands-based specialty pharmaceutical company. The sale included two preclinical biologics made with the LEX System, BLX-301, a humanized and glyco-optimized anti-CD20 antibody for non-Hodgkin's B-cell lymphoma and other B-cell malignancies and BLX-155, a direct-acting thrombolytic. The financial terms of the sale were not disclosed.\n\n"}
{"id": "39148820", "url": "https://en.wikipedia.org/wiki?curid=39148820", "title": "Birthing chair", "text": "Birthing chair\n\nA birthing chair, also known as a birth chair, is a device that is shaped to assist a woman in the physiological upright posture during childbirth. It is intended to provide balance and support. If backless, it is known as a birth stool.\n\nThe early birthing chairs varied between having three or four legs, though three legged birthing chairs are most commonly seen. Both styles support the bottom of the women in labor and often have a slender, sloped back for comfort and to allow birthing assistants, who are positioned behind the mother in labor, to massage or support her. Often the arms of the chair have hand holds or arm rests for the mother to grip, providing extra leverage. Birthing chairs are usually 8 to 10 inches (20.32 centimeters- 25.4 centimeters) off the ground specifically to allow laboring women to brace their feet against the ground to help in pushing.\n\nThe birthing chair has been used for millennia. Birthing chairs took the place of laboring mothers sitting on birth attendant’s laps, as it was the previous practice. These chairs were used before male physicians dominated the delivery room. The use of the birthing chair or similar devices has been seen around the globe, not isolated to a particular region. The birthing chair can be traced to Egypt in the year 1450 B.C.E. Pictured on the walls of The Birth House at Luxor, Egypt, is an Egyptian queen giving birth on a stool. It can also be traced to Greece in 200 B.C as it is featured on an ancient Grecian sculpted votive.\nCeltic items from 100 B.C.E in Britain also depict women sitting in the same upright position as if in a birthing chair. Birthing chairs fell out of use after physicians began using the flat bed for women to lie on during delivery. A woman giving birth in the upright position has been seen in Asian, African, Pacific Islander, and Native American art. The concept of the birthing chair has also been written in observations by anthropologist as well as missionaries.\n\nAs of the 1980s the birthing chair has been making a comeback in the modern medicine of childbirth. Some expecting mothers have reverted to the birthing chair for its upright position because it allows gravity to assist in the expulsion of the baby. Studies have shown that the birthing chair speeds up the time of delivery and increases comfort\" for expecting mothers. The position of the birthing chair allows muscles(including vaginal and abdominal as well as those in the back, stomach, legs, and arms) used in childbirth to work to efficiency. Since the 1980s many hospitals have begun installing birthing chairs due to the large number of inquiries about them.\n\nA modern birthing chair/stool can be made of many different materials including PVC inflatables such as the CUB support, plastic such as the Kaya stool and padded wooden stools. More recently birthing chairs/stools have been made to accommodate several upright positions such as squatting, all fours, kneeling and sitting, and are used as supports not necessarily as chairs or stools. Research suggests that being upright during labour can have many positive benefits for mother and baby. Using a modern-day birthing chair/stool/support can aid a woman in having an upright birth.\n\n Line 1 from 1.1 to 1.10 photographs show mother alone on birth-chair (with bowl beneath seat opening to catch amniotic fluid) and with support from partner respectively and photograph 5.5 shows a wooden birthing chair with foot supports.\n"}
{"id": "9411213", "url": "https://en.wikipedia.org/wiki?curid=9411213", "title": "Bobbie the Wonder Dog", "text": "Bobbie the Wonder Dog\n\nBobbie the Wonder Dog (1921–1927) was a dog who covered on his own to return home to Silverton, Oregon after he was lost while his owners were visiting family in Wolcott, Indiana. Ripley's estimated the journey may have been as long as .\n\nIn August 1923, while on a family road trip in Indiana, Frank and Elizabeth Brazier, with their daughters Leona and Nova, were visiting relatives in Wolcott, Indiana. Their two-year-old Scotch Collie/English Shepherd mix dog Bobbie was attacked by three other dogs and ran away. After an exhaustive search, the heartbroken Brazier family were unable to find Bobbie and continued their trip before returning home to Oregon, expecting never to see their dog again.\n\nIn February 1924, six months later, Bobbie returned to Silverton mangy, dirty, and scrawny, with his toenails worn down to nothing. He showed all the signs of having walked the entire distance, including swimming rivers and crossing the Continental Divide during the coldest part of winter.\n\nDuring his ordeal, he crossed at least of plains, desert, and mountains in the winter to return home, an average of approximately per day. After his return to Silverton, he experienced a meteoric rise to fame. His story drew national attention and was featured in numerous newspapers.\n\nHe was the subject of newspaper articles including \"Ripley's Believe It or Not!\", books, and film. Bobbie played himself in the 1924 silent film \"The Call of the West\". He received hundreds of letters from people around the world and was honored with a jewel-studded harness and collar, ribbons, and keys to cities.\n\nPeople who had fed and sheltered Bobbie on his journey wrote the family to tell about their time with Bobbie. The Humane Society of Portland was able to use these stories to assemble a relatively precise description of the route Bobbie took.\n\nThe humane society concluded that after returning to Wolcott and unable to find his owners, Bobbie initially followed their further travels into northeast Indiana. He then struck out in several directions, apparently seeking their scent. He eventually headed west.\n\nDuring their original trip, the Braziers had parked their car in a service station each night. Their dog visited each of these stops on his journey, along with a number of homes, and a hobo camp.\nIn Portland, an Irish woman took care of him for a period of time when she found he had been injured, leaving his legs and paws gashed.\n\nUpon his death in 1927, he was buried with honors at the Oregon Humane Society's pet cemetery in Portland. A week later, German Shepherd film star Rin Tin Tin laid a wreath at his grave. His grave is sheltered by a \"fancy white and red dog house\" received during a promotional appearance at the Portland Home Show. The gravestone has been moved outside the house for better viewing.\n\nBobbie's demonstration of loyalty is celebrated during Silverton's annual children's pet parade that serves as a reminder of the special place animals and pets have in people's lives. The event was started several years after Bobbie's death and the first parade was led by his son, Pal. A outdoor painting featuring Bobbie's story is part of a series of murals that decorate the walls of businesses in Silverton.\n\nIn late 2012, responding to public sentiment that his burial location in Portland did not properly honor his story and his connection to his hometown, a grassroots movement was started by a group of Silvertonians with the goal of repatriating Bobbie's remains to Silverton, for reburial and memorialization.\n\n\n"}
{"id": "2075626", "url": "https://en.wikipedia.org/wiki?curid=2075626", "title": "Chapelle ardente", "text": "Chapelle ardente\n\nA chapelle ardente (Fr. \"burning chapel\") is a chapel or room in which the corpse of a sovereign or other exalted personage lies in state pending the funeral service. The name is in allusion to the many candles which are lighted round the catafalque. This custom is first chronicled as occurring at the obsequies of Dagobert I (602–638).\n"}
{"id": "30690058", "url": "https://en.wikipedia.org/wiki?curid=30690058", "title": "Dean Johnson (entertainer)", "text": "Dean Johnson (entertainer)\n\nDean Johnson (1961–2007) was a cross-dressing musician, party promoter, and was a highly recognisable figure in the downtown New York City nightlife in the 1980s, 1990s, and 2000s. Known for his towering height, shaved head, giant sunglasses, and penchant for wearing short cocktail dresses that exaggerated the length of his pale, lithe figure, Johnson played a seminal role in the emergence of the Queercore gay rock and roll subculture in the East Village.\n\nBoth an underground rock star and a porn star, Dean Johnson fronted two bands: Dean and the Weenies and the Velvet Mafia. He partied with art-world luminaries like Keith Haring, Jean-Michel Basquiat, and Andy Warhol, and performed at nightclubs such as; Area, Danceteria, The Cat Club, and the Pyramid. Johnson was the driving force behind the ground-breaking “Rock and Roll Fag Bar” party, held Tuesday nights in the late 1980s at The World. He subsequently produced the monthly “HomoCorps” live music showcase at CBGB, which featured gay and transgender rock-and-roll bands.\n\nJohnson grappled with drug addiction and also was a sex worker. He died at age 46 from an apparent drug overdose while turning a trick in a Washington, D.C. apartment. There is some suspicion around his death, stemming from the fact that the man he was with had been found not guilty of murder in a trial held only a few weeks prior.\n\n"}
{"id": "20091417", "url": "https://en.wikipedia.org/wiki?curid=20091417", "title": "Domenico Leccisi", "text": "Domenico Leccisi\n\nDomenico Leccisi (20 May 19202 November 2008) was an Italian politician, who is best known for stealing the corpse of the fascist dictator Benito Mussolini from an unmarked grave in 1946.\n\nOn the eve of the first anniversary of Italy's liberation from Nazi occupying forces, Leccisi, then a right-wing journalist, and two helpers dug up the corpse from the city's Musocco cemetery and spirited it away.\n\nLeccisi left behind a note that said: \"Finally, O Duce, you are with us. We will cover you with roses but the smell of your virtue will overpower those roses.\"\n\nAuthorities discovered Mussolini's remains four months later, hidden at a 15th-century monastery at Pavia south of Milan. Two monks were charged with hiding the body.\n\nIn 1957 Mussolini found a final resting place at his birthplace in Predappio, northern Italy, after a campaign led by Leccisi and his party.\n\nLeccisi served as a parliamentary deputy for the neo-fascist Italian Social Movement from 1953 to 1963. He was also a Milan city councillor and wrote an autobiography, \"With Mussolini Before and After Piazzale Loreto\".\n\nLeccisi died at age 88 on 2 November 2008 at a retirement home in Milan due to heart and respiratory disease.\n"}
{"id": "23495395", "url": "https://en.wikipedia.org/wiki?curid=23495395", "title": "Euthanasia in Switzerland", "text": "Euthanasia in Switzerland\n\nIn Switzerland, a total of 742 assisted suicides (320 men, 422 women) was recorded for 2014, compared to 1,029 non-assisted suicides (754 men, 275 women); most of the assisted suicides concerned elderly people suffering from a terminal disease.\n\nEuthanasia organisations have been widely used by foreigners, in what critics have termed suicide tourism. As of 2008, 60% of the total number of suicides assisted by the organisation Dignitas had been Germans.\n\nThe Swiss Criminal Code of 1937 outlaws \"incitement or assistance to suicide from selfish motives\" (Art. 115). Any active role in voluntary euthanasia (\"manslaughter on request\") is also outlawed, even if committed from \"respectable motives\" such as mercy killings (Art. 114).\nHowever, by omission, assisted suicide from non-selfish motives remains legal. \nFor example, lethal drugs may be prescribed as long as the recipient takes an active role in the drug administration, but active euthanasia (such as the act of administering a lethal injection) is not legal. \nAll forms of active euthanasia like administering lethal injection remain prohibited in Switzerland. Swiss law only allows providing means to commit suicide, and reasons for doing so must not be based on self-interest (such as monetary gain).\nBased on this legal situation, non-profit organisations administering life-ending medicine were first established in Switzerland in the 1980s.\n\nArticle 115 of the Swiss Criminal Code reads:\nIt should be noted, however, that the Swiss Criminal Code states that \"English is not an official language of the Swiss Confederation. This translation is\nprovided for information purposes only and has no legal force.\"\n\nThis liberal regulation of assisted suicide also permits the assistance of voluntary euthanasia for non-resident foreigners, which has led to the phenomenon of \"suicide tourism\".\n\nWhen an assisted suicide is declared, a police inquiry may be started. Since no crime has been committed in the absence of a selfish motive, these are mostly open and shut cases. Prosecution can occur if doubts are raised about the patient's competence to make an autonomous choice, or about the motivation of anyone involved in assisting the suicide.\nWhile there is no regulation on permissible reasons for the suicide, the major Swiss non-profit organisations dedicated to assisted suicide may require that a terminal illness has been diagnosed.\n\nA complaint against the health department of the canton of Zurich on the part of a man suffering from bipolar affective disorder and desiring to be issued with pentobarbital by the state in order to end his life was rejected in a Federal Supreme Court of Switzerland decision of 3 November 2006.\nThe court conceded that \"It cannot be denied that an incurable, long-lasting, severe mental impairment similar to a somatic one can create a suffering out of which a patient would find his/her life in the long run not worth living anymore\" but found that no case can be made that the state has any obligation to facilitate the availability of substances used for euthanasia, as had been argued by the plaintiff based on both the Swiss Federal Constitution and on article 8 of the ECHR.\nIn a referendum on 15 May 2011, voters in the canton of Zurich overwhelmingly rejected calls to ban assisted suicide or to outlaw the practice for non-residents. Out of more than 278,000 ballots cast, the initiative to ban assisted suicide was rejected by 85 per cent of voters and the initiative to outlaw it for foreigners was turned down by 78 per cent.\n\nIn a 2007 essay in the \"Hastings Center Report\", bioethicist Jacob M. Appel advocated adopting similar rules in the United States.\n\n"}
{"id": "17574069", "url": "https://en.wikipedia.org/wiki?curid=17574069", "title": "Fawzi al-Ghazzi", "text": "Fawzi al-Ghazzi\n\nFawzi al-Ghazzi (1891–1929) (), was a Syrian politician known for being the father of the Syrian constitution. Ghazzi was born in Damascus, he studied in Damascus and Istanbul, and served in the Turkish army during World War I. After the Arab revolt he was appointed secretary of the Interior Ministry of the Arab Government in 1920. He was a staunch opposer to the French mandate, and was imprisoned several times and exiled, among other nationalist leaders, to the island of Arwad. He was elected as a Member of Parliament in 1928, and the chairman of the Constituent Assembly, which was responsible for writing the first Syrian Constitution, which was approved by the Parliament on July 4, 1928. He was poisoned by his wife for unknown reasons.\n\nDamascus Online\n"}
{"id": "11579", "url": "https://en.wikipedia.org/wiki?curid=11579", "title": "Fermi paradox", "text": "Fermi paradox\n\nThe Fermi paradox, or Fermi's paradox, named after physicist Enrico Fermi, is the apparent contradiction between the lack of evidence and high probability estimates for the existence of extraterrestrial civilizations. The basic points of the argument, made by physicists Enrico Fermi (1901–1954) and Michael H. Hart (born 1932), are:\nAccording to this line of reasoning, the Earth should have already been visited by extraterrestrial aliens. In an informal conversation, Fermi noted no convincing evidence of this, leading him to ask, \"Where is everybody?\" There have been many attempts to explain the Fermi paradox, primarily either suggesting that intelligent extraterrestrial life is extremely rare or proposing reasons that such civilizations have not contacted or visited Earth.\nThe Fermi paradox is a conflict between the argument that scale and probability seem to favor intelligent life being common in the universe, and a total lack of evidence of intelligent life having ever arisen anywhere other than on the Earth.\n\nThe first aspect of the Fermi paradox is a function of the scale or the large numbers involved: there are an estimated 200–400 billion stars in the Milky Way (2–4 × 10) and 70 sextillion (7×10) in the observable universe. Even if intelligent life occurs on only a minuscule percentage of planets around these stars, there might still be a great number of civilizations, and if the percentage were high enough it would produce a significant number of extant civilizations in the Milky Way. This assumes the mediocrity principle, by which the Earth is a typical planet.\n\nThe second aspect of the Fermi paradox is the argument of probability: given intelligent life's ability to overcome scarcity, and its tendency to colonize new habitats, it seems possible that at least some civilizations would be technologically advanced, seek out new resources in space, and colonize their own star system and, subsequently, surrounding star systems. Since there is no significant evidence on Earth, or elsewhere in the known universe, of other intelligent life after 13.8 billion years of the universe's history, there is a conflict requiring a resolution. Some examples of possible resolutions are that intelligent life is rarer than we think, that our assumptions about the general development or behavior of intelligent species are flawed, or, more radically, that our current scientific understanding of the nature of the universe itself is quite incomplete.\n\nThe Fermi paradox can be asked in two ways. The first is, \"Why are no aliens or their artifacts found here on Earth, or in the Solar System?\" If interstellar travel is possible, even the \"slow\" kind nearly within the reach of Earth technology, then it would only take from 5 million to 50 million years to colonize the galaxy. This is relatively brief on a geological scale, let alone a cosmological one. Since there are many stars older than the Sun, and since intelligent life might have evolved earlier elsewhere, the question then becomes why the galaxy has not been colonized already. Even if colonization is impractical or undesirable to all alien civilizations, large-scale \"exploration\" of the galaxy could be possible by probes. These might leave detectable artifacts in the Solar System, such as old probes or evidence of mining activity, but none of these have been observed.\n\nThe second form of the question is \"Why do we see no signs of intelligence elsewhere in the universe?\" This version does not assume interstellar travel, but includes other galaxies as well. For distant galaxies, travel times may well explain the lack of alien visits to Earth, but a sufficiently advanced civilization could potentially be observable over a significant fraction of the size of the observable universe. Even if such civilizations are rare, the scale argument indicates they should exist somewhere at some point during the history of the universe, and since they could be detected from far away over a considerable period of time, many more potential sites for their origin are within range of our observation. It is unknown whether the paradox is stronger for our galaxy or for the universe as a whole.\n\nIn 1950, while working at Los Alamos National Laboratory, Fermi had a casual conversation while walking to lunch with colleagues Emil Konopinski, Edward Teller and Herbert York. The men discussed a recent spate of UFO reports and an Alan Dunn cartoon facetiously blaming the disappearance of municipal trashcans on marauding aliens. The conversation shifted to other subjects, until during lunch Fermi suddenly exclaimed, \"Where are they?\" (alternatively, \"Where is everybody?\"). Teller remembers, \"The result of his question was general laughter because of the strange fact that in spite of Fermi's question coming from the clear blue, everybody around the table seemed to understand at once that he was talking about extraterrestrial life.\" Herbert York recalls that Fermi followed up on his comment with a series of calculations on the probability of Earth-like planets, the probability of life, the likely rise and duration of high technology, etc., and concluded that we ought to have been visited long ago and many times over.\n\nAlthough Fermi's name is most commonly associated with the paradox, he was not the first to ask the question. An earlier implicit mention was by Konstantin Tsiolkovsky in an unpublished manuscript from 1933. He noted \"people deny the presence of intelligent beings on the planets of the universe\" because \"(i) if such beings exist they would have visited Earth, and (ii) if such civilizations existed then they would have given us some sign of their existence.\" This was not a paradox for others, who took this to imply the absence of ETs, but it was for him, since he himself was a strong believer in extraterrestrial life and the possibility of space travel. Therefore, he proposed what is now known as the zoo hypothesis and speculated that mankind is not yet ready for higher beings to contact us. That Tsiolkovsky himself may not have been the first to discover the paradox is suggested by his above-mentioned reference to other people's reasons for denying the existence of extraterrestrial civilizations.\n\nMichael H. Hart published in 1975 a detailed examination of the paradox, which has since become a theoretical reference point for much of the research into what is now sometimes known as the Fermi–Hart paradox. Geoffrey A. Landis prefers that name on the grounds that \"while Fermi is credited with first asking the question, Hart was the first to do a rigorous analysis showing that the problem is not trivial, and also the first to publish his results\". Robert H. Gray argues that the term \"Fermi paradox\" is a misnomer, since in his view it is neither a paradox nor due to Fermi; he instead prefers the name Hart–Tipler argument, acknowledging Michael Hart as its originator, but also the substantial contribution of Frank J. Tipler in extending Hart's arguments.\n\nOther names closely related to Fermi's question (\"Where are they?\") include the Great Silence, and silentium universi (Latin for \"silence of the universe\"), though these only refer to one portion of the Fermi Paradox, that we see no evidence of other civilizations.\n\nThe theories and principles in the Drake equation are closely related to the Fermi paradox. The equation was formulated by Frank Drake in 1961 in an attempt to find a systematic means to evaluate the numerous probabilities involved in the existence of alien life. The speculative equation considers the rate of star formation in the galaxy; the fraction of stars with planets and the number per star that are habitable; the fraction of those planets that develop life; the fraction that develop \"intelligent\" life; the fraction that have detectable, technological intelligent life; and finally the length of time such communicable civilizations are detectable. The fundamental problem is that the last four terms are completely unknown, rendering statistical estimates impossible.\n\nThe Drake equation has been used by both optimists and pessimists, with wildly differing results. The first scientific meeting on the search for extraterrestrial intelligence (SETI), which had 10 attendees including Frank Drake and Carl Sagan, speculated that the number of civilizations was roughly equal to the lifetime in years, and there were probably between 1,000 and 100,000,000 civilizations in the Milky Way galaxy. Conversely, Frank Tipler and John D. Barrow used pessimistic numbers and speculated that the average number of civilizations in a galaxy is much less than one. Almost all arguments involving the Drake equation suffer from the overconfidence effect, a common error of probabilistic reasoning about low-probability events, by guessing specific numbers for likelihoods of events whose mechanism is not yet understood, such as the likelihood of abiogenesis on an Earth-like planet, with current likelihood estimates varying over many hundreds of orders of magnitude. An analysis that takes into account some of the uncertainty associated with this lack of understanding has been carried out by Anders Sandberg, Eric Drexler and Toby Ord, and suggests that with very high probability, either intelligent civilizations are plentiful in our galaxy or humanity is alone in the observable universe, with the lack of observation of intelligent civilizations pointing towards the latter option.\n\nThe Great Filter, in the context of the Fermi paradox, is whatever prevents \"dead matter\" from giving rise, in time, to expanding, lasting life according to the Kardashev scale. The most commonly agreed-upon low probability event is abiogenesis: a gradual process of increasing complexity of the first self-replicating molecules by a randomly occurring chemical process. Other proposed great filters are the emergence of eukaryotes or of meiosis or some of the steps involved in the evolution of a brain capable of complex logical deductions.\n\nThere are two parts of the Fermi paradox that rely on empirical evidence—that there are many potential habitable planets, and that we see no evidence of life. The first point, that many suitable planets exist, was an assumption in Fermi's time that is gaining ground with the discovery of many exoplanets, and models predicting billions of habitable worlds in our galaxy.\n\nThe second part of the paradox, that we see no evidence of extraterrestrial life, is also an active field of scientific research. This includes both efforts to find any indication of life, and efforts specifically directed to finding intelligent life. These searches have been made since 1960, and several are ongoing.\n\nAlthough astronomers do not usually search for extraterrestrials, they have observed phenomena that they could not immediately explain without positing an intelligent civilization as the source. For example, pulsars, when first discovered in 1967, were called little green men (LGM) because of the precise repetition of their pulses. In all cases, explanations with no need for intelligent life have been found for such observations, but the possibility of discovery remains. Proposed examples include asteroid mining that would change the appearance of debris disks around stars, or spectral lines from nuclear waste disposal in stars. An ongoing example is the unusual transit light curves of star KIC 8462852, where natural interpretations are not fully convincing. Although most likely a natural explanation will emerge, some scientists are investigating the remote possibility that it could be a sign of alien technology, such as a Dyson swarm.\n\nRadio technology and the ability to construct a radio telescope are presumed to be a natural advance for technological species, theoretically creating effects that might be detected over interstellar distances. The careful searching for non-natural radio emissions from space may lead to the detection of alien civilizations. Sensitive alien observers of the Solar System, for example, would note unusually intense radio waves for a G2 star due to Earth's television and telecommunication broadcasts. In the absence of an apparent natural cause, alien observers might infer the existence of a terrestrial civilization. It should be noted however that the most sensitive radio telescopes currently available on Earth would not be able to detect non-directional radio signals even at a fraction of a light-year, so it is questionable whether any such signals could be detected by an extraterrestrial civilization. Such signals could be either \"accidental\" by-products of a civilization, or deliberate attempts to communicate, such as the Arecibo message. A number of astronomers and observatories have attempted and are attempting to detect such evidence, mostly through the SETI organization. Several decades of SETI analysis have not revealed any unusually bright or meaningfully repetitive radio emissions.\n\nExoplanet detection and classification is a very active sub-discipline in astronomy, and the first possibly terrestrial planet discovered within a star's habitable zone was found in 2007. New refinements in exoplanet detection methods, and use of existing methods from space (such as the Kepler Mission, launched in 2009) are starting to detect and characterize Earth-size planets, and determine if they are within the habitable zones of their stars. Such observational refinements may allow us to better gauge how common potentially habitable worlds are.\n\nSelf-replicating probes could exhaustively explore a galaxy the size of the Milky Way in as little as a million years. If even a single civilization in the Milky Way attempted this, such probes could spread throughout the entire galaxy. Another speculation for contact with an alien probe—one that would be trying to find human beings—is an alien Bracewell probe. Such a hypothetical device would be an autonomous space probe whose purpose is to seek out and communicate with alien civilizations (as opposed to Von Neumann probes, which are usually described as purely exploratory). These were proposed as an alternative to carrying a slow speed-of-light dialogue between vastly distant neighbors. Rather than contending with the long delays a radio dialogue would suffer, a probe housing an artificial intelligence would seek out an alien civilization to carry on a close-range communication with the discovered civilization. The findings of such a probe would still have to be transmitted to the home civilization at light speed, but an information-gathering dialogue could be conducted in real time.\n\nDirect exploration of the Solar System has yielded no evidence indicating a visit by aliens or their probes. Detailed exploration of areas of the Solar System where resources would be plentiful may yet produce evidence of alien exploration, though the entirety of the Solar System is vast and difficult to investigate. Attempts to signal, attract, or activate hypothetical Bracewell probes in Earth's vicinity have not succeeded.\n\nIn 1959, Freeman Dyson observed that every developing human civilization constantly increases its energy consumption, and, he conjectured, a civilization might try to harness a large part of the energy produced by a star. He proposed that a Dyson sphere could be a possible means: a shell or cloud of objects enclosing a star to absorb and utilize as much radiant energy as possible. Such a feat of astroengineering would drastically alter the observed spectrum of the star involved, changing it at least partly from the normal emission lines of a natural stellar atmosphere to those of black body radiation, probably with a peak in the infrared. Dyson speculated that advanced alien civilizations might be detected by examining the spectra of stars and searching for such an altered spectrum.\n\nThere have been some attempts to find evidence of the existence of Dyson spheres that would alter the spectra of their core stars. Direct observation of thousands of galaxies has shown no explicit evidence of artificial construction or modifications. In October 2015, there was some speculation that a pattern of light from star KIC 8462852, observed by the Kepler Space Telescope, could have been a result of Dyson sphere construction.\n\nThose who think that intelligent extraterrestrial life is (nearly) impossible argue that the conditions needed for the evolution of life—or at least the evolution of biological complexity—are rare or even unique to Earth. Under this assumption, called the rare Earth hypothesis, a rejection of the mediocrity principle, complex multicellular life is regarded as exceedingly unusual.\n\nThe Rare Earth hypothesis argues that the evolution of biological complexity requires a host of fortuitous circumstances, such as a galactic habitable zone, a central star and planetary system having the requisite character, the circumstellar habitable zone, a right sized terrestrial planet, the advantage of a giant guardian like Jupiter and a large natural satellite, conditions needed to ensure the planet has a magnetosphere and plate tectonics, the chemistry of the lithosphere, atmosphere, and oceans, the role of \"evolutionary pumps\" such as massive glaciation and rare bolide impacts, and whatever led to the appearance of the eukaryote cell, sexual reproduction and the Cambrian explosion.\n\nIt is possible that even if complex life is common, intelligence (and consequently civilizations) is not.\nWhile there are remote sensing techniques that could perhaps detect life-bearing planets without relying on the signs of technology, none of them has any ability to tell if any detected life is intelligent. This is sometimes referred to as the \"algae vs. alumnae\" problem.\n\nIt may be that while alien species with intelligence exist, they are primitive or have not reached the level of technological advancement necessary to communicate. Along with non-intelligent life, such civilizations would be also very difficult for us to detect, short of a visit by a probe, a trip that would take hundreds of thousands of years with current technology.\nTo skeptics, the fact that in the history of life on the Earth only one species has developed a civilization to the point of being capable of spaceflight and radio technology, lends more credence to the idea that technologically advanced civilizations are rare in the universe.\n\nThis is the argument that technological civilizations may usually or invariably destroy themselves before or shortly after developing radio or spaceflight technology. Possible means of annihilation are many, including war, accidental environmental contamination or damage, resource depletion, climate change, or poorly designed artificial intelligence. This general theme is explored both in fiction and in scientific hypothesizing. In 1966, Sagan and Shklovskii speculated that technological civilizations will either tend to destroy themselves within a century of developing interstellar communicative capability or master their self-destructive tendencies and survive for billion-year timescales. Self-annihilation may also be viewed in terms of thermodynamics: insofar as life is an ordered system that can sustain itself against the tendency to disorder, the \"external transmission\" or interstellar communicative phase may be the point at which the system becomes unstable and self-destructs.\n\nAnother hypothesis is that an intelligent species beyond a certain point of technological capability will destroy other intelligent species as they appear. The idea that something, or someone, might be destroying intelligent life in the universe has been explored in the scientific literature. A species might undertake such extermination out of expansionist motives, paranoia, or aggression. In 1981, cosmologist Edward Harrison argued that such behavior would be an act of prudence: an intelligent species that has overcome its own self-destructive tendencies might view any other species bent on galactic expansion as a threat. It has also been suggested that a successful alien species would be a superpredator, as are humans. Another possibility invokes the \"tragedy of the commons\" and the anthropic principle: the first lifeform to achieve interstellar travel will necessarily (even if unintentionally) prevent competitors arising, and humans simply happen to be first.\n\nNew life might commonly die out due to runaway heating or cooling on their fledgling planets. On Earth, there have been numerous major extinction events that destroyed the majority of complex species alive at the time; the extinction of the dinosaurs is the best known example. These are thought to have been caused by events such as impact from a large meteorite, massive volcanic eruptions, or astronomical events such as gamma-ray bursts. It may be the case that such extinction events are common throughout the universe and periodically destroy intelligent life, or at least its civilizations, before the species is able to develop the technology to communicate with other intelligent species.\n\nUsing extinct civilizations such as Easter Island as models, a study conducted in 2018 posited that climate change induced by \"energy intensive\" civilizations may prevent sustainability within such civilizations, thus explaining the paradoxical lack of evidence for intelligent extra-terrestrial life.\n\nCosmologist Alan Guth proposed a multiverse solution to the Fermi paradox. This hypothesis uses the synchronous gauge probability distribution, with the result that young universes exceedingly outnumber older ones (by a factor of e for every second of age). Therefore, averaged over all universes, universes with civilizations will almost always have just one, the first to develop. However, Guth notes \"Perhaps this argument explains why SETI has not found any signals from alien civilizations, but I find it more plausible that it is merely a symptom that the synchronous gauge probability distribution is not the right one.\"\n\nIt may be that non-colonizing technologically capable alien civilizations exist, but that they are simply too far apart for meaningful two-way communication. If two civilizations are separated by several thousand light-years, it is possible that one or both cultures may become extinct before meaningful dialogue can be established. Human searches may be able to detect their existence, but communication will remain impossible because of distance. It has been suggested that this problem might be ameliorated somewhat if contact/communication is made through a Bracewell probe. In this case at least one partner in the exchange may obtain meaningful information. Alternatively, a civilization may simply broadcast its knowledge, and leave it to the receiver to make what they may of it. This is similar to the transmission of information from ancient civilizations to the present, and humanity has undertaken similar activities like the Arecibo message, which could transfer information about Earth's intelligent species, even if it never yields a response or does not yield a response in time for humanity to receive it. It is also possible that archaeological evidence of past civilizations may be detected through deep space observations.\n\nA related speculation by Sagan and Newman suggests that if other civilizations exist, and are transmitting and exploring, their signals and probes simply have not arrived yet. However, critics have noted that this is unlikely, since it requires that humanity's advancement has occurred at a very special point in time, while the Milky Way is in transition from empty to full. This is a tiny fraction of the lifespan of a galaxy under ordinary assumptions and calculations resulting from them, so the likelihood that we are in the midst of this transition is considered low in the paradox.\n\nMany speculations about the ability of an alien culture to colonize other star systems are based on the idea that interstellar travel is technologically feasible. While the current understanding of physics rules out the possibility of faster-than-light travel, it appears that there are no major theoretical barriers to the construction of \"slow\" interstellar ships, even though the engineering required is considerably beyond our present capabilities. This idea underlies the concept of the Von Neumann probe and the Bracewell probe as a potential evidence of extraterrestrial intelligence.\n\nIt is possible, however, that present scientific knowledge cannot properly gauge the feasibility and costs of such interstellar colonization. Theoretical barriers may not yet be understood, and the resources needed may be so great as to make it unlikely that any civilization could afford to attempt it. Even if interstellar travel and colonization are possible, they may be difficult, leading to a colonization model based on percolation theory. Colonization efforts may not occur as an unstoppable rush, but rather as an uneven tendency to \"percolate\" outwards, within an eventual slowing and termination of the effort given the enormous costs involved and the expectation that colonies will inevitably develop a culture and civilization of their own. Colonization may thus occur in \"clusters,\" with large areas remaining uncolonized at any one time.\n\nIf human-capability constructs in a machine such as mind uploading are possible, and it is possible to transfer such constructs over vast distances and rebuild on a remote machine, then it might not make a strong economic sense to travel the galaxy by spaceflight. After the first civilization have physically explored or colonized the galaxy, as well as sent such machines for easy explorations, then the subsequent civiliizations, after having contacted the first, may find it cheaper, faster, and easier to explore the galaxy through intelligent construct transfers to the machines built by the first civilization, which is cheaper than spaceflight by a factor of 10-10. However, since only a star system needs only one such remote machine, and the communication is most likely highly directed, transmitted at high-frequencies and at a minimal power to be economical, such signals would be hard to detect from earth.\n\nHumanity's ability to detect intelligent extraterrestrial life has existed for only a very brief period—from 1937 onwards, if the invention of the radio telescope is taken as the dividing line—and \"Homo sapiens\" is a geologically recent species. The whole period of modern human existence to date is a very brief period on a cosmological scale, and radio transmissions have only been propagated since 1895. Thus, it remains possible that human beings have neither existed long enough nor made themselves sufficiently detectable to be found by extraterrestrial intelligence.\n\nThere are some assumptions that underlie the SETI programs that may cause searchers to miss signals that are present. Extraterrestrials might, for example, transmit signals that have a very high or low data rate, or employ unconventional (in our terms) frequencies, which would make them hard to distinguish from background noise. Signals might be sent from non-main sequence star systems that we search with lower priority; current programs assume that most alien life will be orbiting Sun-like stars.\n\nThe greatest challenge is the sheer size of the radio search needed to look for signals (effectively spanning the entire observable universe), the limited amount of resources committed to SETI, and the sensitivity of modern instruments. SETI estimates, for instance, that with a radio telescope as sensitive as the Arecibo Observatory, Earth's television and radio broadcasts would only be detectable at distances up to 0.3 light-years, less than 1/10 the distance to the nearest star. A signal is much easier to detect if the signal energy is limited to either a narrow range of frequencies, or directed at a specific part of the sky. Such signals could be detected at ranges of hundreds to tens of thousands of light-years distance. However, this means that detectors must be listening to an appropriate range of frequencies, and be in that region of space to which the beam is being sent. Many SETI searches assume that extraterrestrial civilizations will be broadcasting a deliberate signal, like the Arecibo message, in order to be found.\n\nThus to detect alien civilizations through their radio emissions, Earth observers either need more sensitive instruments or must hope for fortunate circumstances: that the broadband radio emissions of alien radio technology are much stronger than our own; that one of SETI's programs is listening to the correct frequencies from the right regions of space; or that aliens are deliberately sending focused transmissions in our general direction.\n\nIt may be that alien civilizations are detectable through their radio emissions for only a short time, reducing the likelihood of spotting them. The usual assumption is that civilizations outgrow radio through technological advancement. However, even if radio is not used for communication, it may be used for other purposes such as power transmission from solar power satellites. Such uses may remain visible even after broadcast emission are replaced by less observable technology.\n\nMore hypothetically, advanced alien civilizations may evolve beyond broadcasting at all in the electromagnetic spectrum and communicate by technologies not developed or used by mankind. Some scientists have hypothesized that advanced civilizations may send neutrino signals. If such signals exist, they could be detectable by neutrino detectors that are now under construction for other goals.\n\nIt has been suggested that some advanced beings may divest themselves of physical form, create massive artificial virtual environments, transfer themselves into these environments through mind uploading, and exist totally within virtual worlds, ignoring the external physical universe.\n\nIt may also be that intelligent alien life develops an \"increasing disinterest\" in their outside world. Possibly any sufficiently advanced society will develop highly engaging media and entertainment well before the capacity for advanced space travel, and that the rate of appeal of these social contrivances is destined, because of their inherent reduced complexity, to overtake any desire for complex, expensive endeavors such as space exploration and communication. Once any sufficiently advanced civilization becomes able to master its environment, and most of its physical needs are met through technology, various \"social and entertainment technologies\", including virtual reality, are postulated to become the primary drivers and motivations of that civilization.\n\nAnother possibility is that human theoreticians have underestimated how much alien life might differ from that on Earth. Aliens may be psychologically unwilling to attempt to communicate with human beings. Perhaps human mathematics is parochial to Earth and not shared by other life, though others argue this can only apply to abstract math since the math associated with physics must be similar (in results, if not in methods).\n\nPhysiology might also cause a communication barrier. Carl Sagan speculated that an alien species might have a thought process orders of magnitude slower (or faster) than ours. A message broadcast by that species might well seem like random background noise to us, and therefore go undetected.\n\nAnother thought is that technological civilizations invariably experience a technological singularity and attain a post-biological character. Hypothetical civilizations of this sort may have advanced drastically enough to render communication impossible.\n\nAlien civilizations might be technically capable of contacting Earth, but are only listening instead of transmitting. If all, or even most, civilizations act the same way, the galaxy could be full of civilizations eager for contact, but everyone is listening and no one is transmitting. This is the so-called \"SETI Paradox\".\n\nThe only civilization we know, our own, does not explicitly transmit, except for a few small efforts. Even these efforts, and certainly any attempt to expand them, are controversial. It is not even clear we would respond to a detected signal—the official policy within the SETI community is that \"[no] response to a signal or other evidence of extraterrestrial intelligence should be sent until appropriate international consultations have taken place.\" However, given the possible impact of any reply it may be very difficult to obtain any consensus on \"Who speaks for Earth?\" and \"What should we say?\"\n\nThe zoo hypothesis states that intelligent extraterrestrial life exists and does not contact life on Earth to allow for its natural evolution and development. This hypothesis may break down under the uniformity of motive flaw: all it takes is a single culture or civilization to decide to act contrary to the imperative within our range of detection for it to be abrogated, and the probability of such a violation increases with the number of civilizations.\n\nAnalysis of the inter-arrival times between civilizations in the galaxy based on common astrobiological assumptions suggests that the initial civilization would have a commanding lead over the later arrivals. As such, it may have established what we call the \"zoo hypothesis\" through force or as a galactic/universal norm and the resultant \"paradox\" by a cultural founder effect with or without the continued activity of the founder.\n\nIt is possible that a civilization advanced enough to travel between solar systems could be actively visiting or observing Earth while remaining undetected or unrecognized.\n\nA related idea to the zoo hypothesis is that, beyond a certain distance, the perceived universe is a simulated reality. The planetarium hypothesis speculates that beings may have created this simulation so that the universe appears to be empty of other life.\n\nAn alien civilization might feel it is too dangerous to communicate, either for us or for them. After all, when very different civilizations have met on Earth, the results have often been disastrous for one side or the other, and the same may well apply to interstellar contact. Even contact at a safe distance could lead to infection by computer code or even ideas themselves. Perhaps prudent civilizations actively hide not only from Earth but from everyone, out of fear of other civilizations.\n\nPerhaps the Fermi paradox itself—or the alien equivalent of it—is the reason for any civilization to avoid contact with other civilizations, even if no other obstacles existed. From any one civilization's point of view, it would be unlikely for them to be the first ones to make first contact. Therefore, according to this reasoning, it is likely that previous civilizations faced fatal problems with first contact and doing so should be avoided. So perhaps every civilization keeps quiet because of the possibility that there is a real reason for others to do so.\n\nLiu Cixin's novel The Dark Forest is based upon such a situation.\n\nA significant fraction of the population believes that at least some UFOs (Unidentified Flying Objects) are spacecraft piloted by aliens. While most of these are unrecognized or mistaken interpretations of mundane phenomena, there are those that remain puzzling even after investigation. The consensus scientific view is that although they may be unexplained, they do not rise to the level of convincing evidence.\n\nSimilarly, it is theoretically possible that SETI groups are not reporting positive detections, or governments have been blocking signals or suppressing publication. This response might be attributed to security or economic interests from the potential use of advanced extraterrestrial technology. It has been suggested that the detection of an extraterrestrial radio signal or technology could well be the most highly secret information that exists. Claims that this has already happened are common in the popular press, but the scientists involved report the opposite experience—the press becomes informed and interested in a potential detection even before a signal can be confirmed.\n\n\n\n\n"}
{"id": "246747", "url": "https://en.wikipedia.org/wiki?curid=246747", "title": "Giant salamander", "text": "Giant salamander\n\nThe Cryptobranchidae are a family of fully aquatic salamanders commonly known as the giant salamanders. A single species, the hellbender (\"Cryptobranchus alleganiensis\") occurs in the eastern United States, while Asian species occur in China and Japan. They are the largest living amphibians known today. The Japanese giant salamander (\"Andrias japonicus\") reaches up to in length, feeds at night on fish and crustaceans, and has been known to live for more than 50 years in captivity. The Chinese giant salamander (\"Andrias davidianus\") can reach a length of .\n\nThe family name is from the Ancient Greek \"krypto\" (\"hidden\"), and \"branch\" (\"gill\"), which refer to how the members absorb oxygen through capillaries of their side-frills, which function as gills.\n\nClade Pancryptobrancha (Cryptobranchidae + Ukrainurus)\n\nExtant species in the family Cryptobranchidae are the modern-day members of a lineage that extends back millions of years; the earliest fossil records of a basal species date back to the Middle Jurassic and were found in volcanic deposits in northern China. These specimens are the earliest known relatives of modern salamanders, and together with the numerous other basal groups of salamanders found in the Asian fossil record, they form a firm base of evidence for the fact that \"the early diversification of salamanders was well underway\" in Asia during the Jurassic period. Little has changed in the morphology of the Cryptobranchidae since the time of these fossils, leaving researchers to note \"extant cryptobranchid salamanders can be regarded as living fossils whose structures have remained little changed for over 160 million years.\"\n\nAs the fossil record for the Cryptobranchidae shows an Asian origin for the family, how these salamanders made it to the eastern US has been a point of scientific interest. Research has indicated a dispersal via land bridge, with waves of adaptive radiation seeming to have swept the Americas from north to south.\nIn 1726, the Swiss physician Johann Jakob Scheuchzer described a fossil as \"Homo diluvii testis\" (Latin: Evidence of a diluvian human), believing it to be the remains of a human being who drowned in the biblical flood. The Teylers Museum in Haarlem, Netherlands, bought the fossil in 1802, where it is still exhibited. In 1812, the fossil was examined by Georges Cuvier, who recognized that it was not human. After being identified as a salamander, it was renamed \"Salamandra scheuchzeri\" by Holl in 1831. The genus \"Andrias\" was coined six years later by Tschudi. In doing so, both the genus, \"Andrias\" (which means \"image of man\"), and the specific name, \"scheuchzeri\", ended up honouring Scheuchzer and his beliefs. It and the extant \"A. davidianus\" cannot be mutually diagnosed, and the latter, only described in 1871, is therefore sometimes considered a synonym of the former.\n\nCryptobranchids are large salamanders, with large folds of skin along their flanks. These help increase the animals' surface area, allowing them to absorb more oxygen from the water. They have four toes on the fore limbs, and five on the hind limbs. Their metamorphosis from the larval stage is incomplete, so the adults retain gill slits (although they also have lungs), and lack eyelids.They have bad eyesight. They can reach a length of , though most are considerably smaller today.\n\nIn Japan, their natural habitats are threatened by dam-building. Ramps and staircases have been added to some dams to allow them to move upstream to areas where they spawn.\nThe Japanese giant salamander has lived for as long as 52 years in captivity.\n\nThe Chinese giant salamander eats aquatic insects, fish, frogs, crabs, and shrimp. They hunt mainly at night. As they have poor eyesight, they use sensory nodes on their heads and bodies to detect minute changes in water pressure, enabling them to find their prey.\n\nDuring mating season, the salamanders travel upstream, where the female lays two strings of over 200 eggs each. The male fertilizes the eggs externally by releasing his sperm onto them, and then guards them for at least three months, until they hatch. At this point, the larvae live off their noticeable stored fat until ready to hunt. Once ready, they hunt as a group rather than individually.\n\nScientists at Hiroshima City Asa Zoological Park in Japan have recently discovered the male salamander will spawn with more than one female in his den. On occasion, the male \"den master\" will also allow a second male into the den; the reason for this is unclear.\n\n\n"}
{"id": "12286", "url": "https://en.wikipedia.org/wiki?curid=12286", "title": "Great Plague of London", "text": "Great Plague of London\n\nThe Great Plague, lasting from 1665–1666, was the last major epidemic of the bubonic plague to occur in England. It happened within the centuries-long time period of the Second Pandemic, an extended period of intermittent bubonic plague epidemics which began in China in 1331, the first year of the Black Death, an outbreak which included other forms such as pneumonic plague, and lasted until 1750.\n\nThe Great Plague killed an estimated 100,000 people—almost a quarter of London's population—in 18 months. The plague was caused by the \"Yersinia pestis\" bacterium, which is usually transmitted through the bite of an infected rat flea.\n\nThe 1665–66 epidemic was on a far smaller scale than the earlier Black Death pandemic; it was remembered afterwards as the \"great\" plague mainly because it was the last widespread outbreak of bubonic plague in England during the 400-year timespan of the Second Pandemic.\n\nAs in other European cities of the period, the plague was endemic in 17th century London.\nThe disease periodically erupted into massive epidemics. There were 30,000 deaths due to the plague in 1603, 35,000 in 1625, and 10,000 in 1636, as well as smaller numbers in other years.\n\nDuring the winter of 1664, a bright comet was to be seen in the sky and the people of London were fearful, wondering what evil event it portended. London at that time consisted of a city of about 448 acres surrounded by a city wall, which had originally been built to keep out raiding bands. There were gates at Ludgate, Newgate, Aldersgate, Cripplegate, Moorgate and Bishopsgate and to the south lay the River Thames and London Bridge. In the poorer parts of the city, hygiene was impossible to maintain in the overcrowded tenements and garrets. There was no sanitation, and open drains flowed along the centre of winding streets. The cobbles were slippery with animal dung, rubbish and the slops thrown out of the houses, muddy and buzzing with flies in summer and awash with sewage in winter. The City Corporation employed \"rakers\" to remove the worst of the filth and it was transported to mounds outside the walls where it accumulated and continued to decompose. The stench was overwhelming and people walked around with handkerchiefs or nosegays pressed against their nostrils.\n\nSome of the city's necessities such as coal arrived by barge, but most came by road. Carts, carriages, horses and pedestrians were crowded together and the gateways in the wall formed bottlenecks through which it was difficult to progress. The nineteen-arch London Bridge was even more congested. The better-off used hackney carriages and sedan chairs to get to their destinations without getting filthy. The poor walked, and might be splashed by the wheeled vehicles and drenched by slops being thrown out and water falling from the overhanging roofs. Another hazard was the choking black smoke belching forth from factories which made soap, from breweries and iron smelters and from about 15,000 houses burning coal.\n\nOutside the city walls, suburbs had sprung up providing homes for the craftsmen and tradespeople who flocked to the already overcrowded city. These were shanty towns with wooden shacks and no sanitation. The government had tried to control this development but had failed and over a quarter of a million people lived here. Other immigrants had taken over fine town houses, vacated by Royalists who had fled the country during the Commonwealth, converting them into tenements with different families in every room. These properties were soon vandalised and became rat-infested slums.\n\nAdministration of the City of London was organised by the Lord Mayor, Aldermen and common councillors, but not all of the inhabited area generally comprising London was legally part of the City. Both inside the City and outside its boundaries there were also Liberties, which were areas of varying sizes which historically had been granted rights to self-government. Many had been associated with religious institutions, and when these were abolished in the Dissolution of the Monasteries, their historic rights were transferred along with their property to new owners. The walled City was surrounded by a ring of Liberties which had come under its authority, contemporarily called 'the City and Liberties', but these were surrounded by further suburbs with varying administrations. Westminster was an independent town with its own liberties, although it was joined to London by urban development. The Tower of London was an independent liberty, as were others. Areas north of the river not part of one of these administrations came under the authority of the county of Middlesex, and south of the river under Surrey.\nAt that time, bubonic plague was a much feared disease but its cause was not understood. The credulous blamed emanations from the earth, \"pestilential effluviums\", unusual weather, sickness in livestock, abnormal behaviour of animals or an increase in the numbers of moles, frogs, mice or flies. It was not until 1894 that the identification by Alexandre Yersin of its causal agent \"Yersinia pestis\" was made and the transmission of the bacterium by rat fleas became known. Although the Great Plague in London had long been believed to be bubonic plague caused by \"Yersinia pestis\", this was only definitively confirmed by DNA analysis in 2016.\n\nThe third pandemic of the plague started in 1855 in China and eventually killed about 15 million people, mainly in India. In 1894, the plague hit Hong Kong, a major trade port between China and US.\n\nIn order to judge the severity of an epidemic, it is first necessary to know how big the population was in which it occurred. There was no official census of the population to provide this figure, and the best contemporary count comes from the work of John Graunt (1620–1674), who was one of the earliest Fellows of the Royal Society and one of the first demographers, bringing a scientific approach to the collection of statistics. In 1662, he estimated that 384,000 people lived in the City of London, the Liberties, Westminster and the out-parishes, based on figures in the bills of mortality published each week in the capital. These different districts with different administrations constituted the officially recognised extent of London as a whole. In 1665, he revised his estimate to 'not above 460,000'. Other contemporaries put the figure higher, (the French Ambassador, for example, suggested 600,000) but with no mathematical basis to support their estimates. The next largest city in the kingdom was Norwich, with a population of 30,000.\nThere was no duty to report a death to anyone in authority. Instead, each parish appointed two or more 'searchers of the dead', whose duty was to inspect a corpse and determine the cause of death. A searcher was entitled to charge a small fee from relatives for each death they reported, and so habitually the parish would appoint someone to the post who would otherwise be destitute and would be receiving support from the parish poor rate. Typically, this meant searchers would be old women who were illiterate, might know little about identifying diseases and who would be open to dishonesty. Searchers would typically learn about a death either from the local sexton who had been asked to dig a grave, or from the tolling of a church bell. Anyone who did not report a death to their local church, such as Quakers, Anabaptists, other non-Anglican Christians or Jews, frequently did not get included in the official records. Searchers during times of plague were required to live apart from the community and stay indoors except when performing their duties, for fear of spreading the diseases. Outside they should avoid other people and always carry a white stick to warn of their occupation. Searchers reported to the Parish Clerk, who made a return each week to the Company of Parish Clerks in Brode Lane. Figures were then passed to the Lord Mayor and then to the Minister of State once plague became a matter of national concern. The reported figures were used to compile the Bills of Mortality, which listed total deaths in each parish and whether by plague. The system of Searchers to report the cause of death continued until 1836.\n\nGraunt recorded the incompetence of the Searchers at identifying true causes of death, remarking on the frequent recording of 'consumption' rather than other diseases which were recognised then by physicians. He suggested a cup of ale and a doubling of their fee to two groats rather than one was sufficient for Searchers to change the cause of death to one more convenient for the householders. No one wished to be known as having had a death by plague in their household, and Parish Clerks, too, connived in covering up cases of plague in their official returns. Analysis of the Bills of Mortality during the months plague took hold shows a rise in deaths other than by plague well above the average death rate, which has been attributed to misrepresentation of the true cause of death. As plague spread, a system of quarantine was introduced, whereby any house where someone had died from plague would be locked up and no one allowed to enter or leave for 40 days. This frequently led to the deaths of the other inhabitants, by neglect if not from plague, and provided ample incentive not to report the disease. The official returns record 68,596 cases of plague, but a reasonable estimate suggests this figure is 30,000 short of the true total. A plague house was marked with a red cross on the door with the words \"Lord have mercy upon us\", and a watchman stood guard outside.\n\nReports of plague around Europe began to reach England in the 1660s, causing the Privy Council to consider what steps might be taken to prevent it crossing to England. Quarantining of ships had been used during previous outbreaks and was again introduced for ships coming to London in November 1663, following outbreaks in Amsterdam and Hamburg. Two naval ships were assigned to intercept any vessels entering the Thames estuary. Ships from infected ports were required to moor at Hole Haven on Canvey Island for a period of 30 days before being allowed to travel upriver. Ships from ports free of plague or completing their quarantine were given a certificate of health and allowed to travel on. A second inspection line was established between the forts on opposite banks of the Thames at Tilbury and Gravesend with instructions only to pass ships with a certificate.\n\nThe duration of quarantine was increased to forty days in May 1664 as the continental plague worsened, and the areas subject to quarantine changed with the news of the spread of plague to include all of Holland, Zeeland and Friesland (all regions of the Dutch Republic), although restrictions on Hamburg were removed in November. Quarantine measures against ships coming from the Dutch Republic were put in place in 29 other ports from May, commencing with Great Yarmouth. The Dutch ambassador objected at the constraint of trade with his country, but England responded that it had been one of the last countries introducing such restrictions. Regulations were enforced quite strictly, so that people or houses where voyagers had come ashore without serving their quarantine were also subjected to 40 days quarantine.\n\nPlague was one of the hazards of life in Britain from its dramatic appearance in 1348 with the Black Death. The Bills of Mortality began to be published regularly in 1603, in which year 33,347 deaths were recorded from plague. Between then and 1665, only four years had no recorded cases. In 1563, a thousand people were reportedly dying in London each week. In 1593, there were 15,003 deaths, 1625 saw 41,313 dead, between 1640 and 1646 came 11,000 deaths, culminating in 3,597 for 1647. The 1625 outbreak was recorded at the time as the 'Great Plague', Until 1665, surpassed it. These official figures are likely to under-report actual numbers.\n\nAlthough plague was known, it was still sufficiently uncommon that medical practitioners might have had no personal experience of seeing the disease; medical training varied from those who had attended the college of physicians, to apothecaries who also acted as modern doctors, to simple charlatans. Other diseases abounded, such as an outbreak of smallpox the year before, and these uncertainties all added to difficulties identifying the true start of the epidemic. Contemporary accounts suggest cases of plague occurred through the winter of 1664/5, some of which were fatal but a number of which did not display the virulence of the later epidemic. The winter was cold, the ground frozen from December to March, river traffic on the Thames twice blocked by ice, and it may be that the cold weather held back its spread.\n\nThis outbreak of bubonic plague in England is thought to have spread from the Netherlands, where the disease had been occurring intermittently since 1599. It is unclear exactly where the disease first struck but the initial contagion may have arrived with Dutch trading ships carrying bales of cotton from Amsterdam, which was ravaged by the disease in 1663–1664, with a mortality given of 50,000. The first areas to be struck are believed to be the dock areas just outside London, and the parish of St Giles in the Fields. In both of these localities, poor workers were crowded into ill-kept structures. Two suspicious deaths were recorded in St. Giles parish in 1664 and another in February 1665. These did not appear as plague deaths on the Bills of Mortality, so no control measures were taken by the authorities, but the total number of people dying in London during the first four months of 1665 showed a marked increase. By the end of April, only four plague deaths had been recorded, two in the parish of St. Giles, but total deaths per week had risen from around 290 to 398.\n\nAlthough there had been only three official cases in April, which level of plague in earlier years had not induced any official response, the Privy Council now acted to introduce household quarantine. Justices of the Peace in Middlesex were instructed to investigate any suspected cases and to shut up the house if it was confirmed. Shortly after, a similar order was issued by the King's Bench to the City and Liberties. A riot broke out in St. Giles when the first house was sealed up; the crowd broke down the door and released the inhabitants. Rioters caught were punished severely. Instructions were given to build pest-houses, which were essentially isolation hospitals built away from other people where the sick could be cared for (or stay until they died). This official activity suggests that despite the few recorded cases, the government was already aware that this was a serious outbreak of plague.\n\nWith the arrival of warmer weather, the disease began to take a firmer hold. In the week 2–9 May, there were three recorded deaths in the parish of St Giles, four in neighbouring St Clement Danes and one each in St Andrew, Holborn and St Mary Woolchurch Haw. Only the last was actually inside the city walls. A Privy Council committee was formed to investigate methods to best prevent the spread of plague, and measures were introduced to close some of the ale houses in affected areas and limit the number of lodgers allowed in a household. In the city, the Lord Mayor issued a proclamation that all householders must diligently clean the streets outside their property, which was a householder's responsibility, not a state one (the city employed scavengers and rakers to remove the worst of the mess). Matters just became worse, and Aldermen were instructed to find and punish those failing their duty. As cases in St. Giles began to rise, an attempt was made to quarantine the area and constables were instructed to inspect everyone wishing to travel and contain inside vagrants or suspect persons.\n\nPeople began to be alarmed. Samuel Pepys, who had an important position at the Admiralty, stayed in London and provided a contemporary account of the plague through his diary. On 30 April he wrote: \"Great fears of the sickness here in the City it being said that two or three houses are already shut up. God preserve us all!\" Another source of information on the time is a fictional account, \"A Journal of the Plague Year\", which was written by Daniel Defoe and published in 1722. He had been only six when the plague struck but made use of his family's recollections (his uncle was a saddler in East London and his father a butcher in Cripplegate), interviews with survivors and sight of such official records as were available.\n\nBy July 1665, plague was rampant in the City of London. The rich ran away including King Charles II of England, his family and his court left the city for Salisbury, moving on to Oxford in September when some cases of plague occurred in Salisbury. The aldermen and most of the other city authorities opted to stay at their posts. The Lord Mayor of London, Sir John Lawrence, also decided to stay in the city. Businesses were closed when merchants and professionals fled. Defoe wrote \"Nothing was to be seen but wagons and carts, with goods, women, servants, children, coaches filled with people of the better sort, and horsemen attending them, and all hurrying away\". As the plague raged throughout the summer, only a small number of clergymen, physicians and apothecaries remained to cope with an increasingly large number of victims. Edward Cotes, author of \"London's Dreadful Visitation\", expressed the hope that \"Neither the Physicians of our Souls or Bodies may hereafter in such great numbers forsake us\".\n\nThe poorer people were also alarmed by the contagion and some left the city, but it was not easy for them to abandon their accommodation and livelihoods for an uncertain future elsewhere. Before exiting through the city gates, they were required to possess a certificate of good health signed by the Lord Mayor and these became increasingly difficult to obtain. As time went by and the numbers of plague victims rose, people living in the villages outside London began to resent this exodus and were no longer prepared to accept townsfolk from London, with or without a certificate. The refugees were turned back, were not allowed to pass through towns and had to travel across country, and were forced to live rough on what they could steal or scavenge from the fields. Many died in wretched circumstances of starvation and thirst in the hot summer that was to follow.\n\nIn the last week of July, the London Bill of Mortality showed 3,014 deaths, of which 2,020 had died from the plague. The number of deaths as a result of plague may have been underestimated, as deaths in other years in the same period were much lower, at around 300. As the number of victims affected mounted up, burial grounds became overfull, and pits were dug to accommodate the dead. Drivers of dead-carts travelled the streets calling \"Bring out your dead\" and carted away piles of bodies. The authorities became concerned that the number of deaths might cause public alarm and ordered that body removal and interment should take place only at night. As time went on, there were too many victims, and too few drivers, to remove the bodies which began to be stacked up against the walls of houses. Daytime collection was resumed and the plague pits became mounds of decomposing corpses. In the parish of Aldgate, a great hole was dug near the churchyard, fifty feet long and twenty feet wide. Digging was continued by labourers at one end while the dead-carts tipped in corpses at the other. When there was no room for further extension it was dug deeper until ground water was reached at twenty feet. When finally covered with earth it housed 1,114 corpses.\n\nPlague doctors traversed the streets diagnosing victims, although many of them had no formal medical training. Several public health efforts were attempted. Physicians were hired by city officials and burial details were carefully organized, but panic spread through the city and, out of the fear of contagion, people were hastily buried in overcrowded pits. The means of transmission of the disease were not known but thinking they might be linked to the animals, the City Corporation ordered a cull of dogs and cats. This decision may have affected the length of the epidemic since those animals could have helped keep in check the rat population carrying the fleas which transmitted the disease. Thinking bad air was involved in transmission, the authorities ordered giant bonfires to be burned in the streets and house fires to be kept burning night and day, in hopes that the air would be cleansed. Tobacco was thought to be a prophylactic and it was later said that no London tobacconist had died from the plague during the epidemic.\n\nTrade and business had completely dried up, and the streets were empty of people except for the dead-carts and the desperate dying victims, as witnessed and recorded by Samuel Pepys in his diary: \" Lord! How empty the streets are and how melancholy, so many poor sick people in the streets full of sores… in Westminster, there is never a physician and but one apothecary left, all being dead.\" That people did not starve was down to the foresight of Sir John Lawrence and the Corporation of London who arranged for a commission of one farthing to be paid above the normal price for every quarter of corn landed in the Port of London. Another food source was the villages around London which, denied of their usual sales in the capital, left vegetables in specified market areas, negotiated their sale by shouting, and collected their payment after the money had been left submerged in a bucket of water to \"disinfect\" the coins.\n\nRecords state that plague deaths in London and the suburbs crept up over the summer from 2,000 people per week to over 7,000 per week in September. These figures are likely to be a considerable underestimate. Many of the sextons and parish clerks who kept the records themselves died. Quakers refused to co-operate and many of the poor were just dumped into mass graves unrecorded. It is not clear how many people caught the disease and made a recovery because only deaths were recorded and many records were destroyed in the Great Fire of London the following year. In the few districts where intact records remain, plague deaths varied between 30% and over 50% of the total population.\n\nAlthough concentrated in London, the outbreak affected other areas of the country as well. Perhaps the most famous example was the village of Eyam in Derbyshire. The plague allegedly arrived with a merchant carrying a parcel of cloth sent from London, although this is a disputed point. The villagers imposed a quarantine on themselves to stop the further spread of the disease. This prevented the disease from moving into surrounding areas but the cost to the village was the death of around 80% of its inhabitants over a period of fourteen months.\n\nBy late autumn, the death toll in London and the suburbs began to slow until, in February 1666, it was considered safe enough for the King and his entourage to come back to the city. With the return of the monarch, others began to return: The gentry returned in their carriages accompanied by carts piled high with their belongings. The judges moved back from Windsor to sit in Westminster Hall, although Parliament, which had been prorogued in April 1665, did not reconvene until September 1666. Trade recommenced and businesses and workshops opened up. London was the goal of a new wave of people who flocked to the city in expectation of making their fortunes. Writing at the end of March 1666, Lord Clarendon, the Lord Chancellor, stated \"... the streets were as full, the Exchange as much crowded, the people in all places as numerous as they had ever been seen ...\".\n\nPlague cases continued to occur sporadically at a modest rate until the summer of 1666. On the second and third of September that year, the Great Fire of London destroyed much of the City of London, and some people believed that the fire put an end to the epidemic. However, it is now thought that the plague had largely subsided before the fire took place. In fact, most of the later cases of plague were found in the suburbs, and it was the City of London itself that was destroyed by the Fire.\n\nAccording to the Bills of Mortality, there were in total 68,596 deaths in London from the plague in 1665. Lord Clarendon estimated that the true number of mortalities was probably twice that figure. The next year, 1666, saw further deaths in other cities but on a lesser scale. Dr Thomas Gumble, chaplain to the Duke of Albemarle, both of whom had stayed in London for the whole of the epidemic, estimated that the total death count for the country from plague during 1665 and 1666 was about 200,000.\n\nThe Great Plague of 1665/1666 was the last major outbreak of bubonic plague in Great Britain. The last recorded death from plague came in 1679, and it was removed as a specific category in the Bills of Mortality after 1703. It spread to other towns in East Anglia and the southeast of England but fewer than ten percent of parishes outside London had a higher than average death rate during those years. Urban areas were more affected than rural ones; Norwich, Ipswich, Colchester, Southampton and Winchester were badly affected, while the West of England and areas of the Midlands escaped altogether.\n\nThe population of England in 1650 was approximately 5.25 million, which declined to about 4.9 million by 1680, recovering to just over 5 million by 1700. Other diseases, such as smallpox, took a high toll on the population even without the contribution by plague. The higher death rate in cities, both generally and specifically from the plague, was made up by continuous immigration, from small towns to larger ones and from the countryside to the town.\n\nThere were no contemporary censuses of London's population, but available records suggest that the population returned to its previous level within a couple of years. Burials in 1667 had returned to 1663 levels, Hearth Tax returns had recovered, John Graunt contemporarily analysed baptism records and concluded they represented a recovered population. Part of this could be accounted for by the return of wealthy households, merchants and manufacturing industries, all of which needed to replace losses among their staff and took steps to bring in necessary people. Colchester had suffered more severe depopulation, but manufacturing records for cloth suggested that production had recovered or even increased by 1669, and the total population had nearly returned to pre-plague levels by 1674. Other towns did less well: Ipswich was affected less than Colchester, but in 1674, its population had dropped by 18%, more than could be accounted for by the plague deaths alone.\n\nAs a proportion of the population who died, the London death toll was less severe than in a number of other towns. The total of deaths in London was greater than in any previous outbreak for 100 years, though as a proportion of the population, the epidemics in 1563, 1603 and 1625 were comparable or greater. Perhaps around 2.5% of the English population died.\n\nThe plague in London largely affected the poor, as the rich were able to leave the city by either retiring to their country estates or residing with kin in other parts of the country. The subsequent Great Fire of London, however, ruined many city merchants and property owners. As a result of these events, London was largely rebuilt and Parliament enacted the Rebuilding of London Act 1666. Although the street plan of the capital remained relatively unchanged, some improvements were made: streets were widened, pavements were created, open sewers abolished, wooden buildings and overhanging gables forbidden, and the design and construction of buildings controlled. The use of brick or stone was mandatory and many gracious buildings were constructed. Not only was the capital rejuvenated, but it became a healthier environment in which to live. Londoners had a greater sense of community after they had overcome the great adversities of 1665 and 1666.\n\nRebuilding took over ten years and was supervised by Robert Hooke as Surveyor of London. The architect Sir Christopher Wren was involved in the rebuilding of St Paul's Cathedral and more than fifty London churches. King Charles ll did much to foster the rebuilding work. He was a patron of the arts and sciences and founded the Royal Observatory and supported the Royal Society, a scientific group whose early members included Robert Hooke, Robert Boyle and Sir Isaac Newton. In fact, out of the fire and pestilence flowed a renaissance in the arts and sciences in England.\n\nPlague pits have been archaeologically excavated during underground construction work. Between 2011 and 2015, some 3,500 burials from the 'New Churchyard' or 'Bethlam burial ground' were discovered during the construction of the Crossrail railway at Liverpool Street. \"Yersinia pestis\" DNA was found in the teeth of individuals found buried in pits at the site, confirming they had died of bubonic plague.\n\n\n\n"}
{"id": "49160336", "url": "https://en.wikipedia.org/wiki?curid=49160336", "title": "Habiba Msika", "text": "Habiba Msika\n\nHabiba Msika, also spelled Messika (حبيبة مسيكة), (born 1903 Testour – February 21, 1930 Tunis), was a Tunisian singer, dancer and actress. \nBorn Marguerite Msika, she was the niece of singer Leila Sfez.\n\nShe quickly climbed the ladder of fame under the pseudonym Habiba (\"beloved\"). Prototype of the free, and master of her destiny, charismatic singer and daring actress, adored by Aboriginal Tunisian population, Msika was a social phenomenon in her time. The film \"The Fire Dance\" by Salma Baccar talks about her career.\n\nShe was born in the Jewish quarter of Tunis in a poor family. His parents, Daida and Maïha, worked in the wire trade.\n\nShe learned to read and write in the school of the Israelite covenant, which she left after seven years to follow, through the help of her aunt, singing lessons, music theory and classical Arabic with the famous composer Khemaïs Tarnane and Egyptian tenor Hassan Bannan.\n\nShe married her cousin Victor Chetboun but their union lasted a short time.\n\nHer first recital was held at the palace of La Marsa, where she met her Pygmalion and lover: the Minister de la Plume.\n\nIt was from 1920 that her career took off; she became a sex symbol and initiated the phenomenon of \"soldiers of the night\", the nickname for her fans, mostly young dandies of Tunisia.\n\nIt was at this time that she went with her lover to Paris, where through him she met Pablo Picasso and Coco Chanel.\n\nIn March 1925, she played Romeo and Juliet at the Ben Kamla theatre.\n\nOn the morning of February 20, 1930, her former lover Eliyahu Mimouni entered her apartment in Alfred Durand-Claye street in Tunis, and attacked her. Badly burned, she died the next day, followed soon after by Mimouni. Msika is buried in the cemetery of Borgel in Tunis.\n\n"}
{"id": "15972203", "url": "https://en.wikipedia.org/wiki?curid=15972203", "title": "Harry J. O'Brien", "text": "Harry J. O'Brien\n\nHarry Joseph O'Brien (October 31, 1884 – August 23, 1955), nicknamed \"Shorty\", was an American football, basketball, and baseball coach. He was the fifth head football coach for The Citadel, The Military College of South Carolina, serving for five seasons, from 1916 to 1918 and from 1920 to 1921, compiling a record of 14–15–4. \nO'Brien also coached basketball and baseball for The Citadel for two seasons during World War I. He tallied a record of 6–2 in basketball and 3–9 in baseball. O'Brien died of coronary thrombosis on August 23, 1955, at his home in Philadelphia.\n\n"}
{"id": "40932595", "url": "https://en.wikipedia.org/wiki?curid=40932595", "title": "Hasan al-Rammah", "text": "Hasan al-Rammah\n\nHasan al-Rammah (died 1295) was an Arab chemist and engineer during the Mamluk Sultanate who studied gunpowders and explosives, and sketched prototype instruments of warfare, including what some have maintained is a torpedo.\n"}
{"id": "27489312", "url": "https://en.wikipedia.org/wiki?curid=27489312", "title": "Hervé de Portzmoguer", "text": "Hervé de Portzmoguer\n\nHervé de Portzmoguer (c1470–1512), known as \"Primauguet\", was a Breton naval commander, renowned for his raids on the English and his death in the Battle of St. Mathieu.\n\nPortzmoguer participated in armed convoys, protecting merchant ships from pirates and enemy warships at a time when France was often in conflict with England. He also looted foreign ships. In 1506 he was convicted of looting a Scottish ship.\n\nFrom Morlaix he harassed English ships, which earned complaints from the Ambassador of England who wrote to the King of France that \"more than thirty vessels\" had been captured and looted by Portzmoguer. In retaliation, the troops of Admiral Edward Howard looted and burned Portzmoguer's mansion in the spring of 1512.\n\nHis motto is said to have been «War vor ha war zouar» (Breton for \"On sea and on land\").\n\nOn 10 August 1512 he went down with his ship \"Marie de la Cordelière\" in the Battle of St. Mathieu, when it blew up in a struggle with the English ship \"Regent\". Both ships were sunk. Almost all the crew were killed. The incident immediately became famous. The French poet-scholar Germain de Brie wrote a Latin poem which portrayed de Portzmoguer in such an ultra-heroic light that the English writer with Thomas More attacked it mercilessly. In his epigrams addressed to de Brie, More ridiculed the poem's description of \"Hervé fighting indiscriminately with four weapons and a shield; perhaps the fact slipped your mind, but your reader ought to have been informed in advance that Hervé had five hands.\n\nThe explosion that killed de Portzmoguer was subsequently portrayed as a deliberate act of self-sacrificing heroism. He is supposed to have said \"Nous allons fêter saint Laurent qui périt par le feu!\". (\"we will celebrate the feast of Saint Lawrence, who died by fire\") before blowing up the ship to avoid its otherwise inevitable capture by the English. In fact there is no evidence that the explosion was intentional.\n\nThe Breton poet Théodore Botrel wrote a heroic poem about this version of the incident. An equally heroic version is portrayed Alan Simon in the song \"Belle Marie de la Cordelière\" in his rock opera \"Anne de Bretagne\" (2008).\n\nThese ships in the French navy were named after him, using his gallicised nickname \"Primauguet\":\n"}
{"id": "435128", "url": "https://en.wikipedia.org/wiki?curid=435128", "title": "Home birth", "text": "Home birth\n\nA home birth is a birth that takes place in a residence rather than in a hospital or a birth centre. They may be attended by a midwife, or lay attendant with experience in managing home births. Home birth was, until the advent of modern medicine, the de facto method of delivery. Since the beginning of the 20th century, home birth rates have fallen in most developed countries, often to less than 1% of all births. Infant and mother mortality rates have also dropped drastically over the same time period and initially, assumptions were made that these findings were linked, as reflected in the UK Government's Peel Report (DoH 1970). Epidemiological work later identified there were no causal links, with improvements in mortality linked primarily to improvements in income and general health. Analysis which controlled for socioeconomic factors and for whether birth was planned and attended or unplanned and unattended identified that outcomes of planned home birth were positive. However, by this time, the view that birth should take place in hospital had become more normalised.\n\nWomen with access to high-quality medical care may choose home birth because they prefer the intimacy of a home and family-centered experience, or they desire to avoid a medically-centered experience typical of a hospital, among other reasons. Professionals attending home births can be obstetricians, certified or uncertified midwives, and doulas. In developing countries, where women may not be able to afford medical care or it may not be accessible to them, a home birth may be the only option available, and the woman may or may not be assisted by a professional attendant of any kind. In some cases, therefore, an unattended home birth may be unplanned (owing to lack of access to care or lack of easy access to a facility for birth) or chosen (often called [freebirth]). The latter tends to occur in women seeking to avoid repeat of previous traumatic birth experiences in facilities.\n\nMultiple studies have been performed concerning the safety of home births for both the child and the mother. Standard practices, licensing requirements and access to emergency hospital care differ between countries, and in countries like the US, between regions, making it difficult to compare studies across national borders. A 2014 US systematic review and meta-analysis of studies concluded that neonatal mortality rates were triple that of hospital births (Wax et al 2010), but the methodology has been subject to critical review on the basis of inclusion of studies with poor controls. A US-wide cohort study of planned home or hospital births from 2004-2009 concluded that there were no significant differences in perinatal outcomes but a lower intervention rate for mothers planning a home birth. A US registry-based study of all planned births in Oregon 2012-2013 (Snowden et al NEJM 2015) concluded that adverse outcome rates in low-risk women were low but births planned at home were associated with a higher rate of perinatal death and neonatal seizures when compared to planned birth in hospitals. However, this study relied on retrospective routine data and failed to control for differences in service provision, such as unlicensed midwives attending a proportion of home births and lack of integration of midwifery services in Oregon state). Conversely, a more recent study of integration of midwifery services in the US found that mortality rates and rates of preterm or low-birthweight births were lower in states with integrated midwifery services, in addition to lower rates of obstetric intervention and higher rates of physiological birth. A large-scale prospective cohort study of outcomes of births planned in home, obstetric hospital or midwifery unit settings in England found lower rates of intervention in all midwife-led settings and no differences in adverse neonatal outcome. However, although rates were very low overall, there was an increased risk of adverse perinatal outcomes for first births planned at home. Optimal outcomes in this study were found with freestanding midwifery units, which are units separate from a hospital with an obstetric unit, which are managed by midwives and intended primarily for the care of women with healthy pregnancies. \nVariations in study findings are likely to be associated with the inability to offer timely assistance to mothers with emergency procedures in case of complications during labour in settings with low levels of service integration or lack of universal access to care, as well as with widely varying licensing and training standards for birth attendants between different states and countries.\n\nHome births are either attended or unattended, planned or unplanned. Women are attended when they are assisted through labor and birth by a professional, usually a midwife, and rarely a general practitioner. Women who are unassisted or only attended by a lay person, perhaps their spouse, family, friend, or a non-professional birth attendant, are sometimes called freebirths. A \"planned\" home birth is a birth that occurs at home by intention. An \"unplanned\" home birth is one that occurs at home by necessity but not with intention. Reasons for unplanned home births include inability to travel to the hospital or birthing center due to conditions outside the control of the mother such as weather or road blockages or speed of birth progression.\n\nMany women choose home birth because delivering a baby in familiar surroundings is important to them. Others choose home birth because they dislike a hospital or birthing center environment, do not like a medically centered birthing experience, are concerned about exposing the infant to hospital-borne pathogens, or dislike the presence of strangers at the birth. Others prefer home birth because they feel it is more natural and less stressful. In a study published in the \"Journal of Midwifery and Women's Health\", women were asked, \"Why did you choose a home birth?\" The top five reasons given were safety, avoidance of unnecessary medical interventions common in hospital births, previous negative hospital experiences, more control, and a comfortable and familiar environment. One study found that women experience pain inherent in birth differently, and less negatively, in a home setting. In developing countries, where women may not be able to afford medical care or it may not be accessible to them, a home birth may be the only option available, and the woman may or may not be assisted by a professional attendant of any kind. Some women may not be able to have a safe birth at home, even with highly trained midwives. There are some medical conditions that can prevent a woman from qualifying for a home birth. These often include heart disease, renal disease, diabetes, preeclampsia, placenta previa, placenta abruption, antepartum hemorrhage after 20 weeks gestation, and active genital herpes. Prior cesarean deliveries can sometimes prevent a woman from qualifying for a home birth, though not always. It is important that a woman and her health care provider discuss the individual health risks prior to planning a home birth.\n\nHome birth was, until the advent of modern medicine, the \"de facto\" method of delivery. In many developed countries, home birth declined rapidly over the 20th century. In the United States there was a large shift towards hospital births beginning around 1900, when close to 100% of births were at home. Rates fell to 50% in 1938 and to fewer than 1% in 1955. Since 2000 a shift back towards home births has brought the rate up from 0.54% in 2004 to 0.72% in 2009. In the United Kingdom a similar but slower trend happened with approximately 80% of births occurring at home in the 1920s and only 1% in 1991. In Japan the change in birth location happened much later, but much faster: home birth was at 95% in 1950, but only 1.2% in 1975. Over a similar time period, maternal mortality during childbirth fell during 1900 to 1997 from 6–9 deaths per thousand to 0.077 deaths per thousand, while the infant mortality rate dropped between 1915 and 1997 from around 100 deaths per thousand births to 7.2 deaths per thousand.\n\nOne doctor described birth in a working class home in the 1920s:\n\nThis experience is contrasted with a 1920s hospital birth by Adolf Weber:\n\nMidwifery, the practice supporting a natural approach to birth, enjoyed a revival in the United States during the 1970s. Ina May Gaskin, for example, sometimes called \"the mother of authentic midwifery\" helped open The Farm Midwifery Center in Summertown, Tennessee in 1971, which is still in operation. However, although there was a steep increase in midwife-attended births between 1975 and 2002 (from less than 1.0% to 8.1%), most of these births occurred in the hospital. The US rate of out-of-hospital birth has remained steady at 1% of all births since 1989, with data from 2007 showing that 27.3% of the home births since 1989 took place in a free-standing birth center and 65.4% in a residence. Hence, the actual rate of home birth in the United States remained low (0.65%) over the twenty years prior to 2007.\n\nHome birth in the United Kingdom has also received some press since 2000. There was a movement, most notably in Wales, to increase home birth rates to 10% by 2007. Between 2005 and 2006, there was an increase of 16% of home birth rates in Wales, but by 2007 the total home birth rate was still 3% even in Wales (double the national rate). A 2001 report noted that there was a wide range of home birth rates in the UK, with some regions around 1% and others over 20%. In Australia, birth at home has fallen steadily over the years and was 0.3% as of 2008, ranging from nearly 1% in the Northern Territory to 0.1% in Queensland. In 2004, the New Zealand rate for births at home was nearly three times Australia's with a rate of 2.5% and increasing.\n\nIn the Netherlands, the trend has been somewhat different from other industrialized countries: while in 1965, two-thirds of Dutch births took place at home, that figure has dropped to about 20% in 2013, which is still more than in other industrialized countries. Less than 1% of South Korean infants are born at home.\n\nIn 2014, a comprehensive review in the \"Journal of Medical Ethics\" of 12 previously published studies encompassing 500,000 planned home births in low-risk women concluded that neonatal mortality rates for home births were triple those of hospital births. This finding echoes that of the American College of Obstetricians and Gynecologists. Due to a greater risk of perinatal death, the College advises women who are postterm (greater than 42 weeks gestation), carrying twins, or have a breech presentation not to attempt home birth. The \"Journal of Medical Ethics\" review additionally found that several studies concluded that home births had a higher risk of failing Apgar scores in newborns, as well as a delay in diagnosing hypoxia, acidosis and asphyxia. This contradicts a 2007 UK review study by the National Institute for Health and Clinical Excellence (NICE), a British governmental organization devoted to creating guidelines for coverage throughout the UK, which expressed concern for the lack of quality evidence in studies comparing the potential risks and benefits of home and hospital birthing environments in the UK. Their report noted that intrapartum-related perinatal mortality was low in all settings in the UK, but that in cases of unanticipated obstetric complications, the mortality rate was higher for home births due to the time needed to transfer the mother to an obstetric unit.\n\nA 2002 study of planned home births in the state of Washington found that home births had shorter labors than hospital births. In North America, a 2005 study found that about 12 percent of women intending to give birth at home needed to be transferred to the hospital for reasons such as a difficult labor or pain relief. A 2014 survey of American home births between 2004 and 2010 found the percent of women transferred to a hospital from a planned home birth after beginning labor to be 10.9%.\n\nBoth the \"Journal of Medical Ethics\" and NICE report noted that usage of caesarean sections were lower for women who give birth at home, and both noted a prior study that determined that women who had a planned home birth had greater satisfaction from the experience when compared with women who had a planned birth in a hospital.\n\nIn 2009 a study of 500,000 low-risk planned home and hospital births in the UK, where midwives have a strong licensing requirement, was reported in the \"British Journal of Obstetrics and Gynaecology\". The study concluded that for low-risk women there was no increase in perinatal mortality, provided that the midwives were well-trained and there was easy and quick access to hospitals. Further, the study noted there was evidence that \"low risk women with a planned home birth are less likely to experience referral to secondary care and subsequent obstetric interventions than those with a planned hospital birth.\" The study has been criticised on several grounds, including that some data might be missing and that the findings may not be representative of other populations.\n\nIn 2012, Oregon performed a study of all births in the state during the year as a part of discussing a bill regarding licensing requirements for midwives in the state. They found that the rate of intrapartum infant mortality was 0.6 deaths per thousand births for planned hospital births, and 4.8 deaths per thousand for planned home births. They further found that the death rate for planned home births attended by direct-entry midwives was 5.6 per thousand. The study noted that the statistics for Oregon were different for other areas, such as British Columbia, which had different licensing requirements. Oregon was noted by the Centers for Disease Control and Prevention as having the second-highest rate of home births in the nation in 2009, at 1.96% compared to the national average of 0.72%. A 2014 survey of nearly 17,000 voluntarily-reported home births in the United States between 2004 and 2010 found an intrapartum infant mortality rate of 1.30 per thousand; early neonatal and late neonatal mortality rates were a further 0.41 and 0.35 per thousand. The survey excluded congenital anomaly-related deaths, as well as births where the mother was transferred to a hospital prior to beginning labor.\n\nIn October 2013 the largest study of this kind was published in the American Journal of Obstetrics and Gynecology and included data on more than 13 million births in the United States, assessing deliveries by physicians and midwives in and out of the hospital from 2007 to 2010. The study indicated that babies born at home are roughly 10 times as likely to have an Apgar score of 0 after 5 minutes and almost four times as likely to have neonatal seizures or serious neurological dysfunction when compared to babies born in hospitals. The study findings showed that the risk of Apgar scores of 0 is even greater in first-born babies—14 times the risk of hospital births. The study results were confirmed by analyzing birth certificate files from the U.S. Centers for Disease Control and Prevention (CDC) and the National Center for Health Statistics. Given the study's findings, Dr. Amos Grunebaum, professor of clinical obstetrics and gynecology at Weill Cornell Medical College and lead author of the study, stated that the magnitude of risk associated with home delivery is so alarming that necessitates the need for the parents-to-be to know the risk factors. Another author, Dr. Frank Chervenak, added that the study underplayed the risks of home births, as the data used counted home births where the mother was transferred to a hospital during labor as a hospital birth.\n\nRandomized controlled trials are the \"gold standard\" of research methodology with respect to applying findings to populations; however, such a study design is not feasible or ethical for location of birth. The studies that do exist, therefore, are cohort studies conducted retrospectively by selecting hospital records and midwife records. by matched pairs (by pairing study participants based on their background characteristics), In February 2011 the American Congress of Obstetricians and Gynecologists identified several factors that make quality research on home birth difficult. These include \"lack of randomization; reliance on birth certificate data with inherent ascertainment problems; ascertainment of relying on voluntary submission of data or self-reporting; a limited ability to distinguish between planned and unplanned birth; variation in the skill, training, and certification of the birth attendant; and an inability to account for and accurately attribute adverse outcomes associated with transfers\". Quality studies, therefore, need to take steps in their design to mitigate these problems in order to produce meaningful results.\n\nThe data available on the safety of home birth in developed countries is often difficult to interpret due to issues such as differing home-birth standards between different countries, and difficult to compare with other studies because of varying definitions of perinatal mortality. Additionally, it is difficult to compare home and hospital births because only the risk profiles are different between the two groups, according to the CDC: people who choose to give birth at home are more likely to be healthy and at low risk for complications. There are also unquantifiable differences in home birth patients, such as maternal attitudes towards medical involvement in birth.\n\nWhile a woman in developed countries may choose to deliver her child at home, in a birthing center, or at hospital, health coverage and legal issues influence available options.\n\nIn April 2007, the Western Australian Government expanded coverage for birth at home across the State. Other state governments in Australia, including the Northern Territory, New South Wales and South Australia, also provide government funding for independent, private home birth.\n\nThe 2009 Federal Budget provided additional funds to Medicare to allow more midwives to work as private practitioners, allow midwives to prescribe medication under the Medicare Benefits Schedule, and assist them with medical indemnity insurance. However, this plan only covers hospital births. There are no current plans to extend Medicare and PBS funding to home birth services in Australia.\n\nAs of July 2012, all health professionals must show proof of liability insurance.\n\nIn March 2016 the Coroners Court of Victoria found against midwife Gaye Demanuel in the case of the death of Caroline Lovell.\n. \"Coroner White also called for a review of the regulation of midwives caring for women during home births, and for the government and health authorities to consider an offence banning unregistered health practitioners from taking money for attending home births.\"\n\nPublic health coverage of home birth services varies from province to province as does the availability of doctors and midwives providing home birth services. The Provinces of Ontario, British Columbia, Saskatchewan, Manitoba, Alberta, and Quebec currently cover home birth services.\n\nThere are few legal issues with a home birth in the UK. Woman can not be forced to go to a hospital. The support of the various Health Authorities of the National Health Service may vary, but in general the NHS will cover home births - the Parliamentary Under-Secretary of State for Health, Lord Hunt of King's Heath has stated \"I turn to the issue of home births. The noble Lord, Lord Mancroft, made some helpful remarks. As I understand it, although the NHS has a legal duty to provide a maternity service, there is not a similar legal duty to provide a home birth service to every woman who requests one. However, I certainly hope that when a woman wants a home birth, and it is clinically appropriate, the NHS will do all it can to support that woman in her choice of a home birth.\"\n\n27 states license or regulate in some manner direct-entry midwives, or certified professional midwife (CPM). In the other 23 states there are no licensing laws, and practicing midwives can be arrested for practicing medicine without a license. It is legal in all 50 states to hire a certified nurse midwife, or CNM, who are trained nurses, though most CNMs work in hospitals.\n\n"}
{"id": "48517570", "url": "https://en.wikipedia.org/wiki?curid=48517570", "title": "Japanese submarine I-185", "text": "Japanese submarine I-185\n\nThe Japanese submarine \"I-185 (originally I-85\") was a \"Kaidai\" type cruiser submarine of the KD7 sub-class built for the Imperial Japanese Navy (IJN) during the 1940s. She was sunk with all hands by an American destroyer during the Battle of the Philippine Sea in mid-1944.\n\nThe submarines of the KD7 sub-class were medium-range attack submarines developed from the preceding KD6 sub-class. They displaced surfaced and submerged. The submarines were long, had a beam of and a draft of . The boats had a diving depth of and a complement of 86 officers and crewmen.\n\nFor surface running, the boats were powered by two diesel engines, each driving one propeller shaft. When submerged each propeller was driven by a electric motor. They could reach on the surface and underwater. On the surface, the KD7s had a range of at ; submerged, they had a range of at .\n\nThe boats were armed with six internal torpedo tubes, all in the bow. They carried one reload for each tube; a total of a dozen torpedoes. They were originally intended to be armed with two twin-gun mounts for the Type 96 anti-aircraft gun, but a deck gun for combat on the surface was substituted for one 25 mm mount during construction.\n\nBuilt by the Yokosuka Naval Arsenal, the boat was laid down on 9 February 1942 as \"I-85\" and renamed \"I-185\" in 1942. She was launched on 16 September 1943 and completed on 23 September. The boat was sunk with the loss of all 95 officers and crewmen aboard by the destroyer on 22 June 1944 near Saipan. \"I-185\" was stricken from the Navy List on 10 September 1944.\n\n"}
{"id": "4976475", "url": "https://en.wikipedia.org/wiki?curid=4976475", "title": "Judith Berrisford", "text": "Judith Berrisford\n\nJudith M. Berrisford (born 1921) was a British writer of children's pony stories which are very similar to those of the Pullein-Thompson sisters, as well as other animal story and books on gardening. She also wrote under the name Amanda Hope.\n\n\n\nDates unknown\n\n\n\n\n"}
{"id": "9753150", "url": "https://en.wikipedia.org/wiki?curid=9753150", "title": "Law, Legislation and Liberty", "text": "Law, Legislation and Liberty\n\nLaw, Legislation and Liberty is the 1973 work in three volumes by Nobel laureate economist and political philosopher Friedrich Hayek. In it, Hayek further develops the philosophical principles he discussed earlier in \"The Road to Serfdom\", \"The Constitution of Liberty\", and other writings. \"Law, Legislation and Liberty\" is more abstract than Hayek's earlier work, and it focuses on the conflicting views of society as either a design, a made order (\"taxis\"), on the one hand, or an emergent system, a grown order (\"cosmos\"), on the other. These ideas are then connected to two different forms of law: law proper, or \"nomos\" coinciding more or less with the traditional concept of natural law, which is an emergent property of social interaction, and legislation, or \"thesis\", which is properly confined to the administration of non-coercive government services, but is easily confused with the occasional acts of legislature that do actually straighten out flaws in the nomos.\n\"Vol. 1 : Rules and Order\" (1973)\n\"Vol. 2 : The Mirage of Social Justice\" (1976) \n\"Vol. 3 : The Political Order of a Free People\" (1979)\n\n"}
{"id": "1145054", "url": "https://en.wikipedia.org/wiki?curid=1145054", "title": "MV Joyita", "text": "MV Joyita\n\nMV \"Joyita\" was a merchant vessel from which 25 passengers and crew mysteriously disappeared in the South Pacific in 1955. It was found adrift with no one aboard. The ship was in very poor condition, with corroded pipes and a radio which, while functional, had a range of only about , because of faulty wiring. However, the extreme buoyancy of the ship made sinking nearly impossible. Investigators were puzzled as to why the crew had not remained on board and waited for help.\n\nThe wooden ship was built in 1931 as a luxury yacht by the Wilmington Boat Works in Los Angeles for movie director Roland West, who named the ship for his wife, actress Jewel Carmenille — \"joyita\" in Spanish meaning \"little jewel\". In 1936 the ship was sold and registered to Milton E. Beacon. During this period, she made numerous trips south to Mexico and to the 1939–1940 Golden Gate International Exposition in San Francisco. During part of this time, Chester Mills was the skipper of the vessel.\n\nThe ship's hull was constructed of -thick cedar on oak frames. She was long, with beam of and a draft of ; her net tonnage was 47 tons and her gross tonnage approximately 70 tons. She had tanks for of water and of diesel fuel.\n\nIn October 1941, just before the attack on Pearl Harbor, \"Joyita\" was acquired by the United States Navy and taken to Pearl Harbor, Hawaii, where she was outfitted as Yard patrol boat YP-108. The Navy used her to patrol the Big Island of Hawaii until the end of World War II. In 1943 she ran aground and was heavily damaged, but the Navy was in need of ships so she was repaired. At this point, new pipework was made from galvanized iron instead of copper or brass. In 1946, the ship was surplus to Navy requirements and most of its equipment was removed.\n\nIn 1948 \"Joyita\" was sold to the firm of Louis Brothers. At this point, cork lining was added to the ship's hull along with refrigeration equipment. The ship had two Gray Marine diesel engines providing , and two extra diesel engines for generators. In 1950 William Tavares became the owner; however, he had little use for the vessel, and sold it in 1952 to Dr Katharine Luomala, a professor at the University of Hawaii. She chartered the boat to her friend, Captain Thomas H. \"Dusty\" Miller, a British-born sailor living in Samoa. Miller used the ship as a trading and fishing charter boat.\n\nAbout 5:00 AM on October 3, 1955, \"Joyita\" left Samoa's Apia harbor bound for the Tokelau Islands, about away. The boat had been scheduled to leave on the noon tide the previous day but her departure was delayed because her port engine clutch failed. \"Joyita\" eventually left Samoa on one engine. She was carrying sixteen crew members and nine passengers, including a government official, a doctor (Alfred \"Andy\" Denis Parsons, a World War II surgeon on his way to perform an amputation), a copra buyer, and two children. Her cargo consisted of medical supplies, timber, 80 empty 45 gallon (200 l) oil drums and various foodstuffs.\n\nThe voyage was expected to take between 41 and 48 hours. She was scheduled to return with a cargo of copra. \"Joyita\" was scheduled to arrive in the Tokelau Islands on October 5. On October 6 a message from Fakaofo port reported that the ship was overdue. No ship or land-based operator reported receiving a distress signal from the crew. A search and rescue mission was launched and, from 6 to 12 October, Sunderlands of the Royal New Zealand Air Force covered a probability area of nearly of ocean, but no sign of \"Joyita\" or any of her passengers or crew was found.\n\nFive weeks later, on November 10, Gerald Douglas, captain of the merchant ship \"Tuvalu\", en route from Suva to Funafuti, sighted \"Joyita\" more than west from her scheduled route, drifting north of Vanua Levu. The ship was partially submerged and listing heavily (her port deck rail was awash) and there was no trace of any of the passengers or crew; four tons of cargo were also missing. The recovery party noted that the radio was discovered tuned to 2182 kHz, the international marine radiotelephone distress channel.\n\nThere was still fuel in \"Joyita\"s tanks; from the amount used, it was calculated she made some before the vessel was abandoned, probably within of Tokelau. The leak had probably started after 9 p.m. on the second night of the voyage, with nine hours of darkness ahead.\n\nAlthough \"Joyita\" was found with her bilges and lower decks flooded, her hull was sound. When she was moored back in harbour at Suva, investigators heard the sound of water entering the vessel. It was found that a pipe in the raw-water circuit of the engine's cooling system had failed due to galvanic corrosion, allowing water into the bilges. The first the crew would have known about the leak was when the water rose above the engine room floorboards, by which time it would have been nearly impossible to locate the leak. Also, the bilge pumps were not fitted with strainers, and had become clogged with debris, meaning that it would have been very difficult to pump the water out.\n\nA subsequent inquiry found that the vessel was in a poor state of repair, but determined that the fate of the passengers and crew was \"inexplicable on the evidence submitted at the inquiry.\" An especially perplexing point was that the three liferafts \"Joyita\" carried were missing, but it would not make sense for the crew and passengers to voluntarily abandon the vessel. Fitted out for carrying refrigerated cargo, \"Joyita\" had of cork lining her holds, making her virtually unsinkable. In addition, further buoyancy was provided by a cargo of empty fuel drums.\n\nThe inquiry was only able to establish the reasons for the vessel becoming flooded. It found that the vessel would have begun to flood due to the fractured cooling pipe. The bilge pumps were unserviceable due to becoming blocked. \"Joyita\" lacked watertight bulkheads or subdivisions in the bilges. The water would have gradually flooded the lower decks. As the boat began to sink lower into the water, the one remaining engine would not have been able to maintain enough speed to steer. \"Joyita\" then fell beam-on to a heavy swell and took on the heavy list it was found with. While flooded to an extent which would sink a conventional vessel, \"Joyita\" stayed afloat due to her cork-lined hull and cargo of fuel drums.\n\nThe inquiry also placed much of the responsibility for the events on Miller. They found him reckless for setting out on an ocean-going voyage with only one engine and numerous minor faults, and negligent for failing to provide a working radio or properly equipped lifeboat. He was also in breach of maritime law, since he had allowed \"Joyita\"s license to carry fare-paying passengers to lapse.\n\nThe inquiry made no mention of the used medical equipment found on board.\n\n\"Joyita\" is sometimes referred to as the \"\"Mary Celeste\" of the South Pacific\" and has been the subject of several books and documentaries offering explanations that range from rational and conventional to supernatural and paranormal. Numerous hypotheses for the disappearance of \"Joyita\"s crew and passengers have been advanced. Many were circulated at the time of the event, and several others have been put forward since. Given the fact that the hull of \"Joyita\" was sound and her design made her almost unsinkable, a main concern of investigators was determining why the passengers and crew did not stay on board if the events were simply triggered by the flooding in the engine room.\n\nCaptain Miller should have been well aware of the vessel's ability to stay afloat, leading some to speculate that Miller had died or become incapacitated for some reason (someone on board was injured—hence the bloodstained bandages). Without him to reassure the other people on board, they may have panicked when \"Joyita\" began to flood and taken to the liferafts. However, this in itself would not account for the missing cargo and equipment, unless the vessel had been found abandoned and had her cargo removed.\n\nA friend of Miller, Captain S. B. Brown, was convinced that Miller would never have left \"Joyita\" alive, given his knowledge of her construction. He was aware of tension between Miller and his American first mate, Chuck Simpson. Brown felt that Miller and Simpson's dislike of each other came to blows and both men fell overboard or were severely injured in a struggle. This left the vessel without an experienced seaman and would explain why those remaining on board would panic when the ship began to flood.\n\nThe Fiji \"Times and Herald\" quoted at the time from an \"impeccable source\" to the effect that \"Joyita\" had passed through a fleet of Japanese fishing boats during its trip and \"had observed something the Japanese did not want them to see.\"\n\n\"The Daily Telegraph\" hypothesized that some still-active Japanese forces from World War II were to blame for the disappearances, operating from an isolated island base.\nThere was still strong anti-Japanese feeling in parts of the Pacific, and in Fiji there was specific resentment of Japan being allowed to operate fishing fleets in local waters. Such theories suddenly gained credence when men clearing \"Joyita\" found knives stamped 'Made in Japan'. However, tests on the knives proved negative and it turned out the knives were old and broken- quite possibly left on board from when \"Joyita\" was used for fishing in the late 1940s.\n\nAlso there was a proposition that \"the vessel's occupants were kidnapped by a Soviet submarine, with the world at the time in the midst of the growing Cold War.\"\n\nOthers hypothesize that modern sea pirates attacked the vessel, killed the 25 passengers and crew (and cast their bodies into the ocean), and stole the missing four tons of cargo.\n\nIt was also revealed that Miller had amassed large debts after a series of unsuccessful fishing trips on \"Joyita\". However, it would have been difficult to see the events surrounding \"Joyita\" as insurance fraud, given that no seacocks were found open and the ship would be almost impossible to scuttle. Also, Miller was relying on \"Joyita\" being chartered for regular runs between Samoa and Tokelau—these government charters would have quickly cleared his debts.\n\nA subsequent owner of \"Joyita\", author Robin Maugham, spent many years investigating the vessel's past, and published his findings as \"The Joyita Mystery\" in 1962. Maugham agreed that events were started by the flooding from the broken cooling pipe and the failure of the pumps. The mattresses found covering the starboard engine were used either in an attempt to stem the leak or to protect the electrical switchboard from spray kicked up by the engine's flywheel as the water level rose. At the same time, \"Joyita\" encountered increasingly heavy swells and squally weather.\n\nMiller, knowing \"Joyita\" to be unsinkable and desperate to reach his destination to clear his debt, pressed on. However, Chuck Simpson, and possibly other crew members, demanded that he turn back. This effectively led to mutiny and Miller and the crew struggled, during which Miller sustained a serious injury. By now the ship was entering heavier weather, with winds around , and with one engine and a flooded bilge, was beginning to labor. The flooding in the engine room would have eventually caused the starboard engine to fail, also cutting all the vessel's electrical power. Chuck Simpson was now in control and made the decision to abandon ship, taking the navigational equipment, logbook and supplies, as well as the injured Miller, with them.\n\nIt still seems unlikely that Chuck Simpson would choose to abandon a flooded but floating ship to take to small open rafts in the Pacific Ocean. Maugham proposed that they sighted a nearby island or reef and tried to reach it, but in the strong winds and seas the rafts were carried out to sea, leaving \"Joyita\" drifting and empty. The damage to the lightly built superstructure was caused by wave damage while the vessel was drifting in heavy seas.\n\nIn July 1956, \"Joyita\" was auctioned off by her owners for £2,425 to a Fiji Islander, David Simpson. He refitted and overhauled her and she went to sea again that year. However, she was surrounded by legal disputes over the transfer of her registry from the United States to Britain without permission. In January 1957 she ran aground while carrying 13 passengers in the Koro Sea. She was repaired and in October 1958 began a regular trade between Levuka and Suva.\n\nShe again ran aground on a reef in November, 1959 at Vatuvalu near Levuka. She floated off the reef assisted by high tide, but while heading for port began to take in water through a split seam. The pumps were started, but it became clear that the valves for the pump had been installed the wrong way, meaning that water was pumped into the hull, not out. Now with a reputation as an 'unlucky ship' and with a damaged hull, she was abandoned by her owners and beached. She was stripped of useful equipment and was practically a hulk when she was bought by Robin Maugham, who wrote the book \"The Joyita Mystery\" (1962). He sold the hulk in 1966 to Major J. Casling-Cottle who ran a tourist and publicity bureau at Levuka. He planned to turn it into a museum and tearoom, but the plan never saw daylight. The hulk disappeared piece by piece and the process of disintegration appears to have been complete by the late 1970s.\n\nOn March 14, 1975 The Western Samoa Post Office released a set of five stamps dealing with the mystery of \"Joyita\".\n\nIn 2009 a walkway was named after Dr. Alfred Denis Parsons near his former Torbay home in Auckland, New Zealand.\n\nIn 2012 two memory stones in honor of the event were erected in Apia, Samoa and in Fakaofo Village, Tokelau.\n\n, all aboard were still declared as \"missing\".\n\nNote:\n\n\n"}
{"id": "57627498", "url": "https://en.wikipedia.org/wiki?curid=57627498", "title": "Marye (horse)", "text": "Marye (horse)\n\nMarye was a warhorse owned by the United States Army during the American Civil War who was later captured and pressed into Confederate States service. \n\nMarye's early life and original name and master are unknown. She fought with United States forces during the American Civil War at the Battle of Fredericksburg in 1862. During the assault against fortified Confederate positions at Marye's Heights, the beast's rider was shot off her and the animal galloped into the Confederate lines. The brigade of Georgia troops of the Army of Northern Virginia captured the steed and brought her to their commander, General John Brown Gordon, whose own mount had just succumbed to exhaustion. At the conclusion of active combat, the horse was surrendered to the Quartermaster but purchased back by the Georgia soldiers and presented to Gordon as a trophy.\n\nGordon named the horse Marye in tribute to the location of its capture and is said to have treasured the horse since it was a gift from his men. Gordon used the horse as a mount until 1864 when she was killed in action by gunfire at the Battle of Monocacy.\n\nAccording to Gordon's description of the horse, she was generally sluggish, however, during battle was \"transformed\" into an excellent warhorse with a fearless character who would \"with its head up and its nostrils distended, bound across ditches and over fences\". Gordon would tribute Marye as \"the most superb battlehorse that it was my fortune to mount during the war\".\n\nIn 1907 a statue of Gordon atop Marye, by Solon Borglum, was unveiled on the grounds of the Georgia State Capitol.\n\n"}
{"id": "44162826", "url": "https://en.wikipedia.org/wiki?curid=44162826", "title": "Matilda Newport", "text": "Matilda Newport\n\nMatilda Newport ( 1795–1837) was an Americo-Liberian colonist and folk hero. She is known for her actions in 1822 when she is alleged to have defended the settlement Cape Mesurado with a cannon she lit from her pipe. She is a controversial figure in light of the tensions between Americo-Liberians and native Liberians. The historical accuracy of the account has been challenged. A national holiday in her honour, Matilda Newport Day, was celebrated annually on 1 December from 1916 until it was abolished in 1980.\n\nMatilda was born in the United States circa 1795 (possibly Georgia). She married Thomas Spencer, becoming Matilda Spencer. At the age of 25 she sailed to Liberia with her husband on the ship \"Elizabeth\". They arrived on 30 September 1820 and settled in Cape Mesurado.\n\nNewport is principally known for her actions on 1 December 1822 defending the colonial settlement Cape Mesurado during the Battle of Fort Hill. An American Colonization Society representative had acquired the land through gunboat diplomacy, displacing the indigenous Dei people. During one of two attacks on the settlement by the Dei, Bassa, and Gola, Newport noticed that the cannoneers were either dead or wounded. She used a hot coal from her pipe to light the fuse of a cannon. The resulting explosion deterred the advance of the African warriors and changed the course of the battle.\n\nNewport's husband died, probably in one of the conflicts. Sometime after 1822, she married again to Ralph Newport. He died in a canoeing accident in 1836. Matilda Newport died of pleurisy in Monrovia in 1837.\n\nNewport's role in the 1822 battle was not initially considered particularly significant, but grew over time in oral histories. In 1916 the Liberian legislature designated 1 December as \"Matilda Newport Day\". Until its abolishment, the annual celebration included a grand ball, speeches, parades, and reenactments. She was commemorated with a Liberian postage stamp in 1947. A bronze plaque at the Centennial Pavilion in Monrovia depicts Newport with the cannon. Monrovia also has a monument dedicated to her and a street and a high school bear her name.\n\nAs criticism of Americo-Liberian rule grew in the 1960s, Newport's role in Liberian history was no longer seen as admirable. Following the military coup in 1980, Samuel Doe abolished Matilda Newport Day. Historians have questioned the veracity of the account of Newport's actions. None of the accounts of the battle mention women participating the colony's defense. The first source to acknowledge Newport's role appeared in 1854.\n"}
{"id": "56226666", "url": "https://en.wikipedia.org/wiki?curid=56226666", "title": "Mensacarcin", "text": "Mensacarcin\n\nMensacarcin is a highly oxygenated polyketide first isolated from soil-dwelling \"Streptomyces bottropensis\" bacteria.\n\nThe molecule is a secondary metabolite, and can be obtained in large amounts from its producing organism.\n\nDue to its unique properties it is an important model for drug development against melanoma and other cancers.\n\nIn NCI-60 anti-cancer compound screening mensacarcin has a high cytostatic effect against almost all cell lines (mean of 50% growth inhibition) and a relatively selective cytotoxic effect against melanoma cells.\n\nLow COMPARE correlation with standard antitumor agents indicate a unique mechanism of action.\nFurther examinations reveal mensacarcin effecting the mitochondria.\n\nWith its unique mechanism,\neffective also in BRAF V600E mutation cell lines,\nmensacarcin is a promising model for the development of new anticancer drugs.\n\nExisting therapies for melanoma are limited.\nMensacarcin's powerful effect against melanoma cells make it especially valuable for this disease.\n\nMitochondria provide most of the energy used by eukaryotic cells.\n\nIn a study at the Oregon State University a synthesized fluorescent probe of mensacarcin was localized to the mitochondria within 20 minutes of treatment.\n\nLive-cell bioenergetic flux analysis showed rapid disturbance of energy production and of mitochondrial function.\n\nThe localization together with the metabolic effects provide evidence that mensacarcin targets mitochondria.\n\nMitochondria are also important in cell death signaling.\n\nMensacarcin in melanoma cells activates apoptotic pathways related to caspase 3 and caspase 7, and thus induces cell death.\n\nAfter mensacarcin treatment of two melanoma cell lines, the cells showed characteristic chromatin condensation as well as distinct poly(ADP-ribose)polymerase-1 cleavage;\nflow cytometry identified a large population of apoptotic cells;\nsingle-cell electrophoresis indicated that mensacarcin causes genetic instability, a sign of early apoptosis.\n\nThe BRAF V600E mutation is associated with drug resistance.\nDue to its independent mechanism, mensacarcin has an undiminished effect in melanoma cell lines with this mutation (NCI 60 cell lines SK-Mel-28 and SK-Mel-5).\n"}
{"id": "7360700", "url": "https://en.wikipedia.org/wiki?curid=7360700", "title": "Microneurography", "text": "Microneurography\n\nMicroneurography is a neurophysiological method employed by scientists to visualize and record the normal traffic of nerve impulses that are conducted in peripheral nerves of waking human subjects. The method has been successfully employed to reveal functional properties of a number of neural systems, e.g. sensory systems related to touch, pain, and muscle sense as well as sympathetic activity controlling the constriction state of blood vessels. To study nerve impulses of an identified neural system, a fine tungsten needle electrode is inserted into the nerve and connected to a high gain recording amplifier. The exact position of the electrode tip within the nerve is then adjusted in minute steps until the electrode discriminates impulses of the neural system of interest. A unique feature and a significant strength of the microneurography method is that subjects are fully awake and able to cooperate in tests requiring mental attention, while impulses in a representative nerve fibre or set of nerve fibres are recorded, e.g. when cutaneous sense organs are stimulated or subjects perform voluntary precision movements.\n\nBefore the microneurography technique was developed in the late 1960s, impulses in peripheral nerves had been recorded in animal experiments alone using a technique that involved dissection and splitting the nerve. This approach is not tolerable for general use in man although it has been pursued in one single study. Actually, the concern of nerve damage was a major obstacle for the development of microneurography because the approach of inserting a needle electrode in a human nerve was generally regarded as highly dangerous involving substantial risks of permanent nerve damage. The two Swedish scientists who developed the microneuropgraphy technique (Hagbarth and Vallbo) handled the medical-ethical concern by performing a large series of experiments on their own nerves during a period of about 2 years while carefully checking for nerve damage. Working at the Department of Clinical Neurophysiology, Academic Hospital, Uppsala, they collected data resulting in the first complete papers representing three areas to become major fields of microneurography, i.e. afference from intra-muscular sense organs during voluntary contractions, response of cutaneous sense organs related to touch stimuli, and efferent sympathetic activity controlling the constriction state of human blood vessels. The microneurography approach of Hagbarth and Vallbo based on epoxy resin coated tungsten electrodes is now generally accepted whereas an alternative attempt using glass coated platina-iridium electrodes had obviously limited success as it yielded a single short note alone.\n\n Nerve fibers of various kinds are more or less randomly mixed in most nerves. This is true for fibers of different functions as well as fibers of different size. Basically fiber diameter is closely related to function, e.g. cutaneous pain system is dependent on small fibers whereas discriminative touch is dependent on large fibers. With regard to fiber diameter there are two main categories: A-fibers are large and conduct impulses at high or moderate speed (5–75 m/s). C-fibers are small and conduct impulses at low speed (around 1 m/s). In microneurography recordings, A- and C-fiber impulses differ in shape. Because fibers are mixed in most nerves, it is usually essential to record from an individual nerve fiber at a time to explore the properties of a functional system, although multi-unit recording has been very rewarding in studies of sympathetic efferent activity. An individual nerve consists of a number of parallel fascicles, i.e. bundles of nerve fibers enclosed within a connective tissue sheath that may be quite tough and hard for a needle microelectrode to penetrate.\n\nMicroneurography is based on tungsten needle electrodes which are inserted through the skin and into a nerve. Anaesthetics are not required because surprisingly the procedure is not very painful. The tungsten microelectrodes have a shaft diameter of 100-200 μm, a tip diameter of 1-5 μm, and they are insulated to the tip with an epoxy resin. Electrode impedance varies between 0.3 and 5 MΩ at 1 kHz as measured initially. However, the impedance tends to decrease during experiment and is usually below 1 MΩ while impulses are recorded. Nerve discharges are determined by voltage differences between the intra-neural electrode and a reference needle electrode in the vicinity. The 2 electrodes are connected to an amplifier with a high input impedance and an appropriate band-pass filtering, often 500 to 5000 Hz. Signals are monitored on a computer screen and stored on a hard disc for off-line analysis. Any peripheral nerve that can be reached may be a target for microneurographical recordings, although so far only arm, leg, and facial nerves have been exploited. \nIn order to locate the nerve, electrical stimulation through a needle electrode or ultrasonic monitoring is often used. Weak electrical shocks are delivered either through the recording electrode or through a separate stimulation needle while neural response is observed, either a muscle twitch or a cutaneous sensation reported by the subject.\nIn ultrasonic monitoring a linear, high frequency ultrasound probe is used.\nThe microelectrode is then inserted 1–2 cm from the probe, ideally in a 90° angle to the ultrasonic beam. This generates the best wave reflection and image. Ultrasonic approach accurately locates the depth of the nerve and identifies surrounding anatomical structures of interest, such as blood vessels and bony structures, which may affect the placement of a microelectrode. A particular advantage is that the ultrasonic approach visualizes the electrode and the nerve at the same time, thereby facilitating electrode manipulation to reach the nerve. Once the electrode tip is in the nerve, small adjustments are required, first, to penetrate the sheath of an individual fascicle and, second, to take the tip to contact nerve fibers of the kind you are interested to explore, be it multi-unit sympathetic activity or single unit activity of either a myelinated afferent or a small unmyelinated fibres. The search procedure requires considerable skill and patience as it may be very tedious particularly with small nerves or nerves located deep below the skin surface.\n\nRecording of single afferent impulses from C-fibers is particularly demanding as they have a diameter of only one micrometer. A method to increase the yield is the marking technique which is based on a unique property of many kinds of C-fibres, i.e. a decrease of conduction velocity in the wake of preceding impulses. \n\nBy combining repetitive electrical stimulation and physical stimulation, e.g. skin pinch or light touch, the afferent can be identified and characterized. The marking technique is very efficient as it allows simultaneous recordings of several fibers. However, it generates only semi-quantitative information about unitary activity, whereas recordings of impulse trains allow more comprehensive description of functional properties of sense organs.\n\nThe microneurography electrode may be used not only for recording of nerve impulses but for stimulation of individual fibers as well. An interesting application is to combine successive recording and stimulation of the same afferent. Once the functional properties of an afferent have been defined, e.g. with regard to sensitivity, receptive field structure, and adaptation, the electrode may be reconnected to a stimulator to give trains of electrical pulses of controlled strength, rate, and duration. It has been found that the percept elicited from a single tactile afferent in the glabrous skin of the hand, may be remarkably detailed and closely matching the properties of the afferent, indicating a high degree of specificity. Although this approach to bridge the gap between biophysical events in a single afferent and mental phenomena within the mind is simple and straight forward in principle it is demanding in practice for a number of reasons. Micro-stimulation has also been used to characterize individual motor units with regard to contraction properties.\n\nMicroneurography recordings have elucidated the organization as well as normal and pathological function of a fair number of neural systems in man, whereas the technique is not useful in clinical routine for diagnostic purposes to clarify the condition of the individual patient. Three main groups of neural systems have been explored, i.e. proprioception, cutaneous sensibility, and sympathetic efferent activity.\n\nInformation from a variety of sense organs provides information about joint positions and movements. The most elaborate proprioceptive sense organ is the muscle spindle. It is unique because its functional state is continually controlled from the brain through the fusimotor system. Recordings from muscle spindle afferents indicate that the fusimotor system remains largely passive when the parent muscle is relaxed whereas is it regularly activated in voluntary contractions and more so the stronger the contraction. Thus microneurography suggests a parallelism between the two motor systems, i.e. the skeletomotor system controlling the ordinary muscle fibers and the fusimotor system. This seems to hold at least for weak contractions and small movements which have been explored so far. In contrast, more independent fusimotor activity has been reported in animal experiments, mainly cat hind limb, where larger movements are allowed. Thanks to fusimotor activation, the afferent signal from muscle spindles remains efficient in monitoring large changes of muscle length without turning silent during muscle shortening. On the other hand, very small intramuscular events are monitored as well, thanks to the extreme sensitivity of the sense organ.\nAn example is the small pulsatile component of the muscle contraction which is due to a periodic fluctuation at 8–10 Hz of the motor command. These small variations are insentient but readily monitored by the population of spindle afferents. They are akin to the tremor we may experience when emotionally excited. The functional significance of the insentient spindle response to faint intramuscular events remains to be assessed. However, it seems likely that detailed information on large as well as small mechanical events in the muscles is essential for neural systems in the brain to produce appropriate commands for dexterous movements.\n\nMicroneurography has demonstrated that our brains make use of detailed proprioceptive information not only by deep sense organs but by cutaneous mechanoreceptors as well. Any joint movement causing the slightest skin stretch is accurately monitored by cutaneous Ruffini endings in the skin area surrounding the joint.\n\nCutaneous sensibility includes a number of functions. Microneurography has been particularly used to investigate discriminative and affective touch mechanisms, as well as pain mechanisms, although afferents related to pruritus and temperature have been studied to some extent as well. A separate set of studies concern motor effects from cutaneous tactile afferents in the glabrous skin.\n\nTwo different tactile systems have been identified. A system for discriminative touch has been intensely studied since long whereas a system for affective touch was understood and explored more recently. Discriminative touch is based on large myelinated afferents from skin as well as afferents from deeper structures. This system allows us to extract detailed information on spatial and temporal features of any skin deformation as well as properties of physical objects such as size, shape, and surface structure. The glabrous skin of the human hand has a paramount role in discriminative touch. Thus the tactile organization of this skin area has therefore been extensively explored. \n\nAltogether there are about 17,000 tactile afferents in the glabrous skin area of one hand. They are of four distinct types. Two kinds of afferents have small receptive fields suited for high spatial resolution (Merkel and Meissner). They are particularly numerous in the pulp of the finger, a region often engaged in exploration of object properties. Pacini units are extremely sensitive to fast movements whereas spatial resolution is poor. Ruffini units are characterized by high sensitivity to skin stretch and forces acting on the nails. Micro-stimulation has shown that input from one single Meissner, Merkel, or Pacini unit may produce a distinct and differential percept in the mind of the subject indicating an absolute specificity within the tactile system. It has even been demonstrated that a single impulse in a Meissner afferent may produce a percept.\nIn contrast, no percept is reported when a single Ruffini afferent is stimulated which might indicate that spatial summation is required. Consistent with the perceptive findings, neural responses in the somatosensory cortex have been recorded on micro-stimulation of single afferents connected to Meissner, Merkel, Pacini endings but not with single Ruffini afferents. On the basis of collateral studies in man and monkey a very tight match has been claimed between magnitude estimation of sensation of skin deformation, on the one hand, and response of Merkel afferents in the mnonkey, on the other. In man, deviations from such a linear relation was found in combined psychophysical and microneurography recordings. \nIn the hairy skin Meissner units are lacking altogether. Instead there are hair follicle and field afferents which have large receptive fields while Merkel, Pacini, and Ruffini are present. Cutaneous Ruffini units in the hairy skin are important for position sense and kinesthesia as pointed out in another section. A caveat is justified with regard to end organ norphology. The four kinds of units considered above were physiologically identified in man (FA/RA and SA units, i.e. fast and slowly adapting type I and type II, ) whereas end organ morphology has been inferred on the basis of animal studies. Particularly, it seems likely that SAII afferents may be connected to other morphological structures than the classical Ruffini ending.\n\nLight touch is coded not only in large myelinated afferents but in small unmyelinated afferents as well. Tactile C-afferents (CT) were described long ago in non-human species but did not attract much interest until it was shown that they are numerous in human hairy skin. In contrast, they are lacking altogether in glabrous skin. A number of findings from both normal subjects and from unique patients lacking large tactile afferents indicate that CT afferents are essential for the pleasurable aspect of friendly touch.\n\nParticularly, CT afferents respond vigorously to slow caressing movements, and, importantly, the size of the afferent response matches the sense of pleasure reported by the subject. fMRI studies of brain activity indicate that CT activate the insular cortex but not the primary or secondary somatosensory cortex consistent with the hypothesis that CT may play a role in emotional, behavioral, and hormonal responses to pleasant skin-to-skin contact between individuals.\n\nIt has been shown that tactile afferents from the glabrous skin of the hand exert profound effects on hand and finger muscles in the subconscious control of grip force whenever we lift and manipulate objects.\nThe friction between skin and object surface is extracted as soon as your fingers close around the object and contraction force of the muscles gripping the object is adjusted accordingly. Moreover, any tendency to slipping is monitored by tactile afferents and gives rise to swift reflexes resulting in subconscious adjustments of motor output. Many forms of dexterous handling of objects include successive phases of different motor activity. It has been shown that tactile sense organs in the glabrous skin are involved in timely linking the separated phases to a purposeful motor act.\n\nAfferents responding to noxious stimuli are known as nociceptors. There are 2 main groups, unmyelinated C-afferents and small myelinated Aδ fibers. Most studies are focused on C nociceptiors.\nThe nociceptive C-fibers constitute a very large proportion of somatic afferent nerve fibers. The majority are polymodal because they are activated by several kinds of stimuli, i.e. mechanical, thermal, and chemical stimuli. The activation of the polymodal by heat corresponds to the heat pain threshold for humans whereas a weak response to mechanical stimuli is usually not associated with pain sensation.\nAnother group of unmyelinated nociceptor fibers differ by lacking response to mechanical stimuli. These mechano-insensitive fibers differ from polymodal afferents in other respects as well, e.g. threshold for heat is higher, receptive fields on the skin are larger, conduction velocity is slower, and activity-dependent hyperpolarization of the axon is more pronounced. The mechano-insensitive nociceptors may be sensitized particularly by inflammatory mediators to render them mechano-responsive, a process that may account for the tenderness we experience following a physical injury. Moreover, electrical activation of C-mechano-insensitive fibers demonstrates that they have a role in neurogenic vasodilation which has not been found with polymodal nociceptors.\nIt is suspected that the inflammatory mediators bind to protein receptors on mechano-insensitive nociceptors, but sensitization may also be caused by changes in gene expression that affect expression of transduction proteins. In either case, the sensitization of mechano-insensitive nociceptors has been observed to result in hyperalgesia, chronic pain. About ten percent of the afferents classified as mechano-insensitive nociceptors seem to constitute a group of “itch specific” units because they respond to pruritogen substances including histamine with an activity that corresponds to the sensation of itch.\n\nThermoreceptors can be separated into two groups for warmth and cold detection. A subset of unmyelinated fibers are responsible for warmth detection. They are mechano-insensitive, low in number, and innervate small receptive fields. Aδ fibers are responsible for cold detection. However, there seems to be a subset of C-fibers that may function as cold-receptors along with A-fibers. Remarkably, these C-cold fibers seem to produce a sensation of unpleasant heat when there is no input from A-fibers. Altogether thermoreceptive afferents have not been studied as much as other systems.\n\nMicroneurography exploration of sympathetic efferent system is unique from technical point of view as multiunit recordings have been very prosperous whereas single unit recording is essential with most other systems. Soon after microneurography was launched it was demonstrated that sympathetic activity is much different in muscle and skin nerves. \nIn 1998, microneurography recordings were performed for the first time on a spaceflight aboard the Space Shuttle Columbia with the purpose to explore the effect of microgravity on the human sympathetic nerve system. Two astronauts measured MSNA from peroneal nerves of their fellow astronauts. The findings support earlier observations that weightlessness results in a decrease of MSNA activity through a baroreflex mechanism.\n\nThe microneurography technique allows the recording of impulse activity of individual nerve fibers with absolute resolution in attending human subjects. Hence the subject is able to cooperate in various kinds of tests while the exact and complete information carried by the individual nerve fiber is monitored and offered for analysis of correlations between neural activity and physical or mental events. On the other hand, the particular physical conditions involving a microelectrode freely floating in the tissue preclude brisk and large movements because the exact electrode position is easily jeopardized. The experiment is often time-consuming because the search procedure can be particularly demanding. Hence it is not suited as a diagnostic test in clinical routine whereas its strength is in its unique power for exploration of normal neural mechanisms as well as pathophysiological conditions of various neurological disorders. Microneurography records intact axons in vivo and is minimally invasive. There have been no reports of persistent nerve damage. As a result, repeated recordings with the same subject are possible and longitudinal observations can be made. In the experiment, it is important to create an atmosphere of psychological confidence and to observe carefully the subject´s reactions so that the procedure can be adjusted accordingly. The technique requires considerable training and skill and it is highly recommended that scientists who are interested to take up the method are trained in a laboratory where the method is running and that the scientist her-/himself has participated as the subject in a couple of experiments.\n\n\n"}
{"id": "12021567", "url": "https://en.wikipedia.org/wiki?curid=12021567", "title": "Murder of Benjamin Hermansen", "text": "Murder of Benjamin Hermansen\n\nBenjamin Hermansen (29 May 1985 – 26 January 2001) was a Norwegian-Ghanaian boy whose father was born in Ghana, his mother was Norwegian. He was stabbed to death at Holmlia in Oslo, Norway, just before midnight on 26 January 2001 by people from the Neo-Nazi group Boot Boys. Joe Erling Jahr (born 1981) and Ole Nicolai Kvisler (born 1979) were convicted of the murder and sentenced to 16 and 15 years in prison respectively. A third defendant, Veronica Andreassen, was convicted on a lesser charge of abetting bodily harm causing death and sentenced to three years in prison.\n\nSince the murder was motivated by racism, it mobilised large parts of the Norwegian population. Throughout the entire country, marches were organised to protest against the murder, with nearly 40,000 people participating in Oslo.\n\nHermansen was buried on 6 February 2001. \"Song to Benjamin\", written by several of his friends for the service, was presented at his funeral. The song was later recorded in studio by artists including Noora Noor and Briskeby.\n\nIn 2002, the Benjamin Prize was founded in Hermansen's memory. It is awarded on 27 January every year.\n\nPop singer Michael Jackson dedicated his 2001 album \"Invincible\" to Benjamin Hermansen (and also to his own parents and grandmother). The reason for this has partly to do with the fact that the Holmlia boy Omer Bhatti and Jackson were close friends, and Bhatti was at the same time a good friend of Benjamin Hermansen. On the album cover, next to the image of a rose, it reads:\n\nClara Dorothea Weltzin (1925–2007), an Oslo woman with far-right anti-immigration views, left 250,000 Norwegian kroner (c. US$43,000) to Ole Nicolai Kvisler in her will, something that caused major headlines in Norwegian media, and there were also suspicions regarding the legality of doing it.\n"}
{"id": "57077038", "url": "https://en.wikipedia.org/wiki?curid=57077038", "title": "Nick Corwin", "text": "Nick Corwin\n\nNick Corwin was an American student and athlete. He was murdered in his second grade classroom in Hubbard Woods School in Winnetka, IL. \n\nA dedicated athlete , he was remembered for his sportsmanship and skill. \n\nAlthough the history of school shootings in America goes back to pre-independence times, Nick’s murder was among the first to feature prominently in the 24-hour news cycle, mostly revolving around the mental state of his killer, Laurie Dann.\n\nBecause no other school shooting had received such wide coverage, Nick’s murder is sometimes called “the first school shooting.” Since his murder, a school shooting was widely reported almost every year. \n\nOthers noted that his shooting marked an “end of innocence” for the prosperous community along Chicago’s North Shore, which hadn’t had a murder in 30 years. \n\nDr. Donald Monroe, superintendent of Winnetka School District 36 noted 'his “safe” school was “not as isolated and insulated as we thought.\".\n\nAt the time of the shooting, Hubbard Woods, like many schools, was an open campus, with many doors, such as those to individual classrooms, kept open. After the shooting, a pattern of single-point entry emerged in more schools. \n\nCorwin is the namesake of a popular soccer field and playground in Winnetka. \n\nAccording to a report in \"People\" magazine, 1500 attended his funeral \n\nShortly after his death, playing on the meaning of his name (“giver of gifts”) his friends and schoolmates created a book, The Gifts that Nicholas Gave. He was remembered for his sportsmanship, kindness, and leadership. \n\nClassmates remember Corwin for his exemplary play. One told a reporter that the kids would not be able to play fairly, because Nick was the one who knew all the rules.\n\nHe is interred at Memorial Park Cemetery in Skokie IL \n\nFollowing his death, Winnetka passed a handgun ban, which stood until D.C. vs Heller and subsequent NRA lawsuits \n\nNick Corwin Park\n"}
{"id": "44699248", "url": "https://en.wikipedia.org/wiki?curid=44699248", "title": "Ohan Demirgian", "text": "Ohan Demirgian\n\nOhan Demirgian, also called Habib Bey (14 January 1837 – 1877), was an Egyptian-Armenian diplomat. He was the favorite of king Charles XV of Sweden.\n\nDemirgian was an Armenian and the son of the Egyptian foreign minister Stephan Bey. He was a student of the Mission égyptienne in Paris before he was employed as a secretary in the foreign ministry of the Egyptian vice-roy khediven Isma'il Pasha. \nDemirgian lived in France from 1844 to 1853 and studied at the Collège Stanislas de Paris and in Versailles. Thereafter, he moved to England where he lived for two years. After furthering his education in England, he returned to Egypt in 1855 where he married Arouse Araigian in 1859.\n\nHe met Prince August, Duke of Dalarna in Egypt in 1860. In 1866, he was placed in Sweden as the envoy of the Egyptian khedive. Demirgian were described as an adventurer, and his presence at court was regarded a scandal. He became a favorite of Charles XV, who awarded him Swedish citizenship in 1867, named him his secondary royal stable master at Ulriksdal Palace at 1868, and bestowed him several royal orders including the Order of Vasa.\n\nDemirgian was disliked at the Swedish royal court, where he was referred to as \"The Demiurg\". He resided in a house next to the Ulriksdal Palace, where he reportedly lived in \"Oriental luxury\" and arranged parties with women dancing with veils, who were allegedly then trafficked for prostitution to the Russian Empire. He was a member of the club \"Enkan Bloms Bekanta\" (The Acquaintances of the Widow Blom), the club of the monarch's private circle, including among others Svante Hedin and Daniel Hwasser. According to a contemporary description, he was: \"Insinuative, good-looking, small and dark, with burning eyes, extremely accommodating and polite to the ladies and keeps himself in the first line always\".\n\nIn 1869, he unofficially represented Charles XV at the inauguration ceremony of the Suez Canal in Egypt. At this occasion he awarded several Swedish royal orders to important people in Egypt, such as the Order of the Sword to the mother of the khedive. He had the wish to achieve an alliance between Sweden and Egypt.\nAfter the death of the queen, Louise of the Netherlands, in 1871, he offered to manage the negotiations for the king's next marriage. It was reportedly Ohan Demirgian who suggested the Polish countess Graciosa Krasińska as candidate for the second marriage of Charles XV, rather than Thyra of Denmark or a Russian Grand Duchess. Krasińska, who was distantly related to the House of Savoy, was described as a young beauty and a millionaire after her father, was by then living in Paris with her mother and stepfather. Demirgian was to be given one million in provision for handling the negotiations in collaboration with the Ottoman ambassador in Paris. The plan was to give Krasińska the befitting status for a non-morganatic marriage by making her stepfather a Spanish grande through her relative, the Spanish monarch, and then award him with the title Royal Hignhess by the Italian monarch: thereby, Krasińska, would become Hr Royal Highness Princess Maria and acceptable as Queen of Sweden after marriage to Charles XV, with their potential son heir to the Swedish throne before the brother of Charles XV. These plans were not popular among the royal house nor with the Swedish government, and foreign minister Baltzar von Platen made preparations to prevent the marriage. The negotiations failed because of the sudden and unexpected death of the king in 1872.\n\nSimilarly, he offered to negotiate a new marriage for the widowed Princess Therese before she was placed under guardianship. During his last years, he made several attempts to blackmail the Swedish Royal House. He reportedly died at a mental hospital in Marseilles, France in 1877.\n\n"}
{"id": "53466148", "url": "https://en.wikipedia.org/wiki?curid=53466148", "title": "R. N. Abhyankar", "text": "R. N. Abhyankar\n\nR. N. Abhyankar (dates unknown) was an Indian first-class cricketer active 1961–1968 who played for Vidarbha. He made 15 appearances, scoring 467 runs with a highest score of 73, and took ten wickets with a best innings return of three for 47.\n"}
{"id": "600500", "url": "https://en.wikipedia.org/wiki?curid=600500", "title": "Scenario planning", "text": "Scenario planning\n\nScenario planning, also called scenario thinking or scenario analysis, is a strategic planning method that some organizations use to make flexible long-term plans. It is in large part an adaptation and generalization of classic methods used by military intelligence.\n\nThe original method was that a group of analysts would generate simulation games for policy makers. The methods combine known facts about the future, such as demographics, geography, military, political, industrial information, and mineral reserves, with key driving forces identified by considering social, technical, economic, environmental, and political (STEEP) trends.\n\nIn business applications, the emphasis on gaming the behavior of opponents was reduced (shifting more toward a game against nature). At Royal Dutch/Shell for example, scenario planning was viewed as changing mindsets about the exogenous part of the world, prior to formulating specific strategies.\n\nScenario planning may involve aspects of systems thinking, specifically the recognition that many factors may combine in complex ways to create sometime surprising futures (due to non-linear feedback loops). The method also allows the inclusion of factors that are difficult to formalize, such as novel insights about the future, deep shifts in values, unprecedented regulations or inventions. Systems thinking used in conjunction with scenario planning leads to plausible scenario storylines because the causal relationship between factors can be demonstrated . In these cases when scenario planning is integrated with a systems thinking approach to scenario development, it is sometimes referred to as dynamic scenarios.\n\nThese combinations and permutations of fact and related social changes are called \"scenarios\". The scenarios usually include plausible, but unexpectedly important situations and problems that exist in some small form in the present day. Any particular scenario is unlikely. However, future studies analysts select scenario features so they are both possible and uncomfortable. Scenario planning help policy-makers and firms to anticipate change, prepare a response and create more robust strategies.\n\nScenarios help a firm to anticipate the impact of different scenarios identify weaknesses. When disclosed years in advance, these weaknesses can be avoided or their impacts reduced more effectively than if similar real-life problems were considered under duress of an emergency. For example, a company may discover that it needs to change contractual terms to protect against a new class of risks, or collect cash reserves to purchase anticipated technologies or equipment. Flexible business continuity plans with \"PREsponse protocols\" help cope with similar operational problems and deliver measurable future value-added.\n\nStrategic military intelligence organizations also construct scenarios. The methods and organizations are almost identical, except that scenario planning is applied to a wider variety of problems than merely military and political problems.\n\nAs in military intelligence, the chief challenge of scenario planning is to find out the real needs of policy-makers, when policy-makers may not themselves know what they need to know, or may not know how to describe the information that they really want.\n\nGood analysts design wargames so that policy makers have great flexibility and freedom to adapt their simulated organisations. Then these simulated organizations are \"stressed\" by the scenarios as a game plays out. Usually, particular groups of facts become more clearly important. These insights enable intelligence organizations to refine and repackage real information more precisely to better serve the policy-makers' real-life needs. Usually the games' simulated time runs hundreds of times faster than real life, so policy-makers experience several years of policy decisions, and their simulated effects, in less than a day.\n\nThis chief value of scenario planning is that it allows policy-makers to make and learn from mistakes without risking career-limiting failures in real life. Further, policymakers can make these mistakes in a safe, unthreatening, game-like environment, while responding to a wide variety of concretely presented situations based on facts. This is an opportunity to \"rehearse the future\", an opportunity that does not present itself in day-to-day operations where every action and decision counts.\n\n\nScenario planning is also extremely popular with military planners. Most states' department of war maintains a continuously updated series of strategic plans to cope with well-known military or strategic problems. These plans are almost always based on scenarios, and often the plans and scenarios are kept up-to-date by war games, sometimes played out with real troops. This process was first carried out (arguably the method was invented by) the Prussian general staff of the mid-19th century.\n\nIn the past, strategic plans have often considered only the \"official future\", which was usually a straight-line graph of current trends carried into the future. Often the trend lines were generated by the accounting department, and lacked discussions of demographics, or qualitative differences in social conditions.\n\nThese simplistic guesses are surprisingly good most of the time, but fail to consider qualitative social changes that can affect a business or government. Scenarios focus on the joint effect of many factors. Scenario planning helps us understand how the various strands of a complex tapestry move if one or more threads are pulled. When you just list possible causes, as for instance in fault tree analysis, you may tend to discount any one factor in isolation. But when you explore the factors together, you realize that certain combinations could magnify each other’s impact or likelihood. For instance, an increased trade deficit may trigger an economic recession, which in turn creates unemployment and reduces domestic production. Paul J. H. Schoemaker offers a strong managerial case for the use of scenario planning in business and had wide impact.\n\nScenarios planning starts by dividing our knowledge into two broad domains: (1) things we believe we know something about and (2) elements we consider uncertain or unknowable. The first component – trends – casts the past forward, recognizing that our world possesses considerable momentum and continuity. For example, we can safely make assumptions about demographic shifts and, perhaps, substitution effects for certain new technologies. The second component – true uncertainties – involve indeterminables such as future interest rates, outcomes of political elections, rates of innovation, fads and fashions in markets, and so on. The art of scenario planning lies in blending the known and the unknown into a limited number of internally consistent views of the future that span a very wide range of possibilities. In project management, this is called the cone of uncertainty.\n\nNumerous organizations have applied scenario planning to a broad range of issues, from relatively simple, tactical decisions to the complex process of strategic planning and vision building. The power of scenario planning for business was originally established by Royal Dutch/Shell, which has used scenarios since the early 1970s as part of a process for generating and evaluating its strategic options. Shell has been consistently better in its oil forecasts than other major oil companies, and saw the overcapacity in the tanker business and Europe’s petrochemicals earlier than its competitors. The approach may have had more impact outside Shell than within, as many others firms and consultancies started to benefit as well from scenario planning. Scenario planning is as much art as science, and prone to a variety of traps (both in process and content) as enumerated by Paul J. H. Schoemaker. More recently scenario planning has been discussed as a tool to improve the strategic agility, by cognitively preparing not only multiple scenarios but also multiple consistent strategies. \n\nMost authors attribute the introduction of scenario planning to Herman Kahn through his work for the US Military in the 1950s at the RAND Corporation where he developed a technique of describing the future in stories as if written by people in the future. He adopted the term \"scenarios\" to describe these stories. In 1961 he founded the Hudson Institute where he expanded his scenario work to social forecasting and public policy. One of his most controversial uses of scenarios was to suggest that a nuclear war could be won. Though Kahn is often cited as the father of scenario planning, at the same time Kahn was developing his methods at RAND, Gaston Berger was developing similar methods at the Centre d’Etudes Prospectives which he founded in France. His method, which he named 'La Prospective', was to develop normative scenarios of the future which were to be used as a guide in formulating public policy. During the mid-1960s various authors from the French and American institutions began to publish scenario planning concepts such as 'La Prospective' by Berger in 1964 and 'The Next Thirty-Three Years' by Kahn and Wiener in 1967. By the 1970s scenario planning was in full swing with a number of institutions now established to provide support to business including the Hudson Foundation, the Stanford Research Institute (now SRI International), and the SEMA Metra Consulting Group in France. Several large companies also began to embrace scenario planning including DHL Express, Dutch Royal Shell and General Electric.\n\nPossibly as a result of these very sophisticated approaches, and of the difficult techniques they employed (which usually demanded the resources of a central planning staff), scenarios earned a reputation for difficulty (and cost) in use. Even so, the theoretical importance of the use of alternative scenarios, to help address the uncertainty implicit in long-range forecasts, was dramatically underlined by the widespread confusion which followed the Oil Shock of 1973. As a result, many of the larger organizations started to use the technique in one form or another. By 1983 Diffenbach reported that 'alternate scenarios' were the third most popular technique for long-range forecasting – used by 68% of the large organizations he surveyed.\nPractical development of scenario forecasting, to guide strategy rather than for the more limited academic uses which had previously been the case, was started by Pierre Wack in 1971 at the Royal Dutch Shell group of companies – and it, too, was given impetus by the Oil Shock two years later. Shell has, since that time, led the commercial world in the use of scenarios – and in the development of more practical techniques to support these. Indeed, as – in common with most forms of long-range forecasting – the use of scenarios has (during the depressed trading conditions of the last decade) reduced to only a handful of private-sector organisations, Shell remains almost alone amongst them in keeping the technique at the forefront of forecasting.\n\nThere has only been anecdotal evidence offered in support of the value of scenarios, even as aids to forecasting; and most of this has come from one company – Shell. In addition, with so few organisations making consistent use of them – and with the timescales involved reaching into decades – it is unlikely that any definitive supporting evidenced will be forthcoming in the foreseeable future. For the same reasons, though, a lack of such proof applies to almost all long-range planning techniques. In the absence of proof, but taking account of Shell's well documented experiences of using it over several decades (where, in the 1990s, its then CEO ascribed its success to its use of such scenarios), can be significant benefit to be obtained from extending the horizons of managers' long-range forecasting in the way that the use of scenarios uniquely does.\n\nIn the 1970s, many energy companies were surprised by both environmentalism and the OPEC cartel, and thereby lost billions of dollars of revenue by mis-investment. The dramatic financial effects of these changes led at least one organization, Royal Dutch Shell, to implement scenario planning. The analysts of this company publicly estimated that this planning process made their company the largest in the world. However other observers of Shell's use of scenario planning have suggested that few if any significant long term business advantages accrued to Shell from the use of scenario methodology. Whilst the intellectual robustness of Shell's long term scenarios was seldom in doubt their actual practical use was seen as being minimal by many senior Shell executives. A Shell insider has commented \"The scenario team were bright and their work was of a very high intellectual level. However neither the high level \"Group scenarios\" nor the country level scenarios produced with operating companies really made much difference when key decisions were being taken\".\n\nThe use of scenarios was audited by Arie de Geus's team in the early 1980s and they found that the decision-making processes following the scenarios were the primary cause of the lack of strategic implementation ), rather than the scenarios themselves. Many practitioners today spend as much time on the decision-making process as on creating the scenarios themselves.\n\nAlthough scenario planning has gained much adherence in industry, its subjective and heuristic nature leaves many academics uncomfortable. How do we know if we have the right scenarios? And how do we go from scenarios to decisions? These concerns are legitimate and scenario planning would gain in academic standing if more research were conducted on its comparative performance and underlying theoretical premises. A collection of chapters by noted scenario planners failed to contain a single reference to an academic source, though this may be because academics have not caught up or do not have the resources to either do or teach scenario planning. In general, there are few academically validated analyses of scenario planning (for a notable exception, see Paul J. H. Schoemaker). The technique was born from practice and its appeal is based more on experience than scientific evidence. Furthermore, significant misconceptions remain about its intent and claims. Above all, scenario planning is a tool for collective learning, reframing perceptions and preserving uncertainty when the latter is pervasive. Too many decision makers want to bet on one future scenario, falling prey to the seductive temptation of trying to predict the future rather than to entertain multiple futures. Another trap is to take the scenarios too literally as though they were static beacons that map out a fixed future. In actuality, their aim is to bound the future but in a flexible way that permits learning and adjustment as the future unfolds.\n\nOne criticism of the two-by-two technique commonly used is that the resulting matrix results in four somewhat arbitrary scenario themes. If other key uncertainties had been selected, it might be argued, very different scenarios could emerge. How true this is depends on whether the matrix is viewed as just a starting point to be superseded by the ensuing blueprint or is considered as the grand architecture that nests everything else. In either case, however, the issue should not be which are the “right” scenarios but rather whether they delineate the range of possible future appropriately. Any tool that tries to simplify a complex picture will introduce distortions, whether it is a geographic map or a set of scenarios. Seldom will complexity decompose naturally into simple states. But it might. Consider, for example, the behavior of water (the molecule HO) which, depending on temperature and pressure, naturally exists in just one of three states: gas, liquid or ice. The art of scenarios is to look for such natural states or points of bifurcation in the behavior of a complex system.\n\nApart from some inherent subjectivity in scenario design, the technique can suffer from various process and content traps. These traps mostly relate to how the process is conducted in organizations (such as team composition, role of facilitators, etc.) as well as the substantive focus of the scenarios (long vs. short term, global vs. regional, incremental vs. paradigm shifting, etc.). One might think of these as merely challenges of implementation, but since the process component is integral to the scenario experience, they can also be viewed as weaknesses of the methodology itself. Limited safeguards exist against political derailing, agenda control, myopia and limited imagination when conducting scenario planning exercises within real organizations. But, to varying extents, all forecasting techniques will suffer from such organizational limitations. The benchmark to use is not perfection, especially when faced with high uncertainty and complexity, or even strict adherence to such normative precepts as procedural invariance and logical consistency, but whether the technique performs better than its rivals. And to answer this question fairly, performance must be carefully specified. It should clearly include some measures of accuracy as well as a cost-benefit analysis that considers the tradeoff between effort and accuracy. In addition, legitimation criteria may be important to consider as well as the ability to refine and improve the approach as more experience is gained.\n\nA third limitation of scenario planning in organizational settings is its weak integration into other planning and forecasting techniques. Most companies have plenty of trouble dealing with just one future, let alone multiple ones. Typically, budgeting and planning systems are predicated on single views of the future, with adjustments made as necessary through variance analysis, contingency planning, rolling budgets, and periodic renegotiations. The weaknesses of these traditional approaches were very evident after the tragic attack of September 11, 2001 when many companies became paralyzed and quite a few just threw away the plan and budget. Their strategies were not future-proof and they lacked organized mechanisms to adjust to external turmoil. In cases of crisis, leadership becomes important but so does some degree of preparedness. Once the scenarios are finished, the real works starts of how to craft flexible strategies and appropriate monitoring systems. Managers need a simple but comprehensive compass to navigate uncertainty from beginning to end. Scenario planning is just one component of a more complete management system. The point is that scenario thinking needs to be integrated with the existing planning and budgeting system, as awkward as this fit may be. The reality is that most organizations do not handle uncertainty well and that researchers have not provided adequate answers about how to plan under conditions of high uncertainty and complexity.\n\nThe basic concepts of the process are relatively simple. In terms of the overall approach to forecasting, they can be divided into three main groups of activities (which are, generally speaking, common to all long range forecasting processes):\nThe first of these groups quite simply comprises the normal environmental analysis. This is almost exactly the same as that which should be undertaken as the first stage of any serious long-range planning. However, the quality of this analysis is especially important in the context of scenario planning. \nThe central part represents the specific techniques – covered here – which differentiate the scenario forecasting process from the others in long-range planning.\nThe final group represents all the subsequent processes which go towards producing the corporate strategy and plans. Again, the requirements are slightly different but in general they follow all the rules of sound long-range planning.\n\nThe part of the overall process which is radically different from most other forms of long-range planning is the central section, the actual production of the scenarios. Even this, though, is relatively simple, at its most basic level. As derived from the approach most commonly used by Shell, it follows six steps:\n\nThe first stage is to examine the results of environmental analysis to determine which are the most important factors that will decide the nature of the future environment within which the organisation operates. These factors are sometimes called 'variables' (because they will vary over the time being investigated, though the terminology may confuse scientists who use it in a more rigorous manner). Users tend to prefer the term 'drivers' (for change), since this terminology is not laden with quasi-scientific connotations and reinforces the participant's commitment to search for those forces which will act to change the future. Whatever the nomenclature, the main requirement is that these will be informed assumptions.\nThis is partly a process of analysis, needed to recognise what these 'forces' might be. However, it is likely that some work on this element will already have taken place during the preceding environmental analysis. By the time the formal scenario planning stage has been reached, the participants may have already decided – probably in their sub-conscious rather than formally – what the main forces are.\nIn the ideal approach, the first stage should be to carefully decide the overall assumptions on which the scenarios will be based. Only then, as a second stage, should the various drivers be specifically defined. Participants, though, seem to have problems in separating these stages.\nPerhaps the most difficult aspect though, is freeing the participants from the preconceptions they take into the process with them. In particular, most participants will want to look at the medium term, five to ten years ahead rather than the required longer-term, ten or more years ahead. However, a time horizon of anything less than ten years often leads participants to extrapolate from present trends, rather than consider the alternatives which might face them. When, however, they are asked to consider timescales in excess of ten years they almost all seem to accept the logic of the scenario planning process, and no longer fall back on that of extrapolation. There is a similar problem with expanding participants horizons to include the whole external environment.\n\nBrainstorming\nIn any case, the brainstorming which should then take place, to ensure that the list is complete, may unearth more variables – and, in particular, the combination of factors may suggest yet others. \nA very simple technique which is especially useful at this – brainstorming – stage, and in general for handling scenario planning debates is derived from use in Shell where this type of approach is often used. An especially easy approach, it only requires a conference room with a bare wall and copious supplies of 3M Post-It Notes.\nThe six to ten people ideally taking part in such face-to-face debates should be in a conference room environment which is isolated from outside interruptions. The only special requirement is that the conference room has at least one clear wall on which Post-It notes will stick. At the start of the meeting itself, any topics which have already been identified during the environmental analysis stage are written (preferably with a thick magic marker, so they can be read from a distance) on separate Post-It Notes. These Post-It Notes are then, at least in theory, randomly placed on the wall. In practice, even at this early stage the participants will want to cluster them in groups which seem to make sense. The only requirement (which is why Post-It Notes are ideal for this approach) is that there is no bar to taking them off again and moving them to a new cluster.\n\nA similar technique – using 5\" by 3\" index cards – has also been described (as the 'Snowball Technique'), by Backoff and Nutt, for grouping and evaluating ideas in general.\nAs in any form of brainstorming, the initial ideas almost invariably stimulate others. Indeed, everyone should be encouraged to add their own Post-It Notes to those on the wall. However it differs from the 'rigorous' form described in 'creative thinking' texts, in that it is much slower paced and the ideas are discussed immediately. In practice, as many ideas may be removed, as not being relevant, as are added. Even so, it follows many of the same rules as normal brainstorming and typically lasts the same length of time – say, an hour or so only.\nIt is important that all the participants feel they 'own' the wall – and are encouraged to move the notes around themselves. The result is a very powerful form of creative decision-making for groups, which is applicable to a wide range of situations (but is especially powerful in the context of scenario planning). It also offers a very good introduction for those who are coming to the scenario process for the first time. Since the workings are largely self-evident, participants very quickly come to understand exactly what is involved. \nImportant and uncertain\nThis step is, though, also one of selection – since only the most important factors will justify a place in the scenarios. The 80:20 Rule here means that, at the end of the process, management's attention must be focused on a limited number of most important issues. Experience has proved that offering a wider range of topics merely allows them to select those few which interest them, and not necessarily those which are most important to the organisation.\nIn addition, as scenarios are a technique for presenting alternative futures, the factors to be included must be genuinely 'variable'. They should be subject to significant alternative outcomes. Factors whose outcome is predictable, but important, should be spelled out in the introduction to the scenarios (since they cannot be ignored). The Important Uncertainties Matrix, as reported by Kees van der Heijden of Shell, is a useful check at this stage.\nAt this point it is also worth pointing out that a great virtue of scenarios is that they can accommodate the input from any other form of forecasting. They may use figures, diagrams or words in any combination. No other form of forecasting offers this flexibility.\n\nThe next step is to link these drivers together to provide a meaningful framework. This may be obvious, where some of the factors are clearly related to each other in one way or another. For instance, a technological factor may lead to market changes, but may be constrained by legislative factors. On the other hand, some of the 'links' (or at least the 'groupings') may need to be artificial at this stage. At a later stage more meaningful links may be found, or the factors may then be rejected from the scenarios. In the most theoretical approaches to the subject, probabilities are attached to the event strings. This is difficult to achieve, however, and generally adds little – except complexity – to the outcomes.\nThis is probably the most (conceptually) difficult step. It is where managers' 'intuition' – their ability to make sense of complex patterns of 'soft' data which more rigorous analysis would be unable to handle – plays an important role. There are, however, a range of techniques which can help; and again the Post-It-Notes approach is especially useful:\nThus, the participants try to arrange the drivers, which have emerged from the first stage, into groups which seem to make sense to them. Initially there may be many small groups. The intention should, therefore, be to gradually merge these (often having to reform them from new combinations of drivers to make these bigger groups work). The aim of this stage is eventually to make 6–8 larger groupings; 'mini-scenarios'. Here the Post-It Notes may be moved dozens of times over the length – perhaps several hours or more – of each meeting. While this process is taking place the participants will probably want to add new topics – so more Post-It Notes are added to the wall. In the opposite direction, the unimportant ones are removed (possibly to be grouped, again as an 'audit trail' on another wall). More important, the 'certain' topics are also removed from the main area of debate – in this case they must be grouped in clearly labelled area of the main wall.\nAs the clusters – the 'mini-scenarios' – emerge, the associated notes may be stuck to each other rather than individually to the wall; which makes it easier to move the clusters around (and is a considerable help during the final, demanding stage to reducing the scenarios to two or three).\nThe great benefit of using Post-It Notes is that there is no bar to participants changing their minds. If they want to rearrange the groups – or simply to go back (iterate) to an earlier stage – then they strip them off and put them in their new position.\n\nThe outcome of the previous step is usually between seven and nine logical groupings of drivers. This is usually easy to achieve. The 'natural' reason for this may be that it represents some form of limit as to what participants can visualise. \nHaving placed the factors in these groups, the next action is to work out, very approximately at this stage, what is the connection between them. What does each group of factors represent?\n\nThe main action, at this next stage, is to reduce the seven to nine mini-scenarios/groupings detected at the previous stage to two or three larger scenarios. The challenge in practice seems to come down to finding just two or three 'containers' into which all the topics can be sensibly fitted. This usually requires a considerable amount of debate – but in the process it typically generates as much light as it does heat. Indeed, the demanding process of developing these basic scenario frameworks often, by itself, produces fundamental insights into what are the really important (perhaps life and death) issues affecting the organisation. During this extended debate – and even before it is summarised in the final reports – the participants come to understand, by their own involvement in the debate, what the most important drivers for change may be, and (perhaps even more important) what their peers think they are. Based on this intimate understanding, they are well prepared to cope with such changes – reacting almost instinctively – when they actually do happen; even without recourse to the formal reports which are eventually produced!\nThere is no theoretical reason for reducing to just two or three scenarios, only a practical one. It has been found that the managers who will be asked to use the final scenarios can only cope effectively with a maximum of three versions! Shell started, more than three decades ago, by building half a dozen or more scenarios – but found that the outcome was that their managers selected just one of these to concentrate on. As a result, the planners reduced the number to three, which managers could handle easily but could no longer so easily justify the selection of only one! This is the number now recommended most frequently in most of the literature. \nComplementary scenarios\nAs used by Shell, and as favoured by a number of the academics, two scenarios should be complementary; the reason being that this helps avoid managers 'choosing' just one, 'preferred', scenario – and lapsing once more into single-track forecasting (negating the benefits of using 'alternative' scenarios to allow for alternative, uncertain futures). This is, however, a potentially difficult concept to grasp, where managers are used to looking for opposites; a good and a bad scenario, say, or an optimistic one versus a pessimistic one – and indeed this is the approach (for small businesses) advocated by Foster. In the Shell approach, the two scenarios are required to be equally likely, and between them to cover all the 'event strings'/drivers. Ideally they should not be obvious opposites, which might once again bias their acceptance by users, so the choice of 'neutral' titles is important. For example, Shell's two scenarios at the beginning of the 1990s were titled 'Sustainable World' and 'Global Mercantilism'[xv]. In practice, we found that this requirement, much to our surprise, posed few problems for the great majority, 85%, of those in the survey; who easily produced 'balanced' scenarios. The remaining 15% mainly fell into the expected trap of 'good versus bad'. We have found that our own relatively complex (OBS) scenarios can also be made complementary to each other; without any great effort needed from the teams involved; and the resulting two scenarios are both developed further by all involved, without unnecessary focusing on one or the other.\nTesting\nHaving grouped the factors into these two scenarios, the next step is to test them, again, for viability. Do they make sense to the participants? This may be in terms of logical analysis, but it may also be in terms of intuitive 'gut-feel'. Once more, intuition often may offer a useful – if academically less respectable – vehicle for reacting to the complex and ill-defined issues typically involved. If the scenarios do not intuitively 'hang together', why not? The usual problem is that one or more of the assumptions turns out to be unrealistic in terms of how the participants see their world. If this is the case then you need to return to the first step – the whole scenario planning process is above all an iterative one (returning to its beginnings a number of times until the final outcome makes the best sense).\n\nThe scenarios are then 'written up' in the most suitable form. The flexibility of this step often confuses participants, for they are used to forecasting processes which have a fixed format. The rule, though, is that you should produce the scenarios in the form most suitable for use by the managers who are going to base their strategy on them. Less obviously, the managers who are going to implement this strategy should also be taken into account. They will also be exposed to the scenarios, and will need to believe in these. This is essentially a 'marketing' decision, since it will be very necessary to 'sell' the final results to the users. On the other hand, a not inconsiderable consideration may be to use the form the author also finds most comfortable. If the form is alien to him or her the chances are that the resulting scenarios will carry little conviction when it comes to the 'sale'. \nMost scenarios will, perhaps, be written in word form (almost as a series of alternative essays about the future); especially where they will almost inevitably be qualitative which is hardly surprising where managers, and their audience, will probably use this in their day to day communications. Some, though use an expanded series of lists and some enliven their reports by adding some fictional 'character' to the material – perhaps taking literally the idea that they are stories about the future – though they are still clearly intended to be factual. On the other hand, they may include numeric data and/or diagrams – as those of Shell do (and in the process gain by the acid test of more measurable 'predictions').\n\nThe final stage of the process is to examine these scenarios to determine what are the most critical outcomes; the 'branching points' relating to the 'issues' which will have the greatest impact (potentially generating 'crises') on the future of the organisation. The subsequent strategy will have to address these – since the normal approach to strategy deriving from scenarios is one which aims to minimise risk by being 'robust' (that is it will safely cope with all the alternative outcomes of these 'life and death' issues) rather than aiming for performance (profit) maximisation by gambling on one outcome.\n\nIt is important to note that scenarios may be used in a number of ways:\na) Containers for the drivers/event strings\nMost basically, they are a logical device, an artificial framework, for presenting the individual factors/topics (or coherent groups of these) so that these are made easily available for managers' use – as useful ideas about future developments in their own right – without reference to the rest of the scenario. It should be stressed that no factors should be dropped, or even given lower priority, as a result of producing the scenarios. In this context, which scenario contains which topic (driver), or issue about the future, is irrelevant. \nb) Tests for consistency\nAt every stage it is necessary to iterate, to check that the contents are viable and make any necessary changes to ensure that they are; here the main test is to see if the scenarios seem to be internally consistent – if they are not then the writer must loop back to earlier stages to correct the problem. Though it has been mentioned previously, it is important to stress once again that scenario building is ideally an iterative process. It usually does not just happen in one meeting – though even one attempt is better than none – but takes place over a number of meetings as the participants gradually refine their ideas.\nc) Positive perspectives\nPerhaps the main benefit deriving from scenarios, however, comes from the alternative 'flavors' of the future their different perspectives offer. It is a common experience, when the scenarios finally emerge, for the participants to be startled by the insight they offer – as to what the general shape of the future might be – at this stage it no longer is a theoretical exercise but becomes a genuine framework (or rather set of alternative frameworks) for dealing with that.\n\nScenario planning differs from contingency planning, sensitivity analysis and computer simulations.\n\nContingency planning is a \"What if\" tool, that only takes into account one uncertainty. However, scenario planning considers combinations of uncertainties in each scenario. Planners also try to select especially plausible but uncomfortable combinations of social developments.\n\nSensitivity analysis analyzes changes in one variable only, which is useful for simple changes, while scenario planning tries to expose policy makers to significant interactions of major variables.\n\nWhile scenario planning can benefit from computer simulations, scenario planning is less formalized, and can be used to make plans for qualitative patterns that show up in a wide variety of simulated events.\n\nDuring the past 5 years, computer supported Morphological Analysis has been employed as aid in scenario development by the Swedish Defence Research Agency in Stockholm. This method makes it possible to create a multi-variable morphological field which can be treated as an inference model – thus integrating scenario planning techniques with contingency analysis and sensitivity analysis.\n\nScenario planning concerns planning based on the systematic examination of the future by picturing plausible and consistent images thereof. Delphi, in turn, attempts to develop systematically expert opinion consensus concerning future developments and events. It is a judgmental forecasting procedure in form of an anonymous, written, multi-stage survey process, where feedback of group opinion is provided after each round.\n\nNumerous researchers have stressed that both approaches are best suited to be combined. Kinkel et al. (2006) recently reported on their experiences with both Delphi-scenarios and scenario-Delphis. The authors found that, due to their process similarity, the two methodologies can be easily combined. Generally speaking, the output of the different phases of the Delphi method can be used as input for the scenario method and vice versa. A combination makes a realization of the benefits of both tools possible. In practice, usually one of the two tools is considered the dominant methodology and the other one is integrated at some stage. In fact, the authors found that in either case the combination of the methodologies adds significant value to futures projects.\n\nThe variant that is most often found in practice is the integration of the Delphi method into the scenario process (see e.g. Rikkonen, 2005; von der Gracht, 2007; Transportation & Logistics 2030 – How will supply chains evolve in an energy constrained and low-carbon world; Transportation & Logistics 2030 – Transport infrastructure – Engine or hand brakes for global supply chains?; Future of Logistics – Global Scenarios 2025). Authors refer to this type as Delphi-scenario (writing), expert-based scenarios, or Delphi panel derived scenarios. Von der Gracht (2010) is a scientifically valid example of this method. Since scenario planning is “information hungry”, Delphi research can deliver valuable input for the process. There are various types of information output of Delphi that can be used as input for scenario planning. Researchers can, for example, identify relevant events or developments and, based on expert opinion, assign probabilities to them. Moreover, expert comments and arguments provide deeper insights into relationships of factors that can, in turn, be integrated into scenarios afterwards. Also, Delphi helps to identify extreme opinions and dissent among the experts. Such controversial topics are particularly suited for extreme scenarios or wildcards.\n\nIn his doctoral thesis, Rikkonen (2005) examined the utilization of Delphi techniques in scenario planning and, concretely, in construction of scenarios. The author comes to the conclusion that the Delphi technique has instrumental value in providing different alternative futures and the argumentation of scenarios. It is therefore recommended to use Delphi in order to make the scenarios more profound and to create confidence in scenario planning. Further benefits lie in the simplification of the scenario writing process and the deep understanding of the interrelations between the forecast items and social factors.\n\n\n\n\n\n\n"}
{"id": "5099023", "url": "https://en.wikipedia.org/wiki?curid=5099023", "title": "Tensor product network", "text": "Tensor product network\n\nA tensor product network, in artificial neural networks, is a network that exploits the properties of tensors to model associative concepts such as variable assignment. Orthonormal vectors are chosen to model the ideas (such as variable names and target assignments), and the tensor product of these vectors construct a network whose mathematical properties allow the user to easily extract the association from it.\n\n"}
{"id": "14275659", "url": "https://en.wikipedia.org/wiki?curid=14275659", "title": "United Nations moratorium on the death penalty", "text": "United Nations moratorium on the death penalty\n\nAt Italy's instigation, the UN moratorium on the death penalty resolution was presented by the EU in partnership with eight co-author member States to the General Assembly of the United Nations, calling for general suspension (not abolition) of capital punishment throughout the world. It was twice affirmed: first, on 15 November 2007 by the Third Committee, and then subsequently reaffirmed on 18 December by the United Nations General Assembly resolution 62/149. New Zealand played a central role facilitating agreement between the co-author group and other supporters.\n\nIt calls on States that maintain the death penalty to establish a moratorium on the use of the death penalty with a view to abolition, and in the meantime, to restrict the number of offences which it punishes and to respect the rights of those on death row. It also calls on States that have abolished the death penalty not to reintroduce it. Like all General Assembly resolutions, it is not binding on any state.\n\nOn 18 December 2007, the United Nations General Assembly voted 104 to 54 in favour of resolution A/RES/62/149, which proclaims a global moratorium on the death penalty, with 29 abstentions (as well as 5 absent at the time of the vote). Italy had proposed and sponsored this resolution. After the resolution's approval, Italian Foreign Minister Massimo D'Alema declared: \"Now we must start working on the abolition of the death penalty\".\n\nOn 18 December 2008, the General Assembly adopted another resolution (A/RES/63/168) reaffirming its previous call for a global moratorium on capital punishment 106 to 46 (with 34 abstentions and another 6 were absent at the time of the vote). Working in partnership with the EU, New Zealand and Mexico were co-facilitators of the draft text which was developed over a period of six months, which Chile then presented to the UN General Assembly on behalf of cosponsors.\n\nOn 21 December 2010, the 65th General Assembly adopted a third resolution (A/RES/65/206) with 109 countries voting in favour, 41 against and 35 abstentions (another seven countries were absent at the time of the vote).\n\nOn 20 December 2012, the 67th General Assembly adopted a fourth resolution (A/RES/67/176) with 111 countries voting in favour, 41 against and 34 abstentions (another seven countries were absent).\n\nOn 18 December 2014, the 69th General Assembly adopted a fifth resolution (A/RES/69/186) with 117 countries voting in favour, 38 against and 34 abstentions (another four countries were absent).\n\nOn 19 December 2016, the 71st General Assembly adopted a sixth resolution (A/RES/71/187) with 117 countries voting in favour, 40 against and 31 abstentions (another five countries were absent).\n\nThe UN moratorium campaign was launched in Italy by the association Hands Off Cain, affiliated to the Nonviolent Radical Party. The association against death penalty and torture was founded in Rome in 1993 by former left-wing terrorist and now nonviolent politician and human rights' activist Sergio D'Elia, with his first wife Mariateresa Di Lascia and Italian Radicals' liberal leaders Marco Pannella and Emma Bonino (former European commissioner).\n\nIn 1994 a resolution for a moratorium was presented for the first time at the United Nations General Assembly (UNGA) by the Italian government. It lost by eight votes. Since 1997, through Italy's initiative, and since 1999 through the EU's endeavour, the United Nations Commission of Human Rights (UNCHR) has been approving a resolution calling for a moratorium on executions with a view to completely abolishing the death penalty, every year. The 2007 vote at the Third Committee of the United Nations General Assembly saw intense diplomatic activity in favour of the moratorium by EU countries, and by the Nonviolent Radical Party itself; the Catholic Community of Sant'Egidio joined forces by submitting to the U.N. an appeal and 5,000,000 signatures asking for the moratorium to be passed..\n\n\"The General Assembly\",\n\n\"Guided\" by the purposes and principles contained in the Charter of the United Nations,\n\n\"Recalling\" the Universal Declaration of Human Rights, the International Covenant on Civil and Political Rights and the Convention on the Rights of the Child,\n\n\"Recalling also\" the resolutions on the question of the death penalty adopted over the past decade by the Commission on Human Rights in all consecutive sessions, the last being its resolution 2005/59 of 20 April 2005, in which the Commission called upon states that still maintain the death penalty to abolish it completely and, in the meantime, to establish a moratorium on executions,\n\n\"Recalling further\" the important results accomplished by the former Commission of Human Rights on the question of the death penalty, and envisaging that the Human Rights Council could continue to work on this issue,\n\n\"Considering\" that the use of the death penalty undermines human dignity, and convinced that a moratorium on the use of the death penalty contributes to the enhancement and progressive development of Human Rights, that there is no conclusive evidence that the death penalty's deterrent value and that any miscarriage or failure of justice in the death penalty's implementation is irreversible and irreparable,\n\n\"Welcoming\" the decisions taken by an increasing number of States to apply a moratorium on executions, followed in many cases by the abolition of the death penalty,\n\n1. \"Expresses its deep concern\" about the continued application of the death penalty;\n\n2. \"Calls upon\" all States that still maintain the death penalty to;\n\n(a) Respect international standards that provide safeguards guaranteeing the protection of the rights of those facing the death penalty, in particular the minimum standards, as set out in the annexe to Economic and Social Council resolution 1984/50 of 25 May 1984;\n\n(b) Provide the Secretary-General with information relating to the use of Capital Punishment and the observance of the safeguards guaranteeing the protection of the rights of those facing the death penalty;\n\n(c) Progressively restrict the use of the death penalty and reduce the number of offences for which it may be imposed;\n\n(d) Establish a moratorium on executions with a view to abolishing the death penalty;\n\n3. \"Calls upon\" States which have abolished the death penalty not to reintroduce it;\n\n4. \"Requests\" the Secretary-General to report to the General Assembly at its sixty-third session on the implementation of the present resolution;\n\n5. \"Decides\" to continue consideration of the matter at its sixty-third session under the same agenda item.\n\n\"The General Assembly,\"\n\"Reaffirming\" its resolution 62/149 of 18 December 2007 on a moratorium on \nthe use of the death penalty,\n\"Welcoming\" the decisions taken by an increasing number of States to apply a \nmoratorium on executions and the global trend towards the abolition of the death \npenalty,\n1. \"Welcomes\" the report of the Secretary-General on the implementation of resolution 62/149, and the conclusions and recommendations contained therein;\n2. \"Requests\" the Secretary-General to provide a report on progress made in the implementation of resolution 62/149 and the present resolution, for consideration during its sixty-fifth session, and calls upon Member States to provide the Secretary-General with information in this regard;\n3. \"Decides\" to continue consideration of the matter at its sixty-fifth session under the item entitled \"Promotion and protection of human rights\".\n\n\"The General Assembly,\"\n\n\"Guided\" by the purposes and principles contained in the Charter of the United \nNations,\n\"Recalling\" the Universal Declaration of Human Rights, the International Covenant on Civil and Political Rights and the Convention on the Rights of the Child,\n\n\"Reaffirming\" its resolutions 62/149 of 18 December 2007 and 63/168 of \n18 December 2008 on the question of a moratorium on the use of the death penalty, \nin which the General Assembly called upon States that still maintain the death \npenalty to establish a moratorium on executions with a view to abolishing it,\n\n\"Mindful\" that any miscarriage or failure of justice in the implementation of the \ndeath penalty is irreversible and irreparable,\n\"Convinced\" that a moratorium on the use of the death penalty contributes to \nrespect for human dignity and to the enhancement and progressive development of \nhuman rights, and considering that there is no conclusive evidence of the deterrent \nvalue of the death penalty,\n\n\"Noting\" ongoing national debates and regional initiatives on the death penalty, \nas well as the readiness of an increasing number of Member States to make \navailable information on the use of the death penalty,\n\"Noting also\" the technical cooperation among Member States in relation to \nmoratoriums on the death penalty,\n1. \"Welcomes\" the report of the Secretary-General on the implementation of\nresolution 63/168 and the recommendations contained therein;\n\n2. \"Also welcomes\" the steps taken by some countries to reduce the number of \noffences for which the death penalty may be imposed and the decisions made by an \nincreasing number of States to apply a moratorium on executions, followed in many \ncases by the abolition of the death penalty;\n\n3. \"Calls upon\" all States:\n\n\"(a)\" To respect international standards that provide safeguards guaranteeing \nprotection of the rights of those facing the death penalty, in particular the minimum \nstandards, as set out in the annex to Economic and Social Council resolution \n1984/50 of 25 May 1984, as well as to provide the Secretary-General with \ninformation in this regard;\n\"(b)\" To make available relevant information with regard to their use of the \ndeath penalty, which can contribute to possible informed and transparent national \ndebates;\n\"(c)\" To progressively restrict the use of the death penalty and to reduce the \nnumber of offences for which it may be imposed;\n\n\"(d)\" To establish a moratorium on executions with a view to abolishing the \ndeath penalty;\n4. \"Calls upon\" States which have abolished the death penalty not to \nreintroduce it, and encourages them to share their experience in this regard;\n5. \"Requests\" the Secretary-General to report to the General Assembly at its \nsixty-seventh session on the implementation of the present resolution;\n6. \"Decides\" to continue its consideration of the matter at its sixty-seventh \nsession under the item entitled \"Promotion and protection of human rights\".\n\n\n"}
{"id": "10833249", "url": "https://en.wikipedia.org/wiki?curid=10833249", "title": "Visual ethics", "text": "Visual ethics\n\nVisual ethics is an emerging interdisciplinary field of scholarship that brings together religious studies, philosophy, photo and video journalism, visual arts, and cognitive science in order to explore the ways human beings relate to others ethically through visual perception. Historically, the field of ethics has relied heavily on rational-linguistic approaches, largely ignoring the importance of seeing and visual representation to human moral behavior. At the same time, studies in visual culture tend to analyze imagistic representations while ignoring many of the ethical dimensions involved. Visual ethics is a field of cross-fertilization of ethics and visual culture studies that seeks to understand how the production and reception of visual images is always ethical, whether or not we are consciously aware of this fact.\n\nOn the one hand, visual ethics is concerned with ethical issues involved in the production of visual images. For example, how do representations in newsmedia deploy cultural codes of race, class, ethnicity, gender, and so on in order to create distance from or empathy with specific people and groups? How can visual representations of the other facilitate or foreclose certain ethical responses from viewers? When is it ethically justifiable to capture and share images of another person in a moment of vulnerability? With whom should such images be shared?\n\nVisual ethics is equally concerned with the ethics of reception, that is, with seeing as an ethical act. How do different images influence our ethical responses and moral behavior in different ways? To what extent do our ethical responses to images take place pre-reflectively, by visual-perceptual processes in the body-mind, before images even come to consciousness? It can be looked upon more into the cultural perception. It always depends on the cultural background.\n\nThis topic focuses on ethical theories and methods of ethical reasoning. Controversies and arguments abound as ethical decisions, or the lack thereof, continue to play a role in institutional practice. With the increasing gap between commerce and culture, the prioritization of good business over public service creates an increasingly blurry set of ethical guidelines. Collector-based exhibitions, conflicts of interest, and the de-accessioning practices of collections. One might ask do museums have a responsibility to their public? And if so, is this a part of institutional culture and is it being taught in today’s museum studies programs? Elaine A. King and co-editor Gail Levin addressed many of these issues in the anthology they compiled titled \"Ethics and The Visual Arts\" published in September 2006 by Allworth Press in New York. This volume of 19 essays explores a diverse range of topics about ethics in the visual arts. The dark side of the arts is explored in this volume with nineteen diverse essays by such distinguished authors as Eric Fischl, Suzaan Boettger, Stephen Weil, Richard Serra, and more cover a broad range of topics facing today’s artists, policy makers, art lawyers, galleries, museum professionals, and more.\n\nIn April 2007, under the direction of Timothy Beal of the Baker-Nord Center for the Humanities and William E. Deal of the Inamori Center for Ethics at Case Western Reserve University hosted an interdisciplinary group of scholars in the fields of philosophical ethics, religious studies, theology, visual culture studies, neuroscience, and cognitive science to develop the first research collaboration on visual ethics.\n\nAlthough Visual ethics is an emerging scholarly field, certain books in the fields of ethics, visual culture, and cognitive science have proven particularly influential thus far.\n\n\n"}
{"id": "1475541", "url": "https://en.wikipedia.org/wiki?curid=1475541", "title": "Vortigaunt", "text": "Vortigaunt\n\nVortigaunts are a fictional extra-dimensional species in the \"Half-Life\" series of video games by Valve Corporation. In \"Half-Life\" and its three expansions, Vortigaunts are frequently encountered by the player as hostile non-player characters in \"Half-Life\" and later as allies in \"Half-Life 2\". The Vortigaunts are depicted in \"Half-Life\" as being an enslaved race in an alternative dimension called Xen, subservient to a large creature called the Nihilanth, which itself is a slave to undisclosed masters. In \"Half-Life 2\", the Vortigaunts have broken free of their slavery, and actively assist the player and other humans in resisting the Combine occupation of Earth.\n\nVortigaunts are shown as a very communal and cultural race, believing in a force that binds the fabric of the universe and each Vortigaunt together, as well as producing a tradition of poetry and music. Vortigaunts also display the ability to summon and command electrical energy without the need for technology. This ability is used for various activities, such as a means of attack, powering electrical equipment, and healing.\n\nThe fictional creatures have received a range of critical responses from their various appearances. In addition to their role within the \"Half-Life\" series, Vortigaunts have been adapted for machinima productions and have been made into a plush toy by Valve.\n\nLouis Gossett, Jr. provides the voice for the Vortigaunts in \"Half-Life 2\", and was partially chosen due to his role as the alien in the film \"Enemy Mine\". In \"Episode Two\", the Vortigaunts' voicing was done by Tony Todd. Graphically, the Vortigaunts in both \"Half-Life\" and \"Half-Life 2\" were designed by concept artists Dhabih Eng and Chuck Jones.\n\nInitially, Vortigaunts were planned to begin \"Half-Life\" as enemy non-player characters, which the player has to win over as allies and lead in rebellion. This idea, however, proved impractical and was abandoned in favor of keeping the Vortigaunts as adversaries. The plan re-emerged for \"Half-Life 2\", in which the species are active allies of the player. The Vortigaunts in \"Half-Life 2\" were originally going to be fully integrated in City 17 alongside the humans under Combine rule. This did not reach fruition; due to the focus on developing the city combat, the resources to accomplish this were not available. As there were so few resulting Vortigaunts in the city, the developers instead decided to involve the Vortigaunts far more substantially in the sections of the game taking place in the countryside surrounding the city. The few Vortigaunts placed inside the city were used to hint at the story arc for the Vortigaunts later in the game. Midway through \"Half-Life 2\", the player witnesses a scene with a deceased Vortigaunt in a prison. Originally, the developers were intending to feature this Vortigaunt as an ally character who, if players rescue him, would fight enemy characters alongside the player. The developers liked this idea, but it was too late in the development to attempt this; instead, the idea was preserved for use in \"Episode Two\". The concept was later showcased in an \"Episode Two\" trailer shown at the Games Convention in 2006.\n\nAccording to series writer Marc Laidlaw, one of the most important goals with \"Episode Two\" was to expand on the Vortigaunts as characters, as opposed to just \"purveyors of bugbait or Xen koans\". As such, Valve added new behaviours, new animations, and new audio to the Vortigaunts. Combine devices called \"Vorti-Cells\" were to be encountered in \"Half-Life 2\". They were meant to siphon power from captive Vortigaunts in City 17. The player would then be able to free Vortigaunts from these devices to gain their assistance.\n\nVortigaunts are very intelligent and social creatures. In \"Half-Life\", they serve as basic ground units for the Xen forces, often supporting heavier troops fighting the player and the humans in the Black Mesa Research Facility. They are capable of developing intelligent strategies and tactics to take on their enemies. Initially, Vortigaunts are enslaved to the Nihilanth, a large creature on Xen, who, in turn, claims it is a slave to unknown superiors. On Xen, the Vortigaunts are used as factory workers and drones. In \"Half-Life 2\", Vortigaunts indicate that this slavery has lasted for generations. Despite this, Vortigaunts are shown to have developed an intellectual culture, valuing poetry, music and philosophy. In addition, Vortigaunts practise the husbandry of antlions, large insectoids that live in underground hives, as an ancestral tradition. The death of the Nihilanth at the hands of Gordon Freeman at the end of \"Half-Life\" frees the Vortigaunts from their slavery; consequently the Vortigaunts see Freeman as a messianic figure.\n\nVortigaunts possess their own method of vocal communication, \"flux shifting\", which they can be heard using in \"Half-Life 2\". This method of communication involves both speakers vocalising at the same time and cannot be understood by other species who lack the auditory anatomy to interpret the sounds. In addition to standard conversation, flux shifting can be used by Vortigaunts to communicate over long distances. Vortigaunts use scattered English words in \"Half-Life\" to taunt the player, although by \"Half-Life 2\" they have learned to speak at least the English language fluently, albeit using archaic words and grammatical structures. In addition, the Vortigaunts believe in a binding life-force referred to as the \"Vortessence\". The exact nature of the Vortessence is left unclear, although it is revealed that Vortigaunts believe the Vortessence is the fabric of the universe and connects everything.\n\nIn general, a Vortigaunt is a somewhat humanoid figure with two legs and two arms, but has an additional arm protruding from its thorax. Vortigaunts have mottled green skin and digitigrade legs, allowing them to move quickly. Typically, Vortigaunts have a slightly hunched posture. In addition, Vortigaunts have sharp teeth, clawed hands, strong senses, and their faces are dominated by a large red eye. In \"Half-Life\", this eye is surrounded by five smaller eyes, although, in \"Half-Life 2\", this has been reduced to three smaller eyes. A key aspect of the Vortigaunts is their ability to summon energy to their command without the need for any equipment. In-game, this ability is most commonly used as a means of attack, with Vortigaunts' hurling the energy towards foes in the form of green electrical bolts. In addition, Vortigaunts have been shown to be able to use the energy for other purposes, such as assisting in repair work, removing rubble and blockages, recharging the player's HEV suit or powering up electrical generators. Enslaved Vortigaunts are depicted with green collars and shackles, although the means by which these control Vortigaunts is not explored. Vortigaunts are not typically seen wearing clothing, although several Vortigaunts are observed wearing lab coats and chef's clothing in the course of \"Half-Life 2\" and its expansions. Vortigaunts are depicted as meat eaters; the player can observe Vortigaunts' eating human corpses in \"Half-Life\" and cooking headcrabs in \"Half-Life 2\", although \"Half-Life 2\" also shows Vortigaunts working in kitchens with chef hats, preparing soups and salads, suggesting that Vortigaunts are omnivores.\n\nVortigaunts are first introduced as one of the primary enemies in \"Half-Life\". They are frequently encountered by players throughout \"Half-Life\" and its three expansions, \"\", \"\" and \"\". They are portrayed as one of the sentient races of Xen, often working in groups or with more heavily armored Xen troops. In the later stages of \"Half-Life\", Vortigaunts are seen working in factory-like environments, constructing or maturing the more heavily armored Xen troops in cocoon-like capsules under the direction of other aliens in the Xen hierarchy.\n\nIn the cooperative multiplayer expansion \"Decay\", players have the opportunity to assume the role of two Vortigaunts if they score highly on all missions. In this bonus mission, entitled \"Xen Attacks\", two Vortigaunts, designated as Drone Subjects X-8973 and R-4913, are given a mission by the Nihilanth to retrieve a number of crystals stolen from Xen by human scientists from the Black Mesa Research Facility.\n\nThe role of the Vortigaunts is drastically changed in \"Half-Life 2\". Now acting as allies to the player, Vortigaunts have been freed as a result of protagonist Gordon Freeman killing the Nihilanth at the end of \"Half-Life\". Instead of acting with hostility towards humanity, the Vortigaunts are actively engaged in assisting the human resistance against the Combine, a multi-dimensional empire that has invaded and occupied Earth in between \"Half-Life\" and \"Half-Life 2\". Vortigaunts are frequently seen at resistance compounds and stations, often performing maintenance work or providing help with experiments, and sometimes assist the player by using their energy abilities to recharge the player's HEV suit. However, a number of Vortigaunts are still in captivity, as at various points in the game, Vortigaunts are shown used as slave labor by the Combine; for example, one Vortigaunt is seen cleaning the floor of a Combine train station with a broom, wearing a similar collar to those worn in \"Half-Life\".\n\nVortigaunts play a greater part in the story of the series in \"Half-Life 2\"s continuations, \"Episode One\" and \"Episode Two\". In \"Episode One\", Vortigaunts use unexplained powers to rescue Alyx Vance, the series' main female character, from the top of the Combine Citadel in City 17, saving her from the explosion of the Citadel's reactor at the end of \"Half-Life 2\". The Vortigaunts also free Gordon Freeman from the stasis imposed on him at the end of \"Half-Life 2\" by his enigmatic employer, the G-Man, much to the G-Man's irritation. \"Episode Two\" dedicates much of the early part of the game to the Vortigaunts, with a group of four Vortigaunts working to heal Alyx Vance after she is mortally wounded by a Combine , while tracking and killing the Combine Advisors that fled the Citadel. One Vortigaunt accompanies the player and acts as combat support on an expedition into an antlion hive. In the latter stages of \"Episode Two\", a number of Vortigaunts are observed in the White Forest missile silo, performing various tasks to facilitate the launch of a satellite to close a Combine super portal.\n\nThe use of sandbox applications like \"Garry's Mod\" have allowed for the Vortigaunt to be used in a variety of webcomics and machinima productions. For example, in the webcomic \"Concerned\", the inability for the players to harm allied non-player characters such as the Vortigaunt in \"Half-Life 2\" forms a recurring joke, where the protagonist Gordon Frohman constantly tries to shoot Vortigaunts due to their role as enemies in \"Half-Life\". This eventually backfires on him, as when Vortigaunts try to rescue Frohman from the destruction of the Citadel, his hostile nature leads them to simply abandon him to his fate. Vortigaunts are occasionally featured in machinima productions, often taking on the roles of minor characters, such as janitors under Combine jurisdiction. In addition, Vortigaunts are central to one \"Half-Life\" single-player modification, \"POV – Point of View\", in which the player assumes the role of a Vortigaunt slave attacking Black Mesa.\n\nThe character of the Vortigaunt has been popular enough to inspire the creation of a Vortigaunt plush toy. The toy was designed by Dhabih Eng and was sold and distributed via Valve's online store. The toy was released in December 2006 and due to the proximity to Christmas, the Vortigaunt was accompanied by a to-scale Santa Claus hat. Reviews of the plush toy were favorable, praising the toy as sturdy, as well as giving credit for its articulation and detail.\n\nThe change of the Vortigaunts from enemy characters to allied characters in \"Half-Life 2\" was described as \"intriguing\" by reviewers, with \"PC Zone\" stating that the Vortigaunts were one of the most interesting characters in the game, playing a role \"somewhere between Yoda and ET\".\n\nThe role of the Vortigaunts in \"Episode Two\" was met with more approval by critics, who described the Vortigaunt who accompanies the player in the early parts of the game as \"a great joy to fight alongside\", praising the improved artificial intelligence and noting that \"the fact that they're not trying to intricately imitate real-life people\" significantly aided the presentation of the Vortigaunts. In addition, IGN praised the visual and character overhaul given to the Vortigaunt in \"Episode Two\".\n\n"}
{"id": "35275499", "url": "https://en.wikipedia.org/wiki?curid=35275499", "title": "William Danby (coroner)", "text": "William Danby (coroner)\n\nWilliam Danby (\"fl.\" 1542–1593) was a sixteenth-century lawyer and Coroner of the Queen's Household towards the end of the reign of Queen Elizabeth I. He is particularly noted for having presided over the inquest into the controversial death at Deptford in 1593 of the poet/dramatist Christopher Marlowe.\n\nAlthough the date of Danby's birth is unknown, he is most probably the William Danby who entered Lincoln's Inn on 1 August 1542. If so, his exact contemporary there was the father of Marlowe's friend and patron Thomas Walsingham, another Thomas, who was born in 1526. Danby was therefore probably in his late sixties at the time of Marlowe's inquest.\n\nIn 1589 Danby apparently took over the role of Coroner of The Queen's Household from Richard Vale. The first time Danby's name appears in this capacity (in the Middlesex records held at the London Metropolitan Archives) was for an inquest held in October 1589—in Shepperton, Middlesex—when he presided together with a county coroner, John Chalkhill, at the inquest on one Robert Wrote. Unfortunately for Danby, this case was later declared \"insufficient\" because Chalkhill had not said in his report that Shepperton was within the verge, which was legally required to explain Danby's presence there.\n\nWe know that Danby's predecessor as the Royal coroner, Richard Vale, was also one of the coroners for Middlesex Unfortunately only a very few (and irrelevant) Kentish inquests survive from that time to provide direct evidence that Danby similarly combined his royal responsibilities with those of a county coroner, and if he had been, it should have been noted in his report of Marlowe's inquest, which it did not.\n\nOn the other hand, two other pieces of evidence suggest that he may have nevertheless also been a coroner for Kent. The first is that it would have been illegal for him to have presided on his own over the Marlowe inquest, as he did, \"unless\" he was also a coroner for Kent, as Deptford was both in Kent and, at the time, within the verge. The other is that Leslie Hotson said that he had found a William Danby in Woolwich (in Kent, four or five miles east of Deptford) at that time, and, although Hotson gave no reference for this claim, William Urry was prepared to acknowledge it as quite likely.\n\nHad there been no doubts about Danby's report of the inquest jury's verdict on Marlowe's death, the name of William Danby would have probably disappeared by now. Some biographers still accept the story told at the inquest as a true account, but the majority of the more recent ones find the verdict of a killing in self-defence difficult to accept, and think that it must have been a deliberate murder, even though there is no agreement as to who was behind it or just what their motive might have been for arranging it. The Marlovian theory even argues that the most logical reason for those people to have been there at that time was to fake Marlowe's death, allowing him to escape almost certain trial and execution for his seditious atheism.\n\nThe date of Danby's death is unknown, but there is no record of his having presided over any other inquest after this date, so the last we hear of him is when he obeys the command to send a copy of the Marlowe inquisition to the Court of Chancery on 15 June 1593.\n\n"}
{"id": "11666967", "url": "https://en.wikipedia.org/wiki?curid=11666967", "title": "Şevki Balmumcu", "text": "Şevki Balmumcu\n\nŞevki Balmumcu (1905 – 20 April 1982 Istanbul) was a Turkish architect who designed the Ankara Opera House.\n\nHe was born in 1905 in Istanbul.\n\nIn 1928, he graduated from the Academy of Fine Arts as an architect. Between 1930 and 1932, he worked as the control architect of many buildings including Bursa Cinema Theater in Bursa, which is among the first modern cinema, theater and concert halls of the republic history. [1]\n\nThe winning of a cinema and office building project competition in Elâzığ is one of the important achievements in his professional life. However, the most important turning point in his career was the fact that he won the competition for the Ankara exhibition building in 1933. The building, which was built in 1933-1934, was a successful practice in Soviet Structuralism or Neo-Plasticism styles. The building, which often hosts exhibitions reflecting the development moves of the republic, was used as a work symbolizing the ideals of Kemalist thought in photographs, postcards and republic posters, and most often in the pages of La Turquie Kemaliste. [2]\n\nThe exhibition building was converted into an opera house in 1948 by Paul Bonatz. It is said that the transformation of the structure from its original form into an appropriate form for the style of Second National Architecture with its porticoes and ornaments distressed Şevki Balmumcu and removed him from the architectural profession. [3] The architect did not go to Ankara after this incident. [4]\n\nAfter his success in 1933, Şevki Balmumcu won many architectural design competitions. From 1941 to 1958 he worked in Tekel General Directorate as a construction bureau chief. After 1958, he continued his professional life independently, but there was no remarkable work during this period. He died on April 20, 1982. [1] His funeral was buried in Edirnekapı Cemetery.\n\n"}
