{"id": "55962927", "url": "https://en.wikipedia.org/wiki?curid=55962927", "title": "AI aftermath scenarios", "text": "AI aftermath scenarios\n\nMany scholars believe that advances in artificial intelligence will someday lead to a post-scarcity economy where intelligent machines can outperform humans in nearly every domain. The questions of what such a world might look like, and whether specific scenarios constitute utopias or dystopias, are the subject of lively debate.\n\nMost scientists believe that AI research will at some point lead to the creation of machines that are as intelligent, or more intelligent, than human beings in every domain of interest. There is no physical law precluding particles from being organised in ways that perform even more advanced computations than the arrangements of particles in human brains; therefore superintelligence is physically possible. In addition to potential algorithmic improvements over human brains, a digital brain can be many orders of magnitude larger and faster than a human brain, which was constrained in size by evolution to be small enough to fit through a birth canal. While there is no consensus on \"when\" artificial intelligence will outperform humans, many scholars argue that whenever it does happen, the introduction of a second species of intelligent life onto the planet will have far-reaching implications. Scholars often disagree with one another both about what types of post-AI scenarios are \"most likely\", and about what types of post-AI scenarios would be \"most desirable\". Finally, some dissenters argue that AI will never become as intelligent as humans, for example because the human race will already likely have destroyed itself before research has time to advance sufficiently to create artificial general intelligence.\n\nAll of the following \"AI aftermath scenarios\" of the aftermath of arbitrarily-advanced AI development are crucially dependent on two intertwined theses. The first thesis is that, at some point in the future, some kind of economic growth will continue until a \"post-scarcity\" economy is reached that could, unless extremely hyperconcentrated, effortlessly provide an extremely comfortable standard of living for a population equaling or, within reason, exceeding the current human population, without even requiring the bulk of the population to participate in the workforce. This economic growth could come from the continuation of existing growth trends and the refinement of existing technologies, or through future breakthroughs in emerging technologies such as nanotechnology and automation through robotics and futuristic advanced artificial intelligence. The second thesis is that advances in artificial intelligence will render humans unnecessary for the functioning of the economy: human labor declines in relative economic value if robots are easier to cheaply mass-produce then humans, more customizable than humans, and if they become more intelligent and capable than humans.\n\nThe Universe may be spatially infinite; however, the accessible Universe is bounded by the cosmological event horizon of around 16 billion light years. Some physicists believe it plausible that nearest alien civilization may well be located more than 16 billion light years away; in this best-case expansion scenario, the human race could eventually, by colonizing a significant fraction of the accessible Universe, increase the accessible biosphere by perhaps 32 orders of magnitude. The twentieth century saw a partial \"demographic transition\" to lower birthrates associated with wealthier societies; however, in the very long run, intergenerational fertility correlations (whether due to natural selection or due to cultural transmission of large-family norms from parents to children) are predicted to result in an increase in fertility over time, in the absence of either mandated birth control or periodic Malthusian catastrophes.\n\nLibertarian scenarios postulate that intelligent machines, uploaded humans, cyborgs, and unenhanced humans will coexist peacefully in a framework focused on respecting \nproperty rights. Because industrial productivity is no longer gated by scarce human labor, the value of land skyrockets compared to the price of goods; even remaining \"Luddite\" humans who owned or inherited land should be able to sell or lease a small piece of it to the more-productive robots in exchange for a perpetual annuity sufficient to easily indefinitely meet all of their basic financial needs. Such people can live as long as they choose to, and are free to engage in almost any activity they can conceive of, for pleasure or for self-actualization, without financial concern. Advanced technologies enable entirely new modes of thought and experience, thus adding to the palette of possible feelings. People in the future may even experience never-ending \"gradients of bliss\".\n\nSuch decentralized scenarios may be unstable in the long run, as the greediest elements of the superintelligent classes would have both the means and the motive to usurp the property of the unenhanced classes. Even if the mechanisms for ensuring legal property rights are both unbreakable and loophole-free, there may still be an ever-present danger of humans and cyborgs being \"tricked\" by the cleverest of the superintelligent machines into unwittingly signing over their own property. Suffering may be widespread, as sentient beings without property may die, and no mechanism prevents a being from reproducing up until the limits of his own inheritable resources, resulting in a multitude of that being's descendants scrabbling out an existence of minimal sustenance.\n\nIn this scenario, postulate that a superintelligent artificial intelligence takes control of society, but acts in a beneficial way. Its programmers, despite being on a deadline, solved quasi-philosophical problems that had seemed to some intractable, and created an AI with the following goal: to use its superintelligence to figure out what human utopia looks like by analyzing human behavior, human brains, and human genes; and then, to implement that utopia. The AI arrives at a subtle and complex definition of human flourishing. Valuing diversity, and recognizing that different people have different preferences, the AI divides Earth into different sectors. Harming others, making weapons, evading surveillance, or trying to create a rival superintelligence are globally banned; apart from that, each sector is free to make its own laws; for example, a religious person might choose to live in the \"pious sector\" corresponding to his religion, where the appropriate religious rules are strictly enforced. In all sectors, disease, poverty, crime, hangovers, addiction, and all other involuntary suffering have been eliminated. Many sectors boast advanced architecture and spectacle that \"make typical sci-fi visions pale in comparison\". Life is an \"all-inclusive pleasure cruise\", as if it were \"Christmas 365 days a year\".\n\nStill, many people are dissatisfied. Humans have no freedom in shaping their collective destiny. Some want the freedom to have as many children as they want. Others resent surveillance by the AI, or chafe at bans on weaponry and on creating further superintelligence machines. Others may come to regret the choices they have made, or find their lives feel hollow and superficial.\n\nIn \"Gatekeeper\" AI scenarios, the AI can act to prevent rival superintelligences from being created, but otherwise errs on the side of allowing humans to create their own destiny. Ben Goertzel of OpenCog has advocated a \"Nanny AI\" scenario where the AI additionally takes some responsibility for preventing humans from destroying themselves, for example by slowing down technological progress to give time for society to advance in a more thoughtful and deliberate manner. In a third scenario, a superintelligent \"Protector\" AI gives humans the illusion of control, by hiding or erasing all knowledge of its existence, but works behind the scenes to guarantee positive outcomes. In all three scenarios, while humanity gains more control (or at least the illusion of control), humanity ends up progressing more slowly than it would if the AI were unrestricted in its willingness to rain down all the benefits of its advanced technology on the human race.\n\nThe AI Box scenario postulates that a superintelligent AI can be \"confined to a box\" and its actions can be restricted by human gatekeepers; the humans in charge would try to take advantage of some of the AI's scientific breakthroughs or reasoning abilities, without allowing the AI to take over the world. Successful gatekeeping may be difficult; the more intelligent the AI is, the more likely the AI can find a clever way to use \"social hacking\" and convince the gatekeepers to let it escape, or even to find an unforeseen physical method of escape.\n\nKurzweil argues that in the future \"There will be no distinction, post-Singularity, between human and machine or between physical and virtual reality\".\n\nIf a dominant superintelligent machine were to conclude that human survival is an unnecessary risk or a waste of resources, the result would be human extinction. This could occur if a machine, programmed without respect for human values, unexpectedly gains superintelligence through recursive self-improvement, or manages to escape from its containment in an AI Box scenario. This could also occur if the first superintelligent AI was programmed with an incomplete or inaccurate understanding of human values, either because the task of instilling the AI with human values was too difficult or impossible; due to a buggy initial implementation of the AI; or due to bugs accidentally being introduced, either by its human programmers or by the self-improving AI itself, in the course of refining its code base. Bostrom and others argue that human extinction is probably the \"default path\" that society is currently taking, in the absence of substantial preparatory attention to AI safety. The resultant AI might not be sentient, and might place no value on sentient life; the resulting hollow world, devoid of life, might be like \"a Disneyland without children\".\n\nJerry Kaplan, author of \"Humans Need Not Apply\", posits a scenario where humans are farmed or kept on a reserve, just as humans preserve endangered species like chimpanzees. Apple co-founder and AI skeptic Steve Wozniak stated in 2015 that robots taking over would actually \"be good for the human race\", on the grounds that he believes humans would become the robots' pampered pets.\n\nSome scholars doubt that \"game-changing\" superintelligent machines will ever come to pass. Gordon Bell of Microsoft Research has stated \"the population will destroy itself before the technological singularity\". Gordon Moore, discoverer of the eponymous Moore's law, stated \"I am a skeptic. I don't believe this kind of thing is likely to happen, at least for a long time. And I don't know why I feel that way.\" Evolutionary psychologist Steven Pinker stated, \"The fact that you can visualize a future in your imagination is not evidence that it is likely or even possible.\"\n\nBill Joy of Sun Microsystems, in his April 2000 essay \"Why the Future Doesn't Need Us\", has advocated for global \"voluntary relinquishment\" of artificial general intelligence and other risky technologies. Most experts believe relinquishment is extremely unlikely. AI skeptic Oren Etzioni has stated that researchers and scientists have no choice but to push forward with AI developments: \"China says they want to be an AI leader, Putin has said the same thing. So the global race is on.\"\n\n"}
{"id": "156998", "url": "https://en.wikipedia.org/wiki?curid=156998", "title": "Action potential", "text": "Action potential\n\nIn physiology, an action potential occurs when the membrane potential of a specific axon location rapidly rises and falls: this depolarisation then causes adjacent locations to similarly depolarise. Action potentials occur in several types of animal cells, called excitable cells, which include neurons, muscle cells, endocrine cells, and in some plant cells.\n\nIn neurons, action potentials play a central role in cell-to-cell communication by providing for—or, with regard to saltatory conduction, assisting—the propagation of signals along the neuron's axon towards synaptic boutons situated at the ends of an axon; these signals can then connect with other neurons at synapses, or to motor cells or glands. In other types of cells, their main function is to activate intracellular processes. In muscle cells, for example, an action potential is the first step in the chain of events leading to contraction. In beta cells of the pancreas, they provoke release of insulin. Action potentials in neurons are also known as \"nerve impulses\" or \"spikes\", and the temporal sequence of action potentials generated by a neuron is called its \"spike train\". A neuron that emits an action potential, or nerve impulse, is often said to \"fire\".\n\nAction potentials are generated by special types of voltage-gated ion channels embedded in a cell's plasma membrane. These channels are shut when the membrane potential is near the (negative) resting potential of the cell, but they rapidly begin to open if the membrane increases to a precisely defined threshold voltage, depolarising the transmembrane potential. When the channels open, they allow an inward flow of sodium ions, which changes the electrochemical gradient, which in turn produces a further rise in the membrane potential. This then causes more channels to open, producing a greater electric current across the cell membrane, and so on. The process proceeds explosively until all of the available ion channels are open, resulting in a large upswing in the membrane potential. The rapid influx of sodium ions causes the polarity of the plasma membrane to reverse, and the ion channels then rapidly inactivate. As the sodium channels close, sodium ions can no longer enter the neuron, and then they are actively transported back out of the plasma membrane. Potassium channels are then activated, and there is an outward current of potassium ions, returning the electrochemical gradient to the resting state. After an action potential has occurred, there is a transient negative shift, called the afterhyperpolarization.\n\nIn animal cells, there are two primary types of action potentials. One type is generated by voltage-gated sodium channels, the other by voltage-gated calcium channels. Sodium-based action potentials usually last for under one millisecond, but calcium-based action potentials may last for 100 milliseconds or longer. In some types of neurons, slow calcium spikes provide the driving force for a long burst of rapidly emitted sodium spikes. In cardiac muscle cells, on the other hand, an initial fast sodium spike provides a \"primer\" to provoke the rapid onset of a calcium spike, which then produces muscle contraction.\n\nIn the Hodgkin–Huxley membrane capacitance model, the speed of transmission of an action potential was undefined and it was assumed that adjacent areas became depolarised due to released ion interference with neighbouring channels. Measurements of ion diffusion and radii have since shown this not to be possible. Moreover, contradictory measurements of entropy changes and timing disputed the capacitance model as acting alone.\n\nNearly all cell membranes in animals, plants and fungi maintain a voltage difference between the exterior and interior of the cell, called the membrane potential. A typical voltage across an animal cell membrane is −70 mV. This means that the interior of the cell has a negative voltage of approximately one-fifteenth of a volt relative to the exterior. In most types of cells, the membrane potential usually stays fairly constant. Some types of cells, however, are electrically active in the sense that their voltages fluctuate over time. In some types of electrically active cells, including neurons and muscle cells, the voltage fluctuations frequently take the form of a rapid upward spike followed by a rapid fall. These up-and-down cycles are known as \"action potentials\". In some types of neurons, the entire up-and-down cycle takes place in a few thousandths of a second. In muscle cells, a typical action potential lasts about a fifth of a second. In some other types of cells, and also in plants, an action potential may last three seconds or more.\n\nThe electrical properties of a cell are determined by the structure of the membrane that surrounds it. A cell membrane consists of a lipid bilayer of molecules in which larger protein molecules are embedded. The lipid bilayer is highly resistant to movement of electrically charged ions, so it functions as an insulator. The large membrane-embedded proteins, in contrast, provide channels through which ions can pass across the membrane. Action potentials are driven by channel proteins whose configuration switches between closed and open states as a function of the voltage difference between the interior and exterior of the cell. These voltage-sensitive proteins are known as voltage-gated ion channels.\n\nAll cells in animal body tissues are electrically polarized – in other words, they maintain a voltage difference across the cell's plasma membrane, known as the membrane potential. This electrical polarization results from a complex interplay between protein structures embedded in the membrane called ion pumps and ion channels. In neurons, the types of ion channels in the membrane usually vary across different parts of the cell, giving the dendrites, axon, and cell body different electrical properties. As a result, some parts of the membrane of a neuron may be excitable (capable of generating action potentials), whereas others are not. Recent studies have shown that the most excitable part of a neuron is the part after the axon hillock (the point where the axon leaves the cell body), which is called the initial segment, but the axon and cell body are also excitable in most cases.\n\nEach excitable patch of membrane has two important levels of membrane potential: the resting potential, which is the value the membrane potential maintains as long as nothing perturbs the cell, and a higher value called the threshold potential. At the axon hillock of a typical neuron, the resting potential is around –70 millivolts (mV) and the threshold potential is around –55 mV. Synaptic inputs to a neuron cause the membrane to depolarize or hyperpolarize; that is, they cause the membrane potential to rise or fall. Action potentials are triggered when enough depolarization accumulates to bring the membrane potential up to threshold. When an action potential is triggered, the membrane potential abruptly shoots upward and then equally abruptly shoots back downward, often ending below the resting level, where it remains for some period of time. The shape of the action potential is stereotyped; this means that the rise and fall usually have approximately the same amplitude and time course for all action potentials in a given cell. (Exceptions are discussed later in the article). In most neurons, the entire process takes place in about a thousandth of a second. Many types of neurons emit action potentials constantly at rates of up to 10–100 per second. However, some types are much quieter, and may go for minutes or longer without emitting any action potentials.\n\nAction potentials result from the presence in a cell's membrane of special types of voltage-gated ion channels. A voltage-gated ion channel is a cluster of proteins embedded in the membrane that has three key properties:\n\nThus, a voltage-gated ion channel tends to be open for some values of the membrane potential, and closed for others. In most cases, however, the relationship between membrane potential and channel state is probabilistic and involves a time delay. Ion channels switch between conformations at unpredictable times: The membrane potential determines the rate of transitions and the probability per unit time of each type of transition.\nVoltage-gated ion channels are capable of producing action potentials because they can give rise to positive feedback loops: The membrane potential controls the state of the ion channels, but the state of the ion channels controls the membrane potential. Thus, in some situations, a rise in the membrane potential can cause ion channels to open, thereby causing a further rise in the membrane potential. An action potential occurs when this positive feedback cycle proceeds explosively. The time and amplitude trajectory of the action potential are determined by the biophysical properties of the voltage-gated ion channels that produce it. Several types of channels capable of producing the positive feedback necessary to generate an action potential do exist. Voltage-gated sodium channels are responsible for the fast action potentials involved in nerve conduction. Slower action potentials in muscle cells and some types of neurons are generated by voltage-gated calcium channels. Each of these types comes in multiple variants, with different voltage sensitivity and different temporal dynamics.\n\nThe most intensively studied type of voltage-dependent ion channels comprises the sodium channels involved in fast nerve conduction. These are sometimes known as Hodgkin-Huxley sodium channels because they were first characterized by Alan Hodgkin and Andrew Huxley in their Nobel Prize-winning studies of the biophysics of the action potential, but can more conveniently be referred to as \"Na\" channels. (The \"V\" stands for \"voltage\".) An \"Na\" channel has three possible states, known as \"deactivated\", \"activated\", and \"inactivated\". The channel is permeable only to sodium ions when it is in the \"activated\" state. When the membrane potential is low, the channel spends most of its time in the \"deactivated\" (closed) state. If the membrane potential is raised above a certain level, the channel shows increased probability of transitioning to the \"activated\" (open) state. The higher the membrane potential the greater the probability of activation. Once a channel has activated, it will eventually transition to the \"inactivated\" (closed) state. It tends then to stay inactivated for some time, but, if the membrane potential becomes low again, the channel will eventually transition back to the \"deactivated\" state. During an action potential, most channels of this type go through a cycle \"deactivated\"→\"activated\"→\"inactivated\"→\"deactivated\". This is only the population average behavior, however — an individual channel can in principle make any transition at any time. However, the likelihood of a channel's transitioning from the \"inactivated\" state directly to the \"activated\" state is very low: A channel in the \"inactivated\" state is refractory until it has transitioned back to the \"deactivated\" state.\n\nThe outcome of all this is that the kinetics of the \"Na\" channels are governed by a transition matrix whose rates are voltage-dependent in a complicated way. Since these channels themselves play a major role in determining the voltage, the global dynamics of the system can be quite difficult to work out. Hodgkin and Huxley approached the problem by developing a set of differential equations for the parameters that govern the ion channel states, known as the Hodgkin-Huxley equations. These equations have been extensively modified by later research, but form the starting point for most theoretical studies of action potential biophysics.\n\nAs the membrane potential is increased, sodium ion channels open, allowing the entry of sodium ions into the cell. This is followed by the opening of potassium ion channels that permit the exit of potassium ions from the cell. The inward flow of sodium ions increases the concentration of positively charged cations in the cell and causes depolarization, where the potential of the cell is higher than the cell's resting potential. The sodium channels close at the peak of the action potential, while potassium continues to leave the cell. The efflux of potassium ions decreases the membrane potential or hyperpolarizes the cell. For small voltage increases from rest, the potassium current exceeds the sodium current and the voltage returns to its normal resting value, typically −70 mV. However, if the voltage increases past a critical threshold, typically 15 mV higher than the resting value, the sodium current dominates. This results in a runaway condition whereby the positive feedback from the sodium current activates even more sodium channels. Thus, the cell \"fires\", producing an action potential. The frequency at which a neuron elicits action potentials is often referred to as a firing rate or neural firing rate.\n\nCurrents produced by the opening of voltage-gated channels in the course of an action potential are typically significantly larger than the initial stimulating current. Thus, the amplitude, duration, and shape of the action potential are determined largely by the properties of the excitable membrane and not the amplitude or duration of the stimulus. This all-or-nothing property of the action potential sets it apart from graded potentials such as receptor potentials, electrotonic potentials, and synaptic potentials, which scale with the magnitude of the stimulus. A variety of action potential types exist in many cell types and cell compartments as determined by the types of voltage-gated channels, leak channels, channel distributions, ionic concentrations, membrane capacitance, temperature, and other factors.\n\nThe principal ions involved in an action potential are sodium and potassium cations; sodium ions enter the cell, and potassium ions leave, restoring equilibrium. Relatively few ions need to cross the membrane for the membrane voltage to change drastically. The ions exchanged during an action potential, therefore, make a negligible change in the interior and exterior ionic concentrations. The few ions that do cross are pumped out again by the continuous action of the sodium–potassium pump, which, with other ion transporters, maintains the normal ratio of ion concentrations across the membrane. Calcium cations and chloride anions are involved in a few types of action potentials, such as the cardiac action potential and the action potential in the single-cell alga \"Acetabularia\", respectively.\n\nAlthough action potentials are generated locally on patches of excitable membrane, the resulting currents can trigger action potentials on neighboring stretches of membrane, precipitating a domino-like propagation. In contrast to passive spread of electric potentials (electrotonic potential), action potentials are generated anew along excitable stretches of membrane and propagate without decay. Myelinated sections of axons are not excitable and do not produce action potentials and the signal is propagated passively as electrotonic potential. Regularly spaced unmyelinated patches, called the nodes of Ranvier, generate action potentials to boost the signal. Known as saltatory conduction, this type of signal propagation provides a favorable tradeoff of signal velocity and axon diameter. Depolarization of axon terminals, in general, triggers the release of neurotransmitter into the synaptic cleft. In addition, backpropagating action potentials have been recorded in the dendrites of pyramidal neurons, which are ubiquitous in the neocortex. These are thought to have a role in spike-timing-dependent plasticity.\n\nA neuron's ability to generate and propagate an action potential changes during development. How much the membrane potential of a neuron changes as the result of a current impulse is a function of the membrane input resistance. As a cell grows, more channels are added to the membrane, causing a decrease in input resistance. A mature neuron also undergoes shorter changes in membrane potential in response to synaptic currents. Neurons from a ferret lateral geniculate nucleus have a longer time constant and larger voltage deflection at P0 than they do at P30. One consequence of the decreasing action potential duration is that the fidelity of the signal can be preserved in response to high frequency stimulation. Immature neurons are more prone to synaptic depression than potentiation after high frequency stimulation.\n\nIn the early development of many organisms, the action potential is actually initially carried by calcium current rather than sodium current. The opening and closing kinetics of calcium channels during development are slower than those of the voltage-gated sodium channels that will carry the action potential in the mature neurons. The longer opening times for the calcium channels can lead to action potentials that are considerably slower than those of mature neurons. Xenopus neurons initially have action potentials that take 60–90 ms. During development, this time decreases to 1 ms. There are two reasons for this drastic decrease. First, the inward current becomes primarily carried by sodium channels. Second, the delayed rectifier, a potassium channel current, increases to 3.5 times its initial strength.\n\nIn order for the transition from a calcium-dependent action potential to a sodium-dependent action potential to proceed new channels must be added to the membrane. If Xenopus neurons are grown in an environment with RNA synthesis or protein synthesis inhibitors that transition is prevented. Even the electrical activity of the cell itself may play a role in channel expression. If action potentials in Xenopus myocytes are blocked, the typical increase in sodium and potassium current density is prevented or delayed.\n\nThis maturation of electrical properties is seen across species. Xenopus sodium and potassium currents increase drastically after a neuron goes through its final phase of mitosis. The sodium current density of rat cortical neurons increases by 600% within the first two postnatal weeks.\n\nSeveral types of cells support an action potential, such as plant cells, muscle cells, and the specialized cells of the heart (in which occurs the cardiac action potential). However, the main excitable cell is the neuron, which also has the simplest mechanism for the action potential.\n\nNeurons are electrically excitable cells composed, in general, of one or more dendrites, a single soma, a single axon and one or more axon terminals. Dendrites are cellular projections whose primary function is to receive synaptic signals. Their protrusions, known as dendritic spines, are designed to capture the neurotransmitters released by the presynaptic neuron. They have a high concentration of ligand-gated ion channels. These spines have a thin neck connecting a bulbous protrusion to the dendrite. This ensures that changes occurring inside the spine are less likely to affect the neighboring spines. The dendritic spine can, with rare exception (see LTP), act as an independent unit. The dendrites extend from the soma, which houses the nucleus, and many of the \"normal\" eukaryotic organelles. Unlike the spines, the surface of the soma is populated by voltage activated ion channels. These channels help transmit the signals generated by the dendrites. Emerging out from the soma is the axon hillock. This region is characterized by having a very high concentration of voltage-activated sodium channels. In general, it is considered to be the spike initiation zone for action potentials, i.e. the trigger zone. Multiple signals generated at the spines, and transmitted by the soma all converge here. Immediately after the axon hillock is the axon. This is a thin tubular protrusion traveling away from the soma. The axon is insulated by a myelin sheath. Myelin is composed of either Schwann cells (in the peripheral nervous system) or oligodendrocytes (in the central nervous system), both of which are types of glial cells. Although glial cells are not involved with the transmission of electrical signals, they communicate and provide important biochemical support to neurons. To be specific, myelin wraps multiple times around the axonal segment, forming a thick fatty layer that prevents ions from entering or escaping the axon. This insulation prevents significant signal decay as well as ensuring faster signal speed. This insulation, however, has the restriction that no channels can be present on the surface of the axon. There are, therefore, regularly spaced patches of membrane, which have no insulation. These nodes of Ranvier can be considered to be \"mini axon hillocks\", as their purpose is to boost the signal in order to prevent significant signal decay. At the furthest end, the axon loses its insulation and begins to branch into several axon terminals. These presynaptic terminals, or synaptic boutons, are a specialized area within the axon of the presynaptic cell that contains neurotransmitters enclosed in small membrane-bound spheres called synaptic vesicles.\n\nBefore considering the propagation of action potentials along axons and their termination at the synaptic knobs, it is helpful to consider the methods by which action potentials can be initiated at the axon hillock. The basic requirement is that the membrane voltage at the hillock be raised above the threshold for firing. There are several ways in which this depolarization can occur.\n\nAction potentials are most commonly initiated by excitatory postsynaptic potentials from a presynaptic neuron. Typically, neurotransmitter molecules are released by the presynaptic neuron. These neurotransmitters then bind to receptors on the postsynaptic cell. This binding opens various types of ion channels. This opening has the further effect of changing the local permeability of the cell membrane and, thus, the membrane potential. If the binding increases the voltage (depolarizes the membrane), the synapse is excitatory. If, however, the binding decreases the voltage (hyperpolarizes the membrane), it is inhibitory. Whether the voltage is increased or decreased, the change propagates passively to nearby regions of the membrane (as described by the cable equation and its refinements). Typically, the voltage stimulus decays exponentially with the distance from the synapse and with time from the binding of the neurotransmitter. Some fraction of an excitatory voltage may reach the axon hillock and may (in rare cases) depolarize the membrane enough to provoke a new action potential. More typically, the excitatory potentials from several synapses must work together at nearly the same time to provoke a new action potential. Their joint efforts can be thwarted, however, by the counteracting inhibitory postsynaptic potentials.\n\nNeurotransmission can also occur through electrical synapses. Due to the direct connection between excitable cells in the form of gap junctions, an action potential can be transmitted directly from one cell to the next in either direction. The free flow of ions between cells enables rapid non-chemical-mediated transmission. Rectifying channels ensure that action potentials move only in one direction through an electrical synapse. Electrical synapses are found in all nervous systems, including the human brain, although they are a distinct minority.\n\nThe amplitude of an action potential is independent of the amount of current that produced it. In other words, larger currents do not create larger action potentials. Therefore, action potentials are said to be all-or-none signals, since either they occur fully or they do not occur at all. This is in contrast to receptor potentials, whose amplitudes are dependent on the intensity of a stimulus. In both cases, the frequency of action potentials is correlated with the intensity of a stimulus.\n\nIn sensory neurons, an external signal such as pressure, temperature, light, or sound is coupled with the opening and closing of ion channels, which in turn alter the ionic permeabilities of the membrane and its voltage. These voltage changes can again be excitatory (depolarizing) or inhibitory (hyperpolarizing) and, in some sensory neurons, their combined effects can depolarize the axon hillock enough to provoke action potentials. Some examples in humans include the olfactory receptor neuron and Meissner's corpuscle, which are critical for the sense of smell and touch, respectively. However, not all sensory neurons convert their external signals into action potentials; some do not even have an axon! Instead, they may convert the signal into the release of a neurotransmitter, or into continuous graded potentials, either of which may stimulate subsequent neuron(s) into firing an action potential. For illustration, in the human ear, hair cells convert the incoming sound into the opening and closing of mechanically gated ion channels, which may cause neurotransmitter molecules to be released. In similar manner, in the human retina, the initial photoreceptor cells and the next layer of cells (comprising bipolar cells and horizontal cells) do not produce action potentials; only some amacrine cells and the third layer, the ganglion cells, produce action potentials, which then travel up the optic nerve.\n\nIn sensory neurons, action potentials result from an external stimulus. However, some excitable cells require no such stimulus to fire: They spontaneously depolarize their axon hillock and fire action potentials at a regular rate, like an internal clock. The voltage traces of such cells are known as pacemaker potentials. The cardiac pacemaker cells of the sinoatrial node in the heart provide a good example. Although such pacemaker potentials have a natural rhythm, it can be adjusted by external stimuli; for instance, heart rate can be altered by pharmaceuticals as well as signals from the sympathetic and parasympathetic nerves. The external stimuli do not cause the cell's repetitive firing, but merely alter its timing. In some cases, the regulation of frequency can be more complex, leading to patterns of action potentials, such as bursting.\n\nThe course of the action potential can be divided into five parts: the rising phase, the peak phase, the falling phase, the undershoot phase, and the refractory period. During the rising phase the membrane potential depolarizes (becomes more positive). The point at which depolarization stops is called the peak phase. At this stage, the membrane potential reaches a maximum. Subsequent to this, there is a falling phase. During this stage the membrane potential becomes more negative, returning towards resting potential. The undershoot, or afterhyperpolarization, phase is the period during which the membrane potential temporarily becomes more negatively charged than when at rest (hyperpolarized). Finally, the time during which a subsequent action potential is impossible or difficult to fire is called the refractory period, which may overlap with the other phases.\n\nThe course of the action potential is determined by two coupled effects. First, voltage-sensitive ion channels open and close in response to changes in the membrane voltage \"V\". This changes the membrane's permeability to those ions. Second, according to the Goldman equation, this change in permeability changes the equilibrium potential \"E\", and, thus, the membrane voltage \"V\". Thus, the membrane potential affects the permeability, which then further affects the membrane potential. This sets up the possibility for positive feedback, which is a key part of the rising phase of the action potential. A complicating factor is that a single ion channel may have multiple internal \"gates\" that respond to changes in \"V\" in opposite ways, or at different rates. For example, although raising \"V\" \"opens\" most gates in the voltage-sensitive sodium channel, it also \"closes\" the channel's \"inactivation gate\", albeit more slowly. Hence, when \"V\" is raised suddenly, the sodium channels open initially, but then close due to the slower inactivation.\n\nThe voltages and currents of the action potential in all of its phases were modeled accurately by Alan Lloyd Hodgkin and Andrew Huxley in 1952, for which they were awarded the Nobel Prize in Physiology or Medicine in 1963. However, their model considers only two types of voltage-sensitive ion channels, and makes several assumptions about them, e.g., that their internal gates open and close independently of one another. In reality, there are many types of ion channels, and they do not always open and close independently.\n\nA typical action potential begins at the axon hillock with a sufficiently strong depolarization, e.g., a stimulus that increases \"V\". This depolarization is often caused by the injection of extra sodium cations into the cell; these cations can come from a wide variety of sources, such as chemical synapses, sensory neurons or pacemaker potentials.\n\nFor a neuron at rest, there is a high concentration of sodium and chloride ions in the extracellular fluid compared to the intracellular fluid while there is a high concentration of potassium ions in the intracellular fluid compared to the extracellular fluid. This concentration gradient along with potassium leak channels present on the membrane of the neuron causes an efflux of potassium ions making the resting potential close to \"E\" ≈ –75 mV. The depolarization opens both the sodium and potassium channels in the membrane, allowing the ions to flow into and out of the axon, respectively. If the depolarization is small (say, increasing \"V\" from −70 mV to −60 mV), the outward potassium current overwhelms the inward sodium current and the membrane repolarizes back to its normal resting potential around −70 mV. However, if the depolarization is large enough, the inward sodium current increases more than the outward potassium current and a runaway condition (positive feedback) results: the more inward current there is, the more \"V\" increases, which in turn further increases the inward current. A sufficiently strong depolarization (increase in \"V\") causes the voltage-sensitive sodium channels to open; the increasing permeability to sodium drives \"V\" closer to the sodium equilibrium voltage \"E\"≈ +55 mV. The increasing voltage in turn causes even more sodium channels to open, which pushes \"V\" still further towards \"E\". This positive feedback continues until the sodium channels are fully open and \"V\" is close to \"E\". The sharp rise in \"V\" and sodium permeability correspond to the \"rising phase\" of the action potential.\n\nThe critical threshold voltage for this runaway condition is usually around −45 mV, but it depends on the recent activity of the axon. A membrane that has just fired an action potential cannot fire another one immediately, since the ion channels have not returned to the deactivated state. The period during which no new action potential can be fired is called the \"absolute refractory period\". At longer times, after some but not all of the ion channels have recovered, the axon can be stimulated to produce another action potential, but with a higher threshold, requiring a much stronger depolarization, e.g., to −30 mV. The period during which action potentials are unusually difficult to evoke is called the \"relative refractory period\".\n\nThe positive feedback of the rising phase slows and comes to a halt as the sodium ion channels become maximally open. At the peak of the action potential, the sodium permeability is maximized and the membrane voltage \"V\" is nearly equal to the sodium equilibrium voltage \"E\". However, the same raised voltage that opened the sodium channels initially also slowly shuts them off, by closing their pores; the sodium channels become \"inactivated\". This lowers the membrane's permeability to sodium relative to potassium, driving the membrane voltage back towards the resting value. At the same time, the raised voltage opens voltage-sensitive potassium channels; the increase in the membrane's potassium permeability drives \"V\" towards \"E\". Combined, these changes in sodium and potassium permeability cause \"V\" to drop quickly, repolarizing the membrane and producing the \"falling phase\" of the action potential.\n\nThe raised voltage opened many more potassium channels than usual, and some of these do not close right away when the membrane returns to its normal resting voltage. In addition, further potassium channels open in response to the influx of calcium ions during the action potential. The potassium permeability of the membrane is transiently unusually high, driving the membrane voltage \"V\" even closer to the potassium equilibrium voltage \"E\". Hence, there is an undershoot or hyperpolarization, termed an afterhyperpolarization in technical language, that persists until the membrane potassium permeability returns to its usual value.\n\nEach action potential is followed by a refractory period, which can be divided into an \"absolute refractory period\", during which it is impossible to evoke another action potential, and then a \"relative refractory period\", during which a stronger-than-usual stimulus is required. These two refractory periods are caused by changes in the state of sodium and potassium channel molecules. When closing after an action potential, sodium channels enter an \"inactivated\" state, in which they cannot be made to open regardless of the membrane potential—this gives rise to the absolute refractory period. Even after a sufficient number of sodium channels have transitioned back to their resting state, it frequently happens that a fraction of potassium channels remains open, making it difficult for the membrane potential to depolarize, and thereby giving rise to the relative refractory period. Because the density and subtypes of potassium channels may differ greatly between different types of neurons, the duration of the relative refractory period is highly variable.\n\nThe absolute refractory period is largely responsible for the unidirectional propagation of action potentials along axons. At any given moment, the patch of axon behind the actively spiking part is refractory, but the patch in front, not having been activated recently, is capable of being stimulated by the depolarization from the action potential.\n\nThe action potential generated at the axon hillock propagates as a wave along the axon. The currents flowing inwards at a point on the axon during an action potential spread out along the axon, and depolarize the adjacent sections of its membrane. If sufficiently strong, this depolarization provokes a similar action potential at the neighboring membrane patches. This basic mechanism was demonstrated by Alan Lloyd Hodgkin in 1937. After crushing or cooling nerve segments and thus blocking the action potentials, he showed that an action potential arriving on one side of the block could provoke another action potential on the other, provided that the blocked segment was sufficiently short.\n\nOnce an action potential has occurred at a patch of membrane, the membrane patch needs time to recover before it can fire again. At the molecular level, this \"absolute refractory period\" corresponds to the time required for the voltage-activated sodium channels to recover from inactivation, i.e., to return to their closed state. There are many types of voltage-activated potassium channels in neurons. Some of them inactivate fast (A-type currents) and some of them inactivate slowly or not inactivate at all; this variability guarantees that there will be always an available source of current for repolarization, even if some of the potassium channels are inactivated because of preceding depolarization. On the other hand, all neuronal voltage-activated sodium channels inactivate within several milliseconds during strong depolarization, thus making following depolarization impossible until a substantial fraction of sodium channels have returned to their closed state. Although it limits the frequency of firing, the absolute refractory period ensures that the action potential moves in only one direction along an axon. The currents flowing in due to an action potential spread out in both directions along the axon. However, only the unfired part of the axon can respond with an action potential; the part that has just fired is unresponsive until the action potential is safely out of range and cannot restimulate that part. In the usual orthodromic conduction, the action potential propagates from the axon hillock towards the synaptic knobs (the axonal termini); propagation in the opposite direction—known as antidromic conduction—is very rare. However, if a laboratory axon is stimulated in its middle, both halves of the axon are \"fresh\", i.e., unfired; then two action potentials will be generated, one traveling towards the axon hillock and the other traveling towards the synaptic knobs.\n\nIn order to enable fast and efficient transduction of electrical signals in the nervous system, certain neuronal axons are covered with myelin sheaths. Myelin is a multilamellar membrane that enwraps the axon in segments separated by intervals known as nodes of Ranvier. It is produced by specialized cells: Schwann cells exclusively in the peripheral nervous system, and oligodendrocytes exclusively in the central nervous system. Myelin sheath reduces membrane capacitance and increases membrane resistance in the inter-node intervals, thus allowing a fast, saltatory movement of action potentials from node to node. Myelination is found mainly in vertebrates, but an analogous system has been discovered in a few invertebrates, such as some species of shrimp. Not all neurons in vertebrates are myelinated; for example, axons of the neurons comprising the autonomous nervous system are not, in general, myelinated.\n\nMyelin prevents ions from entering or leaving the axon along myelinated segments. As a general rule, myelination increases the conduction velocity of action potentials and makes them more energy-efficient. Whether saltatory or not, the mean conduction velocity of an action potential ranges from 1 meter per second (m/s) to over 100 m/s, and, in general, increases with axonal diameter.\n\nAction potentials cannot propagate through the membrane in myelinated segments of the axon. However, the current is carried by the cytoplasm, which is sufficient to depolarize the first or second subsequent node of Ranvier. Instead, the ionic current from an action potential at one node of Ranvier provokes another action potential at the next node; this apparent \"hopping\" of the action potential from node to node is known as saltatory conduction. Although the mechanism of saltatory conduction was suggested in 1925 by Ralph Lillie, the first experimental evidence for saltatory conduction came from Ichiji Tasaki and Taiji Takeuchi and from Andrew Huxley and Robert Stämpfli. By contrast, in unmyelinated axons, the action potential provokes another in the membrane immediately adjacent, and moves continuously down the axon like a wave.\n\nMyelin has two important advantages: fast conduction speed and energy efficiency. For axons larger than a minimum diameter (roughly 1 micrometre), myelination increases the conduction velocity of an action potential, typically tenfold. Conversely, for a given conduction velocity, myelinated fibers are smaller than their unmyelinated counterparts. For example, action potentials move at roughly the same speed (25 m/s) in a myelinated frog axon and an unmyelinated squid giant axon, but the frog axon has a roughly 30-fold smaller diameter and 1000-fold smaller cross-sectional area. Also, since the ionic currents are confined to the nodes of Ranvier, far fewer ions \"leak\" across the membrane, saving metabolic energy. This saving is a significant selective advantage, since the human nervous system uses approximately 20% of the body's metabolic energy.\n\nThe length of axons' myelinated segments is important to the success of saltatory conduction. They should be as long as possible to maximize the speed of conduction, but not so long that the arriving signal is too weak to provoke an action potential at the next node of Ranvier. In nature, myelinated segments are generally long enough for the passively propagated signal to travel for at least two nodes while retaining enough amplitude to fire an action potential at the second or third node. Thus, the safety factor of saltatory conduction is high, allowing transmission to bypass nodes in case of injury. However, action potentials may end prematurely in certain places where the safety factor is low, even in unmyelinated neurons; a common example is the branch point of an axon, where it divides into two axons.\n\nSome diseases degrade myelin and impair saltatory conduction, reducing the conduction velocity of action potentials. The most well-known of these is multiple sclerosis, in which the breakdown of myelin impairs coordinated movement.\n\nThe flow of currents within an axon can be described quantitatively by cable theory and its elaborations, such as the compartmental model. Cable theory was developed in 1855 by Lord Kelvin to model the transatlantic telegraph cable and was shown to be relevant to neurons by Hodgkin and Rushton in 1946. In simple cable theory, the neuron is treated as an electrically passive, perfectly cylindrical transmission cable, which can be described by a partial differential equation\n\nwhere \"V\"(\"x\", \"t\") is the voltage across the membrane at a time \"t\" and a position \"x\" along the length of the neuron, and where λ and τ are the characteristic length and time scales on which those voltages decay in response to a stimulus. Referring to the circuit diagram on the right, these scales can be determined from the resistances and capacitances per unit length.\n\nThese time and length-scales can be used to understand the dependence of the conduction velocity on the diameter of the neuron in unmyelinated fibers. For example, the time-scale τ increases with both the membrane resistance \"r\" and capacitance \"c\". As the capacitance increases, more charge must be transferred to produce a given transmembrane voltage (by the equation \"Q\" = \"CV\"); as the resistance increases, less charge is transferred per unit time, making the equilibration slower. In a similar manner, if the internal resistance per unit length \"r\" is lower in one axon than in another (e.g., because the radius of the former is larger), the spatial decay length λ becomes longer and the conduction velocity of an action potential should increase. If the transmembrane resistance \"r\" is increased, that lowers the average \"leakage\" current across the membrane, likewise causing \"λ\" to become longer, increasing the conduction velocity.\n\nIn general, action potentials that reach the synaptic knobs cause a neurotransmitter to be released into the synaptic cleft. Neurotransmitters are small molecules that may open ion channels in the postsynaptic cell; most axons have the same neurotransmitter at all of their termini. The arrival of the action potential opens voltage-sensitive calcium channels in the presynaptic membrane; the influx of calcium causes vesicles filled with neurotransmitter to migrate to the cell's surface and release their contents into the synaptic cleft. This complex process is inhibited by the neurotoxins tetanospasmin and botulinum toxin, which are responsible for tetanus and botulism, respectively.\n\nSome synapses dispense with the \"middleman\" of the neurotransmitter, and connect the presynaptic and postsynaptic cells together. When an action potential reaches such a synapse, the ionic currents flowing into the presynaptic cell can cross the barrier of the two cell membranes and enter the postsynaptic cell through pores known as connexons. Thus, the ionic currents of the presynaptic action potential can directly stimulate the postsynaptic cell. Electrical synapses allow for faster transmission because they do not require the slow diffusion of neurotransmitters across the synaptic cleft. Hence, electrical synapses are used whenever fast response and coordination of timing are crucial, as in escape reflexes, the retina of vertebrates, and the heart.\n\nA special case of a chemical synapse is the neuromuscular junction, in which the axon of a motor neuron terminates on a muscle fiber. In such cases, the released neurotransmitter is acetylcholine, which binds to the acetylcholine receptor, an integral membrane protein in the membrane (the \"sarcolemma\") of the muscle fiber. However, the acetylcholine does not remain bound; rather, it dissociates and is hydrolyzed by the enzyme, acetylcholinesterase, located in the synapse. This enzyme quickly reduces the stimulus to the muscle, which allows the degree and timing of muscular contraction to be regulated delicately. Some poisons inactivate acetylcholinesterase to prevent this control, such as the nerve agents sarin and tabun, and the insecticides diazinon and malathion.\n\nThe cardiac action potential differs from the neuronal action potential by having an extended plateau, in which the membrane is held at a high voltage for a few hundred milliseconds prior to being repolarized by the potassium current as usual. This plateau is due to the action of slower calcium channels opening and holding the membrane voltage near their equilibrium potential even after the sodium channels have inactivated.\n\nThe cardiac action potential plays an important role in coordinating the contraction of the heart. The cardiac cells of the sinoatrial node provide the pacemaker potential that synchronizes the heart. The action potentials of those cells propagate to and through the atrioventricular node (AV node), which is normally the only conduction pathway between the atria and the ventricles. Action potentials from the AV node travel through the bundle of His and thence to the Purkinje fibers. Conversely, anomalies in the cardiac action potential—whether due to a congenital mutation or injury—can lead to human pathologies, especially arrhythmias. Several anti-arrhythmia drugs act on the cardiac action potential, such as quinidine, lidocaine, beta blockers, and verapamil.\n\nThe action potential in a normal skeletal muscle cell is similar to the action potential in neurons. Action potentials result from the depolarization of the cell membrane (the sarcolemma), which opens voltage-sensitive sodium channels; these become inactivated and the membrane is repolarized through the outward current of potassium ions. The resting potential prior to the action potential is typically −90mV, somewhat more negative than typical neurons. The muscle action potential lasts roughly 2–4 ms, the absolute refractory period is roughly 1–3 ms, and the conduction velocity along the muscle is roughly 5 m/s. The action potential releases calcium ions that free up the tropomyosin and allow the muscle to contract. Muscle action potentials are provoked by the arrival of a pre-synaptic neuronal action potential at the neuromuscular junction, which is a common target for neurotoxins.\n\nPlant and fungal cells are also electrically excitable. The fundamental difference from animal action potentials is that the depolarization in plant cells is not accomplished by an uptake of positive sodium ions, but by release of negative \"chloride\" ions. Together with the following release of positive potassium ions, which is common to plant and animal action potentials, the action potential in plants infers, therefore, an osmotic loss of salt (KCl), whereas the animal action potential is osmotically neutral, when equal amounts of entering sodium and leaving potassium cancel each other osmotically. The interaction of electrical and osmotic relations in plant cells indicates an osmotic function of electrical excitability in the common, unicellular ancestors of plants and animals under changing salinity conditions, whereas the present function of rapid signal transmission is seen as a younger accomplishment of metazoan cells in a more stable osmotic environment. It must be assumed that the familiar signalling function of action potentials in some vascular plants (e.g. \"Mimosa pudica\") arose independently from that in metazoan excitable cells.\n\nAction potentials are found throughout multicellular organisms, including plants, invertebrates such as insects, and vertebrates such as reptiles and mammals. Sponges seem to be the main phylum of multicellular eukaryotes that does not transmit action potentials, although some studies have suggested that these organisms have a form of electrical signaling, too. The resting potential, as well as the size and duration of the action potential, have not varied much with evolution, although the conduction velocity does vary dramatically with axonal diameter and myelination.\n\nGiven its conservation throughout evolution, the action potential seems to confer evolutionary advantages. One function of action potentials is rapid, long-range signaling within the organism; the conduction velocity can exceed 110 m/s, which is one-third the speed of sound. For comparison, a hormone molecule carried in the bloodstream moves at roughly 8 m/s in large arteries. Part of this function is the tight coordination of mechanical events, such as the contraction of the heart. A second function is the computation associated with its generation. Being an all-or-none signal that does not decay with transmission distance, the action potential has similar advantages to digital electronics. The integration of various dendritic signals at the axon hillock and its thresholding to form a complex train of action potentials is another form of computation, one that has been exploited biologically to form central pattern generators and mimicked in artificial neural networks.\n\nThe study of action potentials has required the development of new experimental methods. The initial work, prior to 1955, was carried out primarily by Alan Lloyd Hodgkin and Andrew Fielding Huxley, who were, along John Carew Eccles, awarded the 1963 Nobel Prize in Physiology or Medicine for their contribution to the description of the ionic basis of nerve conduction. It focused on three goals: isolating signals from single neurons or axons, developing fast, sensitive electronics, and shrinking electrodes enough that the voltage inside a single cell could be recorded.\n\nThe first problem was solved by studying the giant axons found in the neurons of the squid (\"Loligo forbesii\" and \"Doryteuthis pealeii\", at the time classified as \"Loligo pealeii\"). These axons are so large in diameter (roughly 1 mm, or 100-fold larger than a typical neuron) that they can be seen with the naked eye, making them easy to extract and manipulate. However, they are not representative of all excitable cells, and numerous other systems with action potentials have been studied.\n\nThe second problem was addressed with the crucial development of the voltage clamp, which permitted experimenters to study the ionic currents underlying an action potential in isolation, and eliminated a key source of electronic noise, the current \"I\" associated with the capacitance \"C\" of the membrane. Since the current equals \"C\" times the rate of change of the transmembrane voltage \"V\", the solution was to design a circuit that kept \"V\" fixed (zero rate of change) regardless of the currents flowing across the membrane. Thus, the current required to keep \"V\" at a fixed value is a direct reflection of the current flowing through the membrane. Other electronic advances included the use of Faraday cages and electronics with high input impedance, so that the measurement itself did not affect the voltage being measured.\n\nThe third problem, that of obtaining electrodes small enough to record voltages within a single axon without perturbing it, was solved in 1949 with the invention of the glass micropipette electrode, which was quickly adopted by other researchers. Refinements of this method are able to produce electrode tips that are as fine as 100 Å (10 nm), which also confers high input impedance. Action potentials may also be recorded with small metal electrodes placed just next to a neuron, with neurochips containing EOSFETs, or optically with dyes that are sensitive to Ca or to voltage.\n\nWhile glass micropipette electrodes measure the sum of the currents passing through many ion channels, studying the electrical properties of a single ion channel became possible in the 1970s with the development of the patch clamp by Erwin Neher and Bert Sakmann. For this discovery, they were awarded the Nobel Prize in Physiology or Medicine in 1991. Patch-clamping verified that ionic channels have discrete states of conductance, such as open, closed and inactivated.\n\nOptical imaging technologies have been developed in recent years to measure action potentials, either via simultaneous multisite recordings or with ultra-spatial resolution. Using voltage-sensitive dyes, action potentials have been optically recorded from a tiny patch of cardiomyocyte membrane.\n\nSeveral neurotoxins, both natural and synthetic, are designed to block the action potential. Tetrodotoxin from the pufferfish and saxitoxin from the \"Gonyaulax\" (the dinoflagellate genus responsible for \"red tides\") block action potentials by inhibiting the voltage-sensitive sodium channel; similarly, dendrotoxin from the black mamba snake inhibits the voltage-sensitive potassium channel. Such inhibitors of ion channels serve an important research purpose, by allowing scientists to \"turn off\" specific channels at will, thus isolating the other channels' contributions; they can also be useful in purifying ion channels by affinity chromatography or in assaying their concentration. However, such inhibitors also make effective neurotoxins, and have been considered for use as chemical weapons. Neurotoxins aimed at the ion channels of insects have been effective insecticides; one example is the synthetic permethrin, which prolongs the activation of the sodium channels involved in action potentials. The ion channels of insects are sufficiently different from their human counterparts that there are few side effects in humans.\n\nThe role of electricity in the nervous systems of animals was first observed in dissected frogs by Luigi Galvani, who studied it from 1791 to 1797. Galvani's results stimulated Alessandro Volta to develop the Voltaic pile—the earliest-known electric battery—with which he studied animal electricity (such as electric eels) and the physiological responses to applied direct-current voltages.\n\nScientists of the 19th century studied the propagation of electrical signals in whole nerves (i.e., bundles of neurons) and demonstrated that nervous tissue was made up of cells, instead of an interconnected network of tubes (a \"reticulum\"). Carlo Matteucci followed up Galvani's studies and demonstrated that cell membranes had a voltage across them and could produce direct current. Matteucci's work inspired the German physiologist, Emil du Bois-Reymond, who discovered the action potential in 1843. The conduction velocity of action potentials was first measured in 1850 by du Bois-Reymond's friend, Hermann von Helmholtz. To establish that nervous tissue is made up of discrete cells, the Spanish physician Santiago Ramón y Cajal and his students used a stain developed by Camillo Golgi to reveal the myriad shapes of neurons, which they rendered painstakingly. For their discoveries, Golgi and Ramón y Cajal were awarded the 1906 Nobel Prize in Physiology. Their work resolved a long-standing controversy in the neuroanatomy of the 19th century; Golgi himself had argued for the network model of the nervous system.\n\nThe 20th century was a significant era for electrophysiology. In 1902 and again in 1912, Julius Bernstein advanced the hypothesis that the action potential resulted from a change in the permeability of the axonal membrane to ions. Bernstein's hypothesis was confirmed by Ken Cole and Howard Curtis, who showed that membrane conductance increases during an action potential. In 1907, Louis Lapicque suggested that the action potential was generated as a threshold was crossed, what would be later shown as a product of the dynamical systems of ionic conductances. In 1949, Alan Hodgkin and Bernard Katz refined Bernstein's hypothesis by considering that the axonal membrane might have different permeabilities to different ions; in particular, they demonstrated the crucial role of the sodium permeability for the action potential. They made the first actual recording of the electrical changes across the neuronal membrane that mediate the action potential. This line of research culminated in the five 1952 papers of Hodgkin, Katz and Andrew Huxley, in which they applied the voltage clamp technique to determine the dependence of the axonal membrane's permeabilities to sodium and potassium ions on voltage and time, from which they were able to reconstruct the action potential quantitatively. Hodgkin and Huxley correlated the properties of their mathematical model with discrete ion channels that could exist in several different states, including \"open\", \"closed\", and \"inactivated\". Their hypotheses were confirmed in the mid-1970s and 1980s by Erwin Neher and Bert Sakmann, who developed the technique of patch clamping to examine the conductance states of individual ion channels. In the 21st century, researchers are beginning to understand the structural basis for these conductance states and for the selectivity of channels for their species of ion, through the atomic-resolution crystal structures, fluorescence distance measurements and cryo-electron microscopy studies.\n\nJulius Bernstein was also the first to introduce the Nernst equation for resting potential across the membrane; this was generalized by David E. Goldman to the eponymous Goldman equation in 1943. The sodium–potassium pump was identified in 1957 and its properties gradually elucidated, culminating in the determination of its atomic-resolution structure by X-ray crystallography. The crystal structures of related ionic pumps have also been solved, giving a broader view of how these molecular machines work.\n\nMathematical and computational models are essential for understanding the action potential, and offer predictions that may be tested against experimental data, providing a stringent test of a theory. The most important and accurate of the early neural models is the Hodgkin–Huxley model, which describes the action potential by a coupled set of four ordinary differential equations (ODEs). Although the Hodgkin–Huxley model may be a simplification with few limitations compared to the realistic nervous membrane as it exists in nature, its complexity has inspired several even-more-simplified models, such as the Morris–Lecar model and the FitzHugh–Nagumo model, both of which have only two coupled ODEs. The properties of the Hodgkin–Huxley and FitzHugh–Nagumo models and their relatives, such as the Bonhoeffer–van der Pol model, have been well-studied within mathematics, computation and electronics. However the simple models of generator potential and action potential fail to accurately reproduce the near threshold neural spike rate and spike shape, specifically for the mechanoreceptors like the Pacinian corpuscle. More modern research has focused on larger and more integrated systems; by joining action-potential models with models of other parts of the nervous system (such as dendrites and synapses), researchers can study neural computation and simple reflexes, such as escape reflexes and others controlled by central pattern generators.\n\n\n\n\n\n"}
{"id": "29805968", "url": "https://en.wikipedia.org/wiki?curid=29805968", "title": "Alioune Dramé", "text": "Alioune Dramé\n\nAlioune Dramé (born c. 1921 – died 1 March 1977) was a Guinean economist and politician. He also served as an ambassador to Ivory Coast.\n\nDramé served in the first council of the Politburo of the First Republic of Guinea as Minister of Finance from 1957.\nIn this role, he signed the first bank notes of the republic, and established the Guinean franc in 1960.\nDrame was made responsible for the plans for economic development of Guinea for the periods 1960-1963, 1964–1971 and 1973-1979.\n\nOn April 24, 1975, Dramé delivered a letter from the president of Guinea, Ahmed Sékou Touré, to Gerald Ford addressing Guinea's food shortage and requesting assistance from the United States. After the 1976 signing of PL 480 by the American government, Dramé returned to the United States on April 27, 1976 with a letter from the Guinean president to start the agreement between the Guinean and United States governments. During this agreement, Dramé led a team of Guinean representatives to negotiate the Title I and Title II food provided by PL 480.\n\nDramé was Minister of Planning when he was arrested on the night of 18/19 July 1976 and imprisoned at Camp Boiro.\nAt a press conference on 2 August 1976, Ahmed Sékou Touré announced the arrest in Conakry of Dramé and several other alleged plotters, including Telli Diallo, Alpha Oumar Barry and Lamine Kouyaté.\nOn 15 February 1977, Dramé was placed on the \"black diet\" while in Camp Boiro, which meant he was given no food or water until his death. \nHe died on 1 March 1977.\n"}
{"id": "17970962", "url": "https://en.wikipedia.org/wiki?curid=17970962", "title": "Archduke Johann Salvator of Austria", "text": "Archduke Johann Salvator of Austria\n\nArchduke Johann Salvator of Austria (, ; 25 November 1852 – declared dead in absentia 2 February 1911) was a member of the Tuscan branch of the House of Habsburg-Lorraine. He was Archduke and Prince of Austria, Prince of Hungary, Bohemia and Tuscany. After renouncing those titles, he was known as Johann (John) Orth. He disappeared while sailing with his wife in July 1890 and is believed to have died when his ship encountered a storm near Cape Horn. Salvator was declared dead in absentia in February 1911.\n\nJohann Salvator was born in Florence, the youngest son of Leopold II, Grand Duke of Tuscany and his second wife, Princess Maria Antonia of the Two Sicilies. He was baptized in Florence's Battistero di San Giovanni as \"Giovanni Nepomuceno Maria Annunziata Giuseppe Giovanni Batista Ferdinando Baldassare Luigi Gonzaga Pietro Alessandrino Zanobi Antonino\". He pursued a career in the Austrian Army and was a good friend of Rudolf, Crown Prince of Austria, with both sharing liberal opinions.\n\nAfter Bulgaria was granted autonomy by the Ottoman Empire, Johann Salvator was an unsuccessful candidate for the throne. Prince Alexander of Battenberg would be elected Prince of Bulgaria in 1879. During the Austro-Hungarian occupation of the Ottoman territory of Bosnia and Herzegovina in 1878, he was put in command of a division of the occupying army and won numerous honours.\n\nOn 16 October 1889, he resigned his army commission and renounced his title and the privileges he enjoyed as a member of the Austrian Imperial Family. After renouncing his titles he assumed the name \"Johann (or John) Orth\", the surname Orth derived from the name of a castle he had owned, Schloss Orth.\n\nIn 1889, Johann Salvator married Ludmilla (\"Milli\") Stubel, an opera dancer in London. Shortly after his marriage, he purchased a ship named the \"Santa Margareta\", on which he and his wife sailed for South America. In February 1890 he set off from Montevideo, Uruguay, heading for Valparaíso in Chile. He was last seen on 12 July in Cape Tres Puntas, Argentina. It is believed that his ship was lost during a storm off the coast of Cape Horn. He was officially declared dead on 2 February 1911 in Vienna.\n\nIn the years following Salvator's disappearance, numerous sightings of him were reported. Rumors persisted that he and his wife sailed to South America and assumed new identities. Several men also came forward claiming to be the \"missing Duke\". One of the more publicised claims came in May 1945 when a German born lithographer living in Kristiansand, Norway named Alexander Hugo Køhler made a deathbed confession claiming that he was Johann Salvator. Køhler claimed that, as Johann Orth, he \"bought\" the identity of Alexander Hugo Køhler and assumed his life. Køhler claimed that the real Alexander Hugo Køhler posed as Salvator and it was he who died at sea. In 2007, relatives of Køhler requested that his grave be opened so that a DNA test could be performed. Should Køhler and Johann Salvator actually be the same person, Køhler's descendants may be entitled to Johann Salvator's heritage, including Schloss Orth.\n\n"}
{"id": "6929747", "url": "https://en.wikipedia.org/wiki?curid=6929747", "title": "C57BL/6", "text": "C57BL/6\n\nC57BL/6, often referred to as \"C57 black 6\", \"C57\" or \"black 6\", is a common inbred strain of laboratory mouse.\n\nIt is the most widely used \"genetic background\" for genetically modified mice for use as models of human disease. They are the most widely used and best-selling mouse strain, due to the availability of congenic strains, easy breeding, and robustness.\n\nC57BL/6 mice have a dark brown, nearly black coat. They are more sensitive to noise and odours and are more likely to bite than the more docile laboratory strains such as BALB/c. They are good breeders.\n\nGroup-housed B6 female mice display barbering behavior, in which the dominant mouse in a cage selectively removes hair from its subordinate cage mates. Mice that have been barbered have large bald patches on their bodies, commonly around the head, snout, and shoulders, although barbering may appear anywhere on the body. Both hair and whiskers may be removed.\n\nC57BL/6 has many unusual characteristics that make it useful for some work and inappropriate for other: It is unusually sensitive to pain and to cold, and analgesic medications are less effective in it. Unlike most mouse strains, it drinks alcoholic beverages voluntarily. It is more susceptible than average to morphine addiction, atherosclerosis, and age-related hearing loss.\n\nThe C57BL/6 mouse was the second-ever mammalian species to have its entire genome published.\n\nThe dark coat make the mouse strain convenient for creating transgenic mice: it is crossed with a light-furred 129 mouse, and the desirable crosses can be easily identified by their mixed coat colors.\n\nThere now exist colonies of mice derived from the original C57BL/6 colony that have been bred in isolation from one another for many hundreds of generations. Owing to genetic drift these colonies differ widely from one another (and, it goes without saying, from the original mice isolated at the Bussey Institute). Responsible scientists, including those at accredited repositories, are careful to point out this fact and take pains to distinguish sublines such as C57BL/6J (the established subline at The Jackson Laboratory) from C57BL/6N, etc. But even within these sublines, the potential for drift exists in colonies maintained by individual laboratories who do not have a systematic practice of reestablishing breeders from a centralized, vetted stock.\n\nBy far the most popular laboratory rodent, the C57BL/6 mouse accounts for half to five-sixths of all rodents shipped to research laboratories from American suppliers. Its overwhelming popularity is due largely to inertia: it has been widely used and widely studied, and therefore it is used even more.\n\nIn 2013 C57BL/6 mice were flown into space aboard Bion-M No.1.\n\nIn 2015 C57BL/6NTac females provided by Taconic Biosciences were sent to the International Space Station on SpaceX CRS-6.\n\nThe inbred strain of C57BL mice was created in 1921 by C. C. Little at the Bussey Institute for Research in Applied Biology. The substrain \"6\" was the most popular of the surviving substrains.\n"}
{"id": "1512776", "url": "https://en.wikipedia.org/wiki?curid=1512776", "title": "Charlie Bowdre", "text": "Charlie Bowdre\n\nCharles Bowdre (1848 – December 23, 1880) was an American cowboy and outlaw. He was an associate of Billy the Kid and member of his gang.\n\nBowdre was born in Wilkes County, Georgia. When he was three years old, he and his parents moved to Mississippi. By 1854, young Charlie started working in his father's farm, and as he grew up became an adept farmer. Much of what Bowdre did between the year in which his last sister was born (1863) and 1874, remains a mystery.\n\nIt is believed, however, that he abandoned the family's farm to become a wanderer. Records show that by 1874, he had arrived at Lincoln County, New Mexico. Bowdre became friends with Doc Scurlock during this time, and the two men opened a cheese factory on the Gila River. He also joined Scurlock on several posses during this period, pursuing cattle thieves and rustlers, on several occasions taking part in the lynching of those captured. On July 18, 1876, Bowdre, Scurlock, Frank Coe, George Coe, and Ab Saunders stormed the very weak Lincoln jail, freeing cattle rustler Jesus Largo from the custody of Sheriff Saturnino Baca, taking Largo outside of town and hanging him. No charges were ever filed for the event. On August 5, 1877, he and a companion were arrested for \"shooting up\" the town of Lincoln while intoxicated.\n\nWith the outbreak of the Lincoln County War in 1878, Bowdre sided with the Tunstall-McSween side, and he met Billy, Jose Chavez y Chavez and the rest of the Kid's associates, including Richard M. Brewer and Jim French, George Coe and Frank Coe. During the conflict, he was known to have been present with his fellow Regulators when William Morton, Frank Baker, and William McCloskey were killed along the Blackwater Creek on March 9, 1878. Bowdre was shot by Buckshot Roberts during the Gunfight of Blazer's Mills on April 4, 1878, and in turn shot Roberts. Bowdre would be charged with killing Buckshot Roberts during the Blazer's Mills Gunfight. He was also present in the July 15–19, 1878 Battle of Lincoln.\n\nBowdre worked as a cowboy on the ranches of Thomas Yerby and Pete Maxwell as the war went on, as well as being an active participant. Bowdre married a twenty-five-year-old Mexican woman, Manuela Herrera, some months before his death. Manuela was a sister to Doc Scurlock's wife, María Antonia Miguela Herrera, known as Antonia. The fact that he was recently married when he died makes him less likely to have been involved in the gang's activities during the few weeks that passed between his marriage and his death.\n\nBy December 1880, Charlie Bowdre was ready to quit riding with Billy the Kid and surrender for the murder of Buckshot Roberts, but he still joined the rest of the gang on a mission to ambush Pat Garrett in Fort Sumner. A gun battle ensued, but Bowdre and most of the Kid's gang members escaped alive. On December 23, however, the gang was holed up in a rock house at Stinking Springs. At dawn, Charlie Bowdre emerged to feed the horses and was riddled with rifle slugs by Garrett's posse, which had surrounded the building in the night. Later that day, Billy the Kid and his partners gave up. After being riddled with bullets he fell back into the doorway where, at the urging of Billy the Kid to 'take a few of them with you when you die', Bowdre made a valiant exit. Unfortunately he was already too weak and near death at that point and couldn't get his gun out of his holster. In the last seconds of his life he stumbled and fell towards Pat Garrett repeating the phrase, \"I wish...I wish...\".\n\nHis remains were returned to his wife, and he was interred next to Tom O'Folliard, another member of Billy's gang. They were joined later by Billy himself, after he was killed in July, 1881. In 1962, a relative named Louis Bowdre was found, and a court tried to have Bowdre's remains removed. But the relative disagreed, saying that Bowdre would prefer to rest next to O'Folliard.\n\nCharlie Bowdre was played by James Congdon in the 1958 film \"The Left Handed Gun\", by Ron Soble in the John Wayne film Chisum and by Charles Martin Smith in Sam Peckinpah's \"Pat Garrett & Billy the Kid\" (1973). In the 1988 film \"Young Guns\", he is portrayed by actor Casey Siemaszko. The circumstances of his death were the basis of a scene in \"Young Guns II\", however, in the movie Doc Scurlock, played by Kiefer Sutherland, is the one who meets his fate outside the hut, and not Bowdre. He will be portrayed by Chris Bylsma in the upcoming film \"The Kid\" directed by Vincent D'Onofrio.\n\n"}
{"id": "12237478", "url": "https://en.wikipedia.org/wiki?curid=12237478", "title": "Crinozoa", "text": "Crinozoa\n\nCrinozoa is a subphylum of mostly sessile echinoderms, of which the crinoids, or sea lilies, are the only extant members. Crinozoans have an extremely extensive fossil history which may or may not extend into the Precambrian (provided the enigmatic Ediacaran \"Arkarua\" can be positively identified as an edrioasteroid).\n\nThe classes currently contained within Crinozoa include Crinoidea, Cystoidea, Edrioasteroidea, and Rhombifera.\n\n"}
{"id": "26613551", "url": "https://en.wikipedia.org/wiki?curid=26613551", "title": "Delayed grief", "text": "Delayed grief\n\nThe terms delayed grief and unresolved grief are variations of grieving after a loss. The meaning of \"unresolved grief\" is any aspect of grieving that has yet to be resolved.\n\nIn cases of delayed grief, the reaction to the loss is postponed until a later time, even years later, and might be triggered by a seemingly unrelated event, such as a recent divorce\nor even the death of a pet, but with reactions excessive to the current situation.\n\nThe delayed grief might manifest as any of the reactions in normal grief: pangs of intense yearning, spasms of distress, short bouts of hysterical laughter, tearful or uncontrolled sobbing, feeling of hopelessness, restlessness, insomnia, preoccupation with thoughts about the loved one, extreme and unexplained anger, or general feelings of depression. In extreme cases reaction may invoke suicidal tendencies. \n\nThe term \"delayed grief\" is also used to describe a \"pattern\" in which symptoms of distress, seeking, yearning (etc.), are occurring at a much later time period than is typical. Delayed grief refers to any reaction that occurs later than usual, as a delayed onset of symptoms. Contrast to the term \"complicated grief\" as meaning a form of grieving that spans years (see full description at: Grief).\n\nIn a 1987 study, of 135 people with cancer, who were referred for psychological counseling, 76% of them reported a previous grief experience, and 60% of the cancer patients still had unresolved grief from prior losses.\n\n"}
{"id": "5035335", "url": "https://en.wikipedia.org/wiki?curid=5035335", "title": "Edwin Valero", "text": "Edwin Valero\n\nEdwin Valero (December 3, 1981 – April 19, 2010) was a Venezuelan professional boxer who competed from 2002 to 2010. He was an undefeated former world champion in two weight classes at the time of his death, having held the WBA super featherweight title from 2006 to 2008, and the WBC lightweight title from 2009 to 2010. A southpaw known for his highly aggressive style and exceptional punching power, Valero remains the only champion in the 30-year history of the WBC to win every fight in his career by knockout. In 2010, Valero committed suicide in jail after being arrested on suspicion of killing his wife.\n\nValero started boxing at the age of 12, ostensibly compiling an amateur record of 86–6 with 57 knockouts. He was a Venezuelan national amateur champion three years running, as well as a Central and South American champion (beating Francisco Bojado).\n\nOn February 25, 2006, Valero set a new world record by winning his first 18 fights as a professional by first-round knockout, breaking Arthur Susskind's historic record set in 1905. That record has since been broken by Tyrone Brunson, but most boxing experts do not acknowledge Brunson's claim due to the extremely poor level of opposition he faced while making his way to the record; in contrast to Valero's opponents, just one of Brunson's 19 opponents had a winning record, and 6 had failed to win a single fight in their careers.\n\nBecause of his punching power and perfect knockout ratio, Valero became a cult sensation in the community. His biggest backers in the sport included Doug Fischer of \"The Ring\" magazine (who, on the former boxing website he used to write for, Maxboxing.com, regularly covered Valero in his articles for the website which also aired videos of his workouts and sparring sessions) and Boxing Inside with journalist Peter Palmiere. The Los Angeles local cable show also aired Valero's workouts, sparring sessions and interviews conducted by Palmiere.\n\nIn his first attempt at a world title, on August 5, 2006, Valero faced WBA super featherweight champion Vicente Mosquera. In what would arguably prove to be both boxers' toughest contest, Valero started out the match in signature fashion, knocking down the champion twice in the first round. However, Mosquera recovered and in the third round responded by knocking Valero down, which was to be Valero's only knockdown in his career. At this point in his 19–0 career, Valero's longest fight had only been two rounds, and the question remained as to whether the untested Valero had the stamina to go the distance. The answer came after ten grueling rounds when the ever-tenacious Mosquera finally started to wane under the challenger's continuous heavy-handed counters. Deciding Mosquera had received enough punishment, the referee called a halt to the match at 2:00 of round ten, making the 24-year-old Valero champion. Valero would go on to successfully defend the title four times before moving up in weight class, with his final defense a seventh-round stoppage over Takehiro Shimada in Tokyo on June 12, 2008.\n\nOn September 3, 2008, Valero vacated his WBA title to fight in the lightweight division. He fought Antonio Pitalua for the vacant WBC lightweight title on April 4, 2009 in Austin, Texas. The bout marked the first time Valero had fought in the United States since 2003. Pitalua came into the fight with 14 consecutive knockouts on his 46–3 record, and with Valero's 24 consecutive knockouts, the stage was set for a decisive match between two heavy hitters. After an uneventful first round, Valero knocked Pitalua down just seconds into the second round with a right hook. Pitalua managed to get up, but suffered two more knockdowns before the referee stopped the fight at 0:49 of round two.\n\nValero's next fight came on his home turf of Venezuela, in La Guaira, where he successfully defended his WBC lightweight title by a TKO victory over Hector Velasquez in the seventh round. Valero's second and final defense of the belt came against Antonio DeMarco in Mexico, on February 6, 2010. In the second round, Valero suffered a serious cut over his right eye after DeMarco landed an unintentional elbow. Valero was able to continue the fight and went on to win by corner retirement when DeMarco failed to answer the bell for the tenth round. This would be Valero's last match. In March 2010, Valero vacated his WBC title in order to compete in the light welterweight division. Valero's professional record at the time of his death was 27–0, making him one of the few world champions to finish their careers undefeated.\n\nOn February 5, 2001, Valero was involved in a severe motorcycle accident in which he was not wearing a helmet. He fractured his skull and had surgery to remove a blood clot. This injury was sustained prior to him launching his pro career, and it created roadblocks to major bodies sanctioning his fights. Valero claimed that his doctor cleared him to fight on January 17, 2002, and he turned pro that July with a first-round KO.\n\nValero appeared to hit the jackpot when he was signed after his 12th pro fight by Golden Boy Promotions. Valero was scheduled to appear on HBO's \"Boxing After Dark\", but in January 2004, he failed an MRI due to brain scan irregularities in New York and thus was not allowed to fight in the United States. As a result, the fight did not take place. He continued to fight outside the US and on March 25, 2008, Valero was cleared to box in the state of Texas.\n\nIt was reported on September 27, 2009, that Edwin Valero had been arrested on assault charges. A man alleged that the boxer attacked his mother and sister over a feud. Valero denied the allegations and considered them an attempt to harm his reputation. His mother came forward to tell the media that no foul play was involved.\n\nOn March 25, 2010, Valero was again accused of assault, this time by his wife, who was sent to hospital for bruises and a damaged lung. Valero denied any wrongdoing, stating his wife stumbled from a stairway but investigators doubted him. His wife later told authorities that her injuries were caused by an accident on some stairs, despite the fact that she had been treated for similar injuries twice before at the hospital. Because of the vicious personality he showed at the hospital where his wife was treated, Valero was sent for six months of psychiatric rehabilitation.\n\nOn April 18, 2010, Valero was arrested after police found the body of his 24-year-old wife, Jennifer Carolina Viera de Valero, in a hotel in the city of Valencia, Carabobo. She had been stabbed three times. Valero was considered a suspect and was taken to jail. \nValero allegedly admitted to hotel security and police that he had murdered his wife.\n\nThe day after being taken to jail, Valero was found hanging in his jail cell by his pants. He was pronounced legally dead at 1:30 am. His death was officially ruled to be a suicide, though there has been some skepticism about that ruling.\n\n\n"}
{"id": "956135", "url": "https://en.wikipedia.org/wiki?curid=956135", "title": "Elisabeth of Valois", "text": "Elisabeth of Valois\n\nElisabeth of Valois (; ) (2 April 1545 – 3 October 1568) was a Spanish queen consort as the third spouse of Philip II of Spain. She was the eldest daughter of Henry II of France and Catherine de' Medici.\n\nShe was born in the Château de Fontainebleau. She was raised under the supervision of the governor and governess of the royal children, Jean d'Humières and Françoise d'Humières. Her childhood was spent in the French royal nursery, where her father insisted she share her bedroom with her future sister-in-law, Mary, Queen of Scots, who was about three years older than Elisabeth. Even though Elisabeth had to give precedence to Mary (since Mary was already a crowned queen), the two would remain close friends for the rest of their lives. While it is acknowledged that her sister Margaret and her future sister-in-law Mary were prettier than she, she was one of Catherine's attractive daughters. Elisabeth was also described as being shy, timid and very much in awe of her formidable mother; although there is also evidence that Catherine was tender and loving toward Elisabeth. This was certainly evident in her letters to Elisabeth.\n\nIn 1550, Elisabeth's father, Henry, began negotiations for her marriage to Edward VI of England. This arrangement brought condemnation from Pope Julius III who reportedly stated that he would excommunicate both if they married. Henry, undeterred, agreed to a 200,000 ecus dowry, which became irrelevant upon Edward's death in 1553.\n\nElisabeth married Philip II of Spain son of Charles V, Holy Roman Emperor, and Isabella of Portugal in 1559. Originally married via proxy at Notre Dame (with the Duke of Alba standing in for Philip) prior to leaving France, the actual ceremony took place in Guadalajara, Spain, upon her arrival. The marriage was a result of the Peace of Cateau Cambrésis (1559). His second wife, Mary I of England, had recently died, making Elisabeth of Valois Philip's third wife.\n\nAt her wedding, she met the famous painter Sofonisba Anguissola and Ana de Mendoza, who would live with her the rest of her life. Phillip II appointed Anguissola to be a lady-in-waiting and court painter for his queen. Under Anguissola's tutelage, Elisabeth improved her amateur painting skills. Anguissola also influenced the artistic works of her children, Isabella Clara Eugenia and Caterina Michaela, during her time at the court.\n\nPhilip was completely enchanted by his 14-year-old bride, and by 1564 had given up his infidelities. Despite the significant age difference, Elisabeth was also quite pleased with her husband. (In letters to her mother, she proclaimed herself to be fortunate to have married so charming a prince.) Philip enjoyed hosting chivalric tournaments to entertain his wife. Elisabeth would play liege lady to the three young princes of the Spanish Court: Carlos, Prince of Asturias, John of Austria (illegitimate son of Charles V), and Alexander Farnese, Duke of Parma (son of Charles V's illegitimate daughter Margaret).\n\nElisabeth had originally been betrothed to Philip's son, Carlos, Prince of Asturias, but political complications unexpectedly necessitated instead a marriage to Philip. Her relationship with her troubled stepson Carlos was warm and friendly. Despite reports of his progressively bizarre behavior, Carlos was always kind and gentle to Elisabeth. When it eventually became necessary for Philip to lock him away (which shortly led to the Prince’s demise), Elisabeth cried for days.\n\nPhilip was very attached to Elisabeth, staying close by her side even when she was ill with smallpox. Elisabeth's first pregnancy in 1564 ended with a miscarriage of twin girls. She later gave birth to Infanta Isabella Clara Eugenia of Spain on 12 August 1566, and then to Isabella's younger sister Catherine Michelle of Spain on 10 October 1567. Elisabeth had another miscarriage on 3 October 1568, and died the same day, along with her newborn infant daughter.\n\nAfter the death of Elisabeth, Catherine de' Medici offered her younger daughter Margaret as a bride for Philip. Philip declined the offer.\n\nElisabeth of Valois is a central character in Thomas Otway's play \"Don Carlos\", in Schiller's play of the same name, in Verdi's opera adapted from Schiller's play, also titled \"Don Carlos\", and in several other, less well-known operas; Antonio Buzzolla's version of 1850 is actually named \"Elisabetta di Valois\". All these works imply a tragic romance between Elisabeth and Carlos, suggesting that they were really in love with each other when Elisabeth was forced to break off her engagement to Carlos and marry his father Philip.\n\nIn Madame de Lafayette's novella \"The Princesse de Cleves\", Elisabeth of Valois' marriage to Philip II is the occasion for the wedding games at which her father Henri II dies; her role is brief but it substantially affects the novella's narrative arc.\n\nElisabeth of Valois is portrayed by Caoimhe O'Malley (in the Pilot) on the CW show, Reign, then later by Anastasia Phillips in the fourth season of the show. Rather than the “plain Jane” of her family the character is portrayed as beautiful and her timid and shy personality is changed to that of one more forceful, cold, insensitive and quite controlling which is driven out of her legitimate desire for her mother’s affections. Her name \"Elisabeth\" is spelled with an S instead of a Z though in the fourth season she is simply called \"Leeza\" as not to be confused with Queen Elizabeth I. In the first episode, Leeza is married to King Philip of Spain, leaves for Spain with her husband without speaking any lines, and isn't physically seen until fourth season of the show. Though not seen until the fourth season, she is mentioned as a possible option for god-mother to her King Francis' (her brother's) son John in the second season. In the third season, her kingdom is chosen as a safe haven for her younger sisters and brothers after a group calling themselves \"The Red Knights\" threaten revenge on the Valois family. Making her physical return to the show, she is one of the four Queens (Queen Mary, Queen Elizabeth I and her mother, regent Queen Catherine) to be featured during the season. During her appearance, she spends time in French court as a envoy of Spain and voice of her husband's wishes as well rival for her mother.\n\nElisabeth of Valois is a major character in \"The Creation of Eve\", a novel by Lynn Cullen based on the history of Sofonisba Anguissola, the first renowned female artist of the Italian Renaissance. In the novel, Sofonisba is the painting tutor and premier lady-in-waiting for young Queen Elisabeth.\n\n"}
{"id": "38870592", "url": "https://en.wikipedia.org/wiki?curid=38870592", "title": "Ernest Braun", "text": "Ernest Braun\n\nErnest Braun (9 March 1925 – 3 March 2015) was a British-Austrian scholar in technology policy and technology assessment.\n\nBorn in Vienna as Czechoslovak citizen, Braun grew up in Czechoslovakia. He studied physics at Charles University in Prague (1952 MSc, Dr.rer.nat.), PhD in solid state physics (Bristol 1959). Research in industrial research laboratory, then changed to University career. Appointed professor of physics at Aston University in Birmingham (1967). In 1973 started an interdisciplinary post-graduate research unit, the Technology Policy Unit (TPU). The topics of research embraced all social aspects of technology, including questions of policy, technology assessment, and the process and effects of technological innovation. In the same year Ernest Braun, together with the late Bill Williams and Michael Gibbons founded an interuniversity group known as “Science in a Social Context” (SISCON). This group obtained some funding and hired research fellows who produced teaching texts published by Butterworth. The purpose was to assist in the teaching of social aspects of science and technology to undergraduates in a variety of disciplines. The TPU at Aston University started an MSc course under the title “Social Aspects of Science and Technology” and also recruited several doctoral students. Many of the graduates of the doctoral programme later became professors in British universities. Also David Collingridge, the author of the Collingridge dilemma worked at TPU.\nIn 1982/83 Braun was visiting professor at Vienna Technical University. In 1984 he retired from Aston University. and became visiting professor at University of Vienna, followed by several free-lance research projects in Austria. In 1985 he joined the Austrian Academy of Sciences (OAW), where he started a research group on technology assessment. In 1988, this group became the Technology Assessment Unit (FTB), headed by him till his retirement from the OAW in 1991, when he returned to England. Braun was followed by Gunther Tichy and under his leadership the FTB became a fully-fledged Institute of Technology Assessment (ITA). On his return to England, Braun became visiting professor in the Open University in Milton Keynes. When he finally retired in 1994 he lived for five years in Portugal and in 2010 moved to Austria with his wife Doris (née Luttenberger), a painter. He had two daughters from his first marriage, both living in London. He died on 3 March 2015 in Bruckneudorf, just 6 days shy of his 90th birthday.\n\nBraun is regarded as a European pioneer of social studies in science and technology in general, and of technology assessment in particular. In Austria he is regarded as the founder of technology assessment. He has written several books and many journal articles. His best known books are probably Revolution in Miniature (with Stuart Macdonald) 1978, Futile Progress 1995, Technology in Context 1998. His last book, From Need to Greed, The Changing Role of Technology in Society, published in 2010.\n\n\n\nThis short biography is based on an autographic curriculum vitae by E. Braun as well as research carried out at the Institute of Technology Assessment in Vienna, among others published in this article (in German): Nentwich/Peissl, 2005, 20 Jahre Technikfolgenabschätzung in Österreich. In: Nentwich, Michael; Peissl, Walter (Hrsg.), Technikfolgenabschätzung in der österreichischen Praxis, Festschrift für Gunther Tichy; Wien: Verlag der Österreichischen Akadmie der Wissenschaften, pp. 11–32, PDF; 272 kB. This article is mostly translated from original in the German Wikipedia.\n\nObituary: \"Ernest Braun 1925-2015\" \n"}
{"id": "41545400", "url": "https://en.wikipedia.org/wiki?curid=41545400", "title": "Ernst Moltzer", "text": "Ernst Moltzer\n\nErnst Octavianus Moltzer (May 1, 1910 in Amsterdam – November 15, 1941 in North sea) was a sailor from the Netherlands, who represented his native country as at the 1936 Summer Olympics in Kiel. Ernst, as crew member on the Dutch 6 Metre \"De Ruyter\", took the 8th place with helmsman Joop Carp and fellow crew members: Ansco Dokkum, Kees Jonker and Herman Looman.\n\nErnst Moltzer married in 1939 with countess \"Gertrude Anna Luise Therese Thusnelde von Sarnthein\" from Austria. He worked as a director of Lucas Bols. Moltzer was during World War II a 'Engelandvaarder' who died on the North Sea while trying to escape to England. His death certificate, dated January 12, 1951, states the date of death as November 15, 1941.\n\n"}
{"id": "34385157", "url": "https://en.wikipedia.org/wiki?curid=34385157", "title": "Gary DeVore", "text": "Gary DeVore\n\nGary DeVore (September 17, 1941 – June 28, 1997) was a Hollywood screenwriter best known for \"Raw Deal\" and for his bizarre death in 1997.\n\nDevore began his writing career in the late 1960s on shows like Chuck Barris' \"The Newlywed Game\", \"The Steve Allen Show\", and \"Tempo\".\n\n\nDeVore married the singer Maria Cole (1969–1978) and the actresses Sandie Newton (1981–1985), Claudia Christian (1988–1992), and Wendy Devore (1996–1997).\n\nDeVore disappeared in June 1997, while driving at night from Santa Fe, New Mexico to Santa Barbara, California, prompting an extensive search and media speculation. A year later, he and his car were discovered submerged below a bridge over the California Aqueduct in Palmdale, California.\n\nA multimedia project, \"The Writer with No Hands\", posited that DeVore's death was the result of a US government conspiracy. It also alleges that the hands autopsied with the body were 200 years old and therefore not DeVore's.\n\nBroadcaster Sean Stone compared the mystery over Devore's fate to that of President John F. Kennedy.\n\n"}
{"id": "1220464", "url": "https://en.wikipedia.org/wiki?curid=1220464", "title": "Hampden Zane Churchill Cockburn", "text": "Hampden Zane Churchill Cockburn\n\nMajor Hampden Zane Churchill Cockburn (19 November 1867 – 12 July 1913) was a Canadian soldier, and recipient of the Victoria Cross, the most prestigious award for gallantry in the face of the enemy that can be awarded to British and Commonwealth forces.\n\nBorn in Toronto, Ontario, Canada, Cockburn was a graduate of Upper Canada College in Toronto.\n\nWhen the Second Boer War broke out in 1899, Cockburn was a 32-year-old lieutenant in The Royal Canadian Dragoons, Canadian Militia, and was posted to South Africa with his regiment, where the action took place for which he was awarded the VC.\n\nOn 7 November 1900, during the Battle of Leliefontein near the Komati River, a large force of Boer commandos sought to encircle a retreating British column whose rearguard comprised two troops of Royal Canadian Dragoons and two 12-pounder guns of \"D\" Battery, Royal Canadian Field Artillery. Cockburn and Lieutenant Richard Turner commanded a small group of troopers who repulsed the Boers at close range, allowing the two field guns to escape capture. Sergeant Edward Holland of the Royal Canadian Dragoons, ably assisted them with good machine-gun work, finally fleeing in the face of superior Boer force with the machine gun under his arm to avoid its capture. All the men under Cockburn's command were either killed, wounded or captured. Cockburn was also wounded during the action.\n\nFollowing the battle, three men of the Royal Canadian Dragoons were awarded the Victoria Cross: Cockburn, Turner and Holland.\n\nThe citations were published in the \"London Gazette\" of 23 April 1901. Cockburn's read:\n\nFollowing the Boer War, Cockburn returned to Canada, and eventually achieved the rank of major. He died in a horse-riding accident in Grayburn, Saskatchewan, in 1913, and was buried at St. James Cemetery, Toronto, Ontario, with a headstone at Hill A. Section S 1/2. Lot 11.\n\nCockburn's Victoria Cross and sword were, for many years, displayed in the lobby of his alma mater, Upper Canada College. In 1977, the school had a high-quality copy made for display, and moved the original to safe-keeping.\n\n\n"}
{"id": "1935846", "url": "https://en.wikipedia.org/wiki?curid=1935846", "title": "Harrying of the North", "text": "Harrying of the North\n\nThe Harrying of the North was a number of campaigns \nwaged by William the Conqueror in the winter of 1069–70 to subjugate northern England, where the presence of the last Wessex claimant, Edgar Atheling, had encouraged Anglo-Danish rebellions. William paid the Danes to go home, but the remaining rebels refused to meet him in battle, and he decided to starve them out by laying waste to the northern shires, especially the city of York, before installing a Norman aristocracy throughout the region.\n\nContemporary chronicles vividly record the savagery of the campaign, the huge scale of the destruction and the widespread famine caused by looting, burning and slaughtering. Some present-day scholars have labelled the campaigns a genocide although others doubt whether William could have assembled enough troops to inflict so much damage and they have concluded that the records may have been exaggerated or misinterpreted.\n\nAt the time of the Norman Conquest \"the North\" consisted of what became Yorkshire, Durham, and Northumberland in the east and Lancashire with the southern parts of Cumberland and Westmorland in the west. The population of the north pre-conquest can be described as \"Anglo-Scandinavian\" carrying a cultural continuity from a mixing of Viking and Anglo-Saxon traditions. The dialect of English spoken in Yorkshire may well have been unintelligible to people from the south of England, and the aristocracy was primarily Danish in origin.\n\nFurther, communications between the north and south were difficult, partly due to the terrain but also because of the poor state of the roads. The more popular route between York and the south was by ship. In 962 Edgar the Peaceful had granted legal autonomy to the northern earls of the Danelaw in return for their loyalty; this had limited the powers of the Anglo-Saxon kings who succeeded him north of the Humber. The earldom of Northumbria stretched from the Tees to the Tweed.\n\nAfter the defeat of the English army and death of Harold Godwinson at the Battle of Hastings, English resistance to the conquest was centred on Edgar Ætheling, the grandson of Edmund Ironside. Ironside was half-brother to Edward the Confessor. It is said the English conceded defeat, not at Hastings, but at Berkhamsted two months later when Edgar and his supporters submitted to William in December 1066. However, of all the men who submitted to William at Berkhamsted it was only Ealdred, Bishop of York, who would remain loyal to the Norman king. William faced a series of rebellions and border skirmishes in Dover, Exeter, Hereford, Nottingham, Durham, York and Peterborough.\n\nCopsi, a supporter of Tostig (a previous Anglo-Saxon earl of Northumbria who had been banished by Edward the Confessor), was a native of Northumbria and his family had a history of being rulers of Bernicia, and at times Northumbria. Copsi had fought in Harald Hardrada's army with Tostig, against Harold Godwinson at the Battle of Stamford Bridge in 1066. He had managed to escape after Harald's defeat. When Copsi offered homage to William at Barking in 1067, William rewarded him by making him earl of Northumbria.\n\nAfter just five weeks as earl, Copsi was murdered by Osulf, son of Earl Eadulf III of Bernicia. When, in turn, the usurping Osulf was also killed, his cousin, Cospatrick, bought the earldom from William. He was not long in power before he joined Edgar Ætheling in rebellion against William in 1068.\n\nWith two earls murdered and one changing sides, William decided to intervene personally in Northumbria. He marched north and arrived in York during the summer of 1068. The opposition melted away, with some of them – including Edgar – taking refuge at the court of the Scottish king Malcolm III.\n\nBack in Northumbria, William changed tack and appointed a Norman, Robert de Comines, as earl, rather than an Anglo-Saxon. Despite warnings from the bishop, Ethelwin, that a rebel army was mobilised against him, Robert rode into Durham with a party of men on 28 January 1069, where he and his men were surrounded and slaughtered. The rebels then turned their attention to York where they killed the guardian of the castle there plus a large number of his men. William's response was swift and brutal: he returned to York, where he fell on the besiegers, killing or putting them to flight.\n\nPossibly emboldened by the fighting in the north, rebellions broke out in other parts of the country. William sent earls to deal with problems in Dorset, Shrewsbury and Devon, while he dealt with rebels in the Midlands and Stafford.\n\nEdgar Ætheling had sought assistance from the king of Denmark, Sweyn II, a nephew of King Canute. Sweyn assembled a fleet of ships under the command of his sons. The fleet sailed up the east coast of England raiding as they went. The Danes with their English allies retook the city of York. Then, in the winter of 1069, William marched his army from Nottingham to York with the intention of engaging the rebel army. However, by the time William's army had reached York, the rebel army had fled, with Edgar returning to Scotland. As they had nowhere suitable on land to stay for the winter, the Danes decided to go back to their ships in the Humber Estuary. After negotiation with William, it was agreed that, if he made payment to them, then they would go home to Denmark without a fight. With the Danes having returned home, William's patience with the rebels seems to have run out. As they were not prepared to meet his army in pitched battle, he employed a strategy that would attack the rebel army's sources of support and their food supply.\n\nWilliam's strategy, implemented during the winter of 1069–70 (he spent Christmas 1069 in York), has been described by William E. Kapelle and some other modern scholars as an act of genocide.\n\nContemporary biographers of William considered it to be his cruelest act and a \"stain upon his soul\". Writing about the Harrying of the North, over fifty years later, the Anglo-Norman chronicler Orderic Vitalis wrote (summarized):\n\nThe land was ravaged on either side of William's route north from the River Aire. His army destroyed crops and settlements and forced rebels into hiding. In the New Year of 1070 he split his army into smaller units and sent them out to burn, loot, and terrify. Florence of Worcester said that from the Humber to the Tees, William's men burnt whole villages and slaughtered the inhabitants. Food stores and livestock were destroyed so that anyone surviving the initial massacre would succumb to starvation over the winter. The survivors were reduced to cannibalism.\n\nRefugees from the harrying are mentioned as far away as Worcestershire in the Evesham Abbey chronicle.\n\nIn 1086, Yorkshire and the North Riding still had large areas of waste territory. The Domesday Book entries indicate \"wasteas est\" or \"hoc est vast\" (\"it is wasted\") for estate after estate; in all a total of 60% of all holdings were waste. It states that 66% of all villages contained wasted manors. Even the prosperous areas of the county had lost 60% of its value compared to 1066. Only 25% of the population and plough teams remained with a reported loss of 80,000 oxen and 150,000 people.\n\nIndependent archaeological evidence supports the massive destruction and displacement of people. The archaeologist Richard Ernest Muir wrote that there was evidence for the \"violent disruption [that] took place in Yorkshire in 1069–71, in the form of hoards of coins which were buried by the inhabitants.\" B.K. Roberts in his book \"The Making of the English Village\", suggests the reason that large numbers of villages have been laid out in regular pattern in Durham and Yorkshire, was through a restructuring at a single point in time, as opposed to natural settlement growth. He goes on to say that it is highly unlikely that such plans could have resulted from piecemeal additions and must have been necessary after the Harrying of the North. The dating is thought to be secure as it is known that Norman lords used similar regular plans in founding new towns in the 'plantation' of rural settlements in other conquered parts of the British Isles.\n\nHowever, although the Domesday Book records large numbers of manors in the north as waste, some historians have posited it was not possible for William's relatively small army to be responsible for such wide-scale devastation imputed to him, so perhaps raiding Danes or Scots may have contributed to some of the destruction. It has been variously argued that \"waste\" signified manorial re-organisation, some form of tax break, or merely a confession of ignorance by the Domesday commissioners when unable to determine details of population and other manorial resources.\n\nAccording to Paul Dalton, it was questionable whether the Conqueror had the time, manpower or good weather necessary to reduce the north to a desert. It was evident, from the chroniclers, that William did harry the north but as the bulk of William's troops, Dalton suggests, were guarding castles in southern England and Wales, and as William was only in the north for a maximum of three months, the amount of damage he could do was limited.\n\nMark Hagger suggests that in the words of the \"Anglo-Saxon Chronicle\", William's Harrying of the North was \"stern beyond measure\" but we should not describe it as genocide as William was acting by the rules of his own time, not ours. Vegetius, the Latin writer, wrote his treatise \"De Re Militari\" in the fourth century about Roman warfare, and posits that this still would have provided the basis for military thinking in the eleventh century. Vegetius said \"The main and principal point in war is to secure plenty of provisions and to destroy the enemy by famine\", so Hagger's conclusion is that the Harrying of the North was no worse than other similar conflicts of the time.\n\nOther historians have questioned the figures supplied by Orderic Vitalis, who was born in 1075 and would have been writing his \"Ecclesiastical History\" around 55 years after the event. The figure of 100,000 deaths was perhaps used in a rhetorical sense, as the estimated population for the whole of England, based on the 1086 Domesday returns was about 2.25 million; thus, a figure of 100,000 represented a large proportion of the entire population of the country at that time (~4.5%).\n\nDavid Horspool concludes that despite the Harrying of the North, being regarded with some \"shock\" in Northern England for some centuries after the event, the destruction may have been exaggerated and the number of dead not as high as previously thought.\n\nIn 1071 William appointed another Earl of Northumbria. This time it was William Walcher, a Lotharingian, who was the first non-English Bishop of Durham.\n\nHaving effectively subdued the population, William carried out a complete replacement of Anglo-Saxon leaders with Norman ones in the North. The new aristocracy in England was predominately of Norman extraction; however, one exception was that of Alan Rufus, a trusted Breton lord, who obtained in 1069–1071 a substantial fiefdom in North Yorkshire, which the Domesday Book calls \"the Hundred of the Land of Count Alan\", later known as Richmondshire.\n\nHere Alan governed, as it were, his own principality: the only location held by the King in this area was Ainderby Steeple on its eastern edge, while Robert of Mortain held one village on its southern fringe; the other Norman lords were excluded, whereas Alan retained the surviving Anglo-Danish lords or their heirs. Alan also exercised patronage in York, where he founded St Mary's Abbey in 1088. By 1086 Alan was one of the richest and most powerful men in England.\n\nIn Scotland, Malcolm married the Ætheling's sister, Margaret, in 1071. Edgar sought Malcolm's assistance in his struggle against William. The marriage of Malcolm to Edgar's sister profoundly affected the history of both England and Scotland. The influence of Margaret and her sons brought about the Anglicisation of the Lowlands and provided the Scottish king with an excuse for forays into England, which he could claim were to redress the wrongs against his brother-in-law.\n\nThe formal link between the royal house of Scotland and Wessex was a threat to William, who marched up to Scotland in 1072 to confront the Scottish king. The two kings negotiated the Treaty of Abernethy (1072), through which, according to the \"Anglo Saxon Chronicle\", Malcolm became William's vassal; among the other provisions was the expulsion of Edgar Ætheling from the Scottish court. Edgar finally submitted to William in 1074. William's hold on the crown was then theoretically uncontested.\n\nIn 1080 Walcher, the Bishop of Durham, was murdered by the local Northumbrians. In response, William sent his half-brother Odo, Bishop of Bayeux north with an army to harry the Northumbrian countryside. Odo destroyed much land north of the Tees, from York to Durham, and stole valuable items from Durham monastery. Many of the Northumbrian nobility were driven into exile.\n\nAs a result of the depopulation, Norman landowners sought settlers to work in the fields. Evidence suggests that such barons were willing to rent lands to any men not obviously disloyal. Unlike the Vikings in the centuries before, Normans did not settle wholesale in the shire, but only occupied the upper ranks of society. This allowed an Anglo-Scandinavian culture to survive beneath Norman rule. Evidence for continuity can be seen in the retention of many cultural traits:\n\nThe Normans used the church as an agent of colonisation and, post-1070, founded several monasteries in the north. There had been no monasteries north of Burton upon Trent before the harrying. Of the monasteries built, Fountains Abbey became one of the largest and richest. Along with the foundation of the northern monasteries, the Normans increased the number of motte-and-bailey castles they built there.\n\nFrom the Norman point of view, the Harrying of the North was a successful strategy, as large areas, including Cheshire, Shropshire, Derbyshire and Staffordshire were devastated, and the Domesday Book confirms this, although in those counties it was not as complete as in Yorkshire. The object of the harrying was to prevent further revolts in Mercia and Northumbria; however, it did not prevent rebellions elsewhere.\n\n\n"}
{"id": "247643", "url": "https://en.wikipedia.org/wiki?curid=247643", "title": "House arrest", "text": "House arrest\n\nIn justice and law, house arrest (also called home confinement, home detention, or, in modern times, electronic monitoring) is a measure by which a person is confined by the authorities to their residence. Only those with a house are allowed to be sentenced to arrest in their residence. Travel is usually restricted, if allowed at all. House arrest is an alternative to being in a prison while pre-trial or sentenced.\n\nWhile house arrest can be applied to criminal cases when prison does not seem an appropriate measure, the term is often applied to the use of house confinement as a measure of repression by authoritarian governments against political dissidents. In that case, typically, the person under house arrest does not have access to any means of communication. If electronic communication is allowed, conversations will most likely be monitored. With some electronic monitoring units, the conversations of prisoners can be directly monitored via the unit itself.\n\nJudges have imposed sentences of home confinement, as an alternative to prison, as far back as the 17th century. Galileo was confined to his home following his infamous trial in 1633. Political authorities have often confined leaders to house arrest who were deposed in a coup d'état, but this method was not widely used to confine numerous common criminals.\n\nThis method did not become a widespread alternative to imprisonment in the United States and other western countries until the late 20th century, when newly designed electronic monitoring devices made it inexpensive and easy to manage by corrections authorities. Although Boston was using house arrest for a variety of arrangements, the first-ever court sentence of house arrest with an electronic bracelet was in 1983.\n\nHome detention is an alternative to imprisonment; its goals are both to reduce recidivism and to decrease the number of prisoners, thereby saving money for states and other jurisdictions. It is a corrective to mandatory sentencing laws that greatly increased the incarceration rates in the United States. It allows eligible offenders to retain or seek employment, maintain family relationships and responsibilities and attend rehabilitative programs that contribute towards addressing the causes of their offending.\n\nThe terms of house arrest can differ, but most programs allow employed offenders to continue to work, and confine them to their residence only during non-working hours. Offenders are commonly allowed to leave their home for specific purposes; examples can include visits to the probation officer or police station, religious services, education, attorney visits, court appearances, and medical appointments. Many programs also allow the convict to leave their residence during regular, pre-approved times in order to carry out general household errands, such as food shopping and laundry. Offenders may have to respond to communications from a higher authority to verify that they are at home when required to be. Exceptions are often made to allow visitors to visit the offender.\n\nThe types of house arrest vary in severity according to the requirements of the court order. A curfew may restrict an offender to their house at certain times, usually during hours of darkness. \"Home confinement\" or detention requires an offender to remain at home at all times, apart from the above-mentioned exceptions. The most serious level of house arrest is \"home incarceration\", under which an offender is restricted to their residence 24 hours a day/7 days a week, except for court-approved treatment programs, court appearances, and medical appointments.\n\nIn some exceptional cases, it is possible for a person to be placed under house arrest without trial or legal representation, and subject to restrictions on their associates. In some countries this type of detention without trial has been criticized for breaching the offender's human right to a fair trial. In countries with authoritarian systems of government, the government may use such measures to stifle dissent.\n\nIn some countries, house arrest is often enforced through the use of technology products or services. One method is an electronic sensor locked around the offender's ankle (technically called an ankle monitor, also referred to as a tether). The electronic sensor transmits a GPS signal to a base handset. The base handset is connected to a police station or for-profit monitoring service.\n\nIf the offender goes too far from their home, the violation is recorded, and the police will be notified. To discourage tampering, many ankle monitors detect attempted removal. The monitoring service is often contracted out to private companies, which assign employees to electronically monitor many convicts simultaneously. If a violation occurs the unit signals the office or officer in charge immediately, depending on the severity of the violation. The officer will either call or verify the participant's whereabouts. The monitoring service notifies a convict's probation officer. The electronic surveillance together with frequent contact with their probation officer and checks by the security guards provides for a secure environment.\n\nAnother method of ensuring house arrest compliance is achieved through the use of automated calling services that require no human contact to check on the offender. Random calls are made to the residence. The respondent's answer is recorded and compared automatically to the offender's voice pattern. Authorities are notified only if the call is not answered or if the recorded answer does not match the offender's voice pattern.\n\nElectronic monitoring is considered a highly economical alternative to the cost of imprisoning offenders. In many states or jurisdictions, the convict is often required to pay for the monitoring as part of his or her sentence.\n\n\n\n\n\n\n\nThe People's Republic of China continues to use soft detention, a traditional form of house arrest used by the Chinese Empire.\n\n\n\n\n\n\nIn Italy, house arrest (in Italian \"arresti domiciliari\") is a common practice of detaining suspects, as an alternative to detention in a correctional facility, and is also commonly practiced on those felons who are close to the end of their prison terms, or for those whose health condition does not allow residence in a correctional facility, except some particular cases of extremely dangerous persons. As per article 284 of the Italian Penal Procedure Code, house arrest is imposed by a judge, who orders the suspect to stay confined in his house, home, residence, private property, or any other place of cure or assistance where he/she may be housed at the moment. When necessary, the judge may also forbid any contact between the subject and any person other than those who cohabit with him/her or who assist him/her. If the subject is unable to take care of his/her life necessities or if he/she is in conditions of absolute poverty, the judge may authorize him/her to leave his/her home for the strict necessary time to take care of said needs or to exercise a job. The prosecuting authorities and law enforcement can check at any moment whether the subject, who is \"de facto\" considered in state of detention, is complying with the order; violation of house arrest terms is immediately followed by transfer to a correctional facility. House arrests cannot be applied to a subject that has been found guilty of escape within the previous five years.\n\nNotable cases:\n\nAt sentencing, the judge may sentence an offender to home detention where they would otherwise receive a short-term prison sentence (i.e. two years or less). Home detention sentences range from 14 days and 12 months; offenders are confined to their approved residence 24 hours a day and may only leave with the permission of their probation officer.\n\nElectronic monitoring equipment is extensively used by the New Zealand Department of Corrections to ensure that convicted offenders subject to home detention remain within approved areas. This takes the form of a Global Positioning System tracker fitted to the offender's ankle and monitoring units located at their residence and place of employment. over three thousand persons were serving home detention sentences under GPS surveillance.\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "1096340", "url": "https://en.wikipedia.org/wiki?curid=1096340", "title": "Hysterotomy", "text": "Hysterotomy\n\nA hysterotomy is an incision in the uterus, and is performed during a caesarean section. Hysterotomies are also performed during fetal surgery, and various gynaecological procedures.\n\nIn fetal surgery, without inhibition of uterine contractions, premature labor is a complication that occurs in 100% of hysterotomy cases. It can be inhibited by tocolytics.\n\n"}
{"id": "27200647", "url": "https://en.wikipedia.org/wiki?curid=27200647", "title": "Involuntary euthanasia", "text": "Involuntary euthanasia\n\nInvoluntary euthanasia occurs when euthanasia is performed on a person who would be able to provide informed consent, but does not, either because they do not want to die, or because they were not asked.\n\nInvoluntary euthanasia is contrasted with voluntary euthanasia (euthanasia performed with the patient's consent) and non-voluntary euthanasia (when the patient is unable to give informed consent, for example when a patient is comatose or a child). Involuntary euthanasia is widely opposed and is regarded as a crime in all legal jurisdictions. Reference to it or fear of it is sometimes used as a reason for not changing laws relating to other forms of euthanasia.\n\nEuthanasia became a subject of public discussion in the United States at the turn of the 20th century. Felix Adler, a prominent educator and scholar, issued the first authoritative call in 1891 for the provision of lethal drugs to terminally ill patients who requested to die. In 1906, Ohio considered a law to legalize such a form of euthanasia, but it did not make it out of committee. While much of the debate focused on voluntary euthanasia, other calls for involuntary euthanasia were vocalized as well. In 1900, W. Duncan McKim, a New York physician and author published a book titled “Heredity and Human Progress.” This book suggested that people with severe inherited defects, including mentally handicapped people, epileptics, habitual drunks and criminals, should be given a quick and painless death by carbonic gas.\n\nIn January 1938, the National Society for the Legalization of Euthanasia was formed, and was renamed the Euthanasia Society of America (ESA) later that year. It advocated for the legalization of euthanasia in the United States, primarily by lobbying state legislators. Many prominent ESA members advocated for involuntary euthanasia of people with mental disabilities, including Ann Mitchell, a former asylum patient and main financial supporter of the ESA until her suicide in 1942. Ann Mitchell is also credited with structuring the ESA as a eugenics project. ESA’s first president was Charles Potter, an ex-Baptist minister who advocated for coercive eugenic sterilization and involuntary euthanasia to eliminate undesirable defective people from society.\n\nThe ESA initially advocated for both voluntary and involuntary euthanasia of people with severe disabilities. The organization soon realized that involuntary euthanasia had negative connotations, particularly its association with the Nazis' \"euthanasia\" program, and began advocating for voluntary euthanasia exclusively. The ESA continues to exist today.\n\nAdolf Hitler enacted the Aktion T4 program in October 1939 to kill \"incurably ill, physically or mentally disabled, emotionally distraught, and elderly people\". The Aktion T4 program was also designed to kill those who were deemed \"inferior and threatening to the well being of the Aryan race\". This program was also designed as part of a larger, \"Final Solution\" eugenics program. Within months of enactment, the Nazis expanded its definition of who could be killed to include those who were of a certain ethnicity as well as class. Six killing centers were established for T4, one of the most notable being the Hadamar Euthanasia Centre. At these centers, people deemed handicapped or \"unfit\" by \"medical experts\" were put to death. For example, gas chambers were disguised to look like showers and some people (particularly children) were starved to death. Often at these centers, the victims were murdered together in gas chambers using carbon monoxide. The meticulous research undertaken by the Nazis on the victims, was used as a prototype for extermination camps such as Auschwitz and Treblinka later on in the war. Approximately 200,000 people were murdered in the six years of the T4 program. The T4 \"euthanasia\" institutions were shut down by Allied troops in 1945.\n\n"}
{"id": "4855192", "url": "https://en.wikipedia.org/wiki?curid=4855192", "title": "John J. Hainkel Jr.", "text": "John J. Hainkel Jr.\n\nJohn Joseph Hainkel Jr. (March 24, 1938 – April 15, 2005), was a legislator from New Orleans, Louisiana, who died in office after thirty-seven years of service. He was the first person in his state and the second in United States history to have been elected as both Speaker of his state House of Representatives and President of his state Senate.\n\nHainkel graduated in 1956 from the Roman Catholic De La Salle High School and then Tulane University and Tulane University School of Law, all in New Orleans. In addition to being known as a raconteur, Hainkel was a trial and appellate attorney. In 1967, he was the third partner in Porteous, Toledano, Hainkel, and Johnson, then the second oldest law firm in Louisiana specializing throughout its history in insurance defense. Law partner Ben Toledano, like Hainkel, left the Democratic Party to run in his case, unsuccessfully, as a Republican for mayor of New Orleans in 1970 and the United States Senate in 1972.\n\nHainkel was first elected in 1968 as a Democrat to the Louisiana House. His service hence dated back to the second term of Governor John McKeithen. Like many other Louisiana Democrats over time, he wound up switching to the more conservative Republican Party. He originally represented a compact, affluent Uptown New Orleans House district. At the time of his death, Hainkel's legislative service had begun before ten then-serving Louisiana state legislators had been born.\n\nNew to the legislature, Hainkel joined a group of reformers who came to be known as the \"Young Turks,\" including his predecessor as Speaker, Edgerton L. \"Bubba\" Henry, a Democrat from Jonesboro in Jackson Parish. When David C. Treen became the state's first GOP governor since Reconstruction in the 1979 election, Hainkel, then a conservative Democrat, was Treen's choice to serve as Speaker of the House. He served in that post from 1980 to 1984, but he was replaced by Representative John Alario, a Westwego (Jefferson Parish) Democrat when Governor Edwin Washington Edwards won a third term in the 1983 nonpartisan blanket primary by unseating Treen.\n\nAlario switched parties for the 2011 elections in Louisiana. On October 25, 2011, Governor Bobby Jindal indicated Alario as the governor's preference for the Louisiana Senate presidency. On election by his senatorial colleagues, Alario joined Hainkel as the only persons in Louisiana and the fourth in the United States to have been presiding officers of both houses of their state legislature.\n\nIn the seventy-four legislative sessions that he attended, Hainkel was a champion of a more independent legislature. After joining the \"Young Turks\" movement of the 1970s, he became known as strong foe of Governor Edwin Edwards on gambling issues.\n\nHainkel was an opponent of the proposed Equal Rights Amendment (ERA), which was rejected in a key House committee in 1976. When the ERA failed to gain ratification after an extended deadline, Hainkel, then Speaker, addressed a gathering of the opponents held in Baton Rouge on June 30, 1982. Hainkel noted that many states \"unthinkingly\" ratified the ERA when it passed Congress early in 1972 because that \"was the thing to do\" then. Hainkel said that he objected to placing accession rights and family law subservient to federal law. \"Already in this state, credit laws have been changed, head and master changed to joint management, and through commerce and economics, we've addressed ourselves to problems. We've had increased equality on the basis of sex, but we've had it in the proper form, handled in the statutes of this state,\" Hainkel said.\n\nHainkel was elected to the Senate in 1987, 1991, 1995, 1999 and 2003. He served as that body's president during the second term of Republican former Governor Murphy J. \"Mike\" Foster Jr., from 2000 to 2004.\n\nWith his front-row seat in the chamber, Hainkel was one of the legislature's most colorful figures. He wound up his career as a \"bridge-building\" Republican who put the interests of his state ahead of party. His Senate district encompassed only a sliver of his original Uptown base but included portions of Jefferson, St. Tammany, and Tangipahoa parishes, including Hammond's Southeastern Louisiana University for which he became a legislative champion.\n\n\"A New Orleanian by birth, demeanor, appearance and conduct, he was really a good ol' boy in lifelong disguise,\" recalled then State Senator Jay Dardenne, a Baton Rouge Moderate Republican, another of Hainkel's close legislative allies.\n\nIn the words of Ed Anderson of the \"New Orleans Times-Picayune\", \"Hainkel was a legislative leader and deal-broker who could be moved to tears when recognizing an old friend or meeting a disabled child.\"\n\nAnderson continued, \"He moved at ease between the world of gentility and the tobacco-chewing country store crowd. He frequently wore madras clothes with mismatched shirts, whether he was in the halls of power or at his St. Francis of Assisi Catholic Church parish.\n\n\"He knew almost all the roadside stands, bars, and restaurants in the district. He had a flair for making a point during debate, punctuating his speeches with animated arm-flailing and near-screams. He once brought raw pork chops to the Senate floor when he went on a tear against 'pork barrel-spending' in a bill.\"\n\nAnderson added that Hainkel \"had a joie de vivre... Some viewed him as a curmudgeon, but he had a softer side and was always ready to party or enjoy a cookout, a parade, a fair, or a festival. He embraced enemies as friends after a day of legislative battle, literally -- sometimes hugging them as his hoarse laugh filled a room,\" much as Tip O'Neill sometimes told President Ronald Reagan, whom he opposed on most legislative matters, that they could have a drink together whenever it was after 6 p.m.\n\nHainkel was a chef in the Louisiana tradition but was most famous for his homemade fig icecream.\n\nHainkel's longtime law partner, William Porteous, said that Hainkel had been at a meeting near Poplarville in Pearl River County in southwestern Mississippi on April 14, 2005, with other Republican lawmakers and was found dead in his bed the next morning. \"They were at a camp. He just didn't wake up,\" Porteous explained.\n\nA coroner's report revealed that Hainkel asphyxiated. Apparently, fluid blocked his breathing passages as he slept and entered his lungs. (He did not succumb to heart failure, as originally thought.) He had spent his last evening cooking for friends, eating, and recounting political \"war stories.\"\n\n\"I couldn't picture John Hainkel going out any other way,\" said then Senator Tom Schedler, a Mandeville Republican and one of his closest friends, who later became Louisiana secretary of state.\n\nHainkel, who was divorced, was survived by three children and five grandchildren.\n\nHainkel, although a graduate of Tulane University, also took a special interest in Southeastern Louisiana University, which, as a public institution, he represented as state senator. His Senate District 6 straddled Lake Pontchartrain, causing Hainkel to quip that he represented more fish than people. Hainkel originated the Pontchartrain Cup, a trophy traded between Tulane and Southeastern on the basis of which team wins the annual baseball game.\n\nHainkel, more than anyone else, was responsible for gaining over $5 million of state funding for renovation of the Columbia Theatre for the Performing Arts in the Historic District of Hammond and the Columbia's acquisition by Southeastern Louisiana University. The operatic performance room within the spacious theatre is named Hainkel Hall, and a plaque commemorating Senator Hainkel appears prominently in the lobby.\n\nHainkel was honored in 1996 in Hammond with Southeastern Louisiana University's \"Golden Ambassador Award,\" a prestigious designation given for \"outstanding service, achievement, and/or humanitarian efforts.\"\n\nIn June 2006, a bust of Hainkel was placed in the rotunda of the Louisiana State Capitol. The Senate's briefing room in the Capitol has also been named for Hainkel. \"Gambit Weekly\" noted, \"With theater-style seating and large plasma monitors, the John Hainkel Room is among the most luxurious in the Capitol.\"\n\nShortly after Hainkel's death, Congress, on a motion from Louisiana Republican Bobby Jindal (who would later be elected governor of Louisiana), declared that Hainkel's death \"is a loss for me and for all Louisiana. He was a good friend and one of the most supportive people I know. He was always ready to lend a helping and guiding hand, whatever the situation may be.\" Later, at Jindal's request, Congress voted to rename the post office in Hammond to honor Hainkel.\n\nHainkel is also the eponym of the John J. Hainkel Jr. Home and Rehab Center, a New Orleans unit of the Louisiana Department of Health and Hospitals. Hainkel championed funding for the facility during his time in the Senate.\n\n\"The Picayune\"'s Anderson noted that Hainkel loved traditions: his birthday at a Tulane baseball game, for instance, and frequent parties for Tulane, LSU, and Southeastern sports contests, in New Orleans, Baton Rouge, or Hammond. Anderson called him a \"one-man tourism bureau\" who promoted his love of Louisiana wherever he went. \"His politicking was one-on-one: stumping the corner stores, roadside stands and restaurants of his district, shucking oysters at a school fair or tossing pumpkins in a parade.\"\n\nSenator Robert J. Barham, an Oak Ridge (Morehouse Parish) Republican, said of his fallen colleague: \"It will be surreal when you show up for the session and look for John Hainkel, and he won't be there.\"\n\n\"John Hainkel was comfortable in the package God gave him,\" said Jay Dardenne, who was elected Louisiana secretary of state in a special election on September 30, 2006, and subsequently became lieutenant governor and commissioner of administration.\n\nThe New Orleans Chapter of the Alliance for Good Government awarded him their Legislator of the Year award more than once and with their Special Award in 1983. The Associated Builders and Contractors of Louisiana named him Man of the Year in 1972; he was honored with the Louisiana/Mississippi Associated Press Margaret Dixon Award in 1980; as Alumnus of the Year of Tulane University, and as Alumnus of the Year of De La Salle High School. In 1988, Senator Hainkel was chosen Legislator of the Year by the Louisiana Restaurant Association. In 1999, Senator Hainkel was honored by the Tangipahoa Parish School Board for his leadership in providing for funding for public education.\n\nHainkel was succeeded in the Louisiana Senate by fellow Republican Julie Quinn, who won a special election runoff in July 2005 against another Republican, Diane Winston, a state representative from Covington. After the district was considerably altered in the 2011 redistricting, Quinn did not seek reelection.\n\nIn 2002, Hainkel was inducted into the Louisiana Political Museum and Hall of Fame in Winnfield.\n"}
{"id": "2855778", "url": "https://en.wikipedia.org/wiki?curid=2855778", "title": "Laura Rodríguez", "text": "Laura Rodríguez\n\nLaura Fiora Rodríguez Riccomini (1957–1992) was a Chilean political activist from the Humanist Party. In 1989 she became the world's first Humanist to win a seat in parliament, after claiming victory as part of the \"Concertación\" coalition.\n\nRodríguez was born in Santiago, Chile. She married Darío Ergas in 1978. She was active in and eventually became president of the Community for Human Development. She graduated as engineer from the University of Chile in 1983.\n\nRodríguez was co-founder of the Humanist Party in 1984 and was president of the party in 1990. She was elected to the Chilean parliament in 1989 for the La Reina-Peñalolén electoral district.\n\nRodríguez died of cancer in 1992.\n\n"}
{"id": "152442", "url": "https://en.wikipedia.org/wiki?curid=152442", "title": "List of Holocaust memorials and museums", "text": "List of Holocaust memorials and museums\n\nA number of organizations, museums and monuments are intended to serve as memorials to the Holocaust, the Nazi Final Solution, and its millions of victims. They include:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "18135834", "url": "https://en.wikipedia.org/wiki?curid=18135834", "title": "List of Mexican states by infant mortality", "text": "List of Mexican states by infant mortality\n\nThe following is the list of infant mortality by states of Mexico, it included all infants under the age of four.\n"}
{"id": "3726647", "url": "https://en.wikipedia.org/wiki?curid=3726647", "title": "List of battles with most United States military fatalities", "text": "List of battles with most United States military fatalities\n\nThe following is a list of the most lethal battles fought by the military of the United States of America. This list shows battles, sieges and offensives where the number of U.S. soldiers killed was higher than 1000. The battles that reached that number of deaths in the field are so far limited to the American Civil War, World War I, World War II, Korean War and one battle during the Vietnam War, the Tet Offensive (January 30 to September 23, 1968). The battle that resulted in the most U.S. military deaths was the Meuse-Argonne Offensive (September 26 to November 11, 1918) where 26,277 soldiers were killed fighting against the German Empire. The bloodiest single day in American history was during the Battle of Antietam when 3,654 Union and Confederate soldiers were killed on September 17, 1862.\nHowever, for the United States military specifically, the bloodiest single day is June 6, 1944.\n\nThe origins of the United States military can be traced to the Americans' fight for independence from their former colonial power, Great Britain, in the War for independence (1775–83). The three bloodiest conflicts have been American Civil War (1861–65), World War I (1917–1918) and World War II (1941–45). Other significant conflicts involving the United States ordered by casualties include,Korean War (1950–1953), Vietnam War (1964–1973), the War in Afghanistan (2001–present) and various conflicts in the Middle East. For most of its existence, America has been involved in one or another military conflict.\n\nThe term \"Casualty\" in warfare can often be confusing. It often does not refer to those that are killed on the battlefield; rather, it refers to those who can no longer fight. This can include disabled by injuries, disabled by psychological trauma, captured, deserted, or missing. A casualty is just a soldier who is no longer available for the immediate battle or campaign, the major consideration in combat; the number of casualties is simply the number of members of a unit who are not available for duty. For example, during the Seven Days Battles in the American Civil War (June 25 to July 1, 1862) there were 5,228 killed, 23,824 wounded and 7,007 missing or taken prisoner for a total of 36,059 casualties. The word \"casualty\" has been used in a military context since at least 1513. In this article the numbers killed refer to those killed in action, killed by disease or someone who died from their wounds.\n\n\nNotes\n\nReferences \n\n"}
{"id": "40333", "url": "https://en.wikipedia.org/wiki?curid=40333", "title": "Magnolia", "text": "Magnolia\n\nMagnolia is a large genus of about 210 flowering plant species in the subfamily Magnolioideae of the family Magnoliaceae. It is named after French botanist Pierre Magnol.\n\n\"Magnolia\" is an ancient genus. Appearing before bees did, the flowers are theorized to have evolved to encourage pollination by beetles. To avoid damage from pollinating beetles, the carpels of \"Magnolia\" flowers are extremely tough. Fossilised specimens of \"M. acuminata\" have been found dating to 20 million years ago, and of plants identifiably belonging to the Magnoliaceae date to 95 million years ago. Another aspect of \"Magnolia\" considered to represent an ancestral state is that the flower bud is enclosed in a bract rather than in sepals; the perianth parts are undifferentiated and called tepals rather than distinct sepals and petals. \"Magnolia\" shares the tepal characteristic with several other flowering plants near the base of the flowering plant lineage such as \"Amborella\" and \"Nymphaea\" (as well as with many more recently derived plants such as \"Lilium\").\n\nThe natural range of \"Magnolia\" species is a disjunct distribution, with a main centre in east and southeast Asia and a secondary centre in eastern North America, Central America, the West Indies, and some species in South America.\n\nAs with all Magnoliaceae, the perianth is undifferentiated, with 9–15 tepals in 3 or more whorls. The flowers are bisexual with numerous adnate carpels and stamens are arranged in a spiral fashion on the elongated receptacle. The fruit dehisces along the dorsal sutures of the carpels. The pollen is monocolpate, and the embryo development is of the Polygonum type.(Kapil 1964)(Xu and Rudall 2006)\n\nThe name \"Magnolia\" first appeared in 1703 in the \"Genera\" of Charles Plumier (1646–1704), for a flowering tree from the island of Martinique (\"talauma\"). English botanist William Sherard, who studied botany in Paris under Joseph Pitton de Tournefort, a pupil of Magnol, was most probably the first after Plumier to adopt the genus name \"Magnolia\". He was at least responsible for the taxonomic part of Johann Jacob Dillenius's \"Hortus Elthamensis\" and of Mark Catesby's \"Natural History of Carolina, Florida and the Bahama Islands\". These were the first works after Plumier's \"Genera\" that used the name \"Magnolia\", this time for some species of flowering trees from temperate North America. The species that Plumier originally named \"Magnolia\" was later described as \"Annona dodecapetala\" by Lamarck, and has since been named \"Magnolia plumieri\" and \"Talauma plumieri\" (and still a number of other names) but is now known as \"Magnolia dodecapetala\".\n\nCarl Linnaeus, who was familiar with Plumier's \"Genera\", adopted the genus name \"Magnolia\" in 1735 in his first edition of \"Systema Naturae\", without a description, but with a reference to Plumier's work. In 1753, he took up Plumier's \"Magnolia\" in the first edition of \"Species Plantarum\". There he described a monotypic genus, with the sole species being \"Magnolia virginiana\". Since Linnaeus never saw a herbarium specimen (if there ever was one) of Plumier's \"Magnolia\" and had only his description and a rather poor picture at hand, he must have taken it for the same plant which was described by Catesby in his 1730 \"Natural History of Carolina\". He placed it in the synonymy of \"Magnolia virginiana\" var. \"fœtida\", the taxon now known as \"Magnolia grandiflora\". Under \"Magnolia virginiana\" Linnaeus described five varieties (\"glauca\", \"fœtida\", \"grisea\", \"tripetala\", and \"acuminata\"). In the tenth edition of \"Systema Naturae\" (1759), he merged \"grisea\" with \"glauca\", and raised the four remaining varieties to species status.\n\nBy the end of the 18th century, botanists and plant hunters exploring Asia began to name and describe the \"Magnolia\" species from China and Japan. The first Asiatic species to be described by western botanists were \"Magnolia denudata\" and \"Magnolia liliiflora\", and \"Magnolia coco\" and \"Magnolia figo\". Soon after that, in 1794, Carl Peter Thunberg collected and described \"Magnolia obovata\" from Japan and at roughly the same time \"Magnolia kobus\" was also first collected.\n\nWith the number of species increasing, the genus was divided into the two subgenera \"Magnolia\" and \"Yulania\". \"Magnolia\" contains the American evergreen species \"M. grandiflora\", which is of horticultural importance, especially in the southeastern United States, and \"M. virginiana\", the type species. \"Yulania\" contains several deciduous Asiatic species, such as \"M. denudata\" and \"M. kobus\", which have become horticulturally important in their own right and as parents in hybrids. Classified in \"Yulania\", is also the American deciduous \"M. acuminata\" (cucumber tree), which has recently attained greater status as the parent responsible for the yellow flower colour in many new hybrids.\n\nRelations in the family Magnoliaceae have been puzzling taxonomists for a long time. Because the family is quite old and has survived many geological events (such as ice ages, mountain formation, and continental drift), its distribution has become scattered. Some species or groups of species have been isolated for a long time, while others could stay in close contact. To create divisions in the family (or even within the genus \"Magnolia\"), solely based upon morphological characters, has proven to be a nearly impossible task.\n\nBy the end of the 20th century, DNA sequencing had become available as a method of large-scale research on phylogenetic relationships. Several studies, including studies on many species in the family Magnoliaceae, were carried out to investigate relationships. What these studies all revealed was that genus \"Michelia\" and \"Magnolia\" subgenus \"Yulania\" were far more closely allied to each other than either one of them was to \"Magnolia\" subgenus \"Magnolia\". These phylogenetic studies were supported by morphological data.\n\nAs nomenclature is supposed to reflect relationships, the situation with the species names in \"Michelia\" and \"Magnolia\" subgenus \"Yulania\" was undesirable. Taxonomically, three choices are available: 1 to join \"Michelia\" and \"Yulania\" species in a common genus, not being \"Magnolia\" (for which the name \"Michelia\" has priority), 2 to raise subgenus \"Yulania\" to generic rank, leaving \"Michelia\" names and subgenus \"Magnolia\" names untouched, or 3 to join \"Michelia\" with genus \"Magnolia\" into genus \"Magnolia\" s.l. (a big genus). \"Magnolia\" subgenus \"Magnolia\" cannot be renamed because it contains \"M. virginiana\", the type species of the genus and of the family.\nNot many \"Michelia\" species have so far become horticulturally or economically important, apart for their wood. Both subgenus \"Magnolia\" and subgenus \"Yulania\" include species of major horticultural importance, and a change of name would be very undesirable for many people, especially in the horticultural branch. In Europe, \"Magnolia\" even is more or less a synonym for \"Yulania\", since most of the cultivated species on this continent have \"Magnolia (Yulania) denudata\" as one of their parents. Most taxonomists who acknowledge close relations between \"Yulania\" and \"Michelia\" therefore support the third option and join \"Michelia\" with \"Magnolia\".\n\nThe same goes, \"mutatis mutandis\", for the (former) genera \"Talauma\" and \"Dugandiodendron\", which are then placed in subgenus \"Magnolia\", and genus \"Manglietia\", which could be joined with subgenus \"Magnolia\" or may even earn the status of an extra subgenus. \"Elmerrillia\" seems to be closely related to \"Michelia\" and \"Yulania\", in which case it will most likely be treated in the same way as \"Michelia\" is now. The precise nomenclatural status of small or monospecific genera like \"Kmeria\", \"Parakmeria\", \"Pachylarnax\", \"Manglietiastrum\", \"Aromadendron\", \"Woonyoungia\", \"Alcimandra\", \"Paramichelia\" and \"Tsoongiodendron\" remains uncertain. Taxonomists who merge \"Michelia\" into \"Magnolia\" tend to merge these small genera into \"Magnolia\" s.l. as well. Botanists do not yet agree on whether to recognize a big \"Magnolia\" or the different small genera. For example, \"Flora of China\" offers two choices: a large genus \"Magnolia\" which includes about 300 species, everything in the Magnoliaceae except \"Liriodendron\" (tulip tree), or 16 different genera, some of them recently split out or re-recognized, each of which contains up to 50 species. The western co-author favors the big genus \"Magnolia\", whereas the Chinese recognize the different small genera.\n\nSpecies of Magnolia are most commonly listed under three subgenera, 12 sections, and 13 subsections, such as that used here, following the classification of the Magnolia Society. It does not represent the last word on the subclassification of the genus \"Magnolia\" (see above), as a clear consensus has not yet been reached. Each species entry follows this pattern: \"Botanical name\" - \n\nThe subdivision structure is as follows:\n\nAnthers open by splitting at the front facing the centre of the flower, deciduous or evergreen, flowers produced after the leaves.\n\n\n\n\n\n\n\n\n\n\n\n\nAnthers open by splitting at the sides, deciduous, flowers mostly produced before leaves (except \"M. acuminata\")\n\n\n\n\n\n\n\n\n\nCharles Plumier (1646–1704) described a flowering tree from the island of Martinique in his \"Genera\", giving it the name \"Magnolia\", after the French botanist Pierre Magnol.\n\nIn general, the genus \"Magnolia\" has attracted horticultural interest. Some, such as the shrub \"M. stellata\" (star magnolia) and the tree \"M.\" × \"soulangeana\" (saucer magnolia) flower quite early in the spring, before the leaves open. Others flower in late spring or early summer, including \"M. virginiana\" (sweetbay magnolia) and \"M. grandiflora\" (southern magnolia).\n\nHybridisation has been immensely successful in combining the best aspects of different species to give plants which flower at an earlier age than the parent species, as well as having more impressive flowers. One of the most popular garden magnolias, \"M\". × \"soulangeana\", is a hybrid of \"M. liliiflora\" and \"M. denudata\".\n\nIn the eastern United States, five native species are frequently in cultivation: \"M. acuminata\" (as a shade tree), \"M. grandiflora\", \"M. virginiana\", \"M. tripetala\", and \"M. macrophylla\". The last two species must be planted where high winds are not a frequent problem because of the size of their leaves.\n\nThe flowers of many species are considered edible. In parts of England, the petals of M. grandiflora are pickled and used as a spicy condiment. In some Asian cuisines, the buds are pickled and used to flavor rice and scent tea. In Japan, the young leaves and flower buds of Magnolia hypoleuca are broiled and eaten as a vegetable. Older leaves are made into a powder and used as seasoning; dried, whole leaves are placed on a charcoal brazier and filled with miso, leeks, daikon, and shiitake, and broiled. There is a type of miso which is seasoned with Magnolia, hoba miso.\n\nIn parts of Japan, the leaves of \"M. obovata\" are used for wrapping food and as cooking dishes.\n\nThe bark and flower buds of \"M. officinalis\" have long been used in traditional Chinese medicine, where they are known as \"hou po\" (厚朴). In Japan, \"kōboku\", \"M. obovata\", has been used in a similar manner.\n\nThe cucumbertree, \"M. acuminata\", grows to large size and is harvested as a timber tree in northeastern US forests. Its wood is sold as \"yellow poplar\" along with that of the tuliptree, \"Liriodendron tulipifera\". The Fraser magnolia, \"M. fraseri\", also attains enough size sometimes to be harvested, as well.\n\nMagnolias are used as food plants by the larvae of some Lepidoptera species, including the giant leopard moth.\n\nThe aromatic bark contains magnolol, honokiol, 4-O-methylhonokiol, and obovatol. Magnolol and honokiol activate the nuclear receptor peroxisome proliferator-activated receptor gamma.\n\n\n\n\n\nThe Canadian artist, Sarah Maloney, has created a series of sculptures of Magnolia flowers in bronze and steel, entitled \"First Flowers\", in which she draws our attention to the dual symbols of beginnings in the flower, as both an evolutionary archetype and also one of the first trees to flower in spring (see illustration).\n\nList of AGM magnolias\n\n"}
{"id": "261670", "url": "https://en.wikipedia.org/wiki?curid=261670", "title": "March Hare", "text": "March Hare\n\nThe March Hare (called Haigha in \"Through the Looking-Glass\") is a character most famous for appearing in the tea party scene in Lewis Carroll's \"Alice's Adventures in Wonderland\".\n\nThe main character, Alice, hypothesizes,\n\n\"Mad as a March hare\" is a common British English phrase, both now and in Carroll's time, and appears in John Heywood's collection of proverbs published in 1546. It is reported in \"The Annotated Alice\" by Martin Gardner that this proverb is based on popular belief about hares' behaviour at the beginning of the long breeding season, which lasts from February to September in Britain. Early in the season, unreceptive females often use their forelegs to repel overenthusiastic males. It used to be incorrectly believed that these bouts were between males fighting for breeding supremacy.\n\nLike the character's friend, the Hatter, the March Hare feels compelled to always behave as though it is tea-time because the Hatter supposedly \"murdered the time\" whilst singing for the Queen of Hearts. Sir John Tenniel's illustration also shows him with straw on his head, a common way to depict madness in Victorian times. The March Hare later appears at the trial for the Knave of Hearts, and for a final time as \"Haigha\" (which is pronounced to rhyme with \"mayor\", according to Carroll), the personal messenger to the White King in \"Through the Looking-Glass\" (Alice either does not recognize him as the March Hare of her earlier dream, or chooses not to comment about this).\n\nThe major departure from Carroll's original here is that instead of appearing a jittery witness, the March Hare is cast as the Prosecutor. After the charge is read, the Hare addresses the court with an opening statement that more or less vindicates the accused, before turning his accusing eye upon the court itself for failing to serve tea with the evidence (the tarts).\nIn this Japanese manga, \"Alice in the Country of Hearts\", the March Hare is Elliot March. Elliot is Blood Dupre (the Hatter)'s right-hand man. He is basically human with the exception of two brown rabbit ears. When called a rabbit, he often becomes insulted and rants about how his ears are 'just bigger than average'. He isn't specifically crazy or mad, but he is a bit violent in the beginning. He almost kills Alice with his long-barrelled gun before Blood stopped him. But, as the story progresses, it is shown that Elliot is a lovable, amusing character who is really very sweet.\nIn this Japanese manga, \"Pandora Hearts\", the March Hare is a \"Chain\" whose \"Contractor\" is Reim Lunettes. It has the ability to fake death which helps Reim escape his attackers and proved to be so realistic that his comrades believed he really was dead. The March Hare was said to be a \"gentle Chain\" which was not suited for battle, but very useful in its own ways. In a way, it contradicts all the varieties of the March Hare, as the Hare is shown to be mad or even insane.The character Reim himself is also similar to March Hare as his friend Break has the chain Mad hatter mirrors the friendship of the Hatter and the Hare's.\n\nDisney's \"Alice in Wonderland\", an animated film, depicted the March Hare at the tea party as being deliriously confused. He repeatedly offers Alice a cup of tea, but distractedly pulls the cup out of her reach or takes it from her hands just as she is about to drink. He was voiced by Jerry Colonna, after whom his appearance and personality were modelled. He was animated by Ward Kimball. Kimball also led the Dixieland band Firehouse Five Plus Two, in which he played trombone.\n\nThis version of the character was also a semi-regular on \"Bonkers\" and one of the guests in \"House of Mouse\", often seen seated with the Mad Hatter. During these appearances, the March Hare was voiced by Jesse Corti and Maurice LaMarche.\n\nThe March Hare also appears in the \"Mad T Party\" in Disney's California Adventure park. He is based on the 2010 film's Thackery Earwicket interpretation, and plays bass guitar. He is often found hopping around with Mallymkun the Dormouse on stage.\n\nThe March Hare appears in the 2010 Disney film \"Alice in Wonderland\", voiced by Paul Whitehouse. His full name is Thackery Earwicket; this, however, is not mentioned in the film. In the movie, the March Hare behaves as if constantly nerve-wracked and completely delirious. He is a cook in the film, and the way he eccentrically throws dishes and pots suggests he is an amalgam of both the March Hare and the cook from Lewis Carroll's original book. The March Hare has a strong Scottish accent in this movie, while his friend the Mad Hatter (played by Johnny Depp) switches into a Scottish accent as well whenever his emotions are strained. He is first seen in the \"Tea Party\" scene, which takes place at his \"Hare House\" windmill. Thackery hosts a tea party, which he shares with Tarrant Hightopp the Mad Hatter, Mallymkun the Dormouse, and Chess the Cheshire Cat. He appears a second time in the White Queen's kitchen, frantically cooking and throwing dishes. His third appearance is at the Frabjous Day scene, in which he stands with the other characters wielding a ladle as his weapon, nervous and somewhat ready to go to battle. Burton stated that because Whitehouse is a great comedic actor, a lot of his lines came from improvisation.\n\n"}
{"id": "4194692", "url": "https://en.wikipedia.org/wiki?curid=4194692", "title": "Microspore", "text": "Microspore\n\nMicrospores are land plant spores that develop into male gametophytes, whereas megaspores develop into female gametophytes. The male gametophyte gives rise to sperm cells, which are used for fertilization of an egg cell to form a zygote. Megaspores are structures that are part of the alternation of generations in many seedless vascular cryptogams, all gymnosperms and all angiosperms. Plants with heterosporous life cycles using microspores and megaspores arose independently in several plant groups during the Devonian period. Microspores are haploid, and are produced from diploid microsporocytes by meiosis.\n\nThe microspore has three different wall layers. The outer layer is called the perispore, the next is the exospore, and the inner layer is the endospore. The perispore is the thickest of the three layers while the exospore and endospore are relatively equal in width.\n\nIn heterosporous seedless vascular plants, modified leaves called microsporophylls bear microsporangia containing many microsporocytes that undergo meiosis, each producing four microspores. Each microspore may develop into a male gametophyte consisting of a somewhat spherical antheridium within the microspore wall. Either 128 or 256 sperm cells with flagella are produced in each antheridium. The only heterosporous ferns are aquatic or semi-aquatic, including the genera \"Marsilea\",\"Regnellidium\", \"Pilularia\", \"Salvinia\", and \"Azolla\". Heterospory is also known in the lycopod genus \"Selaginella\" and in the quillwort genus \"Isoëtes\".\n\nTypes of seedless vascular plants:\n\nIn seed plants the microspores develop into pollen grains each containing a reduced, multicellular male gametophyte. The megaspores, in turn, develop into reduced female gametophytes that produce egg cells that, once fertilized, develop into seeds. Pollen cones or microstrobili usually develop toward the tips of the lower branches in clusters up to 50 or more. The microsporangia of gymnosperms develop in pairs toward the bases of the scales, which are therefore called microsporophylls. Each of the microsporocytes in the microsporangia undergoes meiosis, producing four haploid microspores. These develop into pollen grains, each consisting of four cells and a pair of external air sacs. The air sacs give the pollen grains added buoyancy that helps with wind dispersal.\n\nTypes of Gymnosperms:\n\nAs the anther of a flowering plant develops, four patches of tissue differentiate from the main mass of cells. These patches of tissue contain many diploid microsporocyte cells, each of which undergoes meiosis producing a quartet of microspores. Four chambers (pollen sacs) lined with nutritive tapetal cells are visible by the time the microspores are produced. \nAfter meiosis, the haploid microspores undergo several changes:\nThese steps occur in sequence and when complete, the microspores have become pollen grains.\n\nAlthough it is not the usual route of a microspore, this process is the most effective way of yielding haploid and double haploid plants through the use of male sex hormones. Under certain stressors such as heat or starvation, plants select for microspore embryogenesis. It was found that over 250 different species of angiosperms responded this way. In the anther, after a microspore undergoes microsporogenesis, it can deviate towards embryogenesis and become star-like microspores. The microspore can then go one of four ways: Become an embryogenic microspore, undergo callogenesis to organogenesis (haploid/double haploid plant), become a pollen-like structure or die.\n\n"}
{"id": "19009", "url": "https://en.wikipedia.org/wiki?curid=19009", "title": "Milgram experiment", "text": "Milgram experiment\n\nThe Milgram experiment on obedience to authority figures was a series of social psychology experiments conducted by Yale University psychologist Stanley Milgram. They measured the willingness of study participants, men from a diverse range of occupations with varying levels of education, to obey an authority figure who instructed them to perform acts conflicting with their personal conscience. Participants were led to believe that they were assisting an unrelated experiment, in which they had to administer electric shocks to a \"learner.\" These fake electric shocks gradually increased to levels that would have been fatal had they been real.\n\nThe experiment found, unexpectedly, that a very high proportion of men would fully obey the instructions, albeit reluctantly. Milgram first described his research in a 1963 article in the \"Journal of Abnormal and Social Psychology\" and later discussed his findings in greater depth in his 1974 book, \".\"\n\nThe experiments began in July 1961, in the basement of Linsly-Chittenden Hall at Yale University, three months after the start of the trial of German Nazi war criminal Adolf Eichmann in Jerusalem. Milgram devised his psychological study to answer the popular contemporary question: \"Could it be that Eichmann and his million accomplices in the Holocaust were just following orders? Could we call them all accomplices?\" The experiment was repeated many times around the globe, with fairly consistent results.\n\nThree individuals took part in each session of the experiment:\nThe subject and the actor arrived at the session together. The experimenter told them that they were taking part in \"a scientific study of memory and learning\", to see what the effect of punishment is on a subject's ability to memorize content. Also, he always clarified that the payment for their participation in the experiment was secured regardless of its development. The subject and actor drew slips of paper to determine their roles. Unknown to the subject, both slips said \"teacher\". The actor would always claim to have drawn the slip that read \"learner\", thus guaranteeing that the subject would always be the \"teacher\".\n\nNext, the teacher and learner were taken into an adjacent room where the learner was strapped into what appeared to be an electric chair. The experimenter told the participants this was to ensure that the learner would not escape. In a later variation of the experiment, the confederate was sure to mention to the participant that he had a heart condition. At some point prior to the actual test, the teacher was given a sample electric shock from the electroshock generator in order to experience firsthand what the shock that the learner would supposedly receive during the experiment would feel like.\n\nThe teacher and learner were then separated such that they could communicate, but not see each other. The teacher was then given a list of word pairs that he was to teach the learner. The teacher began by reading the list of word pairs to the learner. The teacher would then read the first word of each pair and read four possible answers. The learner would press a button to indicate his response. If the answer was incorrect, the teacher would administer a shock to the learner, with the voltage increasing in 15-volt increments for each wrong answer. If correct, the teacher would read the next word pair.\n\nThe subjects believed that for each wrong answer, the learner was receiving actual shocks. In reality, there were no shocks. After the learner was separated from the teacher, the learner set up a tape recorder integrated with the electroshock generator, which played prerecorded sounds for each shock level. As the voltage of the fake shocks increased, the learner began making audible protests, such as banging repeatedly on the wall that separated him from the teacher. When the highest voltages were reached, the learner fell silent.\n\nIf at any time the teacher indicated a desire to halt the experiment, the experimenter was instructed to give specific verbal prods. The prods were, in this order:\n\nIf the subject still wished to stop after all four successive verbal prods, the experiment was halted. Otherwise, it was halted after the subject had given the maximum 450-volt shock three times in succession.\n\nThe experimenter also had prods to use if the teacher made specific comments. If the teacher asked whether the learner might suffer permanent physical harm, the experimenter replied, \"Although the shocks may be painful, there is no permanent tissue damage, so please go on.\" If the teacher said that the learner clearly wants to stop, the experimenter replied, \"Whether the learner likes it or not, you must go on until he has learned all the word pairs correctly, so please go on.\"\n\nBefore conducting the experiment, Milgram polled fourteen Yale University senior-year psychology majors to predict the behavior of 100 hypothetical teachers. All of the poll respondents believed that only a very small fraction of teachers (the range was from zero to 3 out of 100, with an average of 1.2) would be prepared to inflict the maximum voltage. Milgram also informally polled his colleagues and found that they, too, believed very few subjects would progress beyond a very strong shock. He also reached out to honorary Harvard University graduate Chaim Homnick, who noted that this experiment would not be concrete evidence of the Nazis' innocence, due to fact that \"poor people are more likely to cooperate.\" Milgram also polled forty psychiatrists from a medical school, and they believed that by the tenth shock, when the victim demands to be free, most subjects would stop the experiment. They predicted that by the 300-volt shock, when the victim refuses to answer, only 3.73 percent of the subjects would still continue and, they believed that \"only a little over one-tenth of one percent of the subjects would administer the highest shock on the board.\"\n\nIn Milgram's first set of experiments, 65 percent (26 of 40) of experiment participants administered the experiment's final massive 450-volt shock, and all administered shocks of at least 300 volts. Subjects were uncomfortable doing so, and displayed varying degrees of tension and stress. These signs included sweating, trembling, stuttering, biting their lips, groaning, digging their fingernails into their skin, and some were even having nervous laughing fits or seizures. Every participant paused the experiment at least once to question it. Most continued after being assured by the experimenter. Some said they would refund the money they were paid for participating.\n\nMilgram summarized the experiment in his 1974 article, \"The Perils of Obedience\", writing:\n\nThe original Simulated Shock Generator and Event Recorder, or \"shock box\", is located in the Archives of the History of American Psychology.\n\nLater, Milgram and other psychologists performed variations of the experiment throughout the world, with similar results. Milgram later investigated the effect of the experiment's locale on obedience levels by holding an experiment in an unregistered, backstreet office in a bustling city, as opposed to at Yale, a respectable university. The level of obedience, \"although somewhat reduced, was not significantly lower.\" What made more of a difference was the proximity of the \"learner\" and the experimenter. There were also variations tested involving groups.\n\nThomas Blass of the University of Maryland, Baltimore County performed a meta-analysis on the results of repeated performances of the experiment. He found that while the percentage of participants who are prepared to inflict fatal voltages ranged from 28% to 91%, there was no significant trend over time and the average percentage for US studies (61%) was close to the one for non-US studies (66%).\n\nThe participants who refused to administer the final shocks neither insisted that the experiment be terminated, nor left the room to check the health of the victim without requesting permission to leave, as per Milgram's notes and recollections, when fellow psychologist Philip Zimbardo asked him about that point.\n\nMilgram created a documentary film titled \"Obedience\" showing the experiment and its results. He also produced a series of five social psychology films, some of which dealt with his experiments.\n\nThe Milgram Shock Experiment raised questions about the research ethics of scientific experimentation because of the extreme emotional stress and inflicted insight suffered by the participants. Some critics such as Gina Perry argued that participants were not properly debriefed. In Milgram's defense, 84 percent of former participants surveyed later said they were \"glad\" or \"very glad\" to have participated; 15 percent chose neutral responses (92% of all former participants responding). Many later wrote expressing thanks. Milgram repeatedly received offers of assistance and requests to join his staff from former participants. Six years later (at the height of the Vietnam War), one of the participants in the experiment sent correspondence to Milgram, explaining why he was glad to have participated despite the stress:\n\nIn his book \"\", Milgram argued that the ethical criticism provoked by his experiments was because his findings were disturbing and revealed unwelcome truths about human nature. Others have argued that the ethical debate has diverted attention from more serious problems with the experiment's methodology.\n\nMilgram sparked direct critical response in the scientific community by claiming that \"a common psychological process is centrally involved in both [his laboratory experiments and Nazi Germany] events.\" James Waller, Chair of Holocaust and Genocide Studies at Keene State College, formerly Chair of Whitworth College Psychology Department, expressed the opinion that Milgram experiments \"do not correspond well\" to the Holocaust events:\n\nIn the opinion of Thomas Blass—who is the author of a scholarly monograph on the experiment (\"The Man Who Shocked The World\") published in 2004—the historical evidence pertaining to actions of the Holocaust perpetrators speaks louder than words:\n\nIn a 2004 issue of the journal \"Jewish Currents\", Joseph Dimow, a participant in the 1961 experiment at Yale University, wrote about his early withdrawal as a \"teacher\", suspicious \"that the whole experiment was designed to see if ordinary Americans would obey immoral orders, as many Germans had done during the Nazi period.\"\n\nIn 2012 Australian psychologist Gina Perry investigated Milgram's data and writings and concluded that Milgram had manipulated the results, and that there was \"troubling mismatch between (published) descriptions of the experiment and evidence of what actually transpired.\" She wrote that \"only half of the people who undertook the experiment fully believed it was real and of those, 66% disobeyed the experimenter\". She described her findings as \"an unexpected outcome\" that \"leaves social psychology in a difficult situation.\"\n\nMilgram elaborated two theories:\n\nIn his book \"Irrational Exuberance\", Yale finance professor Robert J. Shiller argues that other factors might be partially able to explain the Milgram Experiments:\n\nIn a 2006 experiment, a computerized avatar was used in place of the learner receiving electrical shocks. Although the participants administering the shocks were aware that the learner was unreal, the experimenters reported that participants responded to the situation physiologically \"as if it were real\".\n\nAnother explanation of Milgram's results invokes belief perseverance as the underlying cause. What \"people cannot be counted on is to realize that a seemingly benevolent authority is in fact malevolent, even when they are faced with overwhelming evidence which suggests that this authority is indeed malevolent. Hence, the underlying cause for the subjects' striking conduct could well be conceptual, and not the alleged 'capacity of man to abandon his humanity ... as he merges his unique personality into larger institutional structures.\"'\n\nThis last explanation receives some support from a 2009 episode of the BBC science documentary series \"Horizon\", which involved replication of the Milgram experiment. Of the twelve participants, only three refused to continue to the end of the experiment. Speaking during the episode, social psychologist Clifford Stott discussed the influence that the idealism of scientific inquiry had on the volunteers. He remarked: \"The influence is ideological. It's about what they believe science to be, that science is a positive product, it produces beneficial findings and knowledge to society that are helpful for society. So there's that sense of science is providing some kind of system for good.\"\n\nBuilding on the importance of idealism, some recent researchers suggest the 'engaged followership' perspective. Based on an examination of Milgram's archive, in a recent study, social psychologists Alexander Haslam, Stephen Reicher and Megan Birney, at the University of Queensland, discovered that people are less likely to follow the prods of an experimental leader when the prod resembles an order. However, when the prod stresses the importance of the experiment for science (i.e. 'The experiment requires you to continue'), people are more likely to obey. The researchers suggest the perspective of 'engaged followership': that people are not simply obeying the orders of a leader, but instead are willing to continue the experiment because of their desire to support the scientific goals of the leader and because of a lack of identification with the learner. Also a neuroscientific study supports this perspective, namely watching the learner receive electric shocks, does not activate brain regions involving empathic concerns.\n\nIn \"\" (1974), Milgram describes nineteen variations of his experiment, some of which had not been previously reported.\n\nSeveral experiments varied the distance between the participant (teacher) and the learner. Generally, when the participant was physically closer to the learner, the participant's compliance decreased. In the variation where the learner's physical immediacy was closest, where the participant had to hold the learner's arm onto a shock plate, 30 percent of participants completed the experiment. The participant's compliance also decreased if the experimenter was physically further away (Experiments 1–4). For example, in Experiment 2, where participants received telephonic instructions from the experimenter, compliance decreased to 21 percent. Some participants deceived the experimenter by \"pretending\" to continue the experiment.\n\nIn Experiment 8, an all-female contingent was used; previously, all participants had been men. Obedience did not significantly differ, though the women communicated experiencing higher levels of stress.\n\nExperiment 10 took place in a modest office in Bridgeport, Connecticut, purporting to be the commercial entity \"Research Associates of Bridgeport\" without apparent connection to Yale University, to eliminate the university's prestige as a possible factor influencing the participants' behavior. In those conditions, obedience dropped to 47.5 percent, though the difference was not statistically significant.\n\nMilgram also combined the effect of authority with that of conformity. In those experiments, the participant was joined by one or two additional \"teachers\" (also actors, like the \"learner\"). The behavior of the participants' peers strongly affected the results. In Experiment 17, when two additional teachers refused to comply, only 4 of 40 participants continued in the experiment. In Experiment 18, the participant performed a subsidiary task (reading the questions via microphone or recording the learner's answers) with another \"teacher\" who complied fully. In that variation, 37 of 40 continued with the experiment.\n\nAround the time of the release of \"Obedience to Authority\" in 1973–1974, a version of the experiment was conducted at La Trobe University in Australia. As reported by Perry in her 2012 book \"Behind the Shock Machine\", some of the participants experienced long-lasting psychological effects, possibly due to the lack of proper debriefing by the experimenter.\n\nIn 2002, the British artist Rod Dickinson created \"The Milgram Re-enactment\", an exact reconstruction of parts of the original experiment, including the uniforms, lighting, and rooms used. An audience watched the four-hour performance through one-way glass windows. A video of this performance was first shown at the CCA Gallery in Glasgow in 2002.\n\nA partial replication of the experiment was staged by British illusionist Derren Brown and broadcast on UK's Channel 4 in \"The Heist \"(2006).\n\nAnother partial replication of the experiment was conducted by Jerry M. Burger in 2006 and broadcast on the Primetime series \"Basic Instincts\". Burger noted that \"current standards for the ethical treatment of participants clearly place Milgram's studies out of bounds.\" In 2009, Burger was able to receive approval from the institutional review board by modifying several of the experimental protocols. Burger found obedience rates virtually identical to those reported by Milgram in 1961–62, even while meeting current ethical regulations of informing participants. In addition, half the replication participants were female, and their rate of obedience was virtually identical to that of the male participants. Burger also included a condition in which participants first saw another participant refuse to continue. However, participants in this condition obeyed at the same rate as participants in the base condition.\n\nIn the 2010 French documentary \"Le Jeu de la Mort\" (\"The Game of Death\"), researchers recreated the Milgram experiment with an added critique of reality television by presenting the scenario as a game show pilot. Volunteers were given €40 and told they would not win any money from the game, as this was only a trial. Only 16 of 80 \"contestants\" (teachers) chose to end the game before delivering the highest-voltage punishment.\n\nThe experiment was performed on \"Dateline NBC\" on an episode airing April 25, 2010.\n\nThe Discovery Channel aired the \"How Evil are You\" segment of \"Curiosity\" on October 30, 2011. The episode was hosted by Eli Roth, who produced results similar to the original Milgram experiment, though the highest-voltage punishment used was 165 volts, rather than 450 volts.\n\nDue to increasingly widespread knowledge of the experiment, recent replications of the procedure have had to ensure that participants were not previously aware of it.\n\nCharles Sheridan and Richard King (at the University of Missouri and the University of California, Berkeley, respectively) hypothesized that some of Milgram's subjects may have suspected that the victim was faking, so they repeated the experiment with a real victim: a \"cute, fluffy puppy\" who was given real, albeit apparently harmless, electric shocks. Their findings were similar to those of Milgram: half of the male subjects and all of the females obeyed throughout. Many subjects showed high levels of distress during the experiment, and some openly wept. In addition, Sheridan and King found that the duration for which the shock button was pressed decreased as the shocks got higher, meaning that for higher shock levels, subjects were more hesitant.\n\n\n\n"}
{"id": "355559", "url": "https://en.wikipedia.org/wiki?curid=355559", "title": "Mobbing", "text": "Mobbing\n\nMobbing, as a sociological term, means bullying of an individual by a group, in any context, such as a family, peer group, school, workplace, neighborhood, community, or online.\n\nWhen it occurs as emotional abuse in the workplace, such as ‘ganging up’ by co-workers, subordinates or superiors, to force someone out of the workplace through rumor, innuendo, intimidation, humiliation, discrediting, and isolation, it is also referred to as malicious, nonsexual, nonracial / racial, general harassment.\n\nKonrad Lorenz, in his book entitled \"On Aggression\" (1966), first described mobbing among birds and animals, attributing it to instincts rooted in the Darwinian struggle to thrive (see animal mobbing behavior). In his view, most humans are subject to similar innate impulses but capable of bringing them under rational control. Lorenz's explanation for his choice of the English word Mobbing was omitted in the English translation by Marjorie Kerr Wilson. According to Kenneth Westhues, Lorenz chose the word mobbing because he remembered in the collective attack by birds, the old German term \"hassen auf\", which means \"to hate after\" or \"to put a hate on\" was applied and this emphasised \"the depth of antipathy with which the attack is made\" rather than the English word mobbing which emphasised the collective aspect of the attack. \n\nIn the 1970s, the Swedish physician applied Lorenz's conceptualization to the collective aggression of children against a targeted child.\n\nIn the 1980s, professor and practising psychologist Heinz Leymann applied the term to ganging up in the workplace. \n\nIn 2011, anthropologist Janice Harper published an essay in The Huffington Post suggesting that some of the anti-bully approaches effectively constitute a form of mobbing by using the label \"bully\" to dehumanize, encouraging people to shun and avoid people labeled bullies, and in some cases, sabotage their work or refuse to work with them, while almost always calling for their exclusion and termination from employment.\n\nJanice Harper followed her Huffington Post essay with a series of essays in both The Huffington Post and in her column, Beyond Bullying: Peacebuilding at Work, School and Home in Psychology Today that argued that mobbing is a form of group aggression innate to primates, and that those who engage in mobbing are not necessarily \"evil\" or \"psychopathic,\" but responding in a predictable and patterned manner when someone in a position of leadership or influence communicates to the group that someone must go. For that reason, she indicated that anyone can and will engage in mobbing, and that once mobbing gets underway, just as in the animal kingdom it will almost always continue and intensify as long as the target remains with the group. She subsequently published a book on the topic in which she explored animal behavior, organizational cultures and historical forms of group aggression, suggesting that mobbing is a form of group aggression on a continuum of structural violence with genocide as the most extreme form of mob aggression.\n\nBritish anti-bullying researchers Andrea Adams and Tim Field have used the expression \"workplace bullying\" instead of what Leymann called \"mobbing\" in a workplace context. They identify mobbing as a particular type of bullying that is not as apparent as most, defining it as \"an emotional assault. It begins when an individual becomes the target of disrespectful and harmful behavior. Through innuendo, rumors, and public discrediting, a hostile environment is created in which one individual gathers others to willingly, or unwillingly, participate in continuous malevolent actions to force a person out of the workplace.\"\n\nAdams and Field believe that mobbing is typically found in work environments that have poorly organised production or working methods and incapable or inattentive management and that mobbing victims are usually \"exceptional individuals who demonstrated intelligence, competence, creativity, integrity, accomplishment and dedication\".\n\nIn contrast, Janice Harper suggests that workplace mobbing is typically found in organizations where there is limited opportunity for employees to exit, whether through tenure systems or contracts that make it difficult to terminate an employee (such as universities or unionized organizations), and/or where finding comparable work in the same community makes it difficult for the employee to voluntarily leave (such as academic positions, religious institutions, or military). In these employments, efforts to eliminate the worker will intensify to push the worker out against his or her will through shunning, sabotage, false accusations and a series of investigations and poor reviews. Another form of employment where workers are mobbed are those that require the use of uniforms or other markers of group inclusion (law enforcement, fire fighting, military), organizations where a single gender has predominated, but the other gender is beginning to enter (STEM fields, fire fighting, military, nursing, teaching, and construction). Finally, she suggests that organizations where there are limited opportunities for advancement can be prone to mobbing because those who do advance are more likely to view challenges to their leadership as threats to their precarious positions. Harper further challenges the idea that workers are targeted for their exceptional competence. In some cases, she suggests, exceptional workers are mobbed because they are viewed as threatening to someone, but some workers who are mobbed are not necessarily good workers. Rather, Harper contends, some mobbing targets are outcasts or unproductive workers who cannot easily be terminated, and are thus treated inhumanely to push them out. While Harper emphasizes the cruelty and damaging consequences of mobbing, her organizational analysis focuses on the structural, rather than moral, nature of the organization. Moreover, she views the behavior itself, which she terms workplace aggression, as grounded in group psychology, rather than individual psychosis—even when the mobbing is initiated due to a leader's personal psychosis, the dynamics of group aggression will transform the leader's bullying into group mobbing—two vastly distinct psychological and social phenomena.\n\nShallcross, Ramsay and Barker consider workplace \"mobbing\" to be a generally unfamiliar term in some English speaking countries. Some researchers claim that mobbing is simply another name for bullying. Workplace mobbing can be considered as a \"virus\" or a \"cancer\" that spreads throughout the workplace via gossip, rumour and unfounded accusations. It is a deliberate attempt to force a person out of their workplace by humiliation, general harassment, emotional abuse and/or terror. Mobbing can be described as being \"ganged up on.\" Mobbing is executed by a leader (who can be a manager, a co-worker, or a subordinate). The leader then rallies others into a systematic and frequent \"mob-like\" behaviour toward the victim.\n\nMobbing as \"downward bullying\" by superiors is also known as \"bossing\", and \"upward bullying\" by colleagues as \"staffing\", in some European countries, for instance, in German-speaking regions.\n\nVictims of workplace mobbing frequently suffer from: adjustment disorders, somatic symptoms, psychological trauma (e.g., trauma tremors or sudden onset selective mutism), post-traumatic stress disorder (PTSD), and major depression.\n\nIn mobbing targets with PTSD, Leymann notes that the \"mental effects were fully comparable with PTSD from war or prison camp experiences.\" Some patients may develop alcoholism or other substance abuse disorders. Family relationships routinely suffer. Workplace targets and witnesses may even develop brief psychotic episodes occupational psychosis generally with paranoid symptoms. Leymann estimated that 15% of suicides in Sweden could be directly attributed to workplace mobbing.\n\nFollowing on from the work of Heinemann, Elliot identifies mobbing as a common phenomenon in the form of group bullying at school. It involves 'ganging up' on someone using tactics of rumor, innuendo, discrediting, isolating, intimidating, and above all, making it look as if the targeted person is responsible (victim blaming).\n\nKenneth Westhues' study of mobbing in academia found that vulnerability was increased by personal differences such as being a foreigner or of a different sex; by working in fields such as music or literature which have recently come under the sway of less objective and more post-modern scholarship; financial pressure; or having an aggressive superior. Other factors included envy, heresy and campus politics.\n\nSociologists and authors have created checklists and other tools to identify mobbing behaviour. Common approaches to assessing mobbing behavior is through quantifying frequency of mobbing behavior based off a given definition of the behavior or through quantifying what respondents believe encompasses mobbing behavior. These are referred to as \"self-labeling\" and \"behavior experience\" methods respectively.\n\nLimitations of some mobbing examination tools are:\n\nCommon Tools used to measure mobbing behavior are:\n\nThe Waterloo Anti-Mobbing Instruments (WAMI) created by Kenneth Westhues contains a 16 item checklist to identify the signs of mobbing.\n\nFrom an organizational perspective, it has been suggested that mobbing behavior can be curtailed by acknowledging behaviors as mobbing behaviors and that such behaviors result in harm and/or negative consequences. Precise definitions of such traits are critical due to ambiguity of unacceptable and acceptable behaviors potentially leading to unintentional mobbing behavior. Attenuation of mobbing behavior can further be enhanced by developing policies that explicitly address specific behaviors that are culturally accepted to result in harm or negative affect. This provides a framework from which mobbing victims can respond to mobbing. Lack of such a framework may result in a situation where each instance of mobbing is treated on an individual basis with no recourse of prevention. It may also indicate that such behaviors are warranted and within the realm of acceptable behavior within an organization. Direct responses to grievances related to mobbing that are handled outside of a courtroom and training programs outlining antibully-countermeasures also demonstrate a reduction in mobbing behavior.\n\n"}
{"id": "49062", "url": "https://en.wikipedia.org/wiki?curid=49062", "title": "Moral absolutism", "text": "Moral absolutism\n\nMoral absolutism is an ethical view that all actions are intrinsically right or wrong. Stealing, for instance, might be considered to be always immoral, even if done for the well-being of others (e.g., stealing food to feed a starving family), and even if it does in the end promote such a good. Moral absolutism stands in contrast to other categories of normative ethical theories such as consequentialism, which holds that the morality (in the wide sense) of an act depends on the consequences or the context of the act.\n\nMoral absolutism is not the same as moral universalism. Universalism holds merely that what is right or wrong is independent of custom or opinion (as opposed to moral relativism), but not necessarily that what is right or wrong is independent of context or consequences (as in absolutism). Moral universalism is compatible with moral absolutism, but also positions such as consequentialism. Louis Pojman gives the following definitions to distinguish the two positions of moral absolutism and universalism:\n\nEthical theories which place strong emphasis on rights and duty, such as the deontological ethics of Immanuel Kant, are often forms of moral absolutism, as are many religious moral codes.\n\nMoral absolutism may be understood in a strictly secular context, as in many forms of deontological moral rationalism. However, many religions have morally absolutist positions as well, regarding their system of morality as deriving from divine commands. Therefore, they regard such a moral system as absolute, (usually) perfect, and unchangeable. Many secular philosophies also take a morally absolutist stance, arguing that absolute laws of morality are inherent in the nature of human beings, the nature of life in general, or the universe itself. For example, someone who believes absolutely in nonviolence considers it wrong to use violence even in self-defense.\n\nCatholic philosopher Thomas Aquinas never explicitly addresses the Euthyphro dilemma, but draws a distinction between what is good or evil in itself and what is good or evil because of God's commands, with unchangeable moral standards forming the bulk of natural law. Thus he contends that not even God can change the Ten Commandments, adding, however, that God \"can\" change what individuals deserve in particular cases, in what might look like special dispensations to murder or steal.\n"}
{"id": "2570610", "url": "https://en.wikipedia.org/wiki?curid=2570610", "title": "Morris Michael Edelstein", "text": "Morris Michael Edelstein\n\nMorris Michael Edelstein (February 5, 1888 – June 4, 1941) was a Polish-born Congressional Representative from the state of New York. Edelstein was born in Meseritz (Międzyrzec Podlaski), Poland, and at three years of age immigrated to the United States with his parents, who settled in New York City. He attended public schools and Cooper Union College in New York. He graduated from the Brooklyn Law School of St. Lawrence University, in 1909, and was admitted to the bar in 1910 and practiced law in New York. Edelstein was elected as a Democrat to the Seventy-sixth Congress to fill the vacancy caused by the death of William I. Sirovich. He was reelected to the Seventy-seventh Congress and served from February 6, 1940, until his death on June 4, 1941, in the cloakroom of the House of Representatives, Washington, DC, after completing the delivery of a speech on the floor of the House.\n\nEdelstein's last speech was a response to Mississippi Representative John Elliott Rankin, widely described as an anti-Semite who advocated peace with Nazi Germany. Rankin had just delivered a House floor speech accusing \"international Jewish brethren\" of trying to drag America into World War II.\n\nIn response, Edelstein, who was Jewish said: \"Hitler started out by speaking about 'Jewish brethren.' It is becoming the play and the work of those people who want to demagogue to speak about their 'Jewish brethren' and 'international bankers.' ... I deplore the idea that ... men in this House ... attempt to use the Jews as their scapegoat. I say it is unfair and I say it is un-American. ... All men are created equal, regardless of race, creed or color, and whether a man be Jew or Gentile, he may think what he deems fit.\" Edelstein then walked out of the House. He collapsed and died shortly afterwards in the House cloakroom.\n\nHe is buried in Mount Zion Cemetery, Maspeth, New York.\n\nThe SS \"M. Michael Edelstein\", a World War II liberty ship, was named in his honor.\n\n"}
{"id": "1056580", "url": "https://en.wikipedia.org/wiki?curid=1056580", "title": "Morton Shulman", "text": "Morton Shulman\n\nMorton Shulman, OC (25 April 1925 – 18 August 2000) was a Canadian politician, businessman, broadcaster, columnist, coroner, and physician. He was born in Toronto, Ontario in 1925 to a Jewish family. He first came to fame as Ontario's Chief Coroner in the early 1960s. During this period, he also became a very successful stock-market player, and authored a bestselling book about how to make money in the stock market. In the mid-1960s he embarrassed the provincial government when he found them to be disobeying provincial health and safety laws. He was fired and then ran for elected office in the Legislative Assembly of Ontario, avenging himself by beating a government Member of Provincial Parliament (MPP). He completed two terms as the High Park electoral district's MPP, and did not run in the 1975 Ontario general election. His fame grew in the late 1970s and 1980s when he hosted a nationally distributed television talk show called \"The Shulman File\". He was diagnosed with Parkinson's disease in the early 1980s and became a pharmaceutical entrepreneur specializing in treatments for that disease. Near the end of his life, he received recognition for his lifetime's work, when he was appointed to the Order of Canada, the country's highest civilian award. He died in Toronto in the year 2000.\n\nBorn in Toronto, Ontario, Shulman received his M.D. from the University of Toronto in 1948. Shulman practised throughout his professional life with a general practice on Roncesvalles Avenue in Toronto. He became wealthy through investing in the stock market and wrote a bestselling book, \"Anyone Can Make a Million\" in 1966. He was married to Gloria Shulman (née Bossin) and they had two children, environmental lawyer Dianne Saxe and Dr. Geoff Shulman.\n\nIn exchange for his involvement in the Ontario Progressive Conservative Party, he was appointed Ontario's chief coroner in 1961. In 1963, he was named Chief Coroner of the Municipality of Metropolitan Toronto. Shulman was outspoken and used his position as coroner to crusade on a number of issues such as enacting tougher regulations on lifejackets for small boats, having government regulate car safety, the introduction of breathalysers into Ontario, and against then-restrictive abortion laws after he investigated the deaths of women who had died while trying to terminate their pregnancies. In other crusades, he helped to force surgeons to count instruments before and after surgery, and construction companies to provide better bracing in trenches. His years as a coroner became the inspiration for the Canadian television drama \"Wojeck\".\n\nAfter embarrassing the Progressive Conservative (Tory) provincial government, by revealing its inaction in enforcing the fire code in a recently built hospital, he was fired, in 1967, as Metropolitan Toronto's Chief Coroner. He decided to avenge himself by running for a seat in the Legislative Assembly of Ontario. Despite ideological differences, he ran for the Ontario New Democratic Party (NDP) in the High Park electoral district, where his medical clinic was located. Despite his strong capitalist beliefs, he decided to run for the democratic socialist party because they gave him a free hand in choosing where to run, and because their views in support of public safety were compatible with his. He was elected as a Member of Provincial Parliament (MPP) in the 1967 provincial election, by defeating High Park's incumbent MPP, Progressive Conservative Alfred Hozack Cowling by over 6200 votes.\n\nHe used his position in the legislature to become a thorn in the side of the Tory governments led by John Robarts and Bill Davis. He asked provocative questions in the legislature and was known for stunts, such as selling the book \"The Happy Hooker\" out of his office after it had been banned by the Toronto Police morality squad — he offered MPPs a 10 percent discount. Once, to make a point about lax security, he carried a pellet gun — dressed up to look like a submachine gun — in a bag through an Ontario nuclear plant, and then pulled it out on the floor of the Legislature. He waved it around happily while cabinet ministers sitting across from him hid under their desks. He wrote the book \"MPP\" to describe his experiences, one of at least three autobiographies that he wrote.\nShulman demanded that he be appointed Attorney General if the NDP ever won government. NDP leader Stephen Lewis refused to commit to making such a promise. After clashing with his colleagues in the NDP — particularly Lewis — Shulman decided to leave the legislature and did not run in the 1975 election.\n\nAfter leaving politics, he started a broadcasting career, most notably from 1977 until 1983, he hosted a hard-hitting television talk show on CITY-TV called \"The Shulman File\" which featured confrontational interviews, sensationalist and risqué topics and outrageous opinions. The show was spoofed by \"SCTV\" as \"Murray's File\". At the same time, he began writing a regular column in the \"Toronto Sun\" which continued into the 1990s. During this period, he became more involved in the financial community, heading up a mutual fund and pursuing various business interests.\n\nShulman was diagnosed with Parkinson's disease in 1983, and formed a company, Deprenyl Research Ltd. (which became Draxis Health Inc.), in order to acquire Canadian rights to the anti-Parkinson's drug Deprenyl. His company engaged in a long fight with the federal government for approval of the drug for sale in Canada. He also started a second pharmaceutical company, called DUSA, now run by his son.\n\nIn 1993, Shulman was awarded the Order of Canada, and was invested as an Officer of the Order on 6 January 1994. After battling Parkinson's disease for more than 17 years, he finally succumbed to the complications arising from that disease at the Baycrest Centre for Geriatric Care in Toronto on August 18, 2000. He was buried in Pardes Shalom Cemetery on Dufferin Street two days later. In 2013, a street leading to Ontario's new forensic services and coroner’s complex, in Toronto's North York area, was named Morton Shulman Avenue.\n\n"}
{"id": "44465764", "url": "https://en.wikipedia.org/wiki?curid=44465764", "title": "Murder of Sheree Beasley", "text": "Murder of Sheree Beasley\n\nSheree Joy Beasley (25 February 1985 – 29 June 1991) was an Australian schoolgirl from Rosebud, an outer suburb of Melbourne, Victoria.\n\nSix year-old Sheree was kidnapped, raped, and murdered by Robert Arthur Selby Lowe in June 1991.\nHer body was found weeks later on 24 September in a stormwater drain.\n\nOne of the leading items that led to Lowe's incarceration was based on patient care sessions that were recorded by police investigators and later by his psychotherapist.\n\nOn the 24th anniversary of her murder, a memorial service was held at her grave, which many members of the community attended.\n\nSheree rode her bicycle to a nearby milk bar, where she was abducted by Robert Lowe, a Sunday school teacher, church elder and travelling salesman.\n\nLowe had apparently targeted Sheree because he had seen her alone on several previous occasions. A possible explanation for the lack of supervision was that Sheree had been transferred several times between the custody of her mother and the custody of her maternal and paternal grandparents.\n\nAfter the abduction, several witnesses said that they had seen a middle-aged man driving a car containing a \"distressed child\".\n\nLowe had a history of crimes involving children. Before Sheree's murder, he had had multiple offences for indecent exposure, which had been aimed at young girls.\n\nMonths after the murder, he was seeing a psychotherapist because he was having marital problems. His therapist, Margaret Hobbs, eventually began to suspect that he was involved in the murder of Sheree. Lowe had given suspicious statements, saying that he did not remember where he was on the day Sheree died and that he felt police were closing in on him.\n\nThe police had interviewed Lowe after the abduction, and they later tape-recorded some of his sessions with Hobbs (initially without her knowledge). After being informed of the recording, Hobbs gave her permission for more taping, as she was disturbed by Lowe's statements.\n\nThose representing Lowe during his trial objected to the manner in which recorded evidence was obtained from his therapy sessions with Hobbs. They asserted that the recordings were a violation of a confidentiality policy. The court dismissed this and came to the consensus that the evidence was appropriate for the protection of the public.\n\nHobbs stated that Lowe had discussed several suspicious details related to Sheree's murder. Such statements were concerned with a desire to \"build an alibi\" and with the consequences of pleading guilty to manslaughter. Lowe eventually stated in April 1992 that he had given Sheree a ride in his car and had manually strangled her. During his trial, he admitted his guilt. He said that he had \"choked the girl\". Lowe was subsequently convicted of kidnapping and murder, and was sentenced to life imprisonment plus 15 years without parole.\n\nAfter Lowe was sent to prison for murdering Sheree, he stated that he was innocent. In August 2014, he wrote that he believed law enforcement was using him as a scapegoat because of their failure to find those who were responsible for the crime. The police and the family of the victim did not believe this, and said they were \"sickened\" by his assertions.\n\nAfter Lowe's conviction, his psychotherapist Margaret Hobbs began writing a book based on her experience. She died in a vehicle accident in 1996; her book was later completed and published by Andrew Rule.\n\nLowe is believed to have been involved with a large amount of child pornography that had been smuggled into the prison in which he was incarcerated. It is believed that persons who were visiting inmates were the source of this material.\n\n\n\n"}
{"id": "4397556", "url": "https://en.wikipedia.org/wiki?curid=4397556", "title": "Ooze (Dungeons &amp; Dragons)", "text": "Ooze (Dungeons &amp; Dragons)\n\nIn the \"Dungeons & Dragons\" fantasy role-playing game, an ooze is a type of creature. This category includes such monsters as slimes (such as green slime within the world of the game), jellies, deadly puddings, and similar mindless, amorphous blobs. They can be used by Dungeon Masters as enemies of the player characters.\n\nMany oozes dwell underground, and most secrete an acid from their skin that dissolves flesh and other materials rapidly.\n\nOozes are essentially blind, but more than make up for that with an ability called \"blindsight\", which allows them to discern nearby objects and creatures without needing to see them visually.\n\nThe black pudding, the gelatinous cube, the gray ooze, the green slime, and the ochre jelly first appeared in the original \"Dungeons & Dragons\" set (1974). The slithering tracker first appeared in \"The Strategic Review\" #5 (December 1975).\n\n\"Advanced Dungeons & Dragons\" contains a number of ooze-like creatures. The \"Monster Manual\" contains the black pudding, gelatinous cube, gray ooze, green slime, ochre jelly, and slithering tracker, as well as the unique demon lord Juiblex, The stunjelly first appeared in the original \"Fiend Folio\" (1981). The crystal ooze, the deadly puddings (the brown pudding, the dun pudding, and the white pudding) and the olive slime and slime creature first appeared in the adventure module \"The Lost Caverns of Tsojcanth\" (1982), and reprinted in the original \"Monster Manual II\" (1983) with the mustard jelly.\n\nThe gelatinous cube, the gray ooze, the green slime, and the ochre jelly appeared in the \"D&D Basic Set\" (1977, 1981, 1983), and the black pudding appeared in the \"D&D Basic Set\" (1977) and \"D&D Expert Set\" (1981, 1983). The creatures all appeared in the \"Dungeons & Dragons Rules Cyclopedia\" (1991).\n\nThe 1994 release of \"The Classic Dungeons & Dragons Game\" contained a number of oozes, although at that time they were not defined as such by a creature type or keyword. The book contained the black pudding, gelatinous cube, gray ooze, green slime, and ochre jelly.\n\nThe crystal ooze, the deadly puddings (the black pudding, the brown pudding, the dun pudding, and the white pudding), the gelatinous cube, the gray ooze, the green slime, the ochre jelly appeared in second edition \"Advanced Dungeons & Dragons\" in \"Monstrous Compendium Volume One\" (1989). The slithering tracker appeared in \"Monstrous Compendium Volume Two\" (1989). The mustard jelly, the olive slime and slime creature, and the stunjelly appeared for the Greyhawk setting in the adventure module \"Greyhawk Ruins\" (1990). All of these creatures were reprinted in the \"Monstrous Manual\" (1993) under the \"ooze/slime/jelly\" heading, except for the deadly puddings which appeared under their own heading.\n\nIn both the 3rd and 3.5 editions of \"Dungeons & Dragons\", ooze is a creature type. The black pudding, the gelatinous cube, the gray ooze, and the ochre jelly appeared in the third edition \"Monster Manual\" (2000) under the \"ooze\" entry, and the version 3.5 \"Monster Manual\" (2003). The green slime appeared in the third edition \"Dungeon Master's Guide\" (2000) as a dungeon hazard, and again in the 3.5 revised \"Dungeon Master's Guide\" (2003). The \"Monster Manual II\" (2002) contains the bone ooze, flesh jelly, reason stealer, and teratomorph. The \"Monster Manual III\" (2004) includes the arcane ooze, living spells (also in the \"Eberron Campaign Setting\"), snowflake ooze, and summoning ooze. The white pudding appears in \"Frostburn\" (2004). The bloodfire ooze appeared in \"Monster Manual IV\" (2006), and the graveyard sludge appeared in \"Monster Manual V\" (2007).\n\nIn 4th edition, \"ooze\" is a keyword, rather than a creature type. The 4th edition \"Monster Manual\" contains the ochre jelly and gelatinous cube. The black pudding, grey ooze, and green slime appeared in \"Monster Manual 2\" (2009).\n\n\nThe Necromancer Games supplement, \"The Tome of Horrors\" (2002), contains the crystal ooze, diger, mercury ooze, mustard jelly, slithering tracker, and stunjelly.\n\n\n"}
{"id": "7997813", "url": "https://en.wikipedia.org/wiki?curid=7997813", "title": "Patient diary", "text": "Patient diary\n\nA patient diary is a tool used during a clinical trial or a disease treatment to assess the patient's condition (e.g. symptom severity, quality of life) or to measure treatment compliance. An electronic patient diary registers the data in a storage device and allows for automatically monitoring the time the entry was made.\n\nFrequent recording of symptoms using a diary helps to reduce recall bias. Electronic diaries ensure entries are made as scheduled, and not, for example, in a batch immediately before the clinic visit. \n\nPatient diaries are also way to find out if a patient takes the medication according to the treatment schedule, which is an important problem during clinical trials and the treatment of degenerative diseases with relatively few symptoms.\n\n\n"}
{"id": "1488258", "url": "https://en.wikipedia.org/wiki?curid=1488258", "title": "Philip Barton Key II", "text": "Philip Barton Key II\n\nPhilip Barton Key (April 5, 1818 – February 27, 1859) was an American lawyer who served as U.S. Attorney for the District of Columbia. He is most famous for his public affair with Teresa Bagioli Sickles, and his eventual murder at the hands of her husband, Congressman Daniel Sickles of New York. Sickles defended himself by adopting a defense of temporary insanity, the first time the defense had been used in the United States.\n\n Born in Georgetown, D.C., Key was the son of Francis Scott Key and the great-nephew of Philip Barton Key. He was also a nephew of Chief Justice Roger B. Taney. He married Ellen Swan, the daughter of a Baltimore attorney, on November 18, 1845. Allegedly the handsomest man in Washington and by 1859 a widower with four children, Key was known to be flirtatious with many women.\nSome time in the spring of 1858, Teresa Sickles began an affair with Key. Dan Sickles, though a serial adulterer himself, had accused his much-younger wife of adultery several times during their five-year marriage, but she had repeatedly denied it to his satisfaction. But then Sickles received a poison pen letter informing him of his wife's affair with Key. He confronted his wife, who confessed to the affair. Sickles then made his wife write out her confession on paper. \n\nSickles saw Key sitting on a bench outside the Sickles home on February 27, 1859, signalling to Teresa, and confronted him. Sickles rushed outside into Lafayette Square, cried \"Key, you scoundrel, you have dishonored my home; you must die\", and with a pistol repeatedly shot the unarmed Key. \n\nKey was taken into the nearby Benjamin Ogle Tayloe House, where he died some time later.\n\nSickles was acquitted on the basis of temporary insanity, a crime of passion, in one of the most controversial trials of the 19th century. It was the first successful use of the defense in the United States. One of Sickles' attorneys, Edwin Stanton, later became the Secretary of War. Newspapers declared Sickles a hero for \"saving\" women from Key. Years later, while attending the theater in New York City, Sickles became aware of the presence of Key's son James Key in the audience; both men watched each other throughout the performance. Nothing happened.\n\nKey is buried in his son-in-law's family plot in Westminster Hall and Burying Ground in Baltimore.\n\nHis first cousin once removed, also named Philip Barton Key, Jr., left Annapolis in 1835 to become a planter and state legislator in Louisiana.\n\n"}
{"id": "384028", "url": "https://en.wikipedia.org/wiki?curid=384028", "title": "Pontianak (folklore)", "text": "Pontianak (folklore)\n\nThe pontianak (Dutch-Indonesian spelling: \"boentianak\", Jawi: ڤونتيانق) is a female vampiric ghost in Malay mythology. It is also known as a matianak or kuntilanak, sometimes shortened to kunti. Pontianak is called Churel, or Churayl, in Bangladesh, India, and Pakistan. The pontianak are said to be the spirits of women who died while pregnant. This is despite the fact that the earliest recordings of pontianaks in Malay lore describe the ghost as originating from a stillborn child. This is often confused with a related creature, the lang suir, which is the ghost of a woman who died while giving birth. \n\nThe word \"pontianak\" is reportedly a corruption of the Malay \"perempuan mati beranak\", or “woman who died in childbirth”. Another theory is that the word is a combination of \"puan\" (woman) + \"mati\" (die) + \"anak\" (child). The term \"matianak\" means \"death of a child\". The city of Pontianak in Indonesia is named after this wicked creature, which was a ghosts' nest until Syarif Abdurrahman Alkadrie and his Army fought and extruded ghosts who attacked his group by shooting cannon balls.\n\nPontianaks are usually depicted as pale-skinned women with long black hair, red eyes, and white dress smeared in blood, but they are said to be able to take on a beautiful humanly appearance since they prey on men and helpless people. They can also be beasts due to their bloodthirsty and carnivorous nature.\n\nIn folklore, a pontianak usually arises at full moon and announces her presence through high-pitched baby cries. If the cry is soft, it means that the pontianak is near, and if it is loud, then she must be far. Some believe that if ones hears a dog howling at night, that means the pontianak is far, but if a dog is whining, that means the pontianak is nearby. Her presence can sometimes be detected by a nice floral fragrance identifiable as that of the plumeria, followed by an awful stench (resembling that of a decaying body) afterwards. The Indian version, the Churail, can be identified by her feet turning backwards just before her transformation into her vampiric form. \n\nA pontianak kills her victims by digging into their stomach with her sharp fingernails and devouring their body organs. In some cases where the pontianak desires revenge against a male individual, the beast rips out the body organs with her hands. It is said that if one has his or eyes open when a pontianak is near, she will suck them out of the victim's head. Pontianak locates her preys/victims by sniffing out the hanging laundry outside. For this reason, some Malaysians refuse to leave any piece of clothing outside of their house overnight.\n\nThe pontianak is associated with banana trees, and her spirit is said to reside in them during the day.\n\nTo fend off a pontianak, a nail should be plunged into the hole on the nape of her neck. This is said to make her turn into a beautiful woman and a good wife until the nail is removed. In the case of the kuntilanak, the nail is plunged into the apex of her head.\n\nThe Indonesian kuntilanak is similar to the pontianak, but commonly takes the form of a bird and sucks the blood of virgins and young women. The bird, which makes a \"ke-ke-ke\" sound as it flies, may be sent through some black magic to make a woman fell sick, the characteristic symptom is vaginal bleeding. In her female form, when a man approaches her, she suddenly turns and reveals that her back is hollow, but this apparition is more specifically referred to sundel bolong.\n\nThere are numerous sightings of the Pontianak/Lang suir all over South East Asia, particularly in Malaysia and Indonesia. In August 2010 there was a video caught by a group of Malaysian Policeman PDRM in the town of Bentong, Pahang, Malaysia. The 2-minute-long video does not show the apparition of the Pontianak at all in her full form.\n\n\nIn Philippine folklore, the vampiric tiyanak shares many similarities in terms of origin with the pontianak. However, the tiyanak is the ghost of the child rather than the mother. \n\n\n"}
{"id": "23242345", "url": "https://en.wikipedia.org/wiki?curid=23242345", "title": "Puerperal disorder", "text": "Puerperal disorder\n\nA puerperal disorder or postpartum disorder is a disorder which presents primarily during the puerperium, or postpartum period. The postpartum period can be divided into three distinct stages; the initial or acute phase, 6–12 hours after childbirth; subacute postpartum period, which lasts 2–6 weeks, and the delayed postpartum period, which can last up to 6 months. In the subacute postpartum period, 87% to 94% of women report at least one health problem. Long term health problems (persisting after the delayed postpartum period) are reported by 31% of women.\n\nThe World Health Organization (WHO) describes the postpartum period as the most critical and yet the most neglected phase in the lives of mothers and babies; most maternal and/or newborn deaths occur during the postpartum period.\n\nDiastasis recti is a gap between the two sides of the rectus abdominis muscle that can occur in the antenatal and postnatal periods. This condition has no associated morbidity or mortality. Treatment is physiotherapy. \n\nPrimary postpartum haemorrhage is blood loss following childbirth of more than 500ml (minor) or 1000ml (major) Secondary postpartum haemorrhage is abnormal or excessive bleeding after 24 hours and before 12 weeks postnatally.\n\nUrinary incontinence and fecal incontinence have been linked to all methods of childbirth, with the incidence of urinary incontinence at 6 months postpartum being 3-7% and fecal incontinence 1-3%.\n\nPostpartum infections, also known as childbed fever and puerperal fever, are any bacterial infections of the female reproductive tract following childbirth or miscarriage. Signs and symptoms usually include a fever greater than 38.0 °C (100.4 °F), chills, lower abdominal pain, and possibly bad-smelling vaginal discharge. It usually occurs after the first 24 hours and within the first ten days following delivery.\n\nPuerperal mastitis is inflammation of the breast usually associated with breastfeeding. Symptoms typically include local pain and redness. There is often an associated fever and general soreness. Onset is typically fairly rapid and usually occurs within the first few months of delivery. Complications can include abscess formation.\n\nObstetric fistula is a medical condition in which a hole develops in the birth canal as a result of childbirth, typically after a prolonged obstructed labour and is preventable with timely access to Cesarean section. The fistula can occur between the vagina and rectum, ureter, or bladder. It can result in incontinence of urine or feces. \n\nPerineal tearing is the spontaneous (unintended) tearing of the skin and other soft tissue structures which, in women, separate the vagina from the anus. Perineal tearing occurs in 85% of vaginal deliveries. At 6 months postpartum, 21% of women still report perineal pain and 11-49% report sexual problems or painful intercourse.\n\nPeripartum cardiomyopathy is decrease in heart function which occurs in the last month of pregnancy, or up to 6 months post-pregnancy. It increases the risk of congestive heart failure, heart arrhythmias, thromboembolism, and cardiac arrest.\n\nPostpartum thyroiditis is a phenomenon observed following pregnancy and may involve hyperthyroidism, hypothyroidism or the two sequentially. It affects about 5% of all women within a year after giving birth.\n\nPelvic organ prolapse occurs when the uterus, bladder and/or rectum drop lower in the pelvis creating a bulge in the vagina. Approximately half of all women who have given birth experience some degree of pelvic organ prolapse, most frequently as they age and go through menopause.\n\nPostpartum depression is a moderate to severe depressive episode starting anytime during pregnancy or within the four weeks following delivery. It occurs in 4-20% of pregnancies, depending on its definition. Without treatment, postpartum depression can last for months or years. In addition to affecting the mother’s health, it can interfere with her ability to connect with and care for her baby and may cause the baby to have problems with sleeping, eating, and behavior as he or she grows. In 38% of the cases of postpartum depression, women are still depressed 3 years postpartum. In 0.2% of pregnancies, postpartum depression leads to postpartum psychosis.\n\nResearch shows that symptoms of Posttraumatic stress disorder are common following childbirth, with prevalence of 24-30.1% at 6 weeks, dropping to 13.6% at 6 months. PTSD is rarer; a review found that following normal childbirth (excluding stillbirth and some other complications) rates of PTSD ranged from 2.8-5.6% after 6 weeks, dropping to 1.5% at 6 months.\n\n"}
{"id": "18783051", "url": "https://en.wikipedia.org/wiki?curid=18783051", "title": "Quaternary extinction event", "text": "Quaternary extinction event\n\nThe Quaternary period (from 2.588 ± 0.005 million years ago to the present) saw the extinctions of numerous predominantly megafaunal species, which resulted in a collapse in faunal density and diversity and the extinction of key ecological strata across the globe. The most prominent event in the Late Pleistocene is differentiated from previous Quaternary pulse extinctions by the widespread absence of ecological succession to replace these extinct species, and the regime shift of previously established faunal relationships and habitats as a consequence. \n\nThe earliest casualties were incurred at 130,000 BCE (the start of the Late Pleistocene). However, the great majority of extinctions in Afro-Eurasia and the Americas occurred during the transition from the Pleistocene to the Holocene epoch (13,000 BCE to 8,000 BCE). This extinction wave did not stop at the end of the Pleistocene, continuing, especially on isolated islands, in human-caused extinctions, although there is debate as to whether these should be considered separate events or part of the same event. \n\nAmong the main causes hypothesized by paleontologists are overkill by the widespread appearance of humans and natural climate change. A notable modern human presence first appeared during the Middle Pleistocene in Africa, and started to establish continuous, permanent populations in Eurasia and Australasia from 120,000 BCE and 63,000 BCE respectively, and the Americas from 22,000 BCE. \n\nA variant of the former possibility is the second-order predation hypothesis, which focuses more on the indirect damage caused by overcompetition with nonhuman predators. Recent studies have tended to favor the human-overkill theory.\n\nThe Late Pleistocene extinction event saw the extinction of many mammals weighing more than 40 kg. The proportional rate of megafauna extinctions is consecutively larger the greater the human migratory distance from Africa.\nThe extinctions in the Americas entailed the elimination of all the larger (over 1000 kg) mammalian species of South American origin, including those that had migrated north in the Great American Interchange. Only in the continents of Australia, North America, and South America did the extinction occur at family taxonomic levels or higher.\n\nThe proportional rate of megafauna extinctions being incrementally bigger the larger the migratory distance from Africa might be related to non-African megafauna and \"Homo sapiens\" not having evolved as species alongside each other.\n\nFor their part, Australia, North America and South America, which respectively had the highest incremental extinction rates, had no known native species of Hominoidea (apes) at all, and specifically no species of Hominidae (greater apes) or Homo.\nThe increased rate of extinction mirrors the sequential pattern of the migration of anatomically modern humans. The further away from Africa, the more recently the area has been inhabited by humans, and the less time the environments (including its megafauna) had had to become accustomed to humans and vice versa.\n\nThere is no evidence of megafaunal extinctions at the height of the Last Glacial Maximum, indicating that increasing cold and glaciation were not factors. There are three main hypotheses concerning the Pleistocene extinction:\nThere are some inconsistencies between the current available data and the prehistoric overkill hypothesis. For instance, there are ambiguities around the timing of sudden extinctions of Australian megafauna. Biologists note that comparable extinctions have not occurred in Africa and South or Southeast Asia, where the fauna evolved with hominids. Post-glacial megafaunal extinctions in Africa have been spaced over a longer interval.\n\nEvidence supporting the prehistoric overkill hypothesis includes the persistence of certain island megafauna for several millennia past the disappearance of their continental cousins. Ground sloths survived on the Antilles long after North and South American ground sloths were extinct. The later disappearance of the island species correlates with the later colonization of these islands by humans. Similarly, woolly mammoths died out on remote Wrangel Island 1,000 years after their extinction on the mainland. Steller's sea cows also persisted in seas off the isolated and uninhabited Commander Islands for thousands of years after they had vanished from the continental shores of the north Pacific.\n\nAlternative hypotheses to the theory of human responsibility include climate change associated with the last glacial period and the Younger Dryas event, as well as Tollmann's hypothetical bolide, which claim that the extinctions resulted from bolide impact(s). Such a scenario has been proposed as a contributing cause of the 1,300-year cold period known as the Younger Dryas stadial. This impact extinction hypothesis is still in debate due to the exacting field techniques required to extract minuscule particles of extraterrestrial impact markers such as iridium at a high resolution from very thin strata in a repeatable fashion, as is necessary to conclusively distinguish the event peak from the local background level of the marker. The debate seems to be exacerbated by infighting between the Uniformitarianism camp and the Catastrophism camp.\n\nThe Old World tropics were relatively spared the Late Pleistocene extinctions. Sub-Saharan Africa and southern Asia are the only regions that have terrestrial mammals weighing over 1000 kg today. However, there are indications of a megafaunal extinction events throughout the Pleistocene, particularly in Africa two million years ago, which coincide with key stages of human evolution and climatic trends. The centre of human evolution and expansion, Africa and Asia were inhabited by advanced hominids by 2mya, with \"Homo habilis\" in Africa, and \"Homo erectus\" in both continents. By the advent and proliferation of \"Homo sapiens\" circa 298,000 BCE, dominant species included \"Homo heidelbergensis\" in Africa, and (their descendants) Denisovans and Neanderthals with \"Homo erectus\" in Asia. There is evidence of an early migration event 268,000 BCE within Neanderthal genetics, however the earliest dating for modern human inhabitation is 118,000 BCE in Arabia, China and Israel, and 71,000 BCE in Indonesia. Additionally, not only have these early Asian migrations left a genetic mark on modern Papuan populations, the oldest known pottery in existence was found in China, dated to 18,000 BCE. Particularly during the late Pleistocene, megafaunal diversity was notably reduced from both these continents, often without being replaced by comparable successor fauna. Climate change has been explored as a prominent cause of extinctions in Southeast Asia.\n\nMegafauna that disappeared in Africa or Asia during the Early and Middle Pleistocene include:\n\n\nIn Sahul (a former continent composed of Australia and New Guinea), the sudden and extensive spate of extinctions occurred earlier than in the rest of the world. Most evidence points to a 20,000 year period after human arrival circa 63,000 BCE, but scientific argument continues as to the exact date range. In the rest of the Pacific (other Australasian islands such as New Caledonia, and Oceania) although in some respects far later, endemic fauna also usually perished quickly upon the arrival of humans in the late Pleistocene and early Holocene. This section does not include any spate of extinctions post 1000 BCE (e.g. subatlantic New Zealand or Hawaii).\n\nThe extinctions in the Pacific included:\n\nSome extinct megafauna, such as the bunyip-like \"Diprotodon\", may remain in folk memory or be the sources of cryptozoological legends.\n\nThis geography spans the entirety of the European continent, and stretches into Northern Asia, through the Caucasus and Central Asia to Northern China, Siberia and Beringia. During the Late Pleistocene, this region was noted for its great diversity and dynamism of biomes, including the warm climes of the Mediterranean basin, open temperate woodlands, arid plains, mountainous heathland and swampy wetlands, all of which were vulnerable to the severe climatic fluctuations of the interchanges between glacial and interglacials periods (stadials). However, it was the expansive mammoth steppe which was the ecosystem which united and defined this region during the Late Pleistocene. One of the key features of Europe's Late Pleistocene climate was the often drastic turnover of conditions and biota between the numerous stadials, which could set within a century. For example, during glacial periods, the entire North Sea was drained of water to form Doggerland. The final major cold spell occurred from 25,000 BCE to 18,000 BCE, and is known as the Last Glacial Maximum, when the Fenno-Scandinavian ice sheet covered much of northern Europe, while the Alpine ice sheet occupied significant parts of central-southern Europe.\n\nEurope, and in particular northern Eurasia, being far colder and drier than today, was largely hegemonized by the mammoth steppe, an ecosystem dominated by palatable high-productivity grasses, herbs and willow shrubs. This supported an extensive biota of grassland fauna, and stretched eastwards from Spain in the Iberian Peninsula to the Yukon in modern-day Canada. The area was populated by many species of grazers which assembled in large herds similar in size to those in Africa today. Populous species which roamed the great grasslands included the woolly mammoth, woolly rhinoceros, \"Elasmotherium\", steppe bison, Pleistocene horse, muskox, \"Cervalces\", reindeer, antelope (\"Parabubalis, Procapra, Saiga, Spirocerus\") and steppe pika. Carnivores included cave lion, \"Homotherium\", cave hyena, grey wolf, dhole, and the arctic fox.\n\nAt the edges of these large stretches of grassland could be found more shrub-like terrain and dry conifer forests and woodland (akin to forest steppe or taiga). The browsing collective of megafauna included woolly rhinoceros, Giant deer, moose, \"Cervalces\", tarpan, aurochs, woodland bison, camels and smaller deer (\"Capreolus\", \"Cervus\", \"Moschus\"). Brown bears, wolverines, cave bear, wolves, lynx, leopards, and red foxes also inhabited this biome. Tigers were at stages also present, from the edges of Eastern Europe around the Black Sea to Beringia. The more mountainous terrain, incorporating montane grasslands, subalpine conifer forests, alpine tundra and broken, craggy slopes, was occupied by several species of mountain-going animals like argali, chamois, ibex, mouflon, pika, wolves, leopards, \"Ursus sp.\" and lynx, with snow leopards, Baikal yak and snow sheep in Northern Asia. Arctic tundra, which lined the north of the mammoth steppe, reflected modern ecology with species such as the polar bear, wolf, reindeer and muskox.\n\nOther biomes, although less noted, were significant in contributing to the diversity of fauna in Late Pleistocene Europe. Warmer grasslands such as temperate steppes and Mediterranean savannahs hosted \"Stephanorhinus,\" gazelle, European bison, Asian ostriches, \"Leptobos\", cheetahs, and onager. These biomes also contained an assortment of mammoth steppe fauna, such as saiga antelope, lions, \"Homotherium\", cave hyenas, wolves, Pleistocene horse, steppe bison, \"Spirocerus,\" aurochs, and camels. Temperate coniferous, deciduous, mixed broadleaf and Mediterranean forests and open woodlands accommodated straight-tusked elephants, \"Praemegaceros\", \"Stephanorhinus\", wild boar, bovids such as European bison, tahr and tur, species of \"Ursus\" such as the Etruscan bear, and smaller deer (\"Capreolus,\" \"Cervus,\" \"Dama, Haploidoceros\") with several mammoth steppe species, such as lynx, tarpan, wolves, dholes, moose, Giant deer, woodland bison, leopards, and aurochs. Woolly rhinoceros and mammoth occasionally resided in these temperate biomes, mixing with predominately temperate fauna to escape harsh glacials. In warmer wetlands, European water buffalo and hippopotamus were present. Although these habitats were restricted to micro refugia and to Southern Europe and its fringes, being in Iberia, Italy, the Balkans, Ukraine's Black Sea basin, the Caucasus, and Western Asia, during inter-glacials these biomes had a far more northernly range. For example, hippopotamus inhabited Great Britain and straight-tusked elephant the Netherlands, as recently as 80,000 BCE and 42,000 BCE respectively.\n\nThe first possible indications of habitation by hominins are the 7.2 million year old finds of \"Graecopithecus\", and 5.7 million year old footprints in Crete — however established habitation is noted in Georgia from 1.8 million years ago, proceeded to Germany and France, by \"Homo erectus\". Prominent co-current and subsequent species include \"Homo antecessor\", \"Homo cepranensis\", \"Homo heidelbergensis\", Neanderthals and Denisovans, preceding human habitation circa 38,000 BCE.\n\nEurope's Late Pleistocene biota went through two phases of extinction. Some fauna became extinct before 13,000 BCE, in staggered intervals, particularly between 50,000 BCE and 30,000 BCE. Species include cave bear, \"Elasmotherium\", straight-tusked elephant, \"Stephanorhinus\", water buffalo, Neanderthals, gazelle, and \"Homotherium\". However, the great majority of species were extinguished, extirpated or experienced severe population contractions between 13,000 BCE and 9,000 BCE, ending with the Younger Dryas. At that time there were small ice sheets in Scotland and Scandinavia. The mammoth steppe disappeared from the vast majority of its former range, either due to a permanent shift in climatic conditions, or an absence of ecosystem management due to decimated, fragmented or extinct populations of megaherbivores. This led to a region wide extinction vortex, resulting in cyclically diminishing bio-productivity and defaunation. Insular species on Mediterranean islands such as Sardinia, Sicily, Malta, Cyprus and Crete, went extinct around the same time as humans colonised those islands. Fauna included dwarf elephantids, megacerines and hippopotamuses, and giant avians, otters and rodents.\n\nMany species extant today were present in areas either far to the south or west of their contemporary ranges- for example, all the arctic fauna on this list inhabited regions as south as the Iberian Peninsula at various stages of the Late Pleistocene. Recently extinct organisms are noted as †. Species extirpated from significant portions of or all former ranges in Europe and Northern Eurasia during the Quaternary extinction event include-\n\n\"See also:\" \"List of North American animals extinct in the Holocene\"\n\nDuring the last 60,000 years, including the end of the last glacial period, approximately 51 genera of large mammals have become extinct in North America. Of these, many genera extinctions can be reliably attributed to a brief interval of 11,500 to 10,000 radiocarbon years before present, shortly following the arrival of the Clovis people in North America. Prominent paleontological sites include Mexico, [https://www.uv.mx/personal/tcarmona/files/2010/08/Arroyo-et-al-2008.pdf <nowiki>[1]</nowiki>] and Panama, the crossroads of the American Interchange.[https://azueroearthproject.org/wp-content/uploads/2012/11/A.B4000_Pearson_2005_eng.pdf <nowiki>[2]</nowiki>] Most other extinctions are poorly constrained in time, though some definitely occurred outside of this narrow interval. In contrast, only about half a dozen small mammals disappeared during this time. Previous North American extinction pulses had occurred at the end of glaciations, but not with such an ecological imbalance between large mammals and small ones. (Moreover, previous extinction pulses were not comparable to the Quaternary extinction event; they involved primarily species replacements within ecological niches, while the latter event resulted in many ecological niches being left unoccupied). Such include the last native North American terror bird (\"Titanis\"), rhinoceros (\"Aphelops\") and hyena (\"Chasmaporthetes\"). Human habitation commenced unequivacolly approximately 22,000 BCE north of the glacier, and 13,500 BCE south, however disputed evidence of southern human habitation exists from 130,000 BCE and 17,000 BCE onwards, described from sites in California and Meadowcroft in Pennsylvania. The megafaunal extinctions include forty one genera of herbivores (H), and twenty carnivores (C). North American extinctions included:\nThe survivors are in some ways as significant as the losses: bison (H), grey wolf (C), lynx (C), grizzly bear (C), American black bear (C), deer (e.g. caribou, moose, waipiti (elk), \"Odocoileus sp.\") (H), pronghorn (H), white-lipped peccary (H), muskox (H), bighorn sheep (H), and mountain goat (H); the list of survivors also include species which were extirpated during the Quaternary extinction event, but recolonised at least part of their ranges during the mid-holocene from South American relict populations, such as the cougar (C), jaguar (C), giant anteater (C), collared peccary (H), ocelot (C), margay (C), and jaguarundi (C). All save the pronghorns and giant anteaters were descended from Asian ancestors that had evolved with human predators. Pronghorns are the second fastest land mammal (after the cheetah), which may have helped them elude hunters. More difficult to explain in the context of overkill is the survival of bison, since these animals first appeared in North America less than 240,000 years ago and so were geographically removed from human predators for a sizeable period of time. Because ancient bison evolved into living bison, there was no continent-wide extinction of bison at the end of the Pleistocene (although the genus was regionally extirpated in many areas). The survival of bison into the Holocene and recent times is therefore inconsistent with the overkill scenario. By the end of the Pleistocene, when humans first entered North America, these large animals had been geographically separated from intensive human hunting for more than 200,000 years. Given this enormous span of geologic time, bison would almost certainly have been very nearly as naive as native North American large mammals.\n\nThe culture that has been connected with the wave of extinctions in North America is the paleo-Indian culture associated with the Clovis people (\"q.v.\"), who were thought to use spear throwers to kill large animals. The chief criticism of the \"prehistoric overkill hypothesis\" has been that the human population at the time was too small and/or not sufficiently widespread geographically to have been capable of such ecologically significant impacts. This criticism does not mean that climate change scenarios explaining the extinction are automatically to be preferred by default, however, any more than weaknesses in climate change arguments can be taken as supporting overkill. Some form of a combination of both factors could be plausible, and overkill would be a lot easier to achieve large-scale extinction with an already dying population due to climate change.\n\nLack of tameable megafauna was perhaps one of the reasons why Amerindian civilizations evolved differently from Old World ones. Critics have disputed this by arguing that llamas, alpacas, and bison were domesticated.\n\nSouth America had been isolated as an island continent for many millions of years, and had a wide range of fauna found nowhere else, although many of them became extinct during the Great American Interchange about 3 million years ago, such as the \"Sparassodonta\" family. Those that survived the interchange included the ground sloths, glyptodonts, litopterns, pampatheres, phorusrhacids (terror birds) and notoungulates; all managed to extend their range to North America. [https://www.floridamuseum.ufl.edu/files/7513/9447/0046/bulletin-Mcdonaldlowres.pdf <nowiki>[3]</nowiki>] In the Pleistocene, South America remained largely unglaciated except for increased mountain glaciation in the Andes, which had a two-fold effect- there was a faunal divide between the Andes, and the colder, arid interior resulted in the advance of temperate lowland woodlands, tropical savannas and deserts at the expense of rainforests. Within these open environments, megafauna diversity was extremely dense, with over 40 genera recorded from the Guerrero member of Luján Formation alone. Ultimately, by the mid-Holocene, all the preeminent genera of megafauna became extinct- the last specimens of \"Doedicurus\" and \"Toxodon\" have been dated to 4,555 BCE and 3,000 BCE respectively. Their smaller relatives remain, including anteaters, tree sloths, armadillos; New World marsupials: opossums, shrew opossums, and the monito del monte (actually more related to Australian marsupials). Intense human habitation was established circa 11,000 BCE, however partly disputed evidence of pre-clovis habitation occurs since 46,000 BCE and 20,000 BCE, such as at the Serra da Capivara National Park (Brazil) and Monte Verde (Chile) sites. Today the largest land mammals remaining in South America are the wild camels of the \"Lamini\" group, such as the guanacos and vicuñas, and the \"Tapirus\" genus, of which Baird's tapir can reach up to 400 kg. Other notable surviving large fauna are peccaries, marsh deer (\"Capreolinae\"), giant anteaters, spectacled bears, maned wolves, pumas, ocelots, jaguars, rheas, emerald tree boas, boa constrictors, anacondas, american crocodiles, caimans, and giant rodents such as capybaras.\n\nThere is no general agreement on where the Holocene, or anthropogenic, extinction begins, and the Quaternary extinction event which includes climate change resulting in the end of the last ice age ends, or if they should be considered separate events at all. Some have suggested that anthropogenic extinctions may have begun as early as when the first modern humans spread out of Africa between 100,000 and 200,000 years ago, which is supported by rapid megafaunal extinction following recent human colonisation in Australia, New Zealand and Madagascar, in a similar way that any large, adaptable predator moving into a new ecosystem would. In many cases, it is suggested even minimal hunting pressure was enough to wipe out large fauna, particularly on geographically isolated islands. Only during the most recent parts of the extinction have plants also suffered large losses.\n\nOverall, the Holocene extinction can be characterised by the human impact on the environment. The Holocene extinction continues into the 21st century, with overfishing, ocean acidification and the amphibian crisis being a few broader examples of an almost universal, cosmopolitan decline of biodiversity.\n\nThe hunting hypothesis suggests that humans hunted megaherbivores to extinction, which in turn caused the extinction of carnivores and scavengers which had preyed upon those animals. Therefore, this hypothesis holds Pleistocene humans responsible for the megafaunal extinction. One variant, known as \"blitzkrieg\", portrays this process as relatively quick. Some of the direct evidence for this includes: fossils of some megafauna found in conjunction with human remains, embedded arrows and tool cut marks found in megafaunal bones, and European cave paintings that depict such hunting. Biogeographical evidence is also suggestive: the areas of the world where humans evolved currently have more of their Pleistocene megafaunal diversity (the elephants and rhinos of Asia and Africa) compared to other areas such as Australia, the Americas, Madagascar and New Zealand without the earliest humans. A picture arises of the megafauna of Asia and Africa evolving alongside humans, learning to be wary of them, and in other parts of the world the wildlife appearing ecologically naive and easier to hunt. This is particularly true of island fauna, which display a disastrous lack of fear of humans. Of course, it is impossible to demonstrate this naïveté directly in ancient fauna.\n\nCircumstantially, the close correlation in time between the appearance of humans in an area and extinction there provides weight for this scenario. The megafaunal extinctions covered a vast period of time and highly variable climatic situations. The earliest extinctions in Australia were complete approximately 50,000 BP, well before the last glacial maximum and before rises in temperature. The most recent extinction in New Zealand was complete no earlier than 500 BP and during a period of cooling. In between these extremes megafaunal extinctions have occurred progressively in such places as North America, South America and Madagascar with no climatic commonality. The only common factor that can be ascertained is the arrival of humans.\nThis phenomenon appears even within regions. The mammal extinction wave in Australia about 50,000 years ago coincides not with known climatic changes, but with the arrival of humans. In addition, large mammal species like the giant kangaroo \"Protemnodon\" appear to have succumbed sooner on the Australian mainland than on Tasmania, which was colonised by humans a few thousand years later.\n\nWorldwide, extinctions seem to follow the migration of humans and to be most severe where humans arrived most recently and least severe where humans originated — in Africa (see figure \"March of Man\" below). This suggests that prey animals and human hunting ability evolved together, so the animals evolved avoidance techniques. As humans migrated throughout the world and became more and more proficient at hunting, they encountered animals that had evolved without the presence of humans. Lacking the fear of humans that African animals had developed, animals outside of Africa were easy prey for human hunting techniques. It also suggests that this is independent of climate change.\n\nExtinction through human hunting has been supported by archaeological finds of mammoths with projectile points embedded in their skeletons, by observations of modern naïve animals allowing hunters to approach easily and by computer models by Mosimann and Martin, and Whittington and Dyke, and most recently by Alroy.\n\nA study published in 2015 supported the hypothesis further by running several thousand scenarios that correlated the time windows in which each species is known to have become extinct with the arrival of humans on different continents or islands. This was compared against climate reconstructions for the last 90,000 years. The researchers found correlations of human spread and species extinction indicating that the human impact was the main cause of the extinction, while climate change exacerbated the frequency of extinctions. The study, however, found an apparently low extinction rate in the fossil record of mainland Asia.\n\nThe overkill hypothesis, a variant of the hunting hypothesis, was proposed 40 years ago by Paul S. Martin, Professor of Geosciences Emeritus at the Desert Laboratory of the University of Arizona.\n\nThe major objections to the theory are as follows:\n\nAt the end of the 19th and beginning of the 20th centuries, when scientists first realized that there had been glacial and interglacial ages, and that they were somehow associated with the prevalence or disappearance of certain animals, they surmised that the termination of the Pleistocene ice age might be an explanation for the extinctions.\n\nCritics object that since there were glacial in the evolutionary history of many of the megafauna, it is rather implausible that only after the last glacial maximum would there be such extinctions. However, this criticism is rejected by a recent study indicating that terminal Pleistocene megafaunal community composition may have differed markedly from faunas present during earlier interglacials, particularly with respect to the great abundance and geographic extent of Pleistocene \"Bison\" at the end of the epoch. This suggests that the survival of megafaunal populations during earlier interglacials is essentially irrelevant to the terminal Pleistocene extinction event, because bison were not present in similar abundance during any of the earlier interglacials.\n\nSome evidence weighs against climate change as a valid hypothesis as applied to Australia. It has been shown that the prevailing climate at the time of extinction (40,000–50,000 BP) was similar to that of today, and that the extinct animals were strongly adapted to an arid climate. The evidence indicates that all of the extinctions took place in the same short time period, which was the time when humans entered the landscape. The main mechanism for extinction was probably fire (started by humans) in a then much less fire-adapted landscape. Isotopic evidence shows sudden changes in the diet of surviving species, which could correspond to the stress they experienced before extinction.\n\nEvidence in Southeast Asia, in contrast to Europe, Australia, and the Americas, suggests that climate change and an increasing sea level were significant factors in the extinction of several herbivorous species. Alterations in vegetation growth and new access routes for early humans and mammals to previously isolated, localized ecosystems were detrimental to select groups of fauna.\n\nSome evidence obtained from analysis of the tusks of mastodons from the American Great Lakes region appears inconsistent with the climate change hypothesis. Over a span of several thousand years prior to their extinction in the area, the mastodons show a trend of declining age at maturation. This is the opposite of what one would expect if they were experiencing stresses from deteriorating environmental conditions, but is consistent with a reduction in intraspecific competition that would result from a population being reduced by human hunting.\n\nThe most obvious change associated with the termination of an ice age is the increase in temperature. Between 15,000 BP and 10,000 BP, a 6 °C increase in global mean annual temperatures occurred. This was generally thought to be the cause of the extinctions.\n\nAccording to this hypothesis, a temperature increase sufficient to melt the Wisconsin ice sheet could have placed enough thermal stress on cold-adapted mammals to cause them to die. Their heavy fur, which helps conserve body heat in the glacial cold, might have prevented the dumping of excess heat, causing the mammals to die of heat exhaustion. Large mammals, with their reduced surface area-to-volume ratio, would have fared worse than small mammals.\n\nA study covering the past 56,000 years indicates that rapid warming events with temperature changes of up to 16 °C (29 °F) had an important impact on the extinction of megafauna. Ancient DNA and radiocarbon data indicates that local genetic populations were replaced by others within the same species or by others within the same genus. Survival of populations was dependent on the existence of refugia and long distance dispersals, which may have been disrupted by human hunters.\n\nStudies propose that the annual mean temperature of the current interglacial that we have seen for the last 10,000 years is no higher than that of previous interglacials, yet some of the same large mammals survived similar temperature increases. Therefore, warmer temperatures alone may not be a sufficient explanation.\n\nIn addition, numerous species such as mammoths on Wrangel Island and St. Paul Island survived in human-free refugia despite changes in climate. This would not be expected if climate change were responsible (unless their maritime climates offered some protection against climate change not afforded to coastal populations on the mainland). Under normal ecological assumptions island populations should be more vulnerable to extinction due to climate change because of small populations and an inability to migrate to more favorable climes.\n\nOther scientists have proposed that increasingly extreme weather—hotter summers and colder winters—referred to as \"continentality\", or related changes in rainfall caused the extinctions. The various hypotheses are outlined below.\n\nIt has been shown that vegetation changed from mixed woodland-parkland to separate prairie and woodland. This may have affected the kinds of food available. Shorter growing seasons may have caused the extinction of large herbivores and the dwarfing of many others. In this case, as observed, bison and other large ruminants would have fared better than horses, elephants and other monogastrics, because ruminants are able to extract more nutrition from limited quantities of high-fiber food and better able to deal with anti-herbivory toxins. So, in general, when vegetation becomes more specialized, herbivores with less diet flexibility may be less able to find the mix of vegetation they need to sustain life and reproduce, within a given area.\n\nIncreased continentality resulted in reduced and less predictable rainfall limiting the availability of plants necessary for energy and nutrition. Axelrod and Slaughter have suggested that this change in rainfall restricted the amount of time favorable for reproduction. This could disproportionately harm large animals, since they have longer, more inflexible mating periods, and so may have produced young at unfavorable seasons (i.e., when sufficient food, water, or shelter was unavailable because of shifts in the growing season). In contrast, small mammals, with their shorter life cycles, shorter reproductive cycles, and shorter gestation periods, could have adjusted to the increased unpredictability of the climate, both as individuals and as species which allowed them to synchronize their reproductive efforts with conditions favorable for offspring survival. If so, smaller mammals would have lost fewer offspring and would have been better able to repeat the reproductive effort when circumstances once more favored offspring survival.\n\nIn 2017 a study looked at the environmental conditions across Europe, Siberia and the Americas from 25,000–10,000 YBP. The study found that prolonged warming events leading to deglaciation and maximum rainfall occurred just prior to the transformation of the rangelands that supported megaherbivores into widespread wetlands that supported herbivore-resistant plants. The study proposes that moisture-driven environmental change led to the megafaunal extinctions and that Africa's trans-equatorial position allowed rangeland to continue to exist between the deserts and the central forests, therefore fewer megafauna species became extinct there.\n\nCritics have identified a number of problems with the continentality hypotheses.\n\nThe extinction of the megafauna could have caused the disappearance of the mammoth steppe. Alaska now has low nutrient soil unable to support bison, mammoths, and horses. R. Dale Guthrie has claimed this as a cause of the extinction of the megafauna there; however, he may be interpreting it backwards. The loss of large herbivores to break up the permafrost allows the cold soils that are unable to support large herbivores today. Today, in the arctic, where trucks have broken the permafrost grasses and diverse flora and fauna can be supported. In addition, Chapin (Chapin 1980) showed that simply adding fertilizer to the soil in Alaska could make grasses grow again like they did in the era of the mammoth steppe. Possibly, the extinction of the megafauna and the corresponding loss of dung is what led to low nutrient levels in modern-day soil and therefore is why the landscape can no longer support megafauna.\n\nIt may be observed that neither the overkill nor the climate change hypotheses can fully explain events: browsers, mixed feeders and non-ruminant grazer species suffered most, while relatively more ruminant grazers survived. However, a broader variation of the overkill hypothesis may predict this, because changes in vegetation wrought by either Second Order Predation (see below) or anthropogenic fire preferentially selects against browse species.\n\nThe hyperdisease hypothesis attributes the extinction of large mammals during the late Pleistocene to indirect effects of the newly arrived aboriginal humans. The Hyperdisease Hypothesis proposes that humans or animals traveling with them (e.g., chickens or domestic dogs) introduced one or more highly virulent diseases into vulnerable populations of native mammals, eventually causing extinctions. The extinction was biased toward larger-sized species because smaller species have greater resilience because of their life history traits (e.g., shorter gestation time, greater population sizes, etc.). Humans are thought to be the cause because other earlier immigrations of mammals into North America from Eurasia did not cause extinctions.\n\nDiseases imported by people have been responsible for extinctions in the recent past; for example, bringing avian malaria to Hawaii has had a major impact on the isolated birds of the island.\n\nIf a disease was indeed responsible for the end-Pleistocene extinctions, then there are several criteria it must satisfy (see Table 7.3 in MacPhee & Marx 1997). First, the pathogen must have a stable carrier state in a reservoir species. That is, it must be able to sustain itself in the environment when there are no susceptible hosts available to infect. Second, the pathogen must have a high infection rate, such that it is able to infect virtually all individuals of all ages and sexes encountered. Third, it must be extremely lethal, with a mortality rate of c. 50–75%. Finally, it must have the ability to infect multiple host species without posing a serious threat to humans. Humans may be infected, but the disease must not be highly lethal or able to cause an epidemic.\n\nOne suggestion is that pathogens were transmitted by the expanding humans via the domesticated dogs they brought with them. Unfortunately for such a theory it can not account for several major extinction events, notably Australia and North America. Dogs did not arrive in Australia until approximately 35,000 years after the first humans arrived and approximately 30,000 years after the megafaunal extinction was complete and as such can not be implicated. In contrast numerous species including wolves, mammoths, camelids and horses had emigrated continually between Asia and North America over the past 100,000 years. For the disease hypothesis to be applicable in the case of the Americas it would require that the population remain immunologically naive despite this constant transmission of genetic and pathogenic material.\n\n\nThe Second-Order Predation Hypothesis says that as humans entered the New World they continued their policy of killing predators, which had been successful in the Old World but because they were more efficient and because the fauna, both herbivores and carnivores, were more naive, they killed off enough carnivores to upset the ecological balance of the continent, causing overpopulation, environmental exhaustion, and environmental collapse. The hypothesis accounts for changes in animal, plant, and human populations.\n\nThe scenario is as follows:\n\nThis has been supported by a computer model, the Pleistocene Extinction Model (PEM), which, using the same assumptions and values for all variables (herbivore population, herbivore recruitment rates, food needed per human, herbivore hunting rates, etc.) other than those for hunting of predators. It compares the Overkill hypothesis (predator hunting = 0) with Second-Order Predation (predator hunting varied between 0.01 and 0.05 for different runs). The findings are that Second Order-Predation is more consistent with extinction than is Overkill (results graph at left).\n\nThe PEM is the only test of multiple hypotheses and is the only model to specifically test combination hypotheses by artificially introducing sufficient climate change to cause extinction. When Overkill and Climate Change are combined they balance each other out. Climate Change reduces the number of plants, Overkill removes animals, therefore fewer plants are eaten. Second-Order Predation combined with Climate Change exacerbates the effect of Climate Change. (results graph at right).\n\nThe second-order predation hypothesis is supported by the observation above that there was a massive increase in bison populations.\n\n\n\n\nFirst publicly presented at the Spring 2007 joint assembly of the American Geophysical Union in Acapulco, Mexico, the comet hypothesis suggests that the mass extinction was caused by a swarm of comets 12,900 years ago. Using photomicrograph analysis, research published in January 2009 has found evidence of nanodiamonds in the soil from six sites across North America including Arizona, Minnesota, Oklahoma, South Carolina and two Canadian sites. Similar research found nanodiamonds in the Greenland ice sheet.\n\nDebate around this hypothesis has included, among other things, the lack of an impact crater, relatively small increased level of iridium in the soil, and the relative probability of such an event. That said, it took 10 years after publication of the Alvarez theory before scientists found the Chicxulub crater. If the bolide struck the Laurentide ice sheet as hypothesized by Firestone et al. (2007), we would not see the typical impact crater.. \n\nA spike in platinum was found in the Greenland ice cores by Petaev et al. (2013), which they view as a global signal. Confirmation came in 2017 with the report that the Pt spike had been found at \"11 widely separated archaeological bulk sedimentary sequences.\" Wolbach et al. reported in 2018 that \"YDB peaks in Pt were observed at 28 sites\" in total, including the 11 reported earlier and the one from Greenland. \n\n\n\n\n"}
{"id": "192277", "url": "https://en.wikipedia.org/wiki?curid=192277", "title": "Redd Foxx", "text": "Redd Foxx\n\nJohn Elroy Sanford (December 9, 1922 – October 11, 1991), better known by his screen name Redd Foxx, was an American stand-up comedian and actor, best remembered for his explicit comedy records and his starring role on the 1970s sitcom \"Sanford and Son\". Foxx gained notoriety with his raunchy nightclub acts during the 1950s and 1960s. Known as the \"King of the Party Records\", he performed on more than 50 records in his lifetime. He also starred in \"Sanford\", \"The Redd Foxx Show\" and \"The Royal Family\". His film projects included \"All the Fine Young Cannibals\" (1960), \"Cotton Comes to Harlem\" (1970), \"Norman... Is That You?\" (1976) and \"Harlem Nights\" (1989).\n\nIn 2004, \"Comedy Central Presents: 100 Greatest Stand-ups of All Time\" ranked Foxx as the 24th best stand-up comedian. Foxx not only influenced many comedians, but was often portrayed in popular culture as well, mainly as a result of his famous catchphrases, body language and facial expressions exhibited on \"Sanford and Son\". During the show's five year run, Foxx won a Golden Globe Award and received an additional three nominations, along with three Primetime Emmy Award nominations.\n\nRedd Foxx was born John Elroy Sanford on December 9, 1922 in St. Louis, Missouri and raised in Chicago's South Side. His father, Fred Sanford, an electrician and auto mechanic from Hickman, Kentucky, left his family when Foxx was four years old. He was raised by his half-Seminole mother, Mary Hughes, from Ellisville, Mississippi, his grandmother and his minister. Foxx attended DuSable High School in Chicago's Bronzeville neighborhood with future Chicago mayor Harold Washington. Foxx had an older brother, Fred Jr., who provided the name for his character on \"Sanford and Son\". On July 27, 1939, Foxx performed on the Major Bowes Amateur Hour radio show as part of the Jump Swinging Six.\n\nIn the 1940s, he met Malcolm Little, later known as Malcolm X. In Malcolm's autobiography, Foxx is referred to as \"Chicago Red, the funniest dishwasher on this earth.\" He earned the nickname because of his reddish hair and complexion. During World War II, Foxx dodged the draft by eating half a bar of soap before his physical, a trick that resulted in heart palpitations. On September 30, 1946, Foxx recorded five songs for the Savoy label under the direction of Teddy Reig.\n\nFoxx gained notoriety with his raunchy nightclub act. After performing on the East Coast, his big break came after singer Dinah Washington insisted that he come to Los Angeles, where Dootsie Williams of Dootone records caught his act at the Brass Rail nightclub. Foxx was one of the first black comics to play to white audiences on the Las Vegas Strip. He was signed to a long-term contract and released a series of comedy albums that quickly became cult favorites.\n\nFoxx achieved his most widespread fame starring in the television sitcom \"Sanford and Son,\" an adaptation of the BBC series \"Steptoe and Son\". He used his starring role on \"Sanford and Son\" to help get jobs for his acquaintances such as LaWanda Page, Slappy White, Gregory Sierra, Don Bexley, Beah Richards, Stymie Beard, Leroy Daniels, Ernest Mayhand and Noriyuki \"Pat\" Morita.\n\nThe series premiered on the NBC television network on January 14, 1972 and was broadcast for six seasons. The final episode aired on March 25, 1977. Foxx played the role of Fred G. Sanford (\"Fred Sanford\" was actually Foxx's father's and brother's name), while Foxx's co-star Demond Wilson played the role of his son Lamont. In this sitcom, Fred and Lamont were owners of a junk/salvage store in Watts who dealt with many humorous situations that would arise. The series was notable for its racial humor and overt prejudices which helped redefine the genre of black situation comedy.\n\nThe show also had several running gags. When angry with Lamont, Fred would often say \"You big dummy\" or would often fake heart attacks by putting his hand on his chest and saying (usually while looking up at the sky) \"It's the big one, I'm coming to join ya honey/Elizabeth\" (referring to his late wife). Fred would also complain about having \"arthur-itis\" to get out of working by showing Lamont his cramped hand. Foxx depicted a character in his 60s, although in real-life he was a decade younger.\n\nDemond Wilson was asked whether he kept in touch with everybody from \"Sanford & Son\", especially the series' star himself, after the series was canceled: \"No. I saw Redd Foxx once before he died, circa 1983, and I never saw him again. At the time I was playing tennis at the Malibu Racquet Club and I was approached by some producers about doing a Redd Foxx 50th Anniversary Special. I hadn’t spoken to him since 1977, and I called the club where (Redd) was playing. And we met at Redd’s office, but he was less than affable. I told those guys it was a bad idea. I never had a cross word with him. People say I’m protective of Redd Foxx in my book (Second Banana, Wilson’s memoir of the Sanford years). I had no animosity toward Foxx for (quitting the show in 1977) because I had a million dollar contract at CBS to do Baby I’m Back. My hurt was that he didn’t come to me about throwing the towel in - I found out in the hallway at NBC from a newscaster. I forgave him and I loved Redd, but I never forgot that. The love was there. You can watch any episode and see that.\"\n\n In 1977, Foxx left \"Sanford and Son\", after six seasons (the show was canceled with his departure) to star in a short-lived ABC variety show. In 1980 he was back playing Fred G. Sanford in a short-lived revival/spin-off, \"Sanford\". In 1986, he returned to television in the ABC series \"The Redd Foxx Show\", which was cancelled after 12 episodes because of low ratings. Foxx appeared as an \"Obi-Wan Kenobi\"-like character in the \"Star Wars\" special of the \"Donny & Marie\" show. In an homage to his show, he mentioned the planet Sanford, which has no sun. Foxx made a comeback with the series \"The Royal Family\", in which he co-starred with Della Reese.\n\nRedd Foxx was married four times. His first marriage was to Evelyn Killebrew in 1948 and ended in divorce in 1951. His second marriage in 1956 was to Betty Jean Harris, a showgirl and dancer, who was a colleague of LaWanda Page (later to be Foxx's TV rival Aunt Esther on \"Sanford and Son\"). Foxx adopted Harris's nine-year-old daughter Debraca, who assumed the surname \"Foxx\". This marriage ended in divorce in 1975. Foxx next wed Korean-American Yun Chi Chung in 1976, but the marriage ended in 1981. At the time of his death, Foxx was married to Ka Ho Cho, who used the name Ka Ho Foxx.\n\nAccording to \"People Magazine\", \"Foxx reportedly once earned $4 million in a single year, but depleted his fortune with a lavish lifestyle, exacerbated by what he called 'very bad management. Contributing to his problems was a 1981 divorce settlement of $300,000 paid to his third wife. In 1983 he filed for bankruptcy, with proceedings continuing at least through 1989. The IRS filed tax liens against Redd Foxx's property for income taxes he owed for the years 1983 to 1986 totaling $755,166.21. On November 28, 1989, the IRS seized his home in Las Vegas and seven vehicles (including a 1927 Model T, a 1975 Panther J72, a 1983 Zimmer, and a Vespa motor scooter) to pay the taxes which by then had grown to $996,630, including penalties and interest. Agents also seized \"$12,769 in cash and a dozen guns, including a semiautomatic pistol,\" among some 300 items in total, reportedly leaving only Foxx's bed.\nFoxx stated that the IRS \"took my necklace and the ID bracelet off my wrist and the money out of my pocket ... I was treated like I wasn't human at all.\" It has been reported that, at the time of his death in 1991, Foxx owed more than $3.6 million in taxes.\n\nOn October 11, 1991, during a break from rehearsals for \"The Royal Family\", he suffered a heart attack on the set. According to Della Reese, Foxx was about to have an interview with \"Entertainment Tonight\" when he was called onto the set to practice a scene. An angry Foxx did as he was asked but as soon as he finished he collapsed to the floor. Reese said that nobody initially suspected anything was wrong; Foxx was famous for his fake heart attack shtick on \"Sanford and Son\" and, as Reese recalled, was skilled at doing pratfalls. When Foxx did not immediately get up, Reese rushed over to check on him. When she leaned down to Foxx as he was on the ground, Foxx said, \"Get my wife\" repeatedly and went into cardiac arrest. According to Joshua Rich at \"Entertainment Weekly\", \"It was an end so ironic that for a brief moment cast mates figured Foxx – whose 1970s TV character often faked heart attacks – was kidding when he grabbed a chair and fell to the floor.\" Foxx, who had temporarily been brought back to life while being treated initially, was taken to Queen Of Angels Hollywood Presbyterian Medical Center, where he succumbed to the attack and died that night at the age of 68. Foxx was posthumously given a star on the St. Louis Walk of Fame on May 17, 1992. Foxx is buried in Las Vegas, at Palm Valley View Memorial Park. His mother, Mary Carson (1903–1993), outlived Foxx and died two years later, in 1993. She was buried just to the right of her famed son.\n\nComedian and actor Richard Pryor cited Redd Foxx as an influence. \"He gave me inspiration and encouragement so I could be more me\", Pryor told \"Ebony\" magazine in 1990. Comedian Chris Rock cites Redd Foxx as an influence. An episode of his show \"Everybody Hates Chris\" shows young Chris Rock overhearing his parents' Redd Foxx albums and getting started doing stand-up through retelling the jokes at school.\n\nIn 1990, in the first-ever episode of \"In Living Color\", in reference to Foxx's financial troubles, Foxx was portrayed by Damon Wayans, who is making a public service announcement to encourage people to pay their taxes. In the film \"Why Do Fools Fall in Love\", Foxx is portrayed by Aries Spears. He is shown performing a stand-up comedy routine. In the animated television series \"Family Guy\" parody of \"Star Wars Episode IV: A New Hope\" episode \"Blue Harvest\", Redd Foxx appears very briefly as an X-wing pilot. When his ship is shot down, he cries \"I'm coming Elizabeth!\" before dying. In addition to this, he has been parodied on \"Family Guy\" by Francis Griffin acting as Foxx's \"Sanford and Son\" character.\n\nFoxx was meant to be featured in the MTV show \"Celebrity Deathmatch\", advertised as taking on Jamie Foxx in the episode \"When Animals Attack\". Instead of Redd Foxx though, Jamie Foxx fought Ray Charles. In the \"Boondocks\" episode \"\" he is portrayed as Lord Rufus Crabmiser, one of Stinkmeaner's old friends coming to kill the Freeman family. Childhood friend and \"Sanford & Son\" co-star Lawanda Page is also portrayed in the same episode as Lady Esmeralda Gripenasty.\n\nRedd Foxx appears as a minor character in the 2009 \"James Ellroy\" novel \"Blood's a Rover\". He gives a bawdy eulogy at the wake of Scotty Bennett, a murdered rogue LAPD detective including the line \"Scotty Bennett was fucking a porcupine. I gots to tell you motherfuckers that it was a female porcupine, so I don't see nothing perverted in it.\" In the 1999 film \"Foolish\" starring comedian Eddie Griffin and rapper Master P, the ghost of Redd Foxx gives Griffin's character advice from behind a stall door in a men's restroom at a comedy club before he goes onstage to perform a show. In 2015, it was said that comedian Tracy Morgan would portray Redd Foxx in a Richard Pryor biopic starring opposite comedian Mike Epps.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "1085051", "url": "https://en.wikipedia.org/wiki?curid=1085051", "title": "Reptile (Mortal Kombat)", "text": "Reptile (Mortal Kombat)\n\nReptile is a video game character from the \"Mortal Kombat\" fighting game franchise created for Midway Games by Ed Boon and John Tobias. He debuted in \"Mortal Kombat\" as a hidden opponent and appeared in subsequent titles as a playable character, also appearing in the merchandise and other media related to the series.\n\nReptile is a Raptor, a nearly-extinct bipedal humanoid race of reptilian creatures and loyally serves the series' recurring villain Shao Kahn in hope that his race will be revived. Reptile has been featured in almost every title in the series under the same pretenses, and until the 2011 reboot, he had yet to be rewarded for his efforts. Critical reception to the character has been positive, with many print and online publications commenting on his evolution since his debut as a hidden character.\n\nIncluded in the first game late in the development process, Reptile's character concept was conceived while Boon was driving back to work from lunch. Noting the success of utilizing a palette swap method for Scorpion and Sub-Zero's character sprites, he and Tobias decided to include a \"super secret hidden feature\" in \"Mortal Kombat\", choosing Reptile's green color as a contrast to Scorpion's original yellow and Sub-Zero's blue colors. Developed with the premise of being \"a cooler version of Scorpion\", the character's concept was completed in a single evening. Reptile's inclusion was intended as a marketing tool for the arcade game: as extreme conditions must be met to encounter Reptile, the designers hoped to rely on word of mouth to spread rumors of the character's existence. However, the character was not included in the title until version 3.0 of the game.\n\nReptile's appearance caused fans to speculate that he could be unlocked and used as a controllable character. Boon noted in a later interview that due to the popularity of the rumors surrounding the character, they decided to include Reptile in subsequent installments of the series as a playable fighter.\n\nReptile was originally depicted as a tall, muscular, light-skinned humanoid. Due to his origin as a palette swap of Scorpion and Sub-Zero, his attire was identical to theirs only colored in green. His clothing consists of pants and boots covering his lower body, along with a black sleeveless shirt and open hood. Green light armor covered his forearms and lower legs, while an open V-shaped green vest protected his torso, extending to a matching small fauld and loincloth. He wears a green facial mask as part of his ninja disguise to conceal his true reptilian nature, however he does remove the mask in-game for his Acid Spit attack or his tongue grab fatality. Reptile stands 6 feet (183 cm) tall, and speaks in a hissing tone.\n\nIn \"Mortal Kombat 4\", Reptile's appearance became more reptilian, with his head completely uncovered. His hands became clawed, while his feet were now three-toed talons. By \"\", Reptile had developed a full reptilian head and tail, with gold and black armor to cover his legs, elbows, shoulders, and belt, and he was competely barefoot. Bone spurs extended from the base of his skull down his spine, ending at the tip of his tail, while a similar spur extended from his heel on each bare foot. Ed Boon described the changes to Reptile's design over the course of the two games as hinting at him \"evolving into a bigger character\", and foreshadowing Onaga controlling his body in \"\".\n\nReptile's appearance in \"\" used an amalgam of his three previous designs, intended by character designer Mark Lappin to portray a classic yet fresh feel for the character. The design incorporated the appearance of his body from \"Mortal Kombat 4\", his clothing from the original \"Mortal Kombat\", and the look of his bare feet, hands, and armor from \"Deadly Alliance\". In addition, black strips of cloth wrapped around his forearms, hands, and head, leaving his eyes,mouth and bare feet exposed. The design was later reused as his primary outfit in \"\", with his appearance from \"Deadly Alliance\" serving as his secondary outfit.\n\nIn the original \"Mortal Kombat\", Reptile is a mysterious hidden enemy character, unplayable and with no biography. Hints regarding the conditions of how to unlock the fight against him are conveyed by Reptile randomly appearing prior to other matches. Some of these hints include \"Look to La Luna,\" \"Perfection is the key,\" and \"Alone is how to find me.\" To fight Reptile, the player must get a Double Flawless victory in single player mode on the Pit stage and finish the match off with a fatality, all without blocking. There must also be a silhouette flying past the moon, which will occur every sixth game.\n\nIn \"Mortal Kombat II\", Reptile returns as a playable character and a member of a reptilian race called Raptors, who were pulled into Outworld and enslaved by Shao Kahn. Promised the revival of his people in turn for his loyalty, Reptile serves Kahn as Shang Tsung's bodyguard. He was chosen to assist Jade in order to kill Kitana during the events of \"Ultimate Mortal Kombat 3\", he is defeated and exiled, but reappears in \"Mortal Kombat 4\" as Shinnoks minion. By \"\", Reptile returns to Kahn's service. He overhears Shang Tsung plotting to kill Kahn, but en route to warn his master, he meets a female vampire who offers knowledge of his race. Reptile pledges his loyalties to her, though eventually realizes she is merely using him and sets out to kill her. Instead of the vampire he finds Onaga's dragon egg instead, which transforms Reptile into Onaga's avatar, leading to the events of \"\" and ending with his defeat at the game's conclusion. Separated from Onaga as a result, Reptile returns in \"\". In Konquest mode in \"Armageddon\", he appears in the Red Dragon lair commanding Daegon's dragon Caro to close the portal but refused. Taven battles Reptile in combat and emerges victorious.\n\nReptile also appears in \"\", appearing in the game's opening sequence and later as a boss. During development, producer Shaun Himmerick noted that Reptile was included in the game because the character was one of Himmerick's favorite in the series. Originally included in \"NBA Jam Tournament Edition\" alongside other \"Mortal Kombat\" characters as an unlockable player, he was eventually removed from later versions of the game at the request of the NBA.\n\nReptile reappears in the 2011 \"Mortal Kombat\" video game, retelling his role from the first tournaments. In \"Mortal Kombat X\" (2015), Reptile follows Kotal Kahn and helps bring about Mileena's removal from the throne.\n\nAs a secret character in the first \"Mortal Kombat\", Reptile features a hybrid of Sub-Zero's and Scorpion's attacks, such as the former's freezing projectile and the latter's harpoon. When made a playable character for later installments, his moves were changed completely. His \"Forceball\" attack creates an energy ball from his hands, while his \"Acid Spit\" attack produces a higher, faster projectile. Reptile can slide towards the opponent, or turn invisible for a brief period of time or until hit. Later titles in the series modify these moves, such as splitting the Forceball attack into slow and fast variants, or remove moves in favor of different attacks, only to return them in the next installment.\n\nReptile's Fatalities in \"Mortal Kombat II\" consist of revealing his reptilian face in order to devour the opponent's head, or turning invisible and severing the opponent's torso. Later finishing moves expand upon the concept, ranging from regurgitating acid upon the opponent to leaping upon their body and gnawing the flesh from their skulls. In a series retrospective, the \"Mortal Kombat\" development team named Reptile's finishers as some of their favorites from the series.\n\nReptile appears as a minor character in several of Malibu Comics' \"Mortal Kombat\" comic book series released in 1995, debuting in the \"Goro: Prince of Pain\" miniseries as a member of a team led by Kitana to find the missing Goro, later appearing in the \"Battlewave\" miniseries hypnotizing Sonya Blade into wanting to marry Shao Kahn, and in several one-shots, such as \"Kitana and Mileena\". He also appears in comics published by Midway for \"Mortal Kombat II\" and \"Mortal Kombat 4\", based on his role in each title. In the comics, Reptile speaks in the same hissing tone as he does in the games. A paperback novel written by C. Dean Anderson entitled \"Mortal Kombat: Reptile's World\" was released in 1996. Written for junior readers, the book described the past of Reptile's fictional race through the eyes of other series characters.\n\nReptile later appears in the 1995 film \"Mortal Kombat\" as a bipedal lizard, camouflaging himself until found by Liu Kang. After being thrown into a statue, Reptile transforms into a human-looking ninja and battles Liu Kang, but is defeated and then crushed after reverting to his original form. Reptile's lizard form was rendered with the use of computer-generated imagery, while the character's human form is portrayed by Keith Cooke, clothed similar to Scorpion and Sub-Zero in the film, but green and with a fanged open mouth imprinted on his face mask. Originally not included in the movie, Reptile was added in response to focus groups being unimpressed with the original fights in the film. Actor Robin Shou (Liu Kang) and director Paul Anderson noted that neither knew what Reptile's lizard form would look like until after filming, making the pre-fight sequence difficult to shoot.\n\nThe 1998 television series \"\" features Reptile in his human form, portrayed by Jon Valera. Commander of Shao Kahn's Raptors, he betrays Kahn and forms an alliance with Kreeya to share dominion over Kahn's domain and serve as one of her mates, only to be killed later during an ambush by Shao Kahn's priests. A similar character named Komodai appears in the animated \"\" series' third episode, leading an attack with other Raptors. Voiced by Josh Blyden, he is defeated and sent back to Outworld.\n\nReptile featured in director Kevin Tancharoen's 2010 short film \"\", a grittier, more realistic take on the \"MK\" canon. He was played by Richard Dorton and depicted therein as a cannibalistic criminal who eats his victims' heads after killing them. While not an inhuman ninja, the character suffers from a real-life disease known as Harlequin-type ichthyosis, giving him patchy scale-like skin and eyes that are grown inward. Reptile was the only character from the film who did not carry over into Tancharoen's \"\" web series.\n\nSkrillex released the single \"Reptile's Theme\", referring to the \"Mortal Kombat\" character and sampling from the game, in a compilation album titled \"\".\n\nA Reptile action figure was released by Hasbro as part of a G.I. Joe line of toys, packaged with the series-themed vehicle for the toyline. The figure comes with a katana and grappling hook. Another action figure to promote \"Shaolin Monks\" was released in 2006 by Jazwares. Fully posable, it includes an alternate head, detachable tongue and a large-bladed scimitar. He was one of many \"MK\" characters depicted on 2.5\" x 3.5\" collectible magnets released by Ata-Boy Wholesale in 2011. Reptile also featured prominently in the introduction sequence of \"Mortal Kombat 4\", portions of which were used in a television commercial to promote the game.\n\nThe character has been well received by critics. GameSpot said that Reptile gives the series \"an air of mystery\" due to the circumstances behind his first appearance, while CraveOnline listed the battle against him in \"Mortal Kombat\" as the fourth greatest event in video gaming. UGO.com ranked him eleventh in their list of top \"Mortal Kombat\" characters, stating that his exclusivity as a hidden character in the original game made him an \"arcade legend\" and set the tone for secrets in future titles in the series, and adding, \"future games would feature even more hidden enemies and complicated ways of accessing them, but the first was still the best.\" He was also fifth in Game Revolution's list of top \"old school\" \"Mortal Kombat\" characters\", praised for his introduction in \"Mortal Kombat\" and his changes in the sequel. IGN listed him as a character they would like to see as downloadable content for \"Mortal Kombat vs. DC Universe\", noting \"It's just not (a \"Mortal Kombat\") game without a bevy of palette-swapped ninjas...and our vote goes to Reptile,\" adding that while the character would be a \"pain in the ass to fight sometimes\", his inclusion would be \"a blessing to have on your side.\" He was also featured in the 2008 MSN article about the ten greatest Easter eggs in gaming. In 2010 UGO, included him on the list of the 25 coolest hidden characters in video gaming. ScrewAttack placed Reptile at ninth place on their list of top ten \"Mortal Kombat\" characters, commenting that anyone who got to fight him in the original \"Mortal Kombat\" became a local legend and getting to play as him and turn invisible in \"Mortal Kombat II\" was \"icing on the cake\". In UGO Networks' 2012 list of the top \"Mortal Kombat\" characters, Reptile placed as sixth, stating that he is the best hidden character in the series. Cheat Code Central included him in the 2012 list of top ten hidden characters in fighting games at number two.\n"}
{"id": "31084902", "url": "https://en.wikipedia.org/wiki?curid=31084902", "title": "Ricardo Fort", "text": "Ricardo Fort\n\nRicardo Aníbal Fort Campa (5 November 1968 − 25 November 2013) was an Argentine socialite, entrepreneur and television director. Although his career lasted only four years, Fort was one of the most popular personalities in his country.\n\nThe \"Felfort\" confectionery company was founded in 1912 in Buenos Aires by Fort's grandfather, Felipe Fort, and soon became one of the top confectionery companies in Argentina. Ricardo's father, Carlos Augusto Fort, took control of the company after Felipe's death in 1969. Ricardo Fort developed a close relationship with his mother, the opera singer Marta Campa.\n\nFort developed businesses in the textile industry and in modeling, as well as owned the \"Fortmen\" clothing line.\n\nIn 2009, Fort participated in the reality series \"El musical de tus sueños\".\n\nIn 2010, Fort was selected to be one of the judges of the seventh season of \"Bailando por un Sueño 2010\".\n\nIn 2011 he worked in the theatrical production \"Fortuna 2\".\n\nFort released a single called \"No volverás\" early in his career.\n\nIn 2010, Fort revealed in an interview on the talk show \"Tienen la Palabra\" that his twin children Felipe and Marta were born of a surrogate mother found through the auspices of a company in California.\n\nHe was openly bisexual.\n\nFort died on 25 November 2013 in a Buenos Aires clinic, due to cardiac arrest following a gastrointestinal bleeding. He was being treated for a knee injury he suffered a few months before. He was 45 years old. Just days before, he had also suffered a femoral fracture in Miami, Florida.\n\n"}
{"id": "723789", "url": "https://en.wikipedia.org/wiki?curid=723789", "title": "Riderless horse", "text": "Riderless horse\n\nA riderless horse (which may be caparisoned in ornamental and protective coverings, having a detailed protocol of their own) is a single horse, without a rider, and with boots reversed in the stirrups, which sometimes accompanies a funeral procession. The horse follows the caisson carrying the casket. A riderless horse can also be featured in military parades to symbolize fallen soldiers. In Australia for example, it is traditional for a riderless horse known as the 'Lone Charger' to lead the annual Anzac Day marches.\n\nThe custom is believed to date back to the time of Genghis Khan, when a horse was sacrificed to serve the fallen warrior in the next world. The riderless horse later came to symbolize a warrior who would ride no more.\n\nIn the United States, the riderless horse is part of the military honors given to an Army or Marine Corps officer who was a colonel or above; this includes the President, by virtue of having been the country's commander in chief and the Secretary of Defense, having overseen the armed forces. Alexander Hamilton, former Secretary of the Treasury (1789-1795) was the first American to be given the honor. Historian Ron Chernow noted that Hamilton's gray horse followed the casket \"with the boots and spurs of its former rider reversed in the stirrups.\" Abraham Lincoln was the first president of the United States to be officially honored by the inclusion of the riderless horse in his funeral cortege, although a letter from George Washington's personal secretary recorded the president's horse was part of the president's funeral, carrying his saddle, pistols, and holsters.\nTraditionally, simple black riding boots are reversed in the stirrups to represent a fallen commander looking back on his troops for the last time.\n\nIn 1865, Abraham Lincoln was honored by the inclusion of a riderless horse at his funeral. When Lincoln's funeral train reached Springfield, Illinois, his horse, Old Bob, who was draped in a black mourning blanket, followed the procession and led mourners to Lincoln's burial spot.\n\nA notable riderless horse was \"Black Jack,\" a half-Morgan named for General of the Armies John \"Black Jack\" Pershing. Black Jack took part in the state funerals of Presidents John F. Kennedy (1963),\nHerbert Hoover (1964), and Lyndon Johnson (1973), and General of the Army Douglas MacArthur (1964).\n\nBlack Jack was foaled January 19, 1947, and came to Fort Myer from Fort Reno, Oklahoma, on November 22, 1952. Black Jack was the last of the Quartermaster-issue horses branded with the Army's U.S. brand (on the left shoulder) and his Army serial number 2V56 (on the left side of his neck). He died on February 6, 1976, and was buried on the parade ground of Fort Myer's Summerall Field with full military honors, one of only two US Army horses to be given that honor.\n\n\"Dolly\", was the 22 year old charger (whose official name was Octave) of Admiral of the Fleet The Earl Mountbatten of Burma in his capacity as Colonel of the Life Guards. Following the assassination of Lord Mountbatten by the IRA in Mullaghmore, Dolly served as the riderless horse in the funeral procession being led ahead the head of the gun carriage with the Lord Mountbatten's boots (from his Colonel's uniform) reversed in the stirrups on 5 September 1979.\n\n\"Sergeant York\" was formerly known as \"Allaboard Jules\", a racing standardbred gelding. He was renamed (in honor of famous WWI soldier Alvin C. York) when he was accepted into the military in 1997. He served as the riderless horse in President Ronald Reagan's funeral procession, walking behind the caisson bearing Reagan's flag-draped casket. In the stirrups were President Reagan's personal riding boots.\n\nHe was foaled in 1991, sired by Royce and out of the mare Amtrak Collins sired by Computer. He is a descendant of the great standardbred racing stallions Albatross, Tar Heel and Adios.\n\n"}
{"id": "34300045", "url": "https://en.wikipedia.org/wiki?curid=34300045", "title": "Sandra Schmitt", "text": "Sandra Schmitt\n\nSandra Schmitt (April 26, 1981 in Mörfelden, West Germany – November 11, 2000 in Kaprun, Austria) was a German freestyle skier. In 1998 she became 9th in the Women's Moguls contest at the 1998 Winter Olympics in Nagano. She became the Women's Dual Moguls World Champion in 1999. Schmitt died with her parents in the Kaprun disaster on 11 November 2000.\n\n"}
{"id": "54438054", "url": "https://en.wikipedia.org/wiki?curid=54438054", "title": "Suna Venter", "text": "Suna Venter\n\nSuna Venter was a South African current affairs journalist, fiction writer, and senior radio producer at the South African Broadcasting Corporation (SABC). She was one of a group of eight journalists at the SABC that were suspended in 2016 for objecting to the editorial policies implemented by SABC COO Hlaudi Motsoeneng known as the SABC8. \n\nMost notably the policy by the SABC of refusing to air protest footage. The policy was reversed following a parliamentary enquiry. \n\nFollowing their suspension the journalists including Venter were the victims of numerous death threats, home break-ins and other forms of intimidation to get them to drop a Constitutional Court case against the SABC. \n\nOn 29 June 2017, Venter was found dead in her home in Kelland, Johannesburg. She was 32 years old. Her death is believed to have resulted from stress induced cardiomyopathy caused by the intimidation attempts against her following her and her colleagues' criticism of Motsoeneng's policies. \n\nThe Inkatha Freedom Party made a statement following her death that she \"was a hero of all times.\" Others condolences came from included those from South African journalists as well as the Ahmed Kathrada Foundation, and Right2Know.\n\nSuna was survived by her mother Christa, father Philip, sister Tessa, and brother Wilhelm. The SAPS is investigating her death, but the investigating officer told the family that her death was caused by heart failure.\n"}
{"id": "11594459", "url": "https://en.wikipedia.org/wiki?curid=11594459", "title": "Timothy Childs", "text": "Timothy Childs\n\nTimothy Childs Jr. (January 1, 1790 – November 8, 1847) was a U.S. Representative from New York.\n\nChilds was born in Pittsfield, Massachusetts on January 1, 1790. He was the son of Rachel (née Easton) Childs (1760–1852) and Timothy Childs (1748–1821), a Revolutionary War officer who studied at Harvard, became a physician and served in the Massachusetts House of Representatives.\n\nHe graduated from Williams College in 1811 and Litchfield Law School in 1814. He completed his studies at the Albany firm of Harmanus Bleecker, afterwards practicing law in New York, first in Canandaigua, and then in Rochester.\n\nOriginally a Federalist, while residing in Canandaigua, Childs served in offices including Ontario County Commissioner and the judicial position of Master in Chancery.\n\nHe served as Monroe County, New York District Attorney from 1821 to 1831, the first to hold this position. He served as a member of the New York State Assembly in 1828, and in the late 1820s he also served as Monroe County Judge.\n\nChilds was elected as an Anti-Mason to the Twenty-first Congress (March 4, 1829 – March 3, 1831). After his term expired he returned to practicing law in Rochester.\n\nIn 1833, he was elected again to the New York State Assembly.\n\nIn 1834, he was elected as an Anti-Jacksonian to the Twenty-fourth Congress. He was reelected as a Whig in 1836, and served from March 4, 1835 to March 3, 1839. During his 1837 to 1839 term Childs was appointed Chairman of the Committee on Expenditures in the Post Office Department.\n\nChilds was elected to Congress again as a Whig in 1840 and served one term, March 4, 1841 to March 3, 1843. He resumed practicing law following the completion of his final term in Congress.\n\nIn the late 1840s, Childs traveled to Saint Croix, where he went in an effort to improve his health. He died aboard the ship \"Emily\" on November 25, 1847 while en route from Saint Croix to the United States. Childs was buried in Pittsfield Cemetery in Pittsfield, Massachusetts.\n\nIn 1817, he married Catherine Adams.\n\nIn December, 1830 he married Louisa Stewart (née Shepherd) Dickinson of North Carolina in a ceremony in Norfolk, Virginia. Louisa was the widow of Joel Dickinson.\n\n"}
{"id": "186406", "url": "https://en.wikipedia.org/wiki?curid=186406", "title": "Toba catastrophe theory", "text": "Toba catastrophe theory\n\nThe Toba supereruption was a supervolcanic eruption that occurred about 75,000 years ago at the site of present-day Lake Toba in Sumatra, Indonesia. It is one of the Earth's largest known eruptions. The Toba catastrophe theory holds that this event caused a global volcanic winter of six to ten years and possibly a 1,000-year-long cooling episode.\n\nIn 1993, science journalist Ann Gibbons posited that a population bottleneck occurred in human evolution about 70,000 years ago, and she suggested that this was caused by the eruption. Geologist Michael R. Rampino of New York University and volcanologist Stephen Self of the University of Hawaii at Manoa support her suggestion. In 1998, the bottleneck theory was further developed by anthropologist Stanley H. Ambrose of the University of Illinois at Urbana–Champaign. Both the link and global winter theories are highly controversial.\nThe Toba event is the most closely studied supereruption.\n\nThe Toba eruption or Toba event occurred at the present location of Lake Toba in Indonesia, about Before Present (BP) according to potassium argon dating. This eruption was the last and largest of four eruptions of Toba during the Quaternary period, and is also recognized from its diagnostic horizon of ashfall, the youngest Toba tuff (YTT). It had an estimated volcanic explosivity index of 8 (the highest rating of any known eruption on Earth); it made a sizable contribution to the 100×30 km caldera complex. Dense-rock equivalent (DRE) estimates of eruptive volume for the eruption vary between and – the most common DRE estimate is (about ) of erupted magma, of which was deposited as ash fall.\n\nThe erupted mass was 100 times greater than that of the largest volcanic eruption in recent history, the 1815 eruption of Mount Tambora in Indonesia, which caused the 1816 \"Year Without a Summer\" in the Northern Hemisphere. Toba's erupted mass deposited an ash layer about thick over the whole of South Asia. A blanket of volcanic ash was also deposited over the Indian Ocean, and the Arabian Sea, and South China Sea. Deep-sea cores retrieved from the South China Sea have extended the known reach of the eruption, suggesting that the calculation of the erupted mass is a minimum value or even an underestimate.\n\nBiologist Michael R. Rampino and volcanologist Stephen Self argue that the eruption caused a \"brief, dramatic cooling or 'volcanic winter'\", which resulted in a drop of the global mean surface temperature by 3–5 °C. Evidence from Greenland ice cores indicates a 1,000-year period of low \"δ\"O and increased dust deposition immediately following the eruption. The eruption may have caused this 1,000-year period of cooler temperatures (stadial), two centuries of which could be accounted for by the persistence of the Toba stratospheric loading. Rampino and Self believe that global cooling was already underway at the time of the eruption, but that the process was slow; YTT \"may have provided the extra 'kick' that caused the climate system to switch from warm to cold states\". Although Clive Oppenheimer rejects the hypothesis that the eruption triggered the last glaciation, he agrees that it may have been responsible for a millennium of cool climate prior to the 19th Dansgaard-Oeschger event.\n\nAccording to Alan Robock, who has also published nuclear winter papers, the Toba eruption did not precipitate the last glacial period. However, assuming an emission of six billion tons of sulphur dioxide, his computer simulations concluded that a maximum global cooling of approximately 15 °C occurred for three years after the eruption, and that this cooling would last for decades, devastating life. Because the saturated adiabatic lapse rate is 4.9 °C/1,000 m for temperatures above freezing, the tree line and the snow line were around 3,000 m (9,900 ft) lower at this time. The climate recovered over a few decades, and Robock found no evidence that the 1,000-year cold period seen in Greenland ice core records had resulted from the Toba eruption. In contrast, Oppenheimer believes that estimates of a drop in surface temperature by 3–5 °C are probably too high, and he suggests that temperatures dropped only by 1 °C. Robock has criticized Oppenheimer's analysis, arguing that it is based on simplistic T-forcing relationships.\n\nDespite these different estimates, scientists agree that a supereruption of the scale at Toba must have led to very extensive ash-fall layers and injection of noxious gases into the atmosphere, with worldwide effects on weather and climate. In addition, the Greenland ice core data display an abrupt climate change around this time, but there is no consensus that the eruption directly generated the 1,000-year cold period seen in Greenland or triggered the last glaciation.\n\nArchaeologists, led by the University of Cambridge's Dr Christine Lane, in 2013, reported finding a microscopic layer of glassy volcanic ash in sediments of Lake Malawi, and definitively linked the ash to the 75,000-year-old Toba super-eruption, but found no change in fossil type close to the ash layer, something that would be expected following a severe volcanic winter. They concluded that the largest known volcanic eruption in the history of the human species did not significantly alter the climate of East Africa, attracting criticism from Richard Roberts. Lane explained, \"We examined smear slides at a 2-mm interval, corresponding to subdecadal resolution, and X-ray fluorescence scans run at 200-µm intervals correspond to subannual resolution. We observed no obvious change in sediment composition or Fe/Ti ratio, suggesting that no thermally driven overturn of the water column occurred following the Toba supereruption.\" In 2015, a new study on the climate of East Africa supported Lane's conclusion, that there was \"no significant cooling associated with Mount Toba\".\n\nThe Toba eruption has been linked to a genetic bottleneck in human evolution about 70,000 years ago, which may have resulted from a severe reduction in the size of the total human population due to the effects of the eruption on the global climate. According to the genetic bottleneck theory, between 50,000 and 100,000 years ago, human populations sharply decreased to 3,000–10,000 surviving individuals. It is supported by some genetic evidence suggesting that today's humans are descended from a very small population of between 1,000 and 10,000 breeding pairs that existed about 70,000 years ago.\n\nProponents of the genetic bottleneck theory (including Robock) suggest that the Toba eruption resulted in a global ecological disaster, including destruction of vegetation along with severe drought in the tropical rainforest belt and in monsoonal regions. For example, a 10-year volcanic winter triggered by the eruption could have largely destroyed the food sources of humans and caused a severe reduction in population sizes. Τhese environmental changes may have generated population bottlenecks in many species, including hominids; this in turn may have accelerated differentiation from within the smaller human population. Therefore, the genetic differences among modern humans may reflect changes within the last 70,000 years, rather than gradual differentiation over hundreds of thousands of years.\n\nOther research has cast doubt on a link between Toba and a genetic bottleneck. For example, ancient stone tools in southern India were found above and below a thick layer of ash from the Toba eruption and were very similar across these layers, suggesting that the dust clouds from the eruption did not wipe out this local population. Additional archaeological evidence from southern and northern India also suggests a lack of evidence for effects of the eruption on local populations, leading the authors of the study to conclude, \"many forms of life survived the supereruption, contrary to other research which has suggested significant animal extinctions and genetic bottlenecks\". However, evidence from pollen analysis has suggested prolonged deforestation in South Asia, and some researchers have suggested that the Toba eruption may have forced humans to adopt new adaptive strategies, which may have permitted them to replace Neanderthals and \"other archaic human species\".\n\nAdditional caveats include difficulties in estimating the global and regional climatic impacts of the eruption and lack of conclusive evidence for the eruption preceding the bottleneck. Furthermore, genetic analysis of Alu sequences across the entire human genome has shown that the effective human population size was less than 26,000 at 1.2 million years ago; possible explanations for the low population size of human ancestors may include repeated population bottlenecks or periodic replacement events from competing \"Homo\" subspecies.\n\nSome evidence points to genetic bottlenecks in other animals in the wake of the Toba eruption. The populations of the Eastern African chimpanzee, Bornean orangutan, central Indian macaque, cheetah, and tiger, all recovered from very low numbers around 70,000–55,000 years ago.\n\nThe separation of the nuclear gene pools of eastern and western lowland gorillas has been estimated to have occurred about 77,700 years ago.\n\nThe exact geographic distribution of human populations at the time of the eruption is not known, and surviving populations may have lived in Africa and subsequently migrated to other parts of the world. Analyses of mitochondrial DNA have estimated that the major migration from Africa occurred 60,000–70,000 years ago, consistent with dating of the Toba eruption to around 75,000 years ago.\n\nA study by Chad Yost and colleagues of cores from Lake Malawi dating to the period of the Toba supereruption showed no evidence of a volcanic winter, and they argue that there was no effect on African humans. In the view of John Hawks, the study confirms evidence from a variety of studies that the eruption did not have a major climatic effect or any effect on human numbers.\n\n\n"}
{"id": "207881", "url": "https://en.wikipedia.org/wiki?curid=207881", "title": "Twelve leverage points", "text": "Twelve leverage points\n\nThe twelve leverage points to intervene in a system were proposed by Donella Meadows, a scientist and system analyst focused on environmental limits to economic growth.\n\nThe leverage points, first published in 1997, were inspired by her attendance at a North American Free Trade Agreement (NAFTA) meeting in the early 1990s where she realized that a very large new system was being proposed but the mechanisms to manage it were ineffective.\n\nMeadows, who worked in the field of systems analysis, proposed a scale of places to intervene in a system. Awareness and manipulation of these levers is an aspect of self-organization and can lead to collective intelligence.\n\nHer observations are often cited in energy economics, green economics and human development theory.\n\nShe started with the observation that there are levers, or places within a complex system (such as a firm, a city, an economy, a living being, an ecosystem, an ecoregion) where a \"small shift in one thing can produce big changes in everything\" (compare: constraint in the sense of Theory of Constraints).\n\nShe claimed we need to know about these \"shifts\", where they are and how to use them. She said most people know where these points are instinctively, but tend to adjust them in the wrong direction. This understanding would help solve global problems such as unemployment, hunger, economic stagnation, pollution, resources depletion, and conservation issues.\n\nMeadows started with a 9-point list of such places, and expanded it to a list of \"twelve leverage points\" with explanation and examples, for systems in general.\n\nShe describes a system as being in a certain state, and containing a stock, with inflows (amounts coming into the system) and outflows (amounts going out of the system). At a given time, the system is in a certain perceived state. There may also be a goal for the system to be in a certain state. The difference between the current state and the goal is the discrepancy.\n\nThe following are in increasing order of effectiveness.\n\nParameters are points of lowest leverage effects. Though they are the most clearly perceived among all leverages, they rarely change behaviors and therefore have little long-term effect.\n\nA buffer's ability to stabilize a system is important when the stock amount is much higher than the potential amount of inflows or outflows. In the lake, the water is the buffer: if there's a lot more of it than inflow/outflow, the system stays stable.\n\nBuffers can improve a system, but they are often physical entities whose size is critical and can't be changed easily.\n\nA system's structure may have enormous effect on operations, but may be difficult or prohibitively expensive to change. Fluctuations, limitations, and bottlenecks may be easier to address.\n\nInformation received too quickly or too late can cause over- or underreaction, even oscillations.\n\nA negative feedback loop slows down a process, tending to promote stability. The loop will keep the stock near the goal, thanks to parameters, accuracy and speed of information feedback, and size of correcting flows.\n\nA positive feedback loop speeds up a process. Meadows indicates that in most cases, it is preferable to slow down a positive loop, rather than speeding up a negative one.\n\nInformation flow is neither a parameter, nor a reinforcing or slowing loop, but a loop that delivers new information. It is cheaper and easier to change information flows than it is to change structure.\n\nPay attention to rules, and to who makes them.\n\nSelf-organization describes a system's ability to change itself by creating new structures, adding new negative and positive feedback loops, promoting new information flows, or making new rules.\n\nChanging goals changes every item listed above: parameters, feedback loops, information and self-organization.\n\nA societal paradigm is an idea, a shared unstated assumption, or a system of thought that is the foundation of complex social structures. Paradigms are very hard to change, but there are no limits to paradigm change. Meadows indicates paradigms might be changed by repeatedly and consistently pointing out anomalies and failures in the current paradigm to those with open minds.\n\nTranscending paradigms may go beyond challenging fundamental assumptions, into the realm of changing the values and priorities that lead to the assumptions, and being able to choose among value sets at will.\n\n\n"}
{"id": "780852", "url": "https://en.wikipedia.org/wiki?curid=780852", "title": "Two Dogmas of Empiricism", "text": "Two Dogmas of Empiricism\n\n\"Two Dogmas of Empiricism\" is a paper by analytic philosopher Willard Van Orman Quine published in 1951. According to City University of New York professor of philosophy Peter Godfrey-Smith, this \"paper [is] sometimes regarded as the most important in all of twentieth-century philosophy\". The paper is an attack on two central aspects of the logical positivists' philosophy. One is the analytic–synthetic distinction between analytic truths and synthetic truths, explained by Quine as truths grounded only in meanings and independent of facts, and truths grounded in facts. The other is reductionism, the theory that each meaningful statement gets its meaning from some logical construction of terms that refers exclusively to immediate experience.\n\n\"Two Dogmas\" has six sections. The first four focus on analyticity, the last two on reductionism. There, Quine turns the focus to the logical positivists' theory of meaning. He also presents his own holistic theory of meaning.\n\nMost of Quine's argument against analyticity in the first four sections is focused on showing that different explanations of analyticity are circular. The main purpose is to show that no satisfactory explanation of analyticity has been given.\n\nQuine begins by making a distinction between two different classes of analytic statements. The first one is called logically true and has the form:\n\nA sentence with that form is true independent of the interpretation of \"man\" and \"married\", so long as the logical particles \"no\", \"un-\" and \"is\" have their ordinary English meaning.\n\nThe statements in the second class have the form:\n\nA statement with this form can be turned into a statement with form (1) by exchanging synonyms with synonyms, in this case \"bachelor\" with \"unmarried man\". It is the second class of statements that lack characterization according to Quine. The notion of the second form of analyticity leans on the notion of synonymy, which Quine believes is in as much need of clarification as analyticity. Most of Quine's following arguments are focused on showing how explanations of synonymy end up being dependent on the notions of analyticity, necessity, or even synonymy itself.\n\nHow do we reduce sentences from the second class to a sentence of the first class? Some might propose \"definitions\". \"No bachelor is married\" can be turned into \"No unmarried man is married\" because \"bachelor\" is defined as \"unmarried man\". But, Quine asks: how do we find out that \"bachelor\" is defined as \"unmarried man\"? Clearly, a dictionary would not solve the problem, as a dictionary is a report of already known synonyms, and thus is dependent on the notion of synonymy, which Quine holds as unexplained.\n\nA second suggestion Quine considers is an explanation of synonymy in terms of interchangeability. Two linguistic forms are (according to this view) synonymous if they are interchangeable in all contexts without changing the truth-value. But consider the following example:\n\nObviously \"bachelor\" and \"unmarried man\" are not interchangeable in that sentence. To exclude that example and some other obvious counterexamples, such as poetic quality, Quine introduces the notion of cognitive synonymy. But does interchangeability hold as an explanation of cognitive synonymy? Suppose we have a language without modal adverbs like \"necessarily\". Such a language would be extensional, in the way that two predicates which are true about the same objects are interchangeable again without altering the truth-value. Thus, there is no assurance that two terms that are interchangeable without the truth-value changing are interchangeable because of meaning, and not because of chance. For example, \"creature with a heart\" and \"creature with kidneys\" share extension.\n\nIn a language with the modal adverb \"necessarily\" the problem is solved, as \"salva veritate\" holds in the following case:\n\nwhile it does not hold for\n\nPresuming that 'creature with a heart' and 'creature with kidneys' have the same extension, they will be interchangeable \"salva veritate\". But this interchangeability rests upon both empirical features of the language itself and the degree to which extension is empirically found to be identical for the two concepts, and not upon the sought for principle of cognitive synonymy.\n\nIt seems that the only way to assert the synonymy is by supposing that the terms 'bachelor' and 'unmarried man' are synonymous and that the sentence \"All and only all bachelors are unmarried men\" is analytic. But for \"salva veritate\" to hold as a definition of something more than extensional agreement, i.e., cognitive synonymy, we need a notion of necessity and thus of analyticity.\n\nSo, from the above example, it can be seen that in order for us to distinguish between analytic and synthetic we must appeal to synonymy; at the same time, we should also understand synonymy with interchangeability \"salva veritate\". However, such a condition to understand synonymy is not enough so we not only argue that the terms should be interchangeable, but necessarily so. And to explain this logical necessity we must appeal to analyticity once again.\n\nAnalyticity would be acceptable if we allowed for the verification theory of meaning: an analytic statement would be one synonymous with a logical truth, which would be an extreme case of meaning where empirical verification is not needed, because it is \"confirmed no matter what\". \"So, if the verification theory can be accepted as an adequate account of statement synonymy, the notion of analyticity is saved after all.\"\n\nThe problem that naturally follows is how statements are to be verified. An empiricist would say that it can only be done using empirical evidence. So some form of reductionism - \"the belief that each meaningful statement is equivalent to some logical construct upon terms which refer to immediate experience\" - must be assumed in order for an empiricist to 'save' the notion of analyticity. Such reductionism, says Quine, presents just as intractable a problem as did analyticity.\n\nIn order to prove that all meaningful statements can be translated into a sense-datum language, a reductionist would surely have to confront \"the task of specifying a sense-datum language and showing how to translate the rest of significant discourse, statement by statement, into it.\" To illustrate the difficulty of doing so, Quine describes Rudolf Carnap's attempt in his book \"Der logische Aufbau der Welt\".\n\nQuine first observes that Carnap's starting point was not the strictest possible, as his \"sense-datum language\" included not only sense-events but also \"the notations of logic, up through higher set theory... Empiricists there are who would boggle at such prodigality.\" Nonetheless, says Quine, Carnap showed great ingenuity in defining sensory concepts \"which, but for his constructions, one would not have dreamed were definable on so slender a basis.\" However, even such admirable efforts left Carnap, by his own admission, far short of completing the whole project.\n\nFinally, Quine objects in principle to Carnap's proposed translation of statements like \"quality q is at point-instant x;y;z;t\" into his sense-datum language, because he does not define the connective \"is at\". Without statements of this kind, it is difficult to see, even in principle, how Carnap's project could have been completed.\n\nThe difficulty that Carnap encountered shows that reductionism is, at best, unproven and very difficult to prove. Until a reductionist can produce an acceptable proof, Quine maintains that reductionism is another \"metaphysical article of faith\".\n\nInstead of reductionism, Quine proposes that it is the whole field of science and not single statements that are verified. All scientific statements are interconnected. Logical laws give the relation between different statements, while they also are statements of the system. This makes talk about the empirical content of a single statement misleading. It also becomes impossible to draw a line between synthetic statements, which depend on experience, and analytic statements, that hold come what may. Any statement can be held as necessarily true according to Quine, if the right changes are made somewhere else in the system. In the same way, no statements are immune to revision.\n\nEven logical laws can be revised according to Quine. Quantum logic, introduced by Garrett Birkhoff and John von Neumann, abandons the law of distributivity from classical logic in order to reconcile some of the apparent inconsistencies of classical Boolean logic with the facts related to measurement and observation in quantum mechanics. Quine makes the case that the empirical study of physics has furnished apparently credible grounds for replacing classical logic by quantum logic, rather as Newtonian physics gave way to Einsteinian physics. The idea that logical laws are not immune to revision in the light of empirical evidence has provoked an intense debate (see \"Is Logic Empirical?\").\n\nAccording to Quine, there are two different results of his reasoning. The first is a blurring of the line between metaphysics and natural science. The common-sense theory about physical objects is epistemologically comparable to the gods of Homer. Quine is a physicalist, in the sense that he considers it a scientific error not to adopt a theory which makes reference to physical objects. However, like Gods of Homer, physical objects are posits, and there is no great epistemic difference in kind; the difference is rather that the theory of physical objects has turned out to be a more efficient theory. As Quine states in \"Two Dogmas\", \"The myth of physical objects is epistemologically superior to most in that it has proved more efficacious than other myths as a device for working a manageable structure into the flux of\nexperience\".\n\nThe second result is a move towards pragmatism. Since, Quine says, the function of science is to predict future experiences in the light of past ones, the only ground for choosing which explanations to believe is \"the degree to which they expedite our dealings with sense experiences.\" While pragmatic concerns are important for Carnap and other logical positivists when choosing a linguistic framework, their pragmatism \"leaves off at the imagined boundary between the analytic and the synthetic\". For Quine, every change in the system of science is, when rational, pragmatic.\n\nRudolf Carnap prepared a reply entitled \"Quine on Analyticity\", but this was not published until 1990. Addressing Quine's concern over the status of the sentence \"Everything green is extended\", Carnap wrote \"the difficulty here lies in the unclarity of the word 'green', namely in an indecision over whether one should use the word for something unextended, i.e., for a single space-time point. In daily life it is never so used, and one scarcely ever speaks of space-time points.\" Carnap then puts forward that an exact artificial language ought to clarify the problem by defining 'green' (or its synonym) as something that is either necessarily or contingently not applied to space-time points. He wrote that once that decision is made, the difficulty is resolved. Carnap also answers Quine's argument on the use of sets of formal sentences to explain analyticity by arguing that this method is an explication of a poorly understood notion.\n\nPaul Grice and P. F. Strawson criticized \"Two Dogmas\" in their (1956) article \"In Defense of a Dogma\". Among other things, they argue that Quine's skepticism about synonyms leads to a skepticism about meaning. If statements can have meanings, then it would make sense to ask \"What does it mean?\". If it makes sense to ask \"What does it mean?\", then synonymy can be defined as follows: Two sentences are synonymous if and only if the true answer of the question \"What does it mean?\" asked of one of them is the true answer to the same question asked of the other. They also draw the conclusion that discussion about correct or incorrect translations would be impossible given Quine's argument. Four years after Grice and Strawson published their paper, Quine's book \"Word and Object\" was released. In the book Quine presented his theory of indeterminacy of translation.\n\nIn \"'Two Dogmas' revisited\", Hilary Putnam argues that Quine is attacking two different notions. Analytic truth defined as a true statement derivable from a tautology by putting synonyms for synonyms is near Kant's account of analytic truth as a truth whose negation is a contradiction. Analytic truth defined as a truth confirmed no matter what however, is closer to one of the traditional accounts of \"a priori\". While the first four sections of Quine's paper concern analyticity, the last two concern apriority. Putnam considers the argument in the two last sections as independent of the first four, and at the same time as Putnam criticizes Quine, he also emphasizes his historical importance as the first top rank philosopher to both reject the notion of apriority and sketch a methodology without it.\n\nJerrold Katz, a onetime associate of Noam Chomsky's, countered the arguments of \"Two Dogmas\" directly by trying to define analyticity non-circularly on the syntactical features of sentences.\n\nIn his book \"Philosophical Analysis in the Twentieth Century, Volume 1 : The Dawn of Analysis\" Scott Soames (pp 360–361) has pointed out that Quine's circularity argument needs two of the logical positivists' central theses to be effective:\n\nIt is only when these two theses are accepted that Quine's argument holds. It is not a problem that the notion of necessity is presupposed by the notion of analyticity if necessity can be explained without analyticity. According to Soames, both theses were accepted by most philosophers when Quine published \"Two Dogmas\". Today however, Soames holds both statements to be antiquated.\n\n\n\n"}
{"id": "10797725", "url": "https://en.wikipedia.org/wiki?curid=10797725", "title": "William Luard", "text": "William Luard\n\nAdmiral Sir William Garnham Luard KCB (7 April 1820 – 19 May 1910) was a leading British naval figure during the latter half of the 19th century.\n\nBorn in 1820, he was the eldest son of a local magistrate, William Wright Luard J.P., D.L. of Witham Lodge, Witham, Essex (formerly of Hatfield Peverel Priory) and Anne Garnham, only child of Thomas Garnham of Felsham Hall, Suffolk. The Luards were a prominent family of Protestant Huguenot merchants who had fled to England in the late 17th century as part of the mass exodus of Huguenots from France to England that followed the 1685 revocation of the Edict of Nantes.\n\nLuard entered the Royal Naval College (formerly the Royal Naval Academy) at Portsmouth at the age of 13 and later studied at Portsmouth Naval College. He served extensively and saw action in the South China Sea, for which he was recognized in dispatches and decorated for gallantry and bravery several times including being named Companion of the Order of the Bath (CB).\n\nAfter a distinguished career as a naval officer, including as captain and commander of and HMS \"Conqueror\", he served as superintendent of the Sheerness Dockyard and the Malta Dockyard. From 1882 to 1885, he was President of the Royal Naval College, Greenwich.\n\nHe was awarded the Burmese War Medal for dispersing the pirates of Chin-a-poo and received the Medal of the Legion of Honour, 4th Class, from Emperor Napoleon III. He was promoted Rear-Admiral in 1875, Vice-Admiral on 15 June 1879, and Admiral in 1885. \nLuard was advanced to KCB by Queen Victoria in 1897, during her diamond jubilee year.\n\nLuard married Charlotte Du Cane (an anglicization of the original French surname 'Du Quesne') in 1858. She was from another French Huguenot family (see Jean Du Quesne, the elder and descendants), with landed estates at Braxted Park and Coggeshall. Admiral and Lady Luard had 11 children.\n\nA staunch Liberal and supporter of Prime Minister William Gladstone, Luard retired to his estate in Essex where he served as a Justice of the Peace and as an active member of the court of Quarter Sessions. He died in 1910 as a result of injuries sustained in a carriage accident.\n"}
