{"id": "626067", "url": "https://en.wikipedia.org/wiki?curid=626067", "title": "Abraham Van Helsing", "text": "Abraham Van Helsing\n\nProfessor Abraham Van Helsing is a fictional character from the 1897 gothic horror novel \"Dracula\". Van Helsing is an aged polymath Dutch doctor with a wide range of interests and accomplishments, partly attested by the string of letters that follows his name: \"MD, D.Ph., D.Litt., etc, etc,\" indicating a wealth of experience, education and expertise. The character is best known throughout many adaptations of the story as a vampire hunter and the archenemy of Count Dracula.\n\nIn the novel, Professor Van Helsing is called in by his former student, Dr. John Seward, to assist with the mysterious illness of Lucy Westenra. Van Helsing's friendship with Seward is based in part upon an unknown prior event in which Van Helsing suffered a grievous wound, and Seward saved his life by sucking out the gangrene. It is Van Helsing who first realizes that Lucy is the victim of a vampire, and he guides Dr. Seward and his friends in their efforts to save Lucy.\n\nAccording to Leonard Wolf's annotations to the novel, Van Helsing had a son who died. Van Helsing says that his son, had he lived, would have had a similar appearance to Lucy's husband Arthur Holmwood. Consequently, Van Helsing developed a particular fondness for Holmwood. Van Helsing's wife went insane after their son's death, but as a Catholic, he refuses to divorce her (\"with my poor wife dead to me, but alive by Church's law, though no wits, all gone, even I, who am faithful husband to this now-no-wife\").\n\nVan Helsing is one of the few characters in the novel who is fully physically described in one place. In chapter 14, Mina Harker describes him as:\nVan Helsing's personality is described by John Seward, his former student, thus:\nIn the novel Van Helsing is described as having what is apparently a thick foreign accent, in that he speaks in broken English and he uses German phrases like, \"Mein Gott\" ().\n\nAdaptations of the novel have tended to play up Van Helsing's role as a vampire expert, sometimes to the extent that it is depicted as his major occupation. In the novel, however, Dr. Seward requests Van Helsing's assistance simply because Lucy's affliction has him baffled and Van Helsing \"knows as much about obscure diseases as any one in the world\", and Van Helsing does not determine that a vampire is the cause in time to prevent Lucy's death.\n\nCount Dracula, having acquired ownership of England's Carfax estate through solicitor Jonathan Harker, moved to the estate and began menacing England. His victims included Lucy Westenra, who is on holiday in Whitby. The aristocratic girl has suitors such as Dr. John Seward, Arthur Holmwood, and Quincey Morris, and has a best friend in Mina Murray, Harker's fiancée. Seward, who worked as a doctor in an insane asylum – where one of the patients, the incurably mad Renfield, has a psychic connection to Dracula – contacts Professor Van Helsing about Lucy's peculiar condition. Van Helsing, recognizing marks upon her neck, eventually deduces that she has been losing blood from a vampire bite. He administers multiple blood transfusions, Van Helsing, Seward, and Arthur each donating blood to her, but each night she continues to lose blood. He prescribes her garlic, makes a necklace of garlic flowers for her and hangs garlic about her room. He also gives her a crucifix to wear around her neck. Lucy's demise was brought by her mother who cleared the room of garlic and opened the window for fresh air; a servant had stolen the gold crucifix. Lucy dies and after the funeral returns as a vampire, seeking out children. Eventually, Van Helsing, Arthur, Morris and Seward free the undead Lucy from her vampiric curse: Arthur uses a hammer to drive the stake through her heart and Van Helsing cuts off her head and puts garlic in her mouth.\n\nMina, now married to Harker, becomes increasingly worried about his brain fever. Van Helsing reviews his journal and Harker's health returns when he learns that his experiences in Transylvania were real. Mina discovers that various letters and accounts provide further intelligence on Dracula's movements, and shares these with Harker, Seward, Morris, and Van Helsing. They learn that Dracula's residence in Carfax was near Seward's, and Van Helsing's research reveals Dracula's weaknesses and strengths. Seward and Van Helsing also write to a university acquaintance to aid in further research. Staying at Seward's residence to better plan strategies in their efforts to deal with Dracula, they have frequent meetings and each member is assigned duties. At a later meeting a bat is seen at a window.\n\nTo destroy Dracula and prevent further spread of evil, the party enter his estate in Carfax and as a group encounter him for the first time. They discover that he has been purchasing properties in and around London, with plans to distribute 50 boxes of Transylvanian earth to them, used as graves so each property would become a safe lair. They visit these lairs and place sacramental bread in the boxes of earth to \"sterilize\" them, preventing Dracula from further using them. Dracula entices Renfield to invite him into Seward's residence. Renfield is found critically injured by Seward and Van Helsing who operate on him, and Renfield informs them that Dracula went to see Mina. They go to Mina's room and find Harker hypnotized while Dracula is giving Mina the 'Vampire's Baptism of Blood', cursing her and the group for plotting against him. The party use sacred items to repel Dracula, who flees into a different room as a vapour. Dracula then destroys all the texts Mina had produced, except for one which was hidden, and breaks Renfield's neck before leaving.\n\nVan Helsing places a wafer of sacramental bread upon Mina's forehead to bless her but it burns her flesh, leaving a scar. Mina, feeling that she is now connected with Dracula, asks Van Helsing to hypnotize her before dawn, the only time she feels she could freely speak. Through this hypnosis they learn that Mina has a telepathic link with Dracula, that she could tell everything he hears and feels, which could be used to track his movements. Mina agrees that any plans should be kept from her for fear that Dracula could read her thoughts. The group have additional encounters with Dracula as they continue to search for his residences throughout London and sterilize the boxes. Learning that his final grave is aboard a boat, Van Helsing deduces that Dracula is fleeing back to his castle.\n\nWhen the party pursues Dracula to Transylvania, they split into groups. While Mina and Van Helsing travel straight to Dracula's castle, the others attempt to ambush the boat on which Dracula is a passenger. Van Helsing's influence over Mina diminishes each day, and her behavior changes as she sleeps more during the day, loses her appetite for food, and ceases to write in her journal. He finds that she cannot cross a circle of crumbled sacramental bread. Later, Dracula's vampiric wives approach their camp but they too are unable to cross into the circle of bread. Failing at their attempts to lure Van Helsing and Mina out of the circle, they flee back to Dracula's castle just before sunrise. Van Helsing binds Mina at a cave to keep her from danger as he goes into Dracula's castle to kill the vampires.\n\nAs Van Helsing runs through the castle searching its rooms, he finds Dracula's empty tomb and the three female vampires he saw earlier. He begins to do his operation on the first vampire but finds himself entranced by her beauty and unable to bring himself to harm her. In his feelings of enchantment he even contemplates love for her. He is broken out of this enchantment when he hears a \"soul wail\" from Mina, awakening him. He proceeds to drive stakes into their hearts and sever their heads, one by one.\n\nVan Helsing returns to Mina and they see the rest of their party as they chase a group of gypsies down the Borgo Pass and corner them. Armed with knives and firearms they overtake the gypsies and open the final box of Dracula; Jonathan Harker brings his Kukri knife down on Dracula's throat as the bowie knife of Quincey Morris simultaneously impales Dracula's heart in the final moments of daylight. At this moment Dracula's body crumbles to dust. After the struggle, Quincey is seen to have been fatally wounded.\n\nSix years later, Van Helsing takes a grandfatherly role in regard to the young Quincey Harker, Jonathan and Mina's son.\n\nVan Helsing is seen utilizing many tools to aid him and his party in fending off Dracula, warding off vampires and in general defeating the undead: \n\n\"Nosferatu, a Symphony of Horror\" (1922) was the first film version of \"Dracula\". Although it followed the same basic plot as the novel, names were changed: Van Helsing is 'Professor Bulwer' (John Gottowt) and appears only in a few scenes. Unlike the book, he is a friend of 'Thomas Hutter' (the film's version of Jonathan Harker) before he meets 'Count Orlok' (a renamed Count Dracula), and he never meets the vampire face to face.\n\nIn the initial 1931 Universal version of \"Dracula\" starring Bela Lugosi as the Count, Professor Van Helsing was portrayed by the actor Edward Van Sloan, who had previously played the part opposite Lugosi on stage. Van Sloan was the only cast member to return to his role for the 1936 sequel \"Dracula's Daughter\".\n\nPeter Cushing's Doctor Van Helsing in the initial 1958 Hammer \"Dracula\" movie and its 1960 sequel \"The Brides of Dracula\" differed from the novel's in that the actor portrayed the character as a visibly younger man (as did Christopher Lee as the Count), and also one whose main vocation appears to be vampire hunting. His first name is never mentioned, though in the later Hammer Dracula films set in the 1970s (which apparently exist on a different timeline) Lee's Dracula battles Cushing's 'Lorrimer Van Helsing', a grandson of a previous vampire hunter, who appears as 'Lawrence Van Helsing' (also Cushing) in the prologue to \"Dracula A.D. 1972\". In the final Hammer Dracula production \"The Legend of the 7 Golden Vampires\" (set mainly in 1904) Cushing again plays the original Van Helsing from the Hammer series.\n\nJohn Badham's 1979 version of \"Dracula\" saw Laurence Olivier portray Van Helsing as a frail old man whose daughter becomes one of the undead, drawing him into a conflict with Frank Langella's Count that eventually costs the Professor his life.\n\nAnthony Hopkins portrayed Professor Van Helsing in \"Bram Stoker's Dracula\" in 1992. Once again implied to be an experienced vampire hunter, Hopkins' Van Helsing often comes across as unbalanced, casually discussing how to kill the un-dead despite the brutal methods involved, and displaying ruthless methods when dispatching Dracula's brides, but he nevertheless leads the group to victory over Dracula's forces.\n\nChristopher Plummer portrayed Van Helsing in \"Dracula 2000\" (he had previously appeared as a vampire hunter, Professor Paris Catalano, in \"Vampire in Venice\"). After defeating Count Dracula (Gerard Butler), Van Helsing finds that the vampire lord cannot die by the conventional means of destroying a vampire and he only succeeded in paralysing him in a deathlike state. When Dracula escapes after his coffin is stolen, Van Helsing's daughter and his assistant are able to defeat him after the elder Van Helsing's death.\n\nHugh Jackman played Gabriel Van Helsing, in \"Van Helsing\" (2004), loosely based on Stoker's character. Having been found on the steps of a church several years before with total amnesia, Gabriel hunts monsters for a secret organization made up of the world's religions (known as the Knights of the Holy Order) to rid the world of evil \"that the rest of mankind has no idea exists\", although he is the most wanted man in Europe for his conspicuous actions. In the movie he is sent to Transylvania to kill Count Dracula. When he arrives, Dracula tells Gabriel that they have already met and have quite a history together, with Dracula revealing over the course of the film that Van Helsing was the one who originally murdered him, as well as claiming ownership of a distinctive ring that Van Helsing has worn as long as he can remember. It is implied that Gabriel is actually the angel Gabriel, with vague references to Dracula's murderer as the \"Left Hand of God\".\nJackman also voiced the character in the animated spin-off \"\" (2004).\n\nOther actors to have portrayed Van Helsing in film and television adaptations of \"Dracula\" include:\n\n\nVan Helsing appears in the Book, Dracula vs Hitler, as a main character\n\nIn the Big Finish Productions audio drama \"The Tangled Skein\" (adapted from the novella by David Stuart Davies) Van Helsing, acting alone, joins forces with Sherlock Holmes and Dr. Watson to investigate the presence of vampires in London. Although initially disbelieving of Van Helsing's convictions when they read about his lectures on vampires, the duo accept his word when they are confronted and nearly killed by a vampire in Hampstead Heath, joining Van Helsing in staking the vampire. While Van Helsing is forced to focus on his lectures over the next few days, he leaves his vampire-hunting equipment with Holmes and Watson, who track Dracula to Dartmoor, where he has hidden in Baskerville Hall, and are able to defeat him using Van Helsing's advice. After Dracula is defeated when trapped in the Grimpen Mire as the sun rises, Holmes gives Van Helsing Dracula's ring as a memento of their victory, concluding that Van Helsing's research is what enabled him to destroy Dracula.\n\nAbraham Van Helsing was also portrayed in \"The Tomb of Dracula\" Marvel Comics series, which was based on the characters of Bram Stoker's novel.\n\nIn the Marvel Comics miniseries \"\", Van Helsing joins forces with the immortal mutant Apocalypse and his worshipers, Clan Akkaba, in order to destroy Dracula, their common enemy. It is noted that Van Helsing had encountered Apocalypse before and previously believed him to be a vampire.\n\nIn the Italian comic book \"Martin Mystère\" and the spin-off series \"Storie di Altrove/Stories from Elsewhere\" Van Helsing's name is Richard. He was originally a knight in the service of the Holy Roman Emperors but he was captured in 1475 by the undead warriors of the Order of the Dragon and turned into a vampire by the Wallachian Prince Vlad Dracula. Four centuries later, Van Helsing killed Dracula, and later came to London to solve the case of Jack the Ripper, eventually discovering that the murderers were mentally controlled by demons from another world. In 1902 he worked together with the resurrected Dracula to prevent the assassination of King Edward VII.\n\nThere have been numerous works of fiction depicting descendants of Van Helsing carrying on the family tradition.\n\n\n\n\n\n\n"}
{"id": "16945353", "url": "https://en.wikipedia.org/wiki?curid=16945353", "title": "Agent 355", "text": "Agent 355\n\nAgent 355 (died after 1780) was the code name of a female spy during the American Revolution, part of the Culper Ring. Agent 355 was one of the first spies for the United States, but her real identity is unknown. The number, 355, could be de-crypted from the system the Culper Ring used to mean \"lady.\" \n\nThe only direct reference to Agent 355 in any of the Culper Ring's missives was from Abraham Woodhull (\"Samuel Culper Sr.\"), to General George Washington. Woodhull described her as \"one who hath been ever serviceable to this correspondence.\"\n\nThe true identity of Agent 355 is still unknown, but some facts about her seem clear. She worked with the American Patriots during the Revolutionary War as a spy. She would have been recruited by Woodhull into the spy ring. The way the code is constructed indicates that she may have had \"some degree of social prominence.\" She was likely living in New York City, and at some point had contact with Major John Andre and Benedict Arnold. One person who may have been Agent 355 was Anna Strong. She was Woodhull's neighbor. Another theory is that Agent 355 may have been Robert Townsend's common-law wife. Stories about Townsend state that he was in love with Agent 355. John Burke and Andrea Meyer have made a different case for 355's involvement in the spy ring using circumstantial evidence that she may have been close to Major John André and also to Benjamin Tallmadge, thereby protecting Woodhull from accusations of being a spy. Other possible candidates for 355 include Sarah Horton Townsend and Elizabeth Burgin.\n\nIt is also occasionally believed that there was no Agent 355, but rather that the code indicated a woman who had useful information, but wasn't \"formally connected to the ring.\" The code itself may have referred to \"a woman,\" not an agent who was a woman.\n\nAgent 355 is thought to have played a major role in exposing Arnold and the arrest of Major John André, who was hanged in Tappan, New York. She may have been member of a prominent Loyalist family which would put her within easy reach of British commanders.\n\nAgent 355 was arrested in 1780 when Benedict Arnold defected to the Loyalists. She was imprisoned on , a prison ship, where she may have given birth to a boy named Robert Townsend, Jr. She later died on the prison ship. However, Alexander Rose disagrees with this narrative, stating that \"females were not kept aboard prison ships,\" and that \"there's no record whatsoever of a birth.\" Strengthening the idea that Agent 355 may have been Anna Strong is the fact that her husband Selah was imprisoned on \"Jersey\" and she was supposedly allowed to bring him food. Her presence on the ship may have led to the legend that Agent 355 was imprisoned there.\n\nAgent 355 has become a part of popular fiction. She is the namesake of one of the main characters in \"\" from Vertigo Comics. She was also an inspiration for a modern spy also called 355, and being included in Assassins Creed 3's in-game database of historical facts, in which her information was critical in helping the protagonist in discovering the British plot to capture.\n\nIn the television series \"\", Agent 355 is a former slave named Abigail who had been owned by Anna Strong until the British army seized Selah's property upon his imprisonment. Though technically free, she is coerced into working for John André. Abigail sends Anna information she overhears in André's home hidden in gifts to her son, whom she was forced to leave behind in Anna's care. She is played by Idara Victor.\n\n\n"}
{"id": "535639", "url": "https://en.wikipedia.org/wiki?curid=535639", "title": "Alexey Troitsky", "text": "Alexey Troitsky\n\nAlexey Alexeyevich Troitsky, or Alexei, Troitzky, or Troitzki () (March 14, 1866–August 1942) is considered to have been one of the greatest composers of chess endgame studies. He is widely regarded as the founder of the modern art of composing chess studies . Troitsky died of starvation during World War II at the siege of Leningrad, where his notes were destroyed. \n\nOne of his most famous works involves analyzing the endgame with two knights versus a pawn, see Troitsky line. John Nunn analyzed this endgame with an endgame tablebase and stated that \"the analysis of Troitsky ... is astonishingly accurate\" .\n\nTroitsky was a prolific composer of endgame studies. Irving Chernev included nine of them in his book \"200 Brilliant Endgames\". The diagram shows one of them.\nThe main line goes:\nand White wins .\n\n\n\n"}
{"id": "32943476", "url": "https://en.wikipedia.org/wiki?curid=32943476", "title": "Attina Marie Cannaday", "text": "Attina Marie Cannaday\n\nAttina Marie Cannaday (born September 8, 1965) was charged with robbery, kidnapping, and homicide. At the time of her trial, she was a sixteen-year-old divorcee, who had married at thirteen and divorced at fourteen. She was convicted in Harrison County Circuit Court of the kidnap and murder of U.S. Air Force Sergeant Ronald Wojcik and the jury sentenced her to death by lethal injection. The guilty verdict was upheld, but the sentence was reversed in 1984, Cannaday v. State, 455 So.2d 713, 720 (Miss. 1984), and she was resentenced to one life sentence and two 25-year sentences at Central Mississippi Correctional Facility (inmate number 42451). She was released on parole on March 9, 2008.\n\nThe movie \"Too Young to Die?\" was very loosely based on her crimes.\n\n"}
{"id": "55327824", "url": "https://en.wikipedia.org/wiki?curid=55327824", "title": "Bernal de Bonaval", "text": "Bernal de Bonaval\n\nBernal(do) de Bonaval(le), also known as Bernardo (de) Bonaval, was a 13th-century troubadour in the Kingdom of Galicia (in the northwest of the Iberian Peninsula, in parts of modern Portugal and Spain) who wrote in the Galician-Portuguese language.\n\nLittle is known for certain about Bernal's background, life, or career.\n\nSources say that he was a native of Santiago de Compostela, which is in the modern Spanish Province of A Coruña. He mentions a place called \"Bonaval\" in several of his poems. It has been suggested that he was born outside the mediaeval city walls of Santiago, because \"de Bonaval\" may refer to the Convent of San Domingos de Bonaval, which is outside those walls. It has also been suggested that \"Bernal de Bonaval\" and (in Latin) \"Frater Bernardus, prior Bone Uallis\" (\"Brother Bernardus, prior of Bone Uallis\") may have been one and the same. If that suggestion is correct, then Bernal may have been a friar in the Dominican Order, and \"de Bonaval\" may refer to the convent rather than to his birthplace.\n\nHe was active in the 13th century. Some sources suggest that he may have been born in the 12th century. He was known at the courts of Fernando III and Alfonso X (kings of Galicia 1231-1252 and 1252-1284 respectively).\n\nA poem of 1266 by King Alfonso X directed at the troubadour mentions Bernal: \"Vós nom trobades come proençal, / mais come Bernaldo de Bonaval; / por ende nom é trobar natural / pois que o del e do dem'aprendestes\" (\"You do not compose like a Provençal / but like Bernaldo de Bonaval / and therefore your poetry-making is not natural / for you learned it from him and from the [D]evil\"). Bernal was also mentioned in verse by the troubadours , João Baveca and Pedro (Pero) da Ponte.\n\nIt has been suggested in recent times by one author that Bernal may have had a reputation as a passive homosexual, and may have been the same man as the one nicknamed \"Bernal Fundado\" (i.e. \"Bernal the Split\").\n\nHe is one of the earliest known \"xograres\" or \"segreis\" (Galician troubadours). Nineteen of his works have survived: ten \"\" (on the theme of courtly love), eight \"cantigas de amigo\", and one \"tensón\". He introduced popular motifs and realistic features into what had been a scholastic form of poetry. He has been called \"Villonesque\", even though François Villon lived two centuries later.\n\nHis songs have been preserved in the \"Cancioneiro da Vaticana\" (CV 660) and the \"Cancioneiro da Biblioteca Nacional\" (CBN 1003).\n\nRúa de Bernal de Bonaval (a street) in Santiago de Compostela is named after him. In 1961, Brazilian scholar ranked him among the principal troubadours. The 1971 album \"Cantigas de Amigos\" includes a duet between Portuguese artists Amália Rodrigues and Ary Dos Santos called \"Vem esperar meu amigo\". It is a version of Bernal's \"cantiga de amigo\" \"Ai, fremosinha, se ben ajades\", named from its refrain rather than from its first line. Spanish musician included his version of Bernal's \"A dona que eu amo\" on his 1984 album \"Leliadoura\". In 1985, Portuguese scholar Ribeiro Miranda published an academic paper analysing Bernal's importance. In 1994, Galician writer Castelao named Bernal among the notable Galicians. In 2012, Galician scholar Souto Cabo called him \"uma das personalides poéticas mais célebres dos nossos cancioneros\" (\"one of the most famous poets in our songbooks\").\n\n\n"}
{"id": "52417056", "url": "https://en.wikipedia.org/wiki?curid=52417056", "title": "Besmilr Brigham", "text": "Besmilr Brigham\n\nBesmilr Brigham (September 28, 1913 - 2000) was an American poet and writer of short stories.\n\nBrigham was born Bess Miller Moore in Pace, Mississippi. She graduated from Mary Hardin-Baylor College (now University of Mary Hardin–Baylor) in Belton, Texas. After that, she studied at the New School for Social Research in New York. In New York she met and married Roy Brigham.\n\nAccording to the \"Encyclopedia of Arkansas History & Culture\", \"She came to prominence during the women’s movement of the 1960s, and her work is noted for its innovative structure, sound, and rhythm.\"\n\nBrigham is also known as Besmilr Moore Brigham. The Besmilr Women Writers Award is named after her.\n\nBrigham died of complications from Alzheimer's disease in 2000.\n\n\n"}
{"id": "17290783", "url": "https://en.wikipedia.org/wiki?curid=17290783", "title": "Bidirectional associative memory", "text": "Bidirectional associative memory\n\nBidirectional associative memory (BAM) is a type of recurrent neural network. BAM was introduced by Bart Kosko in 1988. There are two types of associative memory, auto-associative and hetero-associative. BAM is hetero-associative, meaning given a pattern it can return another pattern which is potentially of a different size. It is similar to the Hopfield network in that they are both forms of associative memory. However, Hopfield nets return patterns of the same size.\n\nA BAM contains two layers of neurons, which we shall denote X and Y. Layers X and Y are fully connected to each other. Once the weights have been established, input into layer X presents the pattern in layer Y, and vice versa\n\nImagine we wish to store two associations, A1:B1 and A2:B2.\n\nThese are then transformed into the bipolar forms:\n\nFrom there, we calculate formula_1 where formula_2 denotes the transpose.\nSo,\n\nformula_3\n\nTo retrieve the association A1, we multiply it by M to get (4, 2, -2, -4), which, when run through a threshold, yields (1, 1, 0, 0), which is B1.\nTo find the reverse association, multiply this by the transpose of M.\n\nThe internal matrix has n x p independent degrees of freedom, where n is the dimension of the first vector (6 in this example) and p is the dimension of the second vector (4). This allows the BAM to be able to reliably store and recall a total of up to min(n,p) independent vector pairs, or min(6,4) = 4 in this example. The capacity can be increased above 4 if one gives up reliability and is willing to accept incorrect bits on the output.\n\n\n"}
{"id": "12883588", "url": "https://en.wikipedia.org/wiki?curid=12883588", "title": "Bird collections", "text": "Bird collections\n\nBird collections are curated repositories of scientific specimens consisting of birds and their parts. They are a research resource for ornithology, the science of birds, and for other scientific disciplines in which information about birds is useful. These collections are archives of avian diversity and serve the diverse needs of scientific researchers, artists, and educators.\nCollections may include a variety of preparation types emphasizing preservation of feathers, skeletons, soft tissues, or (increasingly) some combination thereof. Modern collections range in size from small teaching collections, such as one might find at a nature reserve visitor center or small college, to large research collections of the world's major natural history museums, the\nlargest of which contain hundreds of thousands of specimens. Bird collections function much like libraries, with specimens arranged in drawers and cabinets in taxonomic order, curated by scientists who oversee the maintenance, use, and growth of collections and make them available for study through visits or loans.\n\nThe roots of modern bird collections are found in the 18th- and 19th-century explorations of Europeans intent on documenting global plant and animal diversity. It was a fashion to\ncollect and display \"natural curiosities\" in Victorian England. Some wealthy \"cabinet naturalists\" were able to amass large collections using networks of field collectors. These early collections were not intended for scientific study and the collectors gave importance to aesthetics rather than scientific value. It grew into a more scientific pursuit much later.\n\nEarly scientific bird collections included those belonging to Pallas and Naumann in Germany, Latham and Tunstall in England and Adanson in France. Collections grew in size with increasing maritime activity, exploration and colonialism. For example, Charles Darwin collected over 400 bird specimens during his travels on the \"Beagle\", and it was many years after his return to England that his bird collections from the Galapagos inspired (in part) his theory of evolution through natural selection. The Paris museum had 463 bird specimens in 1793 and this grew to 3411 in 1809; The Berlin museum had 2000 specimens in 1813 growing to 13,760 around 1850. In 1753 there were 1172 bird specimens in the museum established by Sir Hans Sloane but these appear to have perished before they moved to the British Museum. Early specimens from Captain Cook's voyages as well as those described by Latham in his \"General Synopsis of Birds\" (1781–1785) were also lost possibly due to poor preservation technique. The scale of collections grew to the point where they needed more space and full-time curators. In the earliest days of ornithology, collecting was the dominant method of bird observation and study. This approach has diminished with the growth of the discipline. The use of mist-netting and photography, blood sampling (for DNA, immunological and other studies), the development of optics and the use of other new techniques for studying birds have reduced the need to collect specimens for research, yet collections continue to act as a vital shared resource for science (particularly taxonomy) and conservation. In an era of mass extinction, bird collections will evidence lost species.\n\nHistorically, bird specimens were collected mostly using firearms. Shotguns with \"dust\" shot were preferred to reduce damage to the specimens. Today, specimens come from a variety of sources. Many (perhaps most) are salvaged from birds killed by window and communications tower strikes, domestic cats, by-catch from fisheries, die-offs from disease, vehicle strikes, and other accidental sources of mortality. However, the world's bird collections have been argued to be inadequate in documenting avian diversity, from taxonomic, geographic, and temporal perspectives, with some parts of tropical regions considered under-represented in particular museums. Underrepresented taxa continue to be actively collected by ornithologists, generally using either firearms or mist-nets. Permitting agencies oversee these activities in most countries.\n\nTechniques to preserve birds were attempted even from the early 16th century as shown in the writings of Conrad Gesner and Pierre Belon. Belon provided instructions on the removal of viscera and the use of salt to preserve bird specimens in his 1555 book on birds. These were further improved in the 17th century and a range of preservatives included ash (potassium carbonate), salt, sulphur, alum, alcohol and various plant extracts were used. In the early days of bird collections, most specimens were mounted in unrealistic positions often with their wings raised as if they were about to take flight. These were kept in the open and the colours were prone to fading and the specimens themselves prone to damage by beetles. In Berlin, J. L. Frisch started using tightly enclosed glass jars for every mount to prevent pest damage. During this time, Comte de Reaumur at the Paris Museum had managed to find techniques to preserve specimens dry and without loss of colour. This technique was however a secret and similar results were later achieved by pickling using salt, ground pepper and alum and drying for a month with threads holding the bird in a natural position. The use of arsenic to preserve specimens was first introduced by Jean-Baptiste Bécoeur (1718-1777) but this method was publicly revealed only in 1800 by Louis Dufresne in Daudin's \"Traité Élémentaire et Complet d’Ornithologie\" (1800). In modern collections, salvaged or collected birds may be preserved in a number of ways. The most traditional preparation is a study skin, in which almost all of the body inside the skin is removed and replaced with cotton so that the final result resembles a bird lying on its back with its wings folded. Borax is used as the preferred preservative as it is low in toxicity. This stereotypic posture was developed to enable many skins to be kept together in cabinets to protect them from insect and light damage. If a complete skeleton is desired, a flat skin may be prepared: all bones, muscle, digestive and other soft tissue is carefully removed and the feathers and skin are stretched flat and dried.\n\nA more recent preparation method pioneered by the Royal Ontario Museum removes all bones for a complete skeleton while also producing a round skin without bill or legs (called a ROM, though if one set of wing and leg bones remain with the skin the preparation is called a \"shmoo\" in North America). Alternatively, the entire bird (or any soft parts associated with preparations described above) may be preserved in alcohol. For any of these methods, several supplemental preparations may be made. For example, a wing may be removed and preserved separately as a spread wing for better study of flight feathers; a tissue sample may be removed and frozen for molecular analyses; or a recording of the bird's song before collection may be archived. Neither molecular samples nor sound recordings require a bird to be collected (killed). Finally, if the bird is too rotten for the skin and feathers to be preserved, as is the case with some salvaged specimens, the skeleton alone may be preserved. Dried tissue is removed from skeletons by using dermestid beetle larvae (genus \"Dermestes\"). Whereas in the past arsenic was routinely added to skins to protect them from destruction by insects, specimens prepared today are generally protected by an initial freezing period to kill insects and their eggs followed by keeping them in high-quality museum cases in a climate-controlled room. Each specimen has data associated with it, and the amount of data available is usually directly correlated with the specimen's scientific value. Most specimens are of little value for research without accompanying information, such as the time and place the bird was found or collected. This and other important information, such as mass, sex, fat deposition, and degree of skull ossification, is written on a label along with a unique field and museum number. Modern computerized museum databases include all of this information for each specimen, as well as the types of methods used to prepare the bird. Modern collections seek to maximize the utility of each preserved individual, and this includes recording detailed information about it. Most modern specimens also include a tissue sample preserved for genetic study. Online access to collections' data is becoming increasingly available, and a cross-institutional database covering millions of computerized bird records is in development.\n\nBird collections are used for a wide variety of purposes. All biological species including those of birds are represented by a holotype, the vast majority of which are full specimens (mostly skins) and in modern times explicitly designated in the original description of the taxon. All other putative members of the species may be compared to the holotype to confirm their identification. Rigorous studies of avian taxonomy are based on specimens from bird collections. Taxonomic studies rely on morphological and genetic characters to determine species limits and evolutionary relationships. Museum specimens have been the preferred source for scoring these characteristics, as they allow studies to be replicated – anyone may go back and repeat the study using the same specimens to verify the conclusions. However, it has alternatively been argued that such re-examination can be undertaken from archived photographs without killing the study piece.\n\nIn the case of molecular studies, the preservation of a specimen that can vouch for the source of the tissue sample used to gather genetic data has been recommended, as genetic analysis often yields surprising results that make reexamination of the original specimen crucial.\n\nStudies on ectoparasites, usually obtained during capture, but also obtained from old museum specimens, are valuable for studies on coevolution and zoonoses.\n\nIn addition to taxonomic research, collections can provide information relevant to the study of variety of other ornithological questions, including comparative anatomy, ecology, behavior, disease, and conservation. Forensic ornithologists use collections to identify species involved in aircraft bird strikes, imported materials containing bird parts, and birds killed through various human activities, legal and illegal. In addition, collections are used by zooarchaeologists to identify bird bones at prehistoric human sites or species of origin for feathers used in human cultural artifacts. Collections also have been heavily used by artists, particularly for the production of plates for ornithological field guides. The close-up observation and opportunity for manipulation provided by preserved study skins makes them, together with field observations and photography, to be an important basis for painters of field guide plates of birds. Most bird species have several unique plumages that distinguish immature from adults, males from females, and breeders from non-breeders. Thus, many different specimens may be required to produce a thorough plate for identification of a given species. Accurate colour measurements using spectrometry are possible from specimens.\n\nBird collections have been useful for retrospective studies. Bird collections offer the potential for current and future researchers to make in-depth morphological and molecular study of past avian diversity. One of the earliest and most famous examples of this was the use of egg collections from the 19th and early 20th centuries in determining that the pesticide DDT was producing eggshell-thinning in raptors. The ornithologists who collected the eggs could never have known that their work would one day help establish causes for declines and help in making conservation strategies to save bird such as peregrine falcons from possible extinction.\n\nAs threats to bird populations grow and extinctions continue, historical specimens are valuable in documenting the impacts of human activities and causes of decline for threatened species. Bird collections have also been used to gauge the flow of environmental pollutants over time. A study of soot deposits on specimens collected within the United States Manufacturing Belt was used to track concentrations of atmospheric black carbon over a 135-year span. Other possible uses for bird specimens not known today may arise in the future.\n\nThe issue of whether birds should continue to be actively collected for research has been the subject of some debate among ornithologists (examples of this can be found in the lively exchanges between Remsen and Bekoff & Elzanowski, between Vuilleumier and Donegan, and between Dubois & Nemesio and Donegan).\nThose opposed to collecting believe that much of current collecting is unnecessary, arguably motivated by the personal field scores of individuals or by competition between museums, rather than the result of a strict scientific rationale; that collecting, in extreme cases of species on the verge of extinction, can pose a threat to bird populations; and that in many cases in which the \"necessity\" of specimens is claimed, new technology such as digital photography and blood sample analysis of mist-netted individuals could instead be used. Finally, at a time of rampant deforestation and species extinctions, scientists and conservationists should take the lead in providing an example to local people not to kill or hunt birds. Where other techniques not involving killing of a bird are feasible, to take a specimen is viewed by some as simply unethical. Proponents of collecting counter-argue that compared to the many millions of birds killed each year by habitat destruction, domestic cats, window strikes, and tower kills, scientists collect only a few thousand birds per year worldwide and populations will quickly recover from an episode of collecting as long as their habitat remains. Supporters of continued collecting also point to the greater scientific utility and legacy of museum specimens compared to blood samples or photographs, and argue that collecting for research offers the only source of avian mortality with a positive outcome for birds in terms of the biological knowledge gained. Although taking small blood samples from wild birds is often viewed as a harmless alternative to collecting, it reduces survival by as much as 33% and does not provide the benefits of a voucher specimen. Scientists have pointed out that bird populations represent renewable resources, and that scientific collecting represents only a tiny and non-additive proportion of annual bird mortality. However, examples exist of species whose extinction was directly contributed to by museum collecting (e.g. Guadalupe caracara, ivory-billed woodpecker).\n\n"}
{"id": "17310823", "url": "https://en.wikipedia.org/wiki?curid=17310823", "title": "Blockade of Germany", "text": "Blockade of Germany\n\nThe Blockade of Germany, or the Blockade of Europe, occurred from 1914 to 1919. It was a prolonged naval operation conducted by the Allied Powers during and after World War I in an effort to restrict the maritime supply of goods to the Central Powers, which included Germany, Austria-Hungary and the Ottoman Empire. It is considered one of the key elements in the eventual Allied victory in the war. The German Board of Public Health in December 1918 claimed that 763,000 German civilians died from starvation and disease caused by the blockade up until the end of December 1918. An academic study done in 1928 put the death toll at 424,000.\n\nBoth the German Empire and the United Kingdom relied heavily on imports to feed their population and supply their war industry. Imports of foodstuffs and war materiel of all European belligerents came primarily from the Americas and had to be shipped across the Atlantic Ocean, thus Britain and Germany both aimed to blockade each other. The British had the Royal Navy which was superior in numbers and could operate throughout the British Empire, while the German \"Kaiserliche Marine\" surface fleet was mainly restricted to the German Bight, and used commerce raiders and unrestricted submarine warfare to operate elsewhere.\n\nPrior to World War I, a series of conferences were held at Whitehall in 1905–1906 concerning military cooperation with France in the event of a war with Germany. The Director of Naval Intelligence—Charles Ottley—asserted that two of the Royal Navy′s functions in such a war would be the capture of German commercial shipping and the blockade of German ports. A blockade was considered useful for two reasons: it could force the enemy′s fleet to fight and it could also act as an economic weapon to destroy German commerce. It was not until 1908, however, that a blockade of Germany formally appeared in the Navy′s war plans and even then some officials were divided over how feasible it was. The plans remained in a state of constant change and revision until 1914, the Navy undecided over how best to operate such a blockade.\n\nMeanwhile, Germany had made no plans to manage its wartime food supplies since, in peacetime, it was able to produce some 80% of its total consumption. Furthermore, overland imports from the Netherlands, Scandinavia, and Romania would be unaffected by any naval blockade. However, the combined issues of conscription of farm laborers, the requisition of horses, poor weather, and the diversion of nitrogen from fertilizer manufacture into military explosives, all combined to cause a considerable drop in agricultural output.\n\nThe British—with their overwhelming sea power—established a naval blockade of Germany immediately on the outbreak of war in August 1914, issuing a comprehensive list of contraband that all but prohibited American trade with the Central powers, and in early November 1914 declared the North Sea to be a war zone, with any ships entering the North Sea doing so at their own risk. The blockade was unusually restrictive in that even foodstuffs were considered \"contraband of war\". There were complaints about breaches of international law; however, most neutral merchant vessels agreed to dock at British ports to be inspected and then escorted—less any \"illegal\" cargo destined for Germany—through the British minefields to their destinations.\n\nThe Northern Patrol and Dover Patrol closed off access to the North Sea and the English Channel respectively.\n\nThe Germans regarded this as an illegal attempt to starve the German people into submission and wanted to retaliate in kind.\n\nThe blockade hurt American exports. Under pressure especially from commercial interests wishing to profit from wartime trade with both sides, Washington government protested vigorously. Britain did not wish to antagonize the U.S. It set up a program to buy American cotton, guaranteeing the price stayed above peacetime levels and mollifying cotton traders. When American ships were stopped with contraband, the British purchased the entire cargo, and released the empty ship.\n\nA memorandum to the British War Cabinet on 1 January 1917 stated that very few supplies were reaching Germany or its allies either via the North Sea or other areas such as Austria's Adriatic ports (which had been subject to a French blockade since 1914).\n\nThe first official accounts of the blockade, written by Professor A. C. Bell and Brigadier-General Sir James E. Edmonds, both concentrated on the food question but differed on their accounts of the effects. Bell, who used German data, argued that the blockade had led to revolutionary uprisings in Germany and caused the collapse of the Kaiser′s administration. Edmonds, on the other hand, supported by Colonel Irwin L. Hunt, who was in charge of civil affairs in the American occupied zone of the Rhineland, held that food shortages were a post-armistice phenomenon caused solely by the disruptions of the German Revolution of 1918–19.\n\nMore recent studies also disagree on the severity of the blockade′s impact on the affected populations at the time of the revolution and the armistice. Some hold that the blockade starved Germany and the Central Powers into defeat in 1918, but others maintain that while the German population indeed went hungry as a result of the blockade, Germany′s rationing system kept all but a few from actually starving to death. German success against the Russians on the Eastern Front, culminating in the Treaty of Brest-Litovsk, gave Germany access to the resources of Poland and other eastern territories, which did much to counter the effects of the blockade. The armistice on 11 November was forced by events on the Western Front, rather than any actions of the civilian population. Also, Germany's largest ally Austria-Hungary had already signed an armistice on 3 November 1918, exposing Germany to an invasion from the south.\n\nNevertheless, it is still accepted that the blockade made a large contribution to the outcome of the war. By 1915, Germany′s imports had already fallen by 55% from its prewar levels and the exports were 53% of what they were in 1914. Apart from leading to shortages in vital raw materials such as coal and nonferrous metals, the blockade also deprived Germany of supplies of fertiliser that were vital to agriculture. That led to staples such as grain, potatoes, meat, and dairy products becoming so scarce by the end of 1916 that many people were obliged to instead consume \"ersatz\" products including \"Kriegsbrot\" (\"war bread\") and powdered milk. The food shortages caused looting and riots not only in Germany but also in Vienna and Budapest. The food shortages got so bad that Austria-Hungary hijacked ships on the Danube that were meant to deliver food to Germany. Also, during the winter of 1916 to 1917, there was a failure of the potato crop, which resulted in the urban population having to subsist largely on Swedish turnips. That period became known as the \"Steckrübenwinter\" or Turnip Winter.\n\nThe German government made strong attempts to counter the effects of the blockade; the Hindenburg Programme of German economic mobilisation was launched on 31 August 1916 and designed to raise productivity by the compulsory employment of all men between the ages of 17 and 60. A complicated rationing system initially introduced in January 1915 aimed to ensure that a minimum nutritional need was met, with \"war kitchens\" providing cheap mass meals to impoverished civilians in larger cities. All of those schemes enjoyed only limited success, and the average daily diet of 1,000 calories was insufficient to maintain a good standard of health, resulting by 1917 in widespread disorders caused by malnutrition such as scurvy, tuberculosis and dysentery.\n\nGerman official statistics estimated 763,000 civilian malnutrition and disease deaths were caused by the blockade of Germany. That figure was disputed by a subsequent academic study, which put the death toll at 424,000. The German official statistics came from a German government report published in December 1918 that estimated the blockade to be responsible for the deaths of 762,796 civilians, and the report claimed that that figure did not include deaths caused by the Spanish flu epidemic of 1918. The figures for the last six months of 1918 were estimated. Maurice Parmelle maintained that \"it is very far from accurate to attribute to the blockade all of the excess deaths above pre-war mortality\" and believed that the German figures were \"somewhat exaggerated\". The German claims were made at a time that Germany was waging a propaganda campaign to end the Allied blockade of Germany after the armistice that lasted from November 1918 until June 1919. Also in 1919, Germany raised the issue of the Allied blockade to counter charges against the German use of submarine warfare.\n\nIn 1928, a German academic study sponsored by the Carnegie Endowment for International Peace provided a thorough analysis of the German civilian deaths during the war. The study estimated 424,000 war related deaths of civilians over the age of one in Germany, not including Alsace-Lorraine, and the authors attributed the civilian deaths over the prewar level primarily to food and fuel shortages in 1917–1918. The study also estimated an additional 209,000 Spanish flu deaths in 1918 A study sponsored by the Carnegie Endowment for International Peace in 1940 estimated the German civilian death toll at over 600,000. Based on the above-mentioned German study of 1928, they maintained, \"A thorough inquiry has led to the conclusion that the number of 'civilian' deaths traceable to the war was 424,000, to which number must be added about 200,000 deaths caused by the influenza epidemic\".\n\nThe blockade was maintained for eight months after the Armistice in November 1918, into the following year of 1919. According to the New Cambridge Modern History food imports into Germany were controlled by the Allies after the Armistice with Germany until Germany signed the Treaty of Versailles in June 1919.\nThe total blockade was lifted on 17 January 1919 when the Allies allowed the importation of food under their supervision. The Allies requested that the German government send German merchant ships to Allied ports to transport food supplies. However the Germans considered the armistice a temporary cessation of the war and refused, believing that should fighting break out again the ships would be confiscated. The German government notified an American representative in Berlin that the shortage of food would not become critical until late spring. Food deliveries were delayed until March 1919 when the German government agreed to the restrictions imposed by the Allies. From March food imported from America in American ships arrived in Germany. The restrictions on food imports were finally lifted on 12 July 1919 after Germany signed the Treaty of Versailles.\n\nC. Paul Vincent maintains that for the German people, these were the most devastating months of the blockade because \"in the weeks and months following the armistice, Germany's deplorable state further deteriorated.\" However, Sally Marks believes that the German accounts of a hunger blockade are a \"myth\". She points out that although the Germans had denied Belgium and northern France food during the war, leading to starvation, the Allies made no effort to deny Germany food. According to Marks the food situation in 1919 in Belgium, northern France and Poland was worse because the Germans had confiscated the harvest.\n\nNot included in the German government's December 1918 figure of 763,000 deaths are civilian famine related deaths during 1919. A recent academic study maintains that there is no statistical data for the death toll of the period immediately following the Armistice in November 1918. Dr. Max Rubner in an April 1919 article claimed that 100,000 German civilians had died due to the continuation blockade of Germany after the armistice. In the UK a Labour Party anti-war activist Robert Smillie issued a statement in June 1919 condemning continuation of the blockade, and claiming that 100,000 German civilians had died.\n\nThe impact on childhood was assessed in 2014 using a newly-discovered dataset based on heights and weights of nearly 600,000 German schoolchildren measured between 1914 and 1924. They indicate that children suffered severe malnutrition. Family class was a major factor, as the working-class children suffered the most but were the quickest to recover after the war. Recovery to normality was made possible by massive food aid organized by the United States and other former enemies.\n\n\n\n"}
{"id": "288262", "url": "https://en.wikipedia.org/wiki?curid=288262", "title": "CSIRO", "text": "CSIRO\n\nThe Commonwealth Scientific and Industrial Research Organisation (CSIRO) is an independent Australian federal government agency responsible for scientific research. Its chief role is to improve the economic and social performance of industry for the benefit of the community. \n\nCSIRO works with leading organisations around the world. From its headquarters in Canberra, CSIRO maintains more than 50 sites across Australia and in France, Chile and the United States, employing about 5500 people.\n\nFederally funded scientific research began in Australia years ago. The Advisory Council of Science and Industry was established in 1916 but was hampered by insufficient available finance. In 1926 the research effort was reinvigorated by establishment of the Council for Scientific and Industrial Research (CSIR), which strengthened national science leadership and increased research funding. CSIR grew rapidly and achieved significant early successes. In 1949 further legislated changes included re-naming the organisation as CSIRO.\n\nNotable developments by CSIRO have included the invention of atomic absorption spectroscopy, essential components of Wi-Fi technology, development of the first commercially successful polymer banknote, the invention of the insect repellent in Aerogard and the introduction of a series of biological controls into Australia, such as the introduction of myxomatosis and rabbit calicivirus for the control of rabbit populations.\n\nCSIRO is governed by a board appointed by the Australian Government, currently chaired by David Thodey. There are nine directors inclusive of the Chief Executive, presently Dr. Larry Marshall, who is responsible for management of the organisation.\n\nCSIRO is structured into research focus areas, services and infrastructure.\n\n\nCSIRO manages national research facilities and scientific infrastructure on behalf of the nation to assist with the delivery of research. The national facilities and specialised laboratories are available to both international and Australian users from industry and research.\nCSIRO manages a number of collections of animal and plant specimens that contribute to national and international biological knowledge. The National Collections contribute to taxonomic, genetic, agricultural and ecological research.\n\nA precursor to CSIRO, the Advisory Council of Science and Industry, was established in 1916 on the initiative of prime minister Billy Hughes. However, the Advisory Council struggled with insufficient funding during the First World War. In 1920 the Council was renamed the Commonwealth Institute of Science and Industry, and was led by George Handley Knibbs (1921–26), but continued to struggle financially.\n\nIn 1926 the Australian Parliament modified the principal Act for national scientific research (the \"Institute of Science and Industry Act 1920\") by passing \"The Science and Industry Research Act 1926\".\n\nThe new Act replaced the Institute with the Council for Scientific and Industrial Research (CSIR). With encouragement from prime minister Stanley Bruce, strengthened national science leadership and increased research funding, CSIR grew rapidly and achieved significant early successes. The council was structured to represent the federal structure of government in Australia, and had state-level committees and a central council. In addition to an improved structure, CSIR benefited from strong bureaucratic management under George Julius, David Rivett, and Arnold Richardson. Research focused on primary and secondary industries. Early in its existence, CSIR established divisions studying animal health and animal nutrition. After the Great Depression, research was extended into manufacturing and other secondary industries.\n\nIn 1949 the Act was changed again and the entity name amended to the Commonwealth Scientific and Industrial Research Organisation. The amendment enlarged and reconstituted the organisation and its administrative structure. Under Ian Clunies Ross as chairman, CSIRO pursued new areas such as radio astronomy and industrial chemistry. CSIRO still operates under the provisions of the 1949 Act in a wide range of scientific inquiry.\n\nSince 1949 CSIRO has expanded its activities to almost every field of primary, secondary and tertiary industry, including the environment, human nutrition, conservation, urban and rural planning, and water. It works with leading organisations around the world and maintains more than 50 sites across Australia and in France, Chile and the United States of America, employing about 5500 people.\n\nNotable inventions and breakthroughs by CSIRO include:\n\n\nCSIRO had a pioneering role in the scientific discovery of the universe through radio \"eyes\". A team led by Paul Wild built and operated (from 1948) the world’s first solar radiospectrograph, and from 1967 the radioheliograph at Culgoora in New South Wales. For three decades, the Division of Radiophysics had a world-leading role in solar research, attracting prominent solar physicists from around the world.\n\nCSIRO owned the first computer in Australia, CSIRAC, built as part of a project began in the Sydney Radiophysics Laboratory in 1947. The CSIR Mk 1 ran its first program in 1949, the fifth electronic computer in the world. It was over 1,000 times faster than the mechanical calculators available at the time. It was decommissioned in 1955 and recommissioned in Melbourne as CSIRAC in 1956 as a general purpose computing machine used by over 700 projects until 1964. The CSIRAC is the only surviving first-generation computer in the world.\n\nBetween 1965 and 1985, George Bornemissza of CSIRO's Division of Entomology founded and led the Australian Dung Beetle Project. Bornemissza, upon settling in Australia from Hungary in 1951, noticed that the pastureland was covered in dry cattle dung pads which did not seem to be recycled into the soil and caused areas of rank pasture which were unpalatable to the cattle. He proposed that the reason for this was that native Australian dung beetles, which had co-evolved alongside the marsupials (which produce dung very different in its composition from cattle), were not adapted to utilise cattle dung for their nutrition and breeding since cattle had only relatively recently been introduced to the continent in the 1880s. The Australian Dung Beetle Project sought, therefore, to introduce species of dung beetle from South Africa and Europe (which had co-evolved alongside bovids) in order to improve the fertility and quality of cattle pastures. Twenty-three species were successfully introduced throughout the duration of the project and also had the effect of reducing the pestilent bush fly population by 90%.\n\nCSIRO was the first Australian organisation to start using the Internet and was able to register the second-level domain csiro.au (as opposed to csiro.org.au or csiro.com.au). Guidelines were introduced in 1996 to regulate the use of the .au domain.\n\nWhen CSIR was formed in 1926, it was led initially by an Executive Committee of three people, two of whom were designated as the Chairman and the Chief Executive. Since then the roles and responsibilities of the Chair and Chief Executive have changed many times. From 1927 to 1986 the head of CSIR (and from 1949, CSIRO) was the Chairman, who was responsible for the management of the organisation, supported by the Chief Executive. From 1 July 1959 to 4 December 1986 CSIRO had no Chief Executive; the Chairman undertook both functions.\n\nIn 1986, when the Australian Government changed the structure of CSIRO to include a board of non-executive members plus the Chief Executive to lead CSIRO, the roles changed. The Chief Executive is now responsible for management of the organisation in accordance with the strategy, plans and policies approved by the CSIRO Board which, led by the Chair of the Board, is responsible to the Australian Government for the overall strategy, governance and performance of CSIRO.\n\nAs with its governance structure, the priorities and structure of CSIRO, and the teams and facilities that implement its research, have changed as Australia's scientific challenges have evolved.\n\nIn 2005 the CSIRO gained worldwide attention, including some criticism, for promoting a high-protein, low-carbohydrate diet of their own creation called \"Total Wellbeing Diet\". The CSIRO published the diet in a book which sold over half a million copies in Australia and over 100,000 overseas. The diet was criticised in an editorial by \"Nature\" for giving scientific credence to a \"fashionable\" diet sponsored by meat and dairy industries.\n\nIn the early 1990s, CSIRO radio astronomy scientists John O'Sullivan, Graham Daniels, Terence Percival, Diethelm Ostry and John Deane undertook research directed to finding a way to make wireless networks work as fast as wired networks within confined spaces such as office buildings. The technique they developed, involving a particular combination of forward error correction, frequency-domain interleaving, and multi-carrier modulation, became the subject of , which was granted on 23 January 1996.\n\nIn 1997 Macquarie University professor David Skellern and his colleague Neil Weste established the company Radiata, Inc., which took a nonexclusive licence to the CSIRO patent for the purpose of developing commercially viable integrated circuit devices implementing the patented technology.\n\nDuring this period, the IEEE 802.11 Working Group was developing the 802.11a wireless LAN standard. CSIRO did not participate directly in the standards process, however David Skellern was an active participant as secretary of the Working Group, and representative of Radiata. In 1998 it became apparent that the CSIRO patent would be pertinent to the standard. In response to a request from Victor Hayes of Lucent Technologies, who was Chair of the 802.11 Working Group, CSIRO confirmed its commitment to make non-exclusive licenses available to implementers of the standard on reasonable and non-discriminatory terms.\n\nIn 1999, Cisco Systems, Inc. and Broadcom Corporation each invested A$4 million in Radiata, representing an 11% stake for each investor and valuing the company at around A$36 million. In September 2000, Radiata demonstrated a chip set complying with the recently finalised IEEE 802.11a Wi-Fi standard, and capable of handling transmission rates of up to 54 Mbit/s, at a major international exhibition.\n\nIn November 2000, Cisco acquired Radiata in exchange for US$295 million in Cisco common stock with the intention of incorporating the Radiata Baseband Processor and Radio chips into its Aironet family of wireless LAN products. Cisco subsequently took a large write-down on the Radiata acquisition, following the 2001 telecoms crash, and in 2004 it shut down its internal development of wireless chipsets based on the Radiata technology in order to focus on software development and emerging new technologies.\n\nControversy over the CSIRO patent arose in 2006 after the organisation won an injunction against Buffalo Technology in an infringement suit filed in Federal Court in the Eastern District of Texas. The injunction was subsequently suspended on appeal, with the Court of Appeals for the Federal Circuit finding that the judge in Texas should have allowed a trial to proceed on Buffalo’s challenge to the validity of the CSIRO patent. In 2007, CSIRO declined to provide an assurance to the IEEE that it would not sue companies which refused to take a license for use in 802.11n-compliant devices, while at the same time continuing to defend legal challenges to the validity of the patent brought by Intel, Dell, Microsoft, Hewlett-Packard and Netgear.\n\nIn April 2009, Hewlett-Packard broke ranks with the rest of the industry becoming the first to reach a settlement of its dispute with CSIRO. This agreement was followed quickly by settlements with Microsoft, Fujitsu and Asus and then Dell, Intel, Nintendo, Toshiba, Netgear, Buffalo, D-Link, Belkin, SMC, Accton, and 3Com.\n\nThe controversy grew after CSIRO sued US carriers AT&T, Verizon and T-Mobile in 2010, with the organisation being accused of being \"Australia's biggest patent troll\", a wrathful \"patent bully\", and of imposing a \"WiFi tax\" on American innovation.\n\nFurther fuel was added to the controversy after a settlement with the carriers, worth around $229 million, was announced in March 2012. Encouraged in part by an announcement by the Australian Minister for Tertiary Education, Skills Science and Research, Senator Chris Evans, an article in Ars Technica portrayed CSIRO as a shadowy organisation responsible for US consumers being compelled to make \"a multimillion dollar donation\" on the basis of a questionable patent claiming \"decades old\" technology. The resulting debate became so heated that the author was compelled to follow up with a defence of the original article. An alternative view was also published on The Register, challenging a number of the assertions made in the Ars Technica piece.\n\nTotal income to CSIRO from the patent is currently estimated at nearly $430 million. On 14 June 2012, the CSIRO inventors received the European Patent Office (EPO) European Inventor Award (EIA), in the category of \"Non-European Countries\".\n\nOn 14 July 2011, Greenpeace activists vandalised a crop of GM wheat, circumventing the scientific trials being undertaken. Greenpeace was forced to pay reparations to CSIRO of $280,000 for the criminal damage, and were accused by the sentencing judge, Justice Hilary Penfold, of cynically using junior members of the organisation with good standing to avoid custodial sentences, while the offenders were given 9-month suspended sentences.\n\nFollowing the attack Greenpeace criticised CSIRO for a close relationship with industry that had led to an increase in genetically modified crops, even though a core aim of CSIRO is Cooperative Research \"working hand in hand with industry [to] build partnerships and engage with industry to generate impact\".\n\nOn 25 November 2009, a debate was held in the Australian Senate concerning the alleged involvement of the CSIRO and the Labor government in censorship. The debate was called for by opposition parties after evidence came to light that a paper critical of carbon emissions trading was being suppressed. At the time, the Labor government was trying to get such a scheme through the Senate. After the debate, the Science Minister, Kim Carr, was forced to release the paper, but when doing so in the Senate he also delivered a letter from the CEO of the CSIRO, Megan Clark, which attacked the report's author and threatened him with unspecified punishment. The author of the paper, Clive Spash, was cited in the press as having been bullied and harassed, and later gave a radio interview about this. In the midst of the affair, CSIRO management had considered releasing the paper with edits that Nature reported would be \"tiny\". Spash claimed the changes actually demanded amounted to censorship and resigned. He later posted on his website a document detailing the text that CSIRO management demanded be deleted; by itself, this document forms a coherent set of statements criticising emissions trading without any additional wording needed. In subsequent Senate Estimates hearings during 2010, Senator Carr and Clark went on record claiming the paper was originally stopped from publication solely due to its low quality not meeting CSIRO standards. At the time of its attempted suppression, the paper had been accepted for publication in an academic journal, New Political Economy, which in 2010 had been ranked by the Australian Research Council as an 'A class' publication. In an ABC radio interview, Spash called for a Senate enquiry into the affair and the role played by senior management and the Science Minister. After these events, the \"Sydney Morning Herald\" reported that \"Questions are being raised about the closeness of BHP Billiton and the CSIRO under its chief executive, Megan Clark\". After his resignation, an unedited version of the paper was released by Spash as a discussion paper, and later published as an academic journal article.\n\nOn 11 April 2013, the \"Sydney Morning Herald\" ran a story on how CSIRO had \"duped\" the Swiss-based pharmaceutical giant Novartis into purchasing an anti-counterfeit technology for its vials of injectable Voltaren. The invention was marketed by a small Australian company called DataTrace DNA as a method of identifying fake vials, on the basis that a unique tracer code developed by CSIRO was embedded in the product. However, the code sold to Novartis for more than A$2M was apparently not unique, and was based on a \"cheap tracer ... bought in bulk from a Chinese distributor\". Novartis was contractually bound not to reverse-engineer the tracer to verify its uniqueness. The \"Sydney Morning Herald\" report alleges that this was done with the knowledge of key CSIRO personnel.\n\nCSIRO has since conducted a full review of the allegations and found no evidence to support them.\n\nIn recent years the CSIRO has fallen under the spotlight for allegedly exhibiting a culture of workplace bullying and harassment. Former CSIRO employees started to surface with experiences of workplace bullying and other unreasonable behaviour by current and former CSIRO staff members. CSIRO took the allegations seriously and responded to the articles on a number of occasions.\n\nThe shadow minister for innovation, industry, science and research, Sophie Mirabella, wrote to the government requesting it establish an inquiry. Mirabella said she is aware of as many as 100 cases of alleged workplace harassment. On 20 July 2012 Comcare issued CSIRO with an Improvement Notice with regard to handling and management of workplace misconduct/code of conduct type investigations and allegations. On 24 June 2013 Mirabella advised the Australian House of Representatives that in relation to the worker's compensation claim for psychological injuries of ex-CSIRO employee, Martin Williams, which was vigorously defended by Comcare on the advice of the CSIRO, that CSIRO officers had provided false testimony on no less than 128 occasions under oath when the matter went before the Administrative Appeals Tribunal. Mirabella stated, \"even in establishing the framework for this inquiry it is obvious there's an inappropriate 'hands on' approach by CSIRO.\"\n\nIn response to the allegations Clark commissioned Dennis Pearce, who is assisted by an investigation team from HWL Ebsworth Lawyers, to conduct an independent investigation into allegations of workplace bullying and other unreasonable behaviour. Mirabella continued to question the independence of the investigation. The first stage of the investigation is scheduled to publish its findings at the end of July 2013, and the final stage is scheduled to be complete by February 2014.\n\nIn August 2015 the CSIRO discontinued its annual July and August survey, conducted over the previous five years, polling to create a long-term view of how Australians viewed global warming and their support for action. In the previous 2013 poll, 86 per cent agreed with the statement that climate change was occurring and only 7.6 per cent disagreed.\n\nOn the 11th February 2016, Dr Larry Marshall – a former venture capitalist with Southern Cross Venture Holdings, who had been appointed CEO of the CSIRO on the 1st January 2015, caused an international outcry after describing Australia’s national climate change discussion as “more like religion than science,” a week after announcing hundreds of job cuts to the organisation that will reduce the effectiveness of its climate research team.\n\nIn “an open letter to the Australian Government and CSIRO”, 2,800 of the leading climate scientists from 60 countries say the announcement of cuts to the CSIRO’s Oceans and Atmosphere research program has alarmed the global climate research community. They say the decision shows a lack of insight and a misunderstanding of the importance of the depth and significance of Australian contributions to global and regional climate research.\n\n\n\n"}
{"id": "54342", "url": "https://en.wikipedia.org/wiki?curid=54342", "title": "Cellular automaton", "text": "Cellular automaton\n\nA cellular automaton (pl. cellular automata, abbrev. CA) is a discrete model studied in computer science, mathematics, physics, complexity science, theoretical biology and microstructure modeling. Cellular automata are also called cellular spaces, tessellation automata, homogeneous structures, cellular structures, tessellation structures, and iterative arrays.\n\nA cellular automaton consists of a regular grid of \"cells\", each in one of a finite number of \"states\", such as \"on\" and \"off\" (in contrast to a coupled map lattice). The grid can be in any finite number of dimensions. For each cell, a set of cells called its \"neighborhood\" is defined relative to the specified cell. An initial state (time \"t\" = 0) is selected by assigning a state for each cell. A new \"generation\" is created (advancing \"t\" by 1), according to some fixed \"rule\" (generally, a mathematical function) that determines the new state of each cell in terms of the current state of the cell and the states of the cells in its neighborhood. Typically, the rule for updating the state of cells is the same for each cell and does not change over time, and is applied to the whole grid simultaneously, though exceptions are known, such as the stochastic cellular automaton and asynchronous cellular automaton.\n\nThe concept was originally discovered in the 1940s by Stanislaw Ulam and John von Neumann while they were contemporaries at Los Alamos National Laboratory. While studied by some throughout the 1950s and 1960s, it was not until the 1970s and Conway's Game of Life, a two-dimensional cellular automaton, that interest in the subject expanded beyond academia. In the 1980s, Stephen Wolfram engaged in a systematic study of one-dimensional cellular automata, or what he calls elementary cellular automata; his research assistant Matthew Cook showed that one of these rules is Turing-complete. Wolfram published \"A New Kind of Science\" in 2002, claiming that cellular automata have applications in many fields of science. These include computer processors and cryptography.\n\nThe primary classifications of cellular automata, as outlined by Wolfram, are numbered one to four. They are, in order, automata in which patterns generally stabilize into homogeneity, automata in which patterns evolve into mostly stable or oscillating structures, automata in which patterns evolve in a seemingly chaotic fashion, and automata in which patterns become extremely complex and may last for a long time, with stable local structures. This last class are thought to be computationally universal, or capable of simulating a Turing machine. Special types of cellular automata are \"reversible\", where only a single configuration leads directly to a subsequent one, and \"totalistic\", in which the future value of individual cells only depends on the total value of a group of neighboring cells. Cellular automata can simulate a variety of real-world systems, including biological and chemical ones.\n\nOne way to simulate a two-dimensional cellular automaton is with an infinite sheet of graph paper along with a set of rules for the cells to follow. Each square is called a \"cell\" and each cell has two possible states, black and white. The \"neighborhood\" of a cell is the nearby, usually adjacent, cells. The two most common types of neighborhoods are the \"von Neumann neighborhood\" and the \"Moore neighborhood\". The former, named after the founding cellular automaton theorist, consists of the four orthogonally adjacent cells. The latter includes the von Neumann neighborhood as well as the four remaining cells surrounding the cell whose state is to be calculated. For such a cell and its Moore neighborhood, there are 512 (= 2) possible patterns. For each of the 512 possible patterns, the rule table would state whether the center cell will be black or white on the next time interval. Conway's Game of Life is a popular version of this model. Another common neighborhood type is the \"extended von Neumann neighborhood\", which includes the two closest cells in each orthogonal direction, for a total of eight. The general equation for such a system of rules is \"k\", where \"k\" is the number of possible states for a cell, and \"s\" is the number of neighboring cells (including the cell to be calculated itself) used to determine the cell's next state. Thus, in the two dimensional system with a Moore neighborhood, the total number of automata possible would be 2, or .\n\nIt is usually assumed that every cell in the universe starts in the same state, except for a finite number of cells in other states; the assignment of state values is called a \"configuration\". More generally, it is sometimes assumed that the universe starts out covered with a periodic pattern, and only a finite number of cells violate that pattern. The latter assumption is common in one-dimensional cellular automata.\nCellular automata are often simulated on a finite grid rather than an infinite one. In two dimensions, the universe would be a rectangle instead of an infinite plane. The obvious problem with finite grids is how to handle the cells on the edges. How they are handled will affect the values of all the cells in the grid. One possible method is to allow the values in those cells to remain constant. Another method is to define neighborhoods differently for these cells. One could say that they have fewer neighbors, but then one would also have to define new rules for the cells located on the edges. These cells are usually handled with a \"toroidal\" arrangement: when one goes off the top, one comes in at the corresponding position on the bottom, and when one goes off the left, one comes in on the right. (This essentially simulates an infinite periodic tiling, and in the field of partial differential equations is sometimes referred to as \"periodic\" boundary conditions.) This can be visualized as taping the left and right edges of the rectangle to form a tube, then taping the top and bottom edges of the tube to form a torus (doughnut shape). Universes of other dimensions are handled similarly. This solves boundary problems with neighborhoods, but another advantage is that it is easily programmable using modular arithmetic functions. For example, in a 1-dimensional cellular automaton like the examples below, the neighborhood of a cell \"x\" is {\"x\", \"x\", \"x\"}, where \"t\" is the time step (vertical), and \"i\" is the index (horizontal) in one generation.\n\nStanislaw Ulam, while working at the Los Alamos National Laboratory in the 1940s, studied the growth of crystals, using a simple lattice network as his model. At the same time, John von Neumann, Ulam's colleague at Los Alamos, was working on the problem of self-replicating systems. Von Neumann's initial design was founded upon the notion of one robot building another robot. This design is known as the kinematic model. As he developed this design, von Neumann came to realize the great difficulty of building a self-replicating robot, and of the great cost in providing the robot with a \"sea of parts\" from which to build its replicant. Neumann wrote a paper entitled \"The general and logical theory of automata\" for the Hixon Symposium in 1948. Ulam was the one who suggested using a \"discrete\" system for creating a reductionist model of self-replication. Nils Aall Barricelli performed many of the earliest explorations of these models of artificial life.\n\nUlam and von Neumann created a method for calculating liquid motion in the late 1950s. The driving concept of the method was to consider a liquid as a group of discrete units and calculate the motion of each based on its neighbors' behaviors. Thus was born the first system of cellular automata. Like Ulam's lattice network, von Neumann's cellular automata are two-dimensional, with his self-replicator implemented algorithmically. The result was a universal copier and constructor working within a cellular automaton with a small neighborhood (only those cells that touch are neighbors; for von Neumann's cellular automata, only orthogonal cells), and with 29 states per cell. Von Neumann gave an existence proof that a particular pattern would make endless copies of itself within the given cellular universe by designing a 200,000 cell configuration that could do so. This design is known as the tessellation model, and is called a von Neumann universal constructor.\n\nAlso in the 1940s, Norbert Wiener and Arturo Rosenblueth developed a model of excitable media with some of the characteristics of a cellular automaton. Their specific motivation was the mathematical description of impulse conduction in cardiac systems. However their model is not a cellular automaton because the medium in which signals propagate is continuous, and wave fronts are curves. A true cellular automaton model of excitable media was developed and studied by J. M. Greenberg and S. P. Hastings in 1978; see Greenberg-Hastings cellular automaton. The original work of Wiener and Rosenblueth contains many insights and continues to be cited in modern research publications on cardiac arrhythmia and excitable systems.\n\nBy the end of the 1950s it had been noted that cellular automata could be viewed as parallel computers, and particularly in the 1960s a sequence of increasingly detailed and technical theorems—often analogous to ones about Turing machines—were proved about their formal computational capabilities.\n\nIn the 1960s, cellular automata were studied as a particular type of dynamical system and the connection with the mathematical field of symbolic dynamics was established for the first time. In 1969, Gustav A. Hedlund compiled many results following this point of view in what is still considered as a seminal paper for the mathematical study of cellular automata. The most fundamental result is the characterization in the Curtis–Hedlund–Lyndon theorem of the set of global rules of cellular automata as the set of continuous endomorphisms of shift spaces.\n\nIn 1969, German computer pioneer Konrad Zuse published his book \"Calculating Space\", proposing that the physical laws of the universe are discrete by nature, and that the entire universe is the output of a deterministic computation on a single cellular automaton; \"Zuse's Theory\" became the foundation of the field of study called \"digital physics\".\n\nAlso in 1969 computer scientist Alvy Ray Smith completed a Stanford PhD dissertation on Cellular Automata Theory, the first mathematical treatment of CA as a general class of computers. Many papers came from this dissertation: He showed the equivalence of neighborhoods of various shapes, how to reduce a Moore to a von Neumann neighborhood or how to reduce any neighborhood to a von Neumann neighborhood. He proved that two-dimensional CA are computation universal, introduced 1-dimensional CA, and showed that they too are computation universal, even with simple neighborhoods. He showed how to subsume the complex von Neumann proof of construction universality (and hence self-reproducing machines) into a consequence of computation universality in a 1-dimensional CA. Intended as the introduction to the German edition of von Neumann's book on CA, he wrote a survey of the field with dozens of references to papers, by many authors in many countries over a decade or so of work, often overlooked by modern CA researchers.\n\nIn the 1970s a two-state, two-dimensional cellular automaton named Game of Life became widely known, particularly among the early computing community. Invented by John Conway and popularized by Martin Gardner in a \"Scientific American\" article, its rules are as follows:\nDespite its simplicity, the system achieves an impressive diversity of behavior, fluctuating between apparent randomness and order. One of the most apparent features of the Game of Life is the frequent occurrence of \"gliders\", arrangements of cells that essentially move themselves across the grid. It is possible to arrange the automaton so that the gliders interact to perform computations, and after much effort it has been shown that the Game of Life can emulate a universal Turing machine. It was viewed as a largely recreational topic, and little follow-up work was done outside of investigating the particularities of the Game of Life and a few related rules in the early 1970s.\n\nStephen Wolfram independently began working on cellular automata in mid 1981 after considering how complex patterns seemed formed in nature in violation of the Second Law of Thermodynamics. His investigations were initially spurred by an interest in modelling systems such as neural networks. He published his first paper in \"Reviews of Modern Physics\" investigating \"elementary cellular automata\" (Rule 30 in particular) in June 1983. The unexpected complexity of the behavior of these simple rules led Wolfram to suspect that complexity in nature may be due to similar mechanisms. His investigations, however, led him to realize that cellular automata were poor at modelling neural networks. Additionally, during this period Wolfram formulated the concepts of intrinsic randomness and computational irreducibility, and suggested that rule 110 may be universal—a fact proved later by Wolfram's research assistant Matthew Cook in the 1990s.\n\nIn 2002 Wolfram published a 1280-page text \"A New Kind of Science\", which extensively argues that the discoveries about cellular automata are not isolated facts but are robust and have significance for all disciplines of science. Despite confusion in the press, the book did not argue for a fundamental theory of physics based on cellular automata, and although it did describe a few specific physical models based on cellular automata, it also provided models based on qualitatively different abstract systems.\n\nWolfram, in \"A New Kind of Science\" and several papers dating from the mid-1980s, defined four classes into which cellular automata and several other simple computational models can be divided depending on their behavior. While earlier studies in cellular automata tended to try to identify type of patterns for specific rules, Wolfram's classification was the first attempt to classify the rules themselves. In order of complexity the classes are:\n\nThese definitions are qualitative in nature and there is some room for interpretation. According to Wolfram, \"...with almost any general classification scheme there are inevitably cases which get assigned to one class by one definition and another class by another definition. And so it is with cellular automata: there are occasionally rules...that show some features of one class and some of another.\" Wolfram's classification has been empirically matched to a clustering of the compressed lengths of the outputs of cellular automata.\n\nThere have been several attempts to classify cellular automata in formally rigorous classes, inspired by the Wolfram's classification. For instance, Culik and Yu proposed three well-defined classes (and a fourth one for the automata not matching any of these), which are sometimes called Culik-Yu classes; membership in these proved undecidable.\nWolfram's class 2 can be partitioned into two subgroups of stable (fixed-point) and oscillating (periodic) rules.\n\nThe idea that there are 4 classes of dynamical system came originally from nobel-prize winning chemist Ilya Prigogine who identified these 4 classes of for thermodynamical systems - (1) systems in thermodynamic equilibrium, (2) spatially/temporally uniform systems, (3) chaotic systems, and (4) complex far-from-equilibrium systems with dissipative structures (see figure 1 in Nicolis' paper (Prigogine's student)).\n\nA cellular automaton is \"reversible\" if, for every current configuration of the cellular automaton, there is exactly one past configuration (preimage). If one thinks of a cellular automaton as a function mapping configurations to configurations, reversibility implies that this function is bijective. If a cellular automaton is reversible, its time-reversed behavior can also be described as a cellular automaton; this fact is a consequence of the Curtis–Hedlund–Lyndon theorem, a topological characterization of cellular automata. For cellular automata in which not every configuration has a preimage, the configurations without preimages are called \"Garden of Eden\" patterns.\n\nFor one-dimensional cellular automata there are known algorithms for deciding whether a rule is reversible or irreversible. However, for cellular automata of two or more dimensions reversibility is undecidable; that is, there is no algorithm that takes as input an automaton rule and is guaranteed to determine correctly whether the automaton is reversible. The proof by Jarkko Kari is related to the tiling problem by Wang tiles.\n\nReversible cellular automata are often used to simulate such physical phenomena as gas and fluid dynamics, since they obey the laws of thermodynamics. Such cellular automata have rules specially constructed to be reversible. Such systems have been studied by Tommaso Toffoli, Norman Margolus and others. Several techniques can be used to explicitly construct reversible cellular automata with known inverses. Two common ones are the second order cellular automaton and the block cellular automaton, both of which involve modifying the definition of a cellular automaton in some way. Although such automata do not strictly satisfy the definition given above, it can be shown that they can be emulated by conventional cellular automata with sufficiently large neighborhoods and numbers of states, and can therefore be considered a subset of conventional cellular automata. Conversely, it has been shown that every reversible cellular automaton can be emulated by a block cellular automaton.\n\nA special class of cellular automata are \"totalistic\" cellular automata. The state of each cell in a totalistic cellular automaton is represented by a number (usually an integer value drawn from a finite set), and the value of a cell at time \"t\" depends only on the \"sum\" of the values of the cells in its neighborhood (possibly including the cell itself) at time \"t\" − 1. If the state of the cell at time \"t\" depends on both its own state and the total of its neighbors at time \"t\" − 1 then the cellular automaton is properly called \"outer totalistic\". Conway's Game of Life is an example of an outer totalistic cellular automaton with cell values 0 and 1; outer totalistic cellular automata with the same Moore neighborhood structure as Life are sometimes called cellular automata.\n\nThere are many possible generalizations of the cellular automaton concept.\nOne way is by using something other than a rectangular (cubic, \"etc.\") grid. For example, if a plane is tiled with regular hexagons, those hexagons could be used as cells. In many cases the resulting cellular automata are equivalent to those with rectangular grids with specially designed neighborhoods and rules. Another variation would be to make the grid itself irregular, such as with Penrose tiles.\n\nAlso, rules can be probabilistic rather than deterministic. Such cellular automata are called probabilistic cellular automata. A probabilistic rule gives, for each pattern at time \"t\", the probabilities that the central cell will transition to each possible state at time \"t\" + 1. Sometimes a simpler rule is used; for example: \"The rule is the Game of Life, but on each time step there is a 0.001% probability that each cell will transition to the opposite color.\"\n\nThe neighborhood or rules could change over time or space. For example, initially the new state of a cell could be determined by the horizontally adjacent cells, but for the next generation the vertical cells would be used.\n\nIn cellular automata, the new state of a cell is not affected by the new state of other cells. This could be changed so that, for instance, a 2 by 2 block of cells can be determined by itself and the cells adjacent to itself.\n\nThere are \"continuous automata\". These are like totalistic cellular automata, but instead of the rule and states being discrete (\"e.g.\" a table, using states {0,1,2}), continuous functions are used, and the states become continuous (usually values in [0,1]). The state of a location is a finite number of real numbers. Certain cellular automata can yield diffusion in liquid patterns in this way.\n\nContinuous spatial automata have a continuum of locations. The state of a location is a finite number of real numbers. Time is also continuous, and the state evolves according to differential equations. One important example is reaction-diffusion textures, differential equations proposed by Alan Turing to explain how chemical reactions could create the stripes on zebras and spots on leopards. When these are approximated by cellular automata, they often yield similar patterns. MacLennan considers continuous spatial automata as a model of computation.\n\nThere are known examples of continuous spatial automata, which exhibit propagating phenomena analogous to gliders in the Game of Life.\n\n\"Graph rewriting automata\" are extensions of cellular automata based on graph rewriting systems.\n\nThe simplest nontrivial cellular automaton would be one-dimensional, with two possible states per cell, and a cell's neighbors defined as the adjacent cells on either side of it. A cell and its two neighbors form a neighborhood of 3 cells, so there are 2 = 8 possible patterns for a neighborhood. A rule consists of deciding, for each pattern, whether the cell will be a 1 or a 0 in the next generation. There are then 2 = 256 possible rules. \n\nThese 256 cellular automata are generally referred to by their Wolfram code, a standard naming convention invented by Wolfram that gives each rule a number from 0 to 255. A number of papers have analyzed and compared these 256 cellular automata. The rule 30 and rule 110 cellular automata are particularly interesting. The images below show the history of each when the starting configuration consists of a 1 (at the top of each image) surrounded by 0s. Each row of pixels represents a generation in the history of the automaton, with \"t\"=0 being the top row. Each pixel is colored white for 0 and black for 1.\n\n<br>\nRule 30 cellular automaton\n\nRule 30 exhibits \"class 3\" behavior, meaning even simple input patterns such as that shown lead to chaotic, seemingly random histories.\n<br>\nRule 110 cellular automaton\nRule 110, like the Game of Life, exhibits what Wolfram calls \"class 4\" behavior, which is neither completely random nor completely repetitive. Localized structures appear and interact in various complicated-looking ways. In the course of the development of \"A New Kind of Science\", as a research assistant to Wolfram in 1994, Matthew Cook proved that some of these structures were rich enough to support universality. This result is interesting because rule 110 is an extremely simple one-dimensional system, and difficult to engineer to perform specific behavior. This result therefore provides significant support for Wolfram's view that class 4 systems are inherently likely to be universal. Cook presented his proof at a Santa Fe Institute conference on Cellular Automata in 1998, but Wolfram blocked the proof from being included in the conference proceedings, as Wolfram did not want the proof announced before the publication of \"A New Kind of Science\". In 2004, Cook's proof was finally published in Wolfram's journal \"Complex Systems\" (Vol. 15, No. 1), over ten years after Cook came up with it. Rule 110 has been the basis for some of the smallest universal Turing machines.\n\nAn elementary cellular automaton rule is specified by 8 bits, and all elementary cellular automaton rules can be considered to sit on the vertices of the 8-dimensional unit hypercube. This unit hypercube is the cellular automaton rule space. For next-nearest-neighbor cellular automata, a rule is specified by 2 = 32 bits, and the cellular automaton rule space is a 32-dimensional unit hypercube. A distance between two rules can be defined by the number of steps required to move from one vertex, which represents the first rule, and another vertex, representing another rule, along the edge of the hypercube. This rule-to-rule distance is also called the Hamming distance.\n\nCellular automaton rule space allows us to ask the question concerning whether rules with similar dynamical behavior are \"close\" to each other. Graphically drawing a high dimensional hypercube on the 2-dimensional plane remains a difficult task, and one crude locator of a rule in the hypercube is the number of bit-1 in the 8-bit string for elementary rules (or 32-bit string for the next-nearest-neighbor rules). Drawing the rules in different Wolfram classes in these slices of the rule space show that class 1 rules tend to have lower number of bit-1's, thus located in one region of the space, whereas class 3 rules tend to have higher proportion (50%) of bit-1's.\n\nFor larger cellular automaton rule space, it is shown that class 4 rules are located between the class 1 and class 3 rules. This observation is the foundation for the phrase edge of chaos, and is reminiscent of the phase transition in thermodynamics.\n\nSome biological processes occur—or can be simulated—by cellular automata.\n\nPatterns of some seashells, like the ones in the genera \"Conus\" and \"Cymbiola\", are generated by natural cellular automata. The pigment cells reside in a narrow band along the shell's lip. Each cell secretes pigments according to the activating and inhibiting activity of its neighbor pigment cells, obeying a natural version of a mathematical rule. The cell band leaves the colored pattern on the shell as it grows slowly. For example, the widespread species \"Conus textile\" bears a pattern resembling Wolfram's rule 30 cellular automaton.\n\nPlants regulate their intake and loss of gases via a cellular automaton mechanism. Each stoma on the leaf acts as a cell.\n\nMoving wave patterns on the skin of cephalopods can be simulated with a two-state, two-dimensional cellular automata, each state corresponding to either an expanded or retracted chromatophore.\n\nThreshold automata have been invented to simulate neurons, and complex behaviors such as recognition and learning can be simulated.\n\nFibroblasts bear similarities to cellular automata, as each fibroblast only interacts with its neighbors.\n\nThe Belousov–Zhabotinsky reaction is a spatio-temporal chemical oscillator that can be simulated by means of a cellular automaton. In the 1950s A. M. Zhabotinsky (extending the work of B. P. Belousov) discovered that when a thin, homogenous layer of a mixture of malonic acid, acidified bromate, and a ceric salt were mixed together and left undisturbed, fascinating geometric patterns such as concentric circles and spirals propagate across the medium. In the \"Computer Recreations\" section of the August 1988 issue of \"Scientific American\", A. K. Dewdney discussed a cellular automaton developed by Martin Gerhardt and Heike Schuster of the University of Bielefeld (Germany). This automaton produces wave patterns that resemble those in the Belousov-Zhabotinsky reaction.\n\nCellular automaton processors are physical implementations of CA concepts, which can process information computationally. Processing elements are arranged in a regular grid of identical cells. The grid is usually a square tiling, or tessellation, of two or three dimensions; other tilings are possible, but not yet used. Cell states are determined only by interactions with adjacent neighbor cells. No means exists to communicate directly with cells farther away. One such cellular automaton processor array configuration is the systolic array. Cell interaction can be via electric charge, magnetism, vibration (phonons at quantum scales), or any other physically useful means. This can be done in several ways so no wires are needed between any elements. This is very unlike processors used in most computers today, von Neumann designs, which are divided into sections with elements that can communicate with distant elements over wires.\n\nRule 30 was originally suggested as a possible block cipher for use in cryptography. Two dimensional cellular automata can be used for constructing a pseudorandom number generator.\n\nCellular automata have been proposed for public key cryptography. The one-way function is the evolution of a finite CA whose inverse is believed to be hard to find. Given the rule, anyone can easily calculate future states, but it appears to be very difficult to calculate previous states. \n\nCA have been applied to design error correction codes in a paper by D. Roy Chowdhury, S. Basu, I. Sen Gupta, and P. Pal Chaudhuri. The paper defines a new scheme of building single bit error correction and double bit error detection (SEC-DED) codes using CA, and also reports a fast hardware decoder for the code.\n\nAs Andrew Ilachinski points out in his \"Cellular Automata\", many scholars have raised the question of whether the universe is a cellular automaton. Ilachinski argues that the importance of this question may be better appreciated with a simple observation, which can be stated as follows. Consider the evolution of rule 110: if it were some kind of \"alien physics\", what would be a reasonable description of the observed patterns? If an observer did not know how the images were generated, that observer might end up conjecturing about the movement of some particle-like objects. Indeed, physicist James Crutchfield has constructed a rigorous mathematical theory out of this idea, proving the statistical emergence of \"particles\" from cellular automata. Then, as the argument goes, one might wonder if \"our\" world, which is currently well described, at our current level of understanding, by physics with particle-like objects, could be a CA at its most fundamental level with the gaps in information or incomplete understanding of fundamental data appearing as an arbitrary random order that would seem contrary to CA.\n\nWhile a complete theory along this line has not been developed, entertaining and developing this hypothesis led scholars to interesting speculation and fruitful intuitions on how we can make sense of our world within a discrete framework. Marvin Minsky, the AI pioneer, investigated how to understand particle interaction with a four-dimensional CA lattice; Konrad Zuse—the inventor of the first working computer, the Z3—developed an irregularly organized lattice to address the question of the information content of particles. More recently, Edward Fredkin exposed what he terms the \"finite nature hypothesis\", i.e., the idea that \"ultimately every quantity of physics, including space and time, will turn out to be discrete and finite.\" Fredkin and Wolfram are strong proponents of a CA-based physics. In 2016 Gerard 't Hooft published a book-length development of the idea to rebuild quantum mechanics using cellular automata.\n\nIn recent years, other suggestions along these lines have emerged from literature in non-standard computation. Wolfram's \"A New Kind of Science\" considers CA the key to understanding a variety of subjects, physics included. The \"Mathematics of the Models of Reference\"—created by iLabs founder Gabriele Rossi and developed with Francesco Berto and Jacopo Tagliabue—features an original 2D/3D universe based on a new \"rhombic dodecahedron-based\" lattice and a unique rule. This model satisfies universality (it is equivalent to a Turing Machine) and perfect reversibility (a \"desideratum\" if one wants to conserve various quantities easily and never lose information), and it comes embedded in a first-order theory, allowing computable, qualitative statements on the universe evolution.\n\nSpecific types of cellular automata include:\n\n\nProblems that can be solved with cellular automata include:\n\n\n\n"}
{"id": "52286308", "url": "https://en.wikipedia.org/wiki?curid=52286308", "title": "Computational X", "text": "Computational X\n\nComputational X is a term used to describe the various fields of study that have emerged from the applications of informatics and big data to specific disciplines. Examples include computational biology, computational neuroscience, computational physics, and computational linguistics.\n\n\n"}
{"id": "23753068", "url": "https://en.wikipedia.org/wiki?curid=23753068", "title": "Cryonics – Freeze Me", "text": "Cryonics – Freeze Me\n\nCryonics – Freeze Me (originally titled Death in the Deep Freeze) is a television documentary programme created by ZigZag Production for Five in 2006 for in their \"Stranger than Fiction\" series. The program's main topic is cryonics and mainly features interviews with Alcor Life Extension Foundation staff or Alcor members. The documentary is narrated by Michael Lumsden. Directed by Virginia Quinn Virginia Quinn.\n\nInterviews with following people are featured (in order of appearance):\n\n\n\n"}
{"id": "4082972", "url": "https://en.wikipedia.org/wiki?curid=4082972", "title": "EXIT procedure", "text": "EXIT procedure\n\nThe EXIT procedure, or ex utero intrapartum treatment procedure, is a specialized surgical delivery procedure used to deliver babies who have airway compression. Causes of airway compression in newborn babies result from a number of rare congenital disorders, including bronchopulmonary sequestration, congenital cystic adenomatoid malformation, mouth or neck tumor such as teratoma, and lung or pleural tumor such as pleuropulmonary blastoma. Airway compression discovered at birth is a medical emergency. In many cases, however, the airway compression is discovered during prenatal ultrasound exams, permitting time to plan a safe delivery using the EXIT procedure or other means.\n\nThe EXIT is an extension of a standard classical Caesarean section, where an opening is made on the midline of the anesthetized mother's abdomen and uterus. Then comes the EXIT: the baby is partially delivered through the opening but remains attached by its umbilical cord to the placenta, while a pediatric otolaryngologist-head & neck surgeon establishes an airway so the fetus can breathe. Once the EXIT is complete, the umbilical cord is clamped then cut and the infant is fully delivered. Then the remainder of the C-section proceeds.\n\nThe ex utero intrapartum treatment (EXIT) procedure was originally developed to reverse temporary tracheal occlusion in patients who had undergone fetal surgery for severe congenital diaphragmatic hernia (CDH). In a select group of fetuses with CDH, tracheal occlusion is used to obstruct the normal flow of fetal lung fluid and to stimulate lung expansion and growth. With the airway obstructed, airway management at birth is critical. The solution was to arrange delivery in such a way that the occlusion could be removed and the airway secured while the baby remained on placental support. If the uterus was kept relaxed and the utero-placental blood flow kept intact, the fetus could remain on a maternal 'heart-lung machine' while the airway was secured. While the technique of tracheal occlusion remains under study in clinical trials, EXIT procedures have been shown to be useful for management of other causes of fetal airway obstruction.\n\nThe EXIT is much more complex than a standard C-section, as it requires careful coordination between the mother's physicians and the specialists operating on the newborn baby. The difficulty lies in preserving enough blood flow through the umbilical cord, protecting the placenta, and avoiding contractions of the uterus so that there is sufficient time to establish the airway. Also, the umbilical cord should not be manipulated, but should be kept in warmed fluids to avoid physiological occlusion.\n\n"}
{"id": "53853376", "url": "https://en.wikipedia.org/wiki?curid=53853376", "title": "Farhat N. Beg", "text": "Farhat N. Beg\n\nFarhat N. Beg from the University of California, San Diego, was awarded the status of Fellow in the American Physical Society, after he was nominated by his Division of Plasma Physics in 2009, for contributions to the understanding of physics of short pulse high intensity laser matter interactions and pulsed power driven dense Z-pinches. His empirical scaling of hot electron temperature versus laser internsity has contributed significantly to the understanding of relativistic electron generation and transport in matter. He was the recipient of the Department of Energy Early Career Award in 2005 as well as the IEEE Early Achievement Award in 2008.' He also has been a fellow of the IEEE since 2011. He currently is the director of the Center for Energy Research at UCSD with a focus on Inertial Confinement Fusion.'\n"}
{"id": "14506397", "url": "https://en.wikipedia.org/wiki?curid=14506397", "title": "Fiennes Cornwallis", "text": "Fiennes Cornwallis\n\nMajor Fiennes Cornwallis, born Fiennes Wykeham-Martin (1 November 1831 – 24 April 1867), was a British Army officer and related to the Cornwallis family.\n\nBorn\n1 November 1831 at Leeds Castle, Kent, England. Son of Charles Wykeham Martin M.P. \nand Lady Jemima Isabella (née Mann) \nand was educated at Eton College.\n\nMajor Cornwallis, Married Harriet Elizabeth (née Mott), daughter of John Thomas Mott on 29 July 1863 and had 4 children.\nThe eldest became 1st Baron Cornwallis.\n\nCornwallis was the grandson of James Mann, 5th Earl Cornwallis; the great-grandson of James Cornwallis, 4th Earl Cornwallis; the 2nd great-grandson of Charles Cornwallis, 1st Earl Cornwallis; the 3rd great-grandson of Charles Cornwallis, 4th Baron Cornwallis; the 4th great-grandson of Charles Cornwallis, 3rd Baron Cornwallis; the 5th great-grandson of Charles Cornwallis, 2nd Baron Cornwallis; the 6th great-grandson of Frederick Cornwallis, 1st Baron Cornwallis; and the 7th great-grandson of Jane Cornwallis, and Elizabeth Richardson, 1st Lady Cramond. His other ancestors include Edward Cornwallis, Frederick Cornwallis, Charles Cornwallis, 1st Marquess Cornwallis, William Cornwallis, and Charles Cornwallis, 2nd Marquess Cornwallis.\n\nServed in the Crimean War with the 4th Light Dragoons and took part in the Charge of the Light Brigade, becoming aide-de-camp to Lord George Paget in the Crimea.\n\nCornwallis retired from the Army on 5 May 1863 with the rank of Major and died few years later on 24 April 1867 in a hunting accident.\n\n\n"}
{"id": "14713952", "url": "https://en.wikipedia.org/wiki?curid=14713952", "title": "Hamilton Goold-Adams", "text": "Hamilton Goold-Adams\n\nMajor Sir Hamilton John Goold-Adams, (27 June 1858 – 12 April 1920) was an Irish soldier and colonial administrator who was Governor of Queensland from 1915 to 1920.\n\nBorn in the town of Jamesbrook in County Cork, Ireland, fourth son of Richard Wallis Goold-Adams (1802–73) and Mary Sarah Goold-Adams (d. 1899), daughter of Sir William Wrixon-Becher. Hamilton Goold-Adams was a cadet in the training ship HMS \"Conway\" until he decided to join the British Army and was commissioned in the Royal Scots Regiment, serving principally in southern Africa, where he achieved the rank of captain in 1885 and major in 1895. During the Second Boer War he served first as Resident Commissioner in Bechuanaland and afterwards as commander of the Town Guard during the latter half of the Siege of Mafeking where he was twice Mentioned in Despatches.\nHe was appointed Deputy Commissioner of the Orange River Colony under the Administrator Sir Alfred Milner (later Lord Milner) in January 1901. Following the end of hostilities in May 1902, the colony formally received a new constitution on 23 June, and Goold-Adams was appointed Lieutenant-Governor, serving as such until 1907, when he became governor.\n\nHe was made a Companion of the Order of St Michael and St George (CMG) in 1902, and received the Knight Grand Cross of the order (GCMG) in 1907.\n\nHe returned to England in 1911 where he married a Canadian named Elsie Riordon on 4 July. Later that year he was appointed British High Commissioner to Cyprus.\nIn 1914 he was made Governor of Queensland, and arrived in Brisbane just before the election of Queensland's first majority Labor government, under Premier T. J. Ryan. He occasionally disapproved of Labor's policies and majority appointments to the Legislative Council of Queensland.\n\nReturning to England after his retirement, Goold-Adams contracted pleurisy on board ship, and died in Cape Town, South Africa in 1920.\n\n"}
{"id": "38850326", "url": "https://en.wikipedia.org/wiki?curid=38850326", "title": "Having You", "text": "Having You\n\nHaving You is a British drama film directed and written by Sam Hoare. The film stars Anna Friel, Romola Garai, Andrew Buchan, Phil Davis and Harry Hadden-Paton.\n\nAn easy going young man named Jack, a bit of a directionless 'lost-soul' and previous hedonistic 'wild-child' alcoholic (and unfortunate believer that most he's attempted in life he's failed at miserably) is attempting to, on this his seventh straight year of sobriety, to straighten out his life once and for all by taking his relationship in a more serious direction with a proposal of marriage. He's living with his partner Camilla, an assertive aspiring young doctor who is pretty much the driving force (not to mention, main financial supporter) of their otherwise contented life together. And even though he live in a house bought and paid for by Camilla's wealthy father, still Jack struggles to afford an engagement ring to go with the very proposal he's just sprung on his tearfully accepting now fiancee. Camilla, not only jumps straight into reception planning, but having previously suffered a problematic miscarriage, pregnancy preoccupation and planning has shot to the top of her priorities list. Jack, though terrified at the idea of becoming a father, is nevertheless even more fearful of loosing the only thing truly stable and loving in his life, so silently acquiesces to fiancee's wishes. But... little does he know just how much his life is about to change, when he answers the door to someone he doesn't immediately recognise but who certainly seems to know him... \n\nThe incessant ringer at the door, who's just showed up and about to implode his life with a mere but loaded sentence, is Anna (Anna Friel). Soon Jack is reminded that Anna is someone he drunkenly hooked-up with at some concert 8 years previously - a one-night stand as it happens - and informs him that they have a seven-year-old son called Phoenix. At first Jack is rude and dismissive, clutching at denials and misremembered snippets of having worn a condom. But Anna's insistence on a paternity test soon sets him on the right path to accept and meet his son. Jack begins to spend time with Phoenix but is reluctant to tell Camilla about everything that has been going on, unsure how to juggle both sides of his life at once. Meanwhile, Camilla's meticulous ovulation tracking soon means that Jack is about to be father for a second time. However, fearful of losing out, Jack mishandles the situation by not be ing honest with either party. Camilla, however, soon finds out the truth when Phoenix needs to be rushed to the hospital she's working at after a nasty fall from a jungle-gym at the playground during one of their weekly visits. Consequently, feeling lied to and betrayed, Camilla kicks Jack out the house, cancels their wedding plans and wants nothing to do with him. Jack is rather devastated and attempts to explain and set things right, but ultimately understands that the right choice is actually making his son his priority and two just give the situation some time. \n\nLater it is revealed that Anna has a terminal illness (ovarian cancer) and that Anna's awareness of doomed fate is what prompted her to seek out Jack in the first place. In order that Phoenix not be left alone in the world after such a traumatic event as losing his mother. She asks Jack to promise to care for Phoenix as he'll be needing his father more than ever once she's gone - which Jack immediately agrees and promises to do. In the end Anna dies and Phoenix goes to live with Jack, who eventually makes things right with Camilla too. The film ends just before Camilla is due to give birth to their baby. But it's implied that the four will go on to heal and grow into one happy family together, finally having all they wished for (even more so!) at the beginning of the film. \n\n"}
{"id": "209887", "url": "https://en.wikipedia.org/wiki?curid=209887", "title": "Hula painted frog", "text": "Hula painted frog\n\nThe Hula painted frog (\"Latonia nigriventer\") is an amphibian and the only living member of the genus \"Latonia\". It was thought to be extinct as a result of habitat destruction during the 1950s until the species was rediscovered in 2011. It is endemic to the Lake Hula marshes in Israel.\n\nThe draining of Lake Hula and its marshes in the 1950s was thought to have caused the extinction of this frog, along with the cyprinid fish \"Acanthobrama hulensis\" and cichlid fish \"Tristramella intermedia\". Only five individuals had been found prior to the draining of the lake. Environmental improvements in the Hula reserve have been cited as a possible reason for the frog's reemergence.\n\nThe Hula painted frog has a dark belly with small white spots. It is colored ochre above with a rusty colour grading into dark olive-grey to greyish-black on the sides. Differences from the common painted frog (\"Discoglossus pictus\") include its greater interocular distance, longer forelimbs, and a less projecting snout. The type specimen was an adult female with a body length of \n\nLittle is known about its history, because few specimens have been found by scientists. Two adults and two tadpoles were collected in 1940 and a single specimen was found in 1955. This would prove to be the last record of this species until 2011.\n\nThe four 1940 specimens were to be used as types, but the smaller, half-grown frog was eaten by the larger one in captivity.\n\nAccording to an ecologist of the Israel Nature and Parks Authority, the frog's Hebrew name, \"agulashon shehor-gahon\" (\"Black-bellied round-tongued\"), derives from its black belly and round tongue. The scientific name of the species reflects these details as well. Unlike the tongues of other frogs, it is not used to catch prey.\n\nThis frog was originally proposed to be a member of the genus \"Discoglossus\", but further genetic and morphological assessment after the rediscovery of the species led to a reassignment to \"Latonia\", for which no other living examples are known. Other members of \"Latonia\" are known from the fossil record to have lived as recently as ~1 million years ago. However, based on phylogenetic analysis, it was estimated that the last common ancestor of \"Latonia\" and its closest related genus, \"Discoglossus\", lived approximately 32 million years ago. On this basis, the Hula painted frog has been labeled a living fossil, the only extant representative of an ancient genetic split.\n\nIn 1996, the IUCN classified this species as \"extinct in the wild\", the very first amphibian to be given that designation by the IUCN. Israel continued to list it as an endangered species in the slim hope that a relict population may be found in the Golan Heights or in southern Lebanon. Following the rediscovery of the species in 2011, the IUCN now considers the frog to be critically endangered as its known habitat occupies less than 2 km.\n\nIn 2000, a scientist from the Lebanese nature protection organisation A Rocha claimed he had seen a frog species which could be \"Latonia nigriventer\" in the Aammiq Wetland south of the Beqaa Valley in Lebanon. Two French-Lebanese-British expeditions in the years 2004 and 2005 yielded no confirmation as to the further existence of this species. In August 2010, a search organised by the Amphibian Specialist Group of the International Union for Conservation of Nature set out to look for various species of frogs thought to be extinct in the wild, including the Hula painted frog.\n\nIn 2013, a study published in \"Nature Communications\" revealed that in 2011 during a routine patrol at the Hula Nature Reserve, ranger Yoram Malka found the frog, which he immediately suspected as being the Hula painted frog, as he claimed he has been on the lookout for it for many years. Scientists confirmed that it was one of this rare species. An ecologist with the Israel Nature and Parks Authority credited the rehydration of the area for the frog sighting. On November 29, a second specimen was located in the same area. The second Hula painted frog, a female, was found in swampy weeds twenty centimeters deep. It weighed 13 grams, half the weight of its male counterpart. Since the discovery of the first specimen, at least ten more individuals have been found, all in the same area.\n\nIn 2016, a team led by Professor Professor Sarig Gafni of the Ruppin Academic Center’s School of Marine Sciences discovered populations totaling several hundred individuals by searching in water at night, instead of in marsh mud, finding populations in 17 of the 52 Hula Valley water holes they surveyed.\n\n\n\nAdditional data and discoveries were published in Haaretz newspaper on 15/04/2016\nAmong the discoveries are Tadpole data, which is very small, up to 2.5 cm, and becomes even smaller as adult, the largest adult caught was 13 cm long which suggest it was several decades old.\nAlso sound was first noticed and it is a very weak one.\n"}
{"id": "71617", "url": "https://en.wikipedia.org/wiki?curid=71617", "title": "Infant mortality", "text": "Infant mortality\n\nInfant mortality is the death of young children under the age of 1. This death toll is measured by the infant mortality rate (IMR), which is the number of deaths of children under one year of age per 1000 live births. The under-five mortality rate is also an important statistic, considering the infant mortality rate focuses only on children under one year of age. \n\nPremature birth is the biggest contributor to the IMR. Other leading causes of infant mortality are birth asphyxia, pneumonia, congenital malformations, term birth complications such as abnormal presentation of the foetus umbilical cord prolapse, or prolonged labor, neonatal infection, diarrhea, malaria, measles and malnutrition. One of the most common preventable causes of infant mortality is smoking during pregnancy. Many factors contribute to infant mortality, such as the mother's level of education, environmental conditions, and political and medical infrastructure. Improving sanitation, access to clean drinking water, immunization against infectious diseases, and other public health measures can help reduce high rates of infant mortality.\n\nChild mortality is the death of a child before the child's fifth birthday, measured as the under-5 child mortality rate (U5MR). National statistics sometimes group these two mortality rates together. Globally, 9.2 million children die each year before their fifth birthday; more than 60% of these deaths are seen as being avoidable with low-cost measures such as continuous breast-feeding, vaccinations and improved nutrition.\n\nInfant mortality rate was an indicator used to monitor progress towards the Fourth Goal of the Millennium Development Goals of the United Nations for the year 2015. It is now a target in the Sustainable Development Goals for Goal Number 3 (\"Ensure healthy lives and promote well-being for all at all ages\").\n\nThroughout the world, infant mortality rate (IMR) fluctuates drastically, and according to Biotechnology and Health Sciences, education and life expectancy in the country is the leading indicator of IMR. This study was conducted across 135 countries over the course of 11 years, with the continent of Africa having the highest infant mortality rate of any other region studied with 68 deaths per 1,000 live births.\n\nInfant mortality rate (IMR) is the number of deaths per 1,000 live births of children under one year of age. The rate for a given region is the number of children dying under one year of age, divided by the number of live births during the year, multiplied by 1,000.\n\nForms of infant mortality:\n\nCauses of infant mortality directly lead to the death. Environmental and social barriers prevent access to basic medical resources and thus contribute to an increasing infant mortality rate; 99% of infant deaths occur in developing countries, and 86% of these deaths are due to infections, premature births, complications during delivery, and perinatal asphyxia and birth injuries. Greatest percentage reduction of infant mortality occurs in countries that already have low rates of infant mortality.\nCommon causes are preventable with low-cost measures. In the United States, a primary determinant of infant mortality risk is infant birth weight with lower birth weights increasing the risk of infant mortality. The determinants of low birth weight include socio-economic, psychological, behavioral and environmental factors.\n\nCauses of infant mortality that are related to medical conditions include: low birth weight, sudden infant death syndrome, malnutrition,congenital malformations, and infectious diseases, including neglected tropical diseases.\n\nCongenital malformations are birth defects that babies are born with, such as cleft lip and palate, Down syndrome, and heart defects. Often times, this occurs when the mother consumes alcohol, but it can also be a cause of genetics or have an unknown cause. \ncongenital malformations have had a significant impact on infant mortality. Malnutrition and infectious diseases were the main cause of death in more undeveloped countries. In the Caribbean and Latin America, congenital malformations only accounted for 5% of the infant deaths in these countries while malnutrition and infectious diseases took 7% to 27% of infants in the 1980s. In more developed countries such as the United States, there was a rise in infant deaths due to congenital malformations. These birth defects mostly had to do with heart and central nervous system. In the 19th century, there was a decrease in the number of infant deaths from heart diseases. From 1979 to 1997, there was a 39% decline in infant mortality due to heart problems.\n\nLow birth weight makes up 60–80% of the infant mortality rate in developing countries. \"The New England Journal of Medicine\" stated that \"The lowest mortality rates occur among infants weighing . For infants born weighing or less, the mortality rate rapidly increases with decreasing weight, and most of the infants weighing or less die. As compared with normal-birth-weight infants, those with low weight at birth are almost 40 times more likely to die in the neonatal period; for infants with very low weight at birth the relative risk of neonatal death is almost 200 times greater.\" Infant mortality due to low birth weight is usually a direct cause stemming from other medical complications such as preterm birth, poor maternal nutritional status, lack of prenatal care, maternal sickness during pregnancy, and an unhygienic home environments. Along with birth weight, period of gestation makes up the two most important predictors of an infant's chances of survival and their overall health.\n\nAccording to the \"New England Journal of Medicine\", \"in the past two decades, the infant mortality rate (deaths under one year of age per thousand live births) in the United States has declined sharply.\" Low birth weights from African American mothers remain twice as high as that of white women. LBW may be the leading cause of infant deaths, and it is greatly preventable. Although it is preventable, the solutions may not be the easiest but effective programs to help prevent LBW are a combination of health care, education, environment, mental modification and public policy, influencing a culture supporting lifestyle. Preterm birth is the leading cause of newborn deaths worldwide. Even though America excels past many other countries in the care and saving of premature infants, the percentage of American woman who deliver prematurely is comparable to those in developing countries. Reasons for this include teenage pregnancy, increase in pregnant mothers over the age of thirty-five, increase in the use of in-vitro fertilization which increases the risk of multiple births, obesity and diabetes. Also, women who do not have access to health care are less likely to visit a doctor, therefore increasing their risk of delivering prematurely.\n\nSudden infant death syndrome(SIDS) is a syndrome where an infant dies in their sleep with no reason behind it. Even with a complete autopsy, no one has been able to figure out what causes this disease. This disease is more common in Western countries. Even though researchers are not sure what causes this disease, they have discovered that it is healthier for babies to sleep on their backs instead of their stomachs. This discovery saved many families from the tragedy that this disease causes. Scientists have also discovered three causes within a model they created called, the contemporary triple risk model. This model states that three conditions such as the mother smoking while pregnant, the age of the infant, and stress referring to conditions such as overheating, prone sleeping, co-sleeping, and head covering.\n\nMalnutrition or undernutrition is defined as inadequate intake of nourishment, such as proteins and vitamins, which adversely affects the growth, energy and development of people all over the world. It is especially prevalent in women and infants under 5 who live in developing countries within the poorer regions of Africa, Asia, and Latin America. Children are most vulnerable as they are yet to fully develop a strong immune system, as well as being dependent upon parents to provide the necessary food and nutritional intake. It is estimated that about 3.5 million children die each year as a result of childhood or maternal malnutrition, with stunted growth, low body weight and low birth weight accounting for about 2.2 million associated deaths. Factors which contribute to malnutrition are socioeconomic, environmental, gender status, regional location, and breastfeeding cultural practices. It is difficult to assess the most pressing factor as they can intertwine and vary among regions. \n\nChildren suffering from malnutrition face adverse physical effects such as stunting, wasting, or being overweight. Such characteristics entail difference in weight-and-height ratios for age in comparison to adequate standards. In Africa the number of stunted children has risen, while Asia holds the most children under 5 suffering from wasting. The amount of overweight children has increased among all regions of the globe. Inadequate nutrients adversely effect physical and cognitive developments, increasing susceptibility to severe health problems. Micronutrient deficiency such as iron has been linked to children with anemia, fatigue, and poor brain development. Similarly, the lack of Vitamin A is the leading cause of blindness among malnourished children. The outcome of malnutrition in children results in decreased ability of the immune system to fight infections, resulting in higher rates of death from diseases such as malaria, respiratory disease and diarrhea.\n\nBabies born in low to middle income countries in sub-Saharan Africa and southern Asia are at the highest risk of neonatal death. Bacterial infections of the bloodstream, lungs, and the brain's covering (meningitis) are responsible for 25% of neonatal deaths. Newborns can acquire infections during birth from bacteria that are present in their mother's reproductive tract. The mother may not be aware of the infection, or she may have an untreated pelvic inflammatory disease or sexually transmitted disease. These bacteria can move up the vaginal canal into the amniotic sac surrounding the baby. Maternal blood-borne infection is another route of bacterial infection from mother to baby. Neonatal infection is also more likely with the premature rupture of the membranes (PROM) of the amniotic sac.\n\nSeven out of ten childhood deaths are due to infectious diseases: acute respiratory infection, diarrhea, measles, and malaria. Acute respiratory infection such as pneumonia, bronchitis, and bronchiolitis account for 30% of childhood deaths; 95% of pneumonia cases occur in the developing world. Diarrhea is the second-largest cause of childhood mortality in the world, while malaria causes 11% of childhood deaths. Measles is the fifth-largest cause of childhood mortality. Folic acid for mothers is one way to combat iron deficiency. A few public health measures used to lower levels of iron deficiency anemia include iodize salt or drinking water, and include vitamin A and multivitamin supplements into a mother's diet. A deficiency of this vitamin causes certain types of anemia (low red blood cell count).\n\nInfant mortality rate can be a measure of a nation's health and social condition. It is a composite of a number of component rates which have their separate relationship with various social factors and can often be seen as an indicator to measure the level of socioeconomic disparity within a country.\n\nOrganic water pollution is a better indicator of infant mortality than health expenditures per capita. Water contaminated with various pathogens houses a host of parasitic and microbial infections. Infectious disease and parasites are carried via water pollution from animal wastes. Areas of low socioeconomic status are more prone to inadequate plumbing infrastructure, and poorly maintained facilities. The burning of inefficient fuels doubles the rate of children under 5 years old with acute respiratory tract infections. Climate and geography often play a role in sanitation conditions. For example, the inaccessibility of clean water exacerbates poor sanitation conditions.\n\nPeople who live in areas where particulate matter (PM) air pollution is higher tend to have more health problems across the board. Short-term and long-term effects of ambient air pollution are associated with an increased mortality rate, including infant mortality. Air pollution is consistently associated with post neonatal mortality due to respiratory effects and sudden infant death syndrome. Specifically, air pollution is highly associated with SIDs in the United States during the post-neonatal stage. High infant mortality is exacerbated because newborns are a vulnerable subgroup that is affected by air pollution. Newborns who were born into these environments are no exception. Women who are exposed to greater air pollution on a daily basis who are pregnant should be closely watched by their doctors, as well as after the baby is born. Babies who live in areas with less air pollution have a greater chance of living until their first birthday. As expected, babies who live in environments with more air pollution are at greater risk for infant mortality. Areas that have higher air pollution also have a greater chance of having a higher population density, higher crime rates and lower income levels, all of which can lead to higher infant mortality rates.\n\nThe key pollutant for infant mortality rates is carbon monoxide. Carbon monoxide is a colorless, odorless gas that does great harm especially to infants because of their immature respiratory system.\nAnother major pollutant is second-hand smoke, which is a pollutant that can have detrimental effects on a fetus. According to the \"American Journal of Public Health\", \"in 2006, more than 42 000 Americans died of second hand smoke-attributable diseases, including more than 41 000 adults and nearly 900 infants ... fully 36% of the infants who died of low birth weight caused by exposure to maternal smoking in utero were Blacks, as were 28% of those dying of respiratory distress syndrome, 25% dying of other respiratory conditions, and 24% dying of sudden infant death syndrome.\" \"The American Journal of Epidemiology\" also stated that \"Compared with nonsmoking women having their first birth, women who smoked less than one pack of cigarettes per day had a 25% greater risk of mortality, and those who smoked one or more packs per day had a 56% greater risk. Among women having their second or higher birth, smokers experienced 30% greater mortality than nonsmokers.\"\n\nModern research in the United States on racial disparities in infant mortality suggests a link between the institutionalized racism that pervades the environment and high rates of African American infant mortality. In synthesis of this research, it has been observed that \"African American infant mortality remains elevated due to the social arrangements that exist between groups and the lifelong experiences responding to the resultant power dynamics of these arrangements.\"\n\nIt is important to note that infant mortality rates do not decline among African Americans even if their socio-economic status does improve. Parker Dominguez at the University of Southern California has made some headway in determining the reasoning behind this, claiming black women are more prone to psychological stress than other women of different races in the United States. Stress is a lead factor in inducing labor in pregnant women, and therefore high levels of stress during pregnancy could lead to premature births that have the potential to be fatal for the infant.\n\nEarly childhood trauma includes physical, sexual, and psychological abuse of a child ages zero to five years-old. Trauma in early development has extreme impact over the course of a lifetime and is a significant contributor to infant mortality. Developing organs are fragile. When an infant is shaken, beaten, strangled, or raped the impact is exponentially more destructive than when the same abuse occurs in a fully developed body. Studies estimate that 1–2 per 100,000 U.S. children annually are fatally injured. Unfortunately, it is reasonable to assume that these statistics under represent actual mortality. Three-quarters (74.8 percent) of child fatalities in FFY 2015 involved children younger than 3 years, and children younger than 1 year accounted for 49.4 percent of all fatalities. In particular, correctly identifying deaths due to neglect is problematic and children with sudden unexpected death or those with what appear to be unintentional causes on the surface often have preventable risk factors which are substantially similar to those in families with maltreatment. \n\nThere is a direct relationship between age of maltreatment/injury and risk for death. The younger an infant is, the more dangerous the maltreatment. \n\nFamily configuration, child gender, social isolation, lack of support, maternal youth, marital status, poverty, parental ACES, and parenting practices are thought to contribute to increased risk.\nSocial class is a major factor in infant mortality, both historically and today. Between 1912 and 1915, the Children's Bureau in the United States examined data across eight cities and nearly 23,000 live births. They discovered that lower incomes tend to correlate with higher infant mortality. In cases where the father had no income, the rate of infant mortality was 357% more than that for the highest income earners ($1,250+). Differences between races were also apparent. African-American mothers experience infant mortality at a rate 44% higher than average; however, research indicates that socio-economic factors do not totally account for the racial disparities in infant mortality.\n\nWhile infant mortality is normally negatively correlated with GDP, there may indeed be some opposing short-term effects from a recession. A recent study by \"The Economist\" showed that economic slowdowns reduce the amount of air pollution, which results in a lower infant mortality rate. In the late 1970s and early 1980s, the recession's impact on air quality is estimated to have saved around 1,300 US babies. It is only during deep recessions that infant mortality increases. According to Norbert Schady and Marc-François Smitz, recessions when GDP per capita drops by 15% or more increase infant mortality.\n\nSocial class dictates which medical services are available to an individual. Disparities due to socioeconomic factors have been exacerbated by advances in medical technology. Developed countries, most notably the United States, have seen a divergence between those living in poverty who cannot afford medical advanced resources, leading to an increased chance of infant mortality, and others.\n\nIn policy, there is a lag time between realization of a problem's possible solution and actual implementation of policy solutions. Infant mortality rates correlate with war, political unrest, and government corruption.\n\nIn most cases, war-affected areas will experience a significant increase in infant mortality rates. Having a war taking place where a woman is planning on having a baby is not only stressful on the mother and foetus, but also has several detrimental effects.\n\nHowever, many other significant factors influence infant mortality rates in war-torn areas. Health care systems in developing countries in the midst of war often collapse. Attaining basic medical supplies and care becomes increasingly difficult. During the Yugoslav Wars in the 1990s Bosnia experienced a 60% decrease in child immunizations. Preventable diseases can quickly become epidemic given the medical conditions during war.\n\nMany developing countries rely on foreign aid for basic nutrition. Transport of aid becomes significantly more difficult in times of war. In most situations the average weight of a population will drop substantially. Expecting mothers are affected even more by lack of access to food and water. During the Yugoslav Wars in Bosnia the number of premature babies born increased and the average birth weight decreased.\n\nThere have been several instances in recent years of systematic rape as a weapon of war. Women who become pregnant as a result of war rape face even more significant challenges in bearing a healthy child. Studies suggest that women who experience sexual violence before or during pregnancy are more likely to experience infant death in their children. Causes of infant mortality in abused women range from physical side effects of the initial trauma to psychological effects that lead to poor adjustment to society. Many women who became pregnant by rape in Bosnia were isolated from their hometowns making life after childbirth exponentially more difficult.\n\nDeveloping countries have a lack of access to affordable and professional health care resources, and skilled personnel during deliveries. Countries with histories of extreme poverty also have a pattern of epidemics, endemic infectious diseases, and low levels of access to maternal and child healthcare.\n\nThe American Academy of Pediatrics recommends that infants need multiple doses of vaccines such as diphtheria-tetanus-acellular pertussis vaccine, Haemophilus influenzae type b (Hib) vaccine, Hepatitis B (HepB) vaccine, inactivated polio vaccine (IPV), and pneumococcal vaccine (PCV). Research was conducted by the Institute of Medicine's Immunization Safety Review Committee concluded that there is no relationship between these vaccines and risk of SIDS in infants. This tells us that not only is it extremely necessary for every child to get these vaccines to prevent serious diseases, but there is no reason to believe that if your child does receive an immunization that it will have any effect on their risk of SIDS.\n\nPolitical modernization perspective, the neo-classical economic theory that scarce goods are most effectively distributed to the market, say that the level of political democracy influences the rate of infant mortality. Developing nations with democratic governments tend to be more responsive to public opinion, social movements, and special interest groups for issues like infant mortality. In contrast, non-democratic governments are more interested in corporate issues and less so in health issues. Democratic status effects the dependency a nation has towards its economic state via export, investments from multinational corporations and international lending institutions.\n\nLevels of socioeconomic development and global integration are inversely related to a nation's infant mortality rate. Dependency perspective occurs in a global capital system. A nation's internal impact is highly influenced by its position in the global economy and has adverse effects on the survival of children in developing countries. Countries can experience disproportionate effects from its trade and stratification within the global system. It aids in the global division of labor, distorting the domestic economy of developing nations. The dependency of developing nations can lead to a reduce rate of economic growth, increase income inequality inter- and intra-national, and adversely affects the wellbeing of a nation's population. A collective cooperation between economic countries plays a role in development policies in the poorer, peripheral, countries of the world.\n\nThese economic factors present challenges to governments' public health policies. If the nation's ability to raise its own revenues is compromised, governments will lose funding for its health service programs, including services that aim in decreasing infant mortality rates. Peripheral countries face higher levels of vulnerability to the possible negative effects of globalization and trade in relation to key countries in the global market.\n\nEven with a strong economy and economic growth (measured by a country's gross national product), the advances of medical technologies may not be felt by everyone, lending itself to increasing social disparities.\n\nHigh rates of infant mortality occur in developing countries where financial and material resources are scarce and there is a high tolerance to high number of infant deaths. There are circumstances where a number of developing countries to breed a culture where situations of infant mortality such as favoring male babies over female babies are the norm. In developing countries such as Brazil, infant mortality rates are commonly not recorded due to failure to register for death certificates. Failure to register is mainly due to the potential loss of time and money and other indirect costs to the family. Even with resource opportunities such as the 1973 Public Registry Law 6015, which allowed free registration for low-income families, the requirements to qualify hold back individuals who are not contracted workers.\n\nAnother cultural reason for infant mortality, such as what is happening in Ghana, is that \"besides the obvious, like rutted roads, there are prejudices against wives or newborns leaving the house.\" Because of this it is making it even more difficult for the women and newborns to get the treatment that is available to them and that is needed.\n\nCultural influences and lifestyle habits in the United States can account for some deaths in infants throughout the years. According to the Journal of the American Medical Association \"the post neonatal mortality risk (28 to 364 days) was highest among continental Puerto Ricans\" compared to babies of the non-Hispanic race. Examples of this include teenage pregnancy, obesity, diabetes and smoking. All are possible causes of premature births, which constitute the second highest cause of infant mortality. Ethnic differences experienced in the United States are accompanied by higher prevalence of behavioral risk factors and sociodemographic challenges that each ethnic group faces.\n\nHistorically, males have had higher infant mortality rates than females. The difference between male and female infant mortality rates have been dependent on environmental, social, and economic conditions. More specifically, males are biologically more vulnerable to infections and conditions associated with prematurity and development. Before 1970, the reasons for male infant mortality were due to infections, and chronic degenerative diseases. However, since 1970, certain cultures emphasizing males has led to a decrease in the infant mortality gap between males and females. Also, medical advances have resulted in a growing number of male infants surviving at higher rates than females due to the initial high infant mortality rate of males.\n\nGenetic components results in newborn females being biologically advantaged when it comes to surviving their first birthday. Males, biologically, have lower chances of surviving infancy in comparison to female babies. As infant mortality rates saw a decrease on a global scale, the gender most affected by infant mortality changed from males experiences a biological disadvantage, to females facing a societal disadvantage. Some developing nations have social and cultural patterns that reflects adult discrimination to favor boys over girls for their future potential to contribute to the household production level. A country's ethnic composition, homogeneous versus heterogeneous, can explain social attitudes and practices. Heterogeneous level is a strong predictor in explaining infant mortality.\n\nBirth spacing is the time between births. Births spaced at least three years apart from one another are associated with the lowest rate of mortality. The longer the interval between births, the lower the risk for having any birthing complications, and infant, childhood and maternal mortality. Higher rates of pre-term births, and low birth weight are associated with birth to conception intervals of less than six months and abortion to pregnancy interval of less than six months. Shorter intervals between births increase the chances of chronic and general under-nutrition; 57% of women in 55 developing countries reported birth spaces shorter than three years; 26% report birth spacing of less than two years. Only 20% of post-partum women report wanting another birth within two years; however, only 40% are taking necessary steps such as family planning to achieve the birth intervals they want.\n\nUnplanned pregnancies and birth intervals of less than twenty-four months are known to correlate with low birth weights and delivery complications. Also, women who are already small in stature tend to deliver smaller than average babies, perpetuating a cycle of being underweight.\n\nTo reduce infant mortality rates across the world health practitioners, governments, and non-governmental organizations have worked to create institutions, programs and policies to generate better health outcomes. Improvements such as better sanitation practices have proven to be effective in reducing public health outbreaks and rates of disease among mothers and children. Efforts to increase a households' income through direct assistance or economic opportunities decreases mortality rates, as families possess some means for more food and access to healthcare. Education campaigns, disseminating knowledge among urban and rural regions, and better access to education attainment prove to be an effective strategy to reduce infant and mother mortality rates. Current efforts from NGOs and governments are focused developing human resources, strengthening health information systems, health services delivery, etc. Improvements in such areas have increased regional health systems and aided in efforts to reduce mortality rates. \n\nReductions in infant mortality are possible in any stage of a country's development. Rate reductions are evidence that a country is advancing in human knowledge, social institutions and physical capital. Governments can reduce the mortality rates by addressing the combined need for education (such as universal primary education), nutrition, and access to basic maternal and infant health services. A policy focus has the potential to aid those most at risk for infant and childhood mortality allows rural, poor and migrant populations.\n\nReducing chances of babies being born at low birth weights and contracting pneumonia can be accomplished by improving air quality. Improving hygiene can prevent infant mortality. Home-based technology to chlorinate, filter, and solar disinfection for organic water pollution could reduce cases of diarrhea in children by up to 48%. Improvements in food supplies and sanitation has been shown to work in the United States' most vulnerable populations, one being African Americans. Overall, women's health status need to remain high.\n\nSimple behavioral changes, such as hand washing with soap, can significantly reduce the rate of infant mortality from respiratory and diarrheal diseases. According to UNICEF, hand washing with soap before eating and after using the toilet can save more lives of children than any single vaccine or medical intervention, by cutting deaths from diarrhea and acute respiratory infections.\n\nFuture problems for mothers and babies can be prevented. It is important that women of reproductive age adopt healthy behaviors in everyday life, such as taking folic acid, maintaining a healthy diet and weight, being physically active, avoiding tobacco use, and avoiding excessive alcohol and drug use. If women follow some of the above guidelines, later complications can be prevented to help decrease the infant mortality rates. Attending regular prenatal care check-ups will help improve the baby's chances of being delivered in safer conditions and surviving.\n\nFocusing on preventing preterm and low birth weight deliveries throughout all populations can help to eliminate cases of infant mortality and decrease health care disparities within communities. In the United States, these two goals have decreased infant mortality rates on a regional population, it has yet to see further progress on a national level.\n\nTechnological advances in medicine would decrease the infant mortality rate and an increased access to such technologies could decrease racial and ethnic disparities. It has been shown that technological determinants are influenced by social determinants. Those who cannot afford to utilize advances in medicine tend to show higher rates of infant mortality. Technological advances has, in a way, contributed to the social disparities observed today. Providing equal access has the potential to decrease socioeconomic disparities in infant mortality. Specifically, Cambodia is facing issues with a disease that is unfortunately killing infants. The symptoms only last 24 hours and the result is death. As stated if technological advances were increased in countries it would make it easier to find the solution to diseases such as this. Recently, there have been declines in the United States that could be attributed to advances in technology. Advancements in the Neonatal Intensive Care Unit can be related to the decline in infant mortality in addition to the advancement of surfactants. However, the importance of the advancement of technology remains unclear as the number of high-risk births increases in the United States.\n\nIt has been well documented that increased education among mothers, communities, and local health workers results in better family planning, improvement on children's health, and lower rates of children's deaths. High-risk areas, such as Sub-Saharan Africa, have demonstrated that an increase in women's education attainment leads to a reduction in infant mortality by about 35%. Similarly, coordinated efforts to train community health workers in diagnosis, treatment, malnutrition prevention, reporting and referral services has reduced infant mortality in children under 5 as much as 38%. Public health campaigns centered around the \"First 1,000 Days\" of conception have been successful in providing cost-effective supplemental nutrition programs, as well as assisting young mothers in sanitation, hygiene and breastfeeding promotion. Increased intake of nutrients and better sanitation habits have a positive impact on health, especially developing children. Educational attainment and public health campaigns provide the knowledge and means to practice better habits and leads to better outcomes against infant mortality rates. \n\nAwareness of health services, education, and economic opportunities provide means to sustain and increase chance of development and survival. A decrease on GPD, for example, results in increased rates of infant mortality. Negative effects on household income reduces amount being spent on food and healthcare, affecting the quality of life and access to medical services to ensure full development and survival. On the contrary, increased household income translates to more access to nutrients and healthcare, reducing the risks associated with malnutrition and infant mortality. Moreover, increased aggregate household incomes will produce better health facilities, water and sewer infrastructures for the entire community. \n\nGranting women employment raises their status and autonomy. Having a gainful employment can raise the perceived worth of females. This can lead to an increase in the number of women getting an education and a decrease in the number of female infanticide. In the social modernization perspective, education leads to development. Higher number of skilled workers means more earning and further economic growth. According to the economic modernization perspective, this is one type economic growth viewed as the driving force behind the increase in development and standard of living in a country. This is further explained by the modernization theory- economic development promotes physical wellbeing. As economy rises, so do technological advances and thus, medical advances in access to clean water, health care facilities, education, and diet. These changes may decrease infant mortality.\n\nEconomically, governments could reduce infant mortality by building and strengthening capacity in human resources. Increasing human resources such as physicians, nurses, and other health professionals will increase the number of skilled attendants and the number of people able to give out immunized against diseases such as measles. Increasing the number of skilled professionals is negatively correlated with maternal, infant, and childhood mortality. Between 1960 and 2000, the infant mortality rate decreased by half as the number of physicians increased by four folds. With the addition of one physician to every 1000 persons in a population, infant mortality will reduce by 30%.\n\nIn certain parts of the U.S., specific modern programs aim to reduce levels of infant mortality. An example of one such program is the 'Healthy Me, Healthy You' program based in Northeast Texas. It intends to identify factors that contribute to negative birth outcomes throughout a 37-county area. An additional program that aims to reduce infant mortality is the \"Best Babies Zone\" (BBZ) based at the University of California, Berkeley. The BBZ uses the life course approach to address the structural causes of poor birth outcomes and toxic stress in three U.S. neighborhoods. By employing community-generated solutions, the Best Babies Zone's ultimate goal is to achieve health equity in communities that are disproportionately impacted by infant death.\n\nThe infant mortality rate correlates very strongly with, and is among the best predictors of, state failure. IMR is therefore also a useful indicator of a country's level of health or development, and is a component of the physical quality of life index.\n\nHowever, the method of calculating IMR often varies widely between countries, and is based on how they define a live birth and how many premature infants are born in the country. Reporting of infant mortality rates can be inconsistent, and may be understated, depending on a nation's live birth criterion, vital registration system, and reporting practices. The reported IMR provides one statistic which reflects the standard of living in each nation. Changes in the infant mortality rate reflect social and technical capacities of a nation's population. The World Health Organization (WHO) defines a live birth as any infant born demonstrating independent signs of life, including breathing, heartbeat, umbilical cord pulsation or definite movement of voluntary muscles. This definition is used in Austria, for example. The WHO definition is also used in Germany, but with one slight modification: muscle movement is not considered to be a sign of life. Many countries, however, including certain European states (e.g. France) and Japan, only count as live births cases where an infant breathes at birth, which makes their reported IMR numbers somewhat lower and increases their rates of perinatal mortality. In the Czech Republic and Bulgaria, for instance, requirements for live birth are even higher.\n\nAlthough many countries have vital registration systems and certain reporting practices, there are many inaccuracies, particularly in undeveloped nations, in the statistics of the number of infants dying. Studies have shown that comparing three information sources (official registries, household surveys, and popular reporters) that the \"popular death reporters\" are the most accurate. Popular death reporters include midwives, gravediggers, coffin builders, priests, and others—essentially people who knew the most about the child's death. In developing nations, access to vital registries, and other government-run systems which record births and deaths, is difficult for poor families for several reasons. These struggles force stress on families, and make them take drastic measures in unofficial death ceremonies for their deceased infants. As a result, government statistics will inaccurately reflect a nation's infant mortality rate. Popular death reporters have first-hand information, and provided this information can be collected and collated, can provide reliable data which provide a nation with accurate death counts and meaningful causes of deaths that can be measured/studied.\n\nUNICEF uses a statistical methodology to account for reporting differences among countries:\nAnother challenge to comparability is the practice of counting frail or premature infants who die before the normal due date as miscarriages (spontaneous abortions) or those who die during or immediately after childbirth as stillborn. Therefore, the quality of a country's documentation of perinatal mortality can matter greatly to the accuracy of its infant mortality statistics. This point is reinforced by the demographer Ansley Coale, who finds dubiously high ratios of reported stillbirths to infant deaths in Hong Kong and Japan in the first 24 hours after birth, a pattern that is consistent with the high recorded sex ratios at birth in those countries. It suggests not only that many female infants who die in the first 24 hours are misreported as stillbirths rather than infant deaths, but also that those countries do not follow WHO recommendations for the reporting of live births and infant deaths.\n\nAnother seemingly paradoxical finding, is that when countries with poor medical services introduce new medical centers and services, instead of declining, the reported IMRs often increase for a time. This is mainly because improvement in access to medical care is often accompanied by improvement in the registration of births and deaths. Deaths that might have occurred in a remote or rural area, and not been reported to the government, might now be reported by the new medical personnel or facilities. Thus, even if the new health services reduce the actual IMR, the reported IMR may increase.\n\nCollecting the accurate statistics of infant mortality rate could be an issue in some rural communities in developing countries. In those communities, some other alternative methods for calculating infant mortality rate are emerged, for example, popular death reporting and household survey.\nThe country-to-country variation in child mortality rates is huge, and growing wider despite the progress. Among the world's roughly 200 nations, only Somalia showed no decrease in the under-5 mortality rate over the past two decades.The lowest rate in 2011 was in Singapore, which had 2.6 deaths of children under age 5 per 1,000 live births. The highest was in Sierra Leone, which had 185 child deaths per 1,000 births. The global rate is 51 deaths per 1,000 births. For the United States, the rate is eight per 1,000 births.\n\nInfant mortality rate (IMR) is not only a group of statistic but instead it is a reflection of the socioeconomic development and effectively represents the presence of medical services in the countries. IMR is an effective resource for the health department to make decision on medical resources reallocation. IMR also formulates the global health strategies and help evaluate the program success. The existence of IMR helps solve the inadequacies of the other vital statistic systems for global health as most of the vital statistic systems usually neglect the infant mortality statistic number from the poor. There are certain amounts of unrecorded infant deaths in the rural area as they do not have information about infant mortality rate statistic or do not have the concept about reporting early infant death.\n\nThe exclusion of any high-risk infants from the denominator or numerator in reported IMRs can cause problems in making comparisons. Many countries, including the United States, Sweden and Germany, count an infant exhibiting any sign of life as alive, no matter the month of gestation or the size, but according to United States some other countries differ in these practices. All of the countries named adopted the WHO definitions in the late 1980s or early 1990s, which are used throughout the European Union. However, in 2009, the US CDC issued a report that stated that the American rates of infant mortality were affected by the United States' high rates of premature babies compared to European countries. It also outlined the differences in reporting requirements between the United States and Europe, noting that France, the Czech Republic, Ireland, the Netherlands, and Poland do not report all live births of babies under 500 g and/or 22 weeks of gestation. However, the differences in reporting are unlikely to be the primary explanation for the United States' relatively low international ranking. Rather, the report concluded that primary reason for the United States’ higher infant mortality rate when compared with Europe was the United States’ much higher percentage of preterm births.\n\nThe US National Institute of Child Health and Human Development (NICHD) has made great strides in lowering US infant mortality rates. Since the institute was created the US infant mortality rate has dropped 70%, in part due to their research.\n\nUntil the 1990s, Russia and the Soviet Union did not count, as a live birth or as an infant death, extremely premature infants (less than 1,000 g, less than 28 weeks gestational age, or less than 35 cm in length) that were born alive (breathed, had a heartbeat, or exhibited voluntary muscle movement) but failed to survive for at least seven days. Although such extremely premature infants typically accounted for only about 0.5% of all live-born children, their exclusion from both the numerator and the denominator in the reported IMR led to an estimated 22%–25% lower reported IMR. In some cases, too, perhaps because hospitals or regional health departments were held accountable for lowering the IMR in their catchment area, infant deaths that occurred in the 12th month were \"transferred\" statistically to the 13th month (i.e., the second year of life), and thus no longer classified as an infant death.\n\nIn certain rural developing areas, such as northeastern Brazil, infant births are often not recorded in the first place, resulting in the discrepancies between the infant mortality rate (IMR) and the actual amount of infant deaths. Access to vital registry systems for infant births and deaths is an extremely difficult and expensive task for poor parents living in rural areas. Government and bureaucracies tend to show an insensitivity to these parents and their recent suffering from a lost child, and produce broad disclaimers in the IMR reports that the information has not been properly reported, resulting in these discrepancies. Little has been done to address the underlying structural problems of the vital registry systems in respect to the lack of reporting from parents in rural areas, and in turn has created a gap between the official and popular meanings of child death. It is also argued that the bureaucratic separation of vital death recording from cultural death rituals is to blame for the inaccuracy of the infant mortality rate (IMR). Vital death registries often fail to recognize the cultural implications and importance of infant deaths. It is not to be said that vital registry systems are not an accurate representation of a region's socio-economic situation, but this is only the case if these statistics are valid, which is unfortunately not always the circumstance. \"Popular death reporters\" is an alternative method for collecting and processing statistics on infant and child mortality. Many regions may benefit from \"popular death reporters\" who are culturally linked to infants may be able to provide more accurate statistics on the incidence of infant mortality. According to ethnographic data, \"popular death reporters\" refers to people who had inside knowledge of \"anjinhos\", including the grave-digger, gatekeeper, midwife, popular healers etc. —— all key participants in mortuary rituals. By combining the methods of household surveys, vital registries, and asking \"popular death reporters\" this can increase the validity of child mortality rates, but there are many barriers that can reflect the validity of our statistics of infant mortality. One of these barriers are political economic decisions. Numbers are exaggerated when international funds are being doled out; and underestimated during reelection.\n\nThe bureaucratic separation of vital death reporting and cultural death rituals stems in part due to structural violence. Individuals living in rural areas of Brazil need to invest large capital for lodging and travel in order to report infant birth to a Brazilian Assistance League office. The negative financial aspects deters registration, as often individuals are of lower income and cannot afford such expenses. Similar to the lack of birth reporting, families in rural Brazil face difficult choices based on already existing structural arrangements when choosing to report infant mortality. Financial constraints such as reliance on food supplementations may also lead to skewed infant mortality data.\n\nIn developing countries such as Brazil the deaths of impoverished infants are regularly unrecorded into the countries vital registration system; this causes a skew statistically. Culturally validity and contextual soundness can be used to ground the meaning of mortality from a statistical standpoint. In northeast Brazil they have accomplished this standpoint while conducting an ethnographic study combined with an alternative method to survey infant mortality. These types of techniques can develop quality ethnographic data that will ultimately lead to a better portrayal of the magnitude of infant mortality in the region. Political economic reasons have been seen to skew the infant mortality data in the past when governor Ceara devised his presidency campaign on reducing the infant mortality rate during his term in office. By using this new way of surveying, these instances can be minimized and removed, overall creating accurate and sound data.\n\nFor the world, and for both less developed countries (LDCs) and more developed countries (MDCs), IMR declined significantly between 1960 and 2001. According to the State of the World's Mothers report by Save the Children, the world IMR declined from 126 in 1960 to 57 in 2001.\n\nHowever, IMR was, and remains, higher in LDCs. In 2001, the IMR for LDCs (91) was about 10 times as large as it was for MDCs (8). On average, for LDCs, the IMR is 17 times as higher than that of MDCs. Also, while both LDCs and MDCs made significant reductions in infant mortality rates, reductions among less developed countries are, on average, much less than those among the more developed countries.\n\nA factor of about 67 separate countries with the highest and lowest reported infant mortality rates. The top and bottom five countries by this measure (taken from The World Factbook's 2012 estimates) are shown below.\n\nAccording to Guillot, Gerland, Pelletier and Saabneh \"birth histories, however, are subject to a number of errors, including omission of deaths and age misreporting errors.\"\n\nThe infant mortality rate in the US decreased by 2.3% to a historic low of 582 infant deaths per 100,000 live births in 2014.\n\nOf the 27 most developed countries, the U.S. has the highest Infant Mortality Rate, despite spending much more on health care per capita. Significant racial and socio-economic differences in the United States affect the IMR, in contrast with other developed countries, which have more homogeneous populations. In particular, IMR varies greatly by race in the US. The average IMR for the whole country is therefore not a fair representation of the wide variations that exist between segments of the population. Many theories have been explored as to why these racial differences exist with socio economic factors usually coming out as a reasonable explanation. However, more studies have been conducted around this matter, and the largest advancement is around the idea of stress and how it affects pregnancy.\n\nIn the 1850s, the infant mortality rate in the United States was estimated at 216.8 per 1,000 babies born for whites and 340.0 per 1,000 for African Americans, but rates have significantly declined in the West in modern times. This declining rate has been mainly due to modern improvements in basic health care, technology, and medical advances. In the last century, the infant mortality rate has decreased by 93%. Overall, the rates have decreased drastically from 20 deaths in 1970 to 6.9 deaths in 2003 (per every 1000 live births). In 2003, the leading causes of infant mortality in the United States were congenital anomalies, disorders related to immaturity, SIDS, and maternal complications. Babies born with low birth weight increased to 8.1% while cigarette smoking during pregnancy declined to 10.2%. This reflected the amount of low birth weights concluding that 12.4% of births from smokers were low birth weights compared with 7.7% of such births from non-smokers. According to the \"New York Times\", \"the main reason for the high rate is preterm delivery, and there was a 10% increase in such births from 2000 to 2006.\" Between 2007 and 2011, however, the preterm birth rate has decreased every year. In 2011 there was a 11.73% rate of babies born before the 37th week of gestation, down from a high of 12.80% in 2006.\n\nEconomic expenditures on labor and delivery and neonatal care are relatively high in the United States. A conventional birth averages 9,775 USD with a C-section costing 15,041 USD. Preterm births in the US have been estimated to cost $51,600 per child, with a total yearly cost of $26.2 billion. Despite this spending, several reports state that infant mortality rate in the United States is significantly higher than in other developed nations. Estimates vary; the CIA's \"World Factbook\" ranks the US 55th internationally in 2014, with a rate of 6.17, while the UN figures from 2005-2010 place the US 34th.\n\nAforementioned differences in measurement could play a substantial role in the disparity between the US and other nations. A non-viable live birth in the US could be registered as a stillbirth in similarly developed nations like Japan, Sweden, Norway, Ireland, the Netherlands, and France – thereby reducing the infant death count. Neonatal intensive care is also more likely to be applied in the US to marginally viable infants, although such interventions have been found to increase both costs and disability. A study following the implementation of the Born Alive Infant Protection Act of 2002 found universal resuscitation of infants born between 20–23 weeks increased the neonatal spending burden by $313.3 million while simultaneously decreasing quality-adjusted life years by 329.3.\nThe vast majority of research conducted in the late twentieth and early twenty-first century indicates that African-American infants are more than twice as likely to die in their first year of life than white infants. Although following a decline from 13.63 to 11.46 deaths per 1000 live births from 2005 to 2010, non-Hispanic black mothers continued to report a rate 2.2 times as high as that for non-Hispanic white mothers.\n\nContemporary research findings have demonstrated that nationwide racial disparities in infant mortality are linked to the experiential state of the mother and that these disparities cannot be totally accounted for by socio-economic, behavioral or genetic factors. The Hispanic paradox, an effect observed in other health indicators, appears in the infant mortality rate, as well. Hispanic mothers see an IMR comparable to non-Hispanic white mothers, despite lower educational attainment and economic status. A study in North Carolina, for example, concluded that \"white women who did not complete high school have a lower infant mortality rate than black college graduates.\" According to Mustillo's CARDIA (Coronary Artery Risk Development in Young Adults) study, \"self reported experiences of racial discrimination were associated with pre-term and low-birthweight deliveries, and such experiences may contribute to black-white disparities in prenatal outcomes.\" Likewise, dozens of population-based studies indicate that \"the subjective, or perceived experience of racial discrimination is strongly associated with an increased risk of infant death and with poor health prospects for future generations of African Americans.\"\n\nWhile earlier parts of this article have addressed the racial differences in infant deaths, a closer look into the effects of racial differences within the country is necessary to view discrepancies. Non-Hispanic Black women lead all other racial groups in IMR with a rate of 11.3, while the Infant Mortality Rate among white women is 5.1. Black women in the United States experience a shorter life expectancy than white women, so while a higher IMR amongst black women is not necessarily out of line, it is still rather disturbing. While the popular argument leads to the idea that due to the trend of a lower socio-economic status had by black women there is in an increased likelihood of a child suffering. While this does correlate, the theory that it is the contributing factor falls apart when we look at Latino IMR in the United States. Latino people are almost just as likely to experience poverty as blacks in the U.S., however, the Infant Mortality Rate of Latinos is much closer to white women than it is to black women. The Poverty Rates of blacks and Latinos are 24.1% and 21.4% respectively. If there is a direct correlation, then the IMR of these two groups should be rather similar, however, blacks have an IMR double that of Latinos. Also, as black women move out of poverty or never experienced it in the first place, their IMR is not much lower than their counterparts experiencing higher levels of poverty.\n\nSome believe black women are predisposed to a higher IMR, meaning ancestrally speaking, all black women from African descent should experience an elevated rate. This theory is quickly disproven by looking at women of African descent who have immigrated to the United States. These women who come from a completely different social context are not prone to the higher IMR experienced by American-born black women.\n\nTyan Parker Dominguez at the University of Southern California offers a theory to explain the disproportionally high IMR among black women in the United States. She claims African American women experience stress at much higher rates than any other group in the country. Stress produces particular hormones that induce labor and contribute to other pregnancy problems. Considering early births are one of the leading causes of death of infants under the age of one, induced labor is a very legitimate factor. The idea of stress spans socio-economic status as Parker Dominguez claims stress for lower-class women comes from unstable family life and chronic worry over poverty. For black middle-class women, battling racism, real or perceived, can be an extreme stressor. \n\nArline Geronimus, a professor at the University of Michigan School of Public Health calls the phenomenon \"weathering.\" She claims constantly dealing with disadvantages and racial prejudice causes black women's birth outcomes to deteriorate with age. Therefore, younger black women may experience stress with pregnancy due to social and economic factors, but older women experience stress at a compounding rate and therefore have pregnancy complications aside from economic factors.\n\nMary O. Hearst, a professor in the Department of Public Health at Saint Catherine University, researched the effects of segregation on the African American community to see if it contributed to the high IMR amongst black children. Hearst claims that residential segregation contributes to the high rates because of the political, economic, and negative health implications it poses on black mothers regardless of their socioeconomic status. Racism, economic disparities, and sexism in segregated communities are all examples of the daily stressors that pregnant black women face that can affect their pregnancies with conditions such as pre-eclampsia and hypertension.\n\nStudies have also shown that high IMR is due to the inadequate care that pregnant African Americans receive compared to other women in the country. This unequal treatment stems from the idea that there are racial medical differences and is also rooted in racial biases and controlled images of black women. Because of this unequal treatment, research finds that black women do not receive the same urgency in medical care and are not taken as seriously regarding pain they feel or complications they think they are having, as exemplified by the complications tennis-star Serena Williams faced during her delivery.\n\nStrides have been made, however, to combat this epidemic. In Los Angeles County, health officials have partnered with non-profits around the city to help black women after the delivery of their child. One non-profit in particular has made a large impact on many lives is Great Beginnings For Black Babies in Inglewood. The non-profit centers around helping women deal with stress by forming support networks, keeping an open dialogue around race and family life, and also finding these women a secure place in the workforce.\n\nSome research argues that to end high IMR amongst black children, the country needs to fix the social and societal issues that plague African Americans. Some scholars argue that Issues such as institutional racism, mass incarceration, poverty, and health care disparities that are present amongst the African American country need to be addressed by the United States Government in order for policy to be created to combat these issues. Following this theory, if institutional inequalities are addresses and repaired by the United States Government, daily stressors for African Americans, and African American women in particular, will be reduced, therefore lessening the risk of complications in pregnancy and infant mortality. Others argue that adding diversity in the health care industry can help reduce the high IMR because more representation can tackle deep-rooted racial biases and stereotypes that exist towards African American women. Another more recent form of action to reduce high IMR amongst black children is the use of doulas throughout pregnancy. \n\nIt was in the early 1900's that countries around the world started to notice that there was a need for better child health care services. Europe started this rally, the United States fell behind them by creating a campaign to decrease the infant mortality rate. With this program, they were able to lower the IMR to 10 deaths rather than 100 deaths per every 1000 births. Infant mortality was also seen as a social problem when it was being noticed as a national problem. American women who had middle class standing with an educational background started to create a movement that provided housing for families of a lower class. By starting this, they were able to establish public health care and government agencies that were able to make more sanitary and healthier environments for infants. Medical professionals helped further the cause for infant health by creating a pediatrics field that was experienced in medicine for children.\n\nDecreases in infant mortality in given countries across the world during the 20th century have been linked to several common trends, scientific advancements, and social programs. Some of these include the state improving sanitation, improving access to healthcare, improving education, and the development of medical advancements such as penicillin, and safer blood transfusions.\n\nIn the United States, improving infant mortality in the early half of the 20th century meant tackling environmental factors. Improving sanitation, and especially access to safe drinking water, helped the United States dramatically decrease infant mortality, a growing concern in the United States since the 1850s. On top of these environmental factors, during this time the United States endeavored to increase education and awareness regarding infant mortality. Pasteurization of milk also helped the United States combat infant mortality in the early 1900s, a practice which allowed the United States to curb disease in infants. These factors, on top of a general increase in the standard of living for those living in urban settings, helped the United States make dramatic improvements in their rates of infant mortality in the early 20th century.\n\nAlthough the overall infant mortality rate was sharply dropping during this time, within the United States infant mortality varied greatly among racial and socio-economic groups. The change in infant mortality from 1915 to 1933 was, for the white population, 98.6 in 1,000 to 52.8 in 1,000, and for the black population, 181.2 in 1,000 to 94.4 in 1,000. Studies imply that this has a direct correlation with relative economic conditions between these populations. Additionally, infant mortality in southern states was consistently 2% higher than other states in the US across a 20 year period from 1985. Southern states also tend to perform worse in predictors for higher infant mortality, such as per capita income and poverty rate.\n\nIn the latter half of the 20th century, a focus on greater access to medical care for women spurred declines in infant mortality in the United States. The implementation of Medicaid, granting wider access to healthcare, contributed to a dramatic decrease in infant mortality, in addition to access to greater access to legal abortion and family-planning care, such the IUD and the birth control pill.\n\nIn the decades following the 1970's, the United State's decreasing infant mortality rates began to slow, falling behind China's, Cuba's, and other developed countries'. Funding for the federally subsidized Medicaid and Maternal and Infant Care was sharply reduced, and availability of prenatal care greatly decreased for low-income parents.\n\nThe People's Republic of China's growth of medical resources in the latter half of the 20th century partly explains its dramatic improvement with regards to infant mortality during this time. Part of this increase included the adoption of the Rural Cooperative Medical System, which was founded in the 1950s. The Cooperative Medical System granted healthcare access to previously underserved rural populations, is estimated to have covered 90% of China's rural population throughout the 1960s. The Cooperative Medical System achieved an infant mortality rate of 25.09 per 1,000. The Cooperative Medical System was later defunded, leaving many rural populations to rely on an expensive fee-for-service system, although the rate continued to decline in general. This change in medical systems caused a socio-economic gap in accessibility to medical care in China, which fortunately was not reflected in its infant mortality rate of decline. Prenatal care was increasingly used, even as the Cooperative Medical System was replaced, and delivery assistance remained accessible.\n\nChina's one-child policy, adopted in the 1980s, negatively impacted its infant mortality. Women carrying unapproved pregnancies faced state consequences and social stigma and were thus less likely to use prenatal care. Additionally, economic realities and long-held cultural factors incentivized male offspring, leading some families who already had sons to avoid prenatal care or professional delivery services, and causing China to have unusually high female infant mortality rates during this time.\n\n\nRelated statistical categories:\n\n"}
{"id": "20074900", "url": "https://en.wikipedia.org/wiki?curid=20074900", "title": "Jheryl Busby", "text": "Jheryl Busby\n\nJheryl Busby (May 5, 1949 – November 4, 2008) was an American recording company executive who was the former President and Chief Executive Officer of Motown Records.\n\nBusby grew up in South Central Los Angeles, where he went to John C. Fremont High School. He attended Long Beach State College, dropping out after two years.\n\nBusby began his business career at Mattel Toys, starting as an inventory clerk and ultimately being promoted to new-toy coordinator.\n\nHis first exposure to the recording industry was at Stax Records, where he was named head of West Coast promotion and marketing, After leaving Stax, he did promotional work for A&M Records and Casablanca Records.\n\nAt MCA Records, where he was hired in 1984, Busby was vice-president of the black music division building the unit largely from scratch, promoting established acts such as Patti LaBelle and helping to discover and market acts including family band The Jets, Jody Watley, Bobby Brown and New Edition. By the mid-'80s, the division's sales reached $50 million and was top-ranked for years in black album sales. By 1988, Busby was president of the black music division at MCA, and his business unit led the industry in black album sales.\n\nWhen Boston Ventures and Music Corporation of America bought Motown Records from Berry Gordy in June 1988 for $61 million, Gordy stipulated that 20% of the firm be retained by African-American investors; Busby purchased an unspecified stake in the firm. Busby moved to Motown Records in 1988 as the company's President & CEO in 1988. Annual sales for Motown had dropped to $20 million (from a peak of $100 million, at Motown's height), with 60–70% of sales coming from sales of its old hits. Busby fostered the growth of younger talent, including Another Bad Creation, Boyz II Men, Johnny Gill and Queen Latifah. In early 1989, he was able to sign Diana Ross back to Motown after she left for RCA Records in 1981. Busby retained artists such as Lionel Richie, Smokey Robinson and Stevie Wonder, and helped create hits from them and for Ross. When Polygram Records bought Motown for $301 million in 1993, Busby was retained as president. By 1990, the label had five songs reach number one on the R&B charts.\n\nBusby was appointed head of the black music division at DreamWorks Records in 1998. He left DreamWorks in 2001. He was named president of Def Soul Classics in 2004. Umbrella Recordings, which he created with producer Mike City, released the Carl Thomas album \"So Much Better\" in 2007 and Patti LaBelle's \"The Gospel According to Patti LaBelle\", her first gospel album.\n\nBusby was a major shareholder along with Janet Jackson and Magic Johnson in the nation's first African-American-owned national bank, Founders National Bank, and served on the bank's board of directors.\n\nBusby died at the age of 59 on November 4, 2008. He was found dead in the hot tub of his home in Malibu, the cause was later confirmed to be accidental drowning, it is believed preexisting medical conditions including \"atheroscelerotic cardiovascular disease\" and \"diabetes mellitus may have contributed to the drowning.\n"}
{"id": "56434", "url": "https://en.wikipedia.org/wiki?curid=56434", "title": "Julia set", "text": "Julia set\n\nIn the context of complex dynamics, a topic of mathematics, the Julia set and the Fatou set are two complementary sets (Julia 'laces' and Fatou 'dusts') defined from a function. Informally, the Fatou set of the function consists of values with the property that all nearby values behave similarly under repeated iteration of the function, and the Julia set consists of values such that an arbitrarily small perturbation can cause drastic changes in the sequence of iterated function values.\nThus the behavior of the function on the Fatou set is 'regular', while on the Julia set its behavior is 'chaotic'.\n\nThe Julia set of a function \"f\" is commonly denoted \"J\"(\"f\"), and the Fatou set is denoted \"F\"(\"f\"). These sets are named after the French mathematicians Gaston Julia and Pierre Fatou whose work began the study of complex dynamics during the early 20th century.\n\nLet \"f\"(\"z\") be a holomorphic function from the Riemann sphere to itself. Such \"f\"(\"z\") are precisely the complex rational functions, that is, formula_1, where \"p\"(\"z\") and \"q\"(\"z\") are complex polynomials. Assume that p and q are each nonconstant and that they have no common roots. Then there is a finite number of open sets \"F\", ..., \"F\", that are left invariant by \"f\"(\"z\") and are such that:\n\n\nThe last statement means that the termini of the sequences of iterations generated by the points of \"F\" are either precisely the same set, which is then a finite cycle, or they are finite cycles of circular or annular shaped sets that are lying concentrically. In the first case the cycle is \"attracting\", in the second it is \"neutral\".\n\nThese sets \"F\" are the Fatou domains of \"f\"(\"z\"), and their union is the Fatou set \"F\"(\"f\") of \"f\"(\"z\"). Each of the Fatou domains contains at least one critical point of \"f\"(\"z\"), that is, a (finite) point \"z\" satisfying formula_2, or formula_3, if the degree of the numerator \"p\"(\"z\") is at least two larger than the degree of the denominator \"q\"(\"z\"), or if formula_4 for some \"c\" and a rational function \"g\"(\"z\") satisfying this condition.\n\nThe complement of \"F\"(\"f\") is the Julia set \"J\"(\"f\") of \"f\"(\"z\"). \"J\"(\"f\") is a nowhere dense set (it is without interior points) and an uncountable set (of the same cardinality as the real numbers). Like \"F\"(\"f\"), \"J\"(\"f\") is left invariant by \"f\"(\"z\"), and on this set the iteration is repelling, meaning that formula_5 for all \"w\" in a neighbourhood of \"z\" (within \"J\"(\"f\")). This means that \"f\"(\"z\") behaves chaotically on the Julia set. Although there are points in the Julia set whose sequence of iterations is finite, there are only a countable number of such points (and they make up an infinitesimal part of the Julia set). The sequences generated by points outside this set behave chaotically, a phenomenon called \"deterministic chaos\".\n\nThere has been extensive research on the Fatou set and Julia set of iterated rational functions, known as rational maps. For example, it is known that the Fatou set of a rational map has either 0,1,2 or infinitely many components. Each component of the Fatou set of a rational map can be classified into one of four different classes.\n\n\nThe Julia set and the Fatou set of \"f\" are both completely invariant under iterations of the holomorphic function \"f\":\n\nFor formula_9 the Julia set is the unit circle and on this the iteration is given by doubling of angles (an operation that is chaotic on the points whose argument is not a rational fraction of formula_10). There are two Fatou domains: the interior and the exterior of the circle, with iteration towards 0 and ∞, respectively.\n\nFor formula_11 the Julia set is the line segment between −2 and 2. There is one Fatou domain: the points not on the line segment iterate towards ∞. (Apart from a shift and scaling of the domain, this iteration is equivalent to formula_12 on the unit interval, which is commonly used as an example of chaotic system.)\n\nThe functions f and g are of the form formula_13, where \"c\" is a complex number. For such an iteration the Julia set is not in general a simple curve, but is a fractal, and for some values of \"c\" it can take surprising shapes. See the pictures below.\nFor some functions \"f\"(\"z\") we can say beforehand that the Julia set is a fractal and not a simple curve. This is because of the following result on the iterations of a rational function:\n\nTheorem. Each of the Fatou domains has the same boundary, which consequently is the Julia set.\n\nThis means that each point of the Julia set is a point of accumulation for each of the Fatou domains. Therefore, if there are more than two Fatou domains, \"each\" point of the Julia set must have points of more than two different open sets infinitely close, and this means that the Julia set cannot be a simple curve. This phenomenon happens, for instance, when \"f\"(\"z\") is the Newton iteration for solving the equation formula_14:\n\nThe image on the right shows the case \"n\" = 3.\n\nA very popular complex dynamical system is given by the family of complex quadratic polynomials, a special case of rational maps. Such quadratic polynomials can be expressed as \nwhere \"c\" is a complex parameter. The Julia set for this system is the subset of the complex plane given by:\n\nwhere formula_18 is the \"n\"th iterate of formula_19.\n\nThe parameter plane of quadratic polynomials - that is, the plane of possible \"c\"-values - gives rise to the famous Mandelbrot set. Indeed, the Mandelbrot set is defined as the set of all \"c\" such that formula_20 is connected. For parameters outside the Mandelbrot set, the Julia set is a Cantor space: in this case it is sometimes referred to as Fatou dust.\n\nIn many cases, the Julia set of \"c\" looks like the Mandelbrot set in sufficiently small neighborhoods of \"c\". This is true, in particular, for so-called 'Misiurewicz' parameters, i.e. parameters \"c\" for which the critical point is pre-periodic. For instance:\n\nIn other words, the Julia sets formula_20 are locally similar around Misiurewicz points.\n\nThe definition of Julia and Fatou sets easily carries over to the case of certain maps whose image contains their domain; most notably transcendental meromorphic functions and Adam Epstein's finite-type maps.\n\nJulia sets are also commonly defined in the study of dynamics in several complex variables.\n\nThe below pseudocode implementations hard code the functions for each fractal. Consider implementing complex number operations to allow for more dynamic and reusable code.\n\nfor each pixel (x, y) on the screen, do:\n\nfor each pixel (x, y) on the screen, do:\n\nThe Julia set for formula_9 is the unit circle, and on the outer Fatou domain, the \"potential function\" φ(\"z\") is defined by φ(\"z\") = log|\"z\"|. The equipotential lines for this function are concentric circles. As formula_25 we have\n\nwhere formula_27 is the sequence of iteration generated by \"z\". For the more general iteration formula_22, it has been proved that if the Julia set is connected (that is, if \"c\" belongs to the (usual) Mandelbrot set), then there exist a biholomorphic map ψ between the outer Fatou domain and the outer of the unit circle such that formula_29. This means that the potential function on the outer Fatou domain defined by this correspondence is given by:\n\nThis formula has meaning also if the Julia set is not connected, so that we for all \"c\" can define the potential function on the Fatou domain containing ∞ by this formula. For a general rational function \"f\"(\"z\") such that ∞ is a critical point and a fixed point, that is, such that the degree \"m\" of the numerator is at least two larger than the degree \"n\" of the denominator, we define the \"potential function\" on the Fatou domain containing ∞ by:\n\nwhere \"d\" = \"m\" − \"n\" is the degree of the rational function.\n\nIf \"N\" is a very large number (e.g. 10), and if \"k\" is the first iteration number such that formula_32, we have that\n\nfor some real number formula_34, which should be regarded as the real iteration number, and we have that:\n\nwhere the last number is in the interval [0, 1).\n\nFor iteration towards a finite attracting cycle of order \"r\", we have that if \"z*\" is a point of the cycle, then formula_36 (the \"r\"-fold composition), and the number\n\nis the \"attraction\" of the cycle. If \"w\" is a point very near \"z*\" and \"w\"' is \"w\" iterated \"r\" times, we have that\n\nTherefore the number formula_39 is almost independent of \"k\". We define the potential function on the Fatou domain by:\n\nIf ε is a very small number and \"k\" is the first iteration number such that formula_41, we have that\n\nfor some real number formula_34, which should be regarded as the real iteration number, and we have that:\n\nIf the attraction is ∞, meaning that the cycle is \"super-attracting\", meaning again that one of the points of the cycle is a critical point, we must replace α by\n\nwhere \"w\"' is \"w\" iterated \"r\" times and the formula for φ(\"z\") by:\n\nAnd now the real iteration number is given by:\n\nFor the colouring we must have a cyclic scale of colours (constructed mathematically, for instance) and containing \"H\" colours numbered from 0 to \"H\"−1 (\"H\" = 500, for instance). We multiply the real number formula_34 by a fixed real number determining the density of the colours in the picture, and take the integral part of this number modulo \"H\".\n\nThe definition of the potential function and our way of colouring presuppose that the cycle is attracting, that is, not neutral. If the cycle is neutral, we cannot colour the Fatou domain in a natural way. As the terminus of the iteration is a revolving movement, we can, for instance, colour by the minimum distance from the cycle left fixed by the iteration.\n\nIn each Fatou domain (that is not neutral) there are two systems of lines orthogonal to each other: the \"equipotential lines\" (for the potential function or the real iteration number) and the \"field lines\".\n\nIf we colour the Fatou domain according to the iteration number (and \"not\" the real iteration number formula_34, as defined in the previous section), the bands of iteration show the course of the equipotential lines. If the iteration is towards ∞ (as is the case with the outer Fatou domain for the usual iteration formula_50), we can easily show the course of the field lines, namely by altering the colour according as the last point in the sequence of iteration is above or below the \"x\"-axis (first picture), but in this case (more precisely: when the Fatou domain is super-attracting) we cannot draw the field lines coherently - at least not by the method we describe here. In this case a field line is also called an external ray.\n\nLet \"z\" be a point in the attracting Fatou domain. If we iterate \"z\" a large number of times, the terminus of the sequence of iteration is a finite cycle \"C\", and the Fatou domain is (by definition) the set of points whose sequence of iteration converges towards \"C\". The field lines issue from the points of \"C\" and from the (infinite number of) points that iterate \"into\" a point of \"C\". And they end on the Julia set in points that are non-chaotic (that is, generating a finite cycle). Let \"r\" be the order of the cycle \"C\" (its number of points) and let \"z*\" be a point in \"C\". We have formula_51 (the r-fold composition), and we define the complex number α by\n\nIf the points of \"C\" are formula_53, α is the product of the \"r\" numbers formula_54. The real number 1/|α| is the \"attraction\" of the cycle, and our assumption that the cycle is neither neutral nor super-attracting, means that 1 < 1/|α| < ∞. The point \"z*\" is a fixed point for formula_55, and near this point the map formula_55 has (in connection with field lines) character of a rotation with the argument β of α (that is, formula_57).\n\nIn order to colour the Fatou domain, we have chosen a small number ε and set the sequences of iteration formula_58 to stop when formula_41, and we colour the point \"z\" according to the number \"k\" (or the real iteration number, if we prefer a smooth colouring). If we choose a direction from \"z*\" given by an angle θ, the field line issuing from \"z*\" in this direction consists of the points \"z\" such that the argument ψ of the number formula_60 satisfies the condition that\n\nFor if we pass an iteration band in the direction of the field lines (and away from the cycle), the iteration number \"k\" is increased by 1 and the number ψ is increased by β, therefore the number formula_62 is constant along the field line.\n\nA colouring of the field lines of the Fatou domain means that we colour the spaces between pairs of field lines: we choose a number of regularly situated directions issuing from \"z*\", and in each of these directions we choose two directions around this direction. As it can happen that the two field lines of a pair do not end in the same point of the Julia set, our coloured field lines can ramify (endlessly) in their way towards the Julia set. We can colour on the basis of the distance to the center line of the field line, and we can mix this colouring with the usual colouring. Such pictures can be very decorative (second picture).\n\nA coloured field line (the domain between two field lines) is divided up by the iteration bands, and such a part can be put into a one-to-one correspondence with the unit square: the one coordinate is (calculated from) the distance from one of the bounding field lines, the other is (calculated from) the distance from the inner of the bounding iteration bands (this number is the non-integral part of the real iteration number). Therefore, we can put pictures into the field lines (third picture).\n\nMethods : \n\n \nAs mentioned above, the Julia set can be found as the set of limit points of the set of pre-images of (essentially) any given point. So we can try to plot the Julia set of a given function as follows. Start with any point \"z\" we know to be in the Julia set, such as a repelling periodic point, and compute all pre-images of \"z\" under some high iterate formula_63 of \"f\".\n\nUnfortunately, as the number of iterated pre-images grows exponentially, this is not feasible computationally. However, we can adjust this method, in a similar way as the \"random game\" method for iterated function systems. That is, in each step, we choose at random one of the inverse images of \"f\".\n\nFor example, for the quadratic polynomial \"f\", the backwards iteration is described by\n\nAt each step, one of the two square roots is selected at random.\n\nNote that certain parts of the Julia set are quite difficult to access with the reverse Julia algorithm. For this reason, one must modify IIM/J ( it is called MIIM/J) or use other methods to produce better images.\n\nAs a Julia set is infinitely thin we cannot draw it effectively by backwards iteration from the pixels. It will appear fragmented because of the impracticality of examining infinitely many startpoints. Since the iteration count changes vigorously near the Julia set, a partial solution is to imply the outline of the set from the nearest color contours, but the set will tend to look muddy.\n\nA better way to draw the Julia set in black and white is to estimate the distance of pixels (DEM) from the set and to color every pixel whose center is close to the set. The formula for the distance estimation is derived from the formula for the potential function φ(\"z\"). When the equipotential lines for φ(\"z\") lie close, the number formula_65 is large, and conversely, therefore the equipotential lines for the function formula_66 should lie approximately regularly. It has been proven that the value found by this formula (up to a constant factor) converges towards the true distance for z converging towards the Julia set.\n\nWe assume that \"f\"(\"z\") is rational, that is, formula_1 where \"p\"(\"z\") and \"q\"(\"z\") are complex polynomials of degrees \"m\" and \"n\", respectively, and we have to find the derivative of the above expressions for φ(\"z\"). And as it is only formula_27 that varies, we must calculate the derivative formula_69 of formula_27 with respect to \"z\". But as formula_71 (the \"k\"-fold composition), formula_69 is the product of the numbers formula_73, and this sequence can be calculated recursively by formula_74, starting with formula_75 (\"before\" the calculation of the next iteration formula_76).\n\nFor iteration towards ∞ (more precisely when \"m\" ≥ \"n\" + 2, so that ∞ is a super-attracting fixed point), we have\n\n(\"d\" = \"m\" − \"n\") and consequently:\n\nFor iteration towards a finite attracting cycle (that is not super-attracting) containing the point \"z*\" and having order \"r\", we have\n\nand consequently:\n\nFor a super-attracting cycle, the formula is:\n\nWe calculate this number when the iteration stops. Note that the distance estimation is independent of the attraction of the cycle. This means that it has meaning for transcendental functions of \"degree infinity\" (e.g. sin(\"z\") and tan(\"z\")).\n\nBesides drawing of the boundary, the distance function can be introduced as a 3rd dimension to create a solid fractal landscape.\n\n\n\n"}
{"id": "24135147", "url": "https://en.wikipedia.org/wiki?curid=24135147", "title": "Kodecyte", "text": "Kodecyte\n\nA kodecyte (ko•de•cyte) is a living cell that has been modified (koded) by the incorporation of one or more function-spacer-lipid constructs (FSL constructs) to gain a new or novel biological, chemical or technological function. The cell is modified by the lipid tail of the FSL construct incorporating into the bilipid membrane of the cell.\n\nAll kodecytes retain their normal vitality and functionality while gaining the new function of the inserted FSL constructs. The combination of dispersibility in biocompatible media, spontaneous incorporation into cell membranes, and apparent low toxicity, makes FSL constructs suitable as research tools and for the development of new diagnostic and therapeutic applications.\n\nKode FSL constructs consist of three components; a functional moiety (F), a spacer (S) and a lipid (L).\n\nFunction groups on FSL constructs that can be used to create kodecytes include saccharides (including ABO blood group-related determinants, sialic acids, hyaluronin polysaccharides), fluorophores, biotin, and a range of peptides.\n\nAlthough kodecytes are created by modifying natural cells, they are different from natural cells. For example, FSL constructs, influenced by the composition of the lipid tail, are laterally mobile in the membrane and some FSL constructs may also cluster due to the characteristics of the functional group (F). As FSL constructs are anchored in the membrane via a lipid tail (L) it is believed they do not participate in signal transduction, but may be designed to act as agonists or antagonists of the initial binding event. FSL constructs will not actively pass through the plasma membrane but may enter the cell via membrane invagination and endocytosis.\n\nThe “koding” of cells is stable (subject to the rate of turnover of the membrane components). FSL constructs will remain in the membrane of inactive cells (e.g. red blood cells) for the life of the cell provided it is stored in lipid free media. In the peripheral circulation FSL constructs are observed to be lost from red cell kodecytes at a rate of about 1% per hour. The initial “koding” dose and the minimum level required for detection determine how long the presence of “kodecytes” in the circulation can be monitored. For red blood “kodecytes” reliable monitoring of the presence of the “kodecytes” for up to 3 days post intravenous administration has been demonstrated in small mammals.\n\nThe spacer (S) of a FSL construct has been selected so as to have negligible cross-reactivity with serum antibodies so kodecytes can be used with undiluted serum. By increasing the length of the FSL spacer from 1.9 to 7.2 nm it has been shown sensitivity can improve two-fold in red cell agglutination based kodecyte assays. However, increasing the size of the spacer further from 7.2 to 11.5 nm did not result in any further enhancement.\n\nTo view a simple video explaining how Kode Technology works, click the following link: https://www.youtube.com/watch?v=TIbjAl5KYpA\n\nFSL constructs, when in solution (saline) and in contact, will spontaneously incorporate into cell membranes. The methodology involves simply preparing a solution of FSL construct(s) in the range of 1–1000 µg/mL, with the concentration used determining the amount of antigen present on the kodecyte. The ability to control antigen levels on the outside of a kodecyte has allowed for manufacture of quality control sensitivity systems and serologic teaching kits incorporating the entire range of serologic agglutination reactions. The actual concentration will depend on the construct and the quantity of construct required in the membrane. One part of FSL solution is added to one part of cells (up to 100% suspension) and they are incubated at a set temperature within the range of 4–37 °C (39–99 °F) depending on temperature compatibility of the cells being modified. The higher the temperature, the faster the rate of FSL insertion into the membrane. For red blood cells incubation for 2 hours at 37 °C achieves >95% FSL insertion with at least 50% insertion being achieved within 20 minutes. In general, for carbohydrate based FSLs insertion into red blood cells, incubation for 4 hours at room temperature or 20 hours at 4 °C are similar to one hour at 37 °C . The resultant kodecytes do not required to be washed, however this option should be considered if an excess of FSL construct is used in the koding process.\n\nKodecytes can also be created \"in vivo\" by injection of constructs directly into the circulation. However this process will modify all cells in contact with the constructs and usually require significantly more construct than \"in vitro\" preparation, as FSL constructs will preferentially associate with free lipids.\nThe \"in vivo\" creation of kodecytes is untargeted and FSL constructs will insert into all cells non-specifically, but may show a preference for some cell types.\n\nDiagnostic serological analyses including flow cytometry and scanning electron microscopy usually can’t see a difference between “kodecytes” and unmodified cells. However, when compared with natural cells there does appear to be a difference between IgM and IgG antibody reactivities when the functional group (F) is a monomeric peptide antigen. IgM antibodies appear to react poorly with kodecytes made with FSL peptides. Furthermore, FSL constructs may have a restricted antigen/epitope and may not react with a monoclonal antibody unless the FSL construct and monoclonal antibody are complementary.\n\nKodecytes can be studied using standard histological techniques. Kodecytes can be fixed after “koding” subject to the functional moiety (F) of the FSL construct being compatible with the fixative. However, freeze cut or formalin-fixed freeze cut tissues are required because the lipid based FSL constructs (and other glycolipids) will be leached from the “kodecytes” in paraffin imbedded samples during the deparaffination steps.\n\nKoded membranes are described by the construct and the concentration of FSL (in µg/mL) used to create them. For example, kodecytes created with a 100 μg/mL solution of FSL-A would be termed A100 kodecytes. If multiple FSL constructs were used then the definition is expanded accordingly, e.g. A100+B300 kodecytes are created with a solution containing 100 μg/mL solution of FSL-A and 300 μg/mL solution of FSL-B. The “+” symbol is used to separate the construct mixes, e.g. A100+B300. If FSL concentrations are constant then the μg/mL component of the terminology can be dropped, e.g. A kodecytes. Alternatively unrelated constructs such as FSL-A and FSL-biotin will create A+biotin kodecytes, etc. If different cells are used in the same study then inclusion of the cell type into the name is recommended, e.g. RBC A100 kodecytes vs WBC A100 kodecytes, or platelet A100 kodecytes, etc.\n\nKode Technology has been used for the \"in vitro\" modification of murine embryos, spermatozoa, zebra fish, epithelial/endometrial cells and red blood cells to create cellular quality controls systems, serologic kits (teaching), rare antigen expression, add infectious markers onto cells, modified cell adhesion/interaction/separation/immobilisation, and labelling. It has also been intravascularly infused for in vivo modification of blood cells and neutralisation of circulating antibodies and in \"in vivo\" imaging of circulating bone marrow kodecytes in zebrafish. Kode FSL constructs have also been applied to non-biological surfaces such as modified cellulose, paper, silica, polymers, natural fibers, glass and metals and has been shown to be ultra-fast in labelling these surfaces.\n\n\n"}
{"id": "34946307", "url": "https://en.wikipedia.org/wiki?curid=34946307", "title": "List of fictional werewolves", "text": "List of fictional werewolves\n\nThis is a List of fictional werewolves who appear in works of literature, television, comics, films and legends. \n"}
{"id": "9431764", "url": "https://en.wikipedia.org/wiki?curid=9431764", "title": "List of musicians who play left-handed", "text": "List of musicians who play left-handed\n\nThis is a list of notable left-handed musicians who play their instruments naturally. (This does not include left-handed people who play right-handed, such as Noel Gallagher, Duane Allman, Gregg Allman, Steve Morse, Billy Corgan, Jesper Stromblad, Dave Hill, Kiko Loureiro, Mark Knopfler, Gary Moore, Tomo Miličević, James Root, Duff McKagan, Nick Lowe, Joan Jett and Paul Simon.)\n\nLeft-handed people play guitar or electric bass in one of the following four ways: (1) play a right-handed guitar or right-handed bass in a right-handed manner, (2) play a true left-handed guitar or bass, (3) play a right-handed guitar or bass that has been altered to play left-handed, or (4) turn a right-handed guitar or bass upside down, pick with the left hand, but leave the strings as they were – which makes them reversed from the normal order. (The fingering is the same for methods 2 and 3.) Any style of picking with the left hand (flatpicking or fingerstyle guitar) is considered playing left-handed.\n\nGuitarists in this category pick with their left hand and have the strings in the conventional order for a left-handed player (i.e. the low string on the top side of the neck). They either have true left-handed guitars or have right-handed guitars altered so the strings are correct for a left-handed player. Some guitarists in this category (e.g. Paul McCartney) play both genuine left-handed instruments and right-handed instruments altered for left-handed playing.\n\nChanging the strings on a right-handed guitar involves several things. The nut of the guitar has to be changed to accommodate the string widths. The bridge needs to be changed to make the lower strings longer than the top strings for correct intonation. On almost all acoustic guitars the bracing is non-symmetrical. On electric guitars altered this way, the controls will be backwards.\n\nThese are left-handed players who play naturally too, but with the strings organized to emulate an unaltered right-handed guitar, thus the strings are backwards for a left-handed player (e.g. Bob Geldof). Some players in this category (e.g. Dick Dale and Albert King) had custom instruments that were essentially left-handed guitars with the strings as on a right-handed guitar, since they had learned to play that way.\n\nA drum kit for a left-handed person is set up so that percussion instruments drummers would normally play with their right hand (ride cymbal, floor tom, etc.) are played with the left hand. The bass drum and hi-hat configurations are also set up so that the drummer plays the bass drum with their left foot, and operate the hi-hat with their right foot. Some drummers however have been known to play right-handed kit, but play leading with their left hand (e.g. playing open-handed on the hi-hat). This list does not include drummers who are naturally left-handed but play drums purely right-handed such as Ringo Starr, Stewart Copeland, Dave Lombardo, Travis Barker and Chris Adler.\n\n\n\nThe violin can be learned in either hand, and most left-handed players hold the violin under their left chins, the same as right-handed players. This allows all violinists to sit together in an orchestra.\n\n\n\n\n\n\n\n\n"}
{"id": "53307328", "url": "https://en.wikipedia.org/wiki?curid=53307328", "title": "Lucius Appuleius", "text": "Lucius Appuleius\n\nLucius Appuleius (fl. 4th century BCE) was a man of ancient Rome who served as Tribune of the Plebs in 391 BCE. He impeached Marcus Furius Camillus for having secreted part of the spoils of war against the rival Etrurian city of Veii in 406.\n"}
{"id": "1250596", "url": "https://en.wikipedia.org/wiki?curid=1250596", "title": "Machine Empire", "text": "Machine Empire\n\nThe Machine Empire is a group of robotic villains in the Power Rangers universe, who first appeared in the television show \"Power Rangers Zeo\" and later in \"Power Rangers in Space\" and \"Power Rangers Wild Force\".\n\nThe Machine Empire was a massive alien race and galactic empire of mechanical beings, led by the Royal House of Gadgetry. Possessing countless Cogs, Quadra Fighters, and monsters, the Machine Empire had already conquered an entire chain of galaxies before reaching Earth. It was when the Empire arrived on Earth that they finally encountered a foe capable of repelling their forces, the . Numerous monsters from the Empire perished at the Rangers' hands before King Mondo himself took to the battlefield and was destroyed by the Super Zeo Megazord.\n\nIt was during Mondo's absence that the Empire was led by Louie Kaboom, a renegade robot created by Lord Zedd and Rita Repulsa, King Mondo's bitter rivals seeking to reclaim their evil empire. He meant well and succeeded in taking over the Machine Empire by exiling Queen Machina and Prince Sprocket. However, Louie squandered their forces and was eventually overthrown by Mondo and Machina's eldest son, Prince Gasket, and his wife, Archerina. The two led the Machine Empire against the Zeo Rangers rather well, initiating a massive, all-out invasion on Angel Grove. Gasket and Archerina came close to beating the Rangers a few times, but were consistently interfered by both the Rangers and one time by Lord Zedd (whom the latter realized that if Gasket succeeded in defeating the Rangers, the Machine Empire would take over Earth and leave both him and Rita in exile). After this invasion failed, King Mondo returned, causing Gasket and Archerina to flee. When the Royal House of Gadgetry were later destroyed by Rita and Zedd, the Machine Empire went into a period of inactivity, useless without the Royal House to lead it.\n\nThe Royal House of Gadgetry later appeared, rebuilt, as part of 's alliance. When Dark Specter initiated his universal invasion, the Machine Empire invaded KO-35 and the Phantom Ranger's homeworld, conquering both planets and defeating the Karovan rebels, the Phantom Ranger, and the Blue Senturion. When Zordon's energy wave struck these planets (during \"Countdown to Destruction\"), much of the Machine Empire (including King Mondo) was destroyed, being reduced to a pile of sand.\n\nYears later in an episode of \"Power Rangers Wild Force\" called \"Forever Red\", the remnants of the Machine Empire led by General Venjix gathered on the moon to make a final strike against the Power Rangers and the Earth. Unearthing Lord Zedd's Serpentera, they brought the massive Zord to full power, but were foiled by a team of ten Red Rangers. The other generals destroyed, General Venjix and Serpentera were destroyed by the Red Lion Ranger in his Red Wild Force Rider.\n\nThe Royal House was the ruling body of the Machine Empire at the beginning of \"Power Rangers Zeo\". It consisted of King Mondo and Queen Machina, Prince Sprocket, and their top servants Klank and Orbus.\n\nKing Mondo, also known as the Machine King, was the ruler of the Machine Empire, the husband of Queen Machina, the father of Prince Sprocket and Prince Gasket, and the main antagonist in \"Power Rangers Zeo\". Driving Rita Repulsa and Lord Zedd off the moon, Mondo believed the Earth would be an easy conquest, but was shocked when his army of Cogs was repelled by the . He was intrigued by the Rangers, as his other conquests had been far too easy, and decided that the Rangers proving to be formidable enemies would make eventually winning more enjoyable. However, Mondo soon grew tired of the Rangers, and just as it seemed he would defeat them, the Gold Ranger arrived and foiled Mondo again. Mondo, tired of fooling around, unearthed the ancient Sword of Damocles. He grew and battled the Rangers, but was defeated and destroyed by the Super Zeo Megazord.\n\nKing Mondo was later rebuilt and returned to the throne, chasing away Prince Gasket and his wife Archerina. He also discovered that Zedd and Rita has returned to the Moon and is attempting to overthrow his family to reclaim their evil empire. Mondo then failed at an attempt to steal the Gold Ranger powers by growing giant, but was defeated by the Zeo Rangers and Trey the original Gold Ranger who also grew giant to battle him. Afterwards, Mondo confronted Zedd and Rita for not assisting him against the giant sized Power Rangers. They pretended to give up and offer Mondo and his family a peace offering gift, which was a bomb disguised. It soon blew him and the rest of the Royal House of Gadgetry to pieces. Mondo's head survived the blast and he swears revenge on Zedd and Rita.\n\nHe then returned in \"Power Rangers in Space\" as part of the invasion in \"Countdown to Destruction\". Zordon's energy wave destroyed Mondo for good.\n\nIn \"Forever Red,\" there was a statue of King Mondo in the base of the Machine Empire Remnants.\n\nKing Mondo was voiced by David Stenstrom who previously portrayed Hal Stewart from \"Masked Rider\".\n\nQueen Machina is King Mondo's wife, mother of Prince Sprocket and Prince Gasket, and queen of the Machine Empire. She usually carries a metal fan with her. She loves her husband and son dearly. Machina did show favoritism towards her other son, Gasket, over Sprocket a few times. This was most evident when he and Princess Archerina came to help her rid themselves of Louie Kaboom, an interloper machine developed by Rita and Zedd, and they came close to beating the rangers using Tommy against them. At one point, it was her birthday (or to be techinal, the anniversary of her construsction) and Mondo attempted to kidnap the Mysterious Musician (a disguised Skull) to play for her. She was destroyed at the end of the Zeo arc by Lord Zedd's bomb, was rebuilt, and then destroyed again by Zordon's energy wave. She is one of three (possibly five) female characters to be destroyed by Zordon's Wave.\n\nQueen Machina was originally voiced by Alex Borstein. However in \"Power Rangers in Space\", she was voiced by Brianne Siddall.\nPrince Sprocket was the younger son of King Mondo and Queen Machina and the younger brother of Prince Gasket. He was the first childlike villain of Power Rangers. Prince Sprocket was spoiled by his parents and liked to play, often with the Rangers. Sprocket would often steal Klank and Orubs's ideas, claiming them for himself; he always got away with it, since King Mondo considered them to be idiots who deserved blame for failed ideas. He had a bitter sibling rivalry with Gasket in who had better plans in defeating the Zeo Rangers. Sprocket was destroyed at the end of \"Power Rangers Zeo\" by Lord Zedd's bomb, rebuilt, and was briefly seen in \"Countdown to Destruction\" in which he was destroyed for good by the energy wave caused by Zordon's death.\n\nSprocket was voiced by Barbara Goodson.\nKlank is one of the top two robots that serve the Royal House of Gadgetry. Klank is a humanoid robot, while Orbus is more of a module. They serve as the monster enlargers of the Zeo arc, and are often kicked around by their masters. Klank has a rather thick Scottish burr when he speaks. He is also quite pompous and thinks highly of himself. He serves his masters with fierce loyalty, but never inserted himself physically into battle.\n\nOrbus is one of the top two robots that serve the Royal House of Gadgetry. Orbus is a mini module. They serve as the monster enlargers of the Zeo arc, and are often kicked around by their masters. Klank would swing Orbus on a thread and throw him at monsters, and Orbus would then make the monster grow. Orbus has a high-pitched voice.\n\nBoth were most likely destroyed or purified destroyed by Zordon's energy wave in \"Countdown to Destruction\".\n\nKlank was voiced by Oliver Page in a Scottish accent and Orbus was voiced by Barbara Goodson.\n\nTowards the end of the \"Power Rangers Zeo\" series, King Mondo was seemingly destroyed and control of the Machine Empire was left to several usurpers to the throne - first Louie Kaboom and later Prince Gasket and Archerina. Their voice actors, however, were all uncredited.\nLouie Kaboom was sent by Rita Repulsa and Lord Zedd in an attempt to take command of the Machine Empire once King Mondo had been temporarily destroyed. Goldar and Rito Revolto were sent to Earth to launch him from there as an attempt to avoid Machina discovering their secret return to the Moon. Unfortunately, though successful, Goldar and Rito lost the remote control they were using to control Louie, which infuriated Rita and Zedd as it left them with no other way of controlling Louie. After a failed attempt by the Power Rangers to use the remote to shut Louie down, Louie had himself rewired so that the remote no longer affected him.\n\nLouie did his best at leading the Empire, trying to win Queen Machina's affections in the process, but was met with failure each and every time. Meanwhile, Zedd and Rita were trying to regain control of Louie, but were less than successful. Louie quickly grew bored of Machina's resistance in not becoming his queen, took over the Machine Empire and imprisoned both her and Sprocket. Unlike Mondo whom often took the advice of his family, Louie took the advice of Klank and Orbus in how to defeat the Rangers.\n\nWhen Prince Gasket and Archerina arrived on the moon, Archerina took control of Louie with one of her love arrows. She had him attack Angel Grove and battle the Power Rangers, but he was destroyed by the strongest weapon: the Super Zeo Ultrazord.\n\nLouie Kaboom was voiced by Lex Lang in a rather thick Brooklyn accent.\n\nPrince Gasket was the first-built son of King Mondo and Queen Machina and the older brother of Prince Sprocket. He fell in love with Archerina, daughter of Mondo's nemesis King Aradon. Knowing their marriage would never be approved of by their respectful parents, the two of them eloped a la \"Romeo and Juliet\".\n\nGasket has a bitter sibling rivalry with Sprocket over who is the true heir apparent to the Royal House of Gadgetry, over Queen Machina's favoritism, and over who has better plans against the Zeo Rangers.\n\nWhen news of Mondo's death reached him, Gasket returned to take care of his mother, depose of the interloper Louie Kaboom and take his rightful place as the Machine King. Gasket notably brainwashed Tommy Oliver, Zeo Ranger V, into temporarily thinking that he was the King of the Machine Empire and that the other Rangers were his enemies. Another time, he trapped the people of Earth in a time loop, forcing them to relive the same day over and over so that he could observe the best way to attack them. Unfortunately for Gasket, Tommy knew there was a time loop thanks to Gasket's earlier tampering with his brain and was able to use that knowledge to put a stop to the plan.\n\nWhen Mondo was rebuilt and returned, Gasket and Archerina fled temporarily. Prince Gasket and Archerina then came back in the episode \"Hawaii Zeo,\" where Prince Sprocket offered the idea of growing into giants to defeat the Rangers. This actually was a cunning attempt by Sprocket to get rid of his brother (as he knew full well that any creature that grew would be destroyed by the rangers). Following Sprocket's plan, Gasket and Archerina grew into giants. After an intense battle where they held their own and nearly overwhelmed almost the Rangers' entire arsenal of zords, they ultimately became overwhelmed by the Zeo Ultrazord. But just as Prince Sprocket came out of hiding and began celebrating the apparent destruction of his brother and sister-in-law, the two were revealed to have survived the blast (though they shrank back to normal size). Just as they were about to face Sprocket, King Mondo arrived to confront them. Prince Gasket and Archerina then fled and haven't been seen since.\n\nPrince Gasket was voiced by Douglas Sloan, one of the show's directors, writers, and its producer.\n\nNOTE: In \"Ohranger\", Prince Gasket's Japanese equivalent is actually the upgraded form of Prince Sprocket's Japanese equivalent (note that in all instances where the princes appeared together, they use slightly different color schemes as with all of the American costumes for the Machine Empire). Also in \"Ohranger\", Prince Gasket and Archerina were destroyed by the Zeo Ultrazord where they left behind a child prior to their destruction.\n\nPrincess Archerina is a robot princess with the appearance of a female archer, and she is the wife of Prince Gasket. Archerina is the daughter of King Mondo's nemesis King Aradon. Archerina and Gasket eloped and fled from their parents, knowing that both their families will disapprove of their marriage.\n\nShe can fire arrows of energy which she can use to make others fall in love with her. Archerina is also capable of firing arrows of destructive energy from her bow. Archerina can teleport with a fiery aura, turning herself into a living missile. Her bow can also transform into a sword to be used in close combat.\n\nShe and Gasket returned when King Mondo was destroyed by the Zeo Rangers, and took over in his absence. When the tyrant Louie Kaboom tried to take over, Archerina used her love arrows to make him her slave, forcing him to obey her, which later led to his destruction.\n\nArcherina's jealousy of Katherine Hillard led to the formation of a strong grudge against her.\n\nUpon King Mondo's return, Archerina fled the Earth with her husband, but came back in \"Hawaii Zeo\" to confront the Rangers once more. After becoming a giant alongside Gasket, both Gasket and Archerina were defeated by the Zeo Ultrazord, but they survived. However, King Mondo arrived to confront the two of them, and they quickly escaped, never to be seen again. An abandoned concept for \"\" would have featured the pair returning along with other unaccounted for villains from earlier season to threaten the Earth.\n\nArcherina was voiced by Melora Harte.\n\nNOTE: In Ohranger, Prince Gasket and Archerina were destroyed by the Zeo Ultrazord where they left behind a child prior to their destruction.\n\nYears after \"Countdown to Destruction\" in the \"Power Rangers Wild Force\" episode \"Forever Red\", the last generals of the Machine Empire gathered on the moon. Determined to obtain revenge for the destruction of their beloved monarch King Mondo after what happened to him in the \"Countdown to Destruction\" event, they set out to resurrect Serpentera. Led by the Generals Venjix, Tezzla, Gerrok, Steelon, and Automon, the Machine Empire unearthed the giant Zord and placed a neo-plutonium core inside to power it. However, just as they were about to board Serpentera, a team of 10 Red Rangers arrived on the moon. The Generals battled the Rangers, and Tezzla, Gerrok, Steelon, and Automon were destroyed. Venjix escaped to Serpentera, which he flew towards Earth. However, the Red Wild Force Ranger used his Wild Force Rider to reflect Serpentera's blast back into it. The Zord was destroyed, along with Venjix and the Machine Empire.\n\nIt should also be noted that the Machine Empire Remnants are really recycled \"Big Bad Beetleborgs\"/\"Beetleborgs Metallix\" costumes. Due to the use of these Beetleborg costumes, the Machine Empire Generals appear to be more technologically advanced than the rest of the Machine Empire including the Royal House of Gadgetry itself.\n\nFour years after the destruction of the Machine Empire, surviving members of the Empire remain seeking to destroy the Earth. Tezzla, Gerrok, Steelon, and Automon (and many Cogs) are all that is left of the Empire, as well as General Venjix, their leader. They have recently learned that Earth's Moon is the resting place of Serpentera, a gigantic war machine created by Lord Zedd with the power to destroy a whole planet. Ever since Lord Zedd was changed into a human being, this happened in \"Countdown to Destruction\", the location of Serpentera remained a mystery for many years. Venjix finds it and begins using Cog soldiers to dig it up, refitting it with a Neo Plutonium reactor to power it. Venjix and his other Generals prepare to board Serpentera, but they are then interrupted by the Red Rangers. Venjix sends Cogs to distract the eight Rangers while he and the others flee to Serpentera. The Rangers battle the Cogs unmorphed and easily defeat them. Eventually, Cole escapes the barrage of Cogs and chases after the Generals. Venjix fires at him, but Cole is rescued by Leo and Aurico on Leo's Jet Jammer. Then all the 10 Red Rangers gather, morph with their respective morphing calls (except for Aurico, who has already morphed), and battle the Generals. Red Wild Force Ranger Cole Evans and Red Mighty Morphing Ranger Jason Lee Scott fight Venjix, Cole having trouble at first but coming back strong after Jason single-handedly overwhelms Venjix with a corkscrew kick to show off to Cole.\n\nCole later severely damages Venjix with his Crystal Saber (Blazing Lion attack). Battle-damaged, Venjix then jumps into Serpentera and takes off. The Rangers fear they have failed, but Cole calls upon his Wild Force Rider to battle Venjix. Cole uses the Wild Force Rider to fly into Serpentera and destroy it from the inside. Both Venjix and Serpentera are finally destroyed, and Cole ends up surviving the explosion despite the others' momentarily thinking he may have been destroyed too.\n\nGeneral Venjix is voiced by Archie Kao.\n\nNOTE: General Venjix is a recycled version of Shadowborg.\n\nShe fought and engaged the Quantum Ranger Eric Myers and Red Alien Ranger Aurico, they destroyed her with great team work and Eric using his Quantum Defender.\n\nTezzla is voiced by Catherine Sutherland.\n\nNOTE: Tezzla is a recycled version of Ladyborg without the antenna.\n\nHe battled the Red Zeo Ranger Tommy Oliver and Red Time Force Ranger Wesley Collins, Tommy destroys him with the Zeo Flying Power Kick.\n\nGerrok is voiced by Walter Emanuel Jones.\n\nNOTE: Gerrok is a recycled version of the Green Hunter Borg.\n\nHe engaged Red Space Ranger Andros and Red Lightspeed Rescue Ranger Carter Grayson, who destroyed him by combining power from their Astro, Rescue, and Thermo Blasters.\n\nSteelon is voiced by an uncredited Scott Page-Pagter.\n\nNOTE: Steelon is a recycled version of Dragon Borg minus the dragonfly wings on the head.\n\nHe excellently fought against Red Turbo Ranger T. J. Johnson and Red Lost Galaxy Ranger Leo Corbett as they destroy him with their Quasar Saber, Transdagger, Turbo Blade, and Turbo Lightning Sword attack.\n\nAutomon is voiced by an uncredited David Walsh.\n\nNOTE: Automon is a hybrid of two Beetleborgs suits. He has the head of Fire Borg and the body of Lightning Borg.\n\nThe Cogs are the Machine Empire's foot soldiers. They pilot octopus-shaped jets called Quadrafighters and are able to blast lasers out of their eyes. They appeared mostly as silver-suited robots with yellow heads. However, there were also some that had maroon versions of the suit. Some were capable of speech, but very rarely.\n\nNOTE: Zordon states that Cogs have to be completely dismantled in order to be stopped; however, the rangers only badly damage them. This may mean that if not completely destroyed Cogs would simply be repaired and sent back to fight again.\n\nThe monsters of this show are robots created by the Machine Empire. They are adapted from the monsters featured in \"Chōriki Sentai Ohranger\". To make a monster grow, Klank would swing Orbus around until he latched onto the monster emitting an energy that would enlarge the monster.\n\n"}
{"id": "16217120", "url": "https://en.wikipedia.org/wiki?curid=16217120", "title": "Marine band (geology)", "text": "Marine band (geology)\n\nMarine band is a geological term for a bed of rock, commonly black or dark grey shale, containing an abundance of fossils of marine organisms. These strata represent episodes of flooding by seawater and are important in enabling the comparison or correlation of rock sequences in different localities.\n"}
{"id": "49283111", "url": "https://en.wikipedia.org/wiki?curid=49283111", "title": "Massimiliana Landini Aleotti", "text": "Massimiliana Landini Aleotti\n\nMassimiliana Landini Aleotti (born 1942/43) is an Italian billionaire heiress, the owner, with her two children, of the pharmaceutical company Menarini, and one of the world's ten richest women.\n\nAs of February 2016, \"Forbes\" estimated her net worth at US$11.6 billion.\n\nHer daughter Lucia, is chairman of Menarini, and son Alberto Giovanni is vice chairman.\n"}
{"id": "1561378", "url": "https://en.wikipedia.org/wiki?curid=1561378", "title": "Maxwell Lord", "text": "Maxwell Lord\n\nMaxwell Lord IV, or simply Max Lord, is a fictional supervillain appearing in American comic books published by DC Comics. The character first appeared in \"Justice League\" #1 (May 1987) and was created by Keith Giffen, J. M. DeMatteis, and Kevin Maguire.\n\nDepicted as a shrewd and powerful businessman, Maxwell Lord was influential in the formation of the Justice League International in the DC Universe.\n\nMaxwell Lord appeared in an episode of \"Smallville\" played by Gil Bellows. He was also in the first season of the television series \"Supergirl\" played by Peter Facinelli. In this version he is the founder of Lord Technologies and distrusts many government agencies.\n\nMaxwell Lord IV is the son of Maxwell Lord III, a successful businessman and head of the Chimtech Consortium. Maxwell III set out to be a good example for his son by striving to always do what was right. When Maxwell IV was 16, he came home to find his father dead in an apparent suicide. His father had discovered that his company had produced a highly carcinogenic product, and could not bear to live with the guilt and shame caused by the realization.\n\nLord's mother was convinced by her husband to employ a similar practice, cajoling heroic metahumans to help Lord. Thus, he sparked the plans to bring the Justice League, leaderless and broken after the Crisis on Infinite Earths event, under his exclusive control.\n\nLord initially works behind the scenes to establish the Justice League, while under the control of a villainous computer created by Metron. The computer wanted Lord to set up a worldwide peacekeeping organization as part of its plan to dominate the world.\nA later retcon changed his controller to the villainous computer program Kilg%re (pronounced \"Kilgore\"), which had taken over Metron's machine. A much later, post-Infinite Crisis retcon mitigated the Kilg%re's and the New Gods' influence, stating that Lord already had plans for taking over the League, and that he would have pursued them regardless.\n\nLord's ruthlessness at this time is illustrated when he sets up a disturbed would-be terrorist as a villain for the League to defeat, resulting in the man's death (the would-be terrorist believed he had a bomb connected to his heartbeat, but in fact Lord had disconnected it). Later, however, Lord rebels against the computer and (seemingly) destroys it.\n\nOnce free of the computer's influence, Lord is portrayed as an amoral businessman, but not a real villain. During the time that Giffen and DeMatteis were writing the Justice League, Lord is shown struggling with his conscience and developing heroic qualities, though he would remain a con-artist.\nOriginally a normal human, Lord is one of many people on Earth gifted with superpowers during the \"Invasion\" crossover, when a Gene Bomb is exploded by alien invaders. This bomb activates a latent metagene present in a small percentage of Earthlings. Lord gains the ability to control the minds of others, albeit at great difficulty. Despite being a metahuman himself, Lord never feels like one; instead, his omnipresent mother pressures him to act for the benefit of non-powered individuals, shifting his never-ending hatred from the generic \"authority figures\" who caused his father's death to the metahuman community.\n\nAfter he is shot and placed in a coma at the start of the 15-part \"JLAmerica/JLEurope\" crossover \"Breakdowns\", Dreamslayer, a supervillain who (with the aid of the Extremists) had once destroyed all life on the Extremists' planet, takes over Lord's body and supercharges Lord's power, allowing him to control thousands of minds at once. Using Lord's body and power, he causes the Justice League International (JLI) to lose its charter and almost forces it to disband. While the possessed Lord forces the JLI to battle itself, the mortally wounded Silver Sorceress manages to contain Dreamslayer, and holds him within her mind as she dies, taking him with her. When Lord is freed, his power is burnt out.\n\nLater, Lord is diagnosed with a brain tumor and seemingly dies. Kilgre, however, had been waiting patiently for the right moment to reactivate its control of Lord. Kilgre downloads his consciousness into a duplicate of one of the Extremist robots, Lord Havok (in a further retcon the body is said to be a New Genesis-built automaton, which later fell into Checkmate's possession). In this form, he spends some time testing the League for unknown reasons. He also takes control of the secret organization known as the Arcana. His cyborg body later comes to resemble his original human form.\n\nLord pulls together several former JLI members, including L-Ron, Captain Atom, Blue Beetle, Booster Gold, and Fire as the \"Super Buddies\", advertised as \"Heroes the common man could call.\" These stories are told in the six-issue miniseries \"Formerly Known as the Justice League\" in 2003, and its 2005 sequel, \"I Can't Believe It's Not the Justice League.\"\n\nDuring the JLI-era, Doomsday causes the death of Superman among others. Ultimately, due to Mongul's invasion and destruction of Coast City, Maxwell loses his mother, still residing in their Coast City home. This event fuels his hatred and paranoia against the metahumans, as well as leading him to believe that not only can metahumans not be trusted, but that their personal battles and scuffles are enough to shatter world safety.\n\nIn Brad Meltzer's \"Identity Crisis\" (2004), Lord attends Sue Dibny's funeral and speaks to Booster Gold, further denting his already dwindling faith in superheroes.\n\nThe 2005 80-page one-shot \"Countdown to Infinite Crisis\" reveals that Lord is no longer a cyborg, and is apparently a criminal mastermind who spent years running the JLI while gathering sensitive information about the world's superheroes, whom he considered a threat to the planet. Simultaneously, he sabotaged JLI efforts in order to render the superhero team as ineffectual as possible. At the end of the prologue special issue, he shoots and kills one-time JLI member, Ted Kord, the second Blue Beetle, when the hero discovers Lord's secret and refuses to join Lord.\n\nDuring this time, Alexander Luthor, Jr., the god-like son of Lex Luthor from an alternate Earth, gives Lord control over Batman's Brother Eye, a satellite system Batman created to monitor all superhuman contact. Lord uses Brother Eye to create an army of OMACs (humans infected with a nano-virus that transformed them into cyborgs), programmed to hunt down and kill all superhumans.\nLord also uses his powers to influence Superman's mind, causing him to brutally beat Batman in the belief that he is Brainiac. Lord subsequently sends Superman to attack Wonder Woman after making him believe that she is his old enemy Doomsday, and that she is trying to find Lois Lane in order to kill her. Lord justifies the resulting destruction as proof of his argument about the dangers of superhumans, pointing out the devastation that Wonder Woman and Superman could cause if they fought in a crowded area, and arguing that the fact that Superman can be brought under another's control is evidence that superhumans cannot be relied upon. In the midst of her battle with Superman, Diana realizes that even if she defeats him, he would still remain under Lord's absolute mental control. She creates a diversion lasting long enough for her to race back to Lord's location and demand that he tell her how to free Superman from his control. Bound by her lasso of truth, Lord replies, \"Kill me.\" Wonder Woman then snaps his neck (\"The OMAC Project\", 2005). In response, Brother Eye broadcasts the footage of Wonder Woman killing Lord all over the world, destroying her reputation and her friendship with Batman and Superman, who reject her despite the fact that she saved their lives. As the crisis unfolds, the three eventually reconcile as Diana helps Superman talk down his other self and prevents Batman from shooting Alexander Luthor, Jr., accepting that Diana did what she had to do and acknowledging that, for all their differences, they all still want justice.\n\nAt the \"Crisis Counseling\" panel at Wizard World Chicago, Dan DiDio explained DC's reasoning in using Lord's character in \"Infinite Crisis\". After going through several possible characters who could be the \"new leader for the offshoot of Checkmate\", Maxwell Lord was suggested. Many of the editors thought that the idea made sense, as Lord had been shown to have a mean streak and to have killed previously. The idea was dropped due to the continuity errors, such as him being a cyborg, but they went back to it later after deciding none of the other possible characters were suitable. \"We thought about that aspect of the story [where Maxwell was turned into a cyborg] some more,\" DiDio explained. \"And then asked, 'Did anyone read it?' No. 'Did anyone like the idea?' No. So we moved ahead with Max as being a human, and having been a human, and not letting that small part of the past stand in the way of this story. We wanted what was best for \"Countdown\" [\"to Infinite Crisis\"], and for us, that meant that Max had to be a human.\"\n\nLord reappeared in 2007 in the first two story arcs of the new \"Booster Gold\" series by Geoff Johns and Dan Jurgens. At the end of the \"52 Pick-Up\" story arc, Booster Gold and Blue Beetles from the past, present and future go back in time to \"Countdown to Infinite Crisis\" and prevent Lord from killing Ted Kord. In the subsequent \"Blue and Gold\" story arc, Blue Beetle and Booster Gold discover that, in saving Ted Kord, they have created a new timeline where Lord was never killed by Wonder Woman and Lord's OMACs, and that a mind-controlled Superman has turned the entire planet into a police state. Lord reveals that he had been returned to human form after dozens of clandestine operations and that he learned the importance of control during that time. When Booster Gold and Blue Beetle, having assembled their old JLI teammates, storm Brother Eye, Lord is killed by Dr. Light when she blasts a hole through his chest. Ted Kord realizes that his death is the only way to fix the timestream and leaves the battle, seemingly to return to the past and accept his death.\n\nLord next makes an appearance in the \"Trinity\" maxi-series (2008-9). Lord's skull is stolen by a group called the Dreambound and brought to Morgaine Le Fey for usage in a spell, which requires an item connected to Wonder Woman.\n\nDuring the \"Blackest Night\" (2009–10) storyline, Maxwell Lord is identified as one of the deceased people entombed below the Hall of Justice. Lord's corpse is revived as a Black Lantern during the event. Targeting Wonder Woman, he lures her to Arlington National Cemetery with a trail of slaughtered bodies. When Wonder Woman arrives, he springs a trap, using black rings to revive the bodies of fallen soldiers. Wonder Woman uses her lasso to reduce Lord and the soldiers to dust. However, as she leaves, the dust begins to regenerate. Some time later, Lord resumes his attack on Wonder Woman, who had recently been deputized as a Star Sapphire. Wonder Woman encases Lord's body in a violet crystal, then shatters it to pieces. However, Lord continues to taunt her, speaking out of a piece of crystal. He is later brought back to life by the power of the White Light. Though Guy Gardner attempts to restrain him, Lord uses his mind control abilities to make Guy let him leave.\n\nLord is among the other resurrected heroes/villains featured in the \"Brightest Day\" (2010) series. He is first seen attempting to push his mind control powers further than ever, but severely injures himself in the attempt, despite his meticulous preparations, which include a constant blood supply and an ice pool. Later, Deadman's white power ring gives him a vision where Lord is shown shaking hands with Jaime Reyes, the third Blue Beetle. However, in this vision, Lord is hiding a gun behind his back, implying that Lord is planning to kill Reyes, just as Lord killed Reyes' predecessor, Ted Kord.\n\nIn the first issue of \"\", Lord is the subject of an unprecedented international manhunt. He is found hiding in the old Justice League International embassy by Booster Gold, whom Lord is able to defeat. Lord then uses a device to amplify his mind control powers to unprecedented levels. With these, he erases the world's memory of his existence. Initially, it appears that only his former Justice League colleagues Booster Gold, Ice, Fire, and Captain Atom remember Max. It is later revealed in \"Brightest Day\" #8 that Deadman also remembers his existence. Lord uses his powers to disgrace the team, having Fire ousted from Checkmate, Captain Atom turned in as a fugitive for betraying the U.S. Army, and Ice isolated from Guy Gardner, who Lord causes to believe that she tried to kill him. He also influences the superhero community into believing Ted Kord committed suicide, which enrages Booster Gold. He then sends OMACs after the current Blue Beetle, Jaime Reyes, who calls Booster Gold and the others for help. Meanwhile, Lord discovers that his resurrection has come with a side effect: some of his efforts to control others' minds transform his targets into cadavers wearing Black Lantern uniforms. Contacting his former colleagues through a fallen Rocket Red's armor, Lord reveals that he intentionally exempted them from the worldwide mindwipe, and that he wants them to protect the world as they did in the old days. He then warns the group not to come looking for him, unaware that Blue Beetle had located his transmission signal.\n\nCaptain Atom tells the others that an explosion, seen in \"Brightest Day\" #1, propelled him briefly into the 24th century, where he saw a world reduced to a pre-industrial state by a metahuman war instigated by Lord. The team resolves to try to prevent Lord from bringing about this dystopia. Maxwell Lord is at one point contacted by the Entity, who tells him to stop Magog from plunging the world into war. Lord then sees a vision of himself, with Magog's staff, killing a distraught Magog, who begs for mercy. The Entity gives Lord a vision of a possible future where Magog's team attacks Parasite. In this vision, Parasite's absorption of Captain Atom causes an explosion that destroys everything within a large radius and annihilates over a million people (similar to the \"Kingdom Come\" future). Also, Power Girl witnesses her new enemy C.R.A.S.H. confronting Lord before heading towards the teleporter.\n\nWhen the team next encounters Lord, after mind-controlling first Fire and then Booster Gold to prevent them from stopping him, he teleports from the old JLI embassy back to Checkmate, where he attempts to enlist Magog to kill Captain Atom. Lord uses technology to upgrade Magog's staff into emitting energy blasts. Meanwhile, Lord asks Professor Ivo to reprogram the Metal Men. He then asks Doctor Sivana to create a genetically engineered humanoid clone of Power Girl.\n\nWhen Captain Atom and Magog battle in the heart of Chicago, Captain Atom is able to convince Magog that he's being used, and Magog remembers Lord. Lord, watching, uses his powers to force Magog to kill himself. He then makes everyone believe that they watched Captain Atom murder Magog. The Entity proclaims that Lord has completed his task, and his life is restored to him. After briefly receiving a White Ring, the recently returned Bruce Wayne seems to be aware of Max Lord's resurrection. When Captain Atom absorbs the energy from Magog's spear, he is propelled forward through time to 112 years in the future, where Lord, while long dead, has plunged humanity into a massive metahuman war ruled by OMACs. Captain Atom is eventually returned to the present, but not before a dying Power Girl tells him that the catalyst for all this was Wonder Woman's death by Lord's hand. Also, Batman (Damian Wayne) tells him how to stop Lord's ultimate plans. However, Lord is struck with the discovery that, with the exception of the original four ex-JLI members, no one in the world remembers Wonder Woman.\n\nLater, when the Creature Commandos attack the JLI embassy, Lord poses as a member. He captures Jaime Reyes, and heads towards the teleporter with him, while the others are unable to stop him. Lord regains his abilities to transform his targets into cadaver OMACs, and he tortures the captive Blue Beetle. Lord's mindwipes feed off psychic energy, so the more people who are around, the faster some of them will forget. In issue 19, the rest of the team locates Lord's secret facility in a submersible below the Sea of Japan. Seconds before the team reaches him, and as predicted by the White Ring, Lord shoots Jaime in the head (echoing his execution of Jaime's predecessor, Ted Kord). The JLI arrives and attacks Lord, but he escapes from the JLI in one of his headquarters' escape pods, and the headquarters vanishes.\n\nWhile the JLI learns Jaime is alive, the team also learns that Lord not only manipulated them, but also wanted the JLI to chase him to distract Checkmate. Later, Lord uses a device to enhance his mental powers, turning people around the world into OMACs to attack Wonder Woman and the JLI. Before the device activates, Lord sends the OMAC Prime that he controls to attack the heroes. Booster Gold manages to locate Lord's flying headquarters, attacking it to confront him face-to-face. Lord gains the upper hand with his mental powers, but Captain Atom grabs him, after having become overloaded with quantum energy in the fight with OMAC Prime and about to be pulled into the timestream. Atom threatens to take Lord with him unless Lord undoes the global mindwipe, and a desperate Lord complies. Captain Atom is pulled away and Lord teleports to escape from the heroes. Later, he posts an online video where he blames Professor Ivo for Magog's rampage. He also says that he only wants to protect the world from the metahuman threat, and he will continue to do so in secret.\n\nIn September 2011, The New 52 rebooted DC's continuity. In this new timeline, Max Lord still runs Checkmate as the Black King, but now has leadership seniority over Cadmus and its programs. He successfully captures Batman's superpower monitoring satellite, Brother Eye, for his own purposes, but it ends up creating an agent with which it would act through seeking vengeance. Lord ends up getting into heated battle with an all new OMAC who was formerly one of the project's employees.\n\nStill the running head of Checkmate, Lord assembles a team consisting of Doctor Polaris, Emerald Empress, Johnny Sorrow, Rustam of Prime Earth, and Lobo to assist him in his efforts to take down Amanda Waller. He chose these five as they were the 'original' incarnation of the Suicide Squad, and hence have experience working as a team, prompting Waller to capture the Justice League to ask for their help against Lord. While most of the heroes and villains do not recognize Lord, the displaced pre-\"Flashpoint\" Superman shows awareness of him, reflecting on his past contact with Lord before reality was reset.\n\nAlthough the combined forces of the League and the Squad are able to defeat Lord's allies, they are unable to prevent Lord from achieving his goal; with the aid of a controlled Killer Frost, Lord acquires the Heart of Darkness from a vault, using it to enhance his powers to \"infect\" (take control of) the League. Lord uses the infected Justice League (apart from Batman, who Lord did not infect) to achieve \"peace\" across America, and has Waller kidnapped and taken before him. However, despite believing that he had taken precautions to control the Heart, Lord is forced by Waller to recognize that it is manipulating his perceptions, using Lord's powers to spread chaos and evil across the world, and twisting Lord's perception of what is transpiring. When Waller is able to bring him to his senses, Lord tries to remove the Heart of Darkness, but it consumes him and transforms him into Eclipso. Eclipso is driven out of Lord when Killer Frost is able to use her powers to create a prism of ice, channeling Superman's heat vision at just the right frequency to disrupt Lord's hold on his slaves, with Lord subsequently being immobilized by Killer Frost. Lord awakens in a cell specially designed to hold him, with injectors pumping so much blood thinner into him that he would bleed to death if he attempted to access his powers. He mockingly asks Waller if she set this whole thing up just to 'justify' the Squad to the League, but Waller declines to reply, and simply informs him that he is to prepare himself for service on \"Task Force XI\".\n\nIn his original depictions, Maxwell Lord had no abilities, but later became a metahuman as a result of the Dominators' invasion of Earth. Lord's powers allow him to telepathically influence people's minds, typically in the form of pushing a subconscious suggestion to others. Using his power causes Max's nose to bleed, and requires great mental strain. Over time, Lord's powers grew to the point where he could take full control of other beings, even Superman. His powers made him such a threat to global security that Wonder Woman was forced to kill him.\n\nWhen the character was resurrected, following \"Blackest Night\", in the story \"\", he prepared to erase the world's memories of his past criminal actions; in order to survive the trauma to his brain, he placed his body in a large tub of ice and hooked himself up to a blood transfusion machine. The entity somehow changed his abilities and would initially convert living people into the corpses of a Black Lantern. With his task completed and life restored, he then exhibited voluntary activation and control over every resting O.M.A.C. infected within global the populace.\n\nFollowing a company-wide reboot, Lord was reintroduced as part of the DC Rebirth publishing event, now having been a metahuman since his youth. His primary power is tweaked to a form of powerful psychic persuasion which works best when paired with a vocal command. In this later depiction, his powers work by exploiting and promoting people's underlying desires and inhibitions to make them do his bidding. The only draw back being his power doesn't give him outright control over them, he can only push them to act on their own subconscious wants. He also exhibits low-level mind reading abilities.\n\n\nIn the unproduced \"Justice League\" film \"Justice League: Mortal\", Lord was to be the primary antagonist, and was set to be portrayed by Jay Baruchel.\n\nIn Injustice 2 he is mentioned for using his powers to trick the Enchantress (DC Comics)\n"}
{"id": "40230970", "url": "https://en.wikipedia.org/wiki?curid=40230970", "title": "Mirwais Jalil", "text": "Mirwais Jalil\n\nMirwais Jalil (1969 – 29 July 1994), was an Afghan journalist for the BBC World Service near Kabul, Afghanistan. Jalil has been praised for being crucial in the BBC's coverage of the Afghanistan civil war and as a highly credible journalist. On 29 July 1994, he was returning from an interview with Prime Minister Gulbuddin Hekmatyar when masked men kidnapped him, and Jalil was found murdered the next morning outside of Kabul. Jalil's aggressive coverage of the civil war between the mujahedeen was seen as authoritative and is said to be the reason of his fate.\n\nMirwais Jalil was born the son of a medical practitioner in Kabul, Afghanistan in 1969. Growing up, Mirwais became fluent in Persian and Pashto, two main languages in Afghanistan. After Mirwais finished high school in Kabul, his family moved to Pakistan to escape the dangerous and volatile landscape that had erupted in Kabul. Mirwais took an English class and soon became a freelance reporter. His freelance reporting would soon land him a position with the BBC where he would cover the raging Afghanistan civil war that was waging in Kabul. Miwais enjoyed the excitement of covering the front lines in Kabul and his passion and authoritative reporting made him a valued journalists in covering the war in Kabul.\n\nMirwais Jalil began his short career in journalism by taking an English class in Pakistan. He began his career doing freelance journalism which would later lead to his position with the BBC. He soon became a key factor in the raging civil war that took place in Afghanistan in the early 1990s. At the time of his death, Mirwais was working freelance for the BBC's Pashto and Persian services. He had been with the BBC for less than 2 years but was well cultured and involved with the reports from the frontline. Mirwais was only 25 when he was killed but was often to referred to as vigorous and enthusiastic. He enjoyed being on the front lines in Kabul and would remain there even after others had retired for the day. Mirwais enjoyed showing other journalists around. The BBC has said that Mirwais was their top Afghanistan correspondent at the time of his death.\n\nMirwais's life came to an end on 29 July 1994. Mirwais Jalil was returning from an interview with Gulbuddin Hekmatyar, Afghanistan's prime minister in 1994. Mirwais was travelling on a dirt road near Kabul with an Italian journalist, Ettore Mo from \"Corriere della Sera\", when the Volkswagen was ambushed by a black sedan where five half masked figures jumped out and grabbed Mirwais from the car he was travelling in. He was brutally pulled from his backseat position in the car where he was dragged against his will at gunpoint and put into the waiting black sedan. The black sedan fled at high speeds away from Ettore and Sharif, the driver of Mirwais's car. Mirwais was the only passenger kidnapped. Those were the final moments anyone saw Mirwais alive. His body was found with multiple stab wounds and gun shots and was discovered only a couple kilometres from where he had been abducted near Chelsitoon, Afghanistan, a suburb of Kabul.\n\nThere was a civil war in full battle at the time of Mirwais's death. Gulbuddin Hekmatyar was engulfed in a fierce battle with the Afghan President Burhanuddin Rabbani over Kabul and strategic positions that both forces wanted to occupy. At the time of Mirwais's death in July, it had been reported that \"1.5 million Afghans had already lost their life in the war,\" according to the \"New York Times\". The fighting was the fiercest in Kabul for a number of weeks. After Mirwais's death, both Hekmatyar and Rabbani blamed the other party for his death. While the Afghan leaders and commanders enjoyed interviews, they were upset to have reports about their unfavorable war manoeuvres broadcast. This caused tension between journalists and the warring parties.\n\nMirwais Jalil was one of many journalists killed in Afghanistan in 1994. His vibrant, young personality helped getting reports from the front line of the raging war that might not have other wise been reported. He also managed to be one of the first journalists to interview Hekmatyar since the Battle of Laghman. Mirwais Jalil was noted as having witnessed a loss for Hekmatyar's troop in a battle that the Prime Minister denied. He was very vocal in criticising Mirwais's reporting of the battle and Mirwais pointed out what he had witnessed. Mirwais was credited along with his fellow journalists, with giving the BBC a reputation for \"objectivity\" and is noted for performing a \"vital job of informing his own and other people about the day to day realities of life and politics in Afghanistan.\"\n\nAfter Mirwais's death, the editor of the BBC sent out letters to both Gulbuddin Hekmatyar and Burhanuddin Rabbani in an attempt to get better protection for the journalists covering the bloody war. The BBC refused to say who they thought was responsible for Mirwais's death. Prime Minister Gulbuddin Hekmatyar blamed President Burhanuddin Rabbani for the death of Mirwais, and President Burhanuddin Rabbani blamed Prime Minister Gulbuddin Hekmatyar. People from the United Nations and individual governments believed that Afghanistan was in so much turmoil that the country was beyond help and the world was now looking down on the devastation and destruction of Afghanistan.\n\n\n"}
{"id": "58525867", "url": "https://en.wikipedia.org/wiki?curid=58525867", "title": "Moscow Methodological Circle", "text": "Moscow Methodological Circle\n\nThe Moscow Methodological Circle (MMC) was a scientific orgnisation set up by Georgy Shchedrovitsky to examine problems from an inter-disciplinary point of view, looking at the various methodological approaches of each discipline to yield what they described as \"systemic thinking activity\".\n\nThe MMC strated out as an informal group meeting in a pub on Gorky Street which included the mathematician Alexander Zinoviev, the sociologist Boris Grushin and the philosopher Merab Mamardashvili. They attracted the attention of the KGB but were tolerated.\n\nThe MMC has had a lasting impact on Russian systems thinking particularly through the Methodological School of Management. This is acknowledged by Viktor Khristenko.\n"}
{"id": "26255671", "url": "https://en.wikipedia.org/wiki?curid=26255671", "title": "Moshi Monsters", "text": "Moshi Monsters\n\nMoshi Monsters is a British website aimed at children aged 6-12, with over 80 million registered users in 150 territories worldwide. Users choose from one of six virtual pet monsters (Diavlo, Luvli, Katsuma, Poppet, Furi and Zommer) they can create, name and nurture. Once their pet has been customized, players can navigate their way around Monstro City, take daily puzzle challenges to earn 'Rox' (a virtual currency), play games, personalize their room and communicate with other users in a safe environment (most of the time).\n\nFollowing its online success, \"Moshi Monsters\" has expanded commercially with physical products, including toys, \"Moshi Monsters Magazine\" (the number-one selling kids' magazine in the UK in 2011), a best-selling DS video game, a number 4 music album, books, membership cards, bath soap, chocolate calendars, trading cards, figures of many Moshlings, mobile games, and . In December 2012, eight \"Moshi Monster\" toys were included in McDonald's Happy Meals in the United States and Canada.\n\nThe game was created by Michael Acton Smith, and developed in 2008 by entertainment company Mind Candy and finally launched in April 2008. As of December 2009, there were at least 10 million players registered. In March 2010, Mind Candy announced that there were 15 million users and by September 2010, that number had surpassed 25 million. In June 2011, it was announced that there were 50 million users.\n\nThe monsters are the characters that the user plays as. They are given a name by the user when they register at the website. There are six types of monsters; Poppet, Katsuma, Furi, Zommer, Diavlo, and Luvli. You can customise your monster in any way you want, but paid membership gives you more clothes and items and allows you to change your colour.\n\nThe monsters (in-game pets) keep their own pets, called Moshlings. They come in a variety of themed sets, including Arties, Beasties, Koeys and Spookies.\n\nNon-members can only keep 2 Moshlings in their room and Moshi members can keep up to six and visit other pets in the zoo.\n\nIn March 2012, Mind Candy confirmed a major partnership deal with Sony Music. The deal followed the recent launch of Mind Candy's own music label, Moshi Monsters Music. The deal will see Sony Music handle the distribution aspects of \"Moshi Monsters\" music releases, starting with the debut album \"Moshi Monsters, Music Rox!\" Jason Perry, formerly with the UK rock band A and head of Moshi Music, is driving the new album. The \"Moshi Monsters\" series features music from Sonic Boom, Beatie Wolfe, The Blackout, Portia Conn, and songs such as \"Moptop Tweenybop\" and \"Merry Twistmas\". Two albums are available on iTunes and Google Play, as well as on disc. One album contains the songs from \"\", and another album has some of \"Moshi Monster\"s first songs. Not all songs are available to buy on various platforms.\n\nIn October 2011, Ate My Heart Inc, representing the musician Lady Gaga, were granted an interim injunction by the High Court of Justice of England and Wales to stop Mind Candy, parent company of \"Moshi Monsters\", from releasing music on iTunes by a \"Moshi Monster\" character known as Lady Goo Goo. The songs intended for release included the parody \"Peppy-razzi\", similar to the Lady Gaga hit \"Paparazzi\". Justice Vos of the High Court ruled that Lady Goo Goo could appear in the \"Moshi Monsters\" game, but that Mind Candy could not release, promote, advertise, sell, distribute, or otherwise make available \"any musical work or video that purports to be performed by a character by the name of Lady Goo Goo, or that otherwise uses the name Lady Goo Goo or any variant thereon\". Lady Goo Goo was later replaced with a new Moshling named Baby Rox, who is not a parody of any particular celebrity.\n\nIn 2013, Mind Candy announced a \"Moshi Monsters\" film. In September 2013, Issue 34 of the \"Moshi Monsters Magazine\" included a \"Moshi Music\" DVD with a short trailer. On October 10, 2013, a short preview of the trailer was aired on \"ITV Daybreak\". Later that day, the trailer was released on MSN. The film was released on December 20, 2013 in the UK and February 20, 2014 in Australia. The DVD and Blu-ray were released on April 14, 2014 in the UK and April 3, 2014 in Australia.\n\nIn July 2013, Mind Candy released \"Moshi Monsters Village\" on Google Play, a 3D city-builder published by GREE and developed by Tag Games. After GREE UK shut down, Mind Candy decided to take over the game as publisher, leaving the development to Tag Games. The game was relaunched on Apple devices on December 18, 2013 right before the release of the movie.\n\nIn December 2013, Mind Candy published the companion app \"Talking Poppet\", also developed by Tag Games.\n\nIn February 2014, \"Moshi Karts\" was released on iOS by Mind Candy.\n\nIn June 2014, \"Moshling Rescue\" a \"match three\" game game based on the Moshling characters was released on iOS and Android.\n\nIn early 2015 Mind Candy released an app called \"World of Warriors\" which was shut down in October 2018.\n\nIn November 2016, they released the \"Moshi Monsters\" \"Egg Hunt\" app, alongside a companion storybook of the same name.\n\n\"Moshi Monsters\" creator, Mind Candy, made a loss of £2.2m in 2013 due to falling revenues from \"Moshi Monsters\". The company's financial results revealed that its revenues fell by 34.8% from £46.9m in 2012 to £30.6m in 2013.\n\nIn 2015, Mind Candy revealed that they were preparing to relaunch \"Moshi Monsters\" for a younger audience of four- to seven-year-olds, initially as animation with apps and toys to follow. However, no changes have been made to the \"Moshi Monsters\" site since then, apart from the removal of the forums section.\n\nSince 2015, the decline of \"Moshi Monsters\" and the site's creator Mind Candy has continued. The peak of \"Moshi Monsters\" popularity was in 2012 at £46.9m, and it has continued to tumble. Last year total revenues were £7.1m, compared with £13.2m in 2014.\n\n"}
{"id": "1277228", "url": "https://en.wikipedia.org/wiki?curid=1277228", "title": "NASA Astrobiology Institute", "text": "NASA Astrobiology Institute\n\nThe NASA Astrobiology Institute (NAI) was established in 1998 by the National Aeronautics and Space Administration (NASA) \"to develop the field of astrobiology and provide a scientific framework for flight missions.\"\n\nThe NAI is a virtual, distributed organization that integrates astrobiology research and training programs in concert with the national and international science communities.\n\nAlthough NASA had explored the idea of forming an astrobiology institute in the past, when the Viking biological experiments returned negative results for life on Mars, the public lost interest and federal funds for exobiology dried up. In 1996, the announcement of possible traces of ancient life in the Allan Hills 84001 meteorite from Mars led to new interest in the subject. At the same time, NASA developed the Origins Program, broadening its reach from exobiology to \"astrobiology\", the study of the origin, evolution, distribution, and future of life in the universe.\n\nIn 1998, $9 million was set aside to fund the NASA Astrobiology Institute (NAI), an interdisciplinary research effort using the expertise of different scientific research institutions and universities from across the country, centrally linked to Ames Research Center in Mountain View, California. Gerald Soffen former Project Scientist with the Viking program, helped coordinate the new institute. In May, NASA selected eleven science teams, each with a Principal Investigator (PI). NAI was established in July with Scott Hubbard as interim Director. Nobel laureate Baruch S. Blumberg was appointed the first Director of the institute, and served from May 15, 1999 – October 14, 2002.\n\nOn 2 December 2010 the Institute announced that one of its funded projects at the US Geological Survey, had discovered the first microorganism able to incorporate arsenic in its DNA instead of phosphate. The GFAJ-1 bacterium was found by team researchers at Mono Lake in California, but other researchers questioned and debunked the findings.\n\nThe NASA Astrobiology Program includes the NAI as one of four components, including the Exobiology and Evolutionary Biology Program; the Astrobiology Science and Technology Instrument Development (ASTID) Program; and the Astrobiology Science and Technology for Exploring Planets (ASTEP) Program. Program budgets for fiscal year 2008 were as follows: NAI, $16 million; Grants for the Exobiology and Evolutionary Biology Program, $11 million; ASTID, $9 million; ASTEP, $5 million.\n\nAs of 2018, the NAI has 10 teams including about 600 researchers distributed across ~100 institutions. It also has 13 international partner organizations. Some past and present teams are: \nNAI has partnership program with other international astrobiology organizations to provide collaborative opportunities for its researchers within the global science community.\n\n<nowiki>*</nowiki> International Associate Partners. All the rest are Affiliate Partners.\n\nSelected, significant topics of interdisciplinary research by NAI as of 2008:\n\n"}
{"id": "936769", "url": "https://en.wikipedia.org/wiki?curid=936769", "title": "Ol' Rip the Horned Toad", "text": "Ol' Rip the Horned Toad\n\nOl' Rip (died January 19, 1929) was a horned lizard (commonly referred to as a \"horned toad\" or \"horny toad\") whose supposed 31-year hibernation as an entombed animal is believed by some and doubted by others. His name is a reference to the fictional character Rip Van Winkle. In 1897, a horned lizard was placed in a cornerstone of the Eastland County Courthouse in Eastland, Texas along with other time capsule memorabilia. When the courthouse was torn down 31 years later, the cornerstone was opened on February 18, 1928, a live horned lizard was produced, allegedly from within the time capsule. The lizard became a celebrity, and went on tour, even being taken to Washington, D.C. to meet President Calvin Coolidge.\n\nOl' Rip died eleven months later, and his remains are on display in the new Eastland County Courthouse. In 1973 the body was stolen and an anonymous letter explained that the finding of Ol' Rip alive had been a hoax and demanded other unnamed co-conspirators should come forth. When no one did, another letter was received saying the coffin and body could be found in the county fairgrounds. The coffin was found there and returned to the courthouse. Some speculated that the body in the coffin was a substitute, with the real lizard being held in a private collection. Looney Tunes character, Michigan J. Frog was inspired by Ol' Rip's story.\n\n"}
{"id": "3936535", "url": "https://en.wikipedia.org/wiki?curid=3936535", "title": "Oogamy", "text": "Oogamy\n\nOogamy is the familiar form of sexual reproduction. It is a form of anisogamy (heterogamy) in which the female gamete (e.g. egg cell) is significantly larger than the male gamete and is non-motile. The male gametes are typically highly motile and are usually tasked with all of the travel necessary to bring the respective gametes together. The prevalence of oogamy in higher animals leads to the conclusion that this specialization of the gametes results in their performing their respective tasks better and more efficiently than those tasks could be performed by generalist isogametes, particularly the ability to concentrate high-energy substances in a smaller number of ova.\n\nOogamy is seen to be the evolutionary step that led to internal fertilization, in response to selective pressures at work to inspire this adaptation. This has been most attributed to the independence of animals from liquid water as a means of fertilization among terrestrial species, however other differentiations at the microscale could have also contributed to the change between oogamy and internal fertilization.\n\nOne explanation has been the cytoplasmic differences between two gametes, however Randerson & Hurst debunked this theory as unsupported (2001). Most alternative explanations that followed focused on selection based off size, behavioral interactions between gamete cells, and the allocation of resources ecologically.\n\nFor the selection of gamete size necessary to catalyze the transition from isogamy to oogamy, the fitness of an organism would have to increase as egg size increases, and increase as sperm size decreases. The number of gametes within the organism alone would exist as a type of selective pressure that promotes smaller sperm size, but disproportionately selects for less eggs as they grow larger within the organism. This supported Bell’s theory (1997) that the selective pressure had to select for growth of egg size more than shrinkage of sperm. The standard assumption remains that the zygote fitness is exponentially increased with zygote volume, with zygote numbers declining.\n\nMost recently, Dusenbery (2000) proposed a physical model under the assumption that eggs produce a sperm attractant via pheromones. This provided a mechanism of selection that dramatically increased target size, allowing for a powerful selective force that would produce larger eggs. Assuming the resources available to produce pheromone were proportional to egg volume, the target area was determined as the radius of diffusion of the pheromone’s active space, proportional to the rate of pheromone production, and this radius squared would be the target size. The results determined by Dusenbery suggested that this selection process would still occur if the assumption was lessened to only include pheromone production as proportional to egg surface area.\n\nOogamy predominantly occurs in animals, but can also be found in many protists, certain orders of algae (Ochrophytes, Charophyceans), and some plants such as bryophytes, ferns, and some gymnosperms like cycads and ginkgo.\n\nIn some algae, most gymnosperms and all angiosperms, a variation of oogamy occurs where the sperm cells are non-motile as well.\n\nIt appears that isogamy was the first stage of sexual reproduction. In several lineages, this form of reproduction independently evolved to anisogamy with gametes of male and female types to oogamy. There is a good argument that this pattern was driven by the physical constraints on the mechanisms by which two gametes get together as required for sexual reproduction.\n\nSince the discussion of the transition of oogamy generally involves the size proportions of certain sex cells, the ability to reproduce this way is closely associated to cell size of the organism themselves. The process of reproduction is the most prevalent method of restoring cell size and is often triggered when the size of the cell is less than critical level; however, this process differs between centric and pennate diatoms.\n\nCentric diatoms are oogamous. The male gametangial cell undergoes a series of four divisions to form a limited number of microspores, which are defined as sperm mother cells. These microspores undergo meiosis to form a flagellated sperm. The oogonial female cells can produce one to two eggs; this female protoplast can facilitate fertilization of the sperm via several different mechanisms, depending on the species. Fertilization results in a zygote, as characteristic in oogamy, which will enlarge into an auxospore after taking up water. It will be spherical in shape with a different valve morphology.\n\n\n2. Dunesbery, David B. Ecological Models Explaining the Success of Distinctive Sperm and Eggs (Oogamy). 13 December 2001. pdf. 3 June 2002.\n\n3. Halvorson, H. O. & Monroy, A. (1985). The Origin and Evolution of Sex. New York: Alan R. Liss\n\n4. Randerson, J P. and L D. Hurst. \"The uncertain evolution of the sexes.\" Trends Ecol. Evol. 16. n.p.: n.p., n.d.. 28 Mar. 2018.\n\n5. “Diatom Reproduction.” Diatom Reproduction, DePaul University, condor.depaul.edu/diatom/sex2.html.\n"}
{"id": "1558567", "url": "https://en.wikipedia.org/wiki?curid=1558567", "title": "Oral rehydration therapy", "text": "Oral rehydration therapy\n\nOral rehydration therapy (ORT) is a type of fluid replacement used to prevent and treat dehydration, especially that due to diarrhea. It involves drinking water with modest amounts of sugar and salts, specifically sodium and potassium. Oral rehydration therapy can also be given by a nasogastric tube. Therapy should routinely include the use of zinc supplements. Use of oral rehydration therapy decreases the risk of death from diarrhea by about 93%.\nSide effects may include vomiting, high blood sodium, or high blood potassium. If vomiting occurs, it is recommended that use be paused for 10 minutes and then gradually restarted. The recommended formulation includes sodium chloride, sodium citrate, potassium chloride, and glucose. Glucose may be replaced by sucrose and sodium citrate may be replaced by sodium bicarbonate, if not available. It works as glucose increases the uptake of sodium and thus water by the intestines. A number of other formulations are also available including versions that can be made at home. However, the use of homemade solutions has not been well studied.\nOral rehydration therapy was developed in the 1940s, but did not come into common use until the 1970s. Oral rehydration solution is on the World Health Organization's List of Essential Medicines, the most effective and safe medicines needed in a health system. The wholesale cost in the developing world of a package to mix with a liter of water is 0.03 to 0.20 USD. Globally as of 2015 oral rehydration therapy is used by 41% of children with diarrhea. This use has played an important role in reducing the number of deaths in children under the age of five.\n\nORT is less invasive than the other strategies for fluid replacement, specifically intravenous (IV) fluid replacement. Mild to moderate dehydration in children seen in an emergency department is best treated with ORT. Persons taking ORT should eat within 6 hours and return to their full diet within 24–48 hours.\n\nOral rehydration therapy may also be used as a treatment for the symptoms of dehydration and rehydration in burns in resource-limited settings.\n\nORT may lower the mortality rate of diarrhea by as much as 93%. Case studies in 4 developing countries also demonstrated an association between increased use of ORT and reduction in mortality.\n\nThe degree of dehydration should be assessed before initiating ORT. ORT is suitable for people who are not dehydrated and those who show signs and symptoms of mild to moderate dehydration. People who have severe dehydration should seek professional medical help immediately and receive intravenous rehydration as soon as possible to rapidly replenish fluid volume in the body.\n\nORT should be discontinued and fluids replaced intravenously when vomiting is protracted despite proper administration of ORT, signs of dehydration worsen despite giving ORT, the person is unable to drink due to a decreased level of consciousness, or there is evidence of intestinal blockage or ileus. ORT might also be contraindicated in people who are in hemodynamic shock due to impaired airway protective reflexes. Short-term vomiting is not a contraindication to receiving oral rehydration therapy. In persons who are vomiting, drinking oral rehydration solution at a slow and continuous pace will help the vomiting to resolve.\n\nWHO and UNICEF jointly have developed official guidelines for the manufacture of oral rehydration solution and the oral rehydration salts used to make it (both often abbreviated as ORS). They also describe acceptable alternative preparations, depending on material availability. Commercial preparations are available as either pre-prepared fluids or packets of oral rehydration salts ready for mixing with water.\n\nThe formula for the current WHO oral rehydration solution (also known as \"low-osmolar ORS\" or \"reduced-osmolarity ORS\") is salt (NaCl), trisodium citrate dihydrate (), potassium chloride (KCl), anhydrous glucose () per litre of fluid. This is 44 mmol salt, 10 mmol trisodium citrate dihydrate, 20 mmol potassium chloride, and 75 mmol anhydrous glucose per litre. This would have a total osmolarity of (44×2 + 10×4 + 20×2 + 75) = 243 mOsm/L.\n\nA basic oral rehydration therapy solution can also be prepared when packets of oral rehydration salts are not available. It can be made using 6 level teaspoons (25.2 grams) of sugar and 0.5 teaspoon (2.9 grams) of salt in 1 litre of water. The molar ratio of sugar to salt should be 1:1 and the solution should not be hyperosmolar. The Rehydration Project states, \"Making the mixture a little diluted (with more than 1 litre of clean water) is not harmful.\"\n\nThe optimal fluid for preparing oral rehydration solution is clean water. However, if this is not available the usually available water should be used. Oral rehydration solution should \"not\" be withheld simply because the available water is potentially unsafe; rehydration takes precedence.\n\nWhen oral rehydration salts packets and suitable teaspoons for measuring sugar and salt are not available, WHO has recommended that homemade gruels, soups, etc., may be considered to help maintain hydration. A Lancet review in 2013 emphasized the need for more research on appropriate home made fluids to prevent dehydration. Sports drinks are not optimal oral rehydration solutions, but they can be used if optimal choices are not available. They should not be withheld for lack of better options; rehydration takes precedence. But they are not replacements for oral rehydration solutions in nonemergency situations.\n\nIn 2003, WHO and UNICEF recommended that the osmolarity of oral rehydration solution be reduced from 311 to 245 mOsm/L . These guidelines were also updated in 2006. This recommendation was based on multiple clinical trials showing that the reduced osmolarity solution reduces stool volume in children with diarrhea by about twenty-five percent and the need for IV therapy by about thirty percent when compared to standard oral rehydration solution. The incidence of vomiting is also reduced. The reduced osmolarity oral rehydration solution has lower concentrations of glucose and sodium chloride than the original solution, but the concentrations of potassium and citrate are unchanged.\n\nThe reduced osmolarity solution has been criticized by some for not providing enough sodium for adults with cholera. Clinical trials have, however, shown reduced osmolarity solution to be effective for adults and children with cholera. They seem to be safe but some caution is warranted according to the Cochrane review.\n\nORT is based on evidence that water continues to be absorbed from the gastrointestinal tract even while fluid is lost through diarrhea or vomiting. The World Health Organization specify indications, preparations and procedures for ORT.\n\nWHO/UNICEF guidelines suggest ORT should begin at the first sign of diarrhea in order to prevent dehydration. Babies may be given ORS with a dropper or a syringe. Infants under two may be given a teaspoon of ORS fluid every one to two minutes. Older children and adults should take frequent sips from a cup. WHO recommends giving children under two a quarter- to a half-cup of fluid following each loose bowel movement and older children a half- to a full cup. If the person vomits, the caretaker should wait 5–10 minutes and then resume giving ORS. ORS may be given by aid workers or health care workers in refugee camps, health clinics and hospital settings. Mothers should remain with their children and be taught how to give ORS. This will help to prepare them to give ORT at home in the future. Breastfeeding should be continued throughout ORT.\n\nAs part of oral rehydration therapy, WHO recommends supplemental zinc (10 to 20 mg daily) for ten to fourteen days, to reduce the severity and duration of the illness and make recurrent illness in the following two to three months less likely. Preparations are available as a zinc sulfate solution for adults, a modified solution for children and in tablet form.\n\nFeeding the person after severe dehydration is corrected and appetite returns speeds the recovery of normal intestinal function, minimizes weight loss and supports continued growth in children. Small frequent meals are best tolerated (offering the child food every three to four hours). Mothers should continue to breastfeed. A child with watery diarrhea typically regains his or her appetite as soon as dehydration is corrected, whereas a child with bloody diarrhea often eats poorly until the illness resolves. Such children should be encouraged to resume normal feeding as soon as possible. Once diarrhea is corrected, WHO recommends giving the child an extra meal each day for two weeks, and longer if the child is malnourished.\n\nDehydration may be overestimated in wasted children and underestimated in edematous children. Care of these children must also include careful management of their malnutrition and treatment of other infections. Useful signs of dehydration include an eagerness to drink, lethargy, cool and moist extremities, weak or absent radial pulse (wrist), and reduced or absent urine flow. In children with severe malnutrition, it is often impossible to reliably distinguish between moderate and severe dehydration. A severely malnourished child who has signs of severe dehydration but who does not have a history of watery diarrhea should be treated for septic shock.\n\nThe original ORS (90 mmol sodium/L) and the current standard reduced-osmolarity ORS (75 mmol sodium/L) both contain too much sodium and too little potassium for severely malnourished children with dehydration due to diarrhea. ReSoMal (Rehydration Solution for Malnutrition) is recommended for such children. It contains less sodium (45 mmol/l) and more potassium (40 mmol/l) than reduced osmolarity ORS.\n\nIt can be obtained in packets produced by UNICEF or other manufacturers. An exception is if the severely malnourished child also has severe diarrhea (in which case ReSoMal may not provide enough sodium), in which case standard reduced-osmolarity ORS (75 mmol sodium/L) is recommended. Malnourished children should be rehydrated slowly. WHO recommends 10 milliliters of ReSoMal per kilogram body weight for each of the first two hours (for example, a 9-kilogram child should be given 90 ml of ReSoMal over the course of the first hour, and another 90 ml for the second hour) and then continuing at this same rate or slower based on the child's thirst and ongoing stool losses, keeping in mind that a severely dehydrated child may be lethargic. If the child drinks poorly, a nasogastric tube should be used. The IV route should not be used for rehydration except in cases of shock and then only with care, infusing slowly to avoid flooding the circulation and overloading the heart.\n\nFeeding should usually resume within 2–3 hours after starting rehydration and should continue every 2–3 hours, day and night. For an initial cereal diet before a child regains his or her full appetite, WHO recommends combining 25 grams skimmed milk powder, 20 grams vegetable oil, 60 grams sugar, and 60 grams rice powder or other cereal into 1,000 milliliters water and boiling gently for five minutes. Give 130 ml per kilogram of body weight during per 24 hours. A child who cannot or will not eat this minimum amount should be given the diet by nasogastric tube divided into six equal feedings. Later on, the child should be given cereal made with a greater amount of skimmed milk product and vegetable oil and slightly less sugar. As appetite fully returns, the child should be eating 200 ml per kilogram of body weight per day. Zinc, potassium, vitamin A, and other vitamins and minerals should be added to both recommended cereal products, or to the oral rehydration solution itself. Children who are breastfed should continue breastfeeding.\n\nWHO recommends that all severely malnourished children admitted to hospital should receive broad-spectrum antibiotics (for example, gentamicin and ampicillin). In addition, hospitalized children should be checked daily for other specific infections.\n\nIf cholera is suspected give an antibiotic to which \"V. cholera\"e are susceptible. This reduces the volume loss due to diarrhea by 50% and shortens the duration of diarrhea to about 48 hours.\n\nFluid from the body enters the intestinal lumen during digestion. This fluid is isosmotic with the blood and contains a high quantity, about 142 mEq/L, of sodium. A healthy individual secretes 2000–3000 milligrams of sodium per day into the intestinal lumen. Nearly all of this is reabsorbed so that sodium levels in the body remain constant. In a diarrheal illness, sodium-rich intestinal secretions are lost before they can be reabsorbed. This can lead to life-threatening dehydration or electrolyte imbalances within hours when fluid loss is severe. The objective of therapy is the replenishment of sodium and water losses by ORT or intravenous infusion.\n\nSodium absorption occurs in two stages. The first is via intestinal epithelial cells (enterocytes). Sodium passes into these cells by co-transport with glucose, via the SGLT1 protein. From the intestinal epithelial cells, sodium is pumped by active transport via the sodium-potassium pump through the basolateral cell membrane into the extracellular space.\n\nThe sodium–potassium ATPase pump at the basolateral cell membrane moves three sodium ions into the extracellular space, while pulling into the enterocyte two potassium ions. This creates a \"downhill\" sodium gradient within the cell. SGLT proteins use energy from this downhill sodium gradient to transport glucose across the apical membrane of the cell against the glucose gradient. The co-transporters are examples of secondary active transport. The GLUT uniporters then transport glucose across the basolateral membrane. Both SGLT1 and SGLT2 are known as symporters, since both sodium and glucose are transported in the same direction across the membrane.\n\nThe co-transport of glucose into epithelial cells via the SGLT1 protein requires sodium. Two sodium ions and one molecule of glucose (or galactose) are transported together across the cell membrane via the SGLT1 protein. Without glucose, intestinal sodium is not absorbed. This is why oral rehydration salts include both sodium and glucose. For each cycle of the transport, hundreds of water molecules move into the epithelial cell to maintain osmotic equilibrium. The resultant absorption of sodium and water can achieve rehydration even while diarrhea continues.\n\nIn the early 1980s, \"oral rehydration therapy\" referred only to the preparation prescribed by the World Health Organization (WHO) and UNICEF. In 1988, the definition changed to encompass recommended home-made solutions, because the official preparation was not always readily available. The definition was again amended in 1988 to include continued feeding as an appropriate associated therapy. In 1991, the definition became, \"an increase in administered hydrational fluids\" and in 1993, \"an increase in administered fluids and continued feeding\".\n\nUntil 1960, ORT was not known in the West. Dehydration was a major cause of death during the 1829 cholera pandemic in Russia and Western Europe. In 1831, William Brooke O'Shaughnessy noted the loss of water and salt in the stool of people with cholera and prescribed intravenous fluid therapy (IV fluids). The prescribing of hypertonic IV therapy decreased the mortality rate of cholera from 70 to 40 percent. In the West, IV therapy became the \"gold standard\" for the treatment of moderate and severe dehydration.\n\nIn 1957 Indian physician Hemendra Nath Chatterjee published his results of treating people with cholera with ORT. However, he had not performed a controlled trial. Robert A. Phillips attempted to create an effective ORT solution based on his discovery that, in the presence of glucose, sodium and chloride could be absorbed in patients with cholera. However, Phillips' efforts failed because the solution he used was excessively hypertonic.\n\nIn the early 1960s, biochemist Robert K. Crane described the sodium-glucose co-transport mechanism and its role in intestinal glucose absorption. This, combined with evidence that the intestinal mucosa appears undamaged in cholera, suggested that intestinal absorption of glucose and sodium might continue during the illness. This supported the notion that oral rehydration might be possible even during severe diarrhea due to cholera. In 1967-1968, Norbert Hirschhorn and Nathaniel F. Pierce, working in Dhaka, Bangladesh and Calcutta, India, respectively, showed that people with severe cholera can absorb glucose, salt and water and that this can occur in sufficient amounts to maintain hydration. In 1968, David R. Nalin reported that in adults with cholera, given an oral glucose-electrolyte solution in volumes equal to that of the diarrhea losses, reduced the need for IV fluid therapy by eighty percent.\n\nIn 1971, fighting during the Bangladesh Liberation War displaced millions and an epidemic of cholera ensued among the refugees. When IV fluid ran out in the refugee camps, Dilip Mahalanabis, a physician working with the Johns Hopkins International Center for Medical Research and Training in Calcutta, instructed his staff to prepare and distribute an oral rehydration solution prepared from individual ingredients to family members and caregivers. Over 3,000 people with cholera received ORT in this way. The mortality rate was 3.6 percent among those given ORT compared with 30 percent in those given IV fluid therapy.\n\nIn the early 1970s, Norbert Hirschhorn used oral rehydration therapy on the White River Apache Indian Reservation with a grant from the National Institute of Allergy and Infectious Diseases. He made the important observation that children would voluntarily drink as much of the solution as needed to restore hydration, and that rehydration and early re-feeding would protect their nutrition. This led to increased use of ORT for children with diarrhea, especially in developing countries.\n\nIn 1980 the Bangladeshi nonprofit BRAC created a door-to-door and person-to-person sales force to teach ORT for use by mothers at home. A task force of fourteen women, one cook and one male supervisor traveled from village to village. After visiting with women in several villages, they hit upon the idea of encouraging the women in the village to make their own oral rehydration fluid. They used available household equipment, starting with a \"half a seer\" (half a quart) of water and adding a fistful of sugar and a three-finger pinch of salt. Later on, the approach was broadcast over television and radio and a market for oral rehydration salts packets developed. Three decades later, national surveys have found that almost 90% of children with severe diarrhea in Bangladesh are given oral rehydration fluids at home or in a health facility.\n\nFrom 2006 to 2011, UNICEF estimated that worldwide about a third of children under 5 who had diarrhea received an oral rehydration solution, with estimates ranging from 30% to 41% depending on the region.\n\nORT is one of the principal elements of the UNICEF \"GOBI FFF\" program (growth monitoring; ORT; breast feeding; immunization; female education; family spacing and food supplementation). The program aims to increase child survival in developing nations through proven low-cost interventions.\n\nPeople had fled from civil war in Mozambique to southern Malawi. In November 1990, cholera broke out in a refugee camp in southern Malawi where approximately 74,000 persons were temporarily living. David Swerdlow of the U.S.'s CDC Epidemic Intelligence Service (EIS) wrote about the situation. He recommended setting up a tent just for children who would be assisted by some of the best nurses. He recommended against over-reliance on IV tubes, which often were left attached to persons for a week or more and which could lead to infections and septic shock. And he noticed that sick persons were not drinking enough rehydration solution. He assigned so-called \"ORS officers\" whose job was to encourage persons to drink more solution.\nIt was a minor mystery how persons were getting sick since deep-bore wells provided clean water and refugees were encouraged to wash their hands. It was then discovered that the only place for persons to wash hands were in the same buckets in which water was transported and stored. Swerdlow wrote in his report, \"Use of narrow mouthed water containers would probably decrease the likelihood of contamination.\"\n\n\nSources\n\n"}
{"id": "26952333", "url": "https://en.wikipedia.org/wiki?curid=26952333", "title": "Paritarian Institutions", "text": "Paritarian Institutions\n\nParitarian (from the French “paritaire”; ”paritair” in Dutch, “paritätische“ in German, “Paritetico” in Italian) means jointly managed on an equal basis (parity basis).\n\nIn the field of social protection, paritarian institutions are non-profit institutions which are jointly managed by the social partners (representatives of the employers and employees). In other words, the governance of these institutions is based on the equal representation of employees (normally the trade unions) and employers in their governing bodies.\n\nThe social protection funds managed by the paritarian institutions are set up through collective agreements at the company, the industry-wide (such as construction sector, metal sector, etc.) or the inter-sectoral level, and they can provide several social benefits such as pension (in particular occupational pension funds), health care, unemployment, disability, paid holidays and other such benefits.\n\nWithin the paritarian model there are two phases: in the negotiation phase, when a collective agreement between the trade unions and the employers’ representatives set up the social fund; and in the management phase, the signatory parties decide to manage their negotiated social funds themselves by establishing a Paritarian Institution in which they are equally represented.\n\nParitarian Institutions of Social Protection are widespread in Europe, especially in Western Europe and Scandinavia. The combined funds currently managed by the Paritarian Institutions of Social Protection total to a rough estimate of 1.3 trillion euro in assets and cover about 80 million European citizens.\n\nIn 1996, a European Organization the European Association of Paritarian Institutions of Social Protection (AEIP) was created in order to represent the Paritarian Institutions to the European Union. AEIP underlines the specific peculiarities of the Paritarian Institutions compared to other similar actors like private insurance companies or mutual organizations.\n\nEven though of European origins, paritarian institutions also exist in other parts of the world like in North America, South America, India and Japan.\n\n"}
{"id": "51383072", "url": "https://en.wikipedia.org/wiki?curid=51383072", "title": "Paul Peterson (golfer)", "text": "Paul Peterson (golfer)\n\nPaul Peterson (born July 1, 1988) is an American professional golfer, currently playing on the European Tour and the Asian Tour.\n\nPeterson played on the Canadian Tour in 2012, making only two cuts in six events. In February 2014, Peterson was successful at the Asian Tour Qualifying School. In 2015, he finished in the top 10 of two joint events with the European Tour. This gave him 138th place in the European Tour rankings for 2015, earning him conditional status for 2016.\n\nOn August 21, 2016, Peterson won the D+D Real Czech Masters for his first European Tour victory. He was ranked 398th in the world prior to the victory. In January 2018, Peterson won the Myanmar Open, co-sanctioned by the Asian Tour and the Japan Golf Tour, by two strokes, which moved him up to 127th in the Official World Golf Ranking.\n\n Co-sanctioned with the Japan Golf Tour\n\n\n"}
{"id": "26999670", "url": "https://en.wikipedia.org/wiki?curid=26999670", "title": "Postbiological evolution", "text": "Postbiological evolution\n\nPostbiological evolution is a form of evolution which has transitioned from a biological paradigm, driven by the propagation of genes, to a nonbiological (e.g., cultural or technological) paradigm, presumably driven by some alternative replicator (e.g., memes or temes), and potentially resulting in the extinction, obsolescence, or trophic reorganization of the former. Researchers anticipating a postbiological universe tend to describe this transition as marked by the maturation and potential convergence of high technologies, such as artificial intelligence or nanotechnology.\n\nThe dictionary definition of Evolution is any process of formation, growth or development. In biological evolution the main principle behind this development is survival, we evolved to become stronger and quicker, we also evolved to become intelligent. But as we became intelligent biological evolution subsided to a new concept, cultural evolution. Cultural evolution moves at a much faster rate than biological evolution and this is one reason why it isn't very well understood. But as survival is still the main driving force behind life and that intelligence and knowledge is currently the most important factor for that survival, we can reasonably assume that cultural evolution will progress in the direction of furthering intelligence and knowledge.\n\nCultural evolution progressing in this way and being based upon the furthering of intelligence is known as the Intelligence Principle; this was suggested by Dr Steven J Dick.\n\n\"The maintenance, improvement and perpetuation of knowledge and intelligence is the central driving force of cultural evolution, and that to the extent intelligence can be improved, it will be improved\" (Dick 1996)\n\nIf cultural evolution progresses in this direction then due to cultural evolution being much faster than biological, the limiting factor becomes our biology and the capability of our brains. Currently the closest and so most probable solution to this problem is artificial intelligence, (AI). Experts in AI even believe it holds the potential and capability for a postbiological earth in the next several generations, (Moravec 1988, 1999). AI could be utilised to solve scientific problems and to analyse situations much faster and more accurately than our own minds.\n\nThe move to a complete postbiological stage has two different routes. One route is the change of human consciousness from a biological vessel into a mechanical; this would require the digitisation of human consciousness. A mechanical based vessel would increase the computational power and intelligence of the human consciousness exponentially, and also eliminate the weakness of a biological form. This route is therefore a logical progression through cultural evolution with survival and the pursuit of knowledge and intelligence at its centre.\n\nThe first route requires a high level of technology, therefore would take a long time, this results in another possible road to a completely postbiological civilisation (PBC). The other route is the complete replacement of human consciousness by AI, for this the human race would co-exist peacefully with our own creation of AI which is scientific, objective, and free from selfish human nature. \n\nThe future of the human race through cultural evolution is not known and the possible postbiological outcomes are infinite, so to address what we could evolve into is almost futile. But Hans Moravec predicted that;\n\n\"What awaits us is not oblivion but rather a future which, from our present vantage point, is best described as 'postbiological' or even 'supernatural'. It is a world swept away by the tide of cultural change, usurped by its own artificial progeny\"\n\nThe possible forms a PBC may take are as diverse as in biological evolution, if not more. But from our knowledge of technology and with the intelligence principle being the main driving force we may make some predictions.\n\nThe current major limitations imposed upon computation are limited storage space, processing power, dust gathering chips, inefficiency of their human operators and heat dispersion. The only one that is fundamental and fixed is heat dispersion because this is due to the laws of physics. In computation the greater the amount of information to be calculated, (I) the greater the energy needed (E), but the energy needed is also proportional to another factor, the temperature, (T).\n\nE=KIT\n\nWhere K is a constant. Therefore, the greater the temperature the greater the energy needed, and so the greater the inefficiency is also. If we now apply the Intelligence principle to this then a PBC would move to decrease the temperature and so increase the efficiency and computational power.\nIn the universe the greatest source of heat transfer is via radiation, therefore a PBC would look to migrate to an area of low radiation and so low temperature. If we now observe the galaxy we see that the most radiation is generated by the galactic centre by both the high stellar density and also highly energetic events such as supernova. Therefore, the coldest regions are away from the galactic centre or inside giant molecular clouds. Giant molecular clouds although being very low in temperature (T~10K) are areas of giant star formation and so the temperature in one location is irregular, which would make it unsuitable for a PBC.\n\nAnother factor affecting a PBC would be the abundance of metals and heavier elements needed for expansion and repair. The highest concentration of these elements is found near the galactic centre, where they are created by massive stars. But to a PBC with advanced technology the production of metals via stellar nucleosynthesis in stars is highly inefficient, converting only a small amount of hydrogen to heavier nuclei and the high loss of energy that is produced in the nuclear fusion. Therefore, a PBC would most likely have the capability to produce heavier nuclei through controlled fusion and minimise the energy lost.\n\nBy taking the two factors of heat dispersion and heavy nuclei into account we can find a \"galactic technological zone\" (GTZ), similar to the principle of a \"galactic habitable zone\" (GHZ) for biological life. Where temperatures are low enough to maximise computing efficiency but there is also matter available for fusion, this most likely lies on the outskirts of the galaxy.\n\nA migration hypothesis exists that takes the GTZ into account. A PBC would most likely not think on a similar time scale to us, therefore although a migration to GTZ may seem inefficient and lengthy to us, a PBC could consider this on timescales of 10^6 years, where the increased computing efficiency received far outweighs the energy required in transportation. The idea of interstellar migrations already exists in literature, (e.g. Badescu and Cathcart 2000).\n\nIn the search for extraterrestrial intelligence (SETI) the main focus is on biological life. But the timescale of intelligent biological life could be very short; already some experts believe that we could see a postbiological earth in the next few generations. According to Steven J. Dick, for a PBC to arise other than our own and be present, we must make five assumptions:\n\n\nWe know that assumptions 1, 3, 4, and 5 can take place as we have observed or are observing them on the Earth. For assumption 2 we must consider the L term of the Drake equation, and the timescale over which intelligent biological life can form. Around 1 Billion years after the start of the universe the first sun-like star had formed, and there were enough heavy elements around for planet formation (1998, Larson and Bromm 2001). From the earth we know that intelligent life can form within 5 billion years, this puts a lower time scale on which intelligent life can form, 6 billion years. And from the current rate of technological progression the leap from intelligent life to a PBC is negligible compared to the astronomical timescale. This means we could already be looking at a postbiological universe. In our own galaxy the first sun-like stars formed at around 4 billion years therefore we could already have a PBC in our galaxy that formed 3-4 billion years ago.\n\nIf we consider this possibility of a PBC in our galaxy we are still faced with Fermi's paradox. However many of the proposed solutions for Fermi's paradox also hold true for a PBC. In terms of the search for extraterrestrial life and astrobiology because of the almost infinite possible forms a PBC could take and our lack of understanding of these we would effectively be blind in this search. For this reason even though there is a logical argument for the existence of PBCs our best hopes remain with looking for biological life.\n\nWhile in some circles the expression \"postbiological evolution\" is roughly synonymous with human genetic engineering, it is used most often to refer to the general application of the convergence of nanotechnology, biotechnology, information technology, and cognitive science (NBIC) to improve human performance.\n\nSince the 1990s, several academics (such as some of the fellows of the Institute for Ethics and Emerging Technologies) have risen to become cogent advocates of the case for human enhancement while other academics (such as the members of President Bush's Council on Bioethics) have become its most outspoken critics.\n\nAdvocacy of the case for human enhancement is increasingly becoming synonymous with \"transhumanism\", a controversial ideology and movement which has emerged to support the recognition and protection of the right of citizens to either maintain or modify their own minds and bodies; so as to guarantee them the freedom of choice and informed consent of using human enhancement technologies on themselves and their children.\n\nNeuromarketing consultant Zack Lynch argues that neurotechnologies will have a more immediate effect on society than gene therapy and will face less resistance as a pathway of radical human enhancement. He also argues that the concept of \"enablement\" needs to be added to the debate over \"therapy\" versus \"enhancement\".\n\nAlthough many proposals of human enhancement rely on fringe science, the very notion and prospect of human enhancement has sparked public controversy.\n\nMany critics argue that \"human enhancement\" is a loaded term which has eugenic overtones because it may imply the improvement of human hereditary traits to attain a universally accepted norm of biological fitness (at the possible expense of human biodiversity and neurodiversity), and therefore can evoke negative reactions far beyond the specific meaning of the term. Furthermore, they conclude that enhancements which are self-evidently good, like \"fewer diseases\", are more the exception than the norm and even these may involve ethical tradeoffs, as the controversy about ADHD arguably demonstrates.\n\nHowever, the most common criticism of human enhancement is that it is or will often be practiced with a reckless and selfish short-term perspective that is ignorant of the long-term consequences on individuals and the rest of society, such as the fear that some enhancements will create unfair physical or mental advantages to those who can and will use them, or unequal access to such enhancements can and will further the gulf between the \"haves\" and \"have-nots\".\n\nAccordingly, some advocates, who want to use more neutral language, and advance the public interest in so-called \"human enhancement technologies\", prefer the term \"enablement\" over \"enhancement\"; defend and promote rigorous, independent safety testing of enabling technologies; as well as affordable, universal access to these technologies.\n\n\n\n"}
{"id": "4566442", "url": "https://en.wikipedia.org/wiki?curid=4566442", "title": "Proceptive phase", "text": "Proceptive phase\n\nIn biology and sexology, the proceptive phase is the initial period in a relationship when organisms are \"courting\" each other, prior to the acceptive phase when copulation occurs. Behaviors that occur during the proceptive phase depend very much on the species, but may include visual displays, movements, sounds and odors.\n\nThe term \"proceptivity\" was introduced into general sexological use by Frank A. Beach in 1976 and refers to behavior enacted by a female to initiate, maintain, or escalate a sexual interaction. There are large species differences in proceptive behavior. The term has also been used to describe women’s roles in human courtship, with a meaning very close to Beach’s. A near synonym is \"proception.\"\n\nThe term \"proceptive phase\" refers to pre-consummatory, that is, pre-ejaculatory, behavior and focuses attention on the active role played by the female organism in creating, maintaining, and escalating the sexual interaction.\n\n\n"}
{"id": "146764", "url": "https://en.wikipedia.org/wiki?curid=146764", "title": "Punishment", "text": "Punishment\n\nA punishment is the imposition of an undesirable or unpleasant outcome upon a group or individual, meted out by an authority—in contexts ranging from child discipline to criminal law—as a response and deterrent to a particular action or behaviour that is deemed undesirable or unacceptable. The reasoning may be to condition a child to avoid self-endangerment, to impose social conformity (in particular, in the contexts of compulsory education or military discipline), to defend norms, to protect against future harms (in particular, those from violent crime), and to maintain the law—and respect for rule of law—under which the social group is governed. Punishment may be self-inflicted as with self-flagellation and mortification of the flesh in the religious setting, but is most often a form of social coercion. \n\nThe unpleasant imposition may include a fine, penalty, or confinement, or be the removal or denial of something pleasant or desirable. The individual may be a person, or even an animal. The authority may be either a group or a single person, and punishment may be carried out formally under a system of law or informally in other kinds of social settings such as within a family. Negative consequences that are not authorized or that are administered without a breach of rules are not considered to be punishment as defined here. The study and practice of the punishment of crimes, particularly as it applies to imprisonment, is called penology, or, often in modern texts, corrections; in this context, the punishment process is euphemistically called \"correctional process\". Research into punishment often includes similar research into prevention.\n\nJustifications for punishment include retribution, deterrence, rehabilitation, and incapacitation. The last could include such measures as isolation, in order to prevent the wrongdoer's having contact with potential victims, or the removal of a hand in order to make theft more difficult. Of the four justifications, only retribution is part of the definition of punishment and none of the other justifications is a guaranteed outcome, aside from obvious exceptions such as an executed man being incapacitated with regard to further crimes.\n\nIf only some of the conditions included in the definition of punishment are present, descriptions other than \"punishment\" may be considered more accurate. Inflicting something negative, or unpleasant, on a person or animal, without authority is considered revenge or spite rather than punishment. In addition, the word \"punishment\" is used as a metaphor, as when a boxer experiences \"\"punishment\" during a fight. In other situations, breaking a rule may be rewarded, and so receiving such a reward naturally does not constitute punishment. Finally the condition of breaking (or breaching) the rules must be satisfied for consequences to be considered punishment.\n\nPunishments differ in their degree of severity, and may include sanctions such as reprimands, deprivations of privileges or liberty, fines, incarcerations, ostracism, the infliction of pain, amputation and the death penalty.\n\"Corporal punishment\" refers to punishments in which physical pain is intended to be inflicted upon the transgressor.\nPunishments may be judged as fair or unfair in terms of their degree of reciprocity and proportionality to the offense.\nPunishment can be an integral part of socialization, and punishing unwanted behaviour is often part of a system of pedagogy or behavioral modification which also includes rewards.\n\nVarious philosophers have presented definitions of punishment. Conditions commonly considered necessary properly to describe an action as punishment are that \n\nIntroduced by B.F. Skinner, punishment has a more restrictive and technical definition. Along with reinforcement it belongs under the operant conditioning category. Operant conditioning refers to learning with either punishment (often confused as negative reinforcement) or a reward that serves as a positive reinforcement of the lesson to be learned. In psychology, punishment is the reduction of a behavior via application of an unpleasant stimulus (\"positive\" punishment\") or removal of a pleasant stimulus (\"\"negative\" punishment\"). Extra chores or spanking are examples of positive punishment, while removing an offending student's recess or play privileges are examples of negative punishment. The definition requires that punishment is only determined after the fact by the reduction in behavior; if the offending behavior of the subject does not decrease, it is not considered punishment. There is some conflation of punishment and aversives, though an aversion that does not decrease behavior is not considered punishment in psychology. Additionally, \"aversive stimulus\" is a label behaviorists generally apply to negative reinforcers (as in avoidance learning), rather than punishers.\n\nPunishment is sometimes called \"retaliatory\" or \"moralistic aggression\"; it has been observed in all species of social animals, leading evolutionary biologists to conclude that it is an evolutionarily stable strategy, selected because it favors cooperative behavior.\n\nOne criticism of the claim of all social animals being evolutionarily hardwired for punishment comes from studies of animals, such as the octopuses near Capri, Italy that suddenly formed communal cultures from having, until then lived solitary lives. During a period of heavy fishing and tourism that encroached on their territory, they started to live in groups, learning from each other, especially hunting techniques. Small, younger octopuses could be near the fully grown octopuses without being eaten by them, even though they, like other Octopus vulgaris, were cannibals until just before the group formation. The authors stress that this behavior change happened too fast to be a genetic characteristic in the octopuses, and that there were certainly no mammals or other \"naturally\" social animals punishing octopuses for cannibalism involved. The authors also note that the octopuses adopted observational learning without any evolutionary history of specialized adaptation for it.\n\nThere are also arguments against the notion of punishment requiring intelligence, based on studies of punishment in very small-brained animals such as insects. There is proof of honey bee workers with mutations that makes them fertile laying eggs only when other honey bees are not observing them, and that the few that are caught in the act are killed. This is corroborated by computer simulations proving that a few simple reactions well within mainstream views of the extremely limited intelligence of insects are sufficient to emulate the \"political\" behavior observed in great apes. The authors argue that this falsifies the claim that punishment evolved as a strategy to deal with individuals capable of knowing what they are doing.\n\nIn the case of more complex brains, the notion of evolution selecting for specific punishment of intentionally chosen breaches of rules and/or wrongdoers capable of intentional choices (for example, punishing humans for murder while not punishing lethal viruses) is subject to criticism from coevolution issues. That punishment of individuals with certain characteristics (including but, in principle, not restricted to mental abilities) selects against those characteristics, making evolution of any mental abilities considered to be the basis for penal responsibility impossible in populations subject to such selective punishment. Certain scientists argue that this disproves the notion of humans having a biological feeling of intentional transgressions deserving to be punished.\n\nPunishments are applied for various purposes, most generally, to encourage and enforce proper behavior as defined by society or family. Criminals are punished judicially, by fines, corporal punishment or custodial sentences such as prison; detainees risk further punishments for breaches of internal rules. Children, pupils and other trainees may be punished by their educators or instructors (mainly parents, guardians, or teachers, tutors and coaches) – see Child discipline.\n\nSlaves, domestic and other servants are subject to punishment by their masters. Employees can still be subject to a contractual form of fine or demotion. Most hierarchical organizations, such as military and police forces, or even churches, still apply quite rigid internal discipline, even with a judicial system of their own (court martial, canonical courts).\n\nPunishment may also be applied on moral, especially religious, grounds, as in penance (which is voluntary) or imposed in a theocracy with a religious police (as in a strict Islamic state like Iran or under the Taliban) or (though not a true theocracy) by Inquisition.\n\nBelief that an individual's ultimate punishment is being sent by God, the highest authority, to an existence in Hell, a place believed to exist in the after-life, typically corresponds to sins committed during their life. Sometimes these distinctions are specific, with damned souls suffering for each sin committed (see for example Plato's myth of Er or Dante's \"The Divine Comedy\"), but sometimes they are general, with condemned sinners relegated to one or more chamber of Hell or to a level of suffering.\n\nIn many religious cultures, including Christianity and Islam, Hell is traditionally depicted as fiery and painful, inflicting guilt and suffering. Despite these common depictions of Hell as a place of fire, some other traditions portray Hell as cold. Buddhist – and particularly Tibetan Buddhist – descriptions of hell feature an equal number of hot and cold hells. Among Christian descriptions Dante's \"Inferno\" portrays the innermost (9th) circle of Hell as a frozen lake of blood and guilt.\nBut cold also played a part in earlier Christian depictions of hell, beginning with the Apocalypse of Paul, originally from the early third century; the \"Vision of Dryhthelm\" by the Venerable Bede from the seventh century; \"St Patrick's Purgatory\", \"The Vision of Tundale\" or \"Visio Tnugdali\", and the \"Vision of the Monk of Enysham\", all from the twelfth century;\nand the \"Vision of Thurkill\" from the early thirteenth century.\n\nA principle often mentioned with respect to the degree of punishment to be meted out is that the punishment should match the crime.\nOne standard for measurement is the degree to which a crime affects others or society. Measurements of the degree of seriousness of a crime have been developed.\nA felony is generally considered to be a crime of \"high seriousness\", while a misdemeanor is not.\n\nThere are many possible reasons that might be given to justify or explain why someone ought to be punished; here follows a broad outline of typical, possibly conflicting, justifications.\n\nOne reason given to justify punishment is that it is a measure to prevent people from committing an offence - deterring previous offenders from re-offending, and preventing those who may be contemplating an offence they have not committed from actually committing it. This punishment is intended to be sufficient that people would choose not to commit the crime rather than experience the punishment. The aim is to deter everyone in the community from committing offences.\n\nSome criminologists state that the number of people convicted for crime does not decrease as a result of more severe punishment and conclude that deterrence is ineffective. Other criminologists object to said conclusion, citing that while most people do not know the exact severity of punishment such as whether the sentence for murder is 40 years or life, most people still know the rough outlines such as the punishments for armed robbery or statutory rape being more severe than the punishments for driving too fast or misparking a car. These criminologists therefore argue that lack of deterring effect of increasing the sentences for already severely punished crimes say nothing about the significance of the existence of punishment as a deterring factor.\n\nSome criminologists argue that increasing the sentences for crimes can cause criminal investigators to give higher priority to said crimes so that a higher percentage of those committing them are convicted for them, causing statistics to give a false appearance of such crimes increasing. These criminologists argue that the use of statistics to gauge the efficiency of crime fighting methods are a danger of creating a reward hack that makes the least efficient criminal justice systems appear to be best at fighting crime, and that the appearance of deterrance being ineffective may be an example of this.\n\nSome punishment includes work to reform and rehabilitate the culprit so that they will not commit the offence again. This is distinguished from deterrence, in that the goal here is to change the offender's attitude to what they have done, and make them come to see that their behavior was wrong.\n\nIncapacitation as a justification of punishment refers to the offender’s ability to commit further offences being removed. Imprisonment separates offenders from the community, removing or reducing their ability to carry out certain crimes. The death penalty does this in a permanent (and irrevocable) way. In some societies, people who stole have been punished by having their hands amputated.\n\nCriminal activities typically give a benefit to the offender and a loss to the victim. Punishment has been justified as a measure of retributive justice, in which the goal is to try to rebalance any unjust advantage gained by ensuring that the offender also suffers a loss. Sometimes viewed as a way of \"getting even\" with a wrongdoer—the suffering of the wrongdoer is seen as a desired goal in itself, even if it has no restorative benefits for the victim. One reason societies have administered punishments is to diminish the perceived need for retaliatory \"street justice\", blood feud, and vigilantism.\n\nFor minor offenses, punishment may take the form of the offender \"righting the wrong\", or making restitution to the victim. Community service or compensation orders are examples of this sort of penalty. In models of restorative justice, victims take an \"active\" role in a process with their offenders who are encouraged to take responsibility for their actions, \"to repair the harm they've done – by apologizing, returning stolen money, or community service.\" The restorative justice approach aims to help the offender want to avoid future offences.\n\nPunishment can be explained by positive prevention theory to use the criminal justice system to teach people what are the social norms for what is correct, and acts as a reinforcement.\n\nPunishment can serve as a means for society to publicly express denunciation of an action as being criminal. Besides educating people regarding what is not acceptable behavior, it serves the dual function of preventing vigilante justice by acknowledging public anger, while concurrently deterring future criminal activity by stigmatizing the offender.\nThis is sometimes called the \"Expressive Theory\" of denunciation.\nThe pillory was a method for carrying out public denunciation.\n\nSome critics of the education and denunciation model cite evolutionary problems with the notion that a feeling for punishment as a social signal system evolved if punishment was not effective. The critics argue that some individuals spending time and energy and taking risks in punishing others, and the possible loss of the punished group members, would have been selected against if punishment served no function other than signals that could evolve to work by less risky means.\n\nA unified theory of punishment brings together multiple penal purposes — such as retribution, deterrence and rehabilitation — in a single, coherent framework. Instead of punishment requiring we choose between them, unified theorists argue that they work together as part of some wider goal such as the protection of rights.\n\nSome people think that punishment as a whole is unhelpful and even harmful to the people that it is used against. Detractors argue that punishment is simply wrong, of the same design as \"two wrongs make a right\". Critics argue that punishment is simply revenge. Professor Deirdre Golash, author of the book, \"The Case against Punishment: Retribution, Crime Prevention, and the Law\", states in her book that,\n\nThere are critics of punishment who argue that punishment aimed at intentional actions forces people to suppress their ability to act on intent. Advocates of this viewpoint argue that such suppression of intention causes the harmful behaviors to remain, making punishment counterproductive. These people suggest that the ability to make intentional choices should instead be treasured as a source of possibilities of betterment, citing that complex cognition would have been an evolutionarily useless waste of energy if it led to justifications of fixed actions and no change as simple inability to understand arguments would have been the most thrifty protection from being misled by them if arguments were for social manipulation, and reject condemnation of people who intentionally did bad things. Punishment can be effective in stopping undesirable employee behaviors such as tardiness, absenteeism or substandard work performance. However, punishment does not necessarily cause an employee to demonstrate a desirable behavior.\n\n\n\n"}
{"id": "26077688", "url": "https://en.wikipedia.org/wiki?curid=26077688", "title": "Reginald Earnshaw", "text": "Reginald Earnshaw\n\nReginald Hamilton Earnshaw (5 February 1927 – 6 July 1941), known as Reggie Earnshaw, is believed to have been the youngest person in the British services to die in World War II. He was just old when he died under enemy fire on the off the coast of Norfolk on 6 July 1941.\n\nEarnshaw was born in Dewsbury, West Yorkshire, to Dorothy Earnshaw. She then married Eric Shires, and the couple had two daughters. Aged 12, he moved with the family to the Granton area of Edinburgh, attending Bellevue School. He left aged 14, and joined the Merchant Navy in February 1941, giving his birth year as 1926 rather than 1927 in order to appear 15, which was the minimum age for recruitment.\nEarnshaw's body was recovered after the attack and his death certificate, based partly on the false information he had supplied on recruitment, gave his age only as \"about 15 years\". Buried originally in an unmarked grave (section P, grave 440) at Comely Bank Cemetery, Edinburgh, his story and his true age came to light after a shipmate, Alfred Tubb started a search for his burial place. In 2009, the grave was marked by the Commonwealth War Graves Commission with a granite headstone.\n\n"}
{"id": "53245089", "url": "https://en.wikipedia.org/wiki?curid=53245089", "title": "SS John A. McGean", "text": "SS John A. McGean\n\n\"John A. McGean\" was built in 1908 by the American Shipbuilding Company at their shipyard in Lorain, Ohio. She was long, with a beam of and a draft of , and measured 5,100 gross tons. \n\nOn November 7, 1913, \"John A. McGean\" was sailing into the Great Storm when she was sighted for the final time off Tawas Point Light. Sometime the following day, she sank with all 23 crew. Her wreckage was not found until 1985, when it was discovered near Port Hope, Michigan with damage indicating that she had been swamped by a large wave. Portions of the wreckage were found by a local doctor along the shoreline at Bayfield, Ontario in mid-November 1913. \n\nThe body of chief engineer Calvin Smith was found near Black's Point, Ontario (just south of Goderich, Ontario) in late November 1913. Second cook D.M. Betts' remains were identified at the morgue in Goderich, Ontario via a photograph and details furnished by the Lake Carriers' Association. His remains were sent home to Girard, Pennsylvania on November 20, 1913. \n"}
{"id": "6873934", "url": "https://en.wikipedia.org/wiki?curid=6873934", "title": "Steve Irwin", "text": "Steve Irwin\n\nStephen Robert Irwin (22 February 1962 – 4 September 2006), nicknamed \"The Crocodile Hunter\" was an Australian zookeeper, conservationist, and television personality. Irwin achieved worldwide fame from the television series \"The Crocodile Hunter\" (1996–2007), an internationally broadcast wildlife documentary series which he co-hosted with his wife Terri; the couple also hosted the series, \"Croc Files\" (1999–2001), \"The Crocodile Hunter Diaries\" (2002–2006), and \"New Breed Vets\" (2005). They also owned and operated Australia Zoo, founded by Irwin's parents in Beerwah, about north of the Queensland state capital city of Brisbane.\n\nIrwin died at 44, after being pierced in the heart by a stingray barb while filming an underwater documentary film titled \"Ocean's Deadliest\".\n\nIrwin was born on his mother's birthday to Lyn and Bob Irwin in Essendon, a suburb of Melbourne, Victoria. He was of Irish descent on his father's side. He moved with his parents as a child to Queensland in 1970, where he attended Landsborough State School and Caloundra State High School. Irwin described his father as a wildlife expert interested in herpetology, while his mother Lyn was a wildlife rehabilitator. After moving to Queensland, Bob and Lyn Irwin started the small Queensland Reptile and Fauna Park, where Steve grew up around crocodiles and other reptiles.\n\nIrwin became involved with the park in a number of ways, including taking part in daily animal feeding, as well as care and maintenance activities. On his sixth birthday, he was given a scrub python. He began handling crocodiles at the age of nine after his father had educated him on reptiles from an early age. Also at age nine, he wrestled his first crocodile, again under his father's supervision. He worked as a volunteer for Queensland's East Coast Crocodile Management program and captured over 100 crocodiles, some of which were relocated, while others were housed at the family park. Irwin took over the management of the park in 1991 and renamed it \"Australia Zoo\" in 1998.\n\nIn 1991, Irwin met Terri Raines, an American naturalist from Eugene, Oregon, who was visiting wildlife rehabilitation facilities in Australia and had decided to visit the zoo. According to the couple, it was love at first sight. Terri said at the time, \"I thought there was no one like this anywhere in the world. He sounded like an environmental Tarzan, a larger-than-life superhero guy.\" They were engaged four months later and were married in Eugene on 4 June 1992. Together they had two children: a daughter, Bindi Sue Irwin (born 24 July 1998), and a son, Robert Clarence \"Bob\" (named after Irwin's father) Irwin (born 1 December 2003). Bindi Sue is jointly named after two of Steve Irwin's favourite animals: Bindi, a saltwater crocodile, and Sui, a Staffordshire Bull Terrier who died on 23 June 2004. Irwin was as enthusiastic about his family as he was about his work. He once described his daughter Bindi as \"the reason [he] was put on the Earth.\" His wife once said, \"The only thing that could ever keep him away from the animals he loves are the people he loves even more.\" Although the Irwins were happily married, they did not wear wedding rings; they believed that in their line of work, wearing jewellery could pose a hazard to them and/or the animals.\n\nSteve and Terri spent their honeymoon trapping crocodiles together. Film footage of their honeymoon, taken by John Stainton, became the first episode of \"The Crocodile Hunter\". The series debuted on Australian TV screens in 1996 and made its way onto North American television the following year. \"The Crocodile Hunter\" became successful in the United States, the UK, and over 130 other countries, reaching 500 million people. Irwin's exuberant and enthusiastic presenting style, broad Australian accent, signature khaki shorts, and catchphrase \"Crikey!\" became known worldwide. Sir David Attenborough praised Irwin for introducing many to the natural world, saying \"He taught them how wonderful and exciting it was, he was a born communicator.\"\n\nAmerican satellite and cable television channel Animal Planet ended \"The Crocodile Hunter\" with a series finale titled \"Steve's Last Adventure.\" The last Crocodile Hunter documentary spanned three hours with footage of Irwin's across-the-world adventure in locations including the Himalayas, the Yangtze River, Borneo, and the Kruger National Park. Irwin went on to star in other Animal Planet documentaries, including \"Croc Files\", \"The Crocodile Hunter Diaries\", and \"New Breed Vets\". During a January 2006 interview on \"The Tonight Show with Jay Leno\", Irwin announced that Discovery Kids would be developing a show for his daughter, Bindi Sue Irwin – a plan realised after his death as the series \"Bindi the Jungle Girl\".\n\nIn 1998, Irwin continued, working with director Mark Strickson, to present \"The Ten Deadliest Snakes in the World\". He appeared on several episodes of \"The Tonight Show with Jay Leno\". A 2000 FedEx commercial with Irwin lightheartedly dealt with the possibility of occupational death from snakebite and the fanciful notion that FedEx would have saved him, if only FedEx were used.\n\nUnder Irwin's leadership, the operations grew to include the zoo, the television series, the Steve Irwin Conservation Foundation (later renamed Wildlife Warriors), and the International Crocodile Rescue. Improvements to the Australia Zoo include the Animal Planet Crocoseum, the rainforest aviary and Tiger Temple. Irwin mentioned that he was considering opening an Australia Zoo in Las Vegas, Nevada, and possibly at other sites around the world.\n\nIn 2001, Irwin appeared in a cameo role in the Eddie Murphy film \"Dr. Dolittle 2\", in which an alligator warns Dolittle that he knows Irwin is going to grab him and is prepared to attack when he does, but Dolittle fails to warn Irwin in time. Irwin's only starring feature film role was in 2002's \"\", which was released to mixed reviews. In the film Irwin (who portrayed himself and performed numerous stunts) mistakes some CIA agents for poachers. He sets out to stop them from capturing a crocodile, which, unknown to him, has actually swallowed a tracking transmitter. The film won the Best Family Feature Film award for a comedy film at the Young Artist Awards. The film was produced on a budget of about US$12 million, and has grossed $33 million. To promote the film, Irwin was featured in an animated short produced by Animax Entertainment for Intermix.\n\nIn 2002, Irwin and his family appeared in the Wiggles video/DVD release \"Wiggly Safari\", which was set in Australia Zoo and featured singing and dancing inspired by Australian wildlife.\nIn 2003, Irwin fronted an advertising campaign for The Ghan, a passenger train operating between Adelaide, Alice Springs, and Darwin. A Pacific National NR class locomotive was named \"Steve Irwin\" as part of the campaign.\n\nIn 2005, Irwin provided his voice for the 2006 animated film \"Happy Feet\", as an elephant seal named Trev. The film was dedicated to Irwin, as he died during post-production. Another, previously incomplete scene, featuring Irwin providing the voice of an albatross and essentially playing himself, was restored to the DVD release.\n\nIrwin was also involved in several media campaigns. He enthusiastically joined with the Australian Quarantine and Inspection Service to promote Australia's strict quarantine/customs requirements, with advertisements and posters featuring slogans such as, \"Quarantine Matters! Don't muck with it\". His payments for these advertising campaigns were directed into his wildlife fund.\n\nIn 2004, Irwin was appointed ambassador for The Ghan, the passenger train running from Adelaide to Alice Springs in the central Australian outback, when the line was extended all the way to Darwin on the northern coast that year. For some time he was sponsored by Toyota.\n\nIrwin was a keen promoter for Australian tourism in general and Queensland tourism in particular. In 2002, the Australia Zoo was voted Queensland's top tourist attraction. His immense popularity in the United States meant he often promoted Australia as a tourist destination there. As a part of the United States' \"Australia Week\" celebrations in January 2006, Irwin appeared at UCLA's Pauley Pavilion in Los Angeles, California.\n\nIn November 2003, Irwin was filming a documentary on sea lions off the coast of Baja California Peninsula in Mexico when he heard via his boat's radio that two scuba divers were reported missing in the area. Irwin and his entire crew suspended operations to aid in the search. His team's divers searched with the rescue divers, and Irwin used his vessel to patrol the waters around the island where the incident occurred, as well as using his satellite communications system to call in a rescue plane. On the second day of the search, kayakers found one of the divers, Scott Jones, perched on a narrow rock ledge jutting out from the side of a cliff. Irwin and a crew member escorted him to Irwin's boat. Jones did not recognise Irwin. The other lost diver, Katie Vrooman, was found dead by a search plane later the same day not far from Jones' location.\n\nIn 1997, while on a fishing trip on the coast of Queensland with his father, Irwin discovered a new species of turtle. Later given the honour of naming the newly discovered species, he named it Irwin's turtle (\"Elseya irwini\") after his family. Another newly discovered Australian animal – a species of air-breathing land snail, \"Crikey steveirwini\", was named after Irwin in 2009.\n\nIn 2001, Irwin was awarded the Centenary Medal by the Australian government for his \"service to global conservation and to Australian tourism\". In 2004, he was recognised as Tourism Export of the Year. He was also nominated in 2004 for Australian of the Year but it was awarded to Australian cricket captain Steve Waugh, while Irwin was named 2004 Queensland Australian of the Year. Shortly before his death, Irwin was to be named an adjunct professor at the University of Queensland's School of Integrative Biology. On 14 November 2007, Irwin was awarded the adjunct professorship posthumously.\n\nIn May 2007, the government of Rwanda announced that it would name a baby gorilla after Irwin as a tribute to his work in wildlife conservation. Also in 2007, the state government of Kerala, India named the Crocodile Rehabilitation and Research Centre at Neyyar Wildlife Sanctuary in his honour; however, Terri objected that this action had been taken without her permission and asked the Kerala government in 2009 to stop using Irwin's name and images – a request with which the state government complied in mid-2009.\n\nIn 2009, Steve Irwin was inducted into the Queensland Business Leaders Hall of Fame, recognised for international entrepreneurship both in business and wildlife conservation, significantly contributing to Queensland and its international reputation.\n\nIn 2015, Irwin was a posthumous recipient of the Queensland Greats Awards.\n\nOn 22 June 2017, it was announced that Irwin would be posthumously honoured with a star on the Hollywood Walk of Fame. The star was unveiled 26 April 2018.\n\nIrwin was a passionate conservationist and believed in promoting environmentalism by sharing his excitement about the natural world rather than preaching to people. He was concerned with conservation of endangered animals and land clearing leading to loss of habitat. He considered conservation to be the most important part of his work: \"I consider myself a wildlife warrior. My mission is to save the world's endangered species.\" Irwin bought \"large tracts of land\" in Australia, Vanuatu, Fiji and the United States, which he described as \"like national parks\" and stressed the importance of people realising that they could each make a difference.\n\nIrwin founded the Steve Irwin Conservation Foundation, which became an independent charity and was later renamed \"Wildlife Warriors Worldwide\". He also helped found International Crocodile Rescue, the Lyn Irwin Memorial Fund (named in memory of his mother, who died in an automobile crash in 2000), and the Iron Bark Station Wildlife Rehabilitation Facility.\n\nIrwin urged people to take part in considerate tourism and not support illegal poaching through the purchase of items such as turtle shells or shark-fin soup.\n\nSir David Attenborough was an inspiration to Irwin, according to his widow. When presenting a Lifetime Achievement Award to Attenborough after Irwin's death at the British National Television Awards on 31 October 2006, Terri Irwin said, \"If there's one person who directly inspired my husband it's the person being honoured tonight... [Steve's] real, true love was conservation – and the influence of tonight's recipient in preserving the natural world has been immense.\" Attenborough reciprocated by praising Irwin for introducing many to the natural world, saying, \"He taught them how wonderful and exciting it was, he was a born communicator.\"\n\nIrwin, after his death, was described by Mark Townend, CEO of RSPCA Queensland, as a \"modern-day Noah.\" British naturalist David Bellamy lauded his skills as a natural historian and media performer. Canadian environmentalist David Suzuki paid tribute to Irwin, noting that \"[h]umanity will not protect that which we fear or do not understand. Steve Irwin helped us understand those things that many people thought were a nuisance at best, a horror at worst. That made him a great educator and conservationist.\"\n\nAfter his death, the vessel owned by the environmental action group Sea Shepherd Conservation Society was renamed . Shortly before his death, Irwin had been investigating joining Sea Shepherd's 2007–2008 voyage to Antarctica to disrupt Japanese whaling activity. Following his death, the organisation suggested renaming their vessel, and this idea was endorsed by Terri Irwin. Regarding the ship and its new name, Terri said, \"If Steve were alive, he'd be aboard with them!\"\n\nIrwin loved mixed martial arts competitions and trained with Greg Jackson in the fighting/grappling system of Gaidojutsu.\n\nLike many Australians, he was an avid cricket fan. This was seen during his visit to Sri Lanka where he played cricket with some local children and said \"I love cricket\" and \"It's a shame we have to go catch some snakes now\". This was seen during the \"Crocodile Hunter\" episode \"Island of the Snakes\".\n\nHaving grown up in Essendon, Irwin was a fan of the Essendon Bombers, an Australian rules football club in the Australian Football League. Irwin took part in an Australian Rules football promotion in Los Angeles as part of \"Australia Week\" in early 2006. After his death, a picture of Irwin wearing a Bombers Guernsey was shown by ESPN.com in their Bottom 10 ranking of the worst Division I FBS college football teams after Week 1 of the season in tribute to him.\n\nHaving lived in Queensland most of his life, Irwin was also a fan of rugby league. As a teenager, he played for the Caloundra Sharks as a second-rower, and as an adult he was known to be a passionate Brisbane Broncos fan and was involved with the club on several occasions. On one occasion after turning up to training he asked if he could tackle the largest player, Shane Webcke. Despite being thrown to the ground and looking like he'd been crushed he was jovial about the experience. Irwin laughingly shared the experience with the Queensland State of Origin squad before the 2006 series. Irwin also supported rugby union, being a fan of the national team, the Wallabies. He once wore a Wallaby jersey during a demonstration at the zoo. A behind-the-scenes episode of \"The Crocodile Hunter\" showed Irwin and the crew finding a petrol station in a remote part of Namibia to watch the Wallabies defeat France in the 1999 Rugby World Cup Final. Irwin was also a talented surfer.\n\nA controversial incident occurred during a public show on 2 January 2004, when Irwin carried his one-month-old son, Robert, in his arm while hand-feeding a chicken carcass to Murray, a saltwater crocodile. The infant was close to the crocodile, and comparisons were made in the press to Michael Jackson's dangling his son outside a German hotel window. In addition, some child welfare groups, animal rights groups, and some of Irwin's television viewers criticised his actions as irresponsible and tantamount to child abuse. Irwin apologised on the US NBC show \"Today\". Both he and his wife publicly stated that Irwin was in complete control of the situation, as he had dealt with crocodiles since he was a small child, and based on his lifetime of experience neither he nor his son was in any danger. He also showed footage of the event shot from a different angle, demonstrating that they were much farther from the crocodile than they had appeared in the publicised clip. Terri Irwin said their child was in no more danger than one being taught to swim. No charges were filed; according to one journalist, Irwin told officials he would not repeat the action. The incident prompted the Queensland government to change its crocodile-handling laws, banning children and untrained adults from entering crocodile enclosures.\n\nIn June 2004, allegations were made that he disturbed wildlife (namely whales, seals and penguins) while filming a documentary, \"Ice Breaker\", in Antarctica. The matter was subsequently closed without charges being laid.\n\nAfter questions arose in 2003 about Irwin being paid $175,000 worth of taxpayers' money to appear in a television advertisement and his possible political ties, Irwin told the Australian Broadcasting Corporation (ABC) that he was a conservationist and did not choose sides in politics. His comments describing Australian Prime Minister John Howard as the \"greatest leader in the world\" earned him scorn in the media.\n\nIrwin was criticised for having an unsophisticated view of conservation in Australia that seemed more linked to tourism than to the problems Australia faces as a continent. In response to questions of Australia's problems with overgrazing, salinity, and erosion, Irwin responded, \"Cows have been on our land for so long that Australia has evolved to handle those big animals.\" \"The Sydney Morning Herald\" concluded with the opinion that his message was confusing and amounted to \"eating roos and crocs is bad for tourism, and therefore more cruel than eating other animals\".\n\nOn 4 September 2006, Irwin was on location at Batt Reef, near Port Douglas, Queensland, taking part in the production of the documentary series \"Ocean's Deadliest\". During a lull in filming caused by inclement weather, Irwin decided to snorkel in shallow waters while being filmed in an effort to provide footage for his daughter's television programme.\n\nWhile swimming in chest-deep water, Irwin approached a short-tail stingray with an approximate span of two metres (6.5 ft) from the rear, in order to film it swimming away.\n\nAccording to the incident's only witness, “All of a sudden [the stingray] propped on its front and started stabbing wildly with its tail. Hundreds of strikes in a few seconds”. Irwin initially believed he only had a punctured lung. However, the stingray's barb pierced his heart, causing him to bleed to death. The stingray's behaviour appeared to have been a defensive response to being boxed in. Crew members aboard Irwin's boat administered CPR and rushed him to the nearby Low Isles where medical staff pronounced him dead.\n\nIrwin's death is believed to be the only fatality from a stingray ever captured on video.\n\nFootage of the incident was viewed by Queensland state police as part of their mandatory investigations. All copies of the footage were then destroyed at the behest of Irwin's family. Production was completed on \"Ocean's Deadliest\", which was broadcast in the US on the Discovery Channel on 21 January 2007. The documentary was completed with footage shot in the weeks following the accident, but without including any mention of Irwin's accidental death.\n\nNews of Irwin's death prompted reactions around the world. Then-Prime Minister John Howard expressed \"shock and distress\" at the death, saying that \"Australia has lost a wonderful and colourful son.\" Queensland's then-Premier Peter Beattie remarked that Irwin would \"be remembered as not just a great Queenslander, but a great Australian\". The Australian federal parliament opened on 5 September 2006 with condolence speeches by both Howard and the Leader of the Opposition, Kim Beazley. Flags at the Sydney Harbour Bridge were lowered to half mast in honour of Irwin. In the days following Irwin's death, reactions dominated Australian online news sources, talk-back radio programmes, and television networks. In the United States, where Irwin had appeared in over 200 Discovery Network television programmes, special tributes appeared on the Animal Planet channel, as well as on CNN and major TV talk shows. Thousands of Irwin's fans visited Australia Zoo after his death, paying their respects and bringing flowers, candles, stuffed animals and messages of support.\n\nCriticism of Irwin's career following his death came from Dan Mathews, vice-president of the animal rights group People for the Ethical Treatment of Animals. Comparing Irwin to a \"cheap reality TV star\", Mathews accused him of \"antagonising frightened wild animals ... a very dangerous message to send to children\", contrasted his methods with the behaviour of \"a responsible conservationist like Jacques Cousteau\", and said it was \"no shock at all that Steve Irwin should die provoking a dangerous animal.\" The son of Jacques Cousteau, Jean-Michel Cousteau—also a producer of wildlife documentaries—took issue with Irwin's \"very, very spectacular, dramatic way of presenting things\" and suggested instead that \"You don't touch nature, you just look at it.\" Jacques Cousteau's grandson and Jean-Michel's nephew, Philippe Cousteau Jr., on the other hand, called Irwin \"a remarkable individual\"; describing the \"Ocean's Deadliest\" project (on which he worked along with Irwin), Philippe said, \"I think why Steve was so excited about it that we were looking at these animals that people think of as, you know, dangerous and deadly monsters, and they're not. They all have an important place in the environment and in the world. And that was what his whole message was about.\"\n\nIn the weeks following Irwin's death, at least ten stingrays were found dead and mutilated on the beaches of Queensland, with their tails cut off, prompting speculation as to whether they might have been killed by fans of Irwin as an act of revenge, although, according to the chairman of the Queensland fishing information service, anglers regularly cut the tails off of accidentally caught stingrays to avoid being stung. Michael Hornby, a friend of Irwin and executive director of his Wildlife Warrior fund, condemned any revenge killings, saying that \"We just want to make it very clear that we will not accept and not stand for anyone who's taken a form of retribution. That's the last thing Steve would want.\"\n\nFamily and friends of Irwin held a private funeral service in Caloundra on 9 September 2006. Irwin was buried in a private ceremony at Australia Zoo later that same day; the grave site is inaccessible to the zoo's visitors. Prime Minister Howard and Queensland Premier Beattie had offered to hold a state funeral, but Irwin's family decided this would not be appropriate and—in the words of his father—he would have preferred to be remembered as an \"ordinary bloke\".\nOn 20 September, a public memorial service, introduced by Russell Crowe, was held in Australia Zoo's 5,500-seat Crocoseum; this service was broadcast live throughout Australia, the United States, the UK, Germany, and Asia, and it is estimated to have been seen by over 300 million viewers worldwide. The memorial included remarks by Prime Minister Howard; Irwin's father Bob and daughter Bindi; his associates Wes Mannion and John Stainton; and celebrities from Australia and around the world. Anthony Field of The Wiggles partly hosted the service, often sharing the screen with various animals, from koalas to elephants. Australian music star John Williamson sang \"True Blue\", which was Irwin's favourite song. In a symbolic finish to the service, Irwin's truck was loaded up with gear and driven out of the arena for the last time as Williamson sang. As a final tribute, Australia Zoo staff spelled out Irwin's catchphrase \"Crikey\" in yellow flowers as Irwin's truck was driven from the Crocoseum for the last time to end the service.\n\nOn 1 January 2007, Glass House Mountains Road, the road that runs by the Australia Zoo, was officially renamed Steve Irwin Way.\n\nThe Australian government announced in July 2007 that a 135,000-hectare (334,000-acre) national park was being created in northern Queensland and would be named the Steve Irwin Wildlife Reserve.\n\nAn asteroid discovered in 2001 has been named 57567 Crikey, in honour of Irwin and his \"signature phrase\".\n\n"}
{"id": "1929662", "url": "https://en.wikipedia.org/wiki?curid=1929662", "title": "The Arlington Institute", "text": "The Arlington Institute\n\nThe Arlington Institute (or \"TAI\") is a 501(c)(3) non-profit think tank specializing in predictive modeling of future events, that is, futures studies. It was founded in 1989 by former naval officer and military expert John L. Petersen in Arlington, Virginia.\n\nTAI believes society, science, ecology, and commerce are converging and that the issues that mankind is confronting are a product of how we live and think. As transition pioneers, TAI hopes to discover the early indicators of transition and provide solutions. The Arlington Institute is building an interactive web portal called The World's Biggest Problems. The site will allow people to collaborate on finding solutions to problems such as economic crisis, species extinction, peak oil, climate change, and water scarcity.\n\nThe Arlington Institute was founded by John L. Petersen in 1989. Petersen was a naval flight officer in the United States Navy, and has worked at military institutions in the United States, including the National War College, the Institute for National Security Studies, the Office of the Secretary of Defense, and the National Security Council at the White House, and has had experience in business and non-profit organisations. Among the 2003 Board of Directors was Jack DuVall, who had two years earlier created the International Center on Nonviolent Conflict together with Council on Foreign Relations Board member Peter Ackerman. In 2005, the Board of Directors of the Arlington Institute was chaired by John L. Petersen, and included Napier Collyns, George H. Kuper, David Martin, and another military expert, Owen Wormser. Wormser worked in the United States Air Force, worked in the Department of Defense, and in 2005 was a visiting professor at the Joint Military Intelligence College.\n\nThe Arlington Institute gained exposure in 2006 and 2007 when it hosted presentations made by Board member Dr. David Martin, also of M-CAM Inc., in which he detailed the dynamics contributing to the joint failure of credit, insurance, and monetary systems.\n\n"}
{"id": "46341110", "url": "https://en.wikipedia.org/wiki?curid=46341110", "title": "The Millennium Project", "text": "The Millennium Project\n\nThe Millennium Project is an independent non-profit think tank composed of futurists, scholars, decision-makers, and business planners, which focuses on the future. It publishes its annual \"State of the Future\" report. It examines such issues as clean water, population demographics, income inequality, energy, food, science & technology, ethics, economics, health, education, organized crime, decision-making and foresight, gender relations, demographics, and war and peace.\n\n"}
