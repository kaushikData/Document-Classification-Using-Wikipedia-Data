{"id": "47357334", "url": "https://en.wikipedia.org/wiki?curid=47357334", "title": "2015 Swedish submarine wreck discovery", "text": "2015 Swedish submarine wreck discovery\n\nIn 2015, Swedish diving group Ocean X Team found the wreck of what initially appeared to be a foreign mini-submarine off the east coast of Sweden.\n\nThe Swedish armed forces subsequently issued a statement that it was \"most likely\" a Russian that went down with all hands in 1916.\n\nOn 27July 2015, it was reported that a foreign mini-submarine, long and in diameter, had been located the previous week by Swedish diving group Ocean X Team, led by Dennis Åberg. The submarine was found offshore on the east coast of central Sweden. The group had received coordinates for where to search from Icelandic company Ixplorer. \n\nAccording to the divers who found the submarine, it was a modern model and it appeared to be intact, suggesting that the remains of the crew may still be on board. The hull was marked with Cyrillic letters. The pictures from the wreck showed that the yellow submarine was standing on the sea bottom with its tower pointing straight up. The diving group informed the Swedish armed forces on Monday 27July 2015, and video material from the wreckage was handed over for analysis. The hull of the wreck features the \"hard sign\" Ъ in a final position. Since this usage was discontinued during the 1917 orthographic reform, it indicated that the ship predated the Soviet era.\nOn 28July 2015, a statement was released by the Swedish armed forces public relations office that stated that analysis of the video footage and other evidence indicated that the wreck was \"most likely\" that of the Imperial Russian Navy \"Som\"-class submarine \"Som\" (\"Сом\"). \"Som\" sank with all hands on active service in 1916 after a collision with the steamship \"Ångermanland\" in the Sea of Åland, somewhere between Arholma and the Svartklubben lighthouse. According to several sources who previously worked with submarine analyses for the Swedish armed forces, the location of the submarine had been known for at least one year.\n\n, the Swedish armed forces considered the wreck to be of historical or marine archaeological interest rather than military, and handed over the case to the Ministry for Foreign Affairs. The ministry consequently informed the Russian embassy in Stockholm.\n\nThe Ocean X diving group is a privately owned salvage operator not affiliated with the Swedish military. It previously found an American Boeing B-17 Flying Fortress in 1992, and 2,400 bottles of 1907 champagne in 1994.\n\nThe wreck was discovered by using an ROV (remote operated vehicle) called V8 Sii from Ocean Modules in Åtvidaberg. The ROV was equipped with a grapple, a video camera and a Blueview sonar made by Teledyne.\n\n\n"}
{"id": "58959928", "url": "https://en.wikipedia.org/wiki?curid=58959928", "title": "Abdul Hai (chief justice)", "text": "Abdul Hai (chief justice)\n\nAbdul Hai (\"fl.\" late 1500s) was an Armenian who was chief justice (\"Mir Adl\") in the Mughal Empire during the reign of Akbar (1556-1605). He is described in the \"Tabaqat\" as an Amir, and in the \"Ain-i-Akbari\" (Constitution of Akbar) as \"the Qazi of the Imperial Camp\".\n\nAccording to Mesrovb Jacob Seth, his daughter Lady Juliana (died 1598), a doctor in Akbar's seraglio, married Sikandar Mirza.\n\nTheir son was Mirza Zulqarnain (c. 1594 – c. 1656) who was an important official within the court of Shah Jahan.\n"}
{"id": "2872863", "url": "https://en.wikipedia.org/wiki?curid=2872863", "title": "Abiodun (Oyo ruler)", "text": "Abiodun (Oyo ruler)\n\nAbiodun (reigned 1770–1789) was an 18th-century \"alaafin,\" or ruler, of the Oyo people in what is now Nigeria.\n\nComing to the throne shortly after the Oyo subjugation of neighboring Dahomey, Abiodun soon found himself embroiled in a civil war over the goals of the newly wealthy state.\n\nBashorun Gaha, the empire's prime minister and lord marshal, had used his power to pervert the constitutional terms of abdication in a bid to limit the powers of the \"Alaafin\" and gain more political power for himself. During Gaha's power play, he had succeeded in removing three kings before Abiodun curtailed his excesses and had him burned alive.\n\nIn terms of trade, while Abiodun favored economic expansion for its own sake, his opponents favored using the wealth from Dahomey's tribute to finance further military expansion. Abiodun soon proved victorious and pursued a policy of peaceful trade with the European merchants of the coast. This course significantly weakened the army, leaving his successor, Awole, facing a number of local revolts.\n\nAbiodun's reign is generally remembered as a time of peace and prosperity for the Oyo, though Nigerian playwright Femi Òsófisan portrays him as a despot in his play \"The Chattering and the Song\" (1973). \n\nHis son Alaafin Atiba was the founder of the ruling dynasty in the present Oyo. His grandson Cândido da Fonseca Galvão, under the title of \"Dom Oba II\", was an important South American abolitionist during Pedro II of Brazil's rule. His great grandson, Mohammed Shitta Bey, was an African merchant prince titled \"Seriki Musulumi of Lagos\" His other descendants include the 19th-century warrior Oluyole, the historian Samuel Johnson, his brother physician Obadiah Johnson, Samuel Ajayi Crowther, the first African Bishop of the CMS, as well as Nigerian founding father Herbert Macaulay and the Rev. Canon M. C. Adeyemi. A prominent contemporary descendant was Dr. Ameyo Adadevoh.\n\n"}
{"id": "53130758", "url": "https://en.wikipedia.org/wiki?curid=53130758", "title": "Alex Balcoba", "text": "Alex Balcoba\n\nAlex Balcoba, Sr. ( – 27 May 2016) was a Filipino crime journalist for Brigada Mass Media Corporation Weekly tabloid in Quiapo, Manila, Philippines, and was also known as the deputy director at the Manila Police District Press Corp. He was killed outside a watch repair shop that his family owns in Quiapo which allegedly took place due to a personal or business-related dispute. He was one of the many journalist killed in the Philippines in 2016. The case gained international attention after Mayor Rodrigo Duterte was elected president.\n\nBalcoba, a 55-year-old reporter, was born in the Philippines and was living in Quiapo, Manila with his wife, Florabella Balcoba who is a teacher. Alex and Florabella had owned a watch repair shop together with the help of their son. Balcoba's remains were cremated on May 29, 2016 in Caloocan City.\n\nBalcoba was a veteran reporter who worked to cover local news about crime and police brutality. He had been working as a journalist since the 1990s. Balcoba was also known for being the district deputy at the Manila police district, this is what helped get him into reporting the local crime for Brigada Mass Media Corporation. The Philippines is known for its forthright press and is one of the worst countries in terms of protection for journalists.\n\nBalcoba was shot in the head by two armed gunmen outside his family's watch repair store in Quiapo, Manila around 7:00 on Friday, May 24th in 2016. He was on the phone when the armed gunmen who were on a motorcycle drove by and shot while his friends and family were watching. A columnist says that others are suspecting that the gunman who shot Balcoba was a professional because it took him only one deadly shot to take out Balcoba. Several fellow journalist rushed Balcoba to the local hospital where he was pronounced dead. Balcoba's son told reporters that he had received numerous death threats due to his work in crime journalism, but they could not narrow it down because of the number of enemies he had. There were a few suspects questioned, but they had no luck. The investigation is still ongoing today with no leads on who the gunman was or who the gunman's companion was that had drove the getaway motorcycle. Police in Manila are working to look further into the different death threats that Balcoba had received. \n\nBalcoba had been the second journalist to be killed in the Philippines in May 2016, and the 34th since 2010 when president Benigno Aquino III came to power. He is also the 174th journalist killed since a bloodless uprising ended the Ferdinand Marcos' dictatorship 30 years ago. Along with these, one of the world's deadliest attacks against journalists took place in the Philippines in 2009, when 32 journalists were among 58 people killed by a warlord clan that had the intent on stopping a rival's election challenge. It was reported that the killing of Balcoba happened one day after the Philippines Congress had officially dubbed Rodrigo Duterte the next president. The investigators were told that Balcoba was working on a current story when he was killed and has covered crime related stories since the 1990s, but the investigators are also looking for suspects who would target Balcoba's family business as well.\n\nBalcoba was a veteran journalist who had covered the local crime and police beat since the 1990s. He was also known for being a reporter for the People’s Brigada and officer of the Manila Police District Press Corporation. He helped to bring justice for police who experienced brutality and helped to expose criminals. After the attack on Balcoba, it helped to get the public's attention on the problem at hand and take a stand to protect future journalist and reporters.\n\nMayor Rodrigo Duterte spoke out at a press conference after the killing of Alex Balcoba and made certain comments such as, \"Most of you are clean, but do not ever say all journalists are clean,\" he said. \"Just because you are a journalist, you are not exempt from assassination if you are a son of a bitch.\" As well as, \"... Most of those killed, to be frank, have done something. You won't be killed if you don't do anything wrong.\"\n\nSome people were outraged by this. \n\nReporter, Ryan Rosuaro spoke out against this and said, \"It is appalling that President-elect Rodrigo Duterte should justify the murder of journalists in the country by playing the corruption card.\"\n\nExecutive director of the Center for international law, Romel Regalado Bagares, said, \"Duterte's comments showed \"a cynical attitude towards what is a serious concern to the international community\" and could perpetuate impunity for the killers.\"\n\nInternational media monitor the Committee to Protect Journalists made a statement that wrote, \"President-Elect Rodrigo Duterte's shocking remarks apparently excusing extrajudicial killings threaten to make the Philippines into a killing field for journalists, we strongly urge him to retract his comments and to signal that he intends to protect, not target the press.\"\n\nThen finally, the Foreign Correspondent Association of the Philippines showed concern by stating, \"Duterte's statement is a chilling reminder that journalists in the Philippines continue to live under threat, decades after (the association) was founded to fight for press freedom at the height of Ferdinand Marcos's dictatorship.\"\n\nBalcoba's wife spoke up a few days later and replied to the Mayor's comments and standing up for her husband by making a few comments such as, \"Sir, I am appealing to you. If I have to kneel, I will, just do not let this case drag on. I hope that once you assume office, you will prioritize this case so that the media killings, if not all the crimes in the country, will stop. You know, I really hope you could help me. I am drawing my strength from you, so that one day, I can finally get justice. I also hope the death penalty will be reimposed so that the killers of my husband will be punished with it.\"\n\n"}
{"id": "600624", "url": "https://en.wikipedia.org/wiki?curid=600624", "title": "Animals in space", "text": "Animals in space\n\nNon-human animals in space originally served to test the survivability of spaceflight, before human spaceflights were attempted. Later, other non-human animals were flown to investigate various biological processes and the effects microgravity and space flight might have on them. Bioastronautics is an area of bioengineering research which spans the study and support of life in space. To date, seven national space programs have flown animals into space: the Soviet Union, the United States, France, Argentina, China, Japan and Iran.\n\nA wide variety of non-human animals have been launched into space, including monkeys, dogs, tortoises, and insects. The United States launched flights containing monkeys and primates primarily between 1948-1961 with one flight in 1969 and one in 1985. France launched two monkey-carrying flights in 1967. The Soviet Union and Russia launched monkeys between 1983 and 1996. During the 1950s and 1960s, the Soviet space program used a number of dogs for sub-orbital and orbital space flights. Two tortoises and a variety of insects were the first inhabitants of Earth to circle the Moon, on the 1968 Zond 5 mission.\n\nAnimals had been used in aeronautic exploration since 1783 when the Montgolfier brothers sent a sheep, a duck, and a rooster aloft in a hot air balloon (the duck serving as the experimental control). The limited supply of captured German V-2 rockets led to the U.S. use of high-altitude balloon launches carrying fruit flies, mice, hamsters, guinea pigs, cats, dogs, frogs, goldfish and monkeys to heights of up to . These high-altitude balloon flights from 1947 to 1960 tested radiation exposure, physiological response, life support and recovery systems. The U.S. high-altitude manned balloon flights occurred in the same time frame, one of which also carried fruit flies.\nThe first animals sent into space were fruit flies aboard a U.S.-launched V-2 rocket on 20 February 1947 from White Sands Missile Range, New Mexico. The purpose of the experiment was to explore the effects of radiation exposure at high altitudes. The rocket reached 68 miles (109 km) in 3 minutes and 10 seconds, past both the U.S. Air Force 50-mile and the international 100 km definitions of the boundary of space. The Blossom capsule was ejected and successfully deployed its parachute. The fruit flies were recovered alive. Other V-2 missions carried biological samples, including moss.\n\nAlbert II, a rhesus monkey, became the first monkey in space on 14 June 1949, in a U.S.-launched V-2, after the failure of the original Albert's mission on ascent. Albert I reached only 30–39 miles (48–63 km) altitude; Albert II reached about 83 miles (134 km). Albert II died on impact after a parachute failure. Numerous monkeys of several species were flown by the U.S. in the 1950s and 1960s. Monkeys were implanted with sensors to measure vital signs, and many were under anesthesia during launch. The death rate among monkeys at this stage was very high: about two-thirds of all monkeys launched in the 1940s and 1950s died on missions or soon after landing.\n\nOn 31 August 1950, the U.S. launched a mouse into space (137 km) aboard a V-2 (the Albert V flight, which, unlike the Albert I-IV flights, did not have a monkey), but the rocket disintegrated because the parachute system failed. The U.S. launched several other mice in the 1950s.\n\nOn 22 July 1951, the Soviet Union launched the R-1 IIIA-1 flight, carrying the dogs Tsygan (, \"Gypsy\") and Dezik () into space, but not into orbit. These two dogs were the first living higher organisms successfully recovered from a spaceflight. Both space dogs survived the flight, although one would die on a subsequent flight. The U.S. launched mice aboard spacecraft later that year; however, they failed to reach the altitude for true spaceflight.\n\nOn 3 November 1957, the second-ever orbiting spacecraft carried the first animal into orbit, the dog Laika, launched aboard the Soviet Sputnik 2 spacecraft (nicknamed 'Muttnik' in the West). Laika died during the flight, as was intended because the technology to return from orbit had not yet been developed. At least 10 other dogs were launched into orbit and numerous others on sub-orbital flights before the historic date of 12 April 1961, when Yuri Gagarin became the first human in space.\n\nOn 13 December 1958, a Jupiter IRBM, AM-13, was launched from Cape Canaveral, Florida, with a United States Navy-trained South American squirrel monkey named Gordo on board. The nose cone recovery parachute failed to operate and Gordo was lost. Telemetry data sent back during the flight showed that the monkey survived the 10 g of launch, 8 minutes of weightlessness and 40 g of reentry at 10,000 miles per hour (16,000 km/h). The nose cone sank downrange from Cape Canaveral and was not recovered.\n\nMonkeys Able and Baker became the first monkeys to survive spaceflight after their 1959 flight. On 28 May 1959, aboard Jupiter IRBM AM-18, were a 7-pound (3.18 kg) American-born rhesus monkey, Able, from Independence, Kansas, and an 11-ounce (310-gram) squirrel monkey from Peru, Baker. The monkeys rode in the nose cone of the missile to an altitude of 360 miles (579 km) and a distance of 1,700 miles (2,735 km) down the Atlantic Missile Range from Cape Canaveral, Florida. They withstood forces 38 times the normal pull of gravity and were weightless for about 9 minutes. A top speed of 10,000 mph (16,000 km/h) was reached during their 16-minute flight. The monkeys survived the flight in good condition. Able died four days after the flight from a reaction to anesthesia, while undergoing surgery to remove an infected medical electrode. Baker was the center of media attention for the next several months as she was watched closely for any ill-effects from her space flight. She was even mated in an attempt to test her reproductive system. Baker lived until 29 November 1984, at the US Space and Rocket Center in Huntsville, Alabama.\n\nOn 2 July 1959, a launch of a Soviet R2 rocket, which reached , carried two space dogs and Marfusa, the first rabbit to go into space.\n\nA 19 September 1959 launch, Jupiter AM-23, carried 2 frogs along with 12 mice but the rocket was destroyed during launch.\n\nOn 19 August 1960 the Soviet Union launched Sputnik 5 (also known as Korabl-Sputnik 2) which carried the dogs Belka and Strelka, along with a gray rabbit, 40 mice, 2 rats, and 15 flasks of fruit flies and plants. It was the first spacecraft to carry animals into orbit and return them alive. One of Strelka's pups, Pushinka, bred and born after her mission, was given as a present to Caroline Kennedy by Nikita Khrushchev in 1961, and many descendants are known to exist.\n\nThe United States sent 3 black mice Sally, Amy and Moe 1,000 km up and 8,000 km distance from Cape Canaveral on 13 October 1960 using an Atlas D 71D launch vehicle. The mice were retrieved from the nosecone near Ascension Island and were said to be in good condition.\n\nOn 31 January 1961, Ham the Chimp was launched in a Mercury capsule aboard a Redstone rocket. Ham's mission was Mercury-Redstone 2. The chimpanzee had been trained to pull levers to receive rewards of banana pellets and avoid electric shocks. His flight demonstrated the ability to perform tasks during spaceflight. A little over 3 months later the United States sent Alan Shepard into space. Enos the chimp became the first chimpanzee in orbit on 29 November 1961, in another Mercury capsule, an Atlas rocket, Mercury-Atlas 5.\n\nOn 9 March 1961 the Soviet Union launched the Korabl-Sputnik 4 that carried a dog named Chernushka, some mice, frogs and, for the first time into space, a guinea pig. All were successfully recovered.\n\nFrance flew their first rat (Hector) into space on 22 February 1961. Two more rats were flown in October 1962.\n\nOn 18 October 1963, France launched Félicette the cat aboard Veronique AGI sounding rocket No. 47. The launch was directed by the French Centre d'Enseignement et de Recherches de Médecine Aéronautique (CERMA). Félicette was recovered alive after a 15-minute flight and a descent by parachute. Félicette had electrodes implanted into her brain, and the recorded neural impulses were transmitted back to Earth. A second cat was sent to space by CERMA on 24 October 1963, but the flight ran into difficulties that prevented recovery. The final French animal launches were of two monkeys in March 1967.\n\nChina launched mice and rats in 1964 and 1965, and two dogs in 1966.\n\nDuring the Voskhod program, two Soviet space dogs, Veterok (Ветерок, Little Wind) and Ugolyok (Уголёк, Blackie), were launched on 22 February 1966, on board Cosmos 110 and spent 22 days in orbit before landing on 16 March. This spaceflight of record-breaking duration was not surpassed by humans until Soyuz 11 in 1971 and still stands as the longest space flight by dogs.\n\nThe United States launched Biosatellite I in 1966 and Biosatellite I/II in 1967 with fruit flies, parasitic wasps, flour beetles and frog eggs, along with bacteria, amoebae, plants and fungi.\n\nOn 11 April 1967, Argentina also launched the rat Belisario, atop a Yarará rocket, from Cordoba military range, which was recovered successfully. This flight was followed by a series of subsequent flights using rats. It is unclear if any Argentinean biological flights passed the 100 km limit of space.\n\nThe first two tortoises in space were launched on Zond 5 on 14 September 1968 by the Soviet Union. The Horsfield's tortoises were sent on a circumlunar voyage along with wine flies, meal worms, and other biological specimens. These were the first animals in deep space and the first inhabitants of earth to travel around the moon. The capsule overshot its terrestrial landing site but was successfully recovered at sea on 21 September. The animals survived but suffered some weight loss.\n\nOn 28 June 1969, the United States launched the monkey Bonny, a macaque, on Biosatellite 3 in what was intended to have been a 30-day orbit around the Earth, with the monkey being fed by food pellets from a dispenser that he had been trained to operate. However, Bonny's health deteriorated rapidly and he was returned to Earth on July 7, but died the next day after the Biosatellite capsule was recovered in the Pacific Ocean. \n\nIn total in the 1950s and 1960s, the Soviet Union launched missions with passenger slots for at least 57 dogs. The actual number of dogs in space is smaller, because some dogs flew more than once.\n\nOn 23 December 1969, as part of the 'Operación Navidad' (Operation Christmas), Argentina launched Juan (a cai monkey, native of Argentina's Misiones Province) using a Canopus II rocket. It ascended 82 kilometers and then was recovered successfully. Later, on 1 February 1970 the experience was repeated with a female monkey of the same species using a X-1 Panther rocket. It reached a higher altitude than its predecessor, but it was lost after the capsule's parachute failed.\n\nTwo bullfrogs were launched on a one-way mission on the Orbiting Frog Otolith satellite on 9 November 1970, to understand more about space motion sickness.\n\nApollo 16 on 16 April 1972 carried nematodes, and Apollo 17, launched on 7 December 1972 carried five pocket mice, although one died on the circumlunar trip. Skylab 3 carried pocket mice and the first fish in space (a mummichog), and the first spiders in space (garden spiders named Arabella and Anita). Mummichog were also flown by the U.S. on the Apollo-Soyuz joint mission, launched 15 July 1975.\n\nThe Soviets flew several Bion program missions which consisted of satellites with biological cargoes. On these launches they flew tortoises, rats, and mummichog. On Soyuz 20, launched 17 November 1975, tortoises set the duration record for an animal in space when they spent 90.5 days in space. Salyut 5 on 22 June 1976, carried tortoises and a fish (a zebra danio).\n\nThe Soviet Union sent eight monkeys into space in the 1980s on Bion flights. In 1985, the U.S. sent two squirrel monkeys aboard Spacelab 3 on the Space Shuttle with 24 male albino rats and stick insect eggs. Bion flights also flew zebra danio, fruit flies, rats, stick insect eggs and the first newts in space.\n\nBion 7 (1985) had 10 newts (\"Pleurodeles waltl\") on board. The newts had part of their front limbs amputated, to study the rate of regeneration in space, knowledge to understand human recovery from space injuries.\n\nAfter an experiment was lost in the Space Shuttle \"Challenger\" disaster, chicken embryos (fertilized eggs) were sent into space in an experiment on STS-29 in 1989. The experiment was designed for a student contest.\n\nFour monkeys flew aboard the last Bion flights of the Soviet Union as well as frogs and fruit flies. The Foton program flights carried dormant brine shrimp (\"Artemia franciscana\"), newts, fruit flies, and sand desert beetles (\"Trigonoscelis gigas\").\n\nChina launched guinea pigs in 1990.\n\nToyohiro Akiyama, a Japanese journalist carried Japanese tree frogs with him during his trip to the \"Mir\" space station in December 1990. Other biological experiments aboard Mir involved quail eggs.\n\nJapan launched its first animals, a species of newt, into space on 18 March 1995 aboard the Space Flyer Unit.\n\nDuring the 1990s the U.S. carried crickets, mice, rats, frogs, newts, fruit flies, snails, carp, medaka, oyster toadfish, sea urchins, swordtail fish, gypsy moth eggs, stick insect eggs, brine shrimp (\"Artemia salina\"), quail eggs, and jellyfish aboard Space Shuttles.\n\nThe last flight of \"Columbia\" in 2003 carried silkworms, garden orb spiders, carpenter bees, harvester ants, and Japanese killifish (medaka). Nematodes (\"C. elegans\") from one experiment were found still alive in the debris after the Space Shuttle \"Columbia\" disaster.\n\n\"C. elegans\" are also part of experiments aboard the International Space Station as well as research using quail eggs.\n\nEarlier shuttle missions included grade school, junior high and high school projects; some of these included ants, stick insect eggs and brine shrimp cysts. Other science missions included gypsy moth eggs.\n\nOn 12 July 2006, Bigelow Aerospace launched their \"Genesis I\" inflatable space module, containing many small items such as toys and simple experiments chosen by company employees that would be observed via camera. These items included insects, perhaps making it the first private flight to launch animals into space. Included were Madagascar hissing cockroaches and Mexican jumping beans — seeds containing live larvae of the moth \"Cydia deshaisiana\". On 28 June 2007, Bigelow launched \"Genesis II\", a near-twin to \"Genesis I\". This spacecraft also carried the Madagascar hissing cockroaches and added South African flat rock scorpions (\"Hadogenes troglodytes\") and seed-harvester ants (\"Pogonomyrmex californicus\").\n\nIn September 2007, during the European Space Agency's FOTON-M3 mission, tardigrades, also known as water-bears, were able to survive 10 days of exposure to open-space with only their natural protection.\n\nOn the same mission, a number of cockroaches were carried inside a sealed container and at least one of the females conceived during the mission. After they were returned to earth, the one named Nadezhda became the first earth creature to produce young that had been conceived in space.\n\nOn 15 March 2009, during the countdown of the STS-119, a free-tailed bat was seen clinging to the fuel tank. NASA observers believed the bat would fly off once the shuttle started to launch, but it did not. Upon analyzing the images, a wildlife expert who provided support to the center said it likely had a broken left wing and some problem with its right shoulder or wrist. The animal most likely perished quickly during Discovery's climb into orbit.\n\nIn November 2009, STS-129 took painted lady and monarch butterfly larvae into space for a school experiment as well as thousands of \"C. elegans\" roundworms for long-term weight loss studies.\n\nOn 3 February 2010, on the 31st anniversary of its revolution, Iran became the latest country to launch animals into space. The animals (a mouse, two turtles and some worms) were launched on top of the Kavoshgar 3 rocket and returned alive to Earth.\n\nIn May 2011, the last flight of \"Endeavour\" (STS-134) carried two golden orb spiders, named Gladys and Esmeralda, as well as a fruit fly colony as their food source in order to study the effects of microgravity on spiders' behavior. Tardigrades and other extremophiles were also sent into orbit.\n\nIn November 2011, the Living Interplanetary Flight Experiment on the Fobos-Grunt mission planned to carry tardigrades to Mars and back; however, the mission failed to leave Earth orbit.\n\nIn October 2012, 32 medaka fish were delivered to the International Space Station by Soyuz TMA-06M for the new Aquatic Habitat in the Kibo module.\n\nOn 28 January 2013, Iranian news agencies reported that Iran sent a monkey in a \"Pishgam\" rocket to a height of and retrieved a \"shipment\". Later Iran's space research website uploaded an 18-minute video. The video was uploaded later on YouTube.\n\nIn January 2014, the search strategies of pavement ants were studied on the ISS.\n\nOn 19 July 2014, Russia announced that they launched their Foton-M4 satellite into low earth orbit (575 kilometers) with 1 male and 4 female geckos (possibly gold dust day geckos) as the payload. This was an effort to study the effects of microgravity on reproductive habits of reptiles. On 24 July 2014, it was announced that Russia had lost control of the Foton-M4 satellite, leaving only two months to restore contact before the geckos' food supply was exhausted. Control of the satellite was subsequently restored on 28 July 2014. On 1 September 2014 Russia confirmed the death of all five geckos, stating that their mummified bodies seem to indicate they froze to death. Russia is said to have appointed an emergency commission to investigate the animals' deaths.\n\nOn 23 September 2014, the SpaceX CRS-4 mission delivered 20 mice to live on the ISS for study of the long-term effects of microgravity on the rodents.\n\nOn 14 April 2015, the SpaceX CRS-6 delivered 20 C57BL/6NTAC mice to live on the ISS for evaluating microgravity as the extreme opposite of a healthy active lifestyle. In the absence of gravity, astronauts are subject to a decrease in muscle, bone, and tendon mass. \"Although, we’re not out to treat couch potatoes,\" states head Novartis Institute for Biomedical Research (NIBR) scientist on the project Dr. Sam Cadena, \"we’re hoping that these experiments will help us to better understand muscle loss in populations where physical activity in any form is not an option; e.g., in the frail elderly or those subjected to bed rest or immobilization due to surgery or chronic disease.\" \n\nOn 8 April 2016, Rodent Research 3 delivered 20 mice on SpaceX CRS-8. The experiment sponsored by Eli Lilly and Co. was a study of myostatin inhibition for the prevention of skeletal and muscle atrophy and weakness. Mice are known to suffer from rapid loss of muscle and bone mass after as little as 12 days of space flight exposure. The mice were euthanized and dissected on the station and then frozen for eventual return to Earth for further study. \n\nOn 29 June 2018, a SpaceX Dragon spaceship blasted off from Florida carrying 20 mice. The rodent crew arrived at the ISS on 2 July 2018. Their record-breaking journey — this was the longest mice have been off the planet — was part of a study on how Earth-dwellers' guts and sleep schedules responded to the stress of being in space.\n\n\n"}
{"id": "3366914", "url": "https://en.wikipedia.org/wiki?curid=3366914", "title": "Artificial reproduction", "text": "Artificial reproduction\n\nArtificial reproduction/propagation is the creation of new life by other than the natural means available to an organism. Examples include artificial insemination, in vitro fertilization, cloning and embryonic splitting, or cleavage.\n\nCutting plants' stems and placing them in compost is also a form of artificial reproduction.\nWe can grow many plants from one plant by using the man-made methods. The process of growing many plants from one plant by man-made methods is called artificial propagation of plants. A number of methods of artificial propagation of plants are used in agriculture (for raising crops), and horticulture (cultivation of vegetables, fruits and flowers). The three common methods for the artificial propagation of plants are:\n\nWe will now describe all these methods, one by one. Let us start with the cuttings method for the artificial propagation of plants.\n\nIn this method, a branch of the plant is pulled towards the ground and a part of it is covered with moist soil leaving the tip of the branch exposed above the ground. After some time, new roots develop from the part of the branch buried in the soil. The branch is then cut off from the parent plant. The part of the branch which has developed roots grows to become a new plant (just like the parent plant). Jasmine plant (chameli) is propagated or produced by the layering method.\n\nWe can see from that one left side branch and one right side branch of the parent jasmine plant have been buried in moist soil. The parts of branches which are buried in soil grow their own roots. When this happens, the branches of the parent plant connecting the newly formed plants are cut off so that the newly formed plants may grow on their own and develop into mature plants (like the parent plant).\n\nMany plants like strawberry and raspberry are propagated by the natural layering method. The natural layering occurs because these plants form runners (which are soft horizontal stems running above the ground). Wherever the ends of such runners touch the ground, new plants are formed at those places. In this way, many more strawberry or raspberry plants are formed from\" the parent plant in a natural way.\n\nThe layering method is used for the propagation (or reproduction) of plants like : Jasmine, Strawberry, Raspberry, Lemon, Guava, Hibiscus (China rose), Bougainvillea and many slender ornamental plants.Example: Nerium\n\nGrafting is a method in which the cut stems of two different plants are joined together in such a way that the two stems join and grow as a single plant. This new plant will have the \ncharacteristics of both the original plants.\nThe cutted stem of a plant having roots and fixed in the soil is called \"stock\". Stock is the lower part of a plant having the roots.The cutted stem of another plant without is called 'scion'. Scion is the upper part of a plant which may have leaves on it .\n\nIn carrying out grafting, two plants are chosen which are to be used as scion and stock. First, the stem (or branch) is removed from the plant chosen to be made scion (for its desirable characteristics) by making a slanting cut. This gives us the scion with a slanting cut. The stem of second plant (or tree) to be used in grafting is also cut in a slanting way. The lower part of this plant (or tree) is stock. It has also a slanting cut.\n\nThe scion is placed over the stock. The cut surfaces of the scion and stock are fitted together and bound tightly with a piece of cloth and covered properly with polythene sheet (so as to prevent harmful infection by bacteria or fungus, and loss of water and plant sap from the cut and joined ends of stock and scion).\n\nWhile joining the scion to the stock, care should be taken to make sure that the cambium layer of scion is in contact with the cambium layer of stock (because the cambium layer in the stem is responsible for growth).\n\nThe cut soon heals and the stock and scion of two plants grow together and become one plant. The scion continues to produce its original leaves, flowers and fruits but it gets water and minerals for making food from the chosen stock. So, the fruits will have the characteristics of both the plants (from which scion and stock have come).\n\nGrafting is used to breed fruit trees and flowering bushes. Apple, peach, apricot and pear trees are often grafted. We will now describe some of the advantages of the grafting method of artificial propagation.\n Grafting can be used to produce varieties of seedless fruits. Some examples of such plants which are reproduced by artificial vegetative propagation methods are: Banana, Pineapple, Orange, Grape, Rose etc.\n\nwww.cmf.org\n"}
{"id": "492486", "url": "https://en.wikipedia.org/wiki?curid=492486", "title": "Bachem", "text": "Bachem\n\nBachem is a contract manufacturing organization headquartered in Bubendorf, Switzerland.\n\nBachem specializes in the manufacturing of peptides and complex organic molecules such as active pharmaceutical ingredients. In addition, biochemicals for research purposes and manufacturing processes are developed. The firm provides a number of products and services to the pharmaceutical and life science industries, including organic, fine and performance chemicals, custom manufacturing of biopharmaceuticals, chemical synthesis capabilities and new chemical entities, and services for the bioscience sector.\n\nIn 1971, Peter Grogg founded Bachem Feinchemikalien AG with two employees in Liestal near Basel with the focus on peptide synthesis. In 1977, Bachem moved with eight employees to nearby Bubendorf, and, in 1978, it manufactured, for the first time, peptides for use in medicine under cGMP guidelines. Between 1981 and 1991 Bachem tripled its production capacity and built an administration building, and in 1995, facilities were further expanded including the quality control department to a total of 168'000 sq. ft. (15'600 m2).\n\nExpansion into markets outside Europe began with the establishment of Bachem Bioscience, Inc. in Philadelphia, USA in 1987. To strengthen its presence in Europe, Bachem opened sales and marketing centers in Germany in 1988 and in France in 1993. In 1996 it acquired its largest competitor, Bachem, Inc., together with its subsidiaries in Germany and the United Kingdom. In June 1998, following a very successful IPO, Bachem's shares began to be publicly traded on the Swiss Stock Exchange. In 1999 and 2000, a new administration building and a further production facility were respectively opened at the Bubendorf site.\n\nWith the acquisition of the San Carlos, California based Peninsula Laboratories, Inc. and its affiliate Peninsula Laboratories Ltd., United Kingdom in December 1999, Bachem extended its strong position in therapeutically active peptides into immunology, a domain with strong future promise. In August 2000, Peninsula Laboratories Europe Ltd. and Bachem (UK) Ltd. merged to a single Bachem group affiliated company in the United Kingdom.\nThe acquisition of Sochinaz SA, a Swiss-based specialized manufacturer of active pharmaceutical ingredients in 2001, strengthened Bachem's expertise and once again expanded its manufacturing capabilities.\n\nReorganization of distribution was carried out in Germany and France as a result of the positive experiences gained with the realignment of the sales and marketing organization in the USA. As a result the subsidiary in France was closed at the end of 2002. The subsidiary in Germany was relocated from Heidelberg to Weil am Rhein, was renamed Bachem Distribution Services GmbH and is now responsible for all customers in the European Union.\nIn 2003, a holding structure was introduced to align the factual organization of the company with an appropriate legal structure and to support management structure adjustments, given by continued company growth requirements. In 2007, Bachem acquired the Clinalfa® brand for Bachem's ready-to-use clinical trial materials and related services. In 2010, Bachem commenced the partnership with Atheris laboratories offering the Melusine® range of research products for lead generation of new pharmaceutical candidates. In 2011, Bachem celebrated its 40th year as a producer of research ingredients and active pharmaceutical ingredients (APIs) for the pharmaceutical industry.\n\nIn 2013, Bachem and GlyTech Inc. teamed up to offer glycosylated peptides in their product pipelines. Bachem and affiliate were conducting groundbreaking work on interferon beta-1a and to manufacture glycosylated somatostatin analogues by chemical synthesis.\n\nin 2015, Bachem acquired American Peptide Company, a mid-sized biotech establishment in Sunnyvale CA, not far from Bachem's US facility in Torrance, expanding the firm's GMP manufacturing and R&D capabilities for the US and international markets.\n\nBachem Holding AG consists of the following divisions:\n"}
{"id": "40888645", "url": "https://en.wikipedia.org/wiki?curid=40888645", "title": "Bayesian programming", "text": "Bayesian programming\n\nBayesian programming is a formalism and a methodology for having a technique to specify probabilistic models and solve problems when less than the necessary information is available. Bayes’ Theorem is the central concept behind this programming approach, which states that the probability of something occurring in the future can be inferred by past conditions related to the event.\n\nEdwin T. Jaynes proposed that probability could be considered as an alternative and an extension of logic for rational reasoning with incomplete and uncertain information. In his founding book \"Probability Theory: The Logic of Science\" he developed this theory and proposed what he called “the robot,” which was not\na physical device, but an inference engine to automate probabilistic reasoning—a kind of Prolog for probability instead of logic. Bayesian programming is a formal and concrete implementation of this \"robot\".\n\nBayesian programming may also be seen as an algebraic formalism to specify graphical models such as, for instance, Bayesian networks, dynamic Bayesian networks, Kalman filters or hidden Markov models. Indeed, Bayesian Programming is more general than Bayesian networks and has a power of expression equivalent to probabilistic factor graphs.\n\nA Bayesian program is a means of specifying a family of probability distributions.\n\nThe constituent elements of a Bayesian program are presented below:\n\n\nThe purpose of a description is to specify an effective method of computing a joint probability distribution\non a set of variables formula_4 given a set of experimental data formula_3 and some\nspecification formula_2. This joint distribution is denoted as: formula_7.\n\nTo specify preliminary knowledge formula_2, the programmer must undertake the following:\n\n\nGiven a partition of formula_10 containing formula_11 subsets, formula_11 variables are defined\nformula_13, each corresponding to one of these subsets.\nEach variable formula_14 is obtained as the conjunction of the variables formula_15\nbelonging to the formula_16 subset. Recursive application of Bayes' theorem leads to:\n\nConditional independence hypotheses then allow further simplifications. A conditional\nindependence hypothesis for variable formula_14 is defined by choosing some variable formula_19\namong the variables appearing in the conjunction formula_20, labelling formula_21 as the\nconjunction of these chosen variables and setting:\n\nWe then obtain:\n\nSuch a simplification of the joint distribution as a product of simpler distributions is\ncalled a decomposition, derived using the chain rule.\n\nThis ensures that each variable appears at the most once on the left of a conditioning\nbar, which is the necessary and sufficient condition to write mathematically valid\ndecompositions.\n\nEach distribution formula_24 appearing in the product is then associated\nwith either a parametric form (i.e., a function formula_25) or a question to another Bayesian program formula_26.\n\nWhen it is a form formula_25, in general, formula_28 is a vector of parameters that may depend on formula_21 or formula_3 or both. Learning\ntakes place when some of these parameters are computed using the data set formula_3.\n\nAn important feature of Bayesian Programming is this capacity to use questions to other Bayesian programs as components of the definition of a new Bayesian program. formula_24 is obtained by some inferences done by another Bayesian program defined by the specifications formula_33 and the data formula_34. This is similar to calling a subroutine in classical programming and provides an easy way to build hierarchical models.\n\nGiven a description (i.e., formula_7), a question is obtained by partitioning formula_4\ninto three sets: the searched variables, the known variables and\nthe free variables.\n\nThe 3 variables formula_37, formula_38 and formula_39 are defined as the\nconjunction of the variables belonging to\nthese sets.\n\nA question is defined as the set\nof distributions:\n\nmade of many \"instantiated questions\" as the cardinal of formula_38,\neach instantiated question being the distribution:\n\nGiven the joint distribution formula_43, it is always possible to compute any possible question using the following general inference:\n\nwhere the first equality results from the marginalization rule, the second\nresults from Bayes' theorem and the third corresponds to a second application of marginalization. The denominator appears to be a normalization term and can be replaced by a constant formula_45.\n\nTheoretically, this allows to solve any Bayesian inference problem. In practice,\nhowever, the cost of computing exhaustively and exactly formula_46 is too great in almost all cases.\n\nReplacing the joint distribution by its decomposition we get:\n\nwhich is usually a much simpler expression to compute, as the dimensionality of the problem is considerably reduced by the decomposition into a product of lower dimension distributions.\n\nThe purpose of Bayesian spam filtering is to eliminate junk e-mails.\n\nThe problem is very easy to formulate. E-mails should be classified\ninto one of two categories: non-spam or spam. The only available information to classify the e-mails is their content: a set of words. Using these words without taking the order into account is commonly called a bag of words model.\n\nThe classifier should furthermore be able to adapt to its user and to learn\nfrom experience. Starting from an initial standard setting, the classifier should\nmodify its internal parameters when the user disagrees with its own decision.\nIt will hence adapt to the user’s criteria to differentiate between non-spam and\nspam. It will improve its results as it encounters increasingly classified e-mails.\n\nThe variables necessary to write this program are as follows:\n\nThese formula_53 binary variables sum up all the information\nabout an e-mail.\n\nStarting from the joint distribution and applying recursively Bayes' theorem we obtain:\n\nThis is an exact mathematical expression.\n\nIt can be drastically simplified by assuming that the probability of appearance of a word knowing the nature of the text (spam or not) is independent of the appearance of the other words. This is the naive Bayes assumption and this makes this spam filter a naive Bayes model.\n\nFor instance, the programmer can assume that:\n\nto finally obtain:\n\nThis kind of assumption is known as the naive Bayes' assumption. It is \"naive\" in the sense that the independence between words is clearly not completely true. For instance, it completely neglects that the appearance of pairs of words may be more significant than isolated appearances. However, the programmer may assume this hypothesis and may develop the model and the associated inferences to test how reliable and efficient it is.\n\nTo be able to compute the joint distribution, the programmer must now specify the\nformula_53 distributions appearing in the decomposition:\n\n\nwhere formula_64 stands for the number of appearances of the formula_52 word in non-spam e-mails and formula_66 stands for the total number of non-spam e-mails. Similarly, formula_67 stands for the number of appearances of the formula_52 word in spam e-mails and formula_69 stands for the total number of spam e-mails.\n\nThe formula_50 forms formula_61 are not yet completely specified because the formula_72 parameters formula_73, formula_74, formula_66 and formula_69 have no values yet.\n\nThe identification of these parameters could be done either by batch processing a series of classified e-mails or by an incremental updating of the parameters using the user's classifications of the e-mails as they arrive.\n\nBoth methods could be combined: the system could start with initial standard values of these parameters issued from a generic database, then some incremental learning customizes the classifier to each individual user.\n\nThe question asked to the program is: \"what is the probability for a given text to be spam knowing which words appear and don't appear in this text?\"\nIt can be formalized by:\n\nwhich can be computed as follows:\n\nThe denominator appears to be a normalization constant. It is not necessary to compute it to decide if we are dealing with spam. For instance, an easy trick is to compute the ratio:\n\nThis computation is faster and easier because it requires only formula_80 products.\n\nThe Bayesian spam filter program is completely defined by:\n\nBayesian filters (often called Recursive Bayesian estimation) are generic probabilistic models for time evolving processes. Numerous models are particular instances of this generic approach, for instance: the Kalman filter or the Hidden Markov model (HMM).\n\n\nThe decomposition is based:\n\nThe parametrical forms are not constrained and different choices lead to different well-known models: see Kalman filters and Hidden Markov models just below.\n\nThe typical question for such models is formula_94: what is the probability distribution for the state at time formula_95 knowing the observations from instant formula_83 to formula_88?\n\nThe most common case is Bayesian filtering where formula_98, which searches for the present state, knowing past observations.\n\nHowever it is also possible formula_99, to extrapolate a future state from past observations, or to do smoothing formula_100, to recover a past state from observations made either before or after that instant.\n\nMore complicated questions may also be asked as shown below in the HMM section.\n\nBayesian filters formula_101 have a very interesting recursive property, which contributes greatly to their attractiveness. formula_102 may be computed simply from formula_103 with the following formula:\n\nAnother interesting point of view for this equation is to consider that there are two phases: a\nprediction phase and an estimation phase:\n\n\nThe very well-known Kalman filters are a special case of Bayesian\nfilters.\n\nThey are defined by the following Bayesian program:\n\n\nWith these hypotheses and by using the recursive formula, it is possible to solve\nthe inference problem analytically to answer the usual formula_111 question.\nThis leads to an extremely efficient algorithm, which explains the popularity of Kalman filters and the number of their everyday applications.\n\nWhen there are no obvious linear transition and observation models, it is still often\npossible, using a first-order Taylor's expansion, to treat these models as locally linear.\nThis generalization is commonly called the extended Kalman filter.\n\nHidden Markov models (HMMs) are another very popular specialization of Bayesian filters.\n\nThey are defined by the following Bayesian program:\n\nboth specified using probability matrices.\n\nWhat is the most probable series of states that leads to the present state, knowing the past observations?\n\nThis particular question may be answered with a specific and very efficient algorithm\ncalled the Viterbi algorithm.\n\nThe Baum–Welch algorithm has been developed\nfor HMMs.\n\nSince 2000, Bayesian programming has been used to develop both robotics applications and life sciences models.\n\nIn robotics, bayesian programming was applied to autonomous robotics, robotic CAD systems, advanced driver-assistance systems, robotic arm control, mobile robotics, human-robot interaction, human-vehicle interaction (Bayesian autonomous driver models) video game avatar programming and training and real-time strategy games (AI).\n\nIn life sciences, bayesian programming was used in vision to reconstruct shape from motion, to model visuo-vestibular interaction and to study saccadic eye movements; in speech perception and control to study early speech acquisition and the emergence of articulatory-acoustic systems; and to model handwriting perception and control.\n\nBayesian program learning has potential applications voice recognition and synthesis, image recognition and natural language processing. It employs the principles of \"compositionality\" (building abstract representations from parts), \"causality\" (building complexity from parts) and \"learning to learn\" (using previously recognized concepts to ease the creation of new concepts).\n\nThe comparison between probabilistic approaches (not only bayesian programming) and possibility theories continues to be debated.\n\nPossibility theories like, for instance, fuzzy sets, fuzzy logic and possibility theory are alternatives to probability to model uncertainty. They argue that probability is insufficient or inconvenient to model certain aspects of incomplete/uncertain knowledge.\n\nThe defense of probability is mainly based on Cox's theorem, which starts from four postulates concerning rational reasoning in the presence of uncertainty. It demonstrates that the only mathematical framework that satisfies these postulates is probability theory. The argument is that any approach other than probability necessarily infringes one of these postulates and the value of that infringement.\n\nThe purpose of probabilistic programming is to unify the scope of classical programming languages with probabilistic modeling (especially bayesian networks) to deal with uncertainty while profiting from the programming languages' expressiveness to encode complexity.\n\nExtended classical programming languages include logical languages as proposed in Probabilistic Horn Abduction, Independent Choice Logic, PRISM, and ProbLog which proposes an extension of Prolog.\n\nIt can also be extensions of functional programming languages (essentially Lisp and Scheme) such as IBAL or CHURCH. The underlying programming languages can be object-oriented as in BLOG and FACTORIE or more standard ones as in CES and FIGARO.\n\nThe purpose of Bayesian programming is different. Jaynes' precept of \"probability as logic\" argues that probability is an extension of and an alternative to logic above which a complete theory of rationality, computation and programming can be rebuilt. Bayesian programming attempts to replace classical languages with a programming approach based on probability that considers incompleteness and uncertainty.\n\nThe precise comparison between the semantics and power of expression of Bayesian and probabilistic programming is an open question.\n\n"}
{"id": "15705885", "url": "https://en.wikipedia.org/wiki?curid=15705885", "title": "Beltoon", "text": "Beltoon\n\nMomin Khan Biltoon (; ) was an Afghan Pashtun singer from Kabul Province, Afghanistan. He was born in Chakari village of Khaki Jabbar District, Kabul. However, he spent most of his life in Logar Province of Afghanistan. Biltoon sang in both Pashto and Dari languages. His style of music reflects the Kharabat style of Kabul. Biltoon's father died before he was born, and his mother did so when he was young. He was raised by his sister in Logar Province of Afghanistan. Biltoon learned the rubab and tanbur at a young age. He started singing at the age of 15. His first song was in both Persian and Pashto.\n\nHaving made contributions to Afghan traditional music for over 70 years, Biltoon died at the age of 95 in Kabul. He was buried in Beni Hesar of Kabul.\none of true legend in pashto music era \n"}
{"id": "4035", "url": "https://en.wikipedia.org/wiki?curid=4035", "title": "Black", "text": "Black\n\nBlack is the darkest color, the result of the absence or complete absorption of visible light. It is an achromatic color, literally a color without hue, like white and gray. It is often used symbolically or figuratively to represent darkness, while white represents light.\n\nBlack ink is the most common color used for printing books, newspapers and documents, because it has the highest contrast with white paper and is the easiest to read. For the same reason, black text on a white screen is the most common format used on computer screens.\nIn color printing it is used along with the subtractive primaries cyan, yellow, and magenta, in order to help produce the darkest colors.\n\nBlack and white have often been used to describe opposites; particularly truth and ignorance, good and evil, the Dark Ages versus Age of Enlightenment. Since the Middle Ages, black has been the symbolic color of solemnity and authority, and for this reason is still commonly worn by judges and magistrates.\n\nBlack was one of the first colors used by artists in neolithic cave paintings. In the 14th century, it began to be worn by royalty, the clergy, judges and government officials in much of Europe. It became the color worn by English romantic poets, businessmen and statesmen in the 19th century, and a high fashion color in the 20th century.\n\nIn the Roman Empire, it became the color of mourning, and over the centuries it was frequently associated with death, evil, witches and magic. According to surveys in Europe and North America, it is the color most commonly associated with mourning, the end, secrets, magic, force, violence, evil, and elegance.\nThe word \"black\" comes from Old English \"blæc\" (\"black, dark\", \"also\", \"ink\"), from Proto-Germanic *\"blakkaz\" (\"burned\"), from Proto-Indo-European *\"bhleg-\" (\"to burn, gleam, shine, flash\"), from base *\"bhel-\" (\"to shine\"), related to Old Saxon \"blak\" (\"ink\"), Old High German \"blach\" (\"black\"), Old Norse \"blakkr\" (\"dark\"), Dutch \"blaken\" (\"to burn\"), and Swedish \"bläck\" (\"ink\"). More distant cognates include Latin \"flagrare\" (\"to blaze, glow, burn\"), and Ancient Greek \"phlegein\" (\"to burn, scorch\").\n\nThe Ancient Greeks sometimes used the same word to name different colors, if they had the same intensity. \"Kuanos\"' could mean both dark blue and black.\n\nThe Ancient Romans had two words for black: \"ater\" was a flat, dull black, while \"niger\" was a brilliant, saturated black. \"Ater\" has vanished from the vocabulary, but \"niger\" was the source of the country name \"Nigeria\" the English word \"Negro\" and the word for \"black\" in most modern Romance languages (French: \"noir\"; Spanish and Portuguese: \"negro\"; Italian: \"nero\" ).\n\nOld High German also had two words for black: \"swartz\" for dull black and \"blach\" for a luminous black. These are parallelled in Middle English by the terms \"swart\" for dull black and \"blaek\" for luminous black. \"Swart\" still survives as the word \"swarthy\", while \"blaek\" became the modern English \"black\".\n\nIn heraldry, the word used for the black color is sable, named for the black fur of the sable, an animal.\n\nBlack was one of the first colors used in art. The Lascaux Cave in France contains drawings of bulls and other animals drawn by paleolithic artists between 18,000 and 17,000 years ago. They began by using charcoal, and then made more vivid black pigments by burning bones or grinding a powder of manganese oxide.\n\nFor the ancient Egyptians, black had positive associations; being the color of fertility and the rich black soil flooded by the Nile. It was the color of Anubis, the god of the underworld, who took the form of a black jackal, and offered protection against evil to the dead.\n\nFor the ancient Greeks, black was also the color of the underworld, separated from the world of the living by the river Acheron, whose water was black. Those who had committed the worst sins were sent to Tartarus, the deepest and darkest level. In the center was the palace of Hades, the king of the underworld, where he was seated upon a black ebony throne.\n\nBlack was one of the most important colors used by ancient Greek artists. In the 6th century BC, they began making black-figure pottery and later red figure pottery, using a highly original technique. In black-figure pottery, the artist would paint figures with a glossy clay slip on a red clay pot. When the pot was fired, the figures painted with the slip would turn black, against a red background. Later they reversed the process, painting the spaces between the figures with slip. This created magnificent red figures against a glossy black background.\n\nIn the social hierarchy of ancient Rome, purple was the color reserved for the Emperor; red was the color worn by soldiers (red cloaks for the officers, red tunics for the soldiers); white the color worn by the priests, and black was worn by craftsmen and artisans. The black they wore was not deep and rich; the vegetable dyes used to make black were not solid or lasting, so the blacks often turned out faded gray or brown.\n\nIn Latin, the word for black, \"ater\" and to darken, \"atere\", were associated with cruelty, brutality and evil. They were the root of the English words \"atrocious\" and \"atrocity\".\n\nBlack was also the Roman color of death and mourning. In the 2nd century BC Roman magistrates began to wear a dark toga, called a \"toga pulla\", to funeral ceremonies. Later, under the Empire, the family of the deceased also wore dark colors for a long period; then, after a banquet to mark the end of mourning, exchanged the black for a white toga. In Roman poetry, death was called the \"hora nigra\", the black hour.\n\nThe German and Scandinavian peoples worshipped their own goddess of the night, Nótt, who crossed the sky in a chariot drawn by a black horse. They also feared Hel, the goddess of the kingdom of the dead, whose skin was black on one side and red on the other. They also held sacred the raven. They believed that Odin, the king of the Nordic pantheon, had two black ravens, Huginn and Muninn, who served as his agents, traveling the world for him, watching and listening.\nIn the early Middle Ages, black was commonly associated with darkness and evil. In Medieval paintings, the devil was usually depicted as having human form, but with wings and black skin or hair.\n\nIn fashion, black did not have the prestige of red, the color of the nobility. It was worn by Benedictine monks as a sign of humility and penitence. In the 12th century a famous theological dispute broke out between the Cistercian monks, who wore white, and the Benedictines, who wore black. A Benedictine abbot, Pierre the Venerable, accused the Cistercians of excessive pride in wearing white instead of black. Saint Bernard of Clairvaux, the founder of the Cistercians responded that black was the color of the devil, hell, \"of death and sin,\" while white represented \"purity, innocence and all the virtues\".\n\nBlack symbolized both power and secrecy in the medieval world. The emblem of the Holy Roman Empire of Germany was a black eagle. The black knight in the poetry of the Middle Ages was an enigmatic figure, hiding his identity, usually wrapped in secrecy.\n\nBlack ink, invented in China, was traditionally used in the Middle Ages for writing, for the simple reason that black was the darkest color and therefore provided the greatest contrast with white paper or parchment, making it the easiest color to read. It became even more important in the 15th century, with the invention of printing. A new kind of ink, printer's ink, was created out of soot, turpentine and walnut oil. The new ink made it possible to spread ideas to a mass audience through printed books, and to popularize art through black and white engravings and prints. Because of its contrast and clarity, black ink on white paper continued to be the standard for printing books, newspapers and documents; and for the same reason black text on a white background is the most common format used on computer screens.\n\nIn the early Middle Ages, princes, nobles and the wealthy usually wore bright colors, particularly scarlet cloaks from Italy. Black was rarely part of the wardrobe of a noble family. The one exception was the fur of the sable. This glossy black fur, from an animal of the marten family, was the finest and most expensive fur in Europe. It was imported from Russia and Poland and used to trim the robes and gowns of royalty.\n\nIn the 14th century, the status of black began to change. First, high-quality black dyes began to arrive on the market, allowing garments of a deep, rich black. Magistrates and government officials began to wear black robes, as a sign of the importance and seriousness of their positions. A third reason was the passage of sumptuary laws in some parts of Europe which prohibited the wearing of costly clothes and certain colors by anyone except members of the nobility. The famous bright scarlet cloaks from Venice and the peacock blue fabrics from Florence were restricted to the nobility. The wealthy bankers and merchants of northern Italy responded by changing to black robes and gowns, made with the most expensive fabrics.\n\nThe change to the more austere but elegant black was quickly picked up by the kings and nobility. It began in northern Italy, where the Duke of Milan and the Count of Savoy and the rulers of Mantua, Ferrara, Rimini and Urbino began to dress in black. It then spread to France, led by Louis I, Duke of Orleans, younger brother of King Charles VI of France. It moved to England at the end of the reign of King Richard II (1377–1399), where all the court began to wear black. In 1419–20, black became the color of the powerful Duke of Burgundy, Philip the Good. It moved to Spain, where it became the color of the Spanish Habsburgs, of Charles V and of his son, Philip II of Spain (1527–1598). European rulers saw it as the color of power, dignity, humility and temperance. By the end of the 16th century, it was the color worn by almost all the monarchs of Europe and their courts.\n\nWhile black was the color worn by the Catholic rulers of Europe, it was also the emblematic color of the Protestant Reformation in Europe and the Puritans in England and America. John Calvin, Philip Melanchthon and other Protestant theologians denounced the richly colored and decorated interiors of Roman Catholic churches. They saw the color red, worn by the Pope and his Cardinals, as the color of luxury, sin, and human folly. In some northern European cities, mobs attacked churches and cathedrals, smashed the stained glass windows and defaced the statues and decoration. In Protestant doctrine, clothing was required to be sober, simple and discreet. Bright colors were banished and replaced by blacks, browns and grays; women and children were recommended to wear white.\n\nIn the Protestant Netherlands, Rembrandt used this sober new palette of blacks and browns to create portraits whose faces emerged from the shadows expressing the deepest human emotions. The Catholic painters of the Counter-Reformation, like Rubens, went in the opposite direction; they filled their paintings with bright and rich colors. The new Baroque churches of the Counter-Reformation were usually shining white inside and filled with statues, frescoes, marble, gold and colorful paintings, to appeal to the public. But European Catholics of all classes, like Protestants, eventually adopted a sober wardrobe that was mostly black, brown and gray.\nIn the second part of the 17th century, Europe and America experienced an epidemic of fear of witchcraft. People widely believed that the devil appeared at midnight in a ceremony called a Black Mass or black sabbath, usually in the form of a black animal, often a goat, a dog, a wolf, a bear, a deer or a rooster, accompanied by their familiar spirits, black cats, serpents and other black creatures. This was the origin of the widespread superstition about black cats and other black animals. In medieval Flanders, in a ceremony called \"Kattenstoet,\" black cats were thrown from the belfry of the Cloth Hall of Ypres to ward off witchcraft.\n\nWitch trials were common in both Europe and America during this period. During the notorious Salem witch trials in New England in 1692–93, one of those on trial was accused of being able turn into a \"black thing with a blue cap,\" and others of having familiars in the form of a black dog, a black cat and a black bird. Nineteen women and men were hanged as witches.\nIn the 18th century, during the European Age of Enlightenment, black receded as a fashion color. Paris became the fashion capital, and pastels, blues, greens, yellow and white became the colors of the nobility and upper classes. But after the French Revolution, black again became the dominant color.\n\nBlack was the color of the industrial revolution, largely fueled by coal, and later by oil. Thanks to coal smoke, the buildings of the large cities of Europe and America gradually turned black. By 1846 the industrial area of the West Midlands of England was \"commonly called 'the Black Country'”. Charles Dickens and other writers described the dark streets and smoky skies of London, and they were vividly illustrated in the engravings of French artist Gustave Doré.\n\nA different kind of black was an important part of the romantic movement in literature. Black was the color of melancholy, the dominant theme of romanticism. The novels of the period were filled with castles, ruins, dungeons, storms, and meetings at midnight. The leading poets of the movement were usually portrayed dressed in black, usually with a white shirt and open collar, and a scarf carelessly over their shoulder, Percy Bysshe Shelley and Lord Byron helped create the enduring stereotype of the romantic poet.\n\nThe invention of new, inexpensive synthetic black dyes and the industrialization of the textile industry meant that good-quality black clothes were available for the first time to the general population. In the 19th century gradually black became the most popular color of business dress of the upper and middle classes in England, the Continent, and America.\n\nBlack dominated literature and fashion in the 19th century, and played a large role in painting. James McNeil Whistler made the color the subject of his most famous painting, \"Arrangement in grey and black number one\" (1871), better known as \"Whistler's Mother\".\n\nSome 19th-century French painters had a low opinion of black: \"Reject black,\" Paul Gauguin said, \"and that mix of black and white they call gray. Nothing is black, nothing is gray.\" But Édouard Manet used blacks for their strength and dramatic effect. Manet's portrait of painter Berthe Morisot was a study in black which perfectly captured her spirit of independence. The black gave the painting power and immediacy; he even changed her eyes, which were green, to black to strengthen the effect. Henri Matisse quoted the French impressionist Pissarro telling him, \"Manet is stronger than us all – he made light with black.\"\n\nPierre-Auguste Renoir used luminous blacks, especially in his portraits. When someone told him that black was not a color, Renoir replied: \"What makes you think that? Black is the queen of colors. I always detested Prussian blue. I tried to replace black with a mixture of red and blue, I tried using cobalt blue or ultramarine, but I always came back to ivory black.\"\n\nVincent van Gogh used black lines to outline many of the objects in his paintings, such as the bed in the famous painting of his bedroom. making them stand apart. His painting of black crows over a cornfield, painted shortly before he died, was particularly agitated and haunting.\n\nIn the late 19th century, black also became the color of anarchism. (See the section political movements.)\n\nIn the 20th century, black was the color of Italian and German fascism. (See the section political movements.)\n\nIn art, black regained some of the territory that it had lost during the 19th century. The Russian painter Kasimir Malevich, a member of the Suprematist movement, created the \"Black Square\" in 1915, is widely considered the first purely abstract painting. He wrote, \"The painted work is no longer simply the imitation of reality, but is this very reality ... It is not a demonstration of ability, but the materialization of an idea.\"\n\nBlack was also appreciated by Henri Matisse. \"When I didn't know what color to put down, I put down black,\" he said in 1945. \"Black is a force: I used black as ballast to simplify the construction ... Since the impressionists it seems to have made continuous progress, taking a more and more important part in color orchestration, comparable to that of the double bass as a solo instrument.\"\n\nIn the 1950s, black came to be a symbol of individuality and intellectual and social rebellion, the color of those who didn't accept established norms and values. In Paris, it was worn by Left-Bank intellectuals and performers such as Juliette Greco, and by some members of the Beat Movement in New York and San Francisco. Black leather jackets were worn by motorcycle gangs such as the Hells Angels and street gangs on the fringes of society in the United States. Black as a color of rebellion was celebrated in such films as \"The Wild One\", with Marlon Brando. By the end of the 20th century, black was the emblematic color of the punk subculture punk fashion, and the goth subculture. Goth fashion, which emerged in England in the 1980s, was inspired by Victorian era mourning dress.\n\nIn men's fashion, black gradually ceded its dominance to navy blue, particularly in business suits. Black evening dress and formal dress in general were worn less and less. In 1960, John F. Kennedy was the last American President to be inaugurated wearing formal dress; President Lyndon Johnson and all his successors were inaugurated wearing business suits.\n\nWomen's fashion was revolutionized and simplified in 1926 by the French designer Coco Chanel, who published a drawing of a simple black dress in \"Vogue\" magazine. She famously said, \"A woman needs just three things; a black dress, a black sweater, and, on her arm, a man she loves.\" Other designers contributed to the trend of the little black dress. The Italian designer Gianni Versace said, \"Black is the quintessence of simplicity and elegance,\" and French designer Yves Saint Laurent said, \"black is the liaison which connects art and fashion. One of the most famous black dresses of the century was designed by Hubert de Givenchy and was worn by Audrey Hepburn in the 1961 film \"Breakfast at Tiffany's\".\n\nThe American civil rights movement in the 1950s was a struggle for the political equality of African Americans. It developed into the Black Power movement in the late 1960s and 1970s, and popularized the slogan \"Black is Beautiful\".\n\nIn the 1990s, the Black Standard became the banner of several Islamic extremist, jihadist groups. (See the section political movements.)\n\nIn the visible spectrum, black is the absorption of all colors.\n\nBlack can be defined as the visual impression experienced when no visible light reaches the eye. Pigments or dyes that absorb light rather than reflect it back to the eye \"look black\". A black pigment can, however, result from a \"combination\" of several pigments that collectively absorb all colors. If appropriate proportions of three primary pigments are mixed, the result reflects so little light as to be called \"black\".\n\nThis provides two superficially opposite but actually complementary descriptions of black. Black is the absorption of all colors of light, or an exhaustive combination of multiple colors of pigment. See also primary colors.\n\nIn physics, a black body is a perfect absorber of light, but, by a thermodynamic rule, it is also the best emitter. Thus, the best radiative cooling, out of sunlight, is by using black paint, though it is important that it be black (a nearly perfect absorber) in the infrared as well.\n\nIn elementary science, far ultraviolet light is called \"black light\" because, while itself unseen, it causes many minerals and other substances to fluoresce.\n\nOn January 16, 2008, researchers from Troy, New York's Rensselaer Polytechnic Institute announced the creation of the then darkest material on the planet. The material, which reflected only 0.045 percent of light, was created from carbon nanotubes stood on end. This is 1/30 of the light reflected by the current standard for blackness, and one third the light reflected by the previous record holder for darkest substance. As of February 2016, the current darkest material known is claimed to be Vantablack.\n\nA material is said to be black if most incoming light is absorbed equally in the material. Light (electromagnetic radiation in the visible spectrum) interacts with the atoms and molecules, which causes the energy of the light to be converted into other forms of energy, usually heat. This means that black surfaces can act as thermal collectors, absorbing light and generating heat (see Solar thermal collector).\n\nAbsorption of light is contrasted by transmission, reflection and diffusion, where the light is only redirected, causing objects to appear transparent, reflective or white respectively.\n\nThe earliest pigments used by Neolithic man were charcoal, red ocher and yellow ocher. The black lines of cave art were drawn with the tips of burnt torches made of a wood with resin.\n\nDifferent charcoal pigments were made by burning different woods and animal products, each of which produced a different tone. The charcoal would be ground and then mixed with animal fat to make the pigment.\n\nThe 15th-century painter Cennino Cennini described how this pigment was made during the Renaissance in his famous handbook for artists: \"...there is a black which is made from the tendrils of vines. And these tendrils need to be burned. And when they have been burned, throw some water onto them and put them out and then mull them in the same way as the other black. And this is a lean and black pigment and is one of the perfect pigments that we use.\"\n\nCennini also noted that \"There is another black which is made from burnt almond shells or peaches and this is a perfect, fine black.\" Similar fine blacks were made by burning the pits of the peach, cherry or apricot. The powdered charcoal was then mixed with gum arabic or the yellow of an egg to make a paint.\n\nDifferent civilizations burned different plants to produce their charcoal pigments. The Inuit of Alaska used wood charcoal mixed with the blood of seals to paint masks and wooden objects. The Polynesians burned coconuts to produce their pigment.\n\nGood-quality black dyes were not known until the middle of the 14th century. The most common early dyes were made from bark, roots or fruits of different trees; usually the walnut, chestnut, or certain oak trees. The blacks produced were often more gray, brown or bluish. The cloth had to be dyed several times to darken the color. One solution used by dyers was add to the dye some iron filings, rich in iron oxide, which gave a deeper black. Another was to first dye the fabric dark blue, and then to dye it black.\n\nA much richer and deeper black dye was eventually found made from the Oak apple or gall-nut. The gall-nut is a small round tumor which grows on oak and other varieties of trees. They range in size from 2–5 cm, and are caused by chemicals injected by the larva of certain kinds of gall wasp in the family Cynipidae. The dye was very expensive; a great quantity of gall-nuts were needed for a very small amount of dye. The gall-nuts which made the best dye came from Poland, eastern Europe, the near east and North Africa. Beginning in about the 14th century, dye from gall-nuts was used for clothes of the kings and princes of Europe.\n\nAnother important source of natural black dyes from the 17th century onwards was the logwood tree, or Haematoxylum campechianum, which also produced reddish and bluish dyes. It is a species of flowering tree in the legume family, Fabaceae, that is native to southern Mexico and northern Central America. The modern nation of Belize grew from 17th century English logwood logging camps.\n\nSince the mid-19th century, synthetic black dyes have largely replaced natural dyes. One of the important synthetic blacks is Nigrosin, a mixture of synthetic black dyes (CI 50415, Solvent black 5) made by heating a mixture of nitrobenzene, aniline and aniline hydrochloride in the presence of a copper or iron catalyst. Its main industrial uses are as a colorant for lacquers and varnishes and in marker-pen inks.\n\nThe first known inks were made by the Chinese, and date back to the 23rd century B.C. They used natural plant dyes and minerals such as graphite ground with water and applied with an ink brush. Early Chinese inks similar to the modern inkstick have been found dating to about 256 BC at the end of the Warring States period. They were produced from soot, usually produced by burning pine wood, mixed with animal glue. To make ink from an inkstick, the stick is continuously ground against an inkstone with a small quantity of water to produce a dark liquid which is then applied with an ink brush. Artists and calligraphists could vary the thickness of the resulting ink by reducing or increasing the intensity and time of ink grinding. These inks produced the delicate shading and subtle or dramatic effects of Chinese brush painting.\n\nIndia ink (or Indian ink in British English) is a black ink once widely used for writing and printing and now more commonly used for drawing, especially when inking comic books and comic strips. The technique of making it probably came from China. India ink has been in use in India since at least the 4th century BC, where it was called \"masi\". In India, the black color of the ink came from bone char, tar, pitch and other substances.\n\nThe Ancient Romans had a black writing ink they called \"atramentum librarium\". Its name came from the Latin word \"atrare\", which meant to make something black. (This was the same root as the English word \"atrocious\".) It was usually made, like India ink, from soot, although one variety, called \"atramentum elephantinum\", was made by burning the ivory of elephants.\n\nGall-nuts were also used for making fine black writing ink. Iron gall ink (also known as iron gall nut ink or oak gall ink) was a purple-black or brown-black ink made from iron salts and tannic acids from gall nut. It was the standard writing and drawing ink in Europe, from about the 12th century to the 19th century, and remained in use well into the 20th century.\n\n\nThe fact that outer space is black is sometimes called Olbers' paradox. In theory, because the universe is full of stars, and is believed to be infinitely large, it would be expected that the light of an infinite number of stars would be enough to brilliantly light the whole universe all the time. However, the background color of outer space is black. This contradiction was first noted in 1823 by German astronomer Heinrich Wilhelm Matthias Olbers, who posed the question of why the night sky was black.\n\nThe current accepted answer is that, although the universe is infinitely large, it is not infinitely old. It is thought to be about 13.8 billion years old, so we can only see objects as far away as the distance light can travel in 13.8 billion years. Light from stars farther away has not reached Earth, and cannot contribute to making the sky bright. Furthermore, as the universe is expanding, many stars are moving away from Earth. As they move, the wavelength of their light becomes longer, through the Doppler effect, and shifts toward red, or even becomes invisible. As a result of these two phenomena, there is not enough starlight to make space anything but black.\n\nThe daytime sky on Earth is blue because light from the Sun strikes molecules in Earth's atmosphere scattering light in all directions. Blue light is scattered more than other colors, and reaches the eye in greater quantities, making the daytime sky appear blue. This is known as Rayleigh scattering.\n\nThe nighttime sky on Earth is black because the part of Earth experiencing night is facing away from the Sun, the light of the Sun is blocked by Earth itself, and there is no other bright nighttime source of light in the vicinity. Thus, there is not enough light to undergo Rayleigh scattering and make the sky blue. On the Moon, on the other hand, because there is no atmosphere to scatter the light, the sky is black both day and night. This phenomenon also holds true for other locations without an atmosphere.\n\nIn China, the color black is associated with water, one of the five fundamental elements believed to compose all things; and with winter, cold, and the direction north, usually symbolized by a black tortoise. It is also associated with disorder, including the positive disorder which leads to change and new life. When the first Emperor of China Qin Shi Huang seized power from the Zhou Dynasty, he changed the Imperial color from red to black, saying that black extinguished red. Only when the Han Dynasty appeared in 206 BC was red restored as the imperial color.\n\nThe Chinese and Japanese character for black (\"kuro\" in Japanese), can, depending upon the context, also mean dark or evil.\n\nIn Japan, black is associated with mystery, the night, the unknown, the supernatural, the invisible and death. Combined with white, it can symbolize intuition.\n\nIn Japan in the 10th and 11th century, it was believed that wearing black could bring misfortune. It was worn at court by those who wanted to set themselves apart from the established powers or who had renounced material possessions.\n\nIn Japan black can also symbolize experience, as opposed to white, which symbolizes naiveté. The black belt in martial arts symbolizes experience, while a white belt is worn by novices. Japanese men traditionally wear a black kimono with some white decoration on their wedding day.\n\nIn Indonesia black is associated with depth, the subterranean world, demons, disaster, and the left hand. When black is combined with white, however, it symbolizes harmony and equilibrium.\n\nAnarchism is a political philosophy, most popular in the late 19th and early 20th centuries, which holds that governments and capitalism are harmful and undesirable. The symbols of anarchism was usually either a black flag or a black letter A. More recently it is usually represented with a bisected red and black flag, to emphasise the movement's socialist roots in the First International. Anarchism was most popular in Spain, France, Italy, Ukraine and Argentina. There were also small but influential movements in the United States and Russia. In the latter, the movement initially allied itself with the Bolsheviks.\n\nThe Black Army was a collection of anarchist military units which fought in the Russian Civil War, sometimes on the side of the Bolshevik Red Army, and sometimes for the opposing White Army. It was officially known as the Revolutionary Insurrectionary Army of Ukraine, and it was under the command of the famous anarchist Nestor Makhno.\n\nFascism. The Blackshirts () were Fascist paramilitary groups in Italy during the period immediately following World War I and until the end of World War II. The Blackshirts were officially known as the Voluntary Militia for National Security (\"Milizia Volontaria per la Sicurezza Nazionale\", or MVSN).\n\nInspired by the black uniforms of the Arditi, Italy's elite storm troops of World War I, the Fascist Blackshirts were organized by Benito Mussolini as the military tool of his political movement. They used violence and intimidation against Mussolini's opponents. The emblem of the Italian fascists was a black flag with fasces, an axe in a bundle of sticks, an ancient Roman symbol of authority. Mussolini came to power in 1922 through his March on Rome with the blackshirts.\n\nBlack was also adopted by Adolf Hitler and the Nazis in Germany. Red, white and black were the colors of the flag of the German Empire from 1870 to 1918. In \"Mein Kampf\", Hitler explained that they were \"revered colors expressive of our homage to the glorious past.\" Hitler also wrote that \"the new flag ... should prove effective as a large poster\" because \"in hundreds of thousands of cases a really striking emblem may be the first cause of awakening interest in a movement.\" The black swastika was meant to symbolize the Aryan race, which, according to the Nazis, \"was always anti-Semitic and will always be anti-Semitic.\" Several designs by a number of different authors were considered, but the one adopted in the end was Hitler's personal design. Black became the color of the uniform of the SS, the \"Schutzstaffel\" or \"defense corps\", the paramilitary wing of the Nazi Party, and was worn by SS officers from 1932 until the end of World War II.\n\nThe Nazis used a black triangle to symbolize anti-social elements. The symbol originates from Nazi concentration camps, where every prisoner had to wear one of the Nazi concentration camp badges on their jacket, the color of which categorized them according to \"their kind.\" Many Black Triangle prisoners were either mentally disabled or mentally ill. The homeless were also included, as were alcoholics, the Romani people, the habitually \"work-shy,\" prostitutes, draft dodgers and pacifists. More recently the black triangle has been adopted as a symbol in lesbian culture and by disabled activists.\n\nBlack shirts were also worn by the British Union of Fascists before World War II, and members of fascist movements in the Netherlands.\n\nPatriotic Resistance. The Lützow Free Corps, composed of volunteer German students and academics fighting against Napoleon in 1813, could not afford to make special uniforms and therefore adopted black, as the only color that could be used to dye their civilian clothing without the original color showing. In 1815 the students began to carry a red, black and gold flag, which they believed (incorrectly) had been the colors of the Holy Roman Empire (the imperial flag had actually been gold and black). In 1848, this banner became the flag of the German confederation. In 1866, Prussia unified Germany under its rule, and imposed the red, white and black of its own flag, which remained the colors of the German flag until the end of the Second World War. In 1949 the Federal Republic of Germany returned to the original flag and colors of the students and professors of 1815, which is the flag of Germany today.\n\nIslamism. The Black Standard ( , also known as \"banner of the eagle\" or simply as \"the banner\") is the historical flag flown by Muhammad in Islamic tradition, an eschatological symbol in Shi'a Islam (heralding the advent of the Mahdi), and a symbol used in Islamism and Jihadism.\n\nBlack has been a traditional color of cavalry and armoured or mechanized troops. German armoured troops (Panzerwaffe) traditionally wore black uniforms, and even in others, a black beret is common. In Finland, black is the symbolic color for both armoured troops and combat engineers, and military units of these specialities have black flags and unit insignia.\n\nThe black beret and the color black is also a symbol of special forces in many countries. Soviet and Russian OMON special police and Russian naval infantry wear a black beret. A black beret is also worn by military police in the Canadian, Czech, Croatian, Portuguese, Spanish and Serbian armies.\n\nThe silver-on-black skull and crossbones symbol or Totenkopf and a black uniform were used by Hussars and Black Brunswickers, the German Panzerwaffe and the Nazi Schutzstaffel, and U.S. 400th Missile Squadron (crossed missiles), and continues in use with the Estonian Kuperjanov Battalion.\n\n\nIn Christianity, the devil is often called the \"prince of darkness.\" The term was used in John Milton's poem \"Paradise Lost\", published in 1667, referring to Satan, who is viewed as the embodiment of evil. It is an English translation of the Latin phrase \"princeps tenebrarum\", which occurs in the \"Acts of Pilate\", written in the fourth century, in the 11th-century hymn \"Rhythmus de die mortis\" by Pietro Damiani, and in a sermon by Bernard of Clairvaux from the 12th century. The phrase also occurs in \"King Lear\" by William Shakespeare (c. 1606), Act III, Scene IV, l. 14:\n'The prince of darkness is a gentleman.\"\n\nPriests and pastors of the Roman Catholic, Eastern Orthodox and Protestant churches commonly wear black, as do monks of the Benedictine Order, who consider it the color of humility and penitence.\n\n\n\nIn Europe and America, black is the color most commonly associated with mourning and bereavement. It is the color traditionally worn at funerals and memorial services. In some traditional societies, for example in Greece and Italy, some widows wear black for the rest of their lives. In contrast, across much of Africa and parts of Asia like Vietnam, white is a color of mourning and is worn during funerals.\n\nIn Victorian England, the colors and fabrics of mourning were specified in an unofficial dress code: \"non-reflective black paramatta and crape for the first year of deepest mourning, followed by nine months of dullish black silk, heavily trimmed with crape, and then three months when crape was discarded. Paramatta was a fabric of combined silk and wool or cotton; crape was a harsh black silk fabric with a crimped appearance produced by heat. Widows were allowed to change into the colors of half-mourning, such as gray and lavender, black and white, for the final six months.\"\n\nA \"black day\" (or week or month) usually refers to tragic date. The Romans marked \"fasti\" days with white stones and \"nefasti\" days with black. The term is often used to remember massacres. Black months include the Black September in Jordan, when large numbers of Palestinians were killed, and Black July in Sri Lanka, the killing of members of the Tamil population by the Sinhalese government.\n\nIn the financial world, the term often refers to a dramatic drop in the stock market. For example, the Wall Street Crash of 1929, the stock market crash on October 29, 1929, which marked the start of the Great Depression, is nicknamed Black Tuesday, and was preceded by Black Thursday, a downturn on October 24 the previous week.\nIn western popular culture, black has long been associated with evil and darkness. It is the traditional color of witchcraft and black magic.\n\nIn the Book of Revelation, the last book in the New Testament of the Bible, the Four Horsemen of the Apocalypse are supposed to announce the Apocalypse before the Last Judgment. The horseman representing famine rides a black horse.\n\nThe vampire of literature and films, such as Count Dracula of the Bram Stoker novel, dressed in black, and could only move at night. The Wicked Witch of the West in the 1939 film \"The Wizard of Oz\" became the archetype of witches for generations of children. Whereas witches and sorcerers inspired real fear in the 17th century, in the 21st century children and adults dressed as witches for Halloween parties and parades.\n\nBlack is frequently used as a color of power, law and authority. In many countries judges and magistrates wear black robes. That custom began in Europe in the 13th and 14th centuries. Jurists, magistrates and certain other court officials in France began to wear long black robes during the reign of Philip IV of France (1285–1314), and in England from the time of Edward I (1271–1307). The custom spread to the cities of Italy at about the same time, between 1300 and 1320. The robes of judges resembled those worn by the clergy, and represented the law and authority of the King, while those of the clergy represented the law of God and authority of the church.\n\nUntil the 20th century most police uniforms were black, until they were largely replaced by a less menacing blue in France, the U.S. and other countries. In the United States, police cars are frequently Black and white. The riot control units of the Basque Autonomous Police in Spain are known as \"beltzak\" (\"blacks\") after their uniform.\n\nBlack today is the most common color for limousines and the official cars of government officials.\n\nBlack evening dress is still worn at many solemn occasions or ceremonies, from graduations to formal balls. Graduation gowns are copied from the gowns worn by university professors in the Middle Ages, which in turn were copied from the robes worn by judges and priests, who often taught at the early universities. The mortarboard hat worn by graduates is adapted from a square cap called a biretta worn by Medieval professors and clerics\n\nIn the 19th and 20th centuries, many machines and devices, large and small, were painted black, to stress their functionality. These included telephones, sewing machines, steamships, railroad locomotives, and automobiles. The Ford Model T, the first mass-produced car, was available only in black from 1914 to 1926. Of means of transportation, only airplanes were rarely ever painted black.\nBlack house paint is becoming more popular with Sherwin-Williams reporting that the color, Tricorn Black, was the 6th most popular exterior house paint color in Canada and the 12th most popular paint in the United States in 2018.\n\n\nBlack is also commonly used as a racial description in the United Kingdom, since ethnicity was first measured in the 2001 census. The 2011 British census asked residents to describe themselves, and categories offered included Black, African, Caribbean, or Black British. Other possible categories were African British, African Scottish, Caribbean British and Caribbean Scottish. Of the total UK population in 2001, 1.0 percent identified themselves as Black Caribbean, 0.8 percent as Black African, and 0.2 percent as Black (others).\n\nIn Canada, census respondents can identify themselves as Black. In the 2006 census, 2.5 percent of the population identified themselves as black.\n\nIn Australia, the term black is not used in the census. In the 2006 census, 2.3 percent of Australians identified themselves as Aboriginal and/or Torres Strait Islanders.\n\nIn Brazil, the Brazilian Institute of Geography and Statistics (IBGE) asks people to identify themselves as \"branco\" (white), \"pardo\" (brown), \"preto\" (black), or \"amarelo\" (yellow). In 2008 6.8 percent of the population identified themselves as \"preto\".\n\nBlack is commonly associated with secrecy.\n\nBlack is the color most commonly associated with elegance in Europe and the United States, followed by silver, gold, and white.\n\nBlack first became a fashionable color for men in Europe in the 17th century, in the courts of Italy and Spain. (See history above.) In the 19th century, it was the fashion for men both in business and for evening wear, in the form of a black coat whose tails came down the knees. In the evening it was the custom of the men to leave the women after dinner to go to a special smoking room to enjoy cigars or cigarettes. This meant that their tailcoats eventually smelled of tobacco. According to the legend, in 1865 Edward VII, then the Prince of Wales, had his tailor make a special short smoking jacket. The smoking jacket then evolved into the dinner jacket. Again according to legend, the first Americans to wear the jacket were members of the Tuxedo Club in New York State. Thereafter the jacket became known as a tuxedo in the U.S. The term \"smoking\" is still used today in Russia and other countries.\nThe tuxedo was always black until the 1930s, when the Duke of Windsor began to wear a tuxedo that was a very dark midnight blue. He did so because a black tuxedo looked greenish in artificial light, while a dark blue tuxedo looked blacker than black itself.\n\nFor women's fashion, the defining moment was the invention of the simple black dress by Coco Chanel in 1926. (See history.) Thereafter, a long black gown was used for formal occasions, while the simple black dress could be used for everything else. The designer Karl Lagerfeld, explaining why black was so popular, said: \"Black is the color that goes with everything. If you're wearing black, you're on sure ground.\" Skirts have gone up and down and fashions have changed, but the black dress has not lost its position as the essential element of a woman's wardrobe. The fashion designer Christian Dior said, \"elegance is a combination of distinction, naturalness, care and simplicity,\" and black exemplified elegance.\n\nThe expression \"X is the new black\" is a reference to the latest trend or fad that is considered a wardrobe basic for the duration of the trend, on the basis that black is always fashionable. The phrase has taken on a life of its own and has become a cliché.\n\nMany performers of both popular and European classical music, including French singers Edith Piaf and Juliette Greco, and violinist Joshua Bell have traditionally worn black on stage during performances. A black costume was usually chosen as part of their image or stage persona, or because it did not distract from the music, or sometimes for a political reason. Country-western singer Johnny Cash always wore black on stage. In 1971, Cash wrote the song \"Man in Black\" to explain why he dressed in that color: \"We're doing mighty fine I do suppose / In our streak of lightning cars and fancy clothes / But just so we're reminded of the ones who are held back / Up front there ought to be a man in black.\"\n\n"}
{"id": "19344418", "url": "https://en.wikipedia.org/wiki?curid=19344418", "title": "Bovine spongiform encephalopathy", "text": "Bovine spongiform encephalopathy\n\nBovine spongiform encephalopathy (BSE), commonly known as mad cow disease, is a neurodegenerative disease of cattle. Symptoms include abnormal behavior, trouble walking, and weight loss. Later in the course the cow becomes unable to move. The time between infection and onset of symptoms is generally four to five years. Time from onset of symptoms to death is generally weeks to months. Spread to humans is believed to result in variant Creutzfeldt–Jakob disease (vCJD). As of 2018 a total of 231 cases of vCJD have been reported globally.\nBSE is due to an infection by a misfolded protein, known as a prion. Cattle are believed to have been infected by being fed meat-and-bone meal (MBM) that contained the remains of other cattle who spontaneously developed the disease or scrapie-infected sheep products. The outbreak increased throughout the United Kingdom due to the practice of feeding meat-and-bone meal to young calves of dairy cows. Cases are suspected based on symptoms and confirmed by examination of the brain. Cases are classified as classic or atypical, with the latter divided into H- and L types.\nEfforts to prevent the disease in the UK include not allowing any animal older than 30 months to enter either the human food or animal feed supply. In Europe all cattle over 30 month must be tested if they will become human food. In North America tissue of concern, known as specified risk material, may not be added to animal feed or pet food. About 4.4 million cows were killed during the eradication program in the UK.\nFour cases were reported globally in 2017 and the condition has been deemed to be nearly eradicated. In the United Kingdom, from 1986 to 2015, more than 184,000 cattle were diagnosed with the peak of new cases occurring in 1993. A few thousand additional cases have been reported in other regions of the world. It is believed that a few million cattle with the condition likely entered the food supply during the outbreak. \n\nSymptoms are not seen immediately in cattle due to the disease's extremely long incubation period. Some cattle have been observed to have an abnormal gait, changes in behavior, tremors and hyper-responsiveness to certain stimuli. Hindlimb ataxia affects the animal’s gait and occurs when muscle control is lost. This results in poor balance and coordination. Behavioural changes may include aggression, anxiety relating to certain situations, nervousness, frenzy or an overall change in temperament. Some rare but previously observed symptoms also include persistent pacing, rubbing or licking. Additionally, nonspecific symptoms have also been observed which include weight loss, decreased milk production, lameness, ear infections and teeth grinding due to pain. Some animals may show a combination of these symptoms, while others may only be observed demonstrating one of the many reported. Once clinical symptoms arise, they typically get worse over the upcoming weeks and months, eventually leading to recumbency, coma and death.\n\nBSE is due to an infection by a misfolded protein, known as a prion. Cattle are believed to have been infected from being feed meat and bone meal (MBM) that contained the remains of other cattle who spontaneously developed the disease or scrapie-infected sheep products. The outbreak increased throughout the United Kingdom due to the practice of feeding meat-and-bone meal to young calves of dairy cows.\n\nPrions replicate by causing other normally folded proteins of the same type to take on their misfolded shape, which then go on to do the same, leading to an exponential chain reaction. Eventually, the prions aggregate into an alpha helical, beta pleated sheet, which is thought to be toxic to brain cells.\n\nThe agent is not destroyed even if the beef or material containing it is cooked or heat-treated. Transmission can occur when healthy animals come in contact with tainted tissues from others with the disease. In the brain, the agent causes native cellular prion protein to deform into the misfolded state, which then goes on to deform further prion protein in an exponential cascade. This results in protein aggregates, which then form dense plaque fibers. Brain cells begin to die off in massive numbers, eventually leading to the microscopic appearance of \"holes\" in the brain, degeneration of physical and mental abilities, and ultimately death.\n\nThe British Government enquiry took the view the cause was not scrapie, as had originally been postulated, but was some event in the 1970s that was not possible to identify.\n\nSpread to humans is believed to result in variant Creutzfeldt–Jakob disease (vCJD). The agent can be transmitted to humans by eating food contaminated with it. The highest risk to humans is believed to be from eating food contaminated with the brain, spinal cord, or digestive tract though any tissue may be involved.<ref name=\"FDA/CFSAN\">\n</ref>\n\nThe pathogenesis of BSE is not well understood or documented like other diseases of this nature. Even though BSE is a disease that results in neurological defects, its pathogenesis occurs in areas that reside outside of the nervous system. There was a strong deposition of PrP initially located in the Ileal Peyer’s patches of the small intestine. The lymphatic system has been identified in the pathogenesis of scrapies. It has not, however, been determined to be an essential part of the pathogenesis of BSE. The Ileal Peyer’s patches have been the only organ from this system that has been found to play a major role in the pathogenesis. Infectivity of the Ileal Peyer's patches has been observed as early as 4 months after inoculation. PrP accumulation was found to occur mostly in tangible body macrophages of the Ileal Peyer’s patches. Tangible body macrophages involved in PrP clearance are thought to play a role in PrP accumulation in the Peyer’s patches. Accumulation of PrP was also found in follicular dendritic cells; however, it was of a lesser degree. Six months after inoculation, there was no infectivity in any tissues, only that of the ileum. This led researchers to believe that the disease agent replicates here. In naturally confirmed cases, there have been no reports of infectivity in the Ileal Peyer’s patches. Generally, in clinical experiments, high doses of the disease are administered. In natural cases, it was hypothesized that low doses of the agent were present, and therefore, infectivity could not be observed.\n\nDiagnosis of BSE continues to be a practical problem. It has an incubation period of months to years, during which no symptoms are noticed, though the pathway of converting the normal brain prion protein (PrP) into the toxic, disease-related PrP form has started. At present, virtually no way is known to detect PrP reliably except by examining \"post mortem\" brain tissue using neuropathological and immunohistochemical methods. Accumulation of the abnormally folded PrP form of PrP is a characteristic of the disease, but it is present at very low levels in easily accessible body fluids such as blood or urine. Researchers have tried to develop methods to measure PrP, but no methods for use in materials such as blood have been accepted fully.\n\nThe traditional method of diagnosis relies on histopathological examination of the medulla oblongata of the brain, and other tissues, \"post mortem\". Immunohistochemistry can be used to demonstrate prion protein accumulation.\n\nIn 2010, a team from New York described detection of PrP even when initially present at only one part in a hundred billion (10) in brain tissue. The method combines amplification with a novel technology called surround optical fiber immunoassay and some specific antibodies against PrP. After amplifying and then concentrating any PrP, the samples are labelled with a fluorescent dye using an antibody for specificity and then finally loaded into a microcapillary tube. This tube is placed in a specially constructed apparatus so it is totally surrounded by optical fibres to capture all light emitted once the dye is excited using a laser. The technique allowed detection of PrP after many fewer cycles of conversion than others have achieved, substantially reducing the possibility of artifacts, as well as speeding up the assay. The researchers also tested their method on blood samples from apparently healthy sheep that went on to develop scrapie. The animals’ brains were analysed once any symptoms became apparent. The researchers could, therefore, compare results from brain tissue and blood taken once the animals exhibited symptoms of the diseases, with blood obtained earlier in the animals’ lives, and from uninfected animals. The results showed very clearly that PrP could be detected in the blood of animals long before the symptoms appeared. After further development and testing, this method could be of great value in surveillance as a blood- or urine-based screening test for BSE.\n\nBSE is a transmissible disease that primarily affects the central nervous system; it is a form of transmissible spongiform encephalopathy, like Creutzfeldt–Jakob disease and kuru in humans and scrapie in sheep, and chronic wasting disease in deer.\n\nA ban on feeding meat and bone meal to cattle has resulted in a strong reduction in cases in countries where the disease has been present. In disease-free countries, control relies on import control, feeding regulations, and surveillance measures.\n\nIn UK and US slaughterhouses, the brain, spinal cord, trigeminal ganglia, intestines, eyes, and tonsils from cattle are classified as specified risk materials, and must be disposed of appropriately.\n\nAn enhanced BSE-related feed ban is in effect in both the United States and Canada to help improve prevention and elimination of BSE.\n\nThe tests used for detecting BSE vary considerably, as do the regulations in various jurisdictions for when, and which cattle, must be tested. For instance in the EU, the cattle tested are older (30 months or older), while many cattle are slaughtered younger than that. At the opposite end of the scale, Japan tests all cattle at the time of slaughter. Tests are also difficult, as the altered prion protein has very low levels in blood or urine, and no other signal has been found. Newer tests are faster, more sensitive, and cheaper, so future figures possibly may be more comprehensive. Even so, currently the only reliable test is examination of tissues during a necropsy.\n\nAs for vCJD in humans, autopsy tests are not always done, so those figures, too, are likely to be too low, but probably by a lesser fraction. In the United Kingdom, anyone with possible vCJD symptoms must be reported to the Creutzfeldt–Jakob Disease Surveillance Unit. In the United States, the CDC has refused to impose a national requirement that physicians and hospitals report cases of the disease. Instead, the agency relies on other methods, including death certificates and urging physicians to send suspicious cases to the National Prion Disease Pathology Surveillance Center (NPDPSC) at Case Western Reserve University in Cleveland, which is funded by the CDC.\n\nTo control potential transmission of vCJD within the United States, the American Red Cross has established strict restrictions on individuals' eligibility to donate blood. Individuals who have spent a cumulative time of 3 months or more in the United Kingdom between 1980 and 1996, or a cumulative time of 5 years or more from 1980 to present in any combination of countries in Europe, are prohibited from donating blood.\n\nThe first reported case in North America was in December 1993 from Alberta, Canada. Another Canadian case was reported in May 2003. The first known U.S. occurrence came in December of the same year, though it was later confirmed to be a cow of Canadian origin imported to the U.S. The cow was slaughtered on a farm near Yakima, Washington. The cow was included in the United States Department of Agriculture's surveillance program, specifically targeting cattle with BSE. Canada announced two additional cases of BSE from Alberta in early 2005.\nIn June 2005, John R. Clifford, chief veterinary officer for the United States Department of Agriculture animal health inspection service, confirmed a fully domestic case of BSE in Texas.\n\nSoybean meal is cheap and plentiful in the United States, and cottonseed meal (1.5 million tons of which are produced in the U.S. every year, none of which is suitable for humans or any other simple-stomach animals) is even cheaper than soybean meal. Historically, meat and bone meal, blood meal, and meat scraps have almost always commanded a higher price as a feed additive than oilseed meals in the U.S., so not much incentive existed to use animal products to feed ruminants. As a result, the use of animal byproduct feeds was never common, as it was in Europe. However, U.S. regulations only partially prohibited the use of animal byproducts in feed. In 1997, regulations prohibited the feeding of mammalian byproducts to ruminants such as cattle and goats. However, the byproducts of ruminants can still be legally fed to pets or other livestock, including pigs and poultry. In addition, it is legal for ruminants to be fed byproducts from some of these animals. Because of this, some authors have suggested that under certain conditions, it is still possible for BSE incidence to increase in U.S. cattle. \n\nIn February 2001, the US Government Accountability Office reported the FDA, which is responsible for regulating feed, had not adequately enforced the various bans. Compliance with the regulations was shown to be extremely poor before the discovery of a cow in Washington infected with BSE in 2003, but industry representatives report that compliance is now total. Even so, critics call the partial prohibitions insufficient. Indeed, US meat producer Creekstone Farms was forcibly prevented from conducting BSE testing by the USDA, which under an obscure 1913 law had the authority to restrict sales of BSE testing kits, allegedly to protect other producers from being forced to conduct the same tests to stay competitive.\n\nThe USDA has issued recalls of beef supplies that involved introduction of downer cows into the food supply. Hallmark/Westland Meat Packing Company was found to have used electric shocks to prod downer cows into the slaughtering system in 2007.\nPossibly due to pressure from large agribusiness, the United States has drastically cut back on the number of cows inspected for BSE.\n\nJapan was the top importer of US beef, buying 240,000 tons valued at $1.4 billion in 2003. After the discovery of the first case of BSE in the US on 23 December 2003, Japan halted US beef imports. In December 2005, Japan once again allowed imports of US beef, but reinstated its ban in January 2006 after a violation of the US-Japan beef import agreement: a vertebral column, which should have been removed prior to shipment, was included in a shipment of veal.\n\nTokyo yielded to US pressure to resume imports, ignoring consumer worries about the safety of US beef, said Japanese consumer groups. Michiko Kamiyama from Food Safety Citizen Watch and Yoko Tomiyama from Consumers Union of Japan said about this: \"The government has put priority on the political schedule between the two countries, not on food safety or human health.\"\n\nSixty-five nations implemented full or partial restrictions on importing US beef products because of concerns that US testing lacked sufficient rigor. As a result, exports of US beef declined from 1,300,000 metric tons in 2003, (before the first mad cow was detected in the US) to 322,000 metric tons in 2004. This has increased since then to 771,000 metric tons in 2007 and to 1,300,000 metric tons by 2017.\n\nOn 31 December 2006, Hematech, Inc, a biotechnology company based in Sioux Falls, South Dakota, announced it had used genetic engineering and cloning technology to produce cattle that lacked a necessary gene for prion production – thus theoretically making them immune to BSE.\n\nIn April 2012, some South Korean retailers ceased importing beef from the United States after a case of BSE was reported. Indonesia also suspended imports of beef from the US after a dairy cow with mad cow disease was discovered in California.\n\nWith 36 confirmed cases, Japan experienced one of the largest number of cases of BSE outside Europe. It was the only country outside Europe and the Americas to report non-imported cases. Reformation of food safety in light of the BSE cases resulted in the establishment of a governmental Food Safety Commission in 2003.\n\nCattle are naturally herbivores, eating grasses. In modern industrial cattle-farming, though, various commercial feeds are used, which may contain ingredients including antibiotics, hormones, pesticides, fertilizers, and protein supplements. The use of meat and bone meal, produced from the ground and cooked leftovers of the slaughtering process, as well as from the carcasses of sick and injured animals such as cattle or sheep, as a protein supplement in cattle feed was widespread in Europe prior to about 1987. Worldwide, soybean meal is the primary plant-based protein supplement fed to cattle. However, soybeans do not grow well in Europe, so cattle raisers throughout Europe turned to the cheaper animal byproduct feeds as an alternative. The British Inquiry dismissed suggestions that changes to processing might have increased the infectious agents in cattle feed, saying, \"changes in process could not have been solely responsible for the emergence of BSE, and changes in regulation were not a factor at all.\" (The prion causing BSE is not destroyed by heat treatment.)\n\nThe first confirmed instance in which an animal fell ill with the disease occurred in 1986 in the United Kingdom, and lab tests the following year indicated the presence of BSE; by November 1987, the British Ministry of Agriculture accepted it had a new disease on its hands. Subsequently, 177 people (as of June 2014) contracted and died of a disease with similar neurological symptoms subsequently called (new) variant Creutzfeldt–Jakob disease (vCJD). This is a separate disease from 'classical' Creutzfeldt–Jakob disease, which is not related to BSE and has been known about since the early 1900s. Three cases of vCJD occurred in people who had lived in or visited the UK – one each in the Republic of Ireland, Canada, and the United States of America. Also, some concern existed about those who work with (and therefore inhale) cattle meat and bone meal, such as horticulturists, who use it as fertilizer. Up-to-date statistics on all types of CJD are published by the National Creutzfeldt–Jakob Disease Surveillance Unit in Edinburgh, Scotland.\n\nFor many of the vCJD patients, direct evidence exists that they had consumed tainted beef, and this is assumed to be the mechanism by which all affected individuals contracted it. Disease incidence also appears to correlate with slaughtering practices that led to the mixture of nervous system tissue with ground meat (mince) and other beef. An estimated 400,000 cattle infected with BSE entered the human food chain in the 1980s. Although the BSE epizootic was eventually brought under control by culling all suspect cattle populations, people are still being diagnosed with vCJD each year (though the number of new cases currently has dropped to fewer than five per year). This is attributed to the long incubation period for prion diseases, which is typically measured in years or decades. As a result, the full extent of the human vCJD outbreak is still not known.\n\nThe scientific consensus is that infectious BSE prion material is not destroyed through cooking procedures, meaning that even contaminated beef foodstuffs prepared \"well done\" may remain infectious.\nIn fact the infectious agent remains viable over .\n\nAlan Colchester, a professor of neurology at the University of Kent, and Nancy Colchester, writing in the 3 September 2005 issue of the medical journal \"The Lancet\", proposed a theory that the most likely initial origin of BSE in the United Kingdom was the importation from the Indian Subcontinent of bone meal which contained CJD-infected human remains. The government of India vehemently responded to the research, calling it \"misleading, highly mischievous; a figment of imagination; absurd,\" further adding that India maintained constant surveillance and had not had a single case of either BSE or vCJD. The authors responded in the 22 January 2006 issue of \"The Lancet\" that their theory is unprovable only in the same sense as all other BSE origin theories are and that the theory warrants further investigation.\n\nDuring the course of the investigation into the BSE epizootic, an enquiry was also made into the activities of the Department of Health Medicines Control Agency (MCA). On 7 May 1999, David Osborne Hagger, a retired civil servant who worked in the Medicines Division of the Department of Health between 1984 and 1994, produced a written statement to the BSE Inquiry in which he gave an account of his professional experience of BSE.\n\nIn February 1989, the MCA had been asked to \"identify relevant manufacturers and obtain information about the bovine material contained in children’s vaccines, the stocks of these vaccines and how long it would take to switch to other products.\" In July, \"[the] use of bovine insulin in a small group of mainly elderly patients was noted and it was recognised that alternative products for this group were not considered satisfactory.\" In September, the BSE Working Party of the Committee on the Safety of Medicines (CSM) recommended that \"no licensing action is required at present in regard to products produced from bovine material or using prepared bovine brain in nutrient media and sourced from outside the United Kingdom, the Channel Isles and the Republic of Ireland provided that the country of origin is known to be free of BSE, has competent veterinary advisers and is known to practise good animal husbandry.\"\n\nIn 1990, the British Diabetic Association became concerned regarding the safety of bovine insulin. The CSM assured them \"[that] there was no insulin sourced from cattle in the UK or Ireland and that the situation in other countries was being monitored.\"\n\nIn 1991, the European Commission \"[expressed] concerns about the possible transmission of the BSE/scrapie agent to man through use of certain cosmetic treatments.\"\n\nIn 1992, sources in France reported to the MCA \"that BSE had now been reported in France and there were some licensed surgical sutures derived from French bovine material.\" Concerns were also raised at a CSM meeting \"regarding a possible risk of transmission of the BSE agent in gelatin products.\"\n\nFor this failure, France was heavily criticised internationally. Thillier himself queried why there had never been a ban on French beef or basic safety precautions to stop the food chain becoming contaminated, suggesting \"Perhaps because the French government forgot its role in guaranteeing the safety of food products, and this neglect cost the lives of nine people.\" The Sydney Morning Herald added, \"while blustering French politicians blamed Britain for the emergence of the disease – and tried to quarantine the country by banning imports of British beef – they failed to adopt measures to prevent a hidden epidemic at home.\"\n\nIn 2016 France confirmed a further case of BSE.\n\nIn October 2015 a case of BSE was confirmed at a farm in Carmarthenshire in Wales. This seems to be the last case reported in the media in the UK Previous to this there were two confirmed cases in Wales in 2013. In the last 10 years England and Wales have suffered the following number of outbreaks per year: 2007 53 confirmed outbreaks, 2008 33 confirmed, 2009 9 confirmed, 2010 11 confirmed, 2011 5 confirmed, 2012 2 confirmed, 2014 1 confirmed, 2016 0 confirmed \n\nIn October 2018, a case of BSE was confirmed at a firm in Aberdeenshire, Scotland, the first such case in Scotland in a decade. As of 18 October, the case was believed to be an isolated one, but four other animals from the same herd were being culled for precautionary reasons. Scottish officials confirmed that the case had been identified as part of routine testing and that the diseased cow had not entered the human food chain.\n\nA number of other countries had isolated outbreaks of BSE confirmed, including Spain, Portugal, Belgium and Germany.\n\nThe BSE crisis led to the European Union (EU) banning exports of British beef with effect from March 1996; the ban lasted for 10 years before it was finally lifted on 1 May 2006 despite attempts in May through September 1996 by British prime minister John Major to get the ban lifted. The ban, which led to much controversy in Parliament and to the incineration of over one million cattle from at least March 1996, resulted in trade controversies between the UK and other EU states, dubbed \"beef war\" by media. Restrictions remained for beef containing \"vertebral material\" and for beef sold on the bone. France continued to impose a ban on British beef illegally long after the European Court of Justice had ordered it to lift its blockade, although it has never paid any fine for doing so.\n\nRussia was proceeding to lift the ban sometime after November 2012 after 16 years; the announcement was made during a visit by the UK's chief veterinary officer Nigel Gibbens.\n\nIt was successfully negotiated that beef from Wales was allowed to be exported to the Dutch market, which had formerly been an important market for Northern Irish beef. Of two approved export establishments in the United Kingdom in 1999, one was in Scotland – an establishment to which live beef was supplied from Northern Ireland. As the incidence of BSE was very low in Northern Ireland – only six cases of BSE by 1999 – partly due to the early adoption of an advanced herd tagging and computerization system in the region, calls were made to remove the EU ban on exports with regard to Northern Irish beef.\n\nSimilar wildcat bans from countries known to have BSE were imposed in various European countries, although these were mostly subsequently ruled illegal. \"The Economist\" noted, \"Unfortunately, much of the crisis in Europe can be blamed on politicians and bureaucrats. Even while some European countries were clamouring for bans on British beef, they were ignoring warnings from the European Commission about how to avoid the spread of BSE in their own herds.\"\n\nDifferent hypotheses exist for the origin of BSE in cattle. One hypothesis suggests it may have jumped species from the disease scrapie in sheep, and another hypothesis suggests that it evolved from a rare spontaneous form of \"mad cow disease\" that has been seen occasionally in cattle for many centuries. In the 5th century BC, Hippocrates described a similar illness in cattle and sheep, which he believed also occurred in man. Publius Flavius Vegetius Renatus recorded cases of a disease with similar characteristics in the fourth and fifth centuries AD.\n\n\n"}
{"id": "6190347", "url": "https://en.wikipedia.org/wiki?curid=6190347", "title": "Cadaveric spasm", "text": "Cadaveric spasm\n\nCadaveric spasm, also known as postmortem spasm, instantaneous rigor, cataleptic rigidity, or instantaneous rigidity, is a rare form of muscular stiffening that occurs at the moment of death and persists into the period of rigor mortis. Cadaveric spasm can be distinguished from rigor mortis as the former is a stronger stiffening of the muscles that cannot be easily undone, as rigor mortis can. \n\nThe cause is unknown but is usually associated with violent deaths under extremely physical circumstances with intense emotion.\n\nCadaveric spasm may affect all muscles in the body, but typically only groups, such as the forearms, or hands. Cadaveric spasm is seen in cases of drowning victims when grass, weeds, roots or other materials are clutched, and provides evidence of life at the time of entry into the water. Cadaveric spasm often crystallizes the last activity one did before death and is therefore significant in forensic investigations, e.g. holding onto a knife tightly.\n\nATP is required to reuptake calcium into the sarcomere's sarcoplasmic reticulum (SR). When a muscle is relaxed, the myosin heads are returned to their \"high energy\" position, ready and waiting for a binding site on the actin filament to become available. Because there is no ATP available, previously released calcium ions cannot return to the SR. These leftover calcium ions move around inside the sarcomere and may eventually find their way to a binding site on the thin filament's regulatory protein. Since the myosin head is already ready to bind, no additional ATP expenditure is required and the sarcomere contracts.\n\nWhen this process occurs on a larger scale, the stiffening associated with rigor mortis can occur. It mainly occurs during high ATP use. Sometimes, cadaveric spasms can be associated with erotic asphyxiation resulting in death.\n\nCadaveric spasm has been posed as an explanation for President Kennedy's reaction to the fatal head shot in his 1963 assassination, to indicate why his head moved backward after the shot.\n\nWhen the body of Kurt Cobain was discovered, his left hand tightly clutched the barrel of the shotgun that killed him, indicating he had been alive and holding the weapon before his death, rather than having been killed by another person and the scene then arranged to suggest suicide.\n\nMatthias Pfaffli and Dau Wyler, Professors of Legal Medicine at University of Bern, Switzerland, posed five requirements in order for a death to have been observed and classified as containing a cadaveric spasm:\nBecause of the improbability that all of these requirements may be examined in one subject, cadaveric spasms are unlikely to be consistently documented and therefore proved existent.\n\nVery minimal to no pathophysiological or scientific basis exists to support the validity of cadaveric spasms. Chemically, this phenomenon cannot be explained as being analogous to “true” rigor mortis. Therefore, a variety of other factors have been examined and explored in an effort to alternatively account for the cases of supposed instantaneous rigor mortis that have been reported. In a study reported in The International Journal of Legal Medicine, there was no consistent evidence of cadaveric spasms even in deaths of the same type. Out of 65 sharp-force suicides, only two victims still held their weapon post mortem. This low incidence rate suggests that genuine cadaveric spasm was not exhibited. Gravity may play a large factor in the trapping of limbs and other objects under the body at the time of death, and the subsequent observed placement of limbs after death. In fatalities related to cranial or neural injury, nerve damage in the brain may inhibit the ability to release a weapon from the hand. The flexion of agonist and antagonist muscles in conjunction may additionally contribute to the observed fixation of an object or weapon.\n\n"}
{"id": "5167123", "url": "https://en.wikipedia.org/wiki?curid=5167123", "title": "Capital punishment in Europe", "text": "Capital punishment in Europe\n\nThe death penalty has been completely abolished in all European countries except for Belarus and Russia, the latter of which has a moratorium and has not conducted an execution since 1999. The absolute ban on the death penalty is enshrined in both the Charter of Fundamental Rights of the European Union (EU) and two widely adopted protocols of the European Convention on Human Rights of the Council of Europe, and is thus considered a central value. Of all modern European countries, San Marino, Portugal and the Netherlands were the first to abolish capital punishment, whereas only Belarus still practices capital punishment in some form or another. In 2012, Latvia became the last EU Member State to abolish capital punishment in wartime.\n\nAs of 2017, in Europe, the death penalty for peacetime crimes has been abolished in all countries except Belarus, while the death penalty for wartime crimes has been abolished in all countries except Belarus and Kazakhstan. (Kazakhstan is a country situated partly in Europe and partly in Asia).\n\nIn Russia the death penalty has been indefinitely suspended (under moratorium), therefore is uncommon but not unheard of.\n\nExcept for Belarus, which carried out two executions in 2018, the last executions by a European country occurred in Kazakhstan in 2003, and Ukraine in 1997.\n\nThe Council of Europe has two main instruments against capital punishment: Protocol no.6 and Protocol no.13.\n\nThe Protocol no.6 which prohibits the death penalty during peacetime has been ratified by all members of the Council of Europe, except Russia (which has signed, but not ratified).\n\nProtocol no.13 prohibits the death penalty in all circumstances (including for war crimes). All member states of the Council of Europe have ratified it, except Azerbaijan and Russia, which have not signed it, and Armenia, which has signed but not yet ratified. All have, however, abolished the death penalty. In 2014, Poland was the latest country to ratify Protocol no.13.\n\nThe only country in Europe that continues to execute in the 21st century is Belarus (last execution done in 2018).\n\nNo member of the Council of Europe has carried out executions in the 21st century. The last execution on the present day territory of the Council of Europe took place in 1997 in Ukraine.\n\nAbolition has been common in European history, but has only been a real trend since the end of the Second World War when human rights became a particular priority. The Kingdom of Italy had abolished the death penalty for civilians with the adoption of the Zanardelli Penal Code of 1889, but the Fascists had reintroduced it with the 1930 Penal Code.\n\nThe European Convention on Human Rights was adopted in 1950, but some countries took many years to ratify it. The United Kingdom retained the death penalty for high treason until 1998; however, this technicality was superseded by the absolute ban on the death penalty in 1976. William Joyce was the last person to be put to death for high treason in the UK, on 3 January 1946.\n\nA moratorium on the death penalty has been in place in Russia since 1 January 2010. According to the 19 November 2009 decision of the Constitutional Court of the Russian Federation, the death penalty shall not be practiced in Russia at any time before the ratification of the above-mentioned protocol. The Constitutional Court has also clarified that the decision is not an extension of the moratorium but the abolition of the capital punishment, since it will be no longer possible to practice it legally.\n\n2009 was the first year that no one was executed anywhere in Europe, however in March 2010 Belarus executed the last two people on its death row.\n\nThe European Union (EU) has long since been against the death penalty, supporting the European Convention, and its 2000 Charter of Fundamental Rights included an absolute ban on the death penalty in all circumstances. The Charter has been made legally binding by the Treaty of Lisbon as it was fully ratified and became effective on 1 December 2009. The treaty also has a provision for the EU to join the Council of Europe and accede to the European Convention on Human Rights. The EU has been an active promoter of abolition worldwide and has been promoting a United Nations moratorium on the death penalty; however some EU member state governments such as Poland have opposed such moves.\n\nThe Council of Europe has made abolition of the death penalty a prerequisite for membership. As a result, no execution has taken place on the territory of the organisation's member states since 1997. The Parliamentary Assembly of the Council of Europe continues to monitor the capital punishment issue. The current General Rapporteur on the abolition of the death penalty for the Parliamentary Assembly is German member of parliament Marina Schuster.\nformula_1Only used once, at the very last execution in Sweden\n\nThe only European country that executes criminals is Belarus, as that country is not party to the European Convention on Human Rights. Executions in Belarus are carried out by shooting.\n\nCapital punishment in Russia has been indefinitely suspended, although it still remains codified in its law. There exists both an implicit moratorium established by the President Yeltsin in 1996, and an explicit one, established by the Constitutional Court of Russia in 1999 and which was most recently reaffirmed in 2009. Russia has not executed anyone in peacetime since 1996, and in wartime since 1999.\n\nCapital punishment in Kazakhstan has been abolished for ordinary crimes, but is still permitted for crimes occurring in special circumstances (such as war crimes). Kazakhstan has not carried out any executions since 2003, and currently only one person is on death row.\nKazakhstan is not a member of the Council of Europe.\n\nBosnia and Herzegovina have constitutionally abolished the death penalty, but capital punishment remains present in legal statutes, specifically in Republika Srpska, Article 11. However, due to the constitutional abolition, the death penalty cannot be used.\n\nIn Europe there are also partially unrecognized states. In 2006 the Parliamentary Assembly of the Council of Europe wrote that: While Nagorno-Karabakh abolished the death penalty on 1 August 2003, when it decided to implement the Republic of Armenia's new Criminal Code on its territory, the other territories, Abkhazia, Transnistria and South Ossetia, have not done so, retaining capital punishment in their legislation both in peacetime and in wartime. As South Ossetia decided in 1992 to make Russian legislation applicable on its territory, it has observed a moratorium on executions since 1996. The death penalty is in the Transnistrian Criminal Code which came into force in 2002. In July 1999, de facto President Smirnov ordered a moratorium on executions, and there is said to be only one prisoner on death row in Transnistria. Abkhazia formalized its moratorium in 2007, moving towards full abolition. On 12 January 2007 the parliament of Abkhazia adopted a law entitled \"Moratorium on the Death Penalty\", establishing a moratorium on executions during peacetime. Since 1993 the country has had a \"de facto\" moratorium on executions. Although there have been 10 sentences of death in Abkhazia, these have never been implemented.\n\nThe Turkish Republic of Northern Cyprus retains the death penalty only for crimes committed under special circumstances (war-crimes). See also Capital punishment in Cyprus.\n\nThe Donetsk People's Republic introduced the death penalty in 2014 for cases of treason, espionage, and assassination of political leaders. There had already been accusations of extrajudicial execution occurring.\n\n\n"}
{"id": "36765422", "url": "https://en.wikipedia.org/wiki?curid=36765422", "title": "Capital punishment in Syria", "text": "Capital punishment in Syria\n\nCapital punishment is legal in Syria. Current laws allow the death penalty for treason; espionage; murder; arson resulting in death; attempting a death-eligible crime; recidivism for a felony punishable by forced labor for life; political acts and military offences such as bearing arms against Syria in the ranks of the enemy, desertion of the armed forces to the enemy, insubordination, rebellion and acts of incitement under martial law or in wartime; violent robbery; terrorism; subjecting a person to torture or barbaric treatment during the commission of gang-robbery; rape; membership in the Muslim Brotherhood; joining the Islamic State of Iraq and the Levant; drug trafficking; political dissidence and falsification of material evidence resulting in a third party being convicted for a drug offense and sentenced to death. Executions are carried out by hanging in public.\n"}
{"id": "2285013", "url": "https://en.wikipedia.org/wiki?curid=2285013", "title": "Cell therapy", "text": "Cell therapy\n\nCell therapy (also called cellular therapy or cytotherapy) is therapy in which cellular material is injected into a patient; this generally means intact, living cells. For example, T cells capable of fighting cancer cells via cell-mediated immunity may be injected in the course of immunotherapy.\n\nCell therapy originated in the nineteenth century when scientists experimented by injecting animal material in an attempt to prevent and treat illness. Although such attempts produced no positive benefit, further research found in the mid twentieth century that human cells could be used to help prevent the human body rejecting transplanted organs, leading in time to successful bone marrow transplantation.\n\nToday two distinct categories of cell therapy are recognized.\n\nThe first category is cell therapy in mainstream medicine. This is the subject of intense research and the basis of potential therapeutic benefit. Such research can be controversial when it involves human embryonic material.\n\nThe second category is in alternative medicine, and perpetuates the practice of injecting animal materials in an attempt to cure disease. This practice, according to the American Cancer Society, is not backed by any medical evidence of effectiveness, and can have deadly consequences.\n\nCell therapy can be defined as therapy in which cellular material is injected into a patient.\n\nThere are two branches of cell therapy: one is legitimate and established, whereby human cells are transplanted from a donor to a patient; the other is dangerous alternative medicine, whereby injected animal cells are used to attempt to treat illness.\n\nThe origins of cell therapy can perhaps be traced to the nineteenth century, when Charles-Édouard Brown-Séquard (1817–1894) injected animal testicle extracts in an attempt to stop the effects of aging. In 1931 Paul Niehans (1882–1971) – who has been called the inventor of cell therapy – attempted to cure a patient by injecting material from calf embryos. Niehans claimed to have treated many people for cancer using this technique, though his claims have never been validated by research.\n\nIn 1953 researchers found that laboratory animals could be helped not to reject organ transplants by pre-inoculating them with cells from donor animals; in 1968, in Minnesota, the first successful human bone marrow transplantation took place. In more recent work, cell encapulation is pursued as a means to shield therapeutic cells from the host immune response. Recent work includes micro-encapsulating cells in a gel core surrounded by a solid, but permeable, shell.\n\nBone marrow transplants have been found to be effective, along with some other kinds of human cell therapy – for example in treating damaged knee cartilage. In recent times, cell therapy using human material has been recognized as an important field in the treatment of human disease. The experimental field of Stem cell therapy has shown promise for new types of treatment.\n\nIn mainstream medicine, cell therapy is supported by a distinct healthcare industry which sees strong prospects for future growth.\n\nIn allogeneic cell therapy the donor is a different person to the recipient of the cells. In pharmaceutical manufacturing, the allogenic methodology is promising because unmatched allogenic therapies can form the basis of \"off the shelf\" products. There is research interest in attempting to develop such products to treat conditions including Crohn's disease and a variety of vascular conditions.\n\nResearch into human embryonic stem cells is controversial, and regulation varies from country to country, with some countries banning it outright. Nevertheless, these cells are being investigated as the basis for a number of therapeutic applications, including possible treatments for diabetes and Parkinson's disease.\n\nCell therapy is targeted at many clinical indications in multiple organs and by several modes of cell delivery. Accordingly, the specific mechanisms of action involved in the therapies are wide-ranging. However, there are two main principles by which cells facilitate therapeutic action:\n\n\nNeural stem cells (NSCs) are the subject of ongoing research for possible therapeutic applications, for example for treating a number of neurological disorders such as Parkinson's disease and Huntington's disease.\n\nMSCs are immunomodulatory, multipotent and fast proliferating and these unique capabilities mean they can be used for a wide range of treatments including immune-modulatory therapy, bone and cartilage regeneration, myocardium regeneration and the treatment of Hurler syndrome, a skeletal and neurological disorder.\n\nResearchers have demonstrated the use of MSCs for the treatment of osteogenesis imperfecta (OI). Horwitz et al. transplanted bone marrow (BM) cells from human leukocyte antigen (HLA)-identical siblings to patients suffering from OI. Results show that MSCs can develop into normal osteoblasts, leading to fast bone development and reduced fracture frequencies. A more recent clinical trial showed that allogeneic fetal MSCs transplanted in utero in patients with severe OI can engraft and differentiate into bone in a human fetus.\n\nBesides bone and cartilage regeneration, cardiomyocyte regeneration with autologous BM MSCs has also been reported recently. Introduction of BM MSCs following myocardial infarction (MI) resulted in significant reduction of damaged regions and improvement in heart function. Clinical trials for treatment of acute MI with Prochymal by Osiris Therapeutics are underway. Also, a clinical trial revealed huge improvements in nerve conduction velocities in Hurler’s Syndrome patients infused with BM MSCs from HLA-identical siblings.\n\nHSCs possess the ability to self-renew and differentiate into all types of blood cells, especially those involved in the human immune system. Thus, they can be used to treat blood and immune disorders. Since human bone marrow (BM) grafting was first published in 1957, there have been significant advancements in HSCs therapy. Following that, syngeneic marrow infusion and allogeneic marrow grafting were performed successfully. HSCs therapy can also render its cure by reconstituting damaged blood-forming cells and restoring the immune system after high-dose chemotherapy to eliminate disease.\n\nThere are three types of HSCT: syngeneic, autologous, and allogeneic transplants. Syngeneic transplantations occur between identical twins. Autologous transplantations use the HSCs obtained directly from the patient and hence do not cause any complications of tissue incompatibility; whereas allogeneic transplantations involve the use of donor HSCs, either genetically related or unrelated to the recipient. To lower the risks of transplant, which include graft rejection and Graft-Versus-Host Disease (GVHD), allogeneic HSCT must satisfy compatibility at the HLA loci (i.e. genetic matching to reduce the immunogenicity of the transplant). Mismatch of HLA loci would result in treatment-related mortality and higher risk of acute GVHD.\n\nIn addition to BM derived HSCs, the use of alternative sources such as umbilical cord blood (UCB) and peripheral blood stem cells (PBSCs) has been increasing. In comparison with BM derived HSCs recipients, PBSCs recipients afflicted with myeloid malignancies reported a faster engraftment and better overall survival. However, this was at the expense of increased rate of GVHD. Also, the use of UCB requires less stringent HLA loci matching, although the time of engraftment is longer and graft failure rate is higher.\n\nIn alternative medicine, cell therapy is defined as the injection of non-human cellular animal material in an attempt to treat illness. Quackwatch labels this as \"senseless\", since \"cells from the organs of one species cannot replace the cells from the organs of other species\" and because a number of serious adverse effects have been reported.\n\nOf this alternative, animal-based form of cell therapy, the American Cancer Society say: \"Available scientific evidence does not support claims that cell therapy is effective in treating cancer or any other disease. It may in fact be lethal ...\".\n\n"}
{"id": "7347911", "url": "https://en.wikipedia.org/wiki?curid=7347911", "title": "César Basa", "text": "César Basa\n\nCésar Fernándo Basa (1915 – December 12, 1941) was a Filipino pilot and World War II hero. Born in 1915, he was one of the pioneer fighter pilots of the Philippine Air Force and the first Filipino fighter pilot casualty during World War II.\n\nBasa's fight took place at Batangas Field on the morning of December 12, 1941 when 27 Japanese bombers and 17 fighter escorts raided the base.\n\nFive Filipino fighter pilots on Curtiss P-26A \"Peashooter\" fighter planes, led by Captain Jesús Villamor, engaged the numerically superior enemy in aerial combat at .\n\nSeveral dogfights ensued as Villamor and his men fought desperately to prevent the pack of bombers and their fighter escorts from reaching and bombing Batangas Field.\n\nLieutenant Basa, who was already two hours airborne on an air reconnaissance mission, rushed to the scene and attempted to join the aerial engagement with only 15 minutes worth of fuel left on his P-26. While still half the distance away, he was intercepted by seven Japanese fighters and shot down. Although he was able to bail out, he was strafed and killed by machine-gun fire from the A6M Zero fighters while dangling helplessly from his parachute. This made him the first Filipino fighter pilot casualty of war killed in air combat.\n\nCaptain Villamor and his pilots won the battle, with the only casualty being Basa. In recognition of his heroism, Lieutenant Basa was posthumously awarded the Silver Star.\n\nBasa Air Base in Floridablanca, Pampanga, Philippines is named in his honor.\n"}
{"id": "35458904", "url": "https://en.wikipedia.org/wiki?curid=35458904", "title": "Data science", "text": "Data science\n\nData science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from data in various forms, both structured and unstructured, similar to data mining.\n\nData science is a \"concept to unify statistics, data analysis, machine learning and their related methods\" in order to \"understand and analyze actual phenomena\" with data. It employs techniques and theories drawn from many fields within the context of mathematics, statistics, information science, and computer science.\n\nTuring award winner Jim Gray imagined data science as a \"fourth paradigm\" of science (empirical, theoretical, computational and now data-driven) and asserted that \"everything about science is changing because of the impact of information technology\" and the data deluge.\n\nIn 2012, when Harvard Business Review called it \"The Sexiest Job of the 21st Century\", the term \"data science\" became a buzzword. It is now often used interchangeably with earlier concepts like business analytics, business intelligence, predictive modeling, and statistics. Even the suggestion that data science is sexy was paraphrasing Hans Rosling, featured in a 2011 BBC documentary with the quote, \"Statistics is now the sexiest subject around.\" Nate Silver referred to data science as a sexed up term for statistics.<nowiki> In many cases, earlier approaches and solutions are now simply rebranded as \"data science\" to be more attractive, which can cause the term to become \"dilute[d] beyond usefulness.\"</nowiki> While many university programs now offer a data science degree, there exists no consensus on a definition or suitable curriculum contents. To its discredit, however, many data-science and big-data projects fail to deliver useful results, often as a result of poor management and utilization of resources.\n\nThe term \"data science\" has appeared in various contexts over the past thirty years but did not become an established term until recently. In an early usage, it was used as a substitute for computer science by Peter Naur in 1960. Naur later introduced the term \"datalogy\". In 1974, Naur published \"Concise Survey of Computer Methods\", which freely used the term data science in its survey of the contemporary data processing methods that are used in a wide range of applications.\n\nIn 1996, members of the International Federation of Classification Societies (IFCS) met in Kobe for their biennial conference. Here, for the first time, the term data science is included in the title of the conference (\"Data Science, classification, and related methods\"), after the term was introduced in a roundtable discussion by Chikio Hayashi.\n\nIn November 1997, C.F. Jeff Wu gave the inaugural lecture entitled \"Statistics = Data Science?\" for his appointment to the H. C. Carver Professorship at the University of Michigan.\nIn this lecture, he characterized statistical work as a trilogy of data collection, data modeling and analysis, and decision making. In his conclusion,\nhe initiated the modern, non-computer science, usage of the term \"data science\" and advocated that statistics be renamed data science and statisticians data scientists.\nLater, he presented his lecture entitled \"Statistics = Data Science?\" as the first of his 1998 P.C. Mahalanobis Memorial Lectures. These lectures honor Prasanta Chandra Mahalanobis, an Indian scientist and statistician and founder of the Indian Statistical Institute.\n\nIn 2001, William S. Cleveland introduced data science as an independent discipline, extending the field of statistics to incorporate \"advances in computing with data\" in his article \"Data Science: An Action Plan for Expanding the Technical Areas of the Field of Statistics,\" which was published in Volume 69, No. 1, of the April 2001 edition of the International Statistical Review / Revue Internationale de Statistique. In his report, Cleveland establishes six technical areas which he believed to encompass the field of data science: multidisciplinary investigations, models and methods for data, computing with data, pedagogy, tool evaluation, and theory.\n\nIn April 2002, the International Council for Science (ICSU): Committee on Data for Science and Technology (CODATA) started the \"Data Science Journal\", a publication focused on issues such as the description of data systems, their publication on the internet, applications and legal issues. Shortly thereafter, in January 2003, Columbia University began publishing \"The Journal of Data Science\", which provided a platform for all data workers to present their views and exchange ideas. The journal was largely devoted to the application of statistical methods and quantitative research. In 2005, The National Science Board published \"Long-lived Digital Data Collections: Enabling Research and Education in the 21st Century\" defining data scientists as \"the information and computer scientists, database and software and programmers, disciplinary experts, curators and expert annotators, librarians, archivists, and others, who are crucial to the successful management of a digital data collection\" whose primary activity is to \"conduct creative inquiry and analysis.\"\n\nAround 2007, Turing award winner Jim Gray envisioned \"data-driven science\" as a \"fourth paradigm\" of science that uses the computational analysis of large data as primary scientific method and \"to have a world in which all of the science literature is online, all of the science data is online, and they interoperate with each other.\"\n\nIn the 2012 Harvard Business Review article \"Data Scientist: The Sexiest Job of the 21st Century\", DJ Patil claims to have coined this term in 2008 with Jeff Hammerbacher to define their jobs at LinkedIn and Facebook, respectively. He asserts that a data scientist is \"a new breed\", and that a \"shortage of data scientists is becoming a serious constraint in some sectors\", but describes a much more business-oriented role.\n\nIn 2013, the IEEE Task Force on Data Science and Advanced Analytics was launched. In 2013, the first \"European Conference on Data Analysis (ECDA)\" was organised in Luxembourg, establishing the European Association for Data Science (EuADS). The first international conference: IEEE International Conference on Data Science and Advanced Analytics was launched in 2014. In 2014, General Assembly launched student-paid bootcamp and The Data Incubator launched a competitive free data science fellowship. In 2014, the American Statistical Association section on Statistical Learning and Data Mining renamed its journal to \"Statistical Analysis and Data Mining: The ASA Data Science Journal\" and in 2016 changed its section name to \"Statistical Learning and Data Science\". In 2015, the International Journal on Data Science and Analytics was launched by Springer to publish original work on data science and big data analytics. In September 2015 the Gesellschaft für Klassifikation (GfKl) added to the name of the Society \"Data Science Society\" at the third ECDA conference at the University of Essex, Colchester, UK.\n\nThe popularity of the term \"data science\" has exploded in business environments and academia, as indicated by a jump in job openings. However, many critical academics and journalists see no distinction between data science and statistics. Writing in Forbes, Gil Press argues that data science is a buzzword without a clear definition and has simply replaced “business analytics” in contexts such as graduate degree programs. In the question-and-answer section of his keynote address at the Joint Statistical Meetings of American Statistical Association, noted applied statistician Nate Silver said, “I think data-scientist is a sexed up term for a statistician...Statistics is a branch of science. Data scientist is slightly redundant in some way and people shouldn’t berate the term statistician.” Similarly, in business sector, multiple researchers and analysts state that data scientists alone are far from being sufficient in granting companies a real competitive advantage and consider data scientists as only one of the four greater job families companies require to leverage big data effectively, namely: data analysts, data scientists, big data developers and big data engineers.\n\nOn the other hand, responses to criticism are as numerous. In a 2014 Wall Street Journal article, Irving Wladawsky-Berger compares the data science enthusiasm with the dawn of computer science. He argues data science, like any other interdisciplinary field, employs methodologies and practices from across the academia and industry, but then it will morph them into a new discipline. He brings to attention the sharp criticisms computer science, now a well respected academic discipline, had to once face. Likewise, NYU Stern's Vasant Dhar, as do many other academic proponents of data science, argues more specifically in December 2013 that data science is different from the existing practice of data analysis across all disciplines, which focuses only on explaining data sets. Data science seeks actionable and consistent pattern for predictive uses. This practical engineering goal takes data science beyond traditional analytics. Now the data in those disciplines and applied fields that lacked solid theories, like health science and social science, could be sought and utilized to generate powerful predictive models.\n\nIn an effort similar to Dhar's, Stanford professor David Donoho, in September 2015, takes the proposition further by rejecting three simplistic and misleading definitions of data science in lieu of criticisms. First, for Donoho, data science does not equate to big data, in that the size of the data set is not a criterion to distinguish data science and statistics. Second, data science is not defined by the computing skills of sorting big data sets, in that these skills are already generally used for analyses across all disciplines. Third, data science is a heavily applied field where academic programs right now do not sufficiently prepare data scientists for the jobs, in that many graduate programs misleadingly advertise their analytics and statistics training as the essence of a data science program. As a statistician, Donoho, following many in his field, champions the broadening of learning scope in the form of data science, like John Chambers who urges statisticians to adopt an inclusive concept of learning from data, or like William Cleveland who urges to prioritize extracting from data applicable predictive tools over explanatory theories. Together, these statisticians envision an increasingly inclusive applied field that grows out of traditional statistics and beyond.\n\nFor the future of data science, Donoho projects an ever-growing environment for open science where data sets used for academic publications are accessible to all researchers. US National Institute of Health has already announced plans to enhance reproducibility and transparency of research data. Other big journals are likewise following suit. This way, the future of data science not only exceeds the boundary of statistical theories in scale and methodology, but data science will revolutionize current academia and research paradigms. As Donoho concludes, \"the scope and impact of data science will continue to expand enormously in coming decades as scientific data and data about science itself become ubiquitously available.\"\n\n"}
{"id": "37568086", "url": "https://en.wikipedia.org/wiki?curid=37568086", "title": "Desmoschisis", "text": "Desmoschisis\n\nDesmoschisis is asexual reproduction in dinoflagellates which the parent cell divides to produce two daughter cells, each daughter retaining half the parent theca, at least temporarily. During desmoschisis, the theca undergoes fission along a predetermined suture between thecal plates. The fission suture is oblique, usually from the top left to the bottom right (as in oblique binary fission). In terms of asexual division of motile cells, desmoschisis is\ngenerally the case in gonyaulacaleans whereas eleutheroschisis is generally the case in peridinialeans. \n"}
{"id": "5208548", "url": "https://en.wikipedia.org/wiki?curid=5208548", "title": "DictyBase", "text": "DictyBase\n\ndictyBase is an online bioinformatics database for the model organism \"Dictyostelium discoideum\".\n\ndictyBase offers many ways of searching and retrieving data from the database:\n\n"}
{"id": "46694606", "url": "https://en.wikipedia.org/wiki?curid=46694606", "title": "Eric Chalmers", "text": "Eric Chalmers\n\nEric Henry Chalmers (29 November 1900 – 14 September 1930) was an Australian rules footballer who played with in the West Australian Football League (WAFL) and Richmond in the Victorian Football League (VFL).\n\nBorn on 29 November 1900 in Victoria, Chalmers was one of four children. Their maternal grandfather was a Scottish blacksmith, from Dundee, who came to Clunes, Victoria in 1854 to search for gold.\n\nHis father, Alexander James Chalmers (known as James), was a wheat farmer, who moved the family to Fremantle, Western Australia soon after Eric's birth. James Chalmers became well known in the area as the proprietor of the Fremantle Foundry and later became a local councillor. In his younger years, Chalmers had been a Footscray footballer.\n\nChristina Frances Chalmers, Eric's mother, died in 1917, as did one of his brothers William Fraser Chalmers, killed in action on the Western Front.\n\nWhen not playing football, Chalmers worked as an engineer.\n\nChalmers started his football career in the West Australian Football League, where he played for South Fremantle, as a rover and in the forward-line. Graduating from the juniors in 1921, Chalmers played senior football for South Fremantle until 1924, when he was cleared to Richmond at the beginning of the 1924 VFL season.\n\nChalmers, who wore number 23 for Richmond, debuted in round two against Geelong at Corio Oval and played five further senior games in what would be his only season of VFL football.\n\nHe rejoined South Fremantle midway through the 1926 WAFL season and played for a short time.\n\nOn the night of 13 September 1930, Chalmers was stabbed during a confrontation with a 55-year-old hawker, Maurice Alexander, near West Perth's Clarendon Hotel, in an area known locally as the \"sand patch\". Earlier in the night, Chalmers had eaten dinner at the hotel and it was there that he met George Maloney, with whom he walked to the sand patch, a vacant block on Fitzgerald Street. The pair were there to search for Maloney's hat, which had been lost when Maloney was involved in a fight that afternoon, with a man named Bob Brown. At the scene, Chalmers began talking to Alexander, who had an adjoining property, which they both headed to. Maloney eventually gave up on finding his hat and went over to Chalmers, who was by then involved in a fight with Alexander on the verandah. Maloney intervened and was stabbed by Alexander with a table knife, along with Chalmers. Both Chalmers and Maloney, the latter with non life-threatening injuries, fled towards the Clarendon Hotel, where they were given medical assistance by the manager, before being taken to hospital.\n\nAt the hospital, Chalmers made a deposition, in which he identified Alexander as the man who stabbed him.\n\nChalmers, who had suffered wounds to his arm and abdomen, died of his injuries the following day.\n\nAlexander was found guilty of manslaughter and sentenced to six year imprisonment, which was less than would have been imposed had the jury not recommended mercy, on account of \"extreme provocation\". It was argued by the crown by Chalmers had struck Alexander because he believed he was \"behaving indecently\".\n\nAt the time of his death, Chalmers had been employed as a foreman with Shell Oil, a job he had held since returning to Western Australia.\n\n"}
{"id": "18953261", "url": "https://en.wikipedia.org/wiki?curid=18953261", "title": "EyePet", "text": "EyePet\n\nEyePet is a game for the PlayStation 3 and PlayStation Portable, developed by London Studio and Playlogic Game Factory. The original PlayStation 3 version was released in Europe on 23 October 2009 and in Australia on 27 October 2009. This version of \"EyePet\" was originally scheduled to be released in North America on 17 November 2009, but was held back. Instead, a newer version of \"EyePet\" adapted for the PlayStation Move motion control system was released in North America on 5 September 2010, where it is available in a bundle. In October 2010, the Move-adapted version was released in Japan and Europe, with the European edition entitled EyePet Move Edition. The game also supports 3D on 3D enabled TVs. The PSP version of EyePet was released on 2 November 2010.\n\nThe game uses the camera to allow a virtual pet to interact with people and objects in the real world. Using augmented reality, the simian, gremlin-like creature appears to be aware of its environment and surroundings and reacts to them accordingly. The player can place objects in front of the animal and the game will interpret what the object is and respond to it. For example, if the player rolls a ball towards it, it will jump out of the way to avoid being hurt. It will also react to the player's actions and sound allowing the user to, for example, tickle the animal or clap their hands to startle it. A trailer shown at E3 2009 shows the pet being customized with outfits and colored fur and interacting with virtual objects including a trampoline and bubble machine. The trailers also demonstrate users drawing custom objects (first trailer shows a car, E3 2009 trailer shows an airplane) which are scanned by the camera and converted to virtual objects which the pet can then interact with. The pet can also be fed, as in the trailers, it is shown eating cookies and a type of pet food.\n\nEurogamer scored the original \"EyePet\" 6 out of 10. Reviewer Dan Whitehead, praised the game as \"a showcase of what console cameras and motion-sensing is capable of\" initially commenting how convincing the illusion of seeing the CG creature in the real world. However, this illusion is often spoiled by \"clumsy\" gameplay mechanics. Whitehead also criticised the sometimes vague instructions and a lack of feedback provided when the player fails to carry out an instruction properly. IGN gave the game an 8.0 calling it a game that runs smoothly and has a look that can't be beat.\n"}
{"id": "612310", "url": "https://en.wikipedia.org/wiki?curid=612310", "title": "Francis García", "text": "Francis García\n\nFrancis García (born José Francisco García Escalante on April 6, 1958 – October 10, 2007) and known by the artistic name \"Francis\" was a Mexican transvestite who was a famous actor, female impersonator and stage costume designer.\n\nBorn in Campeche, Campeche as Francisco Garcia Escalante, He first gained attention as a dancer in vedette shows, held at the famous \"Blanquita\" theater, some of which included a transvestites' ballet. \nHis first mainstream breakthrough was in the said theater during the casting for a movie called \"Bellas de noche\" (1975) where he was given a role, resulting in him later getting his own show at the Blanquita called \"Francis, la fantasia hecha mujer\" ( Francis, fantasy becomes a woman ) which successfully ran for 17 years, he also performed in Los Angeles at the theater \"Los Pinos\", which was the home of his touring shows. He was an actor, comedian, singer, lip-syncher and choreographer/dancer.\n\nDuring his theater and T.V. shows, he impersonated famous Latin female singers such as Colombian Shakira, Spaniard Rocío Dúrcal and Mexicans Gloria Trevi, Alejandra Guzmán, and Lupita D'Alessio as well as doing stand up comedy and Vegas style dancing numbers.\n\nFashion designer Mitzi, a long-time friend, credits Garcia with helping him during his time of need and aiding to start his design empire.\n\nHe was a beloved public personality in Mexican showbiz and appeared in several movies, telenovelas, variety shows and broadcasts of his theater comedy shows, most notable was his stage persona as a female impersonator during a time when Mexican society was very close-minded and homosexuality was considered a minor crime, he was the first openly gay celebrity in the country and a passionate activist for equality and human and gay rights till his death.\nDuring his lifetime he was relentlessly questioned if he ever wanted to surgically become a woman, which he vehemently denied quoting religious beliefs and jokingly expressing that in today's society, he'd rather get surgery to make his penis bigger, as women are more popular with \"something extra\" down there...\n\nHe never had feminisation surgeries, he did however have rhinoplasty to help him achieve his famed Lupita D'Alessio characterization which was highly acclaimed. \n\nGarcia died of a lung thrombosis in a Mexico City hospital on October 10, 2007. He was 49 years old.\n"}
{"id": "847681", "url": "https://en.wikipedia.org/wiki?curid=847681", "title": "Funerary text", "text": "Funerary text\n\nFunerary texts or funerary literature feature in many belief systems. Its purpose is usually to provide guidance to the newly deceased or the soon-to-be-deceased about how to survive and prosper in the afterlife.\n\nThe most famous example of funerary literature is that of the ancient Egyptians, whose \"Book of the Dead\" was buried with the deceased to guide him or her through the various trials that would be encountered before being allowed into the afterlife. The \"Book of the Dead\" followed a tradition of Egyptian funerary literature that dated back as far as the 26th century BC.\n\nSimilar practices were followed by followers of the cult of Orpheus, who lived in southern Italy and Crete in the 6th–1st century BC. Their dead were buried with gold plates or laminae on which were inscribed directions about the afterlife.\n\nTibetan Buddhists still make use of the \"Bardo Thodol\" (also known as \"Tibetan Book of the Dead\", dating to ca. the 8th century), which describes the experiences of the mind after death. It is recited by lamas over a dying or recently deceased person, or sometimes over an effigy of the deceased.\n\nPure Land Buddhists regularly recite the \"Amitabha Sutra\", which describes the Buddha Amitabha and Sukhavati, the Western Pure Land, in brief detail. It is ordinarily recited at funerals or at memorial services.\n\nIn late 15th-century Europe, the \"Ars moriendi\" (\"The Art of Dying Well\") became one of the most popular and widely circulated early printed books. It was published in Germany around 1470 as a guide to how to meet Death and avoid the temptations (Impatience, Pride, Avarice, etc.) that would consign a soul to purgatory or, worse, to hell.\n\nThe \"Funeral Oration\" () is the oldest extant record of the Hungarian language, dating back to 1192–1195. (, )\n\n\n"}
{"id": "43137828", "url": "https://en.wikipedia.org/wiki?curid=43137828", "title": "Hit Abhilashi", "text": "Hit Abhilashi\n\nHit Abhilashi was a leader of Bharatiya Janata Party from Punjab, India. He was a cabinet minister of the state from 1977 to 1980. He was killed by suspected militants in 1988. He was president of the Punjab chapter of the Bharatiya Janata Party when killed.\n"}
{"id": "42707794", "url": "https://en.wikipedia.org/wiki?curid=42707794", "title": "Holonic map", "text": "Holonic map\n\nA holonic map is a fractal map of perceptions represented as holons, forming a holarchy of perceptual information. Within a holonic map each \"object of perception\", colloquially referred to as a 'thought' or a 'thing', is represented by a holon, which can contain other holons.\n\nThe holonic map is able to store and communicate subjective and inter-subjective information, and therefore by function of the human nervous system abstractions of 'objective' information. Any perceptual object of attention can potentially be represented as a holon within a holonic map.\n\nThe concept of a \"holonic map\" and \"holonic mapping\" was invented in 2011 by Chris Larcombe in an attempt to bring systems thinking to a wider audience, in addition to providing a platform for expanding individual and collective perception of the environment through the process of sharing perceptions.\n\nIn the same year the holonic map was subsequently implemented by Larcombe in the form of a web-based collaborative holonic mapping technology initially known as \"Waysphere\", allowing perceptions to be shared over the internet.\n\nA holonic map can be used to map any whole system or sub-system, since its structure appears to mirror the geometry of natural energetic systems, as \"perceived by\" the human brain: the \"observer-dependence\" of what constitutes a system has been noted within early cybernetics research, while modern research into the human brain has revealed a fractal holarchical structure in the neocortex.\n\nA single holonic map can be co-created by multiple individuals simultaneously in a collaborative mapping process. When multiple individuals are creating a single map in collaboration, the process can be referred to as collaborative holonic mapping, or holonic collaborative mapping.\n\nMost generally, a holonic map co-created by multiple individuals is used to map and represent consensus reality; that which is inter-subjective. In practice a collaborative holonic map is typically focused on something specific, such as a co-created project, resource, topic, time-line of events, etc.\n\nA holonic map can be implemented in a digital, technological form, such as in Noomap.\n"}
{"id": "6853041", "url": "https://en.wikipedia.org/wiki?curid=6853041", "title": "Huberta (hippopotamus)", "text": "Huberta (hippopotamus)\n\nHuberta (initially named Hubert; the gender was discovered after death) was a hippopotamus and one of the most famous animals in South African history.\n\nIn November 1928, Huberta left her waterhole in the St. Lucia Estuary in Zululand and set off on the journey to the Eastern Cape, a journey which took her three years. In that time, Huberta became a minor celebrity in South Africa and attracted crowds wherever she went. She was initially thought to be a male and was nicknamed Hubert by the press. The first report in the press was on 23 November 1928 in the \"Natal Mercury\" and reported the appearance of a hippo in Natal. The report was accompanied by the only photograph of Huberta in life.\n\nHuberta stopped for a while at the mouth of the Mhlanga River about north of Durban and a failed attempt was made to capture her and put her in Johannesburg Zoo. After this, she headed south to Durban where she visited a beach and a country club. Moving on to the Umgeni River, she became revered by Zulus and Xhosas alike.\n\nFinally, Huberta arrived in East London in March 1931. Despite her having been declared \"royal game\" (and thus protected) by the Natal Provincial Council, she was shot by farmers a month later. After a public outcry, the farmers were arrested and fined £25. Huberta's body was recovered and sent to a taxidermist in London. Upon her return to South Africa in 1932, she was greeted by 20,000 people and was displayed at the Amathole Museum (previously known as the Kaffrarian Museum) in King William's Town.\n\nHuberta is the subject of the children's book \"Hubert The Traveling Hippopotamus\" by Edmund Lindop and illustrated by Jane Carlson. The book was published in 1961 by Little, Brown and Company.\n\n"}
{"id": "156935", "url": "https://en.wikipedia.org/wiki?curid=156935", "title": "Joel Barlow", "text": "Joel Barlow\n\nJoel Barlow (March 24, 1754 – December 26, 1812) was an American poet and diplomat, and French politician. In politics, he supported the French Revolution and was an ardent Jeffersonian republican.\n\nHe worked as an agent for American speculator William Duer to set up the Scioto Company in Paris in 1788, and to sell worthless deeds to land in the Northwest Territory which it did not own. Scholars believe that he did not know the transactions were fraudulent. He stayed in Paris, becoming involved in the French Revolution. He was elected to the Assembly and given French citizenship in 1792.\n\nIn his own time, Barlow was known especially for the epic poem \"Vision of Columbus\" (1807), though modern readers rank \"The Hasty-Pudding\" (1793) more highly. \n\nAs American consul at Algiers, he helped draft the Treaty of Tripoli in 1796, to end the attacks of Barbary pirates of North Africa city states. The treaty includes the phrase: \"the Government of the United States of America is not, in any sense, founded on the Christian religion\". He also served as US Minister to France, from 1811 to his death on December 26, 1812 in Żarnowiec, Poland.\n\nBarlow was born in Redding, Fairfield County, Connecticut. He briefly attended Dartmouth College before he graduated from Yale College in 1778, where he continued for two years as a postgraduate student. In 1778, he published an anti-slavery poem entitled \"The Prospect of Peace\". \n\nBarlow was an ardent patriot in the American Revolution. He was engaged in the Battle of Long Island and served as a chaplain for the 4th Massachusetts Brigade from September 1780 until the close of the Revolutionary War. He was a Mason and he became a good friend to Thomas Paine.\n\nIn 1783, Barlow moved to Hartford, Connecticut. In July 1784, he established a weekly paper called \"American Mercury\", with which he was connected for a year. After \"reading the law\" in an established office, in 1786 he was admitted to the bar. In Hartford, Barlow became a member of a group of young writers including Lemuel Hopkins, David Humphreys, and John Trumbull, known in American literary history as the \"Hartford Wits\". He contributed to the \"Anarchiad\", a series of satirico-political papers. In 1787, he published a long and ambitious poem, \"The Vision of Columbus\", which gave him a considerable literary reputation and was once much read.\n\nIn 1788, he went to France as the agent of Colonel William Duer and the Scioto Land Company, which had been registered in Paris the year before. He was to sell lands in part of the newly organized Northwest Territory (this section is now in Ohio), and recruit immigrants for new settlements. He seems to have been ignorant of the fraudulent character of the company, which did not hold title to the lands it sold and failed disastrously in 1790. He had previously recruited a group of French to emigrate to America. Known as the French 500, most of them were among the founders of Gallipolis, Ohio, the second oldest European-American city founded in the new Northwest Territory. \n\nIn Paris, Barlow became a liberal in religion and an advanced republican in politics. He believed that \"American civilization was world civilization\", and was enthusiastic about the cause of world republicanism. He became involved with the French Revolution, going so far as to be elected to the French Assembly, and being granted French citizenship in 1792. Although he dedicated his \"Vision of Columbus\" to Louis XVI, he joined royal opponents in calling for the execution of the king. Barlow helped Thomas Paine publish the first part of \"The Age of Reason\" while Paine was imprisoned during The Reign of Terror in France.\n\nBarlow remained abroad for several years, spending much of his time in London. There he was a member of the London Society for Constitutional Information. He also published various radical essays, including a volume entitled \"Advice to the Privileged Orders\" (1792). This was proscribed by the British government.\n\nBarlow was appointed as American consul at Algiers in 1795-1797, during the period when Barbary pirates were preying on United States and European shipping. He used State Department funds for bribes and ransoms to free more than 100 American merchant sailors held by pirates. He helped negotiate treaties with the Barbary states of Algiers, Tripoli, and Tunis to avert future seizures of American ships. He returned to the United States in 1805, where he lived in the national capital at his mansion, known as Kalorama, now the name of a neighborhood in Northwest Washington, D.C..\n\nIn 1811, Barlow was appointed as U.S. Minister to France; he sailed across the Atlantic on the \"USS Constitution\". His task was to negotiate an end to the Berlin Decree and the Milan Decree, as well as obtain the release of American ships and crews held by the French during the Napoleonic wars. In October 1812, Barlow set off for Vilnius to negotiate a treaty with the French foreign minister, who was based in Lithuania to prepare for the French invasion of Russia. By the time he arrived, the French army was already in full retreat from Moscow. \n\nBarlow chose to take the southerly route to return to Paris, by way of Krakow and Vienna. He became ill and died of pneumonia on December 26, 1812 in the Polish village of Żarnowiec. A monument was later erected to him there.\n\nIn 1807, he published the epic \"Columbiad\", an extended edition of his \"Vision of Columbus\". It added to his reputation in some quarters, but on the whole it was not well received. It has subsequently been much ridiculed. \n\nThe poem for which he is now best known is his mock heroic \"The Hasty-Pudding\" (1793), first published in \"New York Magazine\" and now a standard item in literary anthologies. In addition, Barlow published \"Conspiracy of Kings, a Poem addressed to the Inhabitants of Europe from another Quarter of the Globe\" (1792). He continued writing political essays, publishing \"Political Writings of Joel Barlow\" (2nd ed., 1796) and \"View of the Public Debt, Receipts and Expenditure of the United States\" (1800). But much of his political speculation never passed beyond his voluminous notebooks, many of which are conserved in Harvard's Houghton Library.\n\nHe also composed a satirical version of the British national anthem \"God Save the King\", called \"God Save the Guillotine\".\n\nHistorian William H. Goetzmann describes Barlow as a cosmopolitan, along with Thomas Jefferson, Benjamin Franklin, engineer Robert Fulton, and Thomas Paine, the last of whom Barlow befriended in France. Barlow believed that the new country of America was a model civilization that prefigured the \"uniting of all mankind in one religion, one language and one Newtonian harmonious whole\" and thought of \"the American Revolution as the opening skirmish of a world revolution on behalf of the rights of all humanity.\" An optimist, he believed that scientific and republican progress, along with religion and people's growing sense of humanity, would lead to the coming of the Millennium. For him, American civilization was world civilization. He projected that these concepts would coalesce around the rebuilding of the temple in Jerusalem.\n\n\n\n"}
{"id": "17145521", "url": "https://en.wikipedia.org/wiki?curid=17145521", "title": "Kamokuiki", "text": "Kamokuiki\n\nKamokuiki (c. 1795 – September 26, 1840) was a grandmother of the last two ruling monarchs of the Kingdom of Hawaii.\n\nKamokuiki was born about 1795 as a daughter of the chief Kanepawale, son of Kaʻihelemoana and Kaʻopa, and his wife chiefess Uaua, daughter of Kaʻehunuiamamaliʻi and Koʻi. Her grandfather's sister was Hikuikekualono, the mother of Kahaopulani, who according to legend hid and nursed the baby Kamehameha I during his infancy when he was being hunted down by Alapainui.\n\nKamokuiki married High Chief Kamanawa II and with him had a son, Caesar Kapaʻakea (1815–1866). She also had a daughter named Kekahili by another husband named Alapaimaloiki. The House of Kalākaua descends from her son Kapaʻakea. The House of Kawānanakoa descends from her daughter Kekahili.\n\nAfter converting to Christianity, Kamokuiki grew disgusted at her husband Kamanawa's many affairs. In 1840, she divorced him for the crime of adultery. Kamanawa was not allowed to remarry while she was alive. Six weeks after the divorce, Kamokuiki died in Honolulu on September 26, 1840.\nTwo days later an autopsy was performed on her body and her stomach was discovered to be \"much inflamed, while everything else was in order.\" Her ex-husband and his accomplice Lonoapuakau were arrested and placed in chains. They had poisoned her \"‘awa\" (a name for kava, a narcotic drink) with fatal doses of the fishing poisons ʻākia and ʻauhuhu. They were both tried by a jury of twelve chiefs and found guilty of her murder. They were hanged in the Honolulu Fort on October 20, 1840, the first two to be prosecuted under the criminal laws adopted in the Hawaiian constitution of 1840.\nCharles Wilkes of the United States Exploring Expedition was in Honolulu during the murder and subsequent trial. Calling her \"Kamakinki\", Wilkes recounted how the cries of the natives mourning the death of Kamokuiki kept him awake at night, describing it as \"one of the most startling and mournful sounds I ever heard which lasted all night, and disturbed the whole town of Honolulu.\"\n\nThis incident left a mark on the family and would negatively affect public opinion of Kamokuiki's grandson Kalākaua during his campaign for the throne in 1874. Later opponents of Kalākaua and his sister and successor Liliʻuokalani would use the murder case as an example of their family's idolatrous tendencies.\n\nKamokuiki's remains are interred in the cemetery of the Kawaiahaʻo Church, in the Kapiʻolani family plot. Her husband Alapaimaloiki, Kūhiō Kalanianaʻole, the father of Queen Kapiʻolani, and other relatives of Elizabeth Kahanu Kalanianaʻole are also buried here. Today, an obelisk marks where the Kapiʻolani family plot is located. Most of her royal descendants are buried at the Royal Mausoleum at Mauna ʻAla.\n\n"}
{"id": "560141", "url": "https://en.wikipedia.org/wiki?curid=560141", "title": "Ken Sugimori", "text": "Ken Sugimori\n\nFrom early 1981 until 1986, Sugimori illustrated a gaming fanzine called \"Game Freak\", which had been started by Satoshi Tajiri. Sugimori discovered the magazine in a dōjinshi shop, and decided to get involved. Eventually, the two decided to pitch an arcade game design idea to Namco; they reworked Game Freak into a development company and produced \"Mendel Palace\". Sugimori is most famous as the character designer and art director for the \"Pokémon\" franchise and designed the first 151 Pokémon with Atsuko Nishida, Motofumi Fujiwara, and Shigeki Morimoto. He has worked on the various \"Pokémon\" movies, trading cards, and other games.\n\nFor \"Pokémon Black\" and \"White\", Sugimori directed a team of 17 people in designing new characters for the games, though he always drew the final designs. He drew much of his inspiration from observing animals in aquariums and zoos. Sugimori has also written and illustrated original manga, including one which was distributed with pre-orders of . When he begins a new character, his process normally involves making a rough sketch, then tracing it onto film paper while polishing it and making the illustration more professional looking. After that, he draws the character many times, changing its proportions until he is satisfied. When designing a new Pokemon, Sugimori stated that \"I do feel that I always want to show new Pokemon that people have never seen before. To do that, I think of ways that I can surprise the players.\"\n\n\n\n\n"}
{"id": "996844", "url": "https://en.wikipedia.org/wiki?curid=996844", "title": "Labor induction", "text": "Labor induction\n\nLabor induction is the process or treatment that stimulates childbirth and delivery. Inducing labor can be accomplished with pharmaceutical or non-pharmaceutical methods. In Western countries, it is estimated that one quarter of pregnant women have their labor medically induced with drug treatment. Inductions are most often performed either with prostaglandin drug treatment alone, or with a combination of prostaglandin and intravenus oxytocin treatment.\n\nCommonly accepted medical reasons for induction include:\n\nInduction of labor in those who are either at or after term improves outcomes for the baby and decreases the number of C-sections performed.\n\nMethods of inducing labor include both pharmacological medication and mechanical or physical approaches.\n\nMechanical and physical approaches can include artificial rupture of membranes or membrane sweeping. The use of intrauterine catheters are also indicated. These work by compressing the cervix mechanically to generate release on prostaglandins in local tissues. There is no direct effect on the uterus.\n\nPharmacological methods include dinoprostone (prostaglandin E2), misoprostol (a prostaglandin E1 analogue), and intravenous oxytocin.\n\n\n\nThe American Congress of Obstetricians and Gynecologists has recommended against elective induction before 41 weeks if there is no medical indication and the cervix is unfavorable. One recent study indicates that labor induction at term (41 weeks) or post-term reduces the rate of caesarean section by 12%, and also reduces fetal death.\nSome observational/retrospective studies have shown that non-indicated, elective inductions before the 41st week of gestation are associated with an increased risk of requiring a caesarean section. Randomized clinical trials have not addressed this question. However, researchers have found that multiparous women who undergo labor induction without medical indicators are not predisposed to caesarean sections. Doctors and patients should have a discussion of risks and benefits when considering an induction of labor in the absence of an accepted medical indiction.\n\nStudies have shown a slight increase in risk of infant mortality for births in the 41st and particularly 42nd week of gestation, as well as a higher risk of injury to the mother and child. Due to the increasing risks of advanced gestation, induction appears to reduce the risk for caesarean delivery after 41 weeks' gestation and possibly earlier.\n\nInducing labor before 39 weeks in the absence of a medical indication (such as hypertension, IUGR, or pre-eclampsia) increases the risk of complications of prematurity including difficulties with respiration, infection, feeding, jaundice, neonatal intensive care unit admissions, and perinatal death.\n\nInducing labour after 34 weeks and before 37 weeks in women with hypertensive disorders (pre-eclampsia, eclampsia, pregnancy-induced hypertension) may lead to better outcomes for the woman but does not improve or worsen outcomes for the baby. More research is needed to produce more certain results. If waters break (membranes rupture) between 24 and 37 weeks' gestation, waiting for the labour to start naturally with careful monitoring of the women and baby is more likely to lead to healthier outcomes. For women over 37 weeks pregnant whose babies are suspected of not coping well in the womb, it is not yet clear from research whether it is best to have an induction or caesarean immediately, or to wait until labour happens by itself. Similarly, there is not yet enough research to show whether it is best to deliver babies prematurely if they are not coping in the womb or whether to wait so that they are less premature when they are born.\n\nClinicians assess the odds of having a vaginal delivery after labor induction by a \"Bishop score\". However, recent research has questioned the relationship between the Bishop score and a successful induction, finding that a poor Bishop score actually may improve the chance for a vaginal delivery after induction. A Bishop Score is done to assess the progression of the cervix prior to an induction. In order to do this, the cervix must be checked to see how much it has effaced, thinned out, and how far dilated it is. The score goes by a points system depending on five factors. Each factor is scored on a scale of either 0–2 or 0–3, any total score less than 5 holds a higher risk of delivering by caesarean section.\n\nSometimes when a woman’s waters break after 37 weeks she is induced instead of waiting for labour to start naturally. This may decrease the risks of infection for the women and baby but more research is needed to find out whether inducing is good for women and babies longer term.\n\nWomen who have had a caesarean section for a previous pregnancy are at risk of having a uterine rupture, when their caesarean scar re-opens. Uterine rupture is very serious for the women and the baby, and induction of labour increases this risk further. There is not yet enough research to determine which method of induction is safest for a women who has had a caesarean section before. There is also no research to say whether it is better for these women and their babies to have an elective caesarean section instead of being induced.\n\nInduced labor may be more painful for the woman. This can lead to the increased use of analgesics and other pain-relieving pharmaceuticals. These interventions have been said to lead to an increased likelihood of caesarean section delivery for the baby. However, studies into this matter show differing results. One study indicated that while overall caesarean section rates from 1990–1997 remained at or below 20%, elective induction was associated with a doubling of the rate of Caesarean section. Another study showed that elective induction in women who were not post-term increased a woman's chance of a C-section by two to three times. A more recent study indicated that induction may increase the risk of caesarean section if performed before the 40th week of gestation, but it has no effect or actually lowers the risk if performed after the 40th week.\n\nThe most recent reviews on the subject of induction and its effect on Cesaerean section indicate that there is no increase with induction and in fact there can be a reduction.\n\nThe Institute for Safe Medication Practices labeled pitocin a \"high-alert medication\" because of the high likelihood of \"significant patient harm when it is used in error.\" Correspondingly, the improper use of pitocin is frequently an issue in malpractice litigation.\n\n\n"}
{"id": "435836", "url": "https://en.wikipedia.org/wiki?curid=435836", "title": "List of human spaceflights", "text": "List of human spaceflights\n\nHuman Spaceflight includes all crewed spaceflights which reached an altitude of at least 100 km (the FAI definition of spaceflight, see Kármán line), or were launched with that intention but failed. The USA has adopted a different definition of spaceflight, requiring an altitude of only . During the 1960s, 13 flights of the US X-15 rocket plane met the US criteria, but only two met the FAI's. These lists include only the latter two flights; see the list of highest X-15 flights for all 13.\nAs of the launch of Soyuz MS-10 on 11 October 2018, there have been 320 crewed spaceflights that reached 100 km or more in altitude (323 attempted crewed flights with three failed attempts), 8 of which were sub-orbital spaceflights. One uncrewed flight, Soyuz 34, was launched in order to provide a return vehicle to the crew of Soyuz 32.\n\nTo date, there have been four fatal missions in which 18 astronauts died.\n\nSince 1961, three countries and one private business have conducted human spaceflight using twelve different spacecraft series, or: \"programs\", \"projects\".\n\nThe Salyut series, Skylab, Mir, ISS, and Tiangong series space stations, with which many of these flights docked in orbit, are not listed separately here. See the detailed lists (links below) for information.\n\nMissions which were intended to reach space but which failed to do so are listed in \"italics\", and fatal missions are marked with asterisks.\n\n\n\n\n"}
{"id": "7218842", "url": "https://en.wikipedia.org/wiki?curid=7218842", "title": "List of human spaceflights, 1961–1970", "text": "List of human spaceflights, 1961–1970\n\nThis is a detailed listing of human spaceflights from 1961 to 1970, spanning the Soviet Vostok and Voskhod programs, the start of the Soviet Soyuz program, the American Mercury and Gemini programs, and the first lunar landings of the American Apollo program.\n"}
{"id": "2179793", "url": "https://en.wikipedia.org/wiki?curid=2179793", "title": "Ludovic Kennedy", "text": "Ludovic Kennedy\n\nSir Ludovic Henry Coverley Kennedy (3 November 191918 October 2009) was a British journalist, broadcaster, humanist and author best known for re-examining cases such as the Lindbergh kidnapping and the murder convictions of Timothy Evans and Derek Bentley, and for his role in the abolition of the death penalty in the United Kingdom.\n\nKennedy was born in 1919 in Edinburgh, the son of a career Royal Navy officer, Edward Coverley Kennedy, and his wife, Rosalind Grant, daughter of Sir Ludovic Grant, 11th Baronet. His mother Rosalind was a cousin of the Conservative politician Robert Boothby, later Lord Boothby. \nHe had two younger sisters, Morar and Katherine. Morar married the playwright Royce Ryton in 1954. Katherine married Major Ion Calvocoressi in 1947.\n\nHe was schooled at Eton College (where he played in a jazz band with Humphrey Lyttelton), and \nstudied for a year at Christ Church, Oxford, until the outbreak of war.\n\nKennedy's father, by then a 60-year-old retired captain, returned to the navy and was given command of HMS \"Rawalpindi\", a hastily militarised P&O steamship, known as an Armed Merchant Cruiser. On 23 November 1939, while on patrol southeast of Iceland the \"Rawalpindi\" encountered two of the most powerful German warships, the small battleships (or battlecruisers) and trying to break out through the GIUK gap into the Atlantic. The \"Rawalpindi\" was able to signal the German ships' location back to base. Despite being hopelessly outgunned, Captain Edward Coverley Kennedy of the \"Rawalpindi\" decided to fight, rather than surrender as demanded by the Germans. \"Scharnhorst\" sank \"Rawalpindi\"; of her 312 crew 275 (including her captain) were killed. Captain Kennedy was posthumously mentioned in dispatches and his decision to fight against overwhelming odds entered the folklore of the Royal Navy. His son Ludovic was twenty years old.\n\nLudovic Kennedy followed his father into the navy; he served as an officer on destroyers, mostly in the same northern seas. His ship (HMS \"Tartar\") was one of those that pursued the battleship \"Bismarck\" following the Battle of the Denmark Strait although he did not witness her sinking because \"Tartar\" went to refuel some hours before the end. Kennedy later wrote about this in \"Pursuit\", his chronicle of the chase and sinking of the \"Bismarck\".\n\nHaving studied for one year at Christ Church, Oxford, before the war, he returned to complete his studies in 1945. At Oxford he helped found the Writers' Club and then sought a means of support while he completed a book on Nelson's captains. After leaving Oxford he began a career as an investigative journalist.\n\nA campaigning, investigative reporter, Kennedy wrote for a number of publications, including \"Newsweek\". From 1953, he edited and introduced the \"First Reading\" radio series on the BBC Third Programme, presenting young writers such as Kingsley Amis and Philip Larkin. Later he became a television journalist and a newsreader on ITV's Independent Television News alongside Robin Day and Chris Chataway. He presented the BBC's flagship current affairs programme \"Panorama\" for several years. Kennedy was interested in miscarriages of justice, and he wrote and broadcast on numerous cases.\n\nA major interest of Kennedy's was naval warfare. He wrote and presented a substantial number of television documentaries for the BBC on maritime history in the Second World War, beginning with Scapa Flow, followed by the dramatic narrative of the sinking of the Bismarck in which he was personally involved. Other subjects included the U-Boat war, the story of HMS Belfast, and the raids on Dieppe and St. Nazaire. \"The Life and Death of the Scharnhorst\" (1971) brought him into contact with survivors of the battlecruiser that had sunk his father's ship . The series included \"Target Tirpitz\" (1973), a history of the extraordinary attempts to sink the feared German battleship. Two of these films led to subsequent books.\n\nIn 1980 he presented an episode of the BBC television series Great Railway Journeys of the World, in which he crossed the US.\n\nFrom 1980 to 1988 he presented the television review programme \"Did You See...?\" He interviewed Peter Cook's character Sir Arthur Streeb-Greebling in \"A Life in Pieces\" in 1990. He appeared as himself in several episodes on the political comedy series \"Yes Minister\". Kennedy was the subject of an episode of \"That Reminds Me\" (2002: season 4, episode 1).\nKennedy also expressed to another journalist that there are too many Blacks on television. \n\n\"Private Eye\" magazine sometimes referred to him as 'Ludicrous Kennedy'. In the long-running BBC sitcom \"Till Death Us Do Part\", Alf Garnett – while attacking BBC personalities – spoke of him as a \"Russian Mick\" (\"Mick\" being an offensive term for an Irishman), meaning \"that \"Ludovich Kennedy!\"\"\n\nKennedy's book \"Pursuit: The Chase and Sinking of the \"Bismarck\"\" () detailed the career of the \"Bismarck\", her sinking of British battlecruiser \"Hood\", and her destruction by the Royal Navy.\n\nHe wrote several books that questioned convictions in a number of notable cases in British criminal history. One of the first miscarriages of justice he investigated was the conviction and hanging of Timothy Evans in his 1961 book \"Ten Rillington Place\" (). Evans was found to have murdered his baby daughter in 1950, but Kennedy contended that he was innocent, and that the murders of his wife and baby had been committed by the serial killer John Christie. Christie was hanged three years after the hanging of Evans, following the discovery of six more bodies at 10 Rillington Place, none of which could be ascribed to Evans. Indeed, two of the skeletons found at the house dated back to the war – long before Evans and his family had moved in. After a long campaign, Evans was pardoned in 1966. The scandal helped in the abolition of the death penalty in the UK. Kennedy's book was filmed in 1970 as \"10 Rillington Place\", starring John Hurt as Evans and Richard Attenborough as Christie.\n\nIn 1985, Kennedy published \"The Airman and the Carpenter\" (), in which he argued that Richard Hauptmann did not kidnap and murder Charles Lindbergh's baby, a crime for which he was executed in 1936. The book was made into a 1996 HBO film \"Crime of the Century\", starring Stephen Rea and Isabella Rossellini.\n\nIn 1990, Kennedy became the advisory committee chairman of \"Just Television\", a television production company dedicated to exposing miscarriages of justice.\n\nIn 2003, he wrote \"36 Murders and 2 Immoral Earnings\" (), in which he analysed a number of noted cases, including the Evans case and those of Derek Bentley and the Birmingham Six, a number of which were affected by claims of police failure, police misconduct or perjury. In it he concluded that the adversarial system of justice in the UK and the United States \"is an invitation to the police to commit perjury, which they frequently do\", and said that he preferred the inquisitorial system.\n\nKennedy also wrote:\n\nIn 1958, Kennedy stood for election to Parliament as the Liberal candidate in the Rochdale by-election called after the death of the sitting Conservative MP, Wentworth Schofield in December 1957. He lost to the Labour candidate, Jack McCann, but achieved an increase in the Liberal vote, pushing the Conservatives into third place. The Rochdale contest was the first British by-election to receive live television coverage (locally, by Granada Television).\n\nKennedy supported Scottish Independence.\n\nIn addition to his writing and campaigning on miscarriages of justice, Kennedy campaigned on a number of other issues.\n\nA lifelong atheist, he published \"All in the Mind: A Farewell To God\" in 1999, in which he discussed his philosophical objections to religion, and the ills he felt had come from Christianity. He was a Distinguished Supporter of the British Humanist Association, he contributed to \"New Humanist\" magazine, he was an Honorary Associate of the National Secular Society and a Distinguished Supporter of the Humanist Society Scotland.\n\nHe was also an advocate of the legalisation of assisted suicide, and was a co-founder and former chair of the Voluntary Euthanasia Society. His book, \"Euthanasia: The Case for the Good Death\", was published in 1990.\n\nKennedy resigned from the Liberal Democrats in 2001, citing the incompatibility of his pro-voluntary euthanasia views with those of the then Liberal Democrat leader Charles Kennedy (no relation), who was a Roman Catholic.\n\nHe then stood as an independent on a platform of legalising voluntary euthanasia in the 2001 general election for the Wiltshire constituency of Devizes. He won 2% of the vote and subsequently rejoined the Liberal Democrats.\n\nIn February 1950 he married the dancer and actress Moira Shearer in the Chapel Royal, Hampton Court Palace. He later remembered their meeting in 1949, when he was reluctantly persuaded by a friend to accept a complimentary ticket to a fancy dress ball held at the Lyceum ballroom in London. Shearer – who had recently become famous for her role in \"The Red Shoes\" – was presenting the prizes at the occasion, and Kennedy later recalled that \"I felt a tremor run through me when I caught sight of her. She looked even lovelier than in the film.\"\n\nSummoning up his courage, he approached the 23-year-old dancer and asked her to dance. She would be delighted, she told him, only \"I don't dance very well.\" She was not, Kennedy revealed, a competent ballroom dancer. The couple had one son and three daughters (Alastair, Ailsa, Rachel and Fiona) from a 56-year marriage that ended with her death on 31 January 2006 at the age of 80.\n\nHe received an honorary doctorate from the University of Strathclyde in 1985.\n\nHe was knighted in 1994 for services to journalism, on the recommendation of John Major's government. Major's predecessor Margaret Thatcher had vetoed Kennedy's knighthood.\n\nKennedy died of pneumonia in a nursing home in Salisbury, Wiltshire, on 18 October 2009, aged 89.\n\n"}
{"id": "18215958", "url": "https://en.wikipedia.org/wiki?curid=18215958", "title": "Methuselah Foundation", "text": "Methuselah Foundation\n\nThe Methuselah Foundation is a non-profit organization co-founded in 2003 by David Gobel and Aubrey de Grey. Its mission is to 'make 90 the new 50 by 2030' by supporting tissue engineering and regenerative medicine therapies. Their work includes: incubating and investing in early-stage life science companies, funding scientific research, providing fiscal sponsorship to aligned projects, and sponsoring inducement prizes. The charity was named after Methuselah, the grandfather of Noah in the Hebrew Bible, whose lifespan was recorded as 969 years.\n\nIn 2000, the foundation was originally conceived by David Gobel as the Longitude Prize Society, named after the British government's Longitude Act, which set up monetary rewards for anyone who could devise a portable, practical solution for determining a ship's longitude. In 2003, the organization was made public as the Methuselah Foundation at the 32nd Annual Meeting of the American Aging Association, where they awarded the first Methuselah Mouse Prize to Andrej Bartke for his work on mice that lived the equivalent of 180 human years.\n\nThe Methuselah Fund was created as an LLC subsidiary of the Methuselah Foundation to incubate and invest in early-stage companies. Investments to date have included: Organovo (NYSE: ONVO), a leader in 3D bioprinting; Silverstone Solutions (acquired by BiologicTx in 2013), a maker of kidney-matching software that enables hospitals and transplant organizations to more quickly and accurately pair patients with compatible donors; Oisin Biotechnologies, a company aiming to remove senescent cells, commonly seen as a hallmark of aging; and Leucadia Therapeutics, a company working to address Alzheimer's disease by restoring the flow of cerebrospinal fluid across the cribriform plate.\n\nThe Methuselah Foundation fiscally sponsors the New Organ Alliance, an initiative working to raise awareness and facilitate research to help alleviate organ donation shortages. In 2013, the foundation announced the New Organ Liver Prize, a $1,000,000 award to the first team that can create a bioengineered or regenerative liver therapy for a \"large mammal, enabling the host to recover in the absence of native liver function and survive three months with a normal lifestyle.\"\n\nIn partnership with the Organ Preservation Alliance, New Organ facilitated a technology roadmap report for organ banking and bioengineering solutions to help address organ shortages. The roadmap was developed through a workshop on May 27, 2015 in Washington, D.C., with funding from the National Science Foundation (NSF) and Methuselah, along with a subsequent roundtable held by the White House Office of Science and Technology Policy (OSTP) on May 28. Two follow-up perspectives were published, \"The Promise of Organ and Tissue Preservation to Transform Medicine\" and \"Bioengineering Priorities on a Path to Ending Organ Shortage.\"\n\nIn 2016, NASA announced the Vascular Tissue Challenge in partnership with the New Organ Alliance. Creating a sufficient blood vessel system – vasculature – is often seen by biomedical researchers as a primary impediment in engineering thick tissues. The Vascular Tissue Challenge offers a $500,000 prize \"to be divided among the first three teams that successfully create thick, metabolically-functional human vascularized organ tissue in a controlled laboratory environment.\"\n\nIn conjunction with the Vascular Tissue Challenge, New Organ Alliance hosted the Vascular Tissue Challenge Roadmapping Workshop – with funding from the NSF – on November 9–10, 2016, at the NASA Research Park.\n\nThe Methuselah Mouse Prize (Mprize) was created to increase scientific and public interest in longevity research by awarding two cash prizes: \"one to the research team that broke the world record for the oldest-ever mouse; and one to the team that developed the most successful late-onset rejuvenation strategy.\" The Mprize was announced publicly in 2003 by David Gobel and Aubrey de Grey at the American Aging Association. The prize for longevity was first won by a research team led by Andrzej Bartke of Southern Illinois University. The prize for rejuvenation first went to Stephen Spindler of the University of California, Riverside. Additionally, in 2009, the first Mprize Lifespan Achievement Award went to Z. Dave Sharp of the University of Texas Health Science Center at San Antonio for extending the lifespan of already aged mice using the pharmaceutical rapamycin.\n\nOn May 30, 2014, at the 43rd Annual Meeting of the American Aging Association, Methuselah Foundation awarded a $10,000 Mprize to Huber Warner for his founding of the National Institute on Aging's Interventions Testing Program.\n\nIn 2013, Methuselah Foundation began a partnership with Organovo to fund the use of their 3D bioprinters at academic research centers for biomedical research. Under the grant program, the foundation committed \"at least $500,000 in direct funding for research projects across several institutions.\" Recipients to date include: Yale School of Medicine (John P. Geibel), UCSF School of Medicine (Edward Hsiao), and the Murdoch Children's Research Institute (Melissa Little).\n\nIn 2015, with funding from the Methuselah Foundation and Life Extension Foundation, the bowhead whale genome was sequenced by João Pedro de Magalhães and his team at the University of Liverpool. The bowhead whale is possibly the longest-lived mammal, capable of living over 200 years. The genome project was undertaken to learn more about the mammal's mechanisms for longevity and resistance to age-related diseases, which are unknown. An assembly of the bowhead whale genome has been made available online to promote further research.\n\nIn 2013, Methuselah began fiscally sponsoring and collaborating with the Organ Preservation Alliance (OPA), an initiative coordinating research and stakeholders for the preservation of tissues and organs. OPA's activities have included: hosting Organ Banking Summits, developing a technology roadmap for organ banking, creating the first Organ and Tissue Preservation Community of Practice with the American Society of Transplantation, organizing an \"Organs on Demand\" workshop at the U.S. Military Academy, publishing an expert-consensus article on organ preservation in \"Nature Biotechnology\", and contributing to the Department of Defense's five organ-banking grant programs, seeding an \"estimated $15 million into collaborations among 35 groups.\"\n\nIn 2006, Methuselah contributed capital and fiscal sponsorship to launch the Supercentenarian Research Foundation (SRF). SRF was formed to study why supercentenarians, people over 110 years of age, live longer than most, and why they die. Eight autopsies of supercentenarians were conducted by SRF, with six indicating senile cardiac transthyretin (TTR) amyloidosis at the time of death. TTR amyloidosis \"amasses in and clogs blood vessels, forcing the heart to work harder and eventually fail.\"\n\nFrom 2003-2009, Methuselah Foundation served as the backbone organization for the Strategies for Engineered Negligible Senescence (SENS) program, a long-term research framework developed by Aubrey de Grey. The SENS program aims to prevent or reverse seven forms of molecular or cellular damage associated with aging.\n\nDuring that time, de Grey and David Gobel established SENS-related research programs on human bioremedial biology – \"getting the crud out\" in Methuselah's parlance – at Rice University and Arizona State University. The programs were the first use of environmental remediation principles directed at reversing \"pollution\" in human cells. Additionally, Methuselah sponsored a series of SENS-focused roundtables and conferences, and funded the writing of \"Ending Aging\", co-authored by de Grey and Michael Rae.\n\nUnder de Grey's continued leadership, SENS spun out from Methuselah as the SENS Research Foundation in 2009.\n\nIn 2004, Methuselah Foundation began a donor initiative called the Methuselah 300 (\"The 300\"), a community of philanthropic donors pledging $25,000 over 25 years, at a minimum of $1,000 annually, toward the organization. The initiative was named after the 300 Spartans who held the pass at Thermopylae in 480 BC during the Greco-Persian Wars. In addition, in 2015, the foundation began memorializing The 300 donors with a monument at St. Thomas Island in the U.S. Virgin Islands.\n\nOn September 16, 2006, Peter Thiel announced a pledge of $3.5 million to the Methuselah Foundation and the SENS program to \"support scientific research into the alleviation and eventual reversal of the debilities caused by aging.\"\n\nIn January 2018, the anonymous principal of the Pineapple Fund donated $1 million to the Methuselah Foundation.\n\n"}
{"id": "1548703", "url": "https://en.wikipedia.org/wiki?curid=1548703", "title": "Nature worship", "text": "Nature worship\n\nNature worship is any of a variety of religious, spiritual and devotional practices that focus on the worship of the nature spirits considered to be behind the natural phenomena visible throughout nature. A nature deity can be in charge of nature, a place, a biotope, the biosphere, the cosmos, or the universe. Nature worship is often considered the primitive source of modern religious beliefs and can be found in theism, panentheism, pantheism, deism, polytheism, animism, totemism, shamanism, paganism. Common to most forms of nature worship is a spiritual focus on the individual's connection and influence on some aspects of the natural world and reverence towards it.\n"}
{"id": "644147", "url": "https://en.wikipedia.org/wiki?curid=644147", "title": "Neomugicha incident", "text": "Neomugicha incident\n\nThe is the name given to the hijacking between May 4 and May 5, 2000, of a Japanese bus by a user of internet forum 2channel after placing a warning on the website.\n\nAn hour after posting a cryptic threat in a thread with the name \"Neomugicha\" (\"Neo-Barley Tea\"; ネオむぎ茶), the 17-year-old hijacked a bus \"the Wakakusu\" (Nishi-Nippon Railroad Co., Ltd.) in Dazaifu, Fukuoka, Japan with a Gyuto knife (牛刀 \"gyuutou\" or Chef's knife), stabbing one passenger to death and injuring 2. The Special Assault Team then stormed the hijacked bus and captured the youth alive.\n\nThe incident came as a shock to both 2channers, who had mocked Neomugicha as a liar, thinking he was posting about an attack that had already occurred, and to the general public.\n\nLater, a poster named \"Neouuroncha\" (\"Neo-Oolong tea\") attempted to imitate Neomugicha by plotting to blow up the Odakyu Electric Railway in Japan and posting warnings about it on 2channel. After the Neomugicha incident, however, the Japanese police were keeping a close eye on 2ch, so he was identified and arrested before his plan could be carried out.\n\nA poster called \"Neomugishu\" (\"Neo-Beer\"; shu=alcohol) also attempted to imitate the incident, planning a terrorist attack on a railway company, but he was also arrested.\n\n\n"}
{"id": "40473285", "url": "https://en.wikipedia.org/wiki?curid=40473285", "title": "Neural network synchronization protocol", "text": "Neural network synchronization protocol\n\nThe Neural network synchronization protocol, abbreviated as NNSP, is built on the application-level layer of the OSI upon TCP/IP. Aiming at secure communication, this protocol's design make use of a concept called neural network synchronization. Server and each connected client must create special type of neural network called Tree Parity Machine then compute outputs of their neural networks and exchange them in an iterative manner through the public channel. By learning and exchanging public outputs of their networks, client's and server's neural networks will be synchronized, meaning they will have identical synaptic weights, after some time. Once synchronization is achieved, weights of networks are used for secret key derivation. For the purposes of encryption/decryption of subsequent communication this symmetric key is used.\n\n"}
{"id": "26565579", "url": "https://en.wikipedia.org/wiki?curid=26565579", "title": "Neuroscience of free will", "text": "Neuroscience of free will\n\nNeuroscience of free will, a part of neurophilosophy, is the study of the interconnections between free will and neuroscience.\n\nAs it has become possible to study the human living brain, researchers have begun to watch decision making processes at work. Findings could carry implications for our sense of agency, moral responsibility, and our understanding of consciousness in general. One of the pioneering studies in this domain was designed by Benjamin Libet, while other studies have attempted to predict participant actions before they make them. \nThe field remains highly controversial. There is no consensus among researchers about the significance of findings, their meaning, or what conclusions may be drawn. The precise role of consciousness in decision making therefore remains unclear.\n\nThinkers like Daniel Dennett or Alfred Mele consider the language used by researchers. They explain that \"free will\" means many different things to different people (e.g. some notions of free will are dualistic, some not). Dennett insists that many important and common conceptions of \"free will\" are compatible with the emerging evidence from neuroscience.\n\nOne significant finding of modern studies is that a person's brain seems to commit to certain decisions before the person becomes aware of having made them. Researchers have found delays of about half a second (discussed in sections below). With contemporary brain scanning technology, other scientists in 2008 were able to predict with 60% accuracy whether subjects would press a button with their left or right hand up to 10 seconds before the subject became aware of having made that choice. These and other findings have led some scientists, like Patrick Haggard, to reject some forms of \"free will\". To be clear, no single study would disprove all forms of free will. This is because the term \"free will\" can encapsulate different hypotheses, each of which must be considered in light of existing empirical evidence.\n\nThere have also been a number of problems regarding studies of free will. Particularly in earlier studies, research relied too much on the introspection of the participants, but introspective estimates of event timing were found to be inaccurate. Many brain activity measures have been insufficient and primitive as there is no good independent brain-function measure of the conscious generation of intentions, choices, or decisions. The conclusions drawn from measurements that \"have\" been made are debatable too, as they don't necessarily tell, for example, what a sudden dip in the readings is representing. In other words, the dip might have nothing to do with unconscious decision, since many other mental processes are going on while performing the task. Some of the research mentioned here has gotten more advanced, however, even recording individual neurons in conscious volunteers. Researcher Itzhak Fried says that available studies do at least suggest consciousness comes in a later stage of decision making than previously expected – challenging any versions of \"free will\" where intention occurs at the beginning of the human decision process.\n\nIt is quite likely that a large range of cognitive operations are necessary to freely press a button. Research at least suggests that our conscious self does not initiate all behavior. Instead, the conscious self is somehow alerted to a given behavior that the rest of the brain and body are already planning and performing. These findings do not forbid conscious experience from playing some moderating role, although it is also possible that some form of unconscious process is what is causing modification in our behavioral response. Unconscious processes may play a larger role in behavior than previously thought.\n\nIt may be possible, then, that our intuitions about the role of our conscious 'intentions' have led us astray; it may be the case that we have confused correlation with causation by believing that conscious awareness necessarily causes the body's movement. This possibility is bolstered by findings in neurostimulation, brain damage, but also research into introspection illusions. Such illusions show that humans do not have full access to various internal processes. The discovery that humans possess a determined will would have implications for moral responsibility. Neuroscientist and author Sam Harris believes that we are mistaken in believing the intuitive idea that intention initiates actions. In fact, Harris is even critical of the idea that free will is 'intuitive': he says careful introspection can cast doubt on free will. Harris argues - \"Thoughts simply arise in the brain. What else could they do? The truth about us is even stranger than we may suppose: The illusion of free will is itself an illusion\". Neuroscientist Walter Jackson Freeman III nevertheless talks about the power of even unconscious systems and actions to change the world according to our intentions. He writes \"our intentional actions continually flow into the world, changing the world and the relations of our bodies to it. This dynamic system is the self in each of us, it is the agency in charge, not our awareness, which is constantly trying to keep up with what we do.\" To Freeman, the power of intention and action can be independent of awareness.\n\nSome thinkers like neuroscientist and philosopher Adina Roskies think these studies can still only show, unsurprisingly, that physical factors in the brain are involved before decision making. In contrast, Haggard believes that \"We feel we choose, but we don't\". Researcher John-Dylan Haynes adds \"How can I call a will 'mine' if I don't even know when it occurred and what it has decided to do?\". Philosophers Walter Glannon and Alfred Mele think some scientists are getting the science right, but misrepresenting modern philosophers. This is mainly because \"free will\" can mean many things: It is unclear what someone means when they say \"free will does not exist\". Mele and Glannon say that the available research is more evidence against any dualistic notions of free will – but that is an \"easy target for neuroscientists to knock down\". Mele says that most discussions of free will are now had in materialistic terms. In these cases, \"free will\" means something more like \"not coerced\" or that \"the person could have done otherwise at the last moment\". The existence of these types of free will is debatable. Mele agrees, however, that science will continue to reveal critical details about what goes on in the brain during decision making.\nThis issue may be controversial for good reason: There is evidence to suggest that people normally associate a belief in free will with their ability to affect their lives. Philosopher Daniel Dennett, author of \"Elbow Room\" and a supporter of deterministic free will, believes scientists risk making a serious mistake. He says that there are types of free will that are incompatible with modern science, but he says those kinds of free will are not worth wanting. Other types of \"free will\" are pivotal to people's sense of responsibility and purpose (see also \"believing in free will\"), and many of these types are actually compatible with modern science.\n\nThe other studies described below have only just begun to shed light on the role that consciousness plays in actions and it is too early to draw very strong conclusions about certain kinds of \"free will\". It is worth noting that such experiments – so far – have dealt only with free will decisions made in short time frames (seconds) and \"may\" not have direct bearing on free will decisions made (\"thoughtfully\") by the subject over the course of \"many\" seconds, minutes, hours or longer. Scientists have also only so far studied extremely simple behaviors (e.g. moving a finger). Adina Roskies points out five areas of neuroscientific research: 1.) action initiation, 2.) intention, 3). decision, 4.) Inhibition and control, and 5.) the phenomenology of agency, and for each of these areas Roskies concludes that the science may be developing our understanding of volition or \"will\", but it yet offers nothing for developing the \"free\" part of the \"free will\" discussion.\n\nThere is also the question of the influence of such interpretations in people's behaviour. In 2008, psychologists Kathleen Vohs and Jonathan Schooler published a study on how people behave when they are prompted to think that determinism is true. They asked their subjects to read one of two passages: one suggesting that behaviour boils down to environmental or genetic factors not under personal control; the other neutral about what influences behaviour. The participants then did a few math problems on a computer. But just before the test started, they were informed that because of a glitch in the computer it occasionally displayed the answer by accident; if this happened, they were to click it away without looking. Those who had read the deterministic message were more likely to cheat on the test. \"Perhaps, denying free will simply provides the ultimate excuse to behave as one likes,\" Vohs and Schooler suggested.\n\nA pioneering experiment in this field was conducted by Benjamin Libet in the 1980s, in which he asked each subject to choose a random moment to flick their wrist while he measured the associated activity in their brain (in particular, the build-up of electrical signal called the Bereitschaftspotential (BP), which was discovered by Kornhuber & Deecke in 1965). Although it was well known that the Bereitschaftspotential (sometimes also termed \"readiness potential\") preceded the physical action, Libet asked how the Bereitschaftspotential corresponded to the felt intention to move. To determine when the subjects felt the intention to move, he asked them to watch the second hand of a clock and report its position when they felt that they had felt the conscious will to move.\nLibet found that the \"unconscious\" brain activity leading up to the \"conscious\" decision by the subject to flick their wrist began approximately half a second \"before\" the subject consciously felt that they had decided to move. Libet's findings suggest that decisions made by a subject are first being made on a subconscious level and only afterward being translated into a \"conscious decision\", and that the subject's belief that it occurred at the behest of their will was only due to their retrospective perspective on the event.\n\nThe interpretation of these findings has been criticized by Daniel Dennett, who argues that people will have to shift their attention from their intention to the clock, and that this introduces temporal mismatches between the felt experience of will and the perceived position of the clock hand. Consistent with this argument, subsequent studies have shown that the exact numerical value varies depending on attention. Despite the differences in the exact numerical value, however, the main finding has held. Philosopher Alfred Mele criticizes this design for other reasons. Having attempted the experiment himself, Mele explains that \"the awareness of the intention to move\" is an ambiguous feeling at best. For this reason he remained skeptical of interpreting the subjects' reported times for comparison with their 'Bereitschaftspotential'.\n\nIn a variation of this task, Haggard and Eimer asked subjects to decide not only when to move their hands, but also to decide \"which hand to move\". In this case, the felt intention correlated much more closely with the \"lateralized readiness potential\" (LRP), an ERP component which measures the difference between left and right hemisphere brain activity. Haggard and Eimer argue that the feeling of conscious will must therefore follow the decision of which hand to move, since the LRP reflects the decision to lift a particular hand.\n\nA more direct test of the relationship between the Bereitschaftspotential and the \"awareness of the intention to move\" was conducted by Banks and Isham (2009). In their study, participants performed a variant of the Libet's paradigm in which a delayed tone followed the button press. Subsequently, research participants reported the time of their intention to act (e.g., Libet's \"W\"). If W were time-locked to the Bereitschaftspotential, W would remain uninfluenced by any post-action information. However, findings from this study show that W in fact shifts systematically with the time of the tone presentation, implicating that W is, at least in part, retrospectively reconstructed rather than pre-determined by the Bereitschaftspotential.\n\nA study conducted by Jeff Miller and Judy Trevena (2009) suggests that the Bereitschaftspotential (BP) signal in Libet's experiments doesn't represent a decision to move, but that it's merely a sign that the brain is paying attention. In this experiment the classical Libet experiment was modified by playing an audio tone indicating to volunteers to decide whether to tap a key or not. The researchers found that there was the same RP signal in both cases, regardless of whether or not volunteers actually elected to tap, which suggests that the RP signal doesn't indicate that a decision has been made.\n\nIn a second experiment, researchers asked volunteers to decide on the spot whether to use left hand or right to tap the key while monitoring their brain signals, and they found no correlation among the signals and the chosen hand. This criticism has itself been criticized by free-will researcher Patrick Haggard, who mentions literature that distinguishes two different circuits in the brain that lead to action: a \"stimulus-response\" circuit and a \"voluntary\" circuit. According to Haggard, researchers applying external stimuli may not be testing the proposed voluntary circuit, nor Libet's hypothesis about internally triggered actions.\n\nLibet's interpretation of the ramping up of brain activity prior to the report of conscious \"will\" continues to draw heavy criticism. Studies have questioned participants' ability to report the timing of their \"will\". Authors have found that preSMA activity is modulated by attention (attention precedes the movement signal by 100ms), and the prior activity reported could therefore have been product of paying attention to the movement. They also found that the perceived onset of intention depends on neural activity that takes place after the execution of action. Transcranial magnetic stimulation (TMS) applied over the preSMA after a participant performed an action shifted the perceived onset of the motor intention backward in time, and the perceived time of action execution forward in time.\n\nOthers have speculated that the preceding neural activity reported by Libet may be an artefact of averaging the time of \"will\", wherein neural activity does not always precede reported \"will\". In a similar replication they also reported no difference in electrophysiological signs before a decision not to move, and before a decision to move.\n\nDespite his findings, Libet himself did not interpret his experiment as evidence of the inefficacy of conscious free will — he points out that although the tendency to press a button may be building up for 500 milliseconds, the conscious will retains a right to veto any action at the last moment. According to this model, unconscious impulses to perform a volitional act are open to suppression by the conscious efforts of the subject (sometimes referred to as \"free won't\"). A comparison is made with a golfer, who may swing a club several times before striking the ball. The action simply gets a rubber stamp of approval at the last millisecond. Max Velmans argues however that \"free won't\" may turn out to need as much neural preparation as \"free will\" (see below).\n\nSome studies have however replicated Libet's findings, whilst addressing some of the original criticisms. A recent study has found that individual neurons were found to fire 2 seconds before a reported \"will\" to act (long before EEG activity predicted such a response). Itzhak Fried replicated Libet's findings in 2011 at the scale of the single neuron. This was accomplished with the help of volunteer epilepsy patients, who needed electrodes implanted deep in their brain for evaluation and treatment anyway. Now able to monitor awake and moving patients, the researchers replicated the timing anomalies that were discovered by Libet and are discussed in the following study. Similarly to these tests, Chun Siong Soon, Anna Hanxi He, Stefan Bode and John-Dylan Haynes have conducted a study in 2013 claiming to be able to predict the choice to sum or subtract before the subject reports it.\n\nWilliam R. Klemm pointed out the inconclusiveness of these tests due to design limitations and data interpretations and proposed less ambiguous experiments, while affirming a stand on the existence of free will like Roy F. Baumeister or Catholic neuroscientists such as Tadeusz Pacholczyk. Adrian G. Guggisberg and Annaïs Mottaz have also challenged Itzhak Fried's findings.\n\nA study by Aaron Schurger and colleagues published in PNAS challenged assumptions about the causal nature of the Bereitschaftspotential itself (and the \"pre-movement buildup\" of neural activity in general), thus denying the conclusions drawn from studies such as Libet's and Fried's. See The Information Philosopher and New Scientist for commentary on this study.\n\nA study by Masao Matsuhashi and Mark Hallett, published in 2008, claims to have replicated Libet's findings without relying on subjective report or clock memorization on the part of participants. The authors believe that their method can identify the time (T) at which a subject becomes aware of his own movement. Matsuhashi and Hallet argue that this time not only varies, but often occurs after early phases of movement genesis have already begun (as measured by the readiness potential). They conclude that a person's awareness cannot be the cause of movement, and may instead only notice the movement.\n\nMatsuhashi and Hallett's study can be summarized thus. The researchers hypothesized that, if our conscious intentions are what causes movement genesis (i.e. the start of an action), then naturally, our conscious intentions should always occur before any movement has begun. Otherwise, if we ever become aware of a movement only after it has already been started, our awareness could not have been the cause of that particular movement. Simply put, conscious intention must precede action if it is its cause.\n\nTo test this hypothesis, Matsuhashi and Hallet had volunteers perform brisk finger movements at random intervals, while not counting or planning when to make such (future) movements, but rather immediately making a movement as soon as they thought about it. An externally controlled \"stop-signal\" sound was played at pseudo random intervals, and the volunteers had to cancel their intent to move if they heard a signal while being aware of their own immediate intention to move. Whenever there \"was\" an action (finger movement), the authors documented (and graphed) any tones that occurred before that action. The graph of tones before actions therefore only shows tones (a) before the subject is even aware of his \"movement genesis\" (or else they would have stopped or \"vetoed\" the movement), and (b) after it is too late to veto the action. This second set of graphed tones is of little importance here.\n\nIn this work, \"movement genesis\" is defined as the brain process of making movement, of which physiological observations have been made (via electrodes) indicating that it may occur before conscious awareness of intent to move (see Benjamin Libet).\n\nBy looking to see when tones started preventing actions, the researchers supposedly know the length of time (in seconds) that exists between when a subject holds a conscious intention to move and performs the action of movement. This moment of awareness (as seen in the graph below) is dubbed \"T\" (the mean time of conscious intention to move). It can be found by looking at the border between tones and no tones. This enables the researchers to estimate the timing of the conscious intention to move without relying on the subject's knowledge or demanding them to focus on a clock. The last step of the experiment is to compare time T for each subject with their Event-related potential (ERP) measures (e.g. seen in this page's lead image), which reveal when their finger movement genesis first begins.\n\nThe researchers found that the time of the conscious intention to move T normally occurred \"too late\" to be the cause of movement genesis. See the example of a subject's graph below on the right. Although it is not shown on the graph, the subject's readiness potentials (ERP) tells us that his actions start at –2.8 seconds, and yet this is substantially earlier than his conscious intention to move, time \"T\" (−1.8 seconds). Matsuhashi and Hallet concluded that the feeling of the conscious intention to move does not cause movement genesis; both the feeling of intention and the movement itself are the result of unconscious processing.\n\nThis study is similar to Libet's in some ways: volunteers were again asked to perform finger extensions in short, self-paced intervals. In this version of the experiment, researchers introduced randomly timed \"stop tones\" during the self paced movements. If participants were not conscious of any intention to move, they simply ignored the tone. On the other hand, if they were aware of their intention to move at the time of the tone, they had to try to veto the action, then relax for a bit before continuing self-paced movements. This experimental design allowed Matsuhashi and Hallet to see when, once the subject moved his finger, any tones occurred. The goal was to identify their own equivalent of Libet's W, their own estimation of the timing of the conscious intention to move, which they would call \"T\"(time)\n\nTesting the hypothesis that 'conscious intention occurs after movement genesis has already begun' required the researchers to analyse the distribution of responses to tones before actions. The idea is that, after time T, tones will lead to vetoing and thus a reduced representation in the data. There would also be a point of no return P where a tone was too close to the movement onset for the movement to be vetoed. In other words, the researchers were expecting to see the following on the graph: many unsuppressed responses to tones while the subjects are not yet aware of their movement genesis, followed by a drop in the number of unsuppressed responses to tones during a certain period of time during which the subjects are conscious of their intentions and are stopping any movements, and finally a brief increase again in unsuppressed responses to tones when the subjects do not have the time to process the tone and prevent an action – they have passed the action's \"point of no return\". That is exactly what the researchers found (see the graph on the right, below).\nThe graph shows the times at which unsuppressed responses to tones occurred when the volunteer moved. He showed many unsuppressed responses to tones (dubbed \"tone events\" on the graph) on average up until 1.8 seconds before movement onset, but a significant decrease in tone events immediately after that time. Presumably this is because the subject usually became aware of his intention to move at about −1.8 seconds, which is then labelled point T. Since most actions are vetoed if a tone occurs after point T, there are very few tone events represented during that range. Finally, there is a sudden increase in the number of tone events at 0.1 seconds, meaning this subject has passed point P. Matsuhashi and Hallet were thus able to establish an average time T (−1.8 seconds) without subjective report. This, they compared to ERP measurements of movement, which had detected movement beginning at about −2.8 seconds on average for this participant. Since T — like Libet's original W — was often found after movement genesis had already begun, the authors concluded that the generation of awareness occurred afterwards or in parallel to action, but most importantly, that it was probably not the cause of the movement.\n\nHaggard describes other studies at the neuronal levels as providing \"a reassuring confirmation of previous studies that recorded neural populations\" such as the one just described. Note that these results were gathered using finger movements, and may not necessarily generalize to other actions such as thinking, or even other motor actions in different situations. Indeed, the human act of planning has implications for free will and so this ability must also be explained by any theories of unconscious decision making. Philosopher Alfred Mele also doubts the conclusions of these studies. He explains that simply because a movement may have been initiated before our \"conscious self\" has become aware of it does not mean our consciousness does not still get to approve, modify, and perhaps cancel (called vetoing) the action.\n\nThe possibility that human \"free won't\" is also the prerogative of the subconscious is being explored.\n\nRecent research by Simone Kühn and Marcel Brass suggests that our consciousness may not be what causes some actions to be vetoed at the last moment. First of all, their experiment relies on the simple idea that we ought to know when we consciously cancel an action (i.e. we should have access to that information). Secondly, they suggest that access to this information means humans should find it \"easy\" to tell, just after completing an action, whether it was impulsive (there being no time to decide) and when there was time to deliberate (the participant decided to allow/not to veto the action). The study found evidence that subjects could not tell this important difference. This again leaves some conceptions of free will vulnerable to the introspection illusion. The researchers interpret their results to mean that the decision to \"veto\" an action is determined subconsciously, just as the initiation of the action may have been subconscious in the first place.\n\nThe experiment involved asking volunteers to respond to a go-signal by pressing an electronic \"go\" button as quickly as possible. In this experiment the go-signal was represented as a visual stimulus shown on a monitor (e.g. a green light as shown on the picture). The participants' reaction times (RT) were gathered at this stage, in what was described as the \"primary response trials\".\n\nThe primary response trials were then modified, in which 25% of the go-signals were subsequently followed by an additional signal – either a \"stop\" or \"decide\" signal. The additional signals occurred after a \"signal delay\" (SD), a random amount of time up to 2 seconds after the initial go-signal. They also occurred equally, each representing 12.5% of experimental cases. These additional signals were represented by the initial stimulus changing colour (e.g. to either a red or orange light). The other 75% of go-signals were not followed by an additional signal – and was therefore considered the \"default\" mode of the experiment. The participants' task of responding as quickly as possible to the initial signal (i.e. pressing the \"go\" button) remained.\n\nUpon seeing the initial go-signal, the participant would immediately intend to press the \"go\" button. The participant was instructed to cancel their immediate intention to press the \"go\" button if they saw a stop signal. The participant was instructed to select randomly (at their leisure) between either pressing the \"go\" button, or not pressing it, if they saw a decide signal. Those trials in which the decide signal was shown after the initial go-signal (\"decide trials\"), for example, required that the participants prevent themselves from acting impulsively on the initial go-signal and then decide what to do. Due to the varying delays, this was sometimes impossible (e.g. some decide signals simply appeared too \"late\" in the process of them both intending to and pressing the go button for them to be obeyed).\n\nThose trials in which the subject reacted to the go-signal impulsively without seeing a subsequent signal show a quick RT of about 600 ms. Those trials in which the decide signal was shown too late, and the participant had already enacted their impulse to press the go-button (i.e. had not decided to do so), also show a quick RT of about 600 ms. Those trials in which a stop signal was shown and the participant successfully responded to it, do not show a response time. Those trials in which a decide signal was shown, and the participant decided not to press the go-button, also do not show a response time. Those trials in which a decide signal was shown, and the participant had not already enacted their impulse to press the go-button, but (in which it was theorised that they) had had the opportunity to decide what to do, show a comparatively slow RT, in this case closer to 1400 ms.\n\nThe participant was asked at the end of those \"decide trials\" in which they had actually pressed the go-button whether they had acted impulsively (without enough time to register the decide signal before enacting their intent to press the go-button in response to the initial go-signal stimulus), or had acted based upon a conscious decision made after seeing the decide signal. Based upon the response time data however, it appears there was discrepancy between when the user thought they had had the opportunity to decide (and had therefore not acted on their impulses) – in this case deciding to press the go-button, and when they thought they had acted impulsively (based upon the initial go-signal) – where the decide signal came too late to be obeyed.\n\nKuhn and Brass wanted to test participant self-knowledge. The first step was that after every decide trial, participants were next asked whether they had actually had time to decide. Specifically, the volunteers were asked to label each decide trial as either failed-to-decide (the action was the result of acting impulsively on the initial go-signal) or successful decide (the result of a deliberated decision). See the diagram on the right for this decide trial split: failed-to-decide and successful decide; the next split in this diagram (participant correct or incorrect) will be explained at the end of this experiment. Note also that the researchers sorted the participants’ successful decide trials into \"decide go\" and \"decide nogo\", but were not concerned with the nogo trials since they did not yield any RT data (and are not featured anywhere in the diagram on the right). Note that successful stop trials did not yield RT data either.\nKuhn and Brass now knew what to expect: primary response trials, any failed stop trials, and the \"failed-to-decide\" trials were all instances where the participant obviously acted impulsively – they would show the same quick RT. In contrast, the \"successful \"decide\"\" trials (where the decision was a \"go\" and the subject moved) should show a slower RT. Presumably, if deciding whether to veto is a conscious process, volunteers should have no trouble distinguishing impulsivity from instances of true deliberate continuation of a movement. Again, this is important since decide trials require that participants rely on self-knowledge. Note that stop trials cannot test self-knowledge because if the subject \"does\" act, it is obvious to them that they reacted impulsively.\n\nUnsurprisingly, the recorded RTs for the primary response trials, failed stop trials, and \"failed-to-decide\" trials all showed similar RTs: 600 ms seems to indicate an impulsive action made without time to truly deliberate. What the two researchers found next was not as easy to explain: while some \"successful decide\" trials did show the tell-tale slow RT of deliberation (averaging around 1400 ms), participants had also labelled many impulsive actions as \"successful decide\". This result is startling because participants should have had no trouble identifying which actions were the results of a conscious \"I will not veto\", and which actions were un-deliberated, impulsive reactions to the initial go-signal. As the authors explain:\n\nIn decide trials the participants, it seems, were not able to reliably identify whether they had really had time to decide – at least, not based on internal signals. The authors explain that this result is difficult to reconcile with the idea of a conscious veto, but simple to understand if the veto is considered an unconscious process. Thus it seems that the intention to move might not only arise from the subconscious, but it may only be inhibited if the subconscious says so. This conclusion could suggest that the phenomenon of \"consciousness\" is more of narration than direct arbitration (i.e. unconscious processing causes all thoughts, and these thoughts are again processed subconsciously).\n\nAfter the above experiments, the authors concluded that subjects sometimes could not distinguish between \"producing an action without stopping and stopping an action before voluntarily resuming\", or in other words, they could not distinguish between actions that are immediate and impulsive as opposed to delayed by deliberation. To be clear, one assumption of the authors is that all the early (600 ms) actions are unconscious, and all the later actions are conscious. These conclusions and assumptions have yet to be debated within the scientific literature or even replicated (it is a very early study).\n\nThe results of the trial in which the so-called \"successful decide\" data (with its respective longer time measured) was observed may have possible implications for our understanding of the role of consciousness as the modulator of a given action or response — and these possible implications cannot merely be omitted or ignored without valid reasons, specially when the authors of the experiment suggest that the late decide trials were actually deliberated.\n\nIt is worth noting that Libet consistently referred to a veto of an action that was initiated endogenously. That is, a veto that occurs in the absence of external cues, instead relying on only internal cues (if any at all). This veto may be a different type of veto than the one explored by Kühn and Brass using their decide signal.\n\nDaniel Dennett also argues that no clear conclusion about volition can be derived from Benjamin Libet's experiments supposedly demonstrating the non-existence of conscious volition. According to Dennett, ambiguities in the timings of the different events involved. Libet tells when the readiness potential occurs objectively, using electrodes, but relies on the subject reporting the position of the hand of a clock to determine when the conscious decision was made. As Dennett points out, this is only a report of where it \"seems\" to the subject that various things come together, not of the objective time at which they actually occur.\nSuppose Libet knows that your readiness potential peaked at millisecond 6,810 of the experimental trial, and the clock dot was straight down (which is what you reported you saw) at millisecond 7,005. How many milliseconds should he have to add to this number to get the time you were conscious of it? The light gets from your clock face to your eyeball almost instantaneously, but the path of the signals from retina through lateral geniculate nucleus to striate cortex takes 5 to 10 milliseconds — a paltry fraction of the 300 milliseconds offset, but how much longer does it take them to get to \"you\". (Or are you located in the striate cortex?) The visual signals have to be processed before they arrive at wherever they need to arrive for you to make a conscious decision of simultaneity. Libet's method presupposes, in short, that we can locate the \"intersection\" of two trajectories:\nso that these events occur side-by-side as it were in place where their simultaneity can be noted.\n\nIn early 2016, PNAS published a paper by researchers in Berlin, Germany, \"The point of no return in vetoing self-initiated movements\", in which the authors set out to investigate whether human subjects had the ability to veto an action (in this study, a movement of the foot) after the detection of its Bereitschaftspotential (BP). The Bereitschaftspotential, which was discovered by Kornhuber & Deecke in 1965, is an instance of unconscious electrical activity within the motor cortex, quantified by the use of EEG, that occurs moments before a motion is performed by a person: it is considered a signal that the brain is \"getting ready\" to perform the motion. The study found evidence that these actions can be vetoed even after the BP is detected (i. e. after it can be seen that the brain has started preparing for the action). The researchers maintain this is evidence for the existence of at least some degree of free will in humans: previously, it had been argued that, given the unconscious nature of the BP and its usefulness in predicting a person's movement, these are movements that are initiated by the brain without the involvement of the conscious will of the person. The study showed that subjects were able to \"override\" these signals and stop short of performing the movement that was being anticipated by the BP. Furthermore, researchers identified what was termed a \"point of no return\": once the BP is detected for a movement, the person could refrain from performing the movement only if they attempted to cancel it 200 milliseconds or longer before the onset of the movement. After this point, the person was unable to avoid performing the movement. Previously, Kornhuber & Deecke underlined that absence of conscious will during the early Bereitschaftspotential (termed BP1) is not a proof of the non-existence of free will, as also unconscious agendas may be free and non-deterministic. According to their suggestion, man has relative freedom, i.e. freedom in degrees, that can be in- or decreased through deliberate choices that involve both conscious and unconscious (panencephalic) processes.\n\nDespite criticisms, experimenters are still trying to gather data that may support the case that conscious \"will\" can be predicted from brain activity. fMRI machine learning of brain activity (multivariate pattern analysis) has been used to predict the user choice of a button (left/right) up to 7 seconds before their reported will of having done so. Brain regions successfully trained for prediction included the frontopolar cortex (anterior medial prefrontal cortex) and precuneus/posterior cingulate cortex (medial parietal cortex). In order to ensure report timing of conscious \"will\" to act, they showed the participant a series of frames with single letters (500ms apart), and upon pressing the chosen button (left or right) they were required to indicate which letter they had seen at the moment of decision. This study reported a statistically significant 60% accuracy rate, which may be limited by experimental setup; machine learning data limitations (time spent in fMRI) and instrument precision.\n\nAnother version of the fMRI multivariate pattern analysis experiment was conducted using an abstract decision problem, in an attempt to rule out the possibility of the prediction capabilities being product of capturing a built-up motor urge. Each frame contained a central letter like before, but also a central number, and a surrounding 4 possible \"answers numbers\". The participant first chose in their mind whether they wished to perform an addition or difference (subtraction) operation (and noted the central letter on the screen at the time of this decision). The participant then performed the mathematical operation based on the central numbers shown in the next two frames. In the following frame the participant then chose the \"answer number\" corresponding to the result of the operation. They were further presented with a frame which allowed them to indicate the central letter appearing on the screen at the time of their original decision. This version of the experiment discovered a brain prediction capacity of up to 5 seconds before the conscious will to act.\n\nMultivariate pattern analysis using EEG has suggested that an evidence based perceptual decision model may be applicable to free will decisions. It was found that decisions could be predicted by neural activity immediately after stimulus perception. Furthermore, when the participant was unable to determine the nature of the stimulus the recent decision history predicted the neural activity (decision). The starting point of evidence accumulation was in effect shifted towards a previous choice (suggesting a priming bias). Another study has found that subliminally priming a participant for a particular decision outcome (showing a cue for 13ms) could be used to influence free decision outcomes. Likewise, it has been found that decision history alone can be used to predict future decisions. The prediction capacities of the Soon et al. (2008) experiment were successfully replicated using a linear SVM model based on participant decision history alone (without any brain activity data). Despite this, a recent study has sought to confirm the applicability of a perceptual decision model to free will decisions. When shown a masked and therefore invisible stimulus, participants were asked to either guess between a category or make a free decision for a particular category. Multivariate pattern analysis using fMRI could be trained on \"free decision\" data to successfully predict \"guess decisions\", and trained on \"guess data\" in order to predict \"free decisions\" (in the precuneus and cuneus region).\n\nContemporary voluntary decision prediction tasks have been criticised based on the possibility the neuronal signatures for pre-conscious decisions could actually correspond to lower conscious processing rather than unconscious processing. People may be aware of their decisions before making their report yet need to wait several seconds to be certain. Such a model does not however explain what is left unconscious if everything can be conscious at some level (and the purpose of defining separate systems). Yet limitations remain in free will prediction research to date. In particular, the prediction of considered judgements from brain activity involving thought processes beginning minutes rather than seconds before a conscious will to act, including the rejection of a conflicting desire. Such are generally seen to be the product of sequences of evidence accumulating judgements.\n\nIt has been suggested that sense authorship is an illusion. Unconscious causes of thought and action might facilitate thought and action, while the agent experiences the thoughts and actions as being dependent on conscious will. We may over-assign agency because of the evolutionary advantage that once came with always suspecting there might be an agent doing something (e.g. predator). The idea behind retrospective construction is that, while part of the \"yes, I did it\" feeling of agency seems to occur during action, there also seems to be processing performed after the fact – after the action is performed – to establish the full feeling of agency.\n\nUnconscious agency processing can even alter, in the moment, how we perceive the timing of sensations or actions. Kühn and Brass apply retrospective construction to explain the two peaks in \"successful decide\" RT's. They suggest that the late decide trials were actually deliberated, but that the impulsive early decide trials that should have been labelled \"failed to decide\" were mistaken during unconscious agency processing. They say that people \"persist in believing that they have access to their own cognitive processes\" when in fact we do a great deal of automatic unconscious processing before conscious perception occurs.\n\nIt should be noted that criticism to Wegner's claims regarding the significance of introspection illusion for the notion of free will has been published.\n\nSome research suggests that TMS can be used to manipulate the perception of authorship of a specific choice. Experiments showed that neurostimulation could affect which hands people move, even though the experience of free will was intact. An early TMS study revealed that activation of one side of the neocortex could be used to bias the selection of one's opposite side hand in a forced-choice decision task. Ammon and Gandevia found that it was possible to influence which hand people move by stimulating frontal regions that are involved in movement planning using transcranial magnetic stimulation in the left or right hemisphere of the brain.\n\nRight-handed people would normally choose to move their right hand 60% of the time, but when the right hemisphere was stimulated they would instead choose their left hand 80% of the time (recall that the right hemisphere of the brain is responsible for the left side of the body, and the left hemisphere for the right). Despite the external influence on their decision-making, the subjects continued to report that they believed their choice of hand had been made freely. In a follow-up experiment, Alvaro Pascual-Leone and colleagues found similar results, but also noted that the transcranial magnetic stimulation must occur within 200 milliseconds, consistent with the time-course derived from the Libet experiments.\n\nIn late 2015, a team of researchers from the UK and the US published a paper demonstrating similar findings. The researchers concluded that \"motor responses and the choice of hand can be modulated using tDCS\". However, a different attempt by Sohn \"et al.\" failed to replicate such results; later, Jeffrey Gray wrote in his book \"Consciousness: Creeping up on the Hard Problem\" that tests looking for the influence of electromagnetic fields on brain function have been universally negative in their result.\n\nVarious studies indicate that the perceived intention to move (have moved) can be manipulated. Studies have focused on the pre-supplementary motor area (pre-SMA) of the brain, in which readiness potential indicating the beginning of a movement genesis has been recorded by EEG. In one study, directly stimulating the pre-SMA caused volunteers to report a feeling of intention, and sufficient stimulation of that same area caused physical movement. In a similar study, it was found that people with no visual awareness of their body can have their limbs be made to move without having any awareness of this movement, by stimulating premotor brain regions. When their parietal cortices were stimulated, they reported an urge (intention) to move a specific limb (that they wanted to do so). Furthermore, stronger stimulation of the parietal cortex resulted in the illusion of having moved without having done so.\n\nThis suggests that awareness of an intention to move may literally be the \"sensation\" of the body's early movement, but certainly not the cause. Other studies have at least suggested that \"The greater activation of the SMA, SACC, and parietal areas during and after execution of internally generated actions suggests that an important feature of internal decisions is specific neural processing taking place during and after the corresponding action. Therefore, awareness of intention timing seems to be fully established only after execution of the corresponding action, in agreement with the time course of neural activity observed here.\"\n\nAnother experiment involved an electronic ouija board where the device's movements were manipulated by the experimenter, while the participant was led to believe they were entirely self-conducted. The experimenter stopped the device on occasions and asked the participant how much they themselves felt like they wanted to stop. The participant also listened to words in headphones; and it was found that if experimenter stopped next to an object that came through the headphones they were more likely to say they wanted to stop there. If the participant perceived having the thought at the time of the action, then it was assigned as intentional. It was concluded that a strong illusion of perception of causality requires: priority (we assume the thought must precede the action), consistency (the thought is about the action), and exclusivity (no other apparent causes or alternative hypotheses).\n\nLau et al. set up an experiment where subjects would look at an analogue-style clock, and a red dot would move around the screen. Subjects were told to click the mouse button whenever they felt the intention to do so. One group was given a transcranial magnetic stimulation (TMS) pulse, and the other was given a sham TMS. Subjects in the intention condition were told to move the cursor to where it was when they felt the inclination to press the button. In the movement condition, subjects moved their cursor to where it was when they physically pressed the button. Results showed the TMS was able to shift the perceived intention forward by 16 ms, and shifted back the 14 ms for the movement condition. Perceived intention could be manipulated up to 200 ms after the execution of the spontaneous action, indicating that the perception of intention occurred after the executive motor movements. Often it is thought that if free will were to exist, it would require intention to be the causal source of behavior. These results show that intention may not be the causal source of all behavior.\n\nThe idea that intention co-occurs with (rather than causes) movement is reminiscent of \"forward models of motor control\" (or FMMC, which have been used to try to explain inner speech). FMMCs describe parallel circuits: movement is processed in parallel with other predictions of movement; if the movement matches the prediction – the feeling of agency occurs. FMMCs have been applied in other related experiments. Metcalfe and her colleagues used an FMMC to explain how volunteers determine whether they are in control of a computer game task. On the other hand, they acknowledge other factors too. The authors attribute feelings of agency to desirability of the results (see self serving biases) and top-down processing (reasoning and inferences about the situation).\n\nIn this case, it is by the application of the forward model that one might imagine how other consciousness processes could be the result of efferent, predictive processing. If the conscious self is the efferent copy of actions and vetoes being performed, then the consciousness is a sort of narrator of what is already occurring in the body, and an incomplete narrator at that. Haggard, summarizing data taken from recent neuron recordings, says \"these data give the impression that conscious intention is just a subjective corollary of an action being about to occur\". Parallel processing helps explain how we might experience a sort of contra-causal free will even if it were determined.\n\nHow the brain constructs consciousness is still a mystery, and cracking it open would have a significant bearing on the question of free will. Numerous different models have been proposed, for example, the Multiple Drafts Model which argues that there is no central Cartesian theater where conscious experience would be represented, but rather that consciousness is located all across the brain. This model would explain the delay between the decision and conscious realization, as experiencing everything as a continuous 'filmstrip' comes behind the actual conscious decision. In contrast, there exist models of Cartesian materialism that have gained recognition by neuroscience, implying that there might be special brain areas that store the contents of consciousness; this does not, however, rule out the possibility of a conscious will. Other models such as epiphenomenalism argue that conscious will is an illusion, and that consciousness is a by-product of physical states of the world. Work in this sector is still highly speculative, and researchers favor no single model of consciousness. (See also: Philosophy of mind.)\n\nVarious brain disorders implicate the role of unconscious brain processes in decision making tasks. Auditory hallucinations produced by Schizophrenia seem to suggest a divergence of will and behaviour. The left brain of people whose hemispheres have been disconnected has been observed to invent explanations for body movement initiated by the opposing (right) hemisphere, perhaps based on the assumption that their actions are consciously willed. Likewise, people with 'alien hand syndrome' are known to conduct complex motor movements against their will.\n\nA neural model for voluntary action proposed by Haggard comprises two major circuits. The first involving early preparatory signals (basal ganglia substantia nigra and striatum), prior intention and deliberation (medial prefrontal cortex), motor preparation/readiness potential (preSMA and SMA), and motor execution (primary motor cortex, spinal cord and muscles). The second involving the parietal-pre-motor circuit for object-guided actions, for example grasping (premotor cortex, primary motor cortex, primary somatosensory cortex, parietal cortex, and back to the premotor cortex). He proposed that voluntary action involves external environment input ('when decision'), motivations/reasons for actions (early 'whether decision'), task and action selection ('what decision'), a final predictive check (late 'whether decision') and action execution.\n\nAnother neural model for voluntary action also involves what, when, and whether (WWW) based decisions.\nThe 'what' component of decisions is considered a function of the anterior cingulate cortex, which is involved in conflict monitoring. The timing ('when') of the decisions are considered a function of the preSMA and SMA, which is involved in motor preparation.\nFinally, the 'whether' component is considered a function of the dorsal medial prefrontal cortex.\n\nMartin Seligman and others criticize the classical approach in science which views animals and humans as \"driven by the past\", and suggest instead that people and animals draw on experience to evaluate prospects they face, and act accordingly. The claim is made that this purposive action includes evaluation of possibilities that have never occurred before, and is experimentally verifiable.\n\nSeligman and others argue that free will and the role of subjectivity in consciousness can be better understood by taking such a \"prospective\" stance on cognition, and that \"accumulating evidence in a wide range of research suggests [this] shift in framework\".\n\n\n"}
{"id": "6932050", "url": "https://en.wikipedia.org/wiki?curid=6932050", "title": "Nicholas Tomalin", "text": "Nicholas Tomalin\n\nNicholas Osborne Tomalin (30 October 1931 – 17 October 1973) was an English journalist and writer.\n\nTomalin was the son of Miles Tomalin, a Communist poet and veteran of the Spanish Civil War. He studied English literature at Trinity Hall, Cambridge. As a student he was President of the Cambridge Union and editor of the prestigious undergraduate \"Granta\" magazine. He graduated in 1954 and began work as a foreign correspondent for various London newspapers. He married fellow Cambridge graduate Claire Tomalin in 1955 and they had three daughters and two sons. In spite of numerous affairs on his part (and hers), they remained together until his death.\n\nHe later co-wrote a book with Ron Hall about amateur sailor Donald Crowhurst's failed attempt to circumnavigate the world and subsequent suicide.\nHis article \"The General Goes Zapping Charlie Cong\" was included in Tom Wolfe's collection \"The New Journalism\", which was a collection of non-fiction pieces emblematic of a new movement of reporting aimed at revolutionising the field.\n\nTomalin's articles often began with bombastic statements on their subject matter. The most famous of these is: \"The only qualities essential for real success in journalism are ratlike cunning, a plausible manner and a little literary ability\".\n\nTomalin was killed in Israel by a Syrian wire-guided missile on 17 October 1973 while reporting on the Yom Kippur War.\n\nIn November 2005 the journalism trade publication \"Press Gazette\" named Tomalin among its top forty 'journalists of the modern era'.\n"}
{"id": "3410556", "url": "https://en.wikipedia.org/wiki?curid=3410556", "title": "Overkill (G.I. Joe)", "text": "Overkill (G.I. Joe)\n\nOverkill is a character from the toyline from Hasbro, which has spawned comics and cartoon series. He first appeared in the 1989 \"\" incarnation of the series. He also appears in \"\".\n\nIf the file cards that came with the Overkill action figures are to be believed, there are actually two different characters that owned the name Overkill. Both versions of the character are the leader of the B.A.T.s (short for Battle Android Troopers) that are controlled by the ruthless Cobra Organization.\n\nThe first Overkill was an experimental prototype of the B.A.T. with an advanced computer system and tactical logic programs but was considered by Cobra as too expensive to move to mass production. Despite being a soulless automaton, this Overkill is frequently referred to as a \"he\".\n\nThe second version appeared in 2003 as part of the G.I. Joe vs. Cobra series. This Overkill is a cyborg, which is half-man and half-machine. The file card spells his name as \"Over Kill\" rather than one word although news releases and the credits in spelled it as \"Overkill\".\n\nOverkill made his first comic appearance in the Devil's Due G.I. Joe comics and was revealed to be Robert Skelton, the infamous SAW Viper from the Marvel Comics, who had killed a number of Joes, including the Joes' medical officer Doc, the tank driver Heavy Metal, and the A.W.E. Striker driver Crankcase. Skelton is recruited by Serpentor and his new organization, the Coil. This is after ten men are sent to bring him in, he kills four.\n\nHe chooses the name Overkill, in reference to his impressive body count. He participates in the battle against the Joes and Cobra on Cobra Island and fights with General Hawk, and would have killed him had Kamakura not intervened and saved Hawk. Wounded, Overkill made his way onto a submarine with an escaping Cobra Commander and collapses, expressing the desire for power.\n\nHe was experimented on by Cobra scientists and given his own mobile targeting system and robotic implants (including cutting off his hand for an implant). He did battle with the Joes in Badhikstan and proved to be a formidable opponent before being defeated by Snake Eyes, he begged Snake Eyes to kill him, as he did not want to live like this, but Snake Eyes refused to and Overkill was last seen lying in the desert.\n\nHe was later a patient at a Cobra facility raided by Torpedo, Wet-Suit, and a group of Navy SEALs.\n\nHe was killed in the America's Elite series, during Tomax's raid on \"The Coffin\", a prison run by G.I. Joe.\n\nThe version of Overkill also has a comic appearance, in GI Joe: Sigma 6 #3 also from Devil's Due. In the Sigma 6 comics, each issue featured one or two Joes paired up against a Cobra character. Overkill's Joe opponent was Tunnel Rat. The storyline has Tunnel Rat alone in the Arctic without his Sigma suit having to try and stop Overkill and his army of B.A.T.s. A preview of the comic is available at the Devil's Due minisite for the Sigma 6 comics.\n\nOverkill was first released as an action figure in 1991, as part of the Talking Battle Commanders line.\n\nOverkill made his first animated appearance in the DiC-produced \"\" animated series, voiced by Dale Wilson. Here he appeared to be fully robotic, and his appearance was that of his first action figure. His key episode in this series is \"The Eliminator\", in which he receives an upgrade via the Eliminator chip, which makes him stronger, faster and smarter. This puts him in Cobra Commander's good graces.\n\nOther DiC appearances include:\n\nOverkill returned in the CGI movie \"\" by Reel FX, now clearly a half-human and half-machine cyborg version, with a variety of robotic appendages and a tendency to repeat the end of his sentences in a manic tone of voice. He serves Cobra as Doctor Mindbender's lab assistant and BAT commander. Overkill looks down upon \"organics\", and believes he and his BATs are better candidates to rule the world. In a series of fortuitous circumstances, he makes an alliance with \"Venomous Maximus\", a mutated and brainwashed General Hawk. Together, they attempted to remove Cobra Commander from power, and control Cobra themselves. The Joes are able to turn Maximus back into Hawk, and the Cobra base begins to explode around Overkill, who IS seemingly crushed in the ensuing explosion. However, following the credits, Overkill's robotic hand rises up on the screen. This has been theorized as a preview of the abandoned \"Robot Revolution\" line rumored to be following \"Valor vs. Venom\".\n\nOverkill returns in \"\", he is now inside a rejuvenation tank and wearing new armor, appearing far more robotic than when he last appeared. He is also revamped from a monotonous automaton lab assistant to Cobra's chief scientist. He is smarter and more devious, with a free will of his own.\n\nOverkill is still attempting to overthrow Cobra Commander with his new B.A.T.s, this time in secret, and has a new more powerful BAT called Overlord Vector, which he uses to kidnap General Hawk's son, Scott. Overkill believed Scott had hacked into Cobra's computers, and possessed a formula which would allow his B.A.T.s to evolve as they fought. His kidnapping of Scott ultimately led the Joes to Cobra's underwater headquarters. However, when Overkill scanned Scott's mind, he found that his discovery was just pure dumb luck. Overkill fled as Snake Eyes battled his creation, Overlord Vector. Overkill managed to escape, and still resides in his tank inside his lab in the underground Cobra base, now planning to get hold of the power stones sought by the Joes and Cobra, to launch his Robot Revolution. With the foiling of Cobra's power stones plot, Overkill has apparently given up that quest.\n\nDuring the second season of the cartoon, Overkill is finally able to create himself a body, and is removed from his healing tank.\n\nOverkill appeared as a boss in 1992's \"\" for the Nintendo Entertainment System.\n\n"}
{"id": "70600", "url": "https://en.wikipedia.org/wiki?curid=70600", "title": "Princes in the Tower", "text": "Princes in the Tower\n\n\"The Princes in the Tower\" is an expression frequently used to refer to Edward V, King of England and Richard of Shrewsbury, Duke of York. The two brothers were the only sons of Edward IV of England and Elizabeth Woodville surviving at the time of their father's death in 1483. When they were 12 and 9 years old, respectively, they were lodged in the Tower of London by the man appointed to look after them, their uncle, the Lord Protector: Richard, Duke of Gloucester. This was supposedly in preparation for Edward's forthcoming coronation as king. However, Richard took the throne for himself and the boys disappeared.\n\nIt is unclear what happened to the boys after they disappeared in the Tower. It is generally assumed that they were murdered; a common hypothesis is that they were killed by Richard in an attempt to secure his hold on the throne. Their deaths may have occurred some time in 1483, but apart from their disappearance, the only evidence is circumstantial. As a result, several other hypotheses about their fates have been proposed, including the suggestion that they were murdered by Henry Stafford, 2nd Duke of Buckingham or Henry VII, among others. It has also been suggested that one or both princes may have escaped assassination. In 1487, Lambert Simnel initially claimed to be Richard, Duke of York, but later claimed to be Edward Plantagenet, 17th Earl of Warwick. From 1491 until his capture in 1497, Perkin Warbeck claimed to be Richard, Duke of York, having supposedly escaped to Flanders. Warbeck's claim was supported by some contemporaries (including the aunt of the disappeared princes, Margaret of York).\n\nIn 1674, workmen at the Tower dug up a wooden box containing two small human skeletons. The bones were found in a box under the staircase in the Tower of London. The bones were widely accepted at the time as those of the princes, but this has not been proven and is far from certain. King Charles II had the bones buried in Westminster Abbey, where they remain.\n\nOn 9 April 1483, Edward IV of England died unexpectedly after an illness lasting around three weeks. At the time, Edward's son, the new King Edward V, was at Ludlow Castle, and the dead king's brother, Richard, Duke of Gloucester, was at Middleham Castle in Yorkshire. The news reached Gloucester around 15 April, although he may have been forewarned of Edward's illness. It is reported that he then went to York Minster to publicly \"pledge his loyalty to his new king\". The Croyland Chronicle states that, before his death, Edward IV designated his brother Gloucester as Lord Protector. Edward's request may not have mattered, however, since \"as the precedent of Henry V showed, the council was not bound to follow the wishes of a dead king\".\n\nEdward V and Gloucester set out for London from the west and north respectively, meeting at Stony Stratford on 29 April. The following morning, Gloucester arrested Edward's retinue including the boys' uncle, Anthony Woodville, 2nd Earl Rivers, and their half-brother Sir Richard Grey. They were sent to Pontefract Castle in Yorkshire where, on 25 June, they were beheaded. Gloucester then took possession of the prince himself, prompting Elizabeth Woodville to take her other son, Richard, Duke of York, and her daughters into sanctuary at Westminster Abbey.\n\nEdward V and Gloucester arrived in London together. Plans continued for Edward's coronation, but the date was postponed from 4 May to 25 June. On 19 May 1483 Edward was lodged in the Tower of London, then the traditional residence of monarchs prior to coronation. On 16 June, he was joined by his younger brother Richard, Duke of York, who was previously in sanctuary. At this point the date of Edward's coronation was indefinitely postponed by their uncle, Gloucester. On Sunday 22 June, a sermon was preached at Saint Paul's Cross claiming Gloucester to be the only legitimate heir of the House of York. On 25 June, \"a group of lords, knights and gentlemen\" petitioned Richard to take the throne. Both princes were subsequently declared illegitimate by Parliament; this was confirmed in 1484 by an Act of Parliament known as \"Titulus Regius\". The act stated that Edward IV and Elizabeth Woodville's marriage was invalid because of Edward's pre-contract of marriage with Lady Eleanor Butler. Gloucester was crowned King Richard III of England on 3 July. The declaration of the boys' illegitimacy has been described by Rosemary Horrox as an ex post facto justification for Richard's accession.\n\nDominic Mancini, an Italian friar who visited England in the 1480s and who was in London in the spring and summer of 1483, recorded that after Richard III seized the throne, Edward and his younger brother Richard were taken into the \"inner apartments of the Tower\" and then were seen less and less until they disappeared altogether. Mancini records that during this period Edward was regularly visited by a doctor, who reported that Edward, \"like a victim prepared for sacrifice, sought remission of his sins by daily confession and penance, because he believed that death was facing him.\" The Latin reference to \"Argentinus medicus\", was originally translated as \"a Strasbourg doctor\"; however, D.E. Rhodes suggests it may actually refer to \"Doctor Argentine\", whom Rhodes identifies as John Argentine, an English physician who would later serve as provost of King's College, Cambridge, and as doctor to Arthur, Prince of Wales, eldest son of King Henry VII of England (Henry Tudor).\n\nThere are reports of the two princes being seen playing in the Tower grounds shortly after Richard joined his brother, but there are no recorded sightings of either of them after the summer of 1483. An attempt to rescue them in late July failed. Their fate remains an enduring mystery.\n\nMany historians believe the princes were murdered, some suggesting that the act may have happened towards the end of summer 1483. Maurice Keen argues that the rebellion against Richard in 1483 initially \"aimed to rescue Edward V and his brother from the Tower before it was too late\", but that, when the Duke of Buckingham became involved, it shifted to support of Henry Tudor because \"Buckingham almost certainly knew that the princes in the Tower were dead.\" Alison Weir proposes 3 September 1483 as a potential date; however, Weir's work has been criticised for \"arriving at a conclusion that depends more on her own imagination than on the uncertain evidence she has so misleadingly presented.\"\n\nSir Clements Markham suggests the princes may have been alive as late as July 1484, pointing to the regulations issued by Richard III's household which stated: \"the children should be together at one breakfast\". James Gairdner, however, argues that it is unclear to whom the phrase \"the children\" alludes, and that it may not have been a reference to the princes. It may refer to Edward, Earl of Warwick (son of the Duke of Clarence) and Edward IV's two youngest daughters (Catherine and Bridget), all of whom were living under Richard's care at Sheriff Hutton.\n\nOther than their disappearance, there is no direct evidence that the princes were murdered, and \"no reliable, well-informed, independent or impartial sources\" for the associated events. Nevertheless, following their disappearance, rumours quickly spread that they had been murdered. Only one contemporary narrative account of the boys' time in the tower exists: that of Dominic Mancini. Mancini's account was not discovered until 1934, in the Municipal Library in Lille. Later accounts written after the accession of Henry Tudor are often claimed to be biased or influenced by Tudor propaganda.\n\nFour unidentified bodies have been found which are considered possibly connected with the events of this period: two at the Tower of London and two in Saint George's Chapel, Windsor Castle. Those found in the Tower were buried in Westminster Abbey, but the Abbey authorities have refused to allow either set of remains to be subjected to DNA analysis to positively identify them as the remains of the princes.\n\nSeveral sources suggest there were rumours of the princes' deaths in the time following their disappearance. Rumours of murder also spread to France. In January 1484 , Lord Chancellor of France, urged the Estates General to \"take warning\" from the fate of the princes, as their own king, Charles VIII, was only 13. The early reports, including that of Rochefort, Philippe de Commines (French politician), Caspar Weinreich (contemporary German chronicler) and Jan Allertz (Recorder of Rotterdam), all state that Richard killed the princes before he seized the throne (thus before June 1483). De Commines' \"Memoirs\" (c.1500), however, identifies the Duke of Buckingham as the person \"who put them to death\".\n\nOnly Mancini's account is truly contemporary, having been written in London before November 1483. The Croyland Chronicle and de Commines' account were written three and seventeen years later, respectively (and thus after Richard III's death and the accession of Henry VII). Markham, writing long before Mancini's account was discovered, argued that some accounts, including the Croyland Chronicle, might have been authored or heavily influenced by John Morton, Archbishop of Canterbury, in order to incriminate Richard III.\n\nRobert Fabyan's \"Chronicles of London\", compiled around 30 years after the princes' disappearance, names Richard as murderer.\n\nThomas More (a Tudor loyalist who had grown up in the household of John Morton, an avowed foe of Richard III), wrote The \"History of King Richard III\", c.1513. This identified Sir James Tyrrell as the murderer, acting on Richard's orders. Tyrrell was the loyal servant of Richard III who is said to have confessed to the murder of the princes before his execution for treason in 1502. In his history, More said that the princes were smothered to death in their beds by two agents of Tyrrell (Miles Forrest and John Dighton) and were then buried \"at the stayre foote, metely depe in the grounde vnder a great heape of stones\", but were later disinterred and buried in a secret place.\n\nPolydore Vergil, in his \"Anglica Historia\" (c.1513), also specifies that Tyrrell was the murderer, stating that he \"rode sorrowfully to London\" and committed the deed with reluctance, upon Richard III's orders, and that Richard himself spread the rumours of the princes' death in the belief that it would discourage rebellion.\n\nHolinshed's Chronicles, written in the second half of the 16th century, claims that the princes were murdered by Richard III. The chronicles were one of the main sources used by William Shakespeare for his play \"Richard III\", which also portrays Richard as the murderer, in the sense that he commissions Tyrrell to have the boys killed. A. J. Pollard believes that the chronicle's account reflected the contemporary \"standard and accepted account\", but that by the time it was written \"propaganda had been transformed into historical fact\".\n\nIt should also be noted that Thomas More wrote his account with the intention of writing about a moral point rather than a closely mirrored history. While More's account does rely on some firsthand sources, the account is generally taken from other sources. Additionally, More's account is one of the bases for William Shakespeare's \"Richard III\", which similarly indicts Richard for murdering the young princes.\n\nIn 1675, some workmen remodelling the Tower of London dug up a wooden box containing two small human skeletons. The bones were found buried 10 ft under the staircase leading to the chapel of the White Tower. They were not the first children's skeletons found within the tower; the bones of two children had previously been found \"in an old chamber that had been walled up\", which Pollard suggests could equally well have been those of the princes. The reason the bones were attributed to the princes was because the location partially matched that given by More. However, More also stated that they were later moved to a \"better place\", which does not match with the bones discovered. One anonymous report was that they were found with \"pieces of rag and velvet about them\"; the velvet could indicate that the bodies were those of aristocrats. Four years after their discovery, the bones were placed in an urn and, on the orders of King Charles II, interred in Westminster Abbey, in the wall of the Henry VII Lady Chapel. A monument designed by Christopher Wren marks the resting-place of the putative princes.\n\nThe bones were removed and examined in 1933, by the archivist of Westminster Abbey, Lawrence Tanner; a leading anatomist, Professor William Wright; and the president of the Dental Association, George Northcroft. By measuring certain bones and teeth, they concluded the bones belonged to two children around the correct ages for the princes. The bones were found to have been interred carelessly along with chicken and other animal bones. There were also three very rusty nails. One skeleton was larger than the other, but many of the bones were missing, including part of the smaller jawbone and all of the teeth from the larger one. Many of the bones had been broken by the original workmen. The examination has been criticised, on the grounds that it was conducted under the presumption that the bones were those of the princes and concentrated only on whether the bones showed evidence of suffocation. Thus no attempt was even made to determine whether the bones were male or female.\n\nNo further scientific examination has since been conducted on the bones, which remain in Westminster Abbey, and DNA analysis (if DNA could be obtained) has not been attempted. A petition was started on the British Government's \"e-petition\" website requesting that the bones be DNA tested but closed months before its expected close date. If it had received 100,000 signatories a parliamentary debate would have been triggered. Pollard points out that even if modern DNA and carbon-dating proved the bones belonged to the princes, it would not prove who or what killed them.\n\nIn 1789, workmen carrying out repairs in St. George's Chapel, Windsor, rediscovered and accidentally broke into the vault of Edward IV and Queen Elizabeth Woodville, discovering in the process what appeared to be a small adjoining vault. This vault was found to contain the coffins of two unidentified children. However, no inspection or examination was carried out and the tomb was resealed. The tomb was inscribed with the names of two of Edward IV's children: George, 1st Duke of Bedford who had died at the age of 2, and Mary of York who had died at the age of 14; both had predeceased the King.\nHowever, two lead coffins clearly labelled as George Plantagenet and Mary Plantagenet were subsequently discovered elsewhere in the Chapel (during the excavation for the royal tomb house for King George III under the Wolsey tomb-house in 1810–13), and were moved into the adjoining vault of Edward IV's, but at the time no effort was made to identify the two lead coffins already in Edward IV's vault.\n\nIn the late 1990s, work was being carried out near and around Edward IV's tomb in St George's Chapel; the floor area was excavated to replace an old boiler and also to add a new repository for the remains of future Deans and Canons of Windsor. A request was forwarded to the Dean and Canons of Windsor to consider a possible examination of the two vaults either by fibre-optic camera or, if possible, a reexamination of the two unidentified lead coffins in the tomb also housing the lead coffins of two of Edward IV's children that were discovered during the building of the Royal Tomb for King George III (1810–13) and placed in the adjoining vault at that time. Royal consent would be necessary to open any royal tomb, so it was felt best to leave the medieval mystery unsolved for at least the next few generations. The 2012 Leicester archaeological dig has prompted renewed interest in re-excavating the skeletons of the \"two princes\", but Queen Elizabeth II has not granted the approval required for any such testing of an interred royal.\n\nThe absence of hard evidence of what happened to the Princes has led to a number of theories being put forward. The most common theory is that they were murdered close to the time that they disappeared, and among historians and authors who accept the murder theory, the most common explanation is that they were murdered by Richard.\n\nMany historians conclude that Richard III is the likeliest candidate for the disappearance of the princes for a number of reasons. Although the princes had been eliminated from the succession, Richard's hold on the monarchy was very insecure due to the way in which he had attained the crown, leading to a backlash against him by the Yorkist establishment. An attempt had already been made to rescue them and restore Edward to the throne, clear evidence that the existence of the princes would remain a threat as long as they were alive. The boys could have been used by Richard's enemies as figureheads for rebellion. Rumours of their death were in circulation by late 1483, but Richard never attempted to prove that they were alive by having them seen in public, which strongly suggests that they were dead by then. However, he did not remain silent on the matter. Raphael Holinshed, in his \"Chronicles of England, Scotland and Ireland\", written in 1577, reports that Richard, \"what with purging and declaring his innocence concerning the murder of his nephews towards the world, and what with cost to obtain the love and favour of the communal tie (which outwardlie glosed, and openly dissembled with him) ... gave prodigally so many and so great rewards, that now both he lacked, and scarce with honesty how to borrow.\" Richard also failed to open any investigation into the matter, which would have been in his interest if he was not responsible for the deaths of his nephews.\n\nRichard was away from court on a progression through the Yorkist heartlands at the time the princes disappeared; if they died at this time, he would have been unable to murder them in person. They were under guard in the Tower of London, which was controlled by his men, and access to them was strictly limited by his instructions. He could therefore have dispatched one of his retainers to murder the princes on his behalf, but it is unlikely they could have been murdered without his knowledge. This is the version put forward by More and Polydore Vergil, who both name James Tyrrell as the murderer. Tyrrell was an English knight who fought for the House of York on many occasions. Tyrrell was arrested by Henry VII's forces in 1502 for supporting another Yorkist claimant to the throne. Shortly before his execution, Tyrrell is said by Thomas More to have admitted, under torture, to having murdered the princes at the behest of Richard III. The only record of this is the writing of Thomas More, who wrote that, during his examination, Tyrrell made his confession as to the murders, saying that Richard III ordered their deaths. He also implicated two other men; despite further questioning, however, he was unable to say where the bodies were, claiming that Brackenbury had moved them. William Shakespeare portrays him as the culprit, sought out by Richard after Buckingham demurs. This version of events is accepted by Alison Weir and Hicks notes that his successful career and rapid promotion after 1483 'is consistent with his alleged murder of the princes'. However, the only record of Tyrrell's confession is through More, and \"no actual confession has ever been found\". Pollard casts doubts on the accuracy of More's accounts, suggesting it was \"an elaboration of one of several circulating accounts\"; however, he does not discount the possibility of it being \"just his own invention\", pointing to the \"clear similarities to the stories of the Babes in the Wood\". Clements Markham suggests that More's account was actually written by Archbishop Morton and that Tyrrell was induced to do the deed by Henry VII between June 16 and July 16, 1486, the dates of two general pardons that he received from the king.\n\nRichard's guilt was widely accepted by contemporaries. George Cely, Dominic Mancini, John Rous, Fabyan’s Chronicle, the Crowland Chronicler and the London Chronicle all noted the disappearance of the Princes, and all bar Mancini (who noted that he had no knowledge of what had happened) repeated rumours naming Richard as the murderer. Guillaume de Rochefort, Chancellor of France, named Richard as the murderer to the Estates General at Tours in January 1484. It also appears to have been the belief of Elizabeth Woodville, who would go on to support Henry Tudor in his campaign against Richard III. One possible motive for Elizabeth Woodville subsequently making her peace with Richard and bringing her daughters out of sanctuary could be that Richard had to swear a solemn oath, before witnesses, to protect and provide for her surviving children, which made it much less likely they could be quietly murdered as it was believed their brothers had been.\n\nIn line with this contemporary opinion many current historians, including David Starkey, Michael Hicks, Helen Castor and A. J. Pollard regard Richard himself as the most likely culprit. There was no formal accusation against Richard III on the matter; the Bill of Attainder brought by Henry VII made no definitive mention of the Princes in the Tower, but it did accuse Richard of \"the unnatural, mischievous and great perjuries, treasons, homicides and murders, in shedding of infant's blood, with many other wrongs, odious offences and abominations against God and man\". The \"shedding of infant's blood\" may be an accusation of the Princes' murder. Hicks speculated that it was a reference to speeches made in Parliament condemning the murder of the princes, which suggested that Richard's guilt had become common knowledge, or at least common wisdom.\n\nThe plausibility of Henry Stafford, 2nd Duke of Buckingham, Richard's right-hand man, as a suspect depends on the princes having already been dead by the time Stafford was executed in November 1483. It has been suggested that Buckingham had several potential motives. As a descendant of Edward III, through John of Gaunt, 1st Duke of Lancaster and Thomas of Woodstock, 1st Duke of Gloucester on his father's side, as well as through John of Gaunt, 1st Duke of Lancaster through John Beaufort, son of John of Gaunt on his mother's side, Buckingham may have hoped to accede to the throne himself in due course; alternatively, he may have been acting on behalf of a third party.\n\nSome, notably Paul Murray Kendall, regard Buckingham as the likeliest suspect: his execution, after he had rebelled against Richard in October 1483, might signify that he and the king had fallen out; Weir takes this as a sign that Richard had murdered the princes without Buckingham's knowledge and Buckingham had been shocked by it. A contemporary Portuguese document suggests Buckingham as the guilty party, stating \"...and after the passing away of king Edward in the year of 83, another one of his brothers, the Duke of Gloucester, had in his power the Prince of Wales and the Duke of York, the young sons of the said king his brother, and turned them to the Duke of Buckingham, under whose custody the said Princes were starved to death.\" A document dated some decades after the disappearance was found within the archives of the College of Arms in London in 1980; this stated that the murder \"be the vise of the Duke of Buckingham\". This led Michael Bennett to suggest that possibly some of Richard's prominent supporters, Buckingham and Tyrell, murdered the princes on their own initiative without waiting for Richard's orders. Bennett noted in support of this theory: 'After the King's departure Buckingham was in effective command in the capital, and it is known that when the two men met a month later there was an unholy row between them.'\n\nBuckingham is the only person to be named as responsible in a contemporary chronicle other than Richard himself. However, for two reasons he is unlikely to have acted alone. First of all, if he were guilty of acting without Richard's orders it is extremely surprising that Richard did not lay the blame for the princes' murder on Buckingham after Buckingham was disgraced and executed, especially as Richard could potentially have cleared his own name by doing so. Secondly, it is likely he would have required Richard's help to gain access to the princes, under close guard in the Tower of London, although Kendall argued as Constable of England, he might have been exempt from this ruling. As a result, although it is extremely possible that he was implicated in the decision to murder them, the hypothesis that he acted without Richard's knowledge is not widely accepted by historians. While Jeremy Potter suggested that Richard would have kept silent had Buckingham been guilty because nobody would have believed Richard was not party to the crime, he further notes that 'Historians are agreed that Buckingham would never have dared to act without Richard's complicity, or at least, connivance'. However, Potter also hypothesised that perhaps Buckingham was fantasising about seizing the crown himself at this point and saw the murder of the princes as a first step to achieving this goal. This theory formed the basis of Sharon Penman's historical novel, \"The Sunne in Splendour\".\n\nHenry VII (Henry Tudor), following his seizure of the crown, executed some of the rival claimants to the throne. One who was either executed or died in captivity was John of Gloucester, illegitimate son of Richard III. Henry was out of the country between the princes' disappearance and August 1485, thus his only opportunity to murder them would have been after his accession in 1485. Pollard suggests Henry (or those acting on his orders) is \"the only plausible alternative to Richard III.\"\n\nThe year after becoming king, Henry married the princes' eldest sister, Elizabeth of York, to reinforce his claim to the throne. Not wanting the legitimacy of his wife or her claim as heiress of Edward IV called into question, prior to the marriage he had repealed the \"Titulus Regius\" which had previously declared the princes (and Elizabeth) illegitimate. Markham suggests the princes were executed under Henry's orders between 16 June and 16 July 1486, claiming that it was only after this date that orders went out to circulate the story that Richard had killed the princes, and that the princes' mother, Elizabeth Woodville, knew that this story was false, and so Henry had to have her silenced. Markham suggests this was the motivation behind Henry's decision, in February 1487, to confiscate all of Elizabeth's lands and possessions, and have her confined to Bermondsey Nunnery, \"where she died six years afterwards\". However, Arlene Okerlund suggests that her retirement to the nunnery was her own decision, whilst Michael Bennett and Timothy Elston suggests the move was precautionary, precipitated by Lambert Simnel's claim to be her son Richard. Pollard calls Markham's theory \"highly speculative\", and states that Henry's silence over the princes was more likely \"political calculation than personal guilt\". Henry was also never accused of the murder by any contemporary, not even by his enemies, which he likely would have been had contemporaries thought there was any possibility of his guilt. Jeremy Potter, at the time he wrote Chairman of the Richard III Society, noted, 'With Henry, as with Richard, there is no real evidence and one must suspect that if he had killed the princes himself he would quickly have produced the corpses and some ingeniously appropriate story implicating Richard.' Further, Raphael Holinshed reported in 1577 that Richard \"purged and declared is innocence\" regarding \"the murther of his nephews towards the world\", indicating that the boys did indeed meet their end during Richard's days. It is also unlikely that the princes would have been kept alive in secret by Richard for 2 years after their last sighting while rumours of his responsibility for their murder circulated.\n\nSome writers have also accused John Howard, 1st Duke of Norfolk; Margaret Beaufort, Henry VII's mother; and Jane Shore (Edward IV's mistress). Pollard writes of these theories: \"None deserve serious consideration. The problem with all these accusations is that they beg the question of access to the Tower without Richard's knowledge and overlook the fact that Richard was responsible for the safekeeping of his nephews\". The Beaufort theory has more recently been supported by Philippa Gregory and is explored in her \"Cousins' War\" novels, as well as her BBC documentary series \"The Real White Queen and her Rivals\", but is not supported by any evidence other than a speculative one of possible motive.\n\nHistorian David Baldwin suggests that Henry VII's reticence on the subject may have been due to the fact that at least one of the princes was still alive; he considers that the likelier candidate for survival would be Richard and that Edward may have died of a malady. Baldwin argues that it is \"impossible\" that no one knew what happened to the Princes after they entered the Tower; he believes Richard III and Henry VII, leading courtiers and their mother would all have known the boys' whereabouts and welfare. Baldwin argues that had this been the case, Henry VII would have had the choice of keeping quiet about the survival of Richard, or having him executed, and concluded, \"He [Henry] would have been happy to let people \"think\" the boys had been murdered, but not to speculate when or by whose hand.\"\n\nDuring the reign of Henry VII, two individuals claimed to be Richard, Duke of York, who had somehow escaped death. Lambert Simnel initially claimed to be Richard, before changing his story and claiming to be Edward Plantagenet, 17th Earl of Warwick. Perkin Warbeck later claimed to be Richard, appearing in Ireland and calling himself king Richard IV. Margaret of York, Duchess of Burgundy, formally recognised Warbeck as Richard. Margaret, Richard III's sister, an unrelenting opponent of Henry VII, had previously recognised Simnel as Warwick. Warbeck was also accepted as Richard by James IV of Scotland. After a failed attempt to invade England he was captured. He retracted his claims, was imprisoned and later executed. Many modern historians believe he was an imposter, whose supporters accepted his claim for political reasons.\n\nThe fact that two persons claimed to be Richard led the 18th century writer Horace Walpole to argue that Richard had in fact escaped death, and that Warbeck genuinely was Richard, a view also supported by Malcolm Laing. Walpole, however, later retracted his views and stated that he now believed the princes to have been murdered by Richard III to secure his hold on the crown. In more recent times the theory that Warbeck was Richard has been endorsed by Annette Carson, a freelance writer with a \"lifelong interest\" in Richard III. She suggested that Richard smuggled the princes abroad to the custody of their aunt, the Duchess of Burgundy, and they were raised there under false identities. Baldwin's theory was that by having removed them from sight to prevent them being a focus for opposition, he was then unable to bring them back to court to scotch rumours of their murder without once again having them become a threat.\n\nThe political reality of the disappearance of the princes, whatever happened to them, is that they were believed to have been murdered and Richard was blamed for their murders. Even if he had not been directly responsible for their deaths, the fact that he had deposed them and kept them under tight guard made him responsible for their welfare in the eyes of contemporaries, and the belief that they had been murdered made him guilty via negligence if not directly. As Baldwin noted in support of his conclusion that Richard would not have murdered the princes, \"It seems incredible Richard ever supposed killing his nephews would help secure his position or make him more acceptable to his subjects.\" An initial uprising in September 1483, aimed at deposing Richard and restoring Edward V to the throne, was not stopped by rumours of Edward's murder. Instead, the rebels rallied around Henry Tudor as a potential alternative candidate, who was, as Horrox notes, \"an inconceivable choice if Edward V and his brother were thought to be still available.\" Anthony Cheetham, who considered Richard likely did have the princes murdered, commented that it was \"a colossal blunder. Nothing else could have prompted the deflated Woodvilles to hitch themselves to Henry Tudor's bandwagon.\" The fact that the majority of the rebels were wealthy and powerful southern noblemen, loyal to Edward IV, suggests a degree of revulsion against Richard's usurpation of the throne: their willingness to fight on under an implausible alternative candidate suggests that they regarded anyone as preferable to Richard as King due to his usurpation and the murder of his nephews. Bennett suggested that perhaps those who had initially supported Richard in his seizure of power may have felt complicit in the crime, which he thought \"might explain the bitterness of the subsequent recriminations against him.\" Hicks speculated that these men may have been \"appalled by the character of the regime...shocked by Richard's crimes.\" Their defection severely weakened Richard, who had to impose his supporters among the northern lords as officeholders in the southern counties to maintain order, in itself a very unpopular act that further damaged his reputation. In Pollard's words, \"the belief that he had murdered his nephews seriously handicapped Richard's efforts to secure himself on the throne he had usurped.\"\n\nThe mystery of the Princes in the Tower has had a great impact on literature and television, spawning several award-winning novels such as Josephine Tey's \"The Daughter of Time\" and four novels in Philippa Gregory's Cousins' War series, as well as aspects of George R. R. Martin's \"Game of Thrones\", wherein boy heirs are apparently murdered. The continued publishing of these novels and stories have contributed to the continued fascination with this mystery.\n\n\n\n\nThe brothers also appear in the Victorian-era themed manga series, \"Black Butler\", better known as \"Kuroshitsuji\", by Yana Toboso.\n"}
{"id": "2494932", "url": "https://en.wikipedia.org/wiki?curid=2494932", "title": "Robert Hichens (sailor)", "text": "Robert Hichens (sailor)\n\nRobert Hichens (16 September 1882 – 23 September 1940) was a British sailor who was part of the deck crew on board the when she sank on her maiden voyage on 15 April 1912. He was one of six quartermasters on board the vessel and was at the ship's wheel when the \"Titanic\" struck the iceberg. In 1906, he married Florence Mortimore in Devon, England; when he registered for duty aboard the \"Titanic\", his listed address was in Southampton, where he lived with his wife and two children.\n\nHichens gained notoriety after the disaster because of his conduct in Lifeboat No. 6, of which he was in command. Passengers accused him of refusing to go back to rescue people from the water after the ship sank, that he called the people in the water \"stiffs,\" and that he constantly criticised those at the oars while he was manning the rudder. Hichens was later to testify at the US Inquiry that he had never used the words \"stiffs\" and that he had other words to describe bodies. He would also testify to have been given direct orders by second mate Charles Lightoller and Captain Edward Smith to row to where a light could be seen (a steamer they thought) on the port bow, drop off the passengers and return. Later it was alleged that he complained that the lifeboat was going to drift for days before any rescue came. At least two boat 6 passengers publicly accused Hichens of being drunk: Major Arthur Godfrey Peuchen and Mrs Lucian Philip Smith.\n\nWhen the came to rescue \"Titanic\"s survivors he said that the ship was not there to rescue them, but to pick up the bodies of the dead. By this time the other people in the lifeboat had lost patience with Hichens. Although Hichens protested, Denver millionaire Margaret \"Molly\" Brown told the others to start rowing to keep warm. After a last attempt by Hichens to keep control of the lifeboat, Brown threatened to throw him overboard. These events would later end up being depicted in the Broadway musical and film, \"The Unsinkable Molly Brown\". During the US inquiry into the disaster, Hichens denied the accounts by the passengers and crew in lifeboat 6. He had been initially concerned about the suction from the \"Titanic\" and later by the fact that being a mile away from the wreck, with no compass and in complete darkness, they had no way of returning to the stricken vessel.\n\nHichens served with the Army Service Corps during World War One; by 1919 he was third officer on a small ship named \"Magpie\". The Hitchens moved to Devon sometime in the 1920s where Robert purchased a motor boat from a man named Harry Henley and operated a boat charter. In 1931, his wife and children left him and moved to Southampton. In 1933, Hichens was jailed for attempting to murder Henley and was released in 1937. \n\nOn 23 September 1940, at age 58, Hichens died of heart failure aboard the ship \"English Trader\", while it was moored off the coast of Aberdeen, Scotland. His body was buried in Section 10, Lair 244 of Trinity Cemetery, in Aberdeen.\n\nHichens' conduct was featured in the 1997 blockbuster, \"Titanic\", in which he was played by Paul Brightwell. He was depicted as a tall thin man with a cockney accent, when in fact he was 5' 6\", had a stocky build and spoke with a pronounced Cornish accent. He was also depicted saying \"shut that hole in your face\" to Molly Brown, but in fact those words were spoken by a steward in lifeboat 8.\n\nHichens' conduct was also depicted in the 1996 miniseries \"Titanic\", in which he was played by Martin Evans. Hichens is shown telling the survivors in his lifeboat to \"pipe down\" when they get excited about spotting a flare from a ship on the horizon. He strongly protests when Molly Brown starts encouraging the other women to row towards the light, and she threatens to throw Hichens overboard. This depiction is more accurate than in the 1997 blockbuster.\n\nHichens was portrayed by an uncredited actor in the 1958 film \"A Night to Remember\", which also portrays his conflict with Molly Brown in a more accurate manner.\n\nHichens' negative attitude was further depicted in Diane Hoh's 1998 romance novel \"\", which recounts his conduct as well as that of Molly Brown, from the viewpoint of Elizabeth Farr, a fictional lifeboat passenger. Molly Brown urged lifeboat passengers to start rowing to keep warm, and Hichens protested, declaring that he was commanding the lifeboat, and he made a move to stop her. \"I will throw you overboard if you interfere,\" she told him in this account.\n\nIn September 2010, Hichens' name was brought back into the limelight by Louise Patten, granddaughter of the most senior officer to have survived the \"Titanic\" disaster, second officer Charles Lightoller. In press interviews leading up to the publication of her latest novel, \"Good as Gold\" (into which she has worked the story of the catastrophe), Patten reports that a \"straightforward\" steering error by Hichens, brought about by his misunderstanding of a tiller order, caused the \"Titanic\" to hit an iceberg in 1912. Patten's allegation that Hichens caused the disaster by turning the ship's wheel the wrong way is not supported by testimony at both the British and US enquiries, which established that the second watch officer, Sixth Officer James Moody, was stationed behind Hichens, supervising his actions, and he had confirmed to First Officer William Murdoch that the order had been carried out correctly.\n\nThe claim was also disputed by Hichens' great-granddaughter on Channel 4 News. Sally Nilsson explained that Hichens was a well-trained Quartermaster with years of experience steering large vessels. He had been responsible on his watch for steering the \"Titanic\" for four days before the collision and would not have made such a glaring error. As to the steering orders, in 1912 they were as follows: There was only one way of giving steering orders. The order was always given with reference to the tiller. To go to port the Officer ordered starboard. The Quartermaster turned the wheel to port, tiller went to starboard and the ship turned to port. This was a hangover from the old days when ships were steered with tillers, steering oars etc. The change in steering orders did not occur until the 1930s. Sally Nilsson's biography on the life of Robert Hichens was published in 2011.\n\nHichens also appears in the play \"Iceberg – Right Ahead!\" by Chris Burgess which debuted on 22 March 2012 at Upstairs at the Gatehouse. In this production, he was played by Liam Mulvey.\n\n"}
{"id": "37704661", "url": "https://en.wikipedia.org/wiki?curid=37704661", "title": "Role ethics", "text": "Role ethics\n\nRole ethics is an ethical theory based on family roles. Unlike virtue ethics, role ethics is not individualistic. Morality is derived from a person's relationship with their community. The ethics of Confucianism is an example of role ethics.\n\nConfucian role ethics centers around filial piety or \"xiao\", a respect for family members. The concept is elaborated in the Confucian text \"Classic of Filial Piety\": \"In serving his parents, a filial son reveres them in daily life; he makes them happy while he nourishes them; he takes anxious care of them in sickness; he shows great sorrow over their death; and he sacrifices to them with solemnity.\" Filial duty requires the desire to be filial, and not just the act of filial piety. In Confucian societies, filial piety determines the \"moral worth\" of an individual in a community and acts as a form of social capital.\n\nAccording to Roger T. Ames and Henry Rosemont, \"Confucian normativity is defined by living one's family roles to maximum effect.\" In Confucian role ethics, morality is based on a person's fulfillment of a role, such as that of a parent or a child. These roles are established as relationships, and are not individualistic. Confucian roles are not rational, and originate through the \"xin\", or human emotions.\n\nThe concept of \"li\" or ritual propriety is crucial to Confucian roles. Propriety reinforces family relationships, and binds together the community. The performance of li expresses a person's moral commitment as a human being.\n\nIn Japan, modern Confucian scholars like Uno Tetsuto and Ichimura Sanjiro have attempted to mix Confucian role ethics with concepts such as democracy and human rights.\n"}
{"id": "6990324", "url": "https://en.wikipedia.org/wiki?curid=6990324", "title": "Sandek", "text": "Sandek\n\nA sandek ( 'companion of child') is a person honored at a Jewish \"brit milah\" (circumcision) ceremony, traditionally either by holding the baby boy on the knees or thighs while the mohel performs the brit milah, or by handing the baby to the mohel. The origin of the term has been attributed to a derivation from the Greek \"sunteknos\" (\"syn-\" meaning 'plus' and \"tekno\" meaning 'child'), which means 'companion of child'.\n\nRabbi Moses Isserles (the Rema) recorded the practice of the sandek holding the baby on his thighs (Yoreh Dayah 265:11). The Biur Hagra (YD 265:44) cites the Midrash Shochar Tov, that explains that this is based on Psalm 35:10, which states, \"All my bones shall say: ' who is like you?'\" Midrash outlines how every body part is used in the service of God and says that the sandek's thighs participate in the service of God by placing the baby on them during the brit.\n\nThe Rema records a custom that a father should not honour the same individual twice with being the sandek for his children. The reason is that the sandek is compared to a \"kohen\" (priest) offering the \"ketoret\" (incense offering) in the \"Beit HaMikdash\" (Jewish Temple). The procedure regarding the ketoret is that a kohen does not perform this \"mitzva\" (commandment) more than once in his lifetime. God rewards with wealth the kohen who offers the ketoret. Thus, the opportunity is afforded to as many kohanim as possible to become wealthy (Babylonian Talmud Tractate Yoma 26a). Similarly, the opportunity is afforded to as many people as possible to serve as a sandek and receive God's blessing to become wealthy.\n\nThe Vilna Gaon (Gra) (YD 265:45) expresses some skepticism regarding this custom. First, based on its reasoning, the custom should have been that one should not serve more than once as a sandek for any child, not just two different children of one family. Second, the Gra writes that no one has become wealthy because he served as a sandek. Nevertheless, the Aruch Hashulchan (Y.D. 265:34) concludes, that the custom recorded by the Rema should be observed. The Aruch Hashulchan notes, though, that the custom in many locales is that the \"rav\" (rabbi) of the city serves as the sandek for all the baby boys. The Aruch Hashulchan justifies this practice by comparing the local rav to the \"kohen gadol\" (high priest), who had the right to offer a \"korban\" (sacrifice) or ketoret any time he desired (see Yoma 14a). Indeed, it is related that the Chazon Ish served as the sandek for innumerable baby boys. Rabbi Yissocher Frand relates that Rav Yaakov Yitzchak Ruderman (the \"rosh yeshiva\" (dean) of Yeshivat Ner Yisrael) also served as the sandek for countless baby boys.\n\nThe honor was given traditionally to one Jewish male: some older family member (grandfather, great-grandfather), a rabbi, or another important male who was observant and righteous. The sandek also wore the \"tallit\" (prayer shawl) and held the baby on a pillow while the mohel completed the circumcision. In modern times, among some more liberal Jews, the sandek may be female or even non-Jewish. At most ceremonies there is only a single sandek, but two are permissible, although more than two is uncommon. \n\nDuring the brit, a chair is sometimes placed next to the sandek's seat. The chair is reserved for the prophet Elijah and remains unoccupied during the ceremony; this practice is derived from the tradition that Elijah protects children from danger. According to some sources the sandek is the \"representative\" of Elijah.\n"}
{"id": "206583", "url": "https://en.wikipedia.org/wiki?curid=206583", "title": "Suffering", "text": "Suffering\n\nSuffering, or pain in a broad sense, may be an experience of unpleasantness and aversion associated with the perception of harm or threat of harm in an individual. Suffering is the basic element that makes up the negative valence of affective phenomena. The opposite of suffering is pleasure or happiness.\n\nSuffering is often categorized as physical or mental. It may come in all degrees of intensity, from mild to intolerable. Factors of duration and frequency of occurrence usually compound that of intensity. Attitudes toward suffering may vary widely, in the sufferer or other people, according to how much it is regarded as avoidable or unavoidable, useful or useless, deserved or undeserved.\n\nSuffering occurs in the lives of sentient beings in numerous manners, often dramatically. As a result, many fields of human activity are concerned with some aspects of suffering. These aspects may include the nature of suffering, its processes, its origin and causes, its meaning and significance, its related personal, social, and cultural behaviors, its remedies, management, and uses.\n\nThe word \"suffering\" is sometimes used in the narrow sense of physical pain, but more often it refers to mental pain, or more often yet it refers to pain in the broad sense, i.e. to any unpleasant feeling, emotion or sensation. The word \"pain\" usually refers to physical pain, but it is also a common synonym of \"suffering\". The words \"pain\" and \"suffering\" are often used both together in different ways. For instance, they may be used as interchangeable synonyms. Or they may be used in 'contradistinction' to one another, as in \"pain is physical, suffering is mental\", or \"pain is inevitable, suffering is optional\". Or they may be used to define each other, as in \"pain is physical suffering\", or \"suffering is severe physical or mental pain\".\n\nQualifiers, such as \"physical\", \"mental\", \"emotional\", and \"psychological\", are often used to refer to certain types of pain or suffering. In particular, \"mental pain (or suffering)\" may be used in relationship with \"physical pain (or suffering)\" for distinguishing between two wide categories of pain or suffering. A first caveat concerning such a distinction is that it uses \"physical pain\" in a sense that normally includes not only the 'typical sensory experience of physical pain' but also other unpleasant bodily experiences including air hunger, hunger, vestibular suffering, nausea, sleep deprivation, and itching. A second caveat is that the terms \"physical\" or \"mental\" should not be taken too literally: physical pain or suffering, as a matter of fact, happens through conscious minds and involves emotional aspects, while mental pain or suffering happens through physical brains and, being an emotion, involves important physiological aspects.\n\nThe word \"unpleasantness\", which some people use as a synonym of \"suffering\" or \"pain\" in the broad sense, may be used to refer to the basic affective dimension of pain (its suffering aspect), usually in contrast with the sensory dimension, as for instance in this sentence: \"Pain-unpleasantness is often, though not always, closely linked to both the intensity and unique qualities of the painful sensation.\" Other current words that have a definition with some similarity to \"suffering\" include \"distress, unhappiness, misery, affliction, woe, ill, discomfort, displeasure, disagreeableness\".\n\nHedonism, as an ethical theory, claims that good and bad consist ultimately in pleasure and pain. Many hedonists, in accordance with Epicurus and contrarily to popular perception of his doctrine, advocate that we should first seek to avoid suffering and that the greatest pleasure lies in a robust state of profound tranquility (ataraxia) that is free from the worrisome pursuit or the unwelcome consequences of ephemeral pleasures.\n\nFor Stoicism, the greatest good lies in reason and virtue, but the soul best reaches it through a kind of indifference (apatheia) to pleasure and pain: as a consequence, this doctrine has become identified with stern self-control in regard to suffering.\nJeremy Bentham developed hedonistic utilitarianism, a popular doctrine in ethics, politics, and economics. Bentham argued that the right act or policy was that which would cause \"the greatest happiness of the greatest number\". He suggested a procedure called hedonic or felicific calculus, for determining how much pleasure and pain would result from any action. John Stuart Mill improved and promoted the doctrine of hedonistic utilitarianism. Karl Popper, in \"The Open Society and Its Enemies\", proposed a negative utilitarianism, which prioritizes the reduction of suffering over the enhancement of happiness when speaking of utility: \"I believe that there is, from the ethical point of view, no symmetry between suffering and happiness, or between pain and pleasure. (...) human suffering makes a direct moral appeal for help, while there is no similar call to increase the happiness of a man who is doing well anyway.\" David Pearce, for his part, advocates a utilitarianism that aims straightforwardly at the abolition of suffering through the use of biotechnology (see more details below in section Biology, neurology, psychology). Another aspect worthy of mention here is that many utilitarians since Bentham hold that the moral status of a being comes from its ability to feel pleasure and pain: therefore, moral agents should consider not only the interests of human beings but also those of (other) animals. Richard Ryder came to the same conclusion in his concepts of 'speciesism' and 'painism'. Peter Singer's writings, especially the book \"Animal Liberation\", represent the leading edge of this kind of utilitarianism for animals as well as for people.\n\nAnother doctrine related to the relief of suffering is humanitarianism (see also humanitarian principles, humanitarian aid, and humane society). \"Where humanitarian efforts seek a positive addition to the happiness of sentient beings, it is to make the unhappy happy rather than the happy happier. (...) [Humanitarianism] is an ingredient in many social attitudes; in the modern world it has so penetrated into diverse movements (...) that it can hardly be said to exist in itself.\"\n\nPessimists hold this world to be mainly bad, or even the worst possible, plagued with, among other things, unbearable and unstoppable suffering. Some identify suffering as the nature of the world, and conclude that it would be better if life did not exist at all. Arthur Schopenhauer recommends us to take refuge in things like art, philosophy, loss of the will to live, and tolerance toward 'fellow-sufferers'.\n\nFriedrich Nietzsche, first influenced by Schopenhauer, developed afterward quite another attitude, arguing that the suffering of life is productive, exalting the will to power, despising weak compassion or pity, and recommending us to embrace willfully the 'eternal return' of the greatest sufferings. \n\nPhilosophy of pain is a philosophical specialty that focuses on physical pain and is, through that, relevant to suffering in general.\n\nSuffering plays an important role in a number of religions, regarding matters such as the following: consolation or relief; moral conduct (do no harm, help the afflicted, show compassion); spiritual advancement through life hardships or through self-imposed trials (mortification of the flesh, penance, ascetism); ultimate destiny (salvation, damnation, hell). Theodicy deals with the problem of evil, which is the difficulty of reconciling the existence of an omnipotent and benevolent god with the existence of evil: a quintessential form of evil, for many people, is extreme suffering, especially in innocent children, or in creatures destined to an eternity of torments (see problem of hell).\n\nThe 'Four Noble Truths' of Buddhism are about dukkha, a term often translated as suffering. They state the nature of suffering, its cause, its cessation, and the way leading to its cessation, the Noble Eightfold Path. Buddhism considers liberation from \"dukkha\" and the practice of compassion (karuna) as basic for leading a holy life and attaining nirvana.\n\nHinduism holds that suffering follows naturally from personal negative behaviors in one’s current life or in a past life (see karma in Hinduism). One must accept suffering as a just consequence and as an opportunity for spiritual progress. Thus the soul or true self, which is eternally free of any suffering, may come to manifest itself in the person, who then achieves liberation (moksha). Abstinence from causing pain or harm to other beings (ahimsa) is a central tenet of Hinduism. Suffering is thought to be an inclusive effect of human experience. Beyond this, Hindus are looking to achieve enlightenment and end human suffering by answering questions about life. This will lead to a unity in God as well as find the meaning of their suffering, ultimately achieving bliss.\n\nChristianity also believes that human suffering plays an important role in religion. Suffering is only to be thought of as a positive experience in the case of achieving a higher meaning of life, such as Jesus suffering for the lives of other people as was the case during the atonement. Suffering is the time to find God and value faith while doing so. This allows Christians to face reality of human experience with suffering and find an understanding in the divine.\n\nHinduism and Christianity embrace similar aspects in suffering. Both religions realize the need for God as well as the moral significance for God that suffering provides. This allows enlightenment to be reached and suffering to be seen in the conditions that faith entails rather than an issue. These human experiences with suffering in both Hinduism and Christianity help educators to emphasize the need for dialogue and religious education in schools.\n\nIn Islam, the faithful must endure suffering with hope and faith, not resist or ask why, accept it as Allah's will and submit to it as a test of faith (Allah never asks more than can be endured). One must also work to alleviate suffering of others, as well as one's own. Suffering is also seen as a blessing in Islam for the mankind . Through the gift of suffering the Veil of Forgetfulness is torn apart and the sufferer remembers God and connects with him. When people suffer God makes them think of him. Several Islamic Prophet Muhammad's traditions state that, suffering expunges the sins of mankind and cleanses their soul for the immense reward in afterlife.\n\nThe Bible's Book of Job reflects on the nature and meaning of suffering. It is supplemented in the Hebrew bible by the passages found in the Book of Isaiah and the Book of Jeremiah which elaborate the emotional and physical suffering of a conquered nation with its vanquished inhabitants forced into the suffering of exile and captivity in a foreign land.\n\nIn the New Testament, suffering is portrayed both in the life of Jesus portrayed in the Synoptics, which narrate the suffering of the crucifixion, and in the post-Easter narratives. The suffering associated with punishment is further portrayed in the Apocalypse of John where suffering at the scene of the Last Judgment is depicted as the just recompense for sin and wrongdoing. Pope John Paul II wrote \"On the Christian Meaning of Human Suffering\". This meaning revolves around the notion of redemptive suffering.\n\nAccording to the Bahá'í Faith, all suffering is a brief and temporary manifestation of physical life, whose source is the material aspects of physical existence, and often attachment to them, whereas only joy exists in the spiritual worlds. In the words of `Abdu'l-Bahá, \"All these examples are to show you that the trials which beset our every step, all our sorrow, pain, shame and grief, are born in the world of matter; whereas the spiritual Kingdom never causes sadness. A man living with his thoughts in this Kingdom knows perpetual joy. The ills all flesh is heir to do not pass him by, but they only touch the surface of his life, the depths are calm and serene.\" (Paris Talks, p. 110).\n\nArtistic and literary works often engage with suffering, sometimes at great cost to their creators or performers. The Literature, Arts, and Medicine Database offers a list of such works under the categories art, film, literature, and theater. Be it in the tragic, comic or other genres, art and literature offer means to alleviate (and perhaps also exacerbate) suffering, as argued for instance in Harold Schweizer's \"Suffering and the remedy of art\".\n\nThis Brueghel painting is among those that inspired W. H. Auden's poem Musée des Beaux Arts :\n\n\"About suffering they were never wrong,\" <br>\n\"The Old Masters; how well, they understood\" <br>\n\"Its human position; how it takes place\" <br>\n\"While someone else is eating or opening a window or just walking dully along;\"<br>\n\"(...)\"<br>\n\"In Breughel's Icarus, for instance: how everything turns away\" <br>\n\"Quite leisurely from the disaster; (...)\"\n\n\"Social suffering\", according to Arthur Kleinman and others, describes \"collective and individual human suffering associated with life conditions shaped by powerful social forces\". Such suffering is an increasing concern in medical anthropology, ethnography, mass media analysis, and Holocaust studies, says Iain Wilkinson, who is developing a sociology of suffering.\n\nThe \"Encyclopedia of World Problems and Human Potential\" is a work by the Union of International Associations. Its main databases are about world problems (56,564 profiles), global strategies and solutions (32,547 profiles), human values (3,257 profiles), and human development (4,817 profiles). It states that \"the most fundamental entry common to the core parts is that of pain (or suffering)\" and \"common to the core parts is the learning dimension of new understanding or insight in response to suffering\".\n\nRalph G.H. Siu, an American author, urged in 1988 the \"creation of a new and vigorous academic discipline, called panetics, to be devoted to the study of the infliction of suffering\", The International Society for Panetics was founded in 1991 to study and develop ways to reduce the infliction of human suffering by individuals acting through professions, corporations, governments, and other social groups.\n\nIn economics, the following notions relate not only to the matters suggested by their positive appellations, but to the matter of suffering as well: Well-being or Quality of life, Welfare economics, Happiness economics, Gross National Happiness, Genuine Progress Indicator.\n\nIn law, \"Pain and suffering\" is a legal term that refers to the mental distress or physical pain endured by a plaintiff as a result of injury for which the plaintiff seeks redress. Assessments of pain and suffering are required to be made for attributing legal awards. In the Western world these are typical made by juries in a discretionary fashion and are regarded as subjective, variable, and difficult to predict, for instance in the US, UK, Australia, and New Zealand. See also, in US law, Negligent infliction of emotional distress and Intentional infliction of emotional distress.\n\nSuffering and pleasure are respectively the negative and positive affects, or hedonic tones, or valences that psychologists often identify as basic in our emotional lives. The evolutionary role of physical and mental suffering, through natural selection, is primordial: it warns of threats, motivates coping (fight or flight, escapism), and reinforces negatively certain behaviors (see punishment, aversives). Despite its initial disrupting nature, suffering contributes to the organization of meaning in an individual's world and psyche. In turn, meaning determines how individuals or societies experience and deal with suffering.\n\nMany brain structures and physiological processes are involved in suffering. Various hypotheses try to account for the experience of suffering. One of these, the \"pain overlap theory\" takes note, thanks to neuroimaging studies, that the cingulate cortex fires up when the brain feels suffering from experimentally induced social distress or physical pain as well. The theory proposes therefore that physical pain and social pain (i.e. two radically differing kinds of suffering) share a common phenomenological and neurological basis.\n\nAccording to David Pearce’s online manifesto The Hedonistic Imperative, suffering is the avoidable result of Darwinian genetic design. Pearce promotes replacing the pain/pleasure axis with a robot-like response to noxious stimuli or with gradients of bliss, through genetic engineering and other technical scientific advances.\n\nHedonistic psychology, affective science, and affective neuroscience are some of the emerging scientific fields that could in the coming years focus their attention on the phenomenon of suffering.\n\nDisease and injury may contribute to suffering in humans and animals. For example, suffering may be a feature of mental or physical illness such as borderline personality disorder and occasionally in advanced cancer. Health care addresses this suffering in many ways, in subfields such as medicine, clinical psychology, psychotherapy, alternative medicine, hygiene, public health, and through various health care providers.\n\nHealth care approaches to suffering, however, remain problematic. Physician and author Eric Cassell, widely cited on the subject of attending to the suffering person as a primary goal of medicine, has defined suffering as \"the state of severe distress associated with events that threaten the intactness of the person\". Cassell writes: \"The obligation of physicians to relieve human suffering stretches back to antiquity. Despite this fact, little attention is explicitly given to the problem of suffering in medical education, research or practice.\" Mirroring the traditional body and mind dichotomy that underlies its teaching and practice, medicine strongly distinguishes pain from suffering, and most attention goes to the treatment of pain. Nevertheless, physical pain itself still lacks adequate attention from the medical community, according to numerous reports. Besides, some medical fields like palliative care, pain management (or pain medicine), oncology, or psychiatry, do somewhat address suffering 'as such'. In palliative care, for instance, pioneer Cicely Saunders created the concept of 'total pain' ('total suffering' say now the textbooks), which encompasses the whole set of physical and mental distress, discomfort, symptoms, problems, or needs that a patient may experience hurtfully.\n\nSince suffering is such a universal motivating experience, people, when asked, can relate their activities to its relief and prevention. Farmers, for instance, may claim that they prevent famine, artists may say that they take our minds off our worries, and teachers may hold that they hand down tools for coping with life hazards. In certain aspects of collective life, however, suffering is more readily an explicit concern by itself. Such aspects may include public health, human rights, humanitarian aid, disaster relief, philanthropy, economic aid, social services, insurance, and animal welfare. To these can be added the aspects of security and safety, which relate to precautionary measures taken by individuals or families, to interventions by the military, the police, the firefighters, and to notions or fields like social security, environmental security, and human security.\n\nPhilosopher Leonard Katz wrote: \"But Nature, as we now know, regards ultimately only fitness and not our happiness (...), and does not scruple to use hate, fear, punishment and even war alongside affection in ordering social groups and selecting among them, just as she uses pain as well as pleasure to get us to feed, water and protect our bodies and also in forging our social bonds.\"\n\nPeople make use of suffering for specific social or personal purposes in many areas of human life, as can be seen in the following instances:\n\n\n"}
{"id": "23080235", "url": "https://en.wikipedia.org/wiki?curid=23080235", "title": "Thick concept", "text": "Thick concept\n\nIn philosophy, a thick concept (sometimes: \"thick normative concept\", or \"thick evaluative concept\") is a kind of concept that both has a significant degree of descriptive content and is evaluatively loaded. Paradigmatic examples are various virtues and vices such as \"courage\", \"cruelty\", \"truthfulness\" and \"kindness\". Courage for example, may be given a rough characterization in descriptive terms as '…opposing danger to promote a valued end'. At the same time, characterizing someone as courageous typically involves expressing a pro-attitude, or a (prima facie) good-making quality – i.e. an evaluative statement.\n\nThick concepts thus seem to occupy a 'middle position' between (thin) descriptive concepts and (thin) evaluative concepts. Descriptive concepts such as \"water\", \"gold\", \"length\" and \"mass\" are commonly believed to pick out features of the world rather than provide reasons for action, whereas evaluative concepts such as \"right\" and \"good\" are commonly believed to provide reasons for action rather than picking out genuine features of the world. This 'double feature' of thick concepts has made them the point of debate between moral realists and moral expressivists. Moral realists have argued that the world-guided content and the action-guiding content cannot be usefully separated, indicating that competent use of thick concepts constitutes ethical knowledge. Expressivists, favoring an account of moral values as attitudes projected onto the world, wants to uphold a distinction between the (morally neutral) descriptive feature of a thick concept and the evaluative attitudes that typically goes with them.\n\nAs mentioned above, thick concepts seem to combine the descriptive features of natural concepts such as water with an evaluative content similar to the thin evaluative concepts such as good and right. How are we to understand this ‘combination’? Many theorists treat it as a conjunctive: a thick concept should be analyzed as a conjunction of a descriptive part and an evaluative part, which, at least in principle may be separated. A basic feature of this analysis is thus that the descriptive content of a thick concept may be given in absence of the evaluative content. Returning to the example of courage, ‘…is courageous’ could on this account be analyzed as something along the lines of ‘…opposing danger to promote a valued end’ and ‘this is (prima facie) good-making’. The evaluative part, on this view, may thus be characterized as a ‘prescriptive flag’ attached to the concept. It is, on this view, in principle possible to construct a completely descriptive concept – i.e. without evaluative force – that picked out the same features of the world.\n\nThis account of thick concepts has been criticized by other theorists, notably of moral realist persuasion. In their view, the only way to understand a thick concept is to understand the descriptive and evaluative aspects as a whole. The idea is that for a thick concept, the evaluative aspect is profoundly involved in the practice of using it; one cannot understand a thick concept without understanding also its evaluative point. Therefore, descriptive terms cannot completely fill in the ‘along the lines’ of a description such as ‘…opposing danger to promote a valued end’. These descriptions may allow the novice to see the salient features. However a hooking on to the evaluative perspective allows the person to fully understand the 'thick' concept.\n\nBlackburn, S. (1998) \"Ruling Passions\", Oxford: Clarendon Press.\n\nBlomberg, O. (2007) Disentangling The Thick Concept Argument, \"Sats: Nordic Journal of Philosophy\", 8(2), 63-78. (link)\n\nDancy, J. (1995) In Defence of Thick Concepts, in French, Uehling, and Wettstein eds., \"Midwest Studies in Philosophy 20\", Notre Dame, Ind.: University of Notre Dame Press.\n\nDancy, J. (2004) \"Ethics without Principles\", Oxford: Clarendon Press.\n\nElgin, C. (2005) Williams on Truthfulness, \"The Philosophical Quarterly\" 55.\n\nGibbard, A. (1992) Thick Concepts and Warrant For Feelings, \"Proceedings of the Aristotelian Society\" 66 (Supplementary).\n\nHooker, B. and Little, M. (2000), \"Moral Particularism\", Oxford: Clarendon Press.\n\nLittle, M. (2000) Moral Generalities Revisited, in Hooker and Little 2000. \n\nMcDowell, J. (1978) Are Moral Requirements Hypothetical Imperatives? \"Proceedings of the Aristotelian Society Supplementary Volume\" 52, 13-29.\n\nMcDowell, J. (1979), Virtue and Reason, \"Monist\" 62(3), 331-350.\n\nMcDowell, J. (1981), Non-Cognitivism and Rule-Following, in \"Wittgenstein: To Follow a Rule\", eds. S. Holtzman and C. Leich, London & Boston: Routledge & Kegan Paul, 141-162.\n\nMcNaughton, D. and Rawling, P. (2000) Unprincipled Ethics, in Hooker and Little 2000, 256-275.\n\nWilliams, B. (1985) \"Ethics and the Limits of Philosophy\", Cambridge, Mass.: Harvard University Press.\n"}
{"id": "60075", "url": "https://en.wikipedia.org/wiki?curid=60075", "title": "Unknot", "text": "Unknot\n\nThe unknot arises in the mathematical theory of knots. Intuitively, the unknot is a closed loop of rope without a knot in it. A knot theorist would describe the unknot as an image of any embedding that can be deformed, i.e. ambient-isotoped, to the standard unknot, i.e. the embedding of the circle as a geometrically round circle. The unknot is also called the trivial knot. An unknot is the identity element with respect to the knot sum operation.\n\nDeciding if a particular knot is the unknot was a major driving force behind knot invariants, since it was thought this approach would possibly give an efficient algorithm to recognize the unknot from some presentation such as a knot diagram. Currently there are several well-known unknot recognition algorithms (not using invariants), but they are either known to be inefficient or have no efficient implementation. It is not known whether many of the current invariants, such as finite type invariants, are a complete invariant of the unknot, but knot Floer homology is known to detect the unknot. Even if they were, the problem of computing them efficiently remains.\n\nMany useful practical knots are actually the unknot, including all knots which can be tied in the bight. Other noteworthy unknots are those that consist of rigid line segments connected by universal joints at their endpoints (linkages), that yet cannot be reconfigured into a convex polygon, thus acquiring the name \"stuck\" unknots.\n\nThe Alexander-Conway polynomial and Jones polynomial of the unknot are trivial:\n\nNo other knot with 10 or fewer crossings has trivial Alexander polynomial, but the Kinoshita-Terasaka knot and Conway knot (both of which have 11 crossings) have the same Alexander and Conway polynomials as the unknot. It is an open problem whether any non-trivial knot has the same Jones polynomial as the unknot.\n\nThe knot group of the unknot is an infinite cyclic group, and the knot complement is homeomorphic to a solid torus.\n\n\n"}
{"id": "2170976", "url": "https://en.wikipedia.org/wiki?curid=2170976", "title": "Venusians", "text": "Venusians\n\nIn science fiction and ufology, a Venusian () or Venerian is a native inhabitant of the planet Venus. Many science fiction writers have imagined what extraterrestrial life on Venus might be like.\n\nThe word \"Venusian\" – sometimes spelled \"Venutian\" – is a simple combination of the name of the planet \"Venus\" and the suffix \"-ian\", formed by analogy to \"Martian\" and other similar demonyms.\n\nThe classically derived demonym would be \"Venerean\" or \"Venerian\" (cf. \"belonging to the goddess Venus\"), but these forms have been used by only a few authors (e.g. Robert A. Heinlein). Scientists sometimes use the adjective \"Cytherean\" for things related to Venus, from the goddess' epithet \"Cytherea\". The similarly derived \"Venereal\" is not used due to its association with sexually transmitted infections as \"venereal diseases\".\n\nVenusians appearing in works of fiction are usually fanciful, rather than plausible inhabitants of the planet. Before the mid 20th century, little was generally known about the planet except that it was solid and comparable in size to Earth; its cloud cover obscured remote observation of its environment. This allowed writers to speculate that Venusians might be similar to humans or other Earth species, much as they did for fictional Martians. As more was learned about Venus and the implausibility of humanoid or other life on it, Venusians became increasingly uncommon in science fiction.\n\n\n\n\n\nIn the 1950s a group of contactees told stories in which they claimed to be in contact with friendly, light-haired, light-skinned humans from the planet Venus, as well as other planets in Earth's solar system. The first contactee, and the most famous, was George Adamski of Palomar Mountain, California. He claimed that on November 20, 1952 he met a Venusian named Orthon in a California desert. Adamski said that Orthon communicated with him via telepathy about the dangers of nuclear war and that he left behind footprints with mysterious symbols on them. Adamski also displayed numerous photographs that he claimed showed the Venusian UFOs, and he said some of the photos had been given to him by Orthon. Copies of these photos were sold to visitors at Adamski's campground and restaurant at Palomar Mountain, but later studies by UFO investigators indicated that the photos were fakes; one scientist who analyzed the photos of a Venusian \"scout ship\" said the UFO's \"landing struts\" were General Electric light bulbs.\n\nAdamski wrote or co-wrote three books in the 1950s and early 1960s about his meetings with Orthon and travels in a Venusian UFO through Earth's solar system; the first two books, \"Flying Saucers Have Landed\" (1953), and \"Inside the Space Ships\" (1955), were both bestsellers. Following Adamski's story, others, such as Howard Menger, George Hunt Williamson, Truman Bethurum, George Van Tassel, and Daniel Fry, also wrote books and gave lectures in which they claimed to have met similar friendly, light-skinned humanoids from Venus and other planets in Earth's solar system, and to have taken trips with them in their spaceships. These humanoids were later called Nordic aliens.\n\nThroughout the 1950s and 1960s the contactee movement garnered some popular interest through books, lectures, and conventions, such as the annual Giant Rock UFO conventions in California. In May 1959 Adamski had a private audience with Queen Juliana of the Netherlands to discuss his claimed UFO experiences, which caused some controversy in the Netherlands.\n\nHowever, numerous investigations of the contactee movement revealed many flaws and inaccuracies in the contactees' claims that led most researchers to conclude that their stories were hoaxes. Among the pieces of evidence noted by critics was that Venus has an environment that is extremely hostile to human life, and that none of the other planets in Earth's solar system are capable of supporting humanoid life. Also, investigators such as USAF Captain Edward J. Ruppelt, the head of the Air Force's Project Blue Book, and ufologist James W. Moseley, conducted extensive investigations into the claims and backgrounds of Adamski, Williamson, and other contactees, and concluded that they were either con artists or simply not being truthful in their stories and claims.\n\n\n\n"}
{"id": "59167911", "url": "https://en.wikipedia.org/wiki?curid=59167911", "title": "Virginia Cornish", "text": "Virginia Cornish\n\nVirginia Wood Cornish is the Helena Rubinstein Professor of Chemistry at Columbia University.\n\nCornish received her B.A. in chemistry in 1991, working with professor Ronald Breslow. Her Ph.D. research, on site-specific protein labeling and mutagenesis, was carried out with Peter Schultz. Cornish was an NSF postdoctoral fellow at MIT with Prof. Robert Sauer.\n\nCornish and her lab group use the tools of systems biology, synthetic biology, and DNA encoding to produce desired chemical products from specific organismic hosts. In 2016, she was part of a notable group of genomic scientists calling for increased ethical study and self-regulation as the costs and effort of creating \"de novo\" genomes plummeted. As the \"read\" phase of the Human Genome Project was completed in 2004, this new effort was dubbed Genome Project-Write.\n\n"}
