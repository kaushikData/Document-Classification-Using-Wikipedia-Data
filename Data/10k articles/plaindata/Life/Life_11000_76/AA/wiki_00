{"id": "23231061", "url": "https://en.wikipedia.org/wiki?curid=23231061", "title": "40 Glocc", "text": "40 Glocc\n\nLawrence White (born December 16, 1974), best known by his stage name 40 Glocc, is a rapper from Colton, California.\n\nWhite was born in Galveston, Texas on December 16, 1974. At the age of ten, he moved with his mother to various locations in California, until finally settling in the Inland Empire city of Colton. He began rapping with friends and by 1997, his group, the \"Zoo Crew\", were recording and promoting their first album, \"Migrate, Adapt or Die\", produced by Tony G and Julio G.\n\n40 Glocc negotiated a deal with Empire Musicwerks, with distribution by BMG. It was through this deal, he recorded and released his first solo effort, \"The Jakal\", which featured such guests as Bad Azz , Ras Kass, Kurupt, Mac Minister, and several others; and such producers as Battlecat and Dr. Dre among several others. It was after this album that he decided to part ways with Empire Music.\n\nHis manager was also managing Mobb Deep, who were then signed to G-Unit Records. 40 Glocc joined their label, Infamous Records, under the G-Unit imprint.\n\n40 Glocc has worked with artists such as Dr. Dre, The Alchemist and Havoc. He has also been working with his group, the \"Zoo Babies\", while also promoting his label \"ZooLife Ent.\". He has also been on tour with such acts as G-Unit, Mobb Deep, Ray J, Snoop & Doggpound & many more. He's released a song entitled \"Where the Hammers At?\", the beat for which was later used as the beat for G-Unit's song Rider Pt. 2. He later released a remix to \"Hammers\" which features G-Unit and their respective verses of Rider Pt. 2. In late fall 2010, 40 Glocc signed affiliated rapper Ras Kass to his label Zoo Life. He released a digital street album that year called \"COPS\" with various features from Jayo Felony, Tip Toe, Tony Yayo and more. in 2011, 40 Glocc and Spider Loc released an album \"Graveyard Shift\". The following year, he released his first solo album.\n\nIn 1996, White was arrested following a shootout with police outside the Arbor Terrace apartment complex in Colton, and was charged with attempted murder of a police officer. In October 2010, a California judge ruled that White, who is a gang member with the Colton City Crips, would be included in an injunction prohibiting gang members from congregating near the Arbor Terrace apartments, which was believed to be the gang's headquarters. However, White was arrested on January 24, 2011 in San Bernardino after a loaded 9mm handgun was found in his car, and was found loitering a block away from the Arbor Terrace at the time of his arrest. The charges were ultimately dropped due to lack of evidence.\n\nOn October 3, 2011, White was arrested in Las Vegas and charged with 3 counts of domestic battery by strangulation and coercion with force. 40 allegedly hit his girlfriend several times around the head, face, legs, and rib area and choking her to the point she could not speak or breathe. Once he fell asleep, she was able to sneak away and dial 911. Police arrested 40 as he was trying to flee his home. The woman claims 40 had been assaulting her for over a year and she was afraid to go to the law, in fear she might be killed.\nWhite has additionally been involved in multiple altercations with fellow rap artists. In July 2012, he was involved in a fight with rapper Jayceon \"The Game\" Taylor and his friends outside of a mansion in Hollywood. White sued Taylor for $4.5 million in damages for assault and battery, as well as damage to his reputation. White was awarded $3,000 in 2015, and an additional $200,000 in 2017. In June 2008, a video was posted online of 40 Glocc holding two diamond Young Money necklaces, previously taken from Young Money recording artist Tyga. 40 Glocc held the necklaces for ransom after a fellow member of his entourage took them from Tyga. Both necklaces were reportedly bought back by Lil Wayne in \"Cash Money\".\n\nIn October 2009, a video was posted online showing 40 Glocc and members of his Zoo-Life Crip Gang showing up to Florida rapper Plies' video shoot to test his street credibility. The video shoot, which also featured Jamie Foxx, was being filmed in L.A. Jamie Foxx, who was standing next to Plies when he was about to be approached by 40 Glocc, confirms that Plies ran towards the bathrooms and locked himself inside. He claims Plies was afraid for his life and returned to the video shoot wearing no jewelry. \n\n\n\n\n"}
{"id": "20803306", "url": "https://en.wikipedia.org/wiki?curid=20803306", "title": "ADInstruments", "text": "ADInstruments\n\nADInstruments is an international company that produces data acquisition and analysis systems for the life sciences industry. It is headquartered in Dunedin, New Zealand and has more than 170 staff worldwide. Voted a finalist in Kenexa/JRA Top 10 Best Place to work in 2009, 2010., 2011 and 2012, and voted number one place to work in the life sciences industry in 2012 by The Scientist Magazine. ADInstruments partners with several producers of life sciences equipment, including Transonic Systems Inc., Radnoti Glass Technologies Inc., Panlab s.I. and Millar Instruments Ltd. ADInstruments is also an Applied Science industry partner with the University of Otago (New Zealand)\n\nIn 1985, the Physiology Department at New Zealand's University of Otago encouraged the development of a computer-based data acquisition system to replace their paper-based systems. In 1985, as part of his Computer Science Masters at the university, Michael Macknight built the MacLab, one of the first analog to digital converters that connected to a Macintosh computer.\n\nMichael and his father, Tony Macknight, created a company initially known as Analog Digital Instruments. Very soon, they formed a partnership with Boris Schlensky who bought in skills around manufacturing, marketing and distribution.\n\nTwo software packages were initially developed: Chart (chart-recorder software, now LabChart) and Scope (digital oscilloscope software), which provided software control of the recording unit as well as a range of display and analysis features:\n\n\nThe company provides grants, awards and sponsorship to students, researchers and educators in the life sciences industry:\n\n\n\nIn 2008 the company was nominated for two Life Science Industry Awards:\n\n\n"}
{"id": "419077", "url": "https://en.wikipedia.org/wiki?curid=419077", "title": "Abu Hanifa", "text": "Abu Hanifa\n\nAbū Ḥanīfa al-Nuʿmān b. Thābit b. Zūṭā b. Marzubān (; c. 699 – 767 CE), known as Abū Ḥanīfa for short, or reverently as Imam Abū Ḥanīfa by Sunni Muslims, was an 8th-century Sunni Muslim theologian and jurist of Persian origin, who became the eponymous founder of the Hanafi school of orthodox Sunni jurisprudence, which has remained the most widely practiced law school in the Sunni tradition. He is often alluded to by the reverential epithets al-Imām al-aʿẓam (\"The Great Imam\") and Sirāj al-aʾimma (\"The Lamp of the Imams\") in Sunni Islam.\n\nBorn to a Muslim family in Kufa, Abu Hanifa is known to have travelled to the Hejaz region of Arabia in his youth, where he studied under the most renowned teachers in Mecca and Medina at the time. As his career as a theologian and jurist progressed, Abu Hanifa became known for favoring the use of reason in his legal rulings (\"faqīh dhū raʾy\") and even in his theology. Abu Hanifa's theological school is what would later develop into the Maturidi school of orthodox Sunni theology.\nHe is also considered a renowned Islamic scholar and personality by Zaydi Shia Muslims.\n\nAbū Ḥanīfah was born in the city of Kufa in Iraq, during the reign of the Umayyad caliph Abd al-Malik ibn Marwan. His father, Thabit bin Zuta, a trader from Kabul (today in Afghanistan), was 40 years old at the time of Abū Ḥanīfah's birth.\n\nHis ancestry is generally accepted as being of Persian origin as suggested by the etymology of the names of his grandfather (Zuta) and great-grandfather (Mah). The historian Al-Khatib al-Baghdadi records a statement from Imām Abū Ḥanīfah's grandson, Ismail bin Hammad, who gave Abū Ḥanīfah's lineage as Thabit bin Numan bin Marzban and claiming to be of Persian origin. The discrepancy in the names, as given by Ismail of Abū Ḥanīfah's grandfather and great-grandfather, are thought to be due to Zuta's adoption of the Arabic name (Numan) upon his acceptance of Islam and that Mah and Marzban were titles or official designations in Persia, with the latter, meaning a margrave, referring to the noble ancestry of Abū Ḥanīfah's family as the Sasanian Marzbans (equivalent of margraves). The widely accepted opinion, however, is that most probably he was of Persian ancestry .\nIn 763, al-Mansur, the Abbasid monarch offered Abu Hanifa the post of Chief Judge of the State, but he declined the offer, choosing to remain independent. His student Abu Yusuf was later appointed \"Qadi Al-Qudat\" (Chief Judge of the State) by the Caliph Harun al-Rashid.\n\nIn his reply to al-Mansur, Abū Ḥanīfah said that he was not fit for the post. Al-Mansur, who had his own ideas and reasons for offering the post, lost his temper and accused Abū Ḥanīfah of lying.\n\n\"If I am lying,\" Abū Ḥanīfah said, \"then my statement is doubly correct. How can you appoint a liar to the exalted post of a Chief Qadi (Judge)?\"\n\nIncensed by this reply, the ruler had Abū Ḥanīfah arrested, locked in prison and tortured. He was never fed nor cared for. Even there, the jurist continued to teach those who were permitted to come to him.\n\nOn the 15 Rajab 150 (August 15, 767), Abū Ḥanīfah died in prison. The cause of his death is not clear, as some say that Abū Ḥanīfah issued a legal opinion for bearing arms against Al-Mansur, and the latter had him poisoned. The fellow prisoner and Jewish Karaite founder, Anan Ben David, is said to have received life-saving counsel from the subject. It was said that so many people attended his funeral that the funeral service was repeated six times for more than 50,000 people who had amassed before he was actually buried. On the authority of the historian al-Khatib, it can be said that for full twenty days people went on performing funeral prayer for him. Later, after many years, the Abū Ḥanīfah Mosque was built in the Adhamiyah neighbourhood of Baghdad.\n\nThe tomb of Abū Ḥanīfah and the tomb of Abdul Qadir Gilani were destroyed by Shah Ismail of Safavi empire in 1508. In 1533, Ottomans conquered Baghdad and rebuilt the tomb of Abū Ḥanīfah and other Sunni sites.\n\nThe sources from which Abu Hanifa derived Islamic law, in order of importance and preference, are: the Qur'an, the authentic narrations of the Muslim prophet Muhammad (known as hadith), consensus of the Muslim community (ijma), analogical reasoning (qiyas), juristic discretion (istihsan) and the customs of the local population enacting said law (urf). The development of analogical reason and the scope and boundaries by which it may be used is recognized by the majority of Muslim jurists, but its establishment as a legal tool is the result of the Hanafi school. While it was likely used by some of his teachers, Abu Hanifa is regarded by modern scholarship as the first to formally adopt and institute analogical reason as a part of Islamic law.\n\nAs the fourth Caliph, Ali had transferred the Islamic capital to Kufa, and many of the first generation of Muslims had settled there, the Hanafi school of law based many of its rulings on the prophetic tradition as transmitted by those first generation Muslims residing in Iraq. Thus, the Hanafi school came to be known as the Kufan or Iraqi school in earlier times. Ali and Abdullah, son of Masud formed much of the base of the school, as well as other personalities from the direct relatives (or Ahli-ll-Bayṫ) of Moḥammad from whom Abu Hanifa had studied such as Muhammad al-Baqir (thus apparently creating a link between Sunnis and Shias). Many jurists and historians had reportedly lived in Kufa, including one of Abu Hanifa's main teachers, Hammad ibn Abi Sulayman.\n\nAbū Ḥanīfah is regarded by some as one of the Tabi‘un, the generation after the Sahaba, who were the companions of the Islamic prophet, Muhammad. This is based on reports that he met at least four Sahaba including Anas ibn Malik, with some even reporting that he transmitted Hadith from him and other companions of Muhammad. Others take the view that Abū Ḥanīfah only saw around half a dozen companions, possibly at a young age, and did not directly narrate hadith from them.\n\nAbū Ḥanīfah was born 67 years after the death of Muhammad, but during the time of the first generation of Muslims, some of whom lived on until Abū Ḥanīfah's youth. Anas bin Malik, Muhammad's personal attendant, died in 93 AH and another companion, Abul Tufail Amir bin Wathilah, died in 100 AH, when Abū Ḥanīfah was 20 years old. The author of al-Khairat al-Hisan collected information from books of biographies and cited the names of Muslims of the first generation from whom it is reported that the Abu Hanifa had transmitted hadith. He counted them as sixteen, including Anas ibn Malik, Jabir ibn Abd-Allah and Sahl ibn Sa'd.\n\nAbu Hanifa ranks as one of the greatest jurists of Islamic civilization and one of the major legal philosophers of the entire human community. He attained a very high status in the various fields of sacred knowledge and significantly influenced the development of Muslim theology. During his lifetime he was acknowledged by the people as a jurist of the highest calibre.\n\nOutside of his scholarly achievements Abu Hanifa is popularly known amongst Sunni Muslims as a man of the highest personal qualities: a performer of good works, remarkable for his self-denial, humble spirit, devotion and pious awe of God.\n\nHis tomb, surmounted by a dome erected by admirers in 1066 is still a shrine for pilgrims. It was given a restoration in 1535 by Suleiman the Magnificent upon the Ottoman conquest of Baghdad.\n\nThe honorific title \"al-Imam al-A'zam\" (\"the greatest leader\") was granted to him both in communities where his legal theory is followed and elsewhere. According to John Esposito, 41% of all Muslims follow the Hanafi school.\n\nAbu Hanifa also had critics. The Zahiri scholar Ibn Hazm quotes Sufyan ibn `Uyaynah: \"[T]he aﬀairs of men were in harmony until they were changed by Abù Hanìfa in Kùfa, al-Batti in Basra and Màlik in Medina\". Early Muslim jurist Hammad ibn Salamah once related a story about a highway robber who posed as an old man to hide his identity; he then remarked that were the robber still alive he would be a follower of Abu Hanifa.\n\n\nThe attribution of \"Al-Fiqh Al-Akbar\" has been disputed by A.J. Wensick, as well as Zubair Ali Zai.\n\nOther scholars have upheld that Abu Hanifa was the author such as Muhammad Zahid Al-Kawthari, al-Bazdawi, and Abd al-Aziz al-Bukhari.\n\nScholars such as Mufti Abdur-Rahman have pointed out that the book being brought into question by Wensick is actually another work by Abu Hanifa called: \"Al-Fiqh Al-Absat\".\n\n\n"}
{"id": "1226250", "url": "https://en.wikipedia.org/wiki?curid=1226250", "title": "Acute liver failure", "text": "Acute liver failure\n\nAcute liver failure is the appearance of severe complications rapidly after the first signs of liver disease (such as jaundice), and indicates that the liver has sustained severe damage (loss of function of 80–90% of liver cells). The complications are hepatic encephalopathy and impaired protein synthesis (as measured by the levels of serum albumin and the prothrombin time in the blood). The 1993 classification defines \"hyperacute\" as within 1 week, \"acute\" as 8–28 days, and \"subacute\" as 4–12 weeks. It reflects the fact that the pace of disease evolution strongly influences prognosis. Underlying cause is the other significant determinant of outcome.\n\nThe main features of acute liver failure are rapid-onset jaundice, weakness, and eventually, changes in mental status that can begin as mild confusion but progress to coma.\n\nIn ALF, hepatic encephalopathy leads to cerebral edema, coma, brain herniation, and eventually death. Detection of encephalopathy is central to the diagnosis of ALF. It may vary from subtle deficit in higher brain function (e.g. mood, concentration in grade I) to deep coma (grade IV). Patients presenting as acute and hyperacute liver failure are at greater risk of developing cerebral oedema and grade IV encephalopathy. The pathogenesis remains unclear, but is likely to be a consequence of several phenomena. There is a buildup of toxic substances like ammonia, mercaptan, benzodiazepines, serotonin and tryptophan in the brain. This affects neurotransmitter level and neuroreceptor activation. Autoregulation of cerebral blood flow is impaired, and is associated with anaerobic glycolysis and oxidative stress. Neuronal cell astrocytes are susceptible to these changes, and they swell up, resulting in increased intracranial pressure. Inflammatory mediators also play important role.\n\nUnfortunately, signs of elevated intracranial pressure, such as papilledema and loss of pupillary reflexes, are not reliable, and occur late in the disease process. CT imaging of the brain is also unhelpful in detecting early cerebral oedema, but is often performed to rule out intra-cerebral bleeding. Invasive intracranial pressure monitoring via subdural route is often recommended; however, the risk of complications must be weighed against the possible benefit (1% fatal haemorrhage). The aim is to maintain intracranial pressures below 25 mm Hg, and cerebral perfusion pressures above 50 mm Hg.\n\nCoagulopathy is another cardinal feature of ALF. The liver has the central role in the synthesis of almost all coagulation factors and some inhibitors of coagulation and fibrinolysis. Hepatocellular necrosis leads to impaired synthesis of many coagulation factors and their inhibitors. The former produces a prolongation in prothrombin time which is widely used to monitor the severity of hepatic injury. There is significant platelet dysfunction (with both quantitative and qualitative platelet defects). Progressive thrombocytopenia with the loss of larger and more active platelets is almost universal. Thrombocytopenia with or without DIC increases risk of intracerebral bleeding.\n\nKidney failure is common, present in more than 50% of ALF patients, either due to original insult such as paracetamol resulting in acute tubular necrosis or from hyperdynamic circulation leading to hepatorenal syndrome or functional kidney failure. Because of impaired production of urea, blood urea does not represent the degree of kidney impairment.\n\nAbout 60% of all ALF patients fulfil the criteria for systemic inflammatory syndrome irrespective of presence or absence of infection. This often contributes towards multi organ failure. Impaired host defence mechanism, due to impaired opsonization, chemotaxis and intracellular killing, substantially increases risk of sepsis. Bacterial sepsis mostly due to gram positive organisms and fungal sepsis are observed in up to 80% and 30% patients, respectively.\n\nHyponatraemia is an almost universal finding due to water retention and a shift in intracellular sodium transport from inhibition of Na/K ATPase. Hypoglycaemia (due to depleted hepatic glycogen store and hyperinsulinaemia), hypokalaemia, hypophosphataemia and Metabolic alkalosis are often present, independent of renal function. Lactic acidosis occurs predominantly in paracetomol (also known as acetaminophen) overdose.\n\nHyperdynamic circulation, with peripheral vasodilatation from low systemic vascular resistance, leads to hypotension. There is a compensatory increase in cardiac output. Adrenal insufficiency has been documented in 60% of ALF cases, and is likely to contribute in haemodynamic compromise. There is also abnormal oxygen transport and utilization. Although delivery of oxygen to the tissues is adequate, there is a decrease in tissue oxygen uptake, resulting in tissue hypoxia and lactic acidosis.\n\nPulmonary complications occur in up to 50% of patients. Severe lung injury and hypoxemia result in high mortality. Most cases of severe lung injury are due to ARDS, with or without sepsis. Pulmonary haemorrhage, pleural effusions, atelectasis, and intrapulmonary shunts also contribute to respiratory difficulty.\n\nIn late pregnancy liver function decreases significantly, which can be easily monitored by blood tests. Early clinical manifestations of ALF in late pregnancy include hypodynamia, decrease in appetite, dark amber urine, deep jaundice, nausea, vomiting, and abdominal distention. Among patients whose deaths were attributed to ALF in late pregnancy, the majority had experienced vaginal deliveries.\n\nCommon causes for acute liver failure are paracetamol (acetaminophen) overdose, idiosyncratic reaction to medication (e.g. tetracycline, troglitazone), excessive alcohol consumption (severe alcoholic hepatitis), viral hepatitis (hepatitis A or B — it is extremely uncommon in hepatitis C), acute fatty liver of pregnancy, and idiopathic (without an obvious cause). Reye syndrome is acute liver failure in a child with a viral infection (e.g. chickenpox); it appears that aspirin use may play a significant role. Wilson's disease (hereditary copper accumulation) may infrequently present with acute liver failure. Acute liver failure also results from poisoning by the death cap mushroom (\"Amanita phalloides\") as well as other amatoxin-producing fungus species.\n\nIn the majority of acute liver failure (ALF) there is widespread hepatocellular necrosis beginning in the centrizonal distribution and progressing towards portal tracts. The degree of parenchymal inflammation is variable and is proportional to duration of disease.\n\nZone 1 (periportal) occurs in phosphorus poisoning or eclampsia.\nZone 2 (mid-zonal), although rare, is seen in yellow fever.\nZone 3 (centrilobular) occurs with ischemic injury, toxic effects, carbon tetrachloride exposure, or chloroform ingestion. In acute acetaminophen overdose, toxification occurs, mostly in Zone III which has the highest level of P450 micro-enzymes. That fact along with Zone III's decreased oxygen level helps to explain why it is preferentially one of the initial sites of damage.\n\nAll patients with clinical or laboratory evidence of moderate to severe acute hepatitis should have an immediate measurement of prothrombin time and careful evaluation of mental status. If the prothrombin time is prolonged by ≈ 4–6 seconds or more (INR ≥ 1.5),\nand there is any evidence of altered sensorium, the diagnosis of ALF should be strongly suspected, and hospital admission is mandatory. Initial laboratory examination must be extensive in order to evaluate both the etiology and severity.\n\n\nHistory taking should include a careful review of possible exposures to viral infection and drugs or other toxins. From history and clinical examination, the possibility of underlying chronic disease should be ruled out as it may require different management.\n\nA liver biopsy done via the transjugular route because of coagulopathy is not usually necessary, other than in occasional malignancies. As the evaluation continues, several important decisions have to be made; such as whether to admit the patient to an ICU, or whether to transfer the patient to a transplant facility. Consultation with the transplant center as early as possible is critical due to the possibility of rapid progression of ALF.\n\nAcute liver failure is defined as \"the rapid development of hepatocellular dysfunction, specifically coagulopathy and mental status changes (encephalopathy) in a patient without known prior liver disease\".\n\nThe diagnosis of acute liver failure is based on physical exam, laboratory findings, patient history, and past medical history to establish mental status changes, coagulopathy, rapidity of onset, and absence of known prior liver disease respectively.\n\nThe exact definition of \"rapid\" is somewhat questionable, and different sub-divisions exist which are based on the time from onset of first hepatic symptoms to onset of encephalopathy. One scheme defines \"acute hepatic failure\" as the development of encephalopathy within 26 weeks of the onset of any hepatic symptoms. This is sub-divided into \"fulminant hepatic failure\", which requires onset of encephalopathy within 8 weeks, and \"subfulminant\", which describes onset of encephalopathy after 8 weeks but before 26 weeks. Another scheme defines \"hyperacute\" as onset within 7 days, \"acute\" as onset between 7 and 28 days, and \"subacute\" as onset between 28 days and 24 weeks.\n\nBecause ALF often involves the rapid deterioration of mental status and the potential for multiorgan failure, patients should be managed in the intensive care unit. For patients not at a transplant center, the possibility of rapid progression of ALF makes early consultation with a transplant facility critical. Accordingly, plans for transfer to a transplant center should begin in patients with any abnormal mentation. Early institution of antidotes or specific therapy may prevent the need for liver transplantation and reduce the likelihood of poor outcome. Measures appropriate for specific causes of ALF are described in detail later in this chapter.\n\nPatients with grade I–II encephalopathy should be transferred to a liver transplant facility and listed for transplantation. Consider a brain computed tomography (CT) scan to rule out other causes of altered or impaired mental status. Stimulation and overhydration can cause elevations in intracranial pressure (ICP) and should be avoided. Unmanageable agitation may be treated with short-acting benzodiazepines in small doses. Lactulose can be considered at this stage. A preliminary report from the ALFSG on 117 patients suggests that use of lactulose in the first 7 days after diagnosis is associated with a small increase in survival time, but with no difference in severity of encephalopathy or in the overall outcome. For patients who progress to grade III–IV encephalopathy, intubation for airway protection is generally required. Many centers use propofol for sedation because it may reduce cerebral blood. The head of the bed should be elevated to 30 degrees, and electrolytes, blood gasses, glucose, and neurologic status monitored frequently.\n\nIncreased cardiac output and low systemic vascular resistance are characteristic of ALF. Pulmonary artery catheterization should be considered. Hypotension should be treated preferentially with fluids, but systemic vasopressor support with agents such as epinephrine, norepinephrine, or dopamine should be used if fluid replacement fails to maintain mean arterial pressure of 50–60 mm Hg. Vasoconstrictive agents (especially vasopressin) should be avoided.\n\nPulmonary edema and pulmonary infections are commonly seen in patients with ALF. Mechanical ventilation may be required. However, positive end-expiratory pressure can worsen cerebral edema.\n\nImpaired liver synthesis of clotting factors, low-grade fibrinolysis, and intravascular coagulation are typical of ALF. Thrombocytopenia is common and may also be dysfunctional. Replacement therapy is recommended only in the setting of bleeding or prior to an invasive procedure. Vitamin K can be given to treat an abnormal prothrombin time, regardless of whether there is poor nutritional status. Administration of recombinant factor VIIa has shown promise; however, this treatment approach requires further study. The use of gastrointestinal hemorrhage prophylaxis with a histamine-2 (H2) blocker, proton pump inhibitor, or sucralfate is recommended.\n\nIn patients with grade I or II encephalopathy, enteral feeding should be initiated early. Parenteral nutrition should be used only if enteral feeding is contraindicated as it increases the risk of infection. Severe restriction of protein is not beneficial; 60 g/day of protein is generally reasonable. Fluid replacement with colloid (e.g. albumin) is preferred rather than crystalloid (e.g. saline); all solutions should contain dextrose to maintain euglycemia. Multiple electrolyte abnormalities are common in ALF. Correction of hypokalemia is essential as hypokalemia increases the kidneys' ammonia production, potentially exacerbating encephalopathy. Hypophosphatemia is especially common in patients with acetaminophen-induced ALF and in those with intact renal function. Hypoglycemia occurs in many patients with ALF and is often due to depletion of hepatic glycogen stores and impaired gluconeogenesis. Plasma glucose concentration should be monitored and hypertonic glucose administered as needed.\n\nBacterial and fungal infections are common in ALF, with one study demonstrating culture-proven infection in 80% of ALF patients. Defective cellular and humoral immunity as well as presence of indwelling catheters, coma, broad-spectrum antibiotics, and medications that suppress immunity all predispose to infection. Localizing symptoms of infection such as fever and sputum production are frequently absent and the only clues to an underlying infectious process may be worsening of encephalopathy or renal function. There must be a low threshold for obtaining frequent cultures (blood, urine, and sputum), chest radiographs, and paracentesis. Bacteria that enter through the skin, such as streptococci and staphylococci, tend to predominate. Aggressive surveillance is essential as prophylactic antibiotics have shown little benefit. Fungal infections, particularly in the setting of broad-spectrum antibiotics, are also common, and disseminated fungemia is a poor prognostic sign.\n\nThe advent of transplantation has changed survival from as low as 15% in the pretransplant era to more than 60% today. Liver transplantation is indicated for many patients with ALF, and survival rates of 56–90% can be achieved. In addition to transplantation, better critical care and the trend toward more benign causes, such as acetaminophen, all contribute to improved survival rates. Spontaneous survival is now around 40%. The application of transplantation among patients with ALF remains low, suggesting that the full potential of this modality may not be realized. Timely availability of an allograft is one of the major factors determining transplant outcomes. In the largest U.S. study, only 29% of patients received a liver graft, while 10% of the overall group (one fourth of patients listed for transplantation) died on the waiting list. Other series have reported death rates of those listed for transplant as high as 40%.\nIn the ALFSG, the transplantation rate was higher in the groups with lower short-term spontaneous survival, making overall survival similar in all groups: acetaminophen, 73%; drug induced, 70%; indeterminate group, 64%; and other causes,61%. Causes of death for the 101 patients who died within the 3-week period included cerebral edema, multiorgan failure, sepsis, cardiac arrhythmia or arrest and respiratory failure. The median time to death after admission was 5 days.\n\nIntravenous N-acetylcysteine has been found to be beneficial in both acetaminophen toxicity and non-acetaminophen-related acute liver failure.\n\nHistorically mortality has been high, being in excess of 80%. In recent years the advent of liver transplantation and multidisciplinary intensive care support have improved survival significantly. At present overall short-term survival with transplant is more than 65%.\n\nSeveral prognostic scoring systems have been devised to predict mortality and to identify who will require an early liver transplant. These include King's College Hospital criteria, MELD score, APACHE II, and Clichy criteria.\n\nTo date, no universally accepted nomenclature has been adopted. Trey and Davidson introduced the phrase \"fulminant hepatic failure\" in 1970, which they described as a \"... potentially reversible condition, the consequence of severe liver injury, with an onset of encephalopathy within 8 weeks of the appearance of the first symptoms and in the absence of pre-existing liver disease\". Later, it was suggested that the term \"fulminant\" should be confined to patients who develop jaundice to encephalopathy within 2 weeks. Phrases \"subfulminant\" hepatic failure and \"late onset' hepatic failure were coined for onset between 2 weeks to 3 months and for 8 weeks to 24 weeks, respectively. The umbrella phrase of \"acute liver failure\" was proposed by King's College group, which has been adopted in this article. Paradoxically, in this classification, the best prognosis is in the \"hyperacute\" group.\n"}
{"id": "151043", "url": "https://en.wikipedia.org/wiki?curid=151043", "title": "Adolf of Germany", "text": "Adolf of Germany\n\nAdolf (c. 1255 – 2 July 1298) was Count of Nassau from about 1276 and elected King of Germany (King of the Romans) from 1292 until his deposition by the prince-electors in 1298. He was never crowned by the Pope, which would have secured him the title of Holy Roman Emperor. He was the first physically and mentally healthy ruler of the Holy Roman Empire ever to be deposed without a papal excommunication. Adolf died shortly afterwards in the Battle of Göllheim fighting against his successor Albert of Habsburg.\n\nHe was the second in the succession of so-called count-kings of several rivalling comital houses striving after the Roman-German royal dignity.\n\nHis last agnatic dynastic descendant was William IV of Luxembourg.\n\nAdolf was the reigning count of a small German state. He was born about 1255 and was the son of Walram II, Count of Nassau and Adelheid of Katzenelnbogen. Adolf’s brother was , who was appointed Archbishop of Trier in 1300.\n\nAdolf was married in 1270 to Imagina of Isenburg-Limburg (died after 1313) and they had eight children. Agnes of Isenburg-Limburg, the sister of Imagina, was married to Henry (Heinrich) of Westerburg, the brother of Siegfried II of Westerburg, the Archbishop of Cologne.\n\nIn 1276 or 1277, Adolf followed his father as Count of Nassau. From his father, he inherited the family’s lands south of the Lahn River in the Taunus Mountains. These included Wiesbaden and Idstein, as fiefdoms, and the Vogtship in Weilburg under the Bishopric of Worms. He also shared ownership of the family homelands around the castles of Nassau and Laurenburg.\n\nAround 1280, Adolf became involved in the with the Lords of Eppstein, in which the city of Wiesbaden was devastated and Sonnenberg Castle destroyed. The feud was settled in 1283, after which the city and the castle were rebuilt. Sonnenberg, along with Idstein, became Adolf’s residence. He granted Idstein town privileges in 1287 and built its fortifications.\n\nThrough his uncle, Eberhard I of Katzenelnbogen, Adolf came to the court of King Rudolf I of Habsburg. King Rudolf awarded him with the Burghauptmannamt (Castle Lordship) of Kalsmunt Castle in Wetzlar and a year later that of Gutenfels Castle near Kaub (where he became a vassal of the Counts Palatine of the Rhine).\n\nBefore his election, Adolf’s political activities had been limited to his role as Bundesgenosse of the Archbishop of Cologne. Adolf had no particular office, but likely became known through his involvement with the Archbishops of Cologne and Mainz in the politics of the Middle Rhine and Mainz areas. He spoke German, French, and Latin, which was rare at that time for nobles.\n\nAfter his election, King Adolf of Nassau would only rarely be in his home country, having transferred the government there to his burgmen. On 17 January 1294, he purchased Weilburg for 400 pounds from the Bishopric of Worms. He granted Weilburg town privileges on 29 December 1295. He also established the Clarisse abbey of Klarenthal near Wiesbaden in 1296.\n\nRudolf I of Habsburg died on 15 July 1291. For many years before his death, Rudolf had tried to secure the election of his eldest son Albert (Albrecht) as his successor. He was thwarted, however, by the opposition of the Archbishop of Cologne, Siegfried II of Westerburg, and the King of Bohemia, Wenceslaus (\"Václav/Wenzel\") II. Only the Count Palatine Louis II of Upper Bavaria \"the Rigorous\" promised to choose Albert. Wenceslaus, despite Rudolf's recognition of his electoral vote, refused to support Albert because he would not cede Carinthia to him. He took the side of the nobles in the core Habsburg areas of Swabia and in their newly acquired territories in Austria, with whom Albert was unpopular. Wenceslaus was supported by Duke Otto III of Lower Bavaria, whose family were traditional enemies of the Habsburgs. Wenceslaus succeeded in bringing the Electors of Brandenburg and Saxony over to his side: Albert II of Saxony signed an elector pact on 29 November 1291 that he would vote the same as Wenceslaus; Otto IV of Brandenburg made a similar commitment.\n\nArchbishop Siegfried believed that the Emperor should not receive the crown as an inheritance from his father, but should be freely selected by the College of Electors. He convinced the Archbishop of Mainz, , to select a king who would principally serve their interests. Gerard in turn recruited the new Archbishop of Trier, Bohemund I. Thereupon, the Count Palatine was forced to submit to the majority of the College of Electors. Siegfried therefore proposed to the Elector College to select Adolf of Nassau as king. They were ready to elect him, provided he make extensive concessions to the Electors and follow their political demands.\n\nA few days before the election, on 27 April 1292, the first of the electors, Archbishop Siegfried issued the Treaty of Andernach, stating that for Adolf to be chosen king he must promise a long list of acknowledgments of possession (including the imperial cities of Dortmund and Duisburg, and the Vogtship of Essen), pledges of imperial cities and castles, and a sum of 25,000 marks in silver. Furthermore, Adolf promised assistance against specifically listed opponents, but also the general promise that he would not admit any enemy of Siegfried II into his council. After the election, Adolf had to give the archbishop sufficient collateral for the fulfilment of the promise; otherwise he would lose his throne. The last clause is evidence of the fact that the end of the 13th century, the coronation of the king as the constitutive moment of his rule was still very critical. Adolf promised the archbishop to ask him first for his coronation when he had raised the agreed-upon collateral.\n\nThe other electors extracted similar concessions from Adolf, but only after the election. Among the most far-reaching were the concessions to King Wenceslaus of Bohemia on 30 June 1292. Adolf promised Wenceslaus to remove the two duchies of Austria and Styria from Albert of Habsburg. This was to be done as the previous King Rudolf had removed these territories from King Ottokar II of Bohemia, the father of Wenceslaus. Albert would be charged to agree to this arrangement at a court hearing. If Albert would not bend, the decision of the court would be executed by force within a year. Wenceslaus would then recover the lost territories of his father. Gerhard, the Archbishop of Mainz would receive the imperial cities of Mühlhausen and Nordhausen, which corresponded with the interests of Mainz in the Thuringian region. Furthermore, Gerhard received financial benefits. Like his counterpart in Cologne, the Mainz elector also forbade the presence of his opponents in Adolf’s court. In comparison to the benefits the Mainz, Cologne and Bohemian electors received, the donations to the Count Palatine and the Archbishop of Trier were modest.\n\nOn 5 May 1292 in Frankfurt am Main, the Archbishop of Mainz, in the name of all the electors, elected Adolf King of the Germans (Emperor-Elect). He was crowned in Aachen on 24 June by the Archbishop of Cologne.\n\nAdolf had neither influence nor power, and was elected \"Rex Romanorum\" because of the electors' preference for a weak king. His power was limited from the outset because of the commitments he made.\n\nAs he had agreed with the Archbishop of Cologne, Adolf remained in his dominion for four months after his election. The archbishop awaited from the king a revision of the results of the Battle of Worringen in 1288. He had hoped to again win a greater influence in the city of Cologne. But despite the tight specifications, Adolf soon emancipated himself from his Electors and concluded pacts with their opponents. Thus, for example, he confirmed the rights of the nobles and the city of Cologne, who had turned against their ruler, and even extended these rights.\n\nAdolf also very quickly broke the promises concerning the Duchies of Austria and Styria. As a clever diplomat, Albert of Habsburg avoided a confrontation with the new king. In exchange for his surrender of the Imperial Regalia, which he still had in his possession, he received, in November 1292 a formal enfeoffment with Austria, Styria, the Windic March, and the Lordship of Pordenone. The disposition of the prestigious insignia and relics of the empire was an additional and important sign for the legitimacy of the reign of the king, but not a mandatory prerequisite. With each new document, Adolf moved a little farther away from his promises, without having to open himself up to breach of contract accusations.\n\nAdolf acted as a self-assured ruler in other ways as well. His court was an attraction for all who sought protection from the powerful emerging territorial lords. He held numerous court days. At the beginning of his reign, he renewed the general public peace (\"Landfrieden\") of Rudolf I for another ten years, and brought about at least two regional peaces.\n\nAdolf used the feudal system as one of his major tools of power. He demanded from the spiritual princes a payment, called \"Lehnsware\", for their enfeoffment with regalian rights, and increased this demand to the level of a nuisance. Many of Adolf’s contemporaries considered this action to be simony. Many of today's historians, however, view it as an innovative way to open up new state revenue sources, as other Western European kings did. Also, the recovery and management of imperial property was important to him. So he succeeded, through clever marriage policy, to bring former imperial properties back under the control of the emperor.\n\nIn 1294, when Adolf’s rule was at its height, he concluded an alliance with the King Edward I of England against France and was awarded 60,000 pounds sterling, which corresponded to 90,000 gold marks. The pact had been preceded by attempts by Philip IV of France to conquer the Duchy of Burgundy and the County of Flanders. The Count of Flanders, Guy of Dampierre, mediated, therefore, the alliance between Edward I and Adolf for his protection against France. That the alliance was construed by his contemporaries as purely mercenary, and the fact that Adolf did not comply with its obligations, damaged his reputation, but this was initially without consequences.\n\nAdolf began recruiting troops in the empire for a war against France. On 31 August 1294, he sent a declaration of war to the French king, alleging he had seized rights and possessions of the empire. King Philip responded contemptuously on 9 March 1295. Pope Boniface VIII, however, ordered peace in 1295 and threatened to commence the excommunication of Adolf in the event of an outbreak of war.\n\nA little later Adolf intervened in war-torn Thuringia, where fighting had erupted between Landgrave Albert the Degenerate and his sons Frederick and Theodoric IV of Lusatia. He bought the Landgraviate from Albert in his capacity as king and probably using the payments from England. Legally, it was perfectly acceptable for Adolf to induce the feudal lord to abandon his fief and to bring the land under the empire. Furthermore, he seized the Margraviate of Meissen as an imperial fief, since it had been literally ownerless after the extinction of a collateral line of the House of Wettin and had been occupied by a son of Albert the Degenerate.\n\nThis purchase and the Margraviate of Meissen, however, affected the interests of four of the electors. The Archbishop of Mainz asserted that a part of Thuringia was not an imperial fief, but rather a fief of the Archdiocese of Mainz. Wenceslaus II of Bohemia was not thrilled by the growing power of the emperor on his northern border, especially since Adolf had promised to give him the Margraviate of Meissen. Also, all the electors hoped to profit from the turmoil in Thuringia. In addition to the ostensible return of imperial fiefs to the empire, it can not be ruled out that Adolf was anxious to build a dynastic power base (albeit a small one).\n\nFirst, Adolf succeeded in securing his acquisitions diplomatically and provoking the Margrave of Brandenburg toward active support and the Archbishop of Mainz and the Duke of Saxony toward at least acquiescence of the purchase. Two bloody campaigns against the sons of Albert the Degenerate were necessary to secure the acquisitions and a peace assured the achievements. Two years later, in the summer of 1296, Adolf proudly announced on the invitation to a court day that he had by his actions significantly increased the possessions of the empire.\n\nThe electors probably did not plan from the beginning to depose the king, but as events unfolded this result became more inevitable. The reason for the clashes was Adolf’s Thuringia policy. On Pentecost 1297 the Elector of Brandenburg, Duke of Saxony, and King of Bohemia joined together to enforce their interests. The Elector of Cologne, Gerhard II, was associated with this group.\n\nIn February 1298, the situation became alarming for Adolf because Wenceslaus II and Albert of Habsburg put aside their years of disputes over Austria and Styria, and reached an agreement in the event that Adolf was deposed and Albert elected in his place. There may have been a meeting of the electors as early as the coronation of King Wenceslaus of Bohemia, on 2 June 1297. In January 1298, through the efforts of the Archbishop of Mainz, Albert of Habsburg was brought to testify before an imperial court in order to find a compromise between Adolf and Albert. This did not happen; the two came close to battle in the Upper Rhine Valley and the matter was not resolved.\n\nOn 1 May 1298, the Archbishop of Mainz invited the king to his court, so that the dispute could be decided there. Archbishop Gerhard claimed he was authorised to do so as imperial arch-chancellor of Germany, according to an old legal principle. However, the King, as a party to the conflict, could not at the same time act as judge and saw these charges as a provocation given that Albert was raising arms against him, the rightful king. Therefore, the meeting planned for 15 June, at which the dispute was to be resolved, did not take place.\n\nA meeting between the Archbishop of Mainz, Count Albrecht of Saxony-Wittenberg, and three Margraves of Brandenburg on 23 June 1298 then led to a lawsuit against the king himself. The Archbishop of Cologne and the King of Bohemia had previously authorised the Archbishop of Mainz to act in their names. In these proceedings Adolf was charged with numerous crimes, including the continued breach of the peace in Thuringia and the breaking of the promises he had made to the Archbishop of Mainz. Adolf was deemed unworthy of his office and had forfeited his royal dignity.\n\nIt is remarkable that Adolf was not excommunicated by the Pope before being deposed. The pope was probably not even included in the deposition procedure. The princes, it is true, did try to formulate their arguments similar to Innocent IV’s statement in the deposition of Frederick II, but the process was unheard of for this time. Because Adolf had been elected and crowned, the contemporary understanding was that he had been chosen by God to be the ruler and that the princes were breaking their oath in which they swore loyalty to the king. Therefore, included in the list of charges were some that, at first glance, appear peculiar today, such as the desecration of communion wafers and the simonistic extortion of money. Furthermore, there was no imperial legal procedure for the ousting of the king. Therefore, the princes relied on their right to vote, from which also they derived their right to oust a king. This argument was problematic insofar as the deposition of Frederick II was already a precedent for this case. According to church law, only the Pope had the power to depose a king.\n\nFollowing Adolf’s deposition came the election of Albert I of Habsburg as the new king. How this election took place is not very clear today, as the chroniclers have little to report. The question is open, for example, whether Albert actually initially did not want to accept the choice, as he would later assert against Pope Boniface VIII.\n\nTo depose Adolf was one thing; it was another to enforce the decision against him. Adolf refused to accept this decision, but the conflict between him and the princely opposition was soon decided on the battlefield. On 2 July 1298 the armies of Adolf and Albert met at the Battle of Göllheim. The small village of Göllheim is situated in northern Rhineland-Palatinate between Kaiserslautern and Worms in present-day Donnersbergkreis. After violent attacks, Adolf fell together with his standard-bearers and a few faithful. Adolf’s army turned to flee and quickly dispersed.\n\nAlbert did not allow the followers of Adolf to bury the body of the fallen king in the \"Kaiserdom\", the Imperial Cathedral of Speyer. Therefore, Adolf was initially buried in the Cistercian monastery of in present-day Kerzenheim and was only later transferred to Speyer.\n\nOn 29 August 1309, Albert I’s successor, Emperor Henry VII transferred Adolf’s remains to the Speyer Cathedral, where he was buried next to Albert, who had been murdered in 1308. In 1824, Duke William of Nassau built a grave monument in the vestibule of the cathedral. Leo von Klenze was commissioned with the design, which shows King Adolf in armor kneeling in prayer.\n\nProbably in the 19th century, the legend arose that Adolf was a count from the Nuremberg area. This misconception was probably based on confusion with Emich I of Nassau-Hadamar, who after his marriage to Anne of Nuremberg around 1300 was the holder of Kammerstein Castle.\n\nIn 1841 Duke Adolf of Nassau commissioned a portrait of Adolf by the Düsseldorf painter Heinrich Mücke. In 1843 this painting was hung in the Frankfurt \"Kaisersaal\" (Hall of Kings). The picture depicts King Adolf with chest armor, a white coat; and wearing an iron crown with an \"implied spiked helmet”; in his right hand he holds a sword and in the left a shield with an eagle. It also bears the Latin phrase \"Praestat vir sine pecunia quam pecunia sine viro\" (Better a man without money than money without a man). Since no contemporary images of the King exist, the portrait is an idealized representation by the artist in the spirit of historicism. It is not based on previous portraits, since Mücke considered other representations, such as the one attributed to Georg Friedrich Christian Seekatz, to be too moderate\n\nOn 8 May 1858, Duke Adolf of Nassau established a Military and Civil Order of Merit for the Duchy. It was named for King Adoph as the most important representative of Walram line of the House of Nassau. Although the Duchy of Nassau was annexed by Prussia in 1866, Duke Adolf maintained and renewed the Order when he became Grand Duke of Luxembourg. Until today, it is a respected Order of Merit of the House of Nassau.\n\nThomas Carlyle calls him \"a stalwart but necessitous Herr\".\n\nHe married Imagina of Isenburg-Limburg, daughter of Gerlach IV of Isenburg-Limburg and Imagina of Blieskastel. Their children were:\n\n\n\n\n"}
{"id": "11760283", "url": "https://en.wikipedia.org/wiki?curid=11760283", "title": "Arn (bishop of Würzburg)", "text": "Arn (bishop of Würzburg)\n\nSaint Arn or Arno von Endsee (died 13 July 892) was the Bishop of Würzburg from 855 until his death. He was a pupil of Bishop Gozbald, who died on 20 September 855; Arn was elected bishop in his place. Arn was a warrior-prelate, recorded fighting against almost every external foe of the Germans at one point in his career or another.\n\nIn his first year in office, the cathedral of Würzburg was destroyed by lightning and Arn had to rebuild it. He was an active participator in the East Frankish government of Louis the German (who appointed him), Charles the Fat, and Arnulf of Carinthia.\n\nIn 871, Louis the German held an assembly at Frankfurt and from there sent Arn and Ruodolt, Margrave of the Nordgau, to defend the border between the Duchy of Bavaria and Great Moravia because he had heard that the Moravians were planning an invasion. The Moravians had constructed a very large, circular wall to force the Germans through a very narrow opening and thus cut them off from fleeing. Arn, however, aware of the trap, caught a Moravian army leading back a Bohemian bride offguard and forced it into the trap. The Moravian were forced to abandon their horses and flee on foot. In 872, however, he assisted Carloman of Bavaria against Svatopluk of Moravia and was defeated.\n\nIn 884, Arn and Henry of Franconia led the forces of all East Francia against a Viking army invading Saxony and were victorious. In 892, Arn, on the advice of Poppo, Duke of Thuringia, had undertaken an expedition against the Wends and was killed, either during a mass on the Chemnitz near Frankenburg or, after withdrawing to Sandberg (perhaps Wiederau or Taurastein), in a decisive battle with the Slavs. Poppo was deposed from his office for his poor counsel and Arn was replaced by Rudolf, a member of the Conradine family.\n\nHe is buried in St-Aegidien in Colditz and was immediately reckoned a martyr. He was finally canonised in the 18th century. Around 1250, a chapel was built in his honour at Mittweida.\n\n"}
{"id": "6036140", "url": "https://en.wikipedia.org/wiki?curid=6036140", "title": "Bert Combs", "text": "Bert Combs\n\nBertram Thomas Combs (August 13, 1911 – December 4, 1991) was an American jurist and politician from the U.S. state of Kentucky. After serving on the Kentucky Court of Appeals, he was elected the 50th Governor of Kentucky in 1959 on his second run for the office. Following his gubernatorial term, he was appointed to serve as a United States Circuit Judge of the United States Court of Appeals for the Sixth Circuit by President Lyndon B. Johnson, serving from 1967 to 1970.\n\nCombs rose from poverty in his native Clay County to obtain a law degree from the University of Kentucky and open a law practice in Prestonsburg. He was decorated for prosecuting Japanese war criminals before military tribunals following World War II, then returned to Kentucky and his law practice. In 1951, Governor Lawrence Wetherby appointed him to fill a vacancy on the Kentucky Court of Appeals. Later that year, he was elected to a full term on the court, defeating former governor and judge Simeon S. Willis. Kentucky's Democratic Party had split into two factions by 1955 when Earle C. Clements, the leader of one faction, chose Combs to challenge former governor and U.S. Senator A. B. \"Happy\" Chandler, who headed the other, in the upcoming gubernatorial primary. \n\nCombs' uninspiring speeches and candidness about the need for more state revenue cost him the primary election. Chandler, who went on to reclaim the governorship, had promised that he would not need to raise taxes to meet the state's financial obligations, but ultimately he did so. This damaged Chandler's credibility and left Combs looking courageous and honest in the eyes of the electorate. Consequently, in 1959 Combs was elected governor, defeating Lieutenant Governor Harry Lee Waterfield, Chandler's choice to succeed him in office, in the primary. Early in his term, Combs secured passage of a three-percent sales tax to pay a bonus to the state's military veterans. Knowing a tax of one percent would have been sufficient, he used the excess revenue to enact a system of reforms, including expansion of the state's highway and state park systems. He also devoted much of the surplus to education.\n\nFollowing his term in office, Combs was appointed to the United States Court of Appeals for the Sixth Circuit by President Johnson. He served for three years before resigning and running for governor again in 1971. He lost in the Democratic primary to Wendell Ford, his former executive secretary. In 1984, Combs agreed to represent sixty-six of the state's poor school districts in a lawsuit challenging the state's system of financing public education. The suit, \"Rose v. Council for Better Education\", resulted in the Kentucky Supreme Court declaring the state's entire system of public schools unconstitutional. In response, the Kentucky General Assembly drafted a sweeping education measure known as the Kentucky Education Reform Act in 1991. On December 3, 1991, Combs was caught in a flash flood while driving and was killed.\n\nThe Combs family was one of the oldest European families in the United States. John Combs, the family patriarch, arrived in Jamestown, Virginia in 1619, and in 1775 Benjamin John Combs came westward from Virginia into Clark County, Kentucky. He was followed into Kentucky in 1790 by two of his brothers, including Jack Combs, Bert Combs' great-grandfather.\n\nBert Combs was born in the Town Branch section of Manchester, Kentucky on August 13, 1911; he was one of seven children of Stephen Gibson and Martha (Jones) Combs. Combs's father Stephen, a part-time logger and farmer, was active in local politics, despite being a Democrat in a county where a large majority of residents were Republicans. His mother was a teacher, and she impressed upon her children the importance of a good education. Bert's first school was the two-room Beech Creek grade school. When he reached the seventh grade, his parents sent him and his sister to Oneida Baptist Institute in nearby Oneida because its school term was 8 to 9 months long, as opposed to the 5- to 6-month terms at Beech Creek. Later, Combs and his sister began riding a donkey every day to Clay County High School. Combs excelled academically and skipped some grades, graduating as valedictorian of his class in 1927 at age 15.\n\nUnable to afford college tuition, Combs worked at a local drug store and did small jobs for various residents of his community. In 1929, his mother arranged for him to work at a coal company in Williamsburg and attend Cumberland College (then a junior college). The coal company job did not materialize, but Combs was able to afford three semesters at Cumberland by sweeping floors and firing furnaces in campus buildings. In mid-1930, he began working as a clerk for the state highway department. This was one of several patronage jobs that were usually awarded by the governor, but the Democratically-controlled state legislature had stripped Republican Governor Flem D. Sampson of his statutory appointment powers, giving them instead to a three-man highway commission composed of Democratic Lieutenant Governor James Breathitt, Democratic Highway Commissioner Ben Johnson, and Dan Talbott. This allowed Combs, a Democrat, to secure the position.\n\nCombs worked for the highway department for three years in order to earn enough money to attend the University of Kentucky College of Law in Lexington. While at the university, he was managing editor of the \"Kentucky Law Journal\". In 1937 he graduated second in his class, earning a Bachelor of Laws degree and qualifying for the Order of the Coif, a national honor society for the top ten percent of graduating law students. He was admitted to the bar, and returned to Manchester to begin practicing law. It was also in 1937 that Combs married Mabel Hall, with whom he had two children, Lois Ann Combs and Thomas \"Tommy\" George Combs.\n\nOf his law practice in Manchester, Combs later noted: \"I had too many kinfolks and friends in Manchester, and they all expected me to handle things as a favor ... Then they'd get their feelings hurt if I charged them. I was taking in a lot of cases, but not sending out many bills.\" In 1938, Combs accepted an offer from a law school classmate named LeRoy Combs (no relation) to join his father and uncle's law firm in Prestonsburg. Prestonsburg was closer to his wife's home in Knott County. Combs' son Tommy suffered from a form of mental retardation, the result of an injury sustained at birth. After moving to Prestonsburg, Combs started a class for people with mental retardation, in part so Tommy could attend the class.\n\nOn December 22, 1943, Combs enlisted as a private in the U.S. Army for service in World War II. He received his basic training at Fort Knox and participated in the Volunteer Officer Candidate Program, which would have allowed him to attend Officer Candidate School (OCS) immediately after basic training. Instead, he was briefly assigned to teach cartography at the Aberdeen Proving Ground in Aberdeen, Maryland before completing OCS in Ann Arbor, Michigan, joining the Judge Advocate General's Corps, and attaining the rank of captain. On July 1, 1945, he was sent to the South Pacific. He served as chief of the War Crimes Investigating Department under General Douglas MacArthur in the Philippine Islands, conducting tribunals for Japanese war criminals. Upon his discharge in 1946, he was awarded the Bronze Star and the Military Merit Medal of the Philippines.\n\nAfter the war, Combs returned to Prestonsburg, forming the law firm of Howard and Combs with J. Woodford Howard as his partner. He served as president of the Junior Bar Association of Kentucky in 1946 and 1947. Combs often represented coal companies in workers' compensation cases against Carl D. Perkins, later a U.S. Representative, who served as legal counsel for the mine workers.\n\nCombs began his political career with his election to the office of city attorney in Prestonsburg in 1950. Later that year, Governor Lawrence Wetherby appointed him to fill a vacancy in the office of Commonwealth's Attorney for Kentucky's 31st Judicial District. Combs announced, however, that he would serve only until a new election could be held. In April 1951, Governor Wetherby appointed Combs to fill a vacancy on the Kentucky Court of Appeals caused by the death of Judge Roy Helm. Later that year, he sought a full eight-year term on the court. His opponent was Simeon S. Willis, a popular former Republican governor who had previously sat on the court. Combs won the election by a vote of 73,298 to 69,379. In George Robinson's oral history, Combs attributed his victory to Willis's advanced age (68) and the fact that many of Willis' supporters assumed that their candidate would win and did not vote.\n\nA.B. \"Happy\" Chandler, who had served as Kentucky's governor from 1935 to 1939 and was a leader of a faction of the state's Democratic Party, announced his intention to seek a second term in 1955. Members of the anti-Chandler faction scrambled to find a candidate to oppose him. The most likely candidate was Emerson \"Doc\" Beauchamp, the sitting lieutenant governor, but Beauchamp was not a good campaigner and his ties to Logan County – where politics were dominated by sometimes-corrupt political bosses – gave the anti-Chandler faction pause. Instead, the leader of the faction, former governor and sitting U.S. Senator Earle C. Clements, selected Combs as the faction's nominee, and Combs resigned from his position on the Court of Appeals to enter the race.\nIn Combs' first speech of the primary campaign, he admitted that the state needed to raise $25 million ($ million in dollars) in new revenue and that a sales tax should be considered. Chandler, the more experienced politician, attacked Combs for this suggestion, maintaining that an experienced governor like himself would not need to raise taxes to meet the state's obligations. Combs' speech was also attacked as dry and uninspiring, partly because he read it verbatim from prepared notes. \"And you said \"I\" couldn't give a speech,\" Doc Beauchamp later complained to Clements. Hugh Morris, chief of the Louisville \"Courier-Journal\"'s Frankfort bureau, commented that \"Combs opened and closed his campaign on the same night\".\n\nWith little but Combs' inexperience to run against, Chandler portrayed Combs as a pawn of former governors Clements and Wetherby, whom he derisively nicknamed \"Clementine\" and \"Wetherbine\". He accused both administrations of wasteful spending, specifically attacking the construction of the Kentucky Turnpike and Freedom Hall as unnecessary expenditures. Some of Chandler's attacks were more personal in nature; he charged that when Clements was governor, he spent $20,000 ($ in dollars) on a new rug for his office, and that Wetherby had used African mahogany to panel his office, instead of \"good, honest Kentucky wood\". Though receipts later showed that carpeting for the entire first floor of the capitol had cost only $2,700 and that Wetherby's paneling had been purchased from and installed by a Kentucky contractor, Chandler's charges remained effective at keeping the Combs campaign on the defensive.\n\nTwo weeks before the primary, Combs was endorsed by former Vice-President and native Kentuckian Alben W. Barkley, but Combs felt the endorsement came too late to be much help. Chandler defeated Combs in the primary by a vote of 259,875 to 241,754 and went on to win his second term as governor. Combs returned to Prestonsburg, set up a savings and loan company, and re-established his law practice. During the four years of Chandler's term, Combs accepted a number of speaking engagements, but otherwise remained out of the public eye. Meanwhile, the state's need for funds compelled Chandler to raise the state sales tax and other taxes, despite his campaign promises not to do so. Consequently, Chandler lost credibility and Combs gained a reputation as a courageous, forthright, and honest politician for having acknowledged the state's financial need during the campaign.\n\nBarred by the state constitution from seeking consecutive terms, Chandler endorsed his lieutenant governor, Harry Lee Waterfield, to succeed him. Wilson Wyatt, who had managed Adlai Stevenson's presidential campaign in 1952, was the first anti-Chandler candidate to declare his intention to seek the governorship in the 1959 election, doing so on April 9, 1958. Wyatt received several endorsements from leaders in Jefferson County, which contained his home city of Louisville and was vehemently anti-Chandler. Four days after Wyatt's announcement, Combs declared that he would again seek the office, and he was endorsed by Clements a week later. For the remainder of 1958, the anti-Chandler faction's support remained split between Wyatt and Combs. In January 1959, Clements held an all-night meeting at the Standiford Airport Hotel in Louisville in which he brokered a deal whereby Combs would run for governor and Wyatt for lieutenant governor. Clements promised Wyatt his support in future political races.\n\nIn the primary campaign against Waterfield, Combs attacked the Chandler administration. He was especially critical of a rumor which held that Chandler had placed a two-percent assessment on state employees' salaries and had stored the funds in a Cuban bank so they could not be traced. According to the rumor, when Fidel Castro seized power during the Cuban Revolution, the funds Chandler had deposited in Cuba were lost. Chandler countered on Waterfield's behalf with charges that Combs was a \"Clements parrot\". Combs succeeded in uniting the anti-Chandler base, and defeated Waterfield by 25,000 votes; he went on to win the governorship that fall, defeating Republican nominee John M. Robsion, Jr. by 180,093 votes. The victory margin was a record for a governor's race in Kentucky, and was the second highest margin of victory for any election in the state, trailing only Franklin D. Roosevelt's 185,858-vote victory over Herbert Hoover in 1932. Combs was the first governor elected from Eastern Kentucky since Flem D. Sampson in 1927, and was the first veteran of World War II to hold the office.\n\nOne of Combs' first official actions as governor was to call a special session of the legislature on December 19, 1959, to consider revising the state's constitution, which had been in effect since 1891. Calling a constitutional convention required that the General Assembly approve putting the issue of a convention on the ballot in two consecutive legislative sessions. The call then had to be approved by Kentucky voters. Despite near-universal agreement by legal scholars that the constitution was badly in need of updating, Kentucky voters had rejected calls for a constitutional convention in 1931 and 1947, and had only approved 19 amendments since 1891. Combs wanted to address the issue during his four-year term, hence the haste in calling the special legislative session. The General Assembly easily approved the call for a convention during the special legislative session and again during the subsequent regular legislative session in 1960. Combs signed the measure, and the question of a constitutional revision was put on the ballot in November 1960, when Kentucky voters defeated it by a margin of almost 18,000 votes. This was the closest Kentucky has come to replacing the 1891 constitution, which remains in effect today.\n\nDuring the campaign, Combs had advocated a progressive platform that included increased funding for education, highways, parks, industry, and airports. Soon after his election, he won approval for a three-percent sales tax to pay a bonus to military veterans, although he could have funded the bonuses with a one-percent tax. He had asked for the larger tax in order to fund his other priorities. As a result of the sales tax, Combs presided over the state's first billion-dollar budget. One study showed that Kentucky doubled its per capita expenditures between 1957 and 1962, growing its appropriations faster than any other state. Combs held large public relations events for each tax-funded project that was completed, declaring in dedication speeches that the sales tax had made the project possible.\nIn 1960, Kentucky had one of the highest dropout rates in the nation, and ranked second only to Arkansas in the number of one-room schools. Fewer than half of the state's high school graduates attended college. Many teachers educated in Kentucky sought higher salaries available in other states. Combs' biennial budget, passed by the General Assembly in 1960, used money from the new sales tax to increase school funds by fifty percent and establish the state community college system (now the Kentucky Community and Technical College System). It also increased funding for free textbooks by more than $3 million and allocated another $2 million to vocational education. It allocated over $5 million to the state universities for new buildings and another $10.5 million to fund completion of the Albert B. Chandler Hospital, a facility at the University of Kentucky named in honor of Combs' political foe.\n\nThe state's roads were in poor condition when Combs became governor. The Automotive Safety Foundation found that two-thirds of Kentucky's federal roads were below standards for existing traffic demands. It further found that twenty percent of the state's major city streets were inadequate, that another fifty-five percent would soon be inadequate due to increasing traffic, and that half of the state's secondary roads were unfit for modern industrial traffic. To address these problems, Combs issued $100 million in bonds to increase funding for highways, appointing Earle Clements as state highway commissioner to oversee the correction of the road issues. One of the new roads, the Mountain Parkway, which connected Combs' native Eastern Kentucky to Central Kentucky, was later renamed the Bert T. Combs Mountain Parkway in Combs' honor. Because of generous funding in Combs' budget, Kentucky finished its portions of the Interstate Highway System much sooner than surrounding states such as Virginia and Tennessee.\n\nCombs also won approval of a $10 million bond issue to benefit the state parks, which had poor lodging and few amenities. Combs combined the bond issue with $10 million in revenue bonds and effected major renovations at all 26 of the state's parks. Though his dreams of seeing privately owned tourist facilities spring up around the parks did not come to fruition, out-of-state tourism to Kentucky more than doubled during his administration, accounting for about sixty percent of state park visitors and fifty-three percent of the overnight visitations to the parks. Journalist John Ed Pearce recounts that Kentucky natives began to complain that they could not get reservations in the parks during peak seasons and called for limitations on the number of out-of-state visitors or a reservation system that favored Kentuckians, although nothing was done to address these complaints.\nOn April 10, 1961, Combs appropriated $50,000 from the governor's contingency fund to construct a floral clock on the lawn of the state capitol. Combs had seen a similar clock in Edinburgh, Scotland, and believed it would be a colorful addition to the capitol grounds. In a subsequent gubernatorial campaign, Happy Chandler mocked the clock, declaring \"Well, they don't say it's half past 2 in Frankfort anymore. They say it's two petunias past the jimson weed.\" Chandler's derision became the minority view in time, however; according to John Ed Pearce, the clock became one of the most talked-about and visited tourist attractions in the state and the most visited place in Frankfort.\n\nCombs created a merit system for state government workers, ensuring that officials could not be hired or fired for political reasons. This provision attracted more well-qualified people to public service careers. Such careers were made even more attractive when, in 1962, the state courts declared that the salaries of state employees, the amounts of which were specified in the state constitution, could be adjusted for inflation. Combs demanded that state employees stick strictly to the rules governing their offices. In one instance, Combs ordered a state audit of Carter County superintendent Heman McGuire, who was known to use his office for political gain. While Combs did not have the authority to remove McGuire directly, the audit showed McGuire's misappropriation of funds and abuse of power. The state school board investigated these findings and removed the county school board members from office; the replacement board members then ousted McGuire.\n\nIn 1961, a group of citizens from Newport asked Combs for help in cracking down on crime in their city. Just across the Ohio River from Cincinnati, Newport had gained a reputation as a haven for prostitution, gambling, and illegal alcohol. After receiving an affidavit from the citizens, Combs sent agents from the department of Alcoholic Beverage Control to the city. They cited six bars for violating laws governing liquor sales, and instructed Attorney General John B. Breckinridge to prosecute four local officials for failure to enforce the laws. When allegations of civil rights violations in a related trial surfaced, U.S. Attorney General Robert Kennedy sent federal Justice Department officials to Newport, prompting the resignation of the sheriff and a circuit judge. Two other local officials were barred from office for four years.\n\nSome of Combs' crackdowns on corruption were politically damaging, including the so-called \"truck deal\". In 1961, Kyle Vance, a reporter for the Louisville \"Courier-Journal\" reported that the state was about to purchase some dump trucks from one of Combs' former campaign officials for $346,800, far more than they were worth, according to the report. The newspaper, long antagonistic toward Highway Commissioner Clements, painted the deal as a political payoff orchestrated by the highway commissioner. In the interest of preserving his reputation as an honest governor, Combs canceled the proposed deal. This angered Clements, who took Combs' action as a public rebuke. The incident caused a rift between him and Combs that never fully healed; Clements later resigned, ostensibly to work on the presidential campaign of his friend and former Senate colleague, Lyndon B. Johnson. Thereafter, he worked against Combs at every opportunity, even joining with Happy Chandler to ensure Wilson Wyatt's defeat in his 1962 race for the Senate, in a reversal of his previous promise to support Wyatt.\n\nCombs also formed the state's first Human Rights Commission and ordered the desegregation of all public accommodations in Kentucky. The latter action was commended in a letter to Combs from President John F. Kennedy. In 1961, Combs was awarded an honorary Doctor of Laws degree from the University of Kentucky, and on February 17, 1962, he received an award from Keep America Beautiful for his work on cleaning up Kentucky's highways, including securing passage of a bill requiring that auto junkyards near major roadways be screened from view by fences.\n\nAmong Combs' other accomplishments as governor were requiring voting machines in state elections and passage of a law making the assessment of state employees for political campaign funds a felony. At the end of his term, Combs backed Edward T. Breathitt to succeed him as governor. Breathitt defeated Happy Chandler in the Democratic primary, then went on to defeat Republican Louie B. Nunn in the general election. It was the only time in the 20th century that a Kentucky governor's preferred successor won election.\n\nFollowing his term as governor, Combs returned to his legal practice. He was a charter member and chairman of the Eastern Kentucky Historical Society and a trustee at Campbellsville College. In 1963, he was awarded the Joseph P. Kennedy International Award for \"outstanding contributions and leadership in the field of mental retardations.\" He was named Kentucky's outstanding attorney in 1964, and in the spring of that year, he served as a visiting professor in the Political Science Department at the University of Massachusetts. In 1965, he was inducted into the University of Kentucky's Hall of Distinguished Alumni.\n\nIn August 1964, Combs declined a nomination to the bench of the United States District Court for the Western District of Kentucky. During the administration of Combs' successor, Ned Breathitt, Republicans gained strength within the state behind the leadership of Louie Nunn, Marlow Cook, and William O. Cowger. The Republican rise, coupled with Democratic factionalism, prompted many prominent state Democrats to approach Combs about seeking another term as governor. Combs wavered on whether to seek the Democratic nomination until October 1966, when he publicly declared his support for Henry Ward. In a later interview with historian George W. Robinson, Combs recounted that he \"would have run at that time except for a personal family situation\". Ward handily defeated his primary opponents, Happy Chandler and Harry Lee Waterfield, but lost in the general election to Louie Nunn.\n\nOn January 16, 1967, President Lyndon Johnson nominated Combs to the United States Court of Appeals for the Sixth Circuit, replacing the deceased Shackelford Miller Jr., and the Senate confirmed the nomination on April 5, 1967 and Combs received his commission the same day. Because of the rules of the federal judiciary, Combs had to liquidate his business and banking assets and severely restrict contact with many of his political acquaintances to avoid potential conflicts of interest with cases he might adjudicate on the Court of Appeals. He expressed frustration that the cases that came before the court were frequently appealed to the Supreme Court, which often gave little weight to the opinions rendered by the Court of Appeals. Consequently, he resigned from the court on June 5, 1970, and joined the Louisville law firm of Tarrant, Combs, and Bullitt (later Wyatt, Tarrant & Combs).\nWith the end of Governor Nunn's term approaching, a rivalry for leadership of the state Democratic party developed between Lieutenant Governor Wendell H. Ford, who had served as Combs' chief administrative assistant during his gubernatorial term, and Julian M. Carroll, speaker of the state House of Representatives. As soon as Combs resigned from the Court of Appeals, Democratic leaders began asking him to seek the party's gubernatorial nomination in 1971, uniting the party behind him rather than splintering it between Ford and Carroll. In June 1970, Carroll announced his intent to run for lieutenant governor, indicating that he would like to serve under Combs as governor. While Combs considered whether or not to seek the nomination, Ford declared his candidacy. Days later, Combs also entered the race.\n\nCombs and Ford advocated similar platforms, but Combs encouraged the state's teachers to become more politically active, negotiating higher salaries and better benefits for themselves, while Ford was critical of educators becoming involved in politics and only advocated more spending on education if the state could afford it. In his oral history of Combs, Robinson noted that Ford, thirteen years Combs' junior, \"came across better on television\" and that many voters in the state felt that Combs must have ulterior motives in leaving a judgeship that paid a salary of $42,500 for the governorship, which paid only $30,000 annually. Catholics were also upset that Combs had married his second wife, Helen Clark Rechtin, just forty-three days after his divorce from Mabel Hall was finalized on July 18, 1969. (Combs and Hall had been separated for five years prior to finalizing the divorce.) Despite these handicaps, many Democrats assumed that Combs, the proven candidate, would easily defeat the newcomer Ford, and fewer than one-third of registered Democrats voted in the primary. In what the \"Courier-Journal\" called a \"stunning defeat\", Ford defeated Combs in the Democratic primary and went on to win the governorship.\n\nAfter the 1971 primary, Combs retired from politics and resumed his law practice, maintaining an office in Frankfort. He continued to represent large coal companies, drawing the ire of local environmentalist and author Harry M. Caudill, who asserted that Combs claimed to represent the powerless while actually representing the powerful. He was active in the formation of the Rural Housing and Development Corporation and served on the Council on Higher Education. He also served on President Jimmy Carter's General Advisory Committee on Arms Control and Disarmament.\n\nCombs' second marriage ended in divorce on May 19, 1986. On December 30, 1988, he married his law assistant, Sara M. Walter.\n\nOn October 3, 1984, leaders of the Council for Better Education asked Combs to represent them in a legal challenge to Kentucky's school financing system, which it claimed unfairly discriminated against poorer school systems in the state. Combs felt the lawsuit would be difficult to win and could cause retaliation against his other clients by state government officials. He needed this lawsuit \"about like a hog needs a side saddle\", he would later claim; nevertheless, he agreed to take the case if the Council could convince thirty to forty percent of the state's school boards to join it. The Council eventually persuaded 66 of the 177 school boards to join. Working pro bono, Combs assembled a legal team that included Kern Alexander, a Kentucky native and education law expert who was named president of Western Kentucky University in November 1985.\nCombs first attempted to gain legislative concessions that might preclude the need for a lawsuit. Governor Martha Layne Collins proposed an education reform agenda and called the legislature into special session in mid-1985 to consider it. The legislature enacted a corporate income tax to raise $300 million aimed at reducing class sizes, but the Council was seeking more fundamental structural changes to the system and deemed the increased funds insufficient to equalize its members' standing with that of more affluent school districts. Dissatisfied with the results of the special session, Combs and the Council filed their suit, \"Rose v. Council for Better Education\", on November 20, 1985. The governor, state superintendent, state treasurer, leaders of both houses of the state legislature, and every member of the state board of education were named as defendants in the case.\n\nThe defendants' request for summary judgment dismissing the case was not granted, and the trial began in Franklin circuit court on August 4, 1987. During the trial, a new state superintendent was elected. The new superintendent, John Brock, announced that his office would drop its defense and side with the Council, a major blow to the defense. On May 31, 1988, Judge Ray Corns found in favor of the plaintiffs, declaring that the school finance system was \"unconstitutional and discriminatory\". Two days later, the defense announced that it would appeal the ruling to the Kentucky Supreme Court, but recently elected governor Wallace G. Wilkinson refused to join the appeal and supported Judge Corns' ruling.\n\nOpening arguments in the appeal began December 7, 1988. The defense argued that the Council lacked standing to bring the suit; Combs rebutted this argument and cited statistics that ranked Kentucky as the most illiterate state in the nation to show how inequitable financing had adversely affected the state's students. On June 8, 1989, the court handed down a 3–2 ruling declaring Kentucky's entire public school system unconstitutional and giving the General Assembly until the end of their next legislative session, which would convene in January 1990, to create a replacement. Commenting on the ruling, Combs said \"My clients asked for a thimble-full, and [instead] they got a bucket-full\".\n\nThe court set out nine minimum standards. In response to the court's ruling, the General Assembly passed the 1990 Kentucky Education Reform Act, which radically altered Kentucky's school system, providing mechanisms to equalize funding among school districts and implementing some of the toughest accountability standards in the United States. Of the legislature's actions, Combs opined \"Kentucky has now, by reason of this legislation, decided to become educated—and we have embarked on a crusade for that purpose. Don't be surprised if we should within the next decade develop a first class, world-wide educational system.\"\n\nOn December 3, 1991, Combs left his law office during a flash flood about 5:30 pm. He was reported missing hours later, and the following day, he was found dead of hypothermia just downstream from his car in the Red River near Rosslyn, in Powell County. He was buried in the Beech Creek Cemetery in Manchester.\n\nIn addition to the Bert T. Combs Mountain Parkway, Bert T. Combs Lake, an artificial lake constructed in 1963 in Clay County, is named in Combs' honor. On April 20, 2007, two life-sized statues of Combs were dedicated—one in Stanton, near the parkway that bears his name, and another in the county courthouse in Prestonsburg. Combs' widow, Sara Walter Combs, became the first woman to serve on the Kentucky Supreme Court in 1993 and currently sits on the Kentucky Court of Appeals, where she was chief judge from 2004 to 2010, also a first for a woman. Combs' daughter, Lois (Combs) Weinberg, unsuccessfully challenged incumbent Mitch McConnell for his Senate seat in 2002.\n\n\n"}
{"id": "59032954", "url": "https://en.wikipedia.org/wiki?curid=59032954", "title": "Bhanji Dal Jadeja", "text": "Bhanji Dal Jadeja\n\nBhanji Jadeja also known as Bhanji Dal Jadeja is commander of Nawanagar State army and jagirdar of Guana jagir. He commanded Jam Sataji's force during Mughal attack on Junagadh State.\n"}
{"id": "46820994", "url": "https://en.wikipedia.org/wiki?curid=46820994", "title": "Blutzeuge", "text": "Blutzeuge\n\nBlutzeuge () was a term used in Nazi propaganda in the 1930s–1940s in Germany depicting a hero cult of \"fallen\" Nazis who had been killed during the Nazi ascent to political power. An early Nazi usage of the term was Hitler's dedication at the start of \"Mein Kampf\", which he dedicated to the sixteen National Socialist members killed in the 1923 Beer Hall Putsch.\n"}
{"id": "7953974", "url": "https://en.wikipedia.org/wiki?curid=7953974", "title": "Bubba Watson", "text": "Bubba Watson\n\nGerry Lester \"Bubba\" Watson Jr. (born November 5, 1978) is an American professional golfer who plays on the PGA Tour. One of the few left-handed golfers on tour, he is a multiple major champion, with victories at the Masters Tournament in 2012 and 2014. In February 2015, Watson reached a career-high 2nd place in the Official World Golf Ranking.\n\nWatson is among the longest drivers on the PGA Tour; in 2007 he had an average drive of and can hit a ball over , capable of generating a ball speed up to . He has finished top of the driving distance statistics five times in his career, during the 2006, 2007, 2008, 2012, and 2014 seasons.\n\nWatson was born and raised in Bagdad, Florida, near Pensacola. He played on the golf team at Milton High School, which had featured future PGA Tour members Heath Slocum and Boo Weekley just before he attended. Watson played golf for Faulkner State Community College in nearby Baldwin County, Alabama, where he was a junior college All-American. He transferred to the University of Georgia, the defending NCAA champions, and played for the Bulldogs in 2000 and 2001. As a junior, Watson helped lead the Bulldogs to the SEC title in 2000.\n\nWatson turned professional in 2001 and joined the Nationwide Tour where he played until 2005. He finished 21st on the Nationwide Tour's money list in 2005, making him the last player to qualify for the following year's PGA Tour. As a rookie in 2006, he earned $1,019,264 (90th overall) and led the PGA Tour in driving distance at . His longest drive in professional competition was on the PGA Tour at the WGC-Bridgestone Invitational.\n\nWatson played well at the 2007 U.S. Open. He was in the final group on Saturday after shooting rounds of 70-71 (+1) at Oakmont Country Club near Pittsburgh. Watson was one stroke off the lead after 36 holes but then slipped, shooting 74 (+4) in both the third and fourth rounds; he finished in a tie for fifth.\n\nWatson claimed his first PGA Tour win on June 27, 2010, in Cromwell, Connecticut, at the Travelers Championship on the second hole of a sudden-death playoff with Corey Pavin and Scott Verplank. Watson tearfully dedicated the win to his parents, specifically his father who was battling cancer.\n\nWatson was runner-up to Martin Kaymer at the PGA Championship at Whistling Straits, falling in the three-hole aggregate playoff that initially included Dustin Johnson, before he incurred a two-stroke penalty on the 72nd hole. Watson led the playoff after a birdie on the first hole, but Kaymer birdied the par-3 second hole to tie, effectively turning the playoff into sudden-death. Watson's second shot found the water hazard and Kaymer bested him by a stroke to win the major championship.\n\nWatson had his own clothing line called \"Bubba Golf\" at the former Steve & Barry's. He was invited on \"The Ellen DeGeneres Show\" after he sent her a video of a golf trick shot he completed for her birthday.\n\nOn January 30, 2011, Watson won his second PGA Tour event, the Farmers Insurance Open, finishing one stroke ahead of runner-up Phil Mickelson. Watson picked up his second win of the 2011 season and third career PGA Tour title on May 1 when he defeated Webb Simpson at the second playoff hole at the Zurich Classic of New Orleans. Both players birdied the first playoff hole, with Watson holing a 12-footer; he birdied the next hole to win the tournament.\n\nIn July 2011, Watson provoked controversy by criticizing the Alstom Open de France on the European Tour, in which he was playing under a sponsor's exemption. He indicated after his first round that he would not be playing any further events on the European Tour, and complained after his second round about security and organization at the tournament.\n\nWatson took part in the Long Drive Contest for charity at the Hyundai Tournament of Champions alongside Dustin Johnson and Robert Garrigus. He finished in second place, with a longest drive of behind a drive of over by Jamie Sadlowski.\n\nWatson began the year with three top-5 finishes in seven events, including finishing second at the WGC-Cadillac Championship.\n\nWatson's first major championship win came at the Masters. He began the final round at six-under-par, three strokes off the lead, held by Peter Hanson. On the back nine, Watson bogeyed the par-3 12th hole to return to even par for the round. He then recorded four consecutive birdies for a round of 68 (-4) and tied for the 72-hole lead with fourth-round playing partner Louis Oosthuizen at ten-under-par. In the sudden-death playoff, Oosthuizen and Watson both made par on the uphill 18th hole. On the next hole, the downhill 10th, both drove their tee shots towards the woods to the right of the hole. Oosthuizen's landed in the rough away, while Watson's ball landed deep in the woods on pine straw, from the pin without a clear shot to the green. Watson executed a miraculous recovery shot with 40 yards of hook on his 52-degree gap wedge and stopped the ball within fifteen feet of the hole. Oosthuizen's approach shot landed short of the green, but he chipped past the hole and narrowly missed his lengthy putt for par. Watson trickled his birdie putt a foot past the hole, took his time on the very short par putt, then made it for the emotional victory. The win took him to a world ranking of four, a career-high at the time.\n\nFollowing his Masters win, Watson began to struggle. He missed the cut at the Memorial Tournament and the U.S. Open. A week after the U.S. Open, he finished tied for second at the Travelers Championship. A month later, he played Open Championship, the third major of the year. While shooting a first round of −3 to tie him at third place, he never advanced much after that, finishing tied for 23rd place. In the final major of the year, the PGA Championship, Watson tied for 11th. He finished the year with one win, six top-5 finishes, seven top-10 finishes and three missed cuts.\n\nWatson began the season playing the Hyundai Tournament of Champions, where he finished tied for fourth place, and reached the quarterfinals in the WGC-Accenture Match Play Championship. In the subsequent World Golf Championship event, the WGC-Cadillac Championship, he began very strong with rounds of 66 and 69, but finished with rounds of 71 and 75 and tied for 18th place.\n\nAfter finishing tied 14th place in the Arnold Palmer Invitational, he returned to the Masters as the defending champion. Never in contention in the tournament, he finished 50th after a final round of 77. Watson tied for 37th at The Players Championship. On the second major of the year, the U.S. Open, he finished with a solid 71 in the first round, just four shots off the lead, but a second round score of 76 left him out of contention, and he tied for 32nd. At the Travelers Championship he took the lead after a second round of 67, but in the final round, leading by one with three to play, he triple-bogeyed the par-3 16th and finished two shots back in 4th place.\n\nTying for 30th at the Greenbrier Classic, he then played in the third major of the year, The Open Championship. After two solid rounds of 70 and 73, he shot 77 in the third round, and tied for 32nd.\n\nAt the Waste Management Phoenix Open, Watson held the lead for most of the tournament, but he finished runner-up to Kevin Stadler.\nWatson earned his fifth career PGA Tour victory—and his first since the 2012 Masters—at the 2014 Northern Trust Open at Riviera Country Club. He shot back-to-back 64s over the weekend to defeat runner-up Dustin Johnson by two strokes. The victory raised him to 14 in the Official World Golf Ranking. He followed that win with two more strong finishes—a ninth-place tie in the WGC-Accenture Match Play Championship and a second-place tie in the WGC-Cadillac Championship. Those performances elevated him from 14 to 12 in the world ranking.\n\nWatson won the Masters by three shots, with a score of 280 (−8). He entered the final round tied for the lead with 20-year-old Masters rookie Jordan Spieth. Playing together in the final pairing, Spieth birdied the seventh hole for a two-stroke lead over Watson. However, the momentum turned on the par-5 eighth hole. Spieth had a birdie putt, but ended up three-putting for bogey while Watson birdied to pull into a first-place tie. Then, on the ninth hole, Watson birdied again while Spieth bogeyed, and the four-shot swing over two holes gave Watson a lead that he never relinquished in a win over Spieth and Jonas Blixt. With the win, Watson became the 17th player to win the Masters two or more times. The win moved him again to number four in the Official World Golf Ranking.\n\nWatson won the Travelers Championship to move him to third in the Official World Golf Ranking. He garnered his second victory in 2015 by winning the unofficial Hero World Challenge in the Bahamas, besting fellow American Patrick Reed by three strokes.\n\nPrior to the Waste Management Phoenix Open in early February, Watson caused a bit of controversy after publicly admitting his dislike for the course, TPC Scottsdale. As a result, he was jeered by fans for the majority of the tournament, later criticizing the media for \"turning his words around\". Two weeks later though, he returned to the winner's circle after winning the Northern Trust Open at Riviera for a second time in three years, seeing off the challenge of Adam Scott and Jason Kokrak to win by one shot on 15-under-par.\n\nWatson did not chalk a win during the 2017 season, and missed the cut at three of the year's four majors (his only cut a T27 at The Open Championship). He had five top-10 finishes with more than $1.3 million in tour earnings.\n\nThe 2018 season started with a T7 at the QBE Shootout in December 2017 marking the best of his first six starts. Watson returned to the winner's circle with a 12-under finish at the Genesis Open in February, his third victory at this tournament (2014, 2016), all at Riviera. His trifecta at the Genesis (previously known as the Los Angeles Open, Northern Trust Open, and Nissan Open) makes him only the fifth to win this long-standing event at least three times, along with Ben Hogan, Arnold Palmer, Lloyd Mangrum, and Macdonald Smith. On March 25, he gained his eleventh tour win at the WGC-Dell Match Play event in Austin, Texas, with a winner's share of $1.7 million. On June 24, 2018, he won again at the Travelers Championship winning $1.26 million at TPC River Highlands. This was his third career victory at the Travelers tournament (2010, 2015).\n\nIn September 2018, Watson qualified for the U.S. team participating in the 2018 Ryder Cup. Europe defeated the U.S. team 17 1/2 to 10 1/2. Watson went 1–2–0. He lost his singles match against Henrik Stenson.\n\nWatson was nicknamed by his father after the former professional American football player Bubba Smith. Watson is married to Angela \"Angie\" Watson (née Ball), a Canadian whom he met at Georgia while he was on the golf team and she was on the women's basketball team. They were married in September 2004. In 2009, she was diagnosed with an enlarged pituitary gland, which accounts for her height.\n\nUnable to have a child naturally, various family issues, including the illness and death of Watson's father in 2010, kept them from attempting to adopt until 2011–12. In March 2012, one week after a potential adoption fell through at the last moment, Watson and his wife adopted a one-month-old baby boy named Caleb. In late 2014 the Watsons adopted a baby girl.\n\nWatson's father, Gerry Sr., died on October 15, 2010 of throat cancer. His mother is Molly Marie Watson and he has a sister, Melinda Watson Conner.\n\nWatson is also a member of the \"Golf Boys\", a boy band consisting of Watson, Ben Crane, Rickie Fowler, and Hunter Mahan. Their single \"Oh Oh Oh\" is currently on YouTube. The video was produced by Farmers Insurance Group. Farmers donates $1000 to charity for every 100,000 views the video gets.\n\nIn 2011, he made a humorous appearance in the song \"Michael Jackson\" by Christian hip hop artist Andy Mineo on the album \"Formerly Known\". He was featured in the song \"Ima Just Do It\" by KB, another Christian hip hop artist, on the album \"Tomorrow We Live\". His prototype Golf Cart Hovercraft, the BW1, YouTube video has earned more than 8 million views.\n\nWatson is a committed Christian who speaks openly about the importance of faith in his life. Watson devotes much of his money and time to charity. \n\nWatson purchased the mansion in the Isleworth community of Windermere, Florida, that was previously owned by Tiger Woods. In 2013, he was added to the list of Great Floridians by Governor Rick Scott.\n\nWatson purchased a General Lee car from the television series \"The Dukes of Hazzard\" at auction for $110,000 in 2012. Following the Charleston church shooting in June 2015, display of the Confederate flag - which is featured on the car's roof - became the subject of renewed controversy. Watson responded by saying he would paint over the flag with the American flag.\n\nIn 2015, Watson moved to Pensacola, where he has become very involved in the community. Among other ventures, Watson opened an ice cream store, purchased a part ownership in the Pensacola Blue Wahoos Minor League Baseball team. and purchased a Chevrolet dealership in nearby Milton, Florida. Watson has made considerable donations to Studer Family Children’s Hospital in Pensacola. He recently announced that he plans to run for mayor of Pensacola at some future date.\n\nPGA Tour playoff record (5–1)\n\n Defeated Louis Oosthuizen in a sudden-death playoff: Watson (4-4) and Oosthuizen (4-5).\n\nCUT = missed the half-way cut<br>\nT = tied\n\n\n\"Results not in chronological order prior to 2015.\"\n\nQF, R16, R32, R64 = Round in which player lost in match play<br>\n\"T\" = tied\n\n<nowiki>*</nowiki> \"As of the 2018 season\"\n\nProfessional\n\n\nSource:\n\n\n"}
{"id": "2130647", "url": "https://en.wikipedia.org/wiki?curid=2130647", "title": "Caspar Phair", "text": "Caspar Phair\n\nCaspar Phair (died 1933) was one of the early settlers of Lillooet, British Columbia, Canada, arriving about 1877 to take up the role of the village's school teacher.. He emigrated from Ireland. Caspar Phair became Lillooet's Government Agent, a position which at one time encompassed a wide-ranging assemblage of duties. In time he assumed the roles of magistrate, chief constable, coroner, fire chief, and game warden. His lasting mark was made in business as a merchant-launching the family's general store on a 'run' that would extend over 50 years.\n\nIn 1879 he married Cerise Eyre, daughter of Maria Josephine Martley by a previous marriage. Cerise Armit Eyre and her sister Mary Eyre had remained in England with their grandparent's when the Martley's travelled to the new colony of British Columbia in 1861. Maria's daughter's by Eyre joined the family in 1871. The wedding took place on The Grange,the Martley ranching property near Pavilion. Cerise's sister Mary would marry Henry Cornwall of Ashcroft Ranch.\n\nIn the 1890s the Phairs had a residence that they named Longford House, built in Lillooet. This elegant building is said to be partially modeled on and named after Mrs Phair's home in Eyrecourt, of County Galway in Ireland. Longford survives to this day as the village's only significant example of heritage-quality domestic architecture.\n\nCaspar Phair's son, A.W.A. (Artie) Phair, was a coroner as well as a noted chronicler and photographer of Lillooet's history.\n\nCaspar and Cerise Phair died in 1933, leaving their house to Harold Phair, Artie's son. Longford House was later bought by Dr. Masajiro Miyazaki and is now called the Miyazaki House.\n\n\n"}
{"id": "24502933", "url": "https://en.wikipedia.org/wiki?curid=24502933", "title": "CoSMoS", "text": "CoSMoS\n\nCoSMoS was a UK funded research project seeking to build capacity in generic modelling tools and simulation techniques for complex systems. Its acronym stands for Complex Systems Modelling and Simulation.\nThis was a four-year project, running from 2007 to 2011 as a collaboration between the University of York and Kent, with further collaborations from the University of Abertay Dundee and Bristol Robotics Laboratory.\n\nCollaboration between the universities of York and Kent started with the TUNA project, a feasibility study looking at the requirements for developing networks of nanites that behave safely. This project involved researchers from York, Kent and Surrey. TUNA's outcomes were both conceptual and practical. An example of theoretical contribution consists of investigators showing that emergent properties do not refine\n, but gave insights into the engineering of emergence.\nPractical results were obtained through the use of a blood clotting case-study. This led to the development of a process-oriented, large-scale simulation implemented in occam-pi.\n"}
{"id": "25839999", "url": "https://en.wikipedia.org/wiki?curid=25839999", "title": "Computer-automated design", "text": "Computer-automated design\n\nDesign Automation usually refers to electronic design automation, or Design Automation which is a Product Configurator. Extending Computer-Aided Design (CAD), automated design and Computer-Automated Design (CAutoD) are more concerned with a broader range of applications, such as automotive engineering, civil engineering, composite material design, control engineering, dynamic system identification, financial systems, industrial equipment, mechatronic systems, steel construction, structural optimisation, and the invention of novel systems.\n\nThe concept of CAutoD perhaps first appeared in 1963, in the IBM Journal of Research and Development, where a computer program was written.\n\nMore recently, traditional CAD simulation is seen to be transformed to CAutoD by biologically-inspired machine learning or search techniques such as evolutionary computation, including swarm intelligence algorithms.\n\nTo meet the ever-growing demand of quality and competitiveness, iterative physical prototyping is now often replaced by 'digital prototyping' of a 'good design', which aims to meet multiple objectives such as maximised output, energy efficiency, highest speed and cost-effectiveness. The design problem concerns both finding the best design within a known range (i.e., through 'learning' or 'optimisation') and finding a new and better design beyond the existing ones (i.e., through creation and invention). This is equivalent to a search problem in an almost certainly, multidimensional (multivariate), multi-modal space with a single (or weighted) objective or multiple objectives.\n\nUsing single-objective CAutoD as an example, if the objective function, either as a cost function formula_1, or inversely, as a fitness function formula_2, where\n\nis differentiable under practical constraints in the multidimensional space, the design problem may be solved analytically. Finding the parameter sets that result in a zero first-order derivative and that satisfy the second-order derivative conditions would reveal all local optima. Then comparing the values of the performance index of all the local optima, together with those of all boundary parameter sets, would lead to the global optimum, whose corresponding 'parameter' set will thus represent the best design. However, in practice, the optimization usually involves multiple objectives and the matters involving derivatives are lot more complex.\n\nIn practice, the objective value may be noisy or even non-numerical, and hence its gradient information may be unreliable or unavailable. This is particularly true when the problem is multi-objective. At present, many designs and refinements are mainly made through a manual trial-and-error process with the help of a CAD simulation package. Usually, such \"a posteriori\" learning or adjustments need to be repeated many times until a ‘satisfactory’ or ‘optimal’ design emerges.\n\nIn theory, this adjustment process can be automated by computerised search, such as exhaustive search. As this is an exponential algorithm, it may not deliver solutions in practice within a limited period of time.\n\nOne approach to virtual engineering and automated design is evolutionary computation such as evolutionary algorithms.\n\nTo reduce the search time, the biologically-inspired evolutionary algorithm (EA) can be used instead, which is a (non-deterministic) polynomial algorithm. The EA based multi-objective \"search team\" can be interfaced with an existing CAD simulation package in a batch mode. The EA encodes the design parameters (encoding being necessary if some parameters are non-numerical) to refine multiple candidates through parallel and interactive search. In the search process, 'selection' is performed using 'survival of the fittest' \"a posteriori\" learning. To obtain the next 'generation' of possible solutions, some parameter values are exchanged between two candidates (by an operation called 'crossover') and new values introduced (by an operation called 'mutation'). This way, the evolutionary technique makes use of past trial information in a similarly intelligent manner to the human designer.\n\nThe EA based optimal designs can start from the designer's existing design database or from an initial generation of candidate designs obtained randomly. A number of finally evolved top-performing candidates will represent several automatically optimized digital prototypes.\n\nThere are websites that demonstrate interactive evolutionary algorithms for design. EndlessForms.com allows you to evolve 3D objects online and have them 3D printed. PicBreeder.org allows you to do the same for 2D images.\n\n\n"}
{"id": "29144200", "url": "https://en.wikipedia.org/wiki?curid=29144200", "title": "Dana Ulery", "text": "Dana Ulery\n\nDana Ulery (born January 2, 1938) is an American computer scientist and pioneer in scientific computing applications. She began her career in 1961 as the first woman engineer at the NASA Jet Propulsion Laboratory (Pasadena, CA), designing and developing algorithms to model NASA’s Deep Space Network capabilities and automating real-time tracking systems for the Ranger and Mariner space missions using a North American Aviation Recomp II, 40-bit word size computer. Over the course of her career, she has held positions as an applied science and technology researcher and manager in industry, academia, and government. In 2007, she retired from her position as Chief Scientist of the Computational and Information Sciences Directorate at the United States Army Research Laboratory(ARL).\n\nUlery was among the first group of female managers at the US Army Research Laboratory. In these positions, she was also appointed Chair of the US Army Materiel Command Knowledge Management Council, and in 2002 was awarded the Army Knowledge Award for Best Transformation Initiative. She is listed in American Men and Women of Science, Who’s Who of American Women, Who’s Who in the East, Who’s Who in the World, and Who’s Who in America. She was named a Lifetime Achiever by Marquis Who's Who in 2017.\n\nUlery received her BA from Grinnell College in 1959, with a double major in English Literature and Mathematics. She earned her MS and PhD in Computer Science from the University of Delaware, in 1972 and 1975 respectively. In 1976, she accepted visiting faculty appointments at Cairo University in Egypt and the American University in Cairo. On her return to the U.S., she joined the Engineering Services Division of the DuPont Company, where she worked as a computer scientist and technical manager. In the early 1980s, Ulery led initiatives to develop and deploy enterprise application systems to evaluate and control product quality at DuPont sites. For these achievements she was awarded the DuPont Engineering Award for Leadership of Corporate Quality Computer Systems. Ulery also played an active role in establishment of EDI standards, international standards for electronically exchanging technical information used by business and government. She initiated and led multidisciplinary programs at ARL to advance research in multi-source information fusion and situational understanding applied to non-traditional battle environments and homeland defense.\n\nIn the 1990s, Ulery served for many years as Pan American Delegate to the United Nations Electronic Data Interchange for Administration, Commerce, and Trade (UN/EDIFACT). She was Chair of the UN/EDIFACT Multimedia Objects Working Group and Chair of the UN/EDIFACT Product Data Working Group, leading early international development of standards for electronic commerce.\n\n\n"}
{"id": "1730456", "url": "https://en.wikipedia.org/wiki?curid=1730456", "title": "Dictyate", "text": "Dictyate\n\nThe dictyate or dictyotene is a prolonged resting phase in oogenesis. It occurs in the stage of meiotic prophase I in ootidogenesis. It starts late in fetal life and is terminated shortly before ovulation by the LH surge. Thus, although the majority of oocytes are produced in female fetuses before birth, these pre-eggs remain arrested in the dictyate stage until puberty commences and the cells complete ootidogenesis.\n\nIn both mouse and human, oocyte DNA of older individuals has substantially more double-strand breaks than that of younger individuals.\n\nThe dictyate appears to be an adaptation for efficiently removing damages in germ line DNA by homologous recombinational repair.\n\nThere are a lot of mRNAs that have been transcribed but not translated during dictyate. Shortly before ovulation, the oocyte of interest activates these mRNA strains.\n\nTranslation of mRNA in dictyate is partly explained by molecules binding to sites on the mRNA strain, which results in that initiation factors of translation can not bind to that site. Two such molecules, that impedes initiation factors, are CPEB and \"maskin\", which bind to CPE (cytoplasmic polyadenylation element). When these two molecules remain together, then maskin binds the initiation factor eIF-4E, and thus eIF4E can no longer interact with the other initiation factors and no translation occurs. On the other hand, dissolution of the CPEB/maskin complex leads to eIF-4E binding to the initiation factor eIF-4G, and thus translation starts, which contributes to the end of dictyate and further maturation of the oocyte.\n\n"}
{"id": "56180412", "url": "https://en.wikipedia.org/wiki?curid=56180412", "title": "Diego Alvarez (theologian)", "text": "Diego Alvarez (theologian)\n\nDiego Álvarez ( ; c. 1555 – 1632) was a Spanish theologian who opposed the Molinism. He was archbishop of Trani from 1607 to his death.\n\nDiego Álvarez was born at Medina de Rioseco, Old Castile, about 1555. He entered the Dominican Order in his native city, and taught theology for twenty years in the Spanish cities of Burgos, Trianos, Plasencia, and Valladolid, and for ten years (1596-1606) at the Dominican convent of Santa Maria sopra Minerva, in Rome. From 1603 to 1606 he was elected Regent of the \"Collegium Divi Thomae\" of the Dominicans in Rome.\n\nShortly after his arrival in Rome (7 November, 1596) he presented to Pope Clement VIII a memorial requesting him to examine the work \"Concordia liberi Arbitrii\", by Luis de Molina, S.J., which, upon it publication in 1588, had given rise to bitter controversy, known as Molinism, on the extent of knowledge of God in the Divine providence. Before the Congregation (Congregatio de Auxiliis), appointed by the Pope to settle the dispute, he defended the Thomistic doctrines of grace, predestination, etc., alone for three years, and, thereafter, conjointly with his confrere Tomas de Lemos, to whom he gave the first place, until the suspension of the Congregation (1606).\n\nHe was appointed, 19 March, 1606, by Pope Paul V, to the Archbishopric of Trani, The episcopal consecration followed on 1 April in the Basilica of Santa Maria sopra Minerva by the hands of Girolamo Bernerio. He passed the remainder of his life in Trani where he died on 10 May 1632. He was buried in that cathedral.\n\nBesides a commentary on Isaiah, and a manual for preachers, he published: \"De auxiliis divinæ gratiæ et humani arbitrii viribus et libertate, ac legitimâ ejus cum efficaciâ eorumdem auxiliorum concordiâ libri XII\" (Rome, 1610; Lyons, 1620; Douai, 1635); \"Responsionum ad objectiones adversus concordiam liberi arbitrii cum divinâ, præscientiâ, providentiâ, et prædestinatione, atque cum efficaciâ prævenientis gratiæ, prout a S. Thomâ et Thomistis defenditur et explicatur, Libri IV\" (Trani, 1622; Lyons, 1622); \"De origine Pelagianæ hæresis et ejus progressu et damnatione per plures summos pontifices et concilia factâ Historia ex annalibus Card. Baronii et aliis probatis auctoribus collecta\" (Trani, 1629); \"Responsionum liber ultimus hoc titulo: Opus præclarum nunquam hâctenus editum, in quo argumentis validissimis concordia liberi arbitrii cum divinâ præscientiâ, prædestinatione, et efficaciâ gratiæ prævenientis ad mentem S. Thomæ et omnium defenditur et explicatur\" (Douai, 1635); \"Operis de auxiliis divinæ gratiæ et humani arbitrii viribus et libertate, ac legitimâ ejus cum efficaciæ eorumdem auxiliorum concordiâ summa, in IV libros distincta\" (Lyons, 1620; Cologne, 1621; Trani, 1625); \"De incarnatione divini verbi disputationes LXXX; in quibus explicantur et defenduntur, quæ in tertiâ parte summæ theologicæ docet S. Thomas a Q. 1 ad 24\" (Lyons, 1614; Rome, 1615; Cologne, 1622); \"Disputationes theologicæ in primam secundæ S. Thomæ, in quibus præcipua omina quæ adversus doctrinam ejusdem et communem Thomistarum a diversis auctoribus impugnantur, juxta legitimum sensum præceptoris angelici explicantur et defenduntur\" (Trani, 1617; Cologne, 1621).\n"}
{"id": "3779468", "url": "https://en.wikipedia.org/wiki?curid=3779468", "title": "Electrocorticography", "text": "Electrocorticography\n\nElectrocorticography (ECoG), or intracranial electroencephalography (iEEG), is a type of electrophysiological monitoring that uses electrodes placed directly on the exposed surface of the brain to record electrical activity from the cerebral cortex. In contrast, conventional electroencephalography (EEG) electrodes monitor this activity from outside the skull. ECoG may be performed either in the operating room during surgery (intraoperative ECoG) or outside of surgery (extraoperative ECoG). Because a craniotomy (a surgical incision into the skull) is required to implant the electrode grid, ECoG is an invasive procedure.\n\nECoG was pioneered in the early 1950s by Wilder Penfield and Herbert Jasper, neurosurgeons at the Montreal Neurological Institute. The two developed ECoG as part of their groundbreaking Montreal procedure, a surgical protocol used to treat patients with severe epilepsy. The cortical potentials recorded by ECoG were used to identify epileptogenic zones – regions of the cortex that generate epileptic seizures. These zones would then be surgically removed from the cortex during resectioning, thus destroying the brain tissue where epileptic seizures had originated. Penfield and Jasper also used electrical stimulation during ECoG recordings in patients undergoing epilepsy surgery under local anesthesia. This procedure was used to explore the functional anatomy of the brain, mapping speech areas and identifying the somatosensory and somatomotor cortex areas to be excluded from surgical removal.\nA doctor named Robert Galbraith Heath was also an early researcher of the brain at the Tulane University School of Medicine.\n\nECoG signals are composed of synchronized postsynaptic potentials (local field potentials), recorded directly from the exposed surface of the cortex. The potentials occur primarily in cortical pyramidal cells, and thus must be conducted through several layers of the cerebral cortex, cerebrospinal fluid (CSF), pia mater, and arachnoid mater before reaching subdural recording electrodes placed just below the dura mater (outer cranial membrane). However, to reach the scalp electrodes of a conventional electroencephalogram (EEG), electrical signals must also be conducted through the skull, where potentials rapidly attenuate due to the low conductivity of bone. For this reason, the spatial resolution of ECoG is much higher than EEG, a critical imaging advantage for presurgical planning. ECoG offers a temporal resolution of approximately 5 ms and a spatial resolution of 1 cm.\n\nUsing depth electrodes, the local field potential gives a measure of a neural population in a sphere with a radius of 0.5–3 mm around the tip of the electrode. With a sufficiently high sampling rate (more than about 10 kHz), depth electrodes can also measure action potentials. In which case the spatial resolution is down to individual neurons, and the field of view of an individual electrode is approximately 0.05-0.35 mm.\n\nThe ECoG recording is performed from electrodes placed on the exposed cortex. In order to access the cortex, a surgeon must first perform a craniotomy, removing a part of the skull to expose the brain surface. This procedure may be performed either under general anesthesia or under local anesthesia if patient interaction is required for functional cortical mapping. Electrodes are then surgically implanted on the surface of the cortex, with placement guided by the results of preoperative EEG and magnetic resonance imaging (MRI). Electrodes may either be placed outside the dura mater (epidural) or under the dura mater (subdural). ECoG electrode arrays typically consist of sixteen sterile, disposable stainless steel, carbon tip, platinum, Platinum-iridium alloy or gold ball electrodes, each mounted on a ball and socket joint for ease in positioning. These electrodes are attached to an overlying frame in a “crown” or “halo” configuration. Subdural strip and grid electrodes are also widely used in various dimensions, having anywhere from 4 to 256 electrode contacts. The grids are transparent, flexible, and numbered at each electrode contact. Standard spacing between grid electrodes is 1 cm; individual electrodes are typically 5 mm in diameter. The electrodes sit lightly on the cortical surface, and are designed with enough flexibility to ensure that normal movements of the brain do not cause injury. A key advantage of strip and grid electrode arrays is that they may be slid underneath the dura mater into cortical regions not exposed by the craniotomy. Strip electrodes and crown arrays may be used in any combination desired. Depth electrodes may also be used to record activity from deeper structures such as the hippocampus.\n\nDirect cortical electrical stimulation (DCES), also known as cortical stimulation mapping, is frequently performed in concurrence with ECoG recording for functional mapping of the cortex and identification of critical cortical structures. When using a crown configuration, a handheld wand bipolar stimulator may be used at any location along the electrode array. However, when using a subdural strip, stimulation must be applied between pairs of adjacent electrodes due to the nonconductive material connecting the electrodes on the grid. Electrical stimulating currents applied to the cortex are relatively low, between 2 and 4 mA for somatosensory stimulation, and near 15 mA for cognitive stimulation.\nThe functions most commonly mapped through DCES are primary motor, primary sensory, and language. The patient must be alert and interactive for mapping procedures, though patient involvement varies with each mapping procedure. Language mapping may involve naming, reading aloud, repetition, and oral comprehension; somatosensory mapping requires that the patient describe sensations experienced across the face and extremities as the surgeon stimulates different cortical regions.\n\nSince its development in the 1950s, ECoG has been used to localize epileptogenic zones during presurgical planning, map out cortical functions, and to predict the success of epileptic surgical resectioning. ECoG offers several advantages over alternative diagnostic modalities:\n\n\nLimitations of ECoG include:\n\n\nEpilepsy is currently ranked as the third most commonly diagnosed neurological disorder, afflicting approximately 2.5 million people in the United States alone. Epileptic seizures are chronic and unrelated to any immediately treatable causes, such as toxins or infectious diseases, and may vary widely based on etiology, clinical symptoms, and site of origin within the brain. For patients with intractable epilepsy – epilepsy that is unresponsive to anticonvulsants – surgical treatment may be a viable treatment option.\n\n\nBefore a patient can be identified as a candidate for resectioning surgery, MRI must be performed to demonstrate the presence of a structural lesion within the cortex, supported by EEG evidence of epileptogenic tissue. Once a lesion has been identified, ECoG may be performed to determine the location and extent of the lesion and surrounding irritative region. The scalp EEG, while a valuable diagnostic tool, lacks the precision necessary to localize the epileptogenic region. ECoG is considered to be the gold standard for assessing neuronal activity in patients with epilepsy, and is widely used for presurgical planning to guide surgical resection of the lesion and epileptogenic zone. The success of the surgery depends on accurate localization and removal of the epileptogenic zone. ECoG data is assessed with regard to ictal spike activity – “diffuse fast wave activity” recorded during a seizure – and interictal epileptiform activity (IEA), brief bursts of neuronal activity recorded between epileptic events. ECoG is also performed following the resectioning surgery to detect any remaining epileptiform activity, and to determine the success of the surgery. Residual spikes on the ECoG, unaltered by the resection, indicate poor seizure control, and incomplete neutralization of the epileptogenic cortical zone. Additional surgery may be necessary to completely eradicate seizure activity. Extraoperative ECoG is also used to localize functionally-important areas (also known as eloquent cortex) to be preserved during epilepsy surgery.\n\n\nThe objective of the resectioning surgery is to remove the epileptogenic tissue without causing unacceptable neurological consequences. In addition to identifying and localizing the extent of epileptogenic zones, ECoG used in conjunction with DCES is also a valuable tool for functional cortical mapping. It is vital to precisely localize critical brain structures, identifying which regions the surgeon must spare during resectioning (the “eloquent cortex”) in order to preserve sensory processing, motor coordination, and speech. Functional mapping requires that the patient be able to interact with the surgeon, and thus is performed under local rather than general anesthesia. Electrical stimulation using cortical and acute depth electrodes is used to probe distinct regions of the cortex in order to identify centers of speech, somatosensory integration, and somatomotor processing. During the resectioning surgery, intraoperative ECoG may also be performed to monitor the epileptic activity of the tissue and ensure that the entire epileptogenic zone is resectioned.\nAlthough the use of extraoperative and intraoperative ECoG in resectioning surgery has been an accepted clinical practice for several decades, recent studies have shown that the usefulness of this technique may vary based on the type of epilepsy a patient exhibits. Kuruvilla and Flink reported that while intraoperative ECoG plays a critical role in tailored temporal lobectomies, in multiple subpial transections (MST), and in the removal of malformations of cortical development (MCDs), it has been found impractical in standard resection of medial temporal lobe epilepsy (TLE) with MRI evidence of mesial temporal sclerosis (MTS). A study performed by Wennberg, Quesney, and Rasmussen demonstrated the presurgical significance of ECoG in frontal lobe epilepsy (FLE) cases.\n\nECoG has recently emerged as a promising recording technique for use in brain-computer interfaces (BCI). BCIs are direct neural interfaces that provide control of prosthetic, electronic, or communication devices via direct use of the individual’s brain signals. Brain signals may be recorded either invasively, with recording devices implanted directly into the cortex, or noninvasively, using EEG scalp electrodes. ECoG serves to provide a partially invasive compromise between the two modalities – while ECoG does not penetrate the blood–brain barrier like invasive recording devices, it features a higher spatial resolution and higher signal-to-noise ratio than EEG.\n\nThe electrocorticogram is still considered to be the \"gold standard\" for defining epileptogenic zones; however, this procedure is risky and highly invasive. Recent studies have explored the development of a noninvasive cortical imaging technique for presurgical planning that may provide similar information and resolution of the invasive ECoG.\n\nIn one novel approach, Bin He et al. seek to integrate the information provided by a structural MRI and scalp EEG to provide a noninvasive alternative to ECoG. This study investigated a high-resolution subspace source localization approach, FINE (first principle vectors) to image the locations and estimate the extents of current sources from the scalp EEG. A thresholding technique was applied to the resulting tomography of subspace correlation values in order to identify epileptogenic sources. This method was tested in three pediatric patients with intractable epilepsy, with encouraging clinical results. Each patient was evaluated using structural MRI, long-term video EEG monitoring with scalp electrodes, and subsequently with subdural electrodes. The ECoG data were then recorded from implanted subdural electrode grids placed directly on the surface of the cortex. MRI and computed tomography images were also obtained for each subject.\n\nThe epileptogenic zones identified from preoperative EEG data were validated by observations from postoperative ECoG data in all three patients. These preliminary results suggest that it is possible to direct surgical planning and locate epileptogenic zones noninvasively using the described imaging and integrating methods. EEG findings were further validated by the surgical outcomes of all three patients. After surgical resectioning, two patients are seizure-free and the third has experienced a significant reduction in seizures. Due to its clinical success, FINE offers a promising alternative to preoperative ECoG, providing information about both the location and extent of epileptogenic sources through a noninvasive imaging procedure.\n\n\n"}
{"id": "41485673", "url": "https://en.wikipedia.org/wiki?curid=41485673", "title": "Eric Gaucher", "text": "Eric Gaucher\n\nEric Alexander Gaucher (born January 1972) is an American biologist best known for his work in a field he termed Evolutionary Synthetic Biology.\n\nGaucher was guided in biochemistry by Peter Tipton and Bayesian Theory by George Smith. Gaucher subsequently earned his Ph.D. from the University of Florida under the tutelage of Steve Benner and Michael Miyamoto. Gaucher received the Walter M. Fitch Award from the Society for Molecular Biology and Evolution for his graduate work. He then did postdoctoral work with NASA's Astrobiology Institute in conjunction with a National Research Council Fellowship. After the two-year fellowship, Gaucher served as President of the Foundation for Applied Molecular Evolution.\n\nGaucher was hired as an Associate Professor by the Georgia Institute of Technology in 2008 The Gaucher group conducts basic and applied research at the interface of molecular evolution and synthetic biology. As of February 2016, his \"h\"-index, as calculated by Google Scholar, is 25.\n\nGaucher is also the founder and president of the early-stage biotechnology company General Genomics. His company exploits novel platforms to engineer proteins for the biomedical and industrial sectors.\n\n\n"}
{"id": "1797819", "url": "https://en.wikipedia.org/wiki?curid=1797819", "title": "Francis Willoughby, 5th Baron Willoughby of Parham", "text": "Francis Willoughby, 5th Baron Willoughby of Parham\n\nFrancis Willoughby, 5th Baron Willoughby of Parham (baptised 1614 – 23 July 1666 O.S.) was an English peer of the House of Lords.\n\nHe succeeded to the title 14 October 1617 on the death in infancy of his elder brother Henry Willoughby, 4th Lord Willoughby of Parham. Francis Willoughby was second son of William Willoughby, 3rd Lord Willoughby of Parham The young and unexpected death of his elder brother Henry made Francis successor to the hereditary peerage and seat in the House of Lords, the upper house of Parliament. Francis Willoughby was an early supporter of the Parliamentarian cause during the English Civil War but later became a Royalist. He twice served as governor of English colonies in the Caribbean. Francis Willoughby died without male heirs of his body and the title passed to his younger brother William Willoughby, 6th Lord Willoughby of Parham, the third son of William Willoughby, 3rd Lord Willoughby of Parham.\n\nFrancis Willoughby was born in perhaps late 1613 (since he was baptised in 1614) to William Willoughby, 3rd Baron Willoughby of Parham and Frances Manners, daughter of John Manners, 4th Earl of Rutland of Nottingham at Parham in Suffolk. His father died in 1617, and the barony was held by his older brother Henry for one year until he too died barely five years old, at which point Francis inherited the seat in the House of Lords and with it the family title.\n\nOn 16 November 1628 Willoughby married Elizabeth Cecil (1606–1661), the daughter of the soldier Edward Cecil, 1st Viscount Wimbledon and Theodosia Noel.\n\nAs tensions between the king and Parliament grew in the 1630s, he found himself opposed to Charles I over the levying of ship money. His loyalty was further strained by the Bishops' Wars, in which he was reluctant to fight the Scots.\n\nWhen the king in 1642 issued his Commission of Array to form a loyal army, Willoughby rejected his summons and instead took command of a horse regiment under the Parliamentary commander, the Earl of Essex. By January 1643 he was made commander-in-chief of Lincolnshire.\n\nOn 16 July of the same year, he led his soldiers in a surprise attack on Gainsborough where he seized the town. Facing a counterattack, Willoughby's soldiers fought along with those under Oliver Cromwell to hold off an advancing Royalist force of superior strength. The main body of the Parliamentary army withdrew to Boston with only two dead.\n\nThat September, Willoughby was a subordinate commander under the Earl of Manchester and Cromwell. He fought at the Battle of Winceby and accepted the surrender of Bolingbroke Castle in November.\n\nWilloughby's relations with the Parliamentarians began to fray in 1644. In March he joined with Sir John Meldrum in the assault on Newark, the failure of which has been partially attributed to Willoughby's supposed unwillingness to take orders from Meldrum. Willoughby quarrelled with Manchester and was forced to make an apology to the House of Lords as a result. Furthermore, Cromwell himself saw fit to complain about the conduct of Willoughby's soldiers.\n\nIn the next few years, Willoughby became the leader of the Presbyterian force within Parliament, opposed the formation of the New Model Army and was elected as speaker of the House of Lords in July, 1647. However, when the Parliamentary army took London in September, Willoughby was imprisoned along with six other peers and held for four months at which point he was released without charge, fleeing to the Netherlands to join the Royalists.\n\nNow espousing the Royalist cause, Willoughby was promoted to Vice Admiral under the Duke of York, an appointment that may have been designed to engender sympathy among Scots and Presbyterians. He was also assigned a command in the 1648 invasion of England under the Prince of Wales. He later surrendered his naval command to Prince Rupert of the Rhine. When Parliament confiscated his estates, he travelled to the Caribbean.\n\nHe was appointed Governor of Barbados by Charles II. He arrived at Barbados and took up the appointment in May 1650 and attempted to negotiate the strained politics of that island, which also experienced a division between Royalists and Parliamentarians. During this time he also sent a small colonizing party to Suriname, which established Fort Willoughby (now Paramaribo) in honor of the governor.\n\nOn 25 October 1651, a seven ship force under Commodore George Ayscue arrived off Barbados, demanding that the island submit \"for the use of the Parliament of England\". Willoughby's reply (tellingly addressed to \"His Majesty's ship \"Rainbow\"\") was unyielding, declaring that he knew \"no supreme authority over Englishmen but the King\". With some 400 horsemen and 6,000 militia, he was prepared to resist any attempt at coercion.\n\nOver the next month Barbados was blockaded. In early December, with the Royalist cause defeated in England, Ayscue began a series of raids against fortifications on the island and was reinforced by a group of thirteen ships bound for Virginia. On 17 December a force of more than 1,000 Barbadian militia was defeated by one of Ayscue's detachments. Governor Willoughby attempted to stem the spread of Parliamentary sympathies by hanging two of the returning militia soldiers and prohibiting the reading of documents from the blockading fleet. The Royalists held out for several more weeks until one of Willoughby's own commanders declared himself for Parliament. A battle was averted by a week of rain, after which Willoughby, perhaps having seen the hopelessness of his cause, sought negotiations. He was replaced as governor but Barbados and the Royalists there were not punished. Additionally, Willoughby's properties in England were restored. He returned to them in August 1652.\n\nWhile he was twice imprisoned during The Protectorate for involvement in Royalist intrigues, Willoughby survived the Cromwell years and after the Restoration in 1660 he was again appointed to a governorship in the Caribbean, administering the colonies at Saint Kitts, Nevis, Montserrat, and Antigua.\n\nIn June 1664 he organized an expedition from Barbados against the small French garrison at Saint Lucia, expelling it under the pretext that a half-Carib native had effectively \"sold\" it to England and establishing a short-lived English colony there.\n\nDuring the Second Anglo-Dutch War, Willoughby engaged in another expedition, sailing to the Dutch island of Tobago in July 1665. Finding a force under the English privateer Robert Searle already looting the settlement there, he put a stop to the destruction and installed a garrison of fifty men to maintain order.\n\nHis last act on behalf of the English crown came in July 1666 when, having learned of the recent French seizure of Saint Kitts, he formed a relief force of two Royal Navy frigates, twelve other large vessels (including commandeered merchant ships), a fire ship, and a ketch, bearing over 1,000 men. He planned to proceed north to Nevis, Montserrat, and Antigua to gather further reinforcements before descending on the French. Leaving Barbados on 28 July, his force prowled off Martinique and then Guadeloupe, where he sent a frigate to assault the harbor and capture two merchant vessels on 4 August. This success could not be exploited however as that night most of his force was destroyed by a strong hurricane, including the flagship \"Hope\", from which Willoughby did not emerge.\n\nFrancis Willoughby and Elizabeth Cecil had four children:\n\nIn his will, he left extensive holdings in Barbados, Antigua, and Suriname to his children and his nephew Henry Willoughby as well as smaller grants of currency or sugar to various associates and servants.\n\n"}
{"id": "1315820", "url": "https://en.wikipedia.org/wiki?curid=1315820", "title": "Gonçalves Dias", "text": "Gonçalves Dias\n\nAntônio Gonçalves Dias (; August 10, 1823November 3, 1864) was a Brazilian Romantic poet, playwright, ethnographer, lawyer and linguist. A major exponent of Brazilian Romanticism and of the literary tradition known as \"Indianism\", he is famous for writing \"Canção do exílio\" (arguably the most well-known poem of Brazilian literature), the short narrative poem \"I-Juca-Pirama\", the unfinished epic \"Os Timbiras\", and many other nationalist and patriotic poems that would award him posthumously with the title of national poet of Brazil. He was also an avid researcher of Native Brazilian languages and folklore.\n\nHe is the patron of the 15th chair of the Brazilian Academy of Letters.\n\nAntônio Gonçalves Dias was born in Caxias on August 10, 1823, to a Portuguese father, João Manuel Gonçalves Dias and a \"cafuza\" mother, Vicência Ferreira. After completing his studies in Latin, French and Philosophy, he went in 1838 to Portugal to earn a degree in Law at the University of Coimbra. There he wrote his most remembered poem, \"Canção do exílio\". He graduated in 1845 and returned to Brazil in the same year. He went to Rio de Janeiro, living there until 1854. There he wrote for newspapers, and began to write the drama \"Leonor de Mendonça\" in 1846 and his first poetry book, \"Primeiros Cantos\", in 1847. It was very well-received, and Alexandre Herculano wrote an article praising it. Dias finished his play \"Leonor de Mendonça\" also in 1847, and tried to have it performed at the Conservatório de Música do Rio de Janeiro, but the play was not accepted.\n\nIn 1848, he wrote two more poetry books: \"Segundos Cantos\" and \"Sextilhas de Frei Antão\". In 1849 he became professor of Latin and History at the Colégio Pedro II. In 1851, he published his last poetry book, \"Últimos Cantos\". In the same year, he travelled to Northern Brazil, planning to marry 14-year-old Ana Amélia Ferreira do Vale, to whom he dedicated many of his most famous and beautiful love poems, such as \"Seus olhos\", \"Leviana\", \"Palinódia\" and \"Retratação\". Ana Amélia was the cousin of Alexandre Teófilo de Carvalho Leal, who in his turn was the brother of , a famous Brazilian journalist, writer, medician, biographer and historian known as the \"Plutarch of Cantanhede\". (Both Alexandre and Antônio were very close friends with Dias, and Antônio would edit Dias' posthumous works in 1875, in 6 volumes.) However, the girl's mother did not allow the marriage, quoting Dias' mestizo origins as a pretext. (This inspired his famous poem \"Ainda uma vez – adeus!\".) Returning to Rio, he married Olímpia Carolina da Costa later on, having with her a stillborn daughter. Dias divorced Olímpia in 1856.\n\nFrom 1854 to 1858, he went to Europe on special missions for the Secretary of Foreign Affairs, where he studied the state of public instruction in the educational institutions there. In 1856, in Leipzig, he published his three poetry books in a single volume entitled \"Cantos\", wrote the first four cantos of the epic poem \"Os Timbiras\" (that he would leave unfinished) and also published a dictionary of Old Tupi. Returning to Brazil in 1860, he founded the magazine \"Guanabara\" alongside Joaquim Manuel de Macedo and Manuel de Araújo Porto-Alegre in 1849, and went on expeditions to Negro and Madeira Rivers, as a member of the Scientific Commission of Exploration. In 1862 he returned to Rio de Janeiro, but shortly after went to Europe again. In October 1863 he went to Lisbon, where he translated Friedrich Schiller's \"The Bride of Messina\" and some poems by Heinrich Heine.\n\nAfter a short stay in France, he decided to return to Brazil in 1864, in the ship \"Ville de Boulogne\". However, the ship was wrecked on the Bay of Cumã, near the shores of Guimarães, Maranhão. All the passengers but Dias survived the tragedy; he was sleeping in his cabin belowdecks and did not wake up in time to see what was happening; thus he drowned.\n\nDias had a nephew who was also a poet, Teófilo Dias.\n\n\n\n\n\nThe city of Gonçalves Dias, founded in 1958, has this name because its territory formerly belonged to the city of Caxias, Dias' hometown. A river in Paraná is named after him, as well as many public squares and streets all over Brazil.\n\n\n"}
{"id": "899870", "url": "https://en.wikipedia.org/wiki?curid=899870", "title": "Hugo Claus", "text": "Hugo Claus\n\nHugo Maurice Julien Claus (; 5 April 1929 – 19 March 2008) was a leading Belgian author who published under his own name as well as various pseudonyms. Claus' literary contributions spanned the genres of drama, the novel, and poetry; he also left a legacy as a painter and film director. He wrote primarily in Dutch, although he also wrote some poetry in English.\n\nHis death by euthanasia, which is legal in Belgium, led to considerable controversy.\n\nHugo Claus was born on 5 April 1929 at Sint-Janshospitaal in Bruges, Belgium. He was the eldest of the sons born to Jozef (Joseph) Claus, a printer who had a passion for theater; his mother was Germaine Vanderlinden. Three more sons were born into the family within the subsequent decade: Guido (February 1931 – 9 November 1991), Odo (born January 1934), and Johan (November 1938 – 13 February 2009).\n\nEducated at a boarding school, the young Hugo Claus lived in Belgium during the German invasion of the country in World War II. Several of Claus' schoolteachers during the half-decade of the German occupation of the country were right-wing nationalists eager to support the pro-German government; his father was briefly held in custody for pro-German activities after the end of the occupation, and Hugo was himself swayed into supporting the pro-German Flemish fascist youth movement. Claus' experience with the wartime nationalist right would later become a source for his 1983 book \"The Sorrow of Belgium\", a novel which tells the story of Claus' alter ego Louis Seynaeve. A sympathizer of the political left at a more mature period in his life, Claus lauded the socialist model after a visit to Cuba in the 1960s.\n\nClaus' prominence in literary circles and his debut as a novelist came in 1950, with the publication of his \"De Metsiers\" at age twenty-one. His first published poems had in fact been printed by his father as early as 1947. He lived in Paris from 1950 until 1952, where he met many of the members of the CoBrA art movement.\n\nFrom February 1953 until the beginning of 1955, Hugo Claus lived in Italy where his girlfriend (born in 1928) acted in a few films. They were married on 26 May 1955, and had a son, Thomas, on 7 October 1963. In the early 1970s, he had an affair with actress Sylvia Kristel, who was 23 years younger, with whom he had a son, Arthur, in 1975. The relationship ended in 1977, when she left him for actor Ian McShane.\n\nHe was a \"contrarian\", of \"anarchist spirit\". Journalist Guy Duplat recalls that Claus had organized in Knokke the election of a \"Miss Knokke Festival\", which was a typical beauty contest, except for the Claus ruling that the members of the all-male jury would have to be naked.\n\nHugo Claus was considered to be one of the most important contemporary Belgian authors. Claus published the novel \"Schola Nostra\" (1971) under the pseudonym Dorothea Van Male. He also used the pseudonyms Jan Hyoens and Thea Streiner. The 1962 \"De verwondering\" (\"The Astonishment\") and the 1983 \"Het verdriet van België\" (\"The Sorrow of Belgium\") rank among Claus' most significant works as a novelist. Lee views \"Het verdriet van België\" as a postmodern critique of national identity.\n\nMost prolific in literary endeavors as a dramatist, Claus wrote 35 original pieces and 31 translations from English, Greek, Latin, French and Spanish plays and novels. His dramatic sketch \"Masscheroen\" was first staged at Knokke Casino and featured an all-nude cast: three naked men were given the task of portraying the Christian Holy Trinity of God the father, God the son, and the Holy Spirit; the work also made light of the Holy Virgin, a Belgian saint, and the Three Wise Men. Attacked as blasphemous and deleterious to the public's moral well-being, the light-hearted play's performance triggered a notable legal case in which Claus was prosecuted: convicted on charges of public indecency, Claus was ordered to pay a ten-thousand-Belgian franc fine and serve a four-month prison sentence. The prison term was reduced to a suspended sentence after a public outcry.\n\nClaus also wrote the script of a satirical comic strip, \"De Avonturen van Belgman\" (\"The Adventures of Belgian Man\") in 1967, which spoofed the Belgian bi-lingual troubles. The strip itself was drawn by artist \"Hugoké\" (Hugo de Kempeneer).\n\nHugo Claus' name had been put forward many times for the Nobel Prize in literature, on which he would casually comment \"this prize money would suit me fine\".\n\nAs a painter, Claus was a participant in the CoBrA art movement from 1950. He had developed friendships with some of its members, and illustrated a book by Pierre Alechinsky in 1949. He collaborated with key figures in the movement including Karel Appel and Corneille and participated in some exhibitions. He later used his experiences of this time in his book \"Een zachte vernieling\" (\"Mild Destruction\").\n\nClaus directed seven films between 1964 and 2001. His film \"Het sacrament\" was screened in the Un Certain Regard section at the 1990 Cannes Film Festival.\n\nClaus suffered from Alzheimer's disease and requested his life to be terminated through euthanasia, a legal procedure in Belgium, at the Middelheim Hospital in Antwerp on 19 March 2008.\n\nBert Anciaux, then Flemish Minister of Culture, stated \"I knew him well enough to know that he wanted to depart with pride and dignity.\" Former Belgian Prime Minister Guy Verhofstadt said that he imagined the onset of Alzheimer's must have been \"inevitable and unbearable torture\". \"I can live with the fact that he decided thus,\" he said, \"because he left us as a great glowing star, right on time, just before he would have collapsed into a Stellar black hole.\"\n\nHis death by euthanasia has received criticism from the Roman Catholic Church and the Belgian Alzheimer League. The Roman Catholic Church criticized the media coverage; Belgian Cardinal Godfried Danneels referred to Claus' euthanasia in his Easter Homily. The Belgian Alzheimer League respects Claus' decision, but believes the media coverage of his death neglects other options for Alzheimer's patients.\n\nAmongst others:\n\nClaus wrote over a thousand pages of poetry, more than sixty plays, over twenty novels and several essays, film scripts, libretti and translations. Only a small part of this œuvre has been translated into English:\n\n\n"}
{"id": "7791213", "url": "https://en.wikipedia.org/wiki?curid=7791213", "title": "Ilya Rabinovich", "text": "Ilya Rabinovich\n\nIn 1911 Ilya Rabinovich tied for first place with Platz in Saint Petersburg. In 1912 he tied for 4th-5th in Vilnius (Hauptturnier; Karel Hromádka won).\n\nIn July/August 1914 he played in Mannheim, Germany (19th DSB Congress), and tied for 2nd-3rd in interrupted \"Hauptturnier A\" (B. Hallegua won). After the declaration of war for World War I against Russia, eleven 'Russian' players (Alexander Alekhine, Efim Bogoljubow, Fedor Bogatyrchuk, Alexander Flamberg, N. Koppelman, Boris Maljutin, Rabinovich, Peter Romanovsky, Peter Petrovich Saburov, Alexey Selezniev, and Samuil Weinstein) from the Mannheim tournament were interned by Germany. In September 1914, four of them (Alekhine, Bogatyrchuk, Saburov, and Koppelman) were freed and allowed, through Switzerland, to return home. The Russian internees played eight tournaments, the first in Baden-Baden (1914) and all the others in Triberg im Schwarzwald (1914–1917). Rabinovich was 3rd in Baden-Baden (Alexander Flamberg won), took 2nd at Triberg 1914/15, took 2nd at Triberg 1915, took 3rd at Triberg 1915, tied for 2nd-3rd at Triberg 1915, took 2nd at Triberg 1915/16 (all tournaments were won by Efim Bogoljubow). In 1916 Rabinovich won the Triberg chess tournament, and he tied for first with Selezniev at Triberg 1917.\n\nAfter World War I, Rabinovich returned to St Petersburg (Petrograd, Leningrad). In 1920 he won the Petrograd chess championship. In 1920 he took fourth in Moscow (Russian Chess \"Olympiad\", first Soviet Union championship). The event was won by Alexander Alekhine. In 1922 he took second, behind Levenfish, in the Petrograd championship. In 1923 he tied for 7th-8th at Leningrad (2nd USSR Ch., Peter Romanovsky won). In 1923 he won at Novgorod. In 1924 he took 2nd, behind Grigory Levenfish, in the Leningrad championship. In 1924 he took 5th in Moscow (3rd USSR Ch., Efim Bogoljubow won).\n\nIn 1925 Ilya Rabinovich became the first Soviet player to compete outside the USSR. He played at Baden-Baden, Germany and took 7th place. The event was won by Alekhine. In 1925 he tied for 1st-4th in the Leningrad championship. In 1925 he took 3rd at Leningrad (4th USSR Ch., Bogoljubow won). In 1925 he took 16th in Moscow (1st International Tournament; Bogoljubow won). In 1926, he won at Leningrad. In 1926 he tied for 2nd-3rd at Leningrad (Alexander Ilyin-Genevsky won).\n\nIn 1927 Rabinovich wrote the first original book in the Russian language devoted to the endgame (titles \"The Endgame\" in Russian and \"The Russian Endgame Handbook\" in English). \n\nIn 1927 he tied for 10-12th in Moscow (5th URS-ch). The event was won by Fedor Bohatirchuk and Peter Romanovsky. In 1928, he won the Leningrad championship. In 1933 he tied for 3rd-5th in Leningrad (8th USSR Championship; Mikhail Botvinnik won). In 1934/35 Rabinovich shared first place with Grigory Levenfish in Leningrad (9th USSR Championship). At Moscow 1935, the second International Tournament, he tied for 11-14th. The event was won by Botvinnik and Salo Flohr.\n\nIn 1937 he tied for 10-12th in Tbilisi (10th USSR Championship; Levenfish won). In 1938, he tied for 3rd-4th in Leningrad (11th USSR Championships semi-final). In January 1939 he tied for 7-8th in Leningrad–Moscow (International Tournament; Flohr won). In 1939 he tied for 11-12th in Leningrad (11th USSR Championship; Botvinnik won). In 1939 he took 7th in the Leningrad championship (Georgy Lisitsin won). In 1940 he won the Leningrad championship. In June 1941 he played in interrupted semifinal of the USSR Chess Championship in Rostov-on-Don.\n\nRabinovich was taken ill during the siege of Leningrad. He was evacuated, but died of malnutrition in a hospital in Perm.\n"}
{"id": "44285544", "url": "https://en.wikipedia.org/wiki?curid=44285544", "title": "Joyce Vincent", "text": "Joyce Vincent\n\nJoyce Carol Vincent (19 October 1965 – c. December 2003) was a British woman whose death went unnoticed for more than two years as her corpse lay undiscovered in her London bedsit. Prior to her death, Vincent had cut off nearly all contact with those who knew her. She resigned from her job in 2001, and moved into a shelter for victims of domestic abuse. Around the same time, she began to reduce contact with friends and family. She died in her bedsit around December 2003 with neither family, co-workers, nor neighbours taking notice. Her remains were discovered on 25 January 2006, with the cause of death believed to be either an asthma attack or complications from a recent peptic ulcer.\n\nHer life and death were the topic of \"Dreams of a Life\", a 2011 docudrama film. The film and Vincent's life inspired musician Steven Wilson's album \"Hand. Cannot. Erase.\" as well as the band Miss Vincent's name and first single, titled \"No One Knew\".\n\nJoyce Vincent was born in Hammersmith on 19 October 1965 and raised near Fulham Palace Road. Her parents had immigrated to London from the island country of Grenada; her father Lawrence was a carpenter of African descent and her mother Lyris was of Indian descent. Following an operation, her mother died when Vincent was eleven, and her four older sisters took responsibility for her upbringing. She had a strained relationship with her emotionally distant father, who she claimed had died in 2001 (he actually died in 2004). She attended Melcombe Primary School and Fulham Gilliatt School for Girls, and left school at age sixteen with no qualifications.\n\nIn 1985, Vincent began working as a secretary at OCL in the City of London. She then worked at C.Itoh and Law Debenture before joining Ernst & Young. She worked in the treasury department of Ernst & Young for four years, but resigned in March 2001 for unknown reasons.\nShortly afterwards, Vincent spent some time in a domestic abuse shelter in Haringey and worked as a cleaner in a budget hotel. During this period, she became estranged from her family. A source involved in the investigation said: \"She detached herself from her family but there was no bust up. They are a really nice family. We understand she was in a relationship and there was a history of domestic violence.\" It has been speculated that she was ashamed to be a victim of domestic abuse or did not want to be traced by her abuser.\n\nAs a victim of domestic violence, Vincent was moved into a bedsit flat above Wood Green Shopping City in February 2003. The flat was owned by the Metropolitan Housing Trust and was used to house victims of abuse. In November 2003, after vomiting blood, she was hospitalised for two days due to a peptic ulcer at North Middlesex Hospital.\n\nVincent died of unknown causes around December 2003. She was an asthma sufferer, and an asthma attack, or complications surrounding her recent peptic ulcer, have been suggested as a possible cause of death. Her remains were described as \"mostly skeletal\" according to the pathologist, and she was lying on her back, next to a shopping bag, surrounded by Christmas presents she had wrapped but never delivered. It is not known to whom the presents were addressed, and the police report regarding the case has been disposed of.\n\nNeighbours had assumed the flat was unoccupied, and the odour of decomposing body tissue was attributed to nearby waste bins. The flat's windows did not allow direct sight into the accommodation. Drug addicts frequented the area, which may explain why no one questioned the constant noise from the television. Half of her rent was being automatically paid to Metropolitan Housing Trust by benefits agencies, leading officials to believe that she was still alive. However, over two years, £2,400 in unpaid rent accrued, and housing officials decided to repossess the property. Her corpse was discovered on 25 January 2006 when bailiffs had forced entry into the flat. The television and heating were still running due to her bills being continually paid for by automatic debit payments and debt forgiveness.\n\nThe Metropolitan Housing Trust said that due to housing benefits covering the costs of rent for some period after Vincent's death, arrears had not been realised until much later. The Trust also said that no concerns were raised by neighbours or visitors at any time during the two years between her death and discovery of the body.\n\nVincent's remains were too badly decomposed to conduct a full post-mortem, and she had to be identified from dental records. Police ruled death by natural causes as there was nothing to suggest foul play: the front door was double locked and there was no sign of a break-in. At the time of her death she had a boyfriend, but the police were unable to trace him. Her sisters had hired a private detective to look for her and contacted the Salvation Army, but these attempts proved unsuccessful. The detective found the house where Vincent was living, and the family wrote letters to her. But as she was already dead by this time, they received no response, and the family assumed that she had deliberately broken ties with them.\n\nThe \"Glasgow Herald\" reported, \"...her friends noted her as someone who fled at signs of trouble, who walked out of jobs if she clashed with a colleague, and who moved from one flat to the next all over London. She didn't answer the phone to her sister and didn't appear to have her own circle of friends, instead relying on the company of relative strangers who came with the package of a new boyfriend, a colleague, or flatmate.\"\n\nA film about Vincent, \"Dreams of a Life\", written and directed by Carol Morley with Zawe Ashton playing Vincent, was released in 2011. Morley tracked down and interviewed people who had known Vincent. They described a beautiful, intelligent, socially active woman, \"upwardly mobile\" and \"a high flyer\", whom they assumed \"was off somewhere having a better life than they were\". During her life, she met figures such as Nelson Mandela, Ben E. King, Gil Scott-Heron, and Betty Wright, and had also been to dinner with Stevie Wonder.\n\nOn 4 November 2014, English musician Steven Wilson announced that his fourth CD release, titled \"Hand. Cannot. Erase.\", would be based on the life of Vincent. According to Wilson, he was inspired to create a concept album after seeing \"Dreams of a Life\". From the book that accompanied the deluxe release of the album it is clear that the central character is a highly fictionalised version of Vincent: she is born on 8 October 1978 to an Italian mother and dies or disappears 22 December 2014. Her only sister is a girl, 'J.', who was briefly fostered by her parents prior to their divorce. In the album and book the Christmas presents are intended for H.'s estranged brother and his family.\n\nAfter hearing about the story of Vincent, the vocalist of Miss Vincent, Alex Marshall, was inspired to write a song about her life. The song was first titled Miss Vincent but the band eventually decided to change the song's title to No One Knew and name their band itself to Miss Vincent. This was the band's first single and it was released on 5 October 2012.\n\n"}
{"id": "37574003", "url": "https://en.wikipedia.org/wiki?curid=37574003", "title": "Juan de la Abadía", "text": "Juan de la Abadía\n\nJuan de la Abadía el Viejo (fl. 1470-1498, probable death 1498) was a Spanish painter in the gothic Spanish-Flemish style. His son, Juan de la Abadía el Joven, worked with him after 1490. \n\nHis known works include Santa Catalina (1490) in the church of la Magdalena de Huesca, now lost, the Saviour from the hermitage of Broto, now at the Museum of Zaragoza and the Santo Domingo in Almudévar (Huesca), after which he was known as the Maestro de Almudévar until identified by the art historian Ricardo del Arco.\n"}
{"id": "154147", "url": "https://en.wikipedia.org/wiki?curid=154147", "title": "Limerence", "text": "Limerence\n\nLimerence is a state of mind which results from a romantic attraction to another person and typically includes obsessive thoughts and fantasies and a desire to form or maintain a relationship with the object of love and have one's feelings reciprocated.\n\nPsychologist Dorothy Tennov coined the term \"limerence\" for her 1979 book, \"Love and Limerence: The Experience of Being in Love\", to describe a concept that had grown out of her work in the mid-1960s, when she interviewed over 500 people on the topic of love.\n\nLimerence, which is not exclusively sexual, has been defined in terms of its potentially inspirational effects and in relation to attachment theory. It has been described as being \"an involuntary potentially inspiring state of adoration and attachment to a limerent object (LO) involving intrusive and obsessive thoughts, feelings and behaviors from euphoria to despair, contingent on perceived emotional reciprocation\".\n\nAttachment theory emphasizes that \"many of the most intense emotions arise during the formation, the maintenance, the disruption, and the renewal of attachment relationships\". It has been suggested that \"the state of limerence is the conscious experience of sexual incentive motivation\" during attachment formation, \"a kind of subjective experience of sexual incentive motivation\" during the \"intensive ... pair-forming stage\" of human affectionate bonding.\n\nThe concept of 'limerence' \"provides a particular carving up of the semantic domain of love\", and represents an attempt at a scientific study of the nature of love. Limerence is considered as a cognitive and emotional state of being emotionally attached to or even obsessed with another person, and is typically experienced involuntarily and characterized by a strong desire for reciprocation of one's feelings—a near-obsessive form of romantic love. For Tennov, \"sexual attraction is an essential component of limerence ... the limerent is a potential sex partner\".\n\nLimerence is sometimes also interpreted as infatuation, or what is colloquially known as a \"crush\". However, in common speech, infatuation includes aspects of immaturity and extrapolation from insufficient information, and is usually short-lived. Tennov notes how limerence \"may dissolve soon after its initiation, as in an early teenage buzz-centered crush\", but she is more concerned with the point when \"limerent bonds are characterized by 'entropy' crystallization as described by Stendhal in his 1821 treatise \"On Love\", where a new love infatuation perceptually begins to transform ... [and] attractive characteristics are exaggerated and unattractive characteristics are given little or no attention ... [creating] a 'limerent object'\".\n\nAccording to Tennov, there are at least two types of love: limerence, which she describes as, among other things, \"loving attachment\", and \"loving affection\", the bond that exists between an individual and their parents and children. She notes that one form may evolve into the other: \"Those whose limerence was replaced by affectional bonding with the same partner might say ... 'We were very much in love when we married; today we love each other very much'\". The distinction is comparable to that drawn by ethologists \"between the pair-forming and pair-maintaining functions of sexual activity\", just as \"the attachment of the attachment theorists is very similar to the emotional reciprocation longed for in Tennov's limerence, and each is linked to sexuality\".\n\nLimerence is characterized by intrusive thinking and pronounced sensitivity to external events that reflect the disposition of the limerent object towards the individual. It can be experienced as intense joy or as extreme despair, depending on whether the feelings are reciprocated. Basically, it is the state of being completely carried away by unreasoned passion or love, even to the point of addictive-type behavior. Usually, one is inspired with an intense passion or admiration for someone. Limerence can be difficult to understand for those who have never experienced it, and it is thus often dismissed by non-limerents as ridiculous fantasy or a construct of romantic fiction.\n\nTennov differentiates between limerence and other emotions by asserting that love involves concern for the other person's welfare and feeling. While limerence does not require it, those concerns may certainly be incorporated. Affection and fondness exist only as a disposition towards another person, irrespective of whether those feelings are reciprocated, whereas limerence deeply desires reciprocation, but it remains unaltered whether or not it is returned. Physical contact with the object is neither essential nor sufficient to an individual experiencing limerence, unlike with one experiencing sexual attraction. Where early, unhealthy attachment patterns or trauma influence limerence, the limerent object may be construed as an idealization of the figure or figures involved in the original unhealthy attachment or trauma. Lack of reciprocation may in such instances serve to reinforce lessons learned in earlier, unhealthy bonding experiences, and hence strengthen the limerence.\n\nLimerence involves intrusive thinking about the limerent object. Other characteristics include acute longing for reciprocation, fear of rejection, and unsettling shyness in the limerent object's presence. In cases of unrequited limerence, transient relief may be found by vividly imagining reciprocation from the limerent object. Tennov suggests that feelings of limerence can be intensified through adversity, obstacles, or distance—'Intensification through Adversity'. A limerent person may have acute sensitivity to any act, thought, or condition that can be interpreted favorably. This may include a tendency to devise, fabricate, or invent \"reasonable\" explanations for why neutral actions are a sign of hidden passion in the limerent object.\n\nA person experiencing limerence has a general intensity of feeling that leaves other concerns in the background. In their thoughts, such a person tends to emphasize what is admirable in the limerent object and to avoid any negative or problematic attributes.\n\nDuring the height of limerence, thoughts of the limerent object (or person) are at once persistent, involuntary and intrusive. Such 'intrusive thoughts about the LO ... appear to be genetically driven': indeed, limerence is first and foremost a condition of cognitive obsession. This may be caused by low serotonin levels in the brain, comparable to those of people with obsessive–compulsive disorder. All events, associations, stimuli, and experiences return thoughts to the limerent object with unnerving consistency, while conversely the constant thoughts about the limerent object define all other experiences. If a certain thought has no previous connection with the limerent object, immediately one is made. Limerent fantasy is unsatisfactory unless rooted in reality, because the fantasizer may want the fantasy to seem realistic and somewhat possible. At their most severe, intrusive limerent thoughts can occupy an individual's waking hours completely, resulting—like severe addiction—in significant or complete disruption of the limerent's normal interests and activities, including work and family. For serial limerents, this can result in debilitating, lifelong underachievement in school, work, and family life. Comparisons made between limerence and substance addiction may draw attention to the constant, free availability of the limerent's \"drug of choice\".\n\nFantasies that are concerned with far-fetched ideas are usually dropped by the fantasizer. Sometimes fantasizing is retrospective: actual events are replayed from memory with great vividness. This form predominates when what is viewed as evidence of possible reciprocation can be re-experienced (a kind of selective or revisionist history). Otherwise, the long fantasy is anticipatory; it begins in the everyday world and climaxes at the attainment of the limerent goal. A limerent fantasy can also involve an unusual, often tragic, event.\n\nThe long fantasies form bridges between the limerent's ordinary life and that intensely desired ecstatic moment. The duration and complexity of a fantasy depend on the availability of time and freedom from distractions. The bliss of the imagined moment of consummation is greater when events imagined to precede it are possible (though they often represent grave departures from the probable). Not always is it entirely pleasant, and when rejection seems likely the thoughts focus on despair, sometimes to the point of suicide. The pleasantness or unpleasantness of the state seems almost unrelated to the intensity of the reaction. Although the direction of feeling, i.e. happy versus unhappy, shifts rapidly, with 'dramatic surges of buoyancy and despair', the intensity of intrusive and involuntary thinking alters less rapidly, and only in response to an accumulation of experiences with the particular limerent object.\n\nFantasies are occasionally dreamed by the one experiencing limerence. Dreams give out strong emotion and happiness when experienced, but often end with despair when the subject awakens. Dreams can reawaken strong feelings toward the limerent object after the feelings have declined.\n\nAlong with an emphasis on the perceived exceptional qualities, and devotion to them, there is abundant doubt that the feelings are reciprocated: rejection. Considerable self-doubt is encountered, leading to \"personal incapacitation expressed through unsettling timidity in the presence of the person\", something which causes misery and galvanizes desire.\n\nIn most cases, what destroys limerence is a suitably long period of time without reciprocation. Although it appears that limerence advances with adversity, personal discomfort may foul it. This discomfort results from a fear of the limerent object's opinions.\n\nLimerence develops and is sustained when there is a certain balance of hope and uncertainty. The basis for limerent hope is not in objective reality but in reality as it is perceived. The inclination is to sift through nuances of speech and subtleties of behavior for evidence of limerent hope. \"Lovers, of course, are notoriously frantic epistemologists, second only to paranoiacs (and analysts) as readers of signs and wonders.\" \"Little things\" are noticed and endlessly analyzed for meaning. Such excessive concern over trivia may not be entirely unfounded, however, as body language can indicate reciprocated feeling. What the limerent object said and did is recalled with vividness. Alternative meanings for the behaviors recalled are sought. Each word and gesture is permanently available for review, especially those interpreted as evidence in favor of reciprocated feeling. When objects, people, places or situations are encountered with the limerent object, they are vividly remembered, especially if the limerent object interacted with them in some way.\n\nThe belief that the limerent object does not and will not reciprocate can only be reached with great difficulty. Limerence can be carried quite far before acknowledgment of rejection is genuine, especially if it has not been addressed openly by the limerent object.\n\nThe physiological correlations of intense limerence can include seizure-like trembling, pallor, flushing, heart palpitations, pupil dilation and general weakness. Awkwardness, stuttering, shyness, and confusion predominate at the behavioral level. Less common effects include insomnia, loss of appetite, and passing out.\n\nIf there is extensive anxiety, incorrect behaviour may torpedo the relationship, which may cause physical responses to manifest intensely. Some people acutely feel these effects either immediately or following contact with the limerent object. Blended is dire ecstasy or keen despair, depending on the turn of events.\n\nThe sensitivity that stems from fear of rejection can darken limerent objects' perceived body language. Conflicted signs of desire may be emitted that confuse judgment. Often the limerent object is currently involved with another or is in some other way unavailable.\n\nA condition of sustained alertness, a heightening of awareness and an enormous fund of energy to deploy in pursuit of the limerent aim is developed. The sensation of limerence is felt in the midpoint of the chest, bottom of the throat, guts, or in some cases in the abdominal region. This can be interpreted as ecstasy at times of mutuality, but its presence is most noticeable during despair at times of rejection.\n\nSufferers complain of abandonment, despair, and diabolically humiliating disappointment. A sense of paralyzing ambiguity predominates, punctuated by pining. The fact of intermittent or nonreciprocal response lends to labile vacillation. This limbo is the threshold for mental prostration.\n\nAwareness of physical attraction plays a key role in the development of limerence, but is not enough to satisfy the limerent desire, and is almost never the main focus; instead, the limerent focuses on what could be defined as the \"beneficial attributes\". Nevertheless, Tennov stresses that \"the most consistent result of limerence is mating, not merely sexual interaction but also commitment\".\n\nLimerence can be intensified after a sexual relationship has begun, and with more intense limerence there is greater desire for sexual contact. However, while sexual surrender at one time indicated the end of uncertainty felt by the limerent object – because in the past, a sexual encounter more often led to a feeling of obligation to commit – in modern times this is not necessarily the case.\n\nThe sexual aspect of limerence is not consistent from person to person. Most limerents experience limerent sexuality as a component of romantic interest. Some limerents, however, may experience limerence as a consequence of hyperarousal. In such cases, limerence may form as a defense mechanism against the limerent object, who is not perceived initially as a romantic ideal, but as a physical threat to the limerent.\n\nSexual fantasies are distinct from limerent fantasies. Limerent fantasy is rooted in reality and is intrusive rather than voluntary. Sexual fantasies are under more or less voluntary control and may involve strangers, imaginary individuals, and situations that could not take place. Limerence elevates body temperature and increases relaxation, a sensation of viewing the world with rose-tinted glasses, leading to a greater receptiveness to sexuality, and to daydreaming.\n\nPeople can become aroused by the thought of sexual partners, acts, and situations that are not truly desired, whereas every detail of the limerent fantasy is passionately desired actually to take place. Limerence sometimes increases sexual interest in other partners when the limerent object is unreceptive or unavailable.\n\nThe limerent reaction is a composite reaction – that is, it is composed of a series of separate reactions. These reactions occur only where misperceptions meet adversity in the context of a romance. Perhaps because of this unique specificity, limerent reactions can be uniquely quantified and predicted according to the schema described below.\n\nInvolvement increases if obstacles are externally imposed or if the limerent object’s feelings are doubted. Only if the limerent object were to be revealed as highly undesirable might limerence subside. The presence of some degree of doubt causes the intensity of the feelings to increase further. The stage is reached at which the reaction is virtually impossible to dislodge. This adversity may be superficial or deep, internal or external, so that an individual may sometimes generate deep adversity where none exists. Also \"romance\", as it were, need not be present in any genuine way for a limerent reaction to occur.\n\nThe course of limerence results in a more intrusive thinking pattern. This thinking pattern is an expectant and often joyous period with the initial focusing on the limerent object’s admirable qualities: crystallization. Then, under appropriate conditions of hope and uncertainty, the limerence intensifies further.\n\nWith evidence of reciprocation (real or imagined) from the limerent object, a state of extreme pleasure, even euphoria, is enjoyed. Thoughts are mainly occupied with considering and reconsidering what is attractive in the limerent object, replaying whatever events may have thus far transpired with the limerent object, and appreciating personal qualities perceived as possibly having sparked interest in the limerent object. At peak crystallization, almost all waking thoughts revolve around the limerent object. After this peak, the feelings eventually decline.\n\nFantasies are preferred to virtually any other activity with the exception of activities that are believed to help obtain the limerent object, and activities that involve actually being in the presence of the limerent object. The motivation to attain a \"relationship\" continues to intensify so long as a proper mix of hope and uncertainty exist.\n\nTennov estimates, based on both questionnaire and interview data, that the average limerent reaction duration, from the moment of initiation until a feeling of neutrality is reached, is approximately three years. The extremes may be as brief as a few weeks or as long as several decades. When limerence is brief, maximum intensity may not have been attained. According to David Sack, M.D., limerence lasts longer than romantic love, but is shorter than a healthy, committed partnership.\n\nOthers suggest that 'the biogenetic sourcing of limerence determines its limitation, ordinarily, to a two-year span', that limerence generally lasts between 18 months and three years; but further studies on unrequited limerence have suggested longer durations. In turn, a limerent may only experience a single limerent episode, or may experience \"serial\" episodes, in which nearly one's entire mature life, from early puberty through late adulthood, can be consumed in successive limerent obsessions.\n\nOnce the \"limerent reaction\" has initiated, one of three varieties of bonds may form, defined over a set duration of time, in relation to the experience or non-experience of limerence. The constitution of these bonds may vary over the course of the relationship, in ways that may either increase or decrease the intensity of the limerence.\n\nThe basis and interesting characteristic of this delineation made by Tennov, is that based on her research and interviews with people, all human bonded relationships can be divided into three varieties being defined by the amount of limerence or non-limerence each partner contributes to the relationship.\n\nWith an affectional bond, neither partner is limerent. With a Limerent-Nonlimerent bond, one partner is limerent. In a Limerent-Limerent bond, both partners are limerent.\n\nAffectional bonding characterize those affectionate sexual relationships where neither partner is limerent; couples tend to be in love, but do not report continuous and unwanted intrusive thinking, feeling intense need for exclusivity, or define their goals in terms of reciprocity. These types of bonded couples tend to emphasize compatibility of interests, mutual preferences in leisure activities, ability to work together, and in some cases a degree of relative contentment.\n\nThe bulk of relationships, however, according to Tennov, are those between a limerent person and a nonlimerent other, i.e. \"limerent-nonlimerent bonding\". These bonds are characterized by unequal reciprocation.\n\nLastly, those relationship bonds in which there exists mutual reciprocation are defined as \"limerent-limerent bondings\". Tennov argues that since limerence itself is an \"unstable state\", mutually limerent bonds would be expected to be short-lived; mixed relationships probably last longer than limerent-limerent relationships. Some limerent-limerent relationships evolve into affectional bondings over time as limerence declines. Tennov describes such couples as \"old marrieds\" whose interactions are typically both stable and mutually gratifying.\n\nIn her study Tennov identified three ways in which limerence subsides: \n\nTennov's research has been continued by Albert Wakin, who knew Tennov at the University of Bridgeport but did not assist in her research, and Duyen Vo, a graduate student of Southern Connecticut State University. Their goal is to refine the term limerence so that it refers mostly to the negative aspects.\n\nThe term \"limerence\" has been invoked in many popular media, including self-help books, popular magazines, and websites. However, according to a paper by Wakin and Vo, \"In spite of the public’s exposure to limerence, the professional community, particularly clinical, is largely unaware of the concept.\" In 2008, Wakin and Vo presented their updated research to the American Association of Behavioral and Social Sciences. They reported that more research must be gathered before the condition is suitable for inclusion in the \"Diagnostic and Statistical Manual of Mental Disorders\" (\"DSM\").\n\nCritics point out that Tennov's account \"is based on interviews rather than on direct observation\", but conclude that \"despite its shortcomings, Tennov's work may constitute a basis for informed hypothesis formulation\".\n\n\n\n"}
{"id": "16745076", "url": "https://en.wikipedia.org/wiki?curid=16745076", "title": "List of Billown Course fatal accidents", "text": "List of Billown Course fatal accidents\n\nFatal accidents on the Billown Course, Castletown, Isle of Man during the Southern 100, Pre-TT Classic and the Post TT Races.\n\n"}
{"id": "1212565", "url": "https://en.wikipedia.org/wiki?curid=1212565", "title": "List of Nazi concentration camps", "text": "List of Nazi concentration camps\n\nThis article presents a partial list of the most prominent Nazi German concentration camps and extermination camps set up across Europe before and during the course of World War II and the Holocaust. A more complete list drawn up in 1967 by the German Ministry of Justice names about 1,200 camps and subcamps in countries occupied by Germany, while the Jewish Virtual Library writes: \"It is estimated that the Germans established 15,000 camps in the occupied countries.\" Some of the data presented in this table originates from the monograph titled \"The War Against the Jews\" by Lucy Dawidowicz among similar others.<ref name=\"ushmm/search\">Search Results: Mapping the SS Concentration Camp System. Alphabetical listing. United States Holocaust Memorial Museum: Further Reading. Bergen, Dawidowicz, Gilbert, Gutman, Hilberg, Yahil.</ref>\n\nIn 1933–1939, before the onset of war, most prisoners consisted of German Communists, Socialists, Social Democrats, Roma, Jehovah's Witnesses, homosexuals, and persons accused of 'asocial' or socially 'deviant' behavior by the Germans. They were not utilized to sustain the German war effort.\n\nAlthough the term 'concentration camp' is often used as a general term for all German camps during World War II, there were in fact several types of concentration camps in the German camp system. Holocaust scholars make a clear distinction between death camps and concentration camps which served a number of war related purposes including prison facilities, labor camps, prisoner of war camps, and transit camps among others.\n\nConcentration camps served primarily as detention and slave labor exploitation centers. An estimated 15 to 20 million people were imprisoned in 42,500 camps and ghettos, and often pressed into slavery during the subsequent years, according to research by the United States Holocaust Memorial Museum conducted more recently. The system of about 20,000 concentration camps in Germany and German-occupied Europe played a pivotal role in economically sustaining the German reign of terror. Most of them were destroyed by the Germans in an attempt to hide the evidence of war crimes and crimes against humanity; nevertheless tens of thousands of prisoners sent on death marches were liberated by the Allies afterward.\n\nExtermination camps were designed and built exclusively to kill prisoners on a massive scale, often immediately upon arrival. The extermination camps of Operation Reinhard such as Bełżec, Sobibór and Treblinka served as \"death factories\" in which German SS and police murdered nearly 2,700,000 Jews by asphyxiation with poison gas, shooting, and extreme work under starvation conditions.\n\nThe concentration camps held large groups of prisoners without trial or judicial process. In modern historiography, the term refers to a place of systemic mistreatment, starvation, forced labour and murder.\n\nStatistical and numerical data presented in the table below originates from a wide variety of publications and therefore does not constitute a representative sample of the total.\n\nThe Ghettos in German-occupied Europe are generally not included in this list. Relevant information can be found at the separate List of Nazi-era ghettos.\n\n\n"}
{"id": "52551196", "url": "https://en.wikipedia.org/wiki?curid=52551196", "title": "List of executions in Japan", "text": "List of executions in Japan\n\n\"Note: Inmates noted with a * were sentenced to death for murder(s) committed while on parole for another murder\"\n\n"}
{"id": "16554664", "url": "https://en.wikipedia.org/wiki?curid=16554664", "title": "Living systems", "text": "Living systems\n\nLiving systems are open self-organizing life forms that interact with their environment. These systems are maintained by flows of information, energy and matter.\n\nSome scientists have proposed in the last few decades that a general living systems theory is required to explain the nature of life. Such a general theory, arising out of the ecological and biological sciences, attempts to map general principles for how all living systems work. Instead of examining phenomena by attempting to break things down into components, a general living systems theory explores phenomena in terms of dynamic patterns of the relationships of organisms with their environment.\n\nLiving systems theory is a general theory about the existence of all living systems, their structure, interaction, behavior and development. This work is created by James Grier Miller, which was intended to formalize the concept of life. According to Miller's original conception as spelled out in his magnum opus \"Living Systems\", a \"living system\" must contain each of twenty \"critical subsystems\", which are defined by their functions and visible in numerous systems, from simple cells to organisms, countries, and societies. In \"Living Systems\" Miller provides a detailed look at a number of systems in order of increasing size, and identifies his subsystems in each. \nMiller considers living systems as a subset of all systems. Below the level of living systems, he defines space and time, matter and energy, information and entropy, levels of organization, and physical and conceptual factors, and above living systems ecological, planetary and solar systems, galaxies, etc.\n\nLiving systems according to Parent (1996) are by definition \"open self-organizing systems that have the special characteristics of life and interact with their environment. This takes place by means of information and material-energy exchanges. Living systems can be as simple as a single cell or as complex as a supranational organization such as the European Union. Regardless of their complexity, they each depend upon the same essential twenty subsystems (or processes) in order to survive and to continue the propagation of their species or types beyond a single generation\".\n\nMiller said that systems exist at eight \"nested\" hierarchical levels: cell, organ, organism, group, organization, community, society, and supranational system. At each level, a system invariably comprises twenty critical subsystems, which process matter–energy or information except for the first two, which process both matter–energy and information: reproducer and boundary.\n\nThe processors of matter–energy are: \n\nThe processors of information are:\n\nJames Grier Miller in 1978 wrote a 1,102-page volume to present his living systems theory. He constructed a general theory of living systems by focusing on concrete systems—nonrandom accumulations of matter–energy in physical space–time organized into interacting, interrelated subsystems or components. Slightly revising the original model a dozen years later, he distinguished eight \"nested\" hierarchical levels in such complex structures. Each level is \"nested\" in the sense that each higher level contains the next lower level in a nested fashion.\n\nHis central thesis is that the systems in existence at all eight levels are open systems composed of twenty critical subsystems that process inputs, throughputs, and outputs of various forms of matter–energy and information. Two of these subsystems—reproducer and boundary—process both matter–energy and information. Eight of them process only matter–energy. The other ten process information only. \nAll nature is a continuum. The endless complexity of life is organized into patterns which repeat themselves—theme and variations—at each level of system. These similarities and differences are proper concerns for science. From the ceaseless streaming of protoplasm to the many-vectored activities of supranational systems, there are continuous flows through living systems as they maintain their highly organized steady states.\nSeppänen (1998) says that Miller applied general systems theory on a broad scale to describe all aspects of living systems.\n\nMiller's theory posits that the mutual interrelationship of the components of a system extends across the hierarchical levels. Examples: Cells and organs of a living system thrive on the food the organism obtains from its suprasystem; the member countries of a supranational system reap the benefits accrued from the communal activities to which each one contributes. Miller says that his eclectic theory \"ties together past discoveries from many disciplines and provides an outline into which new findings can be fitted\".\n\nMiller says the concepts of space, time, matter, energy, and information are essential to his theory because the living systems exist in space and are made of matter and energy organized by information. Miller's theory of living systems employs two sorts of spaces: physical or geographical space, and conceptual or abstracted spaces. Time is the fundamental \"fourth dimension\" of the physical space–time continuum/spiral. Matter is anything that has mass and occupies physical space. Mass and energy are equivalent as one can be converted into the other. Information refers to the degrees of freedom that exist in a given situation to choose among signals, symbols, messages, or patterns to be transmitted.\n\nOther relevant concepts are system, structure, process, type, level, echelon, suprasystem, subsystem, transmissions, and steady state. A system can be conceptual, concrete or abstracted. The structure of a system is the arrangement of the subsystems and their components in three-dimensional space at any point of time. Process, which can be reversible or irreversible, refers to change over time of matter–energy or information in a system. Type defines living systems with similar characteristics. Level is the position in a hierarchy of systems. Many complex living systems, at various levels, are organized into two or more echelons. The suprasystem of any living system is the next higher system in which it is a subsystem or component. The totality of all the structures in a system which carry out a particular process is a subsystem. Transmissions are inputs and outputs in concrete systems. Because living systems are open systems, with continually altering fluxes of matter–energy and information, many of their equilibria are dynamic—situations identified as steady states or flux equilibria.\n\nMiller identifies the comparable matter–energy and information processing critical subsystems. Elaborating on the eight hierarchical levels, he defines society, which constitutes the seventh hierarchy, as \"a large, living, concrete system with [community] and lower am levels of living systems as subsystems and components\". Society may include small, primitive, totipotential communities; ancient city–states, and kingdoms; as well as modern nation–states and empires that are not supranational systems. Miller provides general descriptions of each of the subsystems that fit all eight levels.\n\nA supranational system, in Miller's view, \"is composed of two or more societies, some or all of whose processes are under the control of a decider that is superordinate to their highest echelons\". However, he contends that no supranational system with all its twenty subsystems under control of its decider exists today. The absence of a supranational decider precludes the existence of a concrete supranational system. Miller says that studying a supranational system is problematical because its subsystems\n...tend to consist of few components besides the decoder. These systems do little matter-energy processing. The power of component societies [nations] today is almost always greater than the power of supranational deciders. Traditionally, theory at this level has been based upon intuition and study of history rather than data collection. Some quantitative research is now being done, and construction of global-system models and simulations is currently burgeoning.\n\nAt the supranational system level, Miller's emphasis is on international organizations, associations, and groups comprising representatives of societies (nation–states). Miller identifies the subsystems at this level to suit this emphasis. Thus, for example, the reproducer is \"any multipurpose supranational system which creates a single purpose supranational organization\" (p. 914); and the boundary is the \"supranational forces, usually located on or near supranational borders, which defend, guard, or police them\" (p. 914).\n\nNot just those specialized in international communication, but all communication science scholars could pay particular attention to the major contributions of living systems theory (LST) to social systems approaches that Bailey has pointed out:\n\nBailey says that LST, perhaps the \"most integrative\" social systems theory, has made many more contributions that may be easily overlooked, such as: providing a detailed analysis of types of systems; making a distinction between concrete and abstracted systems; discussion of physical space and time; placing emphasis on information processing; providing an analysis of entropy; recognition of totipotential systems, and partipotential systems; providing an innovative approach to the structure–process issue; and introducing the concept of joint subsystem—a subsystem that belongs to two systems simultaneously; of dispersal—lateral, outward, upward, and downward; of inclusion—inclusion of something from the environment that is not part of the system; of artifact—an animal-made or human-made inclusion; of adjustment process, which combats stress in a system; and of critical subsystems, which carry out processes that all living systems need to survive.\n\nLST's analysis of the twenty interacting subsystems, Bailey adds, clearly distinguishing between matter–energy-processing and information-processing, as well as LST's analysis of the eight interrelated system levels, enables us to understand how social systems are linked to biological systems. LST also analyzes the irregularities or \"organizational pathologies\" of systems functioning (e.g., system stress and strain, feedback irregularities, information–input overload). It explicates the role of entropy in social research while it equates negentropy with information and order. It emphasizes both structure and process, as well as their interrelations.\n\nIt omits the analysis of subjective phenomena, and it overemphasizes concrete Q-analysis (correlation of objects) to the virtual exclusion of R-analysis (correlation of variables). By asserting that societies (ranging from totipotential communities to nation-states and non-supranational systems) have greater control over their subsystem components than supranational systems have, it dodges the issue of transnational power over the contemporary social systems. Miller's supranational system bears no resemblance to the modern world-system that Immanuel Wallerstein (1974) described, although both of them were looking at the same living (dissipative) structure.\n\n\n\n"}
{"id": "20220978", "url": "https://en.wikipedia.org/wiki?curid=20220978", "title": "Luke Campbell (boxer)", "text": "Luke Campbell (boxer)\n\nLuke Campbell, (born 27 September 1987) is a British professional boxer. He held the WBC Silver and Commonwealth lightweight titles from 2016 to 2017, and has challenged once for the WBA and \"Ring\" magazine lightweight titles in 2017. As an amateur, Campbell won gold medals at the 2008 European Championships and 2012 Olympics, as well as silver at the 2011 World Championships, all in the bantamweight division.\n\nSenior ABA titles\n\nCampbell fought for St. Paul's Amateur Boxing Club in Hull, winning the English senior ABA bantamweight title in 2007 and retaining in 2008 after beating Gareth Smith 23–1 in the final.\n\n2008 European Championships\n\nHe represented England at the 2008 European Amateur Boxing Championships in Liverpool. At the Championships, Campbell defeated Olympic bronze medalist Veaceslav Gojan of Moldova in the quarterfinals and Denis Makarov of Germany in the semifinals before facing the experienced Detelin Dalakliev of Bulgaria in the final.\n\nAfter four rounds the Bulgarian levelled the scores in the fight to 5 each after he scored a point in the last two seconds of the fight. The decision then went to countback and Campbell was awarded the title making Campbell the first Englishman to win a European amateur title since 1961.\n\n2010 Four Nations Challenge\n\nFollowing his successful return, following a year long sabbatical to recover from tendon surgery, to International competition at the WBC Night Of Champions in Cardiff in July, Luke started his 2012 London Olympics preparations by securing Gold at the Four Nations Challenge in Sheffield.\n\nOn the Saturday Luke, who convincingly won his Featherweight (57 kg) fight by an 11–5 points margin over China’s Jun Tan at the WBC Night Of Champions, continued his winning ways by beating Kazakhstan’s Shulakov Madi by 6 – 4 in the semi final of the Four Nations Challenge.\n\nIn Sunday’s final Luke again faced Jun Tan from China, his adversary the previous week at the WBC Night of Champions. As before Luke dominated the proceedings, this time winning by an increased points margin of 11–3.\n\n2011 World Championships\n\nCampbell qualified for the 2012 Olympic Games after winning a silver medal at the 2011 World Amateur Boxing Championships – Bantamweight in Baku, Azerbaijan.\n\nAt the 2012 London Olympics, Campbell won gold in the 56 kg Bantamweight division after beating Ireland's John Joe Nevin 14:11. Campbell had earlier beaten Italian, Vittorio Parrinello, by 11:9. In the quarter-finals he met Detelin Dalakliev of Bulgaria in his closest bout of the competition, controversially edging it by a score of 16:15. His next opponent, Satoshi Shimizu from Japan- who had earlier been reinstated in the tournament by AIBA after a successful appeal after a defeat by Magomed Abdulhamidov was beaten by 20:11. Campbell beat Nevin in the final bout, and dropped the Irishman at the start of the third round, thus becoming the first bantamweight boxer to win Olympic gold for Great Britain since Henry Thomas in 1908.\n\nA first class postage stamp, depicting Campbell, was issued by Royal Mail and a post box in Hessle Road, Hull was painted gold to commemorate his gold medal win. Local telephone network provider, KC, have also commemorated the win by painting one of their telephone boxes, near to St Paul's Boxing Club, gold.\n\nCampbell was appointed Member of the Order of the British Empire (MBE) in the 2013 New Year Honours for services to boxing.\n\nCampbell's first professional fight took place on 13 July 2013 at Craven Park in his home city of Hull. His opponent was Andy Harris who he beat in the first round. Campbell continued his undefeated start to his career with a 5th round stoppage of Lee Connelly in Hull on 2 November 2013. In his fifth professional bout Campbell carried on his undefeated record and became the first person to stop Scott Moises.\n\nIn April 2014, Campbell announced that he would be taking a break from boxing, pulling out of his next scheduled bout, following his father being diagnosed with cancer.\n\nDespite announcing he would be taking a break from boxing, Campbell fought a further four times in 2014. A points win over Craig Woodruff, a knockout win of Steve Trumble, a TKO victory over Krzysztof Szot and another TKO win over Daniel Eduardo Brizuela. In March 2015, Campbell faced off against 24 year old Nicaraguan Levis Morales (11-1-1, 4 KOs) at the Ice Arena in Hull. Campbell knocked down Morales in round 2 and 3, before the referee Howard John Foster stopped the fight resulting in a TKO victory for Campbell.\n\nOn 1 August, Campbell claimed the vacant WBC International lightweight title by defeating fellow Hullensian Tommy Coyle (21-2, 10 KOs) via 10-round TKO at the KC Lightstream Stadium. This was also a WBC lightweight eliminator. Coyle was knocked down once in round 2 following a left to the body and three times in round 12 before the referee waved off the fight. Only four months later in December, Campbell lost the title and mandatory status in a split decision loss to French boxer Yvan Mendy (32-4-1, 16 KOs) at the O2 Arena in London. Campbell was floored for the first time in his professional career in a ragged defensive display as Mendy received a split decision with scores of 115-112, 115-113, while the third judge scored it 115-113 for Campbell.\n\nAfter a 3-month lay off, Campbell started a comeback trail in March on the undercard of Brook-Bizier at the Sheffield Arena in Sheffield fighting British boxer Gary Sykes (28-4, 6 KOs) for the vacant Commonwealth lightweight title. Campbell made a winning return to the ring in spectacular fashion only needing two rounds to finish Sykes off in his first fight since losing his unbeaten professional record. A left hook that landed hard on the top of Sykes' head had him struggling before a straight right put him down. Although Sykes beat the count, Campbell was all over him. It was the left-hook that opened the door again, this time in the midsection, followed by a right to the temple. Referee Steve Gray moved swiftly in as Sykes' corner was throwing their towel to halt the fight.\n\nPromoter Eddie Hearn revealed Campbell was next due to fight on 30 July 2016 at the First Direct Arena in Leeds for the vacant WBC Silver lightweight title against 30 year old former IBF super featherweight champion Argenis Mendez (23-4-1, 12 KOs). Although being floored by a sharp right hand in the second round, Campbell proved to be too quick and dominated the remainder of the fight to claim the vacant WBC Silver lightweight title. From the third round, it became a comfortable fight for Campbell who moved his opponent around the ring and caught him with a succession of punches. The scorecards were all in favour of Campbell 116-111, 117-110, 115-112, who stated he would now like to land a world title shot.\n\nCampbell fought former British and Commonwealth lightweight champion and former world title challenger Derry Mathews (38-10, 20 KOs) on the undercard of the world cruiserweight title fight between Tony Bellew and BJ Flores at the Echo Arena on 15 October, live on Sky Sports. This was the first defence of the WBC Silver lightweight title in a scheduled 12 round bout. The fight started off as a brawl, however Campbell proved to be too quick as he retained his titles and kept his future world title shot alive after stopping Mathews in round four. Mathews was dropped following a couple of left hooks to his body. In the post fight interview, Campbell claimed, despite winning via stoppage, his game plan was 'to outbox Mathews for a points win'.\n\nOn 3 January 2017, it was announced that Campbell would defend his WBC Silver title against Mexican fringe boxer Jairo Lopez (21-6, 14 KOs) at the Ice Arena in Hull on 25 February. The title defence would be part of a triple header, also including fellow Hullensian Tommy Coyle and Gavin McDonnell challenging for the vacant WBC World super bantamweight championship. Campbell won the fight in round 2 after a well timed uppercut floored Lopez. Referee Ian-John Lewis halted the fight. Lopez was also dropped in round 1 just before the bell rang. Campbell retained his WBC silver lightweight title. After the fight, Campbell and promoter Eddie Hearn both said the fight they were chasing next is a rematch with French boxer Yvan Mendy, to avenge his sole loss. “Mendy has done the worst thing possible in beating me\", Campbell said after the fight.\n\nOn 10 April 2017, Eurosport and Sky Sports announced that Campbell would be involved in a WBA Lightweight title eliminator against former interim WBA ightweight titlist Darleys Pérez (33-2-2, 21 KOs) on the Anthony Joshua vs. Wladimir Klitschko undercard at the Wembley Stadium on 29 April 2017. The winner would be in the number one position to challenge champion Jorge Linares. Pérez weighed over the limit at 136.3 pounds, so had he won the bout, he would not have been eligible for the mandatory spot. Campbell racked up the win via a 9th round stoppage win, which was due to injury. It was said that Pérez hurt his left arm after a throwing a left hook. He signaled to the referee that he could not continue and the fight was stopped at 1 minute 28 seconds into the round. Although Pérez started the fight well, enough to win the first three rounds, he physically looked gassed by the later rounds where Campbell took over. With the win, Campbell was made the new mandatory challenger to WBA champion, held by Jorge Linares. Eddie Hearn said he would speak to Golden Boy Promotions to push things forward for the fight to be made.\n\nGolden Boy Promotions matchmaker, Robert Diaz, announced that WBA and \"The Ring\" lightweight champion Jorge Linares (42-3, 27 KOs) would next fight on 23 September 2017 and ruled out Campbell as his opponent. The plan being to have Campbell to fight on the undercard. If both fighters win their respected bouts, they would meet in the future. On 21 July 2017 the WBA ordered Linares to make a mandatory defence against Campbell. As per WBA rules, a titleholder must fight a mandatory within 9 months, this time would expire on 23 July, having won the title from Crolla in September 2016. Both sides were given 30 days to come to an agreement for the fight. On 27 July, a deal was reached for Linares and Campbell to fight at The Forum in Inglewood, California on 23 September 2017. The bout will be shown live on Sky Sports in the United Kingdom and on HBO: Boxing After Dark in the United States. In an interview, Linares said, “I am excited to make my return to the US and to headline a HBO show for the first time. I know Campbell is a tough [...] I am confident that I will emerge victorious on September 23rd.” This fight would mark the second time Campbell fights professionally in California. In front of 4,125, Linares won his 12th straight fight, retaining his WBA world title after 12 rounds against Campbell. One judge scored the fight 115-113 for Campbell, the remaining two had it 115-112 and 114-113 in favour of Linares, giving him the split decision win. ESPN.com also scored the fight 115-112 for Linares. Linares dropped Campbell with a straight right hand to the head in round 2. Between rounds 5 to 9, Campbell took control of the fight. Linares later told HBO that he wanted to reduce his amount of offense so that he wouldn't get hurt. Once the championship rounds started, Linares regained control of the fight. Had Campbell not been dropped early in the fight, the verdict would have been a split decision draw.\n\nCampbell believed he won the fight, speaking to Max Kellerman he said, “No one can ever doubt my heart. Yeah, I got off to a rocky start. He hit me with a nice, clean shot in the second round, caught me on the eye, cut it. I wasn’t dazed. From there, I had double vision in one eye for the rest of the fight. But from then, I out-classed him. I thought I won the fight. He’s a great champion, but I thought I out-classed him. I didn't think he was landing any shots whatsoever, and I was catching him with all the clean shots.” Linares praised Campbell for his efforts, “He was a tough opponent. Many people said he was very easy, but it’s not for no reason he’s an Olympic champion. I fought very well all the way to the 12th round. And I think in the fifth round, I started to box him a little bit because I didn't wanna get hurt.” CompuBox stats showed Linares landed 140 of 414 thrown (34%) , while Campbell was credited to landing 141 of his 524 thrown (27%). After the fight, promoter Eddie Hearn stated that Campbell could fight the winner of Crolla vs. Burns or another possible opponent would be WBO champion Terry Flanagan. A day after the fight, Campell revealed that his father, 58, had lost his battle with cancer and had died two weeks before the fight. Campbell was in the United States at his training camp when his father passed away at home, with family members. The fight drew an average of 687,000 viewers and peaked at 726,000 viewers on HBO.\n\nOn 3 May 2018, it was announced by Matchroom that Campbell would appear on the Bellew-Haye II card the next day at the O2 Arena in London in a six round fight. Campbell fought and defeated Troy James (20-5-1, 5 KOs). In round 2, Campbell hit James with a left uppercut followed by a right hand that dropped him. James quickly recovered and survived the remainder of the round. In round 4, Campbell dropped James with a hard shot, again James showed heart in getting up off the canvas. In the following round, Campbell started to unload and landed a barrage of unanswered punches before the referee stepped in to stop the fight.\n\nOn 31 July, Campbell announced Shane McGuigan as his new trainer. Speaking on the partnership, Campbell said, \"I’m incredibly excited to be teaming up with Shane. I’ve always heard good things about him within boxing but having trained under him for a few weeks now I can vouch for just how good he is. We’ve already struck up a great relationship and I have no doubt he is going to help take me to the next level. I want to become a world champion and Shane has a proven track record of achieving that with his fighters. It’s a thriving gym with a great atmosphere and world class fighters, and that’s where I want to be.” In his statement, McGuigan said he would help Campbell become an elite as well as win a world title.\n\nOn 6 August, Sky Sports announced the rematch between Campbell and 33 year old French boxer Yvan Mendy (40-4-1, 19 KOs) was confirmed to take place on the Anthony Joshua vs. Alexander Povetkin undercard at the Wembley Stadium in London on 22 September. Since defeating Campbell in their first fight in December 2015, Mendy had gone on too win seven fights in a row and picked up a #1 ranking with WBC at lightweight. Hearn called the bout a 'true 50-50 fight' as both boxers had improved since their first meeting. Campbell won the bout on points to avenge the earlier defeat. After 12 rounds the scorecards read 119-109, 118-111 and 116-112 in favour or Campbell. Mendy had his moments in the earlier rounds, but once Campbell adjusted, he was able to box and move to pound out a decision victory in what was a WBC lightweight title eliminator.\n\nCampbell has two sons with his wife Lynsey Kraanen.\n\nCampbell's grandfather was an Irish boxing champion and members of Campbell's family still live in Ireland.\n\nLuke was born in Hull and supports local football team Hull City. He featured in a video by Hull City in 2015.\n\nIn December 2012 it was announced that Campbell would take part in ITV's \"Dancing on Ice\" series 8 which started on 6 January 2013, skating with professional Jenna Smith. He reached the final of Dancing On Ice 2013 with Beth Tweddle and Matt Lapinskas. He came third while Matt came second and Beth came first.\n\n\n"}
{"id": "53970843", "url": "https://en.wikipedia.org/wiki?curid=53970843", "title": "Machine learning in bioinformatics", "text": "Machine learning in bioinformatics\n\nMachine learning, a subfield of computer science involving the development of algorithms that learn how to make predictions based on data, has a number of emerging applications in the field of bioinformatics. Bioinformatics deals with computational and mathematical approaches for understanding and processing biological data. \n\nPrior to the emergence of machine learning algorithms, bioinformatics algorithms had to be explicitly programmed by hand which, for problems such as protein structure prediction, proves extremely difficult. Machine learning techniques such as deep learning enable the algorithm to make use of automatic feature learning which means that based on the dataset alone, the algorithm can learn how to combine multiple features of the input data into a more abstract set of features from which to conduct further learning. This multi-layered approach to learning patterns in the input data allows such systems to make quite complex predictions when trained on large datasets. In recent years, the size and number of available biological datasets have skyrocketed, enabling bioinformatics researchers to make use of these machine learning systems. Machine learning has been applied to six main subfields of bioinformatics: genomics, proteomics, microarrays, systems biology, evolution, and text mining.\n\nGenomics involves the study of the genome, the complete DNA sequence, of organisms. While genomic sequence data has historically been sparse due to the technical difficulty in sequencing a piece of DNA, the number of available sequences is growing exponentially. However, while raw data is becoming increasingly available and accessible, the biological interpretation of this data is occurring at a much slower pace. Therefore, there is an increasing need for the development of machine learning systems that can automatically determine the location of protein-encoding genes within a given DNA sequence. This is a problem in computational biology known as gene prediction.\n\nGene prediction is commonly performed through a combination of what are known as extrinsic and intrinsic searches. For the extrinsic search, the input DNA sequence is run through a large database of sequences whose genes have been previously discovered and their locations annotated. A number of the sequence's genes can be identified by determining which strings of bases within the sequence are homologous to known gene sequences. However, given the limitation in size of the database of known and annotated gene sequences, not all the genes in a given input sequence can be identified through homology alone. Therefore, an intrinsic search is needed where a gene prediction program attempts to identify the remaining genes from the DNA sequence alone.\n\nMachine learning has also been used for the problem of multiple sequence alignment which involves aligning many DNA or amino acid sequences in order to determine regions of similarity that could indicate a shared evolutionary history.\nIt can also be used to detect and visualize genome rearrangements.\n\nProteins, strings of amino acids, gain much of their function from protein folding in which they conform into a three-dimensional structure. This structure is composed of a number of layers of folding, including the primary structure (i.e. the flat string of amino acids), the secondary structure (alpha helices and beta sheets), the tertiary structure, and the quartenary structure.\n\nProtein secondary structure prediction is a main focus of this subfield as the further protein foldings (tertiary and quartenary structures) are determined based on the secondary structure. Solving the true structure of a protein is an incredibly expensive and time-intensive process, furthering the need for systems that can accurately predict the structure of a protein by analyzing the amino acid sequence directly. Prior to machine learning, researchers needed to conduct this prediction manually. This trend began in 1951 when Pauling and Corey released their work on predicting the hydrogen bond configurations of a protein from a polypeptide chain. Today, through the use of automatic feature learning, the best machine learning techniques are able to achieve an accuracy of 82-84%. The current state-of-the-art in secondary structure prediction uses a system called DeepCNF (deep convolutional neural fields) which relies on the machine learning model of artificial neural networks to achieve an accuracy of approximately 84% when tasked to classify the amino acids of a protein sequence into one of three structural classes (helix, sheet, or coil). The theoretical limit for three-state protein secondary structure is 88–90%.\n\nMachine learning has also been applied to proteomics problems such as protein side-chain prediction, protein loop modeling, and protein contact map prediction.\n\nMicroarrays, a type of lab-on-a-chip, are used for automatically collecting data about large amounts of biological material. Machine learning can aid in the analysis of this data, and it has been applied to expression pattern identification, classification, and genetic network induction.\n\nThis technology is especially useful for monitoring the expression of genes within a genome, aiding in diagnosing different types of cancer based on which genes are expressed. One of the main problems in this field is identifying which genes are expressed based on the collected data. In addition, due to the huge number of genes on which data is collected by the microarray, there is a large amount of irrelevant data to the task of expressed gene identification, further complicating this problem. Machine learning presents a potential solution to this problem as various classification methods can be used to perform this identification. The most commonly used methods are radial basis function networks, deep learning, Bayesian classification, decision trees, and random forest.\n\nSystems biology focuses on the study of the emergent behaviors from complex interactions of simple biological components in a system. Such components can include molecules such as DNA, RNA, proteins, and metabolites.\n\nMachine learning has been used to aid in the modelling of these complex interactions in biological systems in domains such as genetic networks, signal transduction networks, and metabolic pathways. Probabilistic graphical models, a machine learning technique for determining the structure between different variables, are one of the most commonly used methods for modeling genetic networks. In addition, machine learning has been applied to systems biology problems such as identifying transcription factor binding sites using a technique known as Markov chain optimization. Genetic algorithms, machine learning techniques which are based on the natural process of evolution, have been used to model genetic networks and regulatory structures.\n\nOther systems biology applications of machine learning include the task of enzyme function prediction, high throughput microarray data analysis, analysis of genome-wide association studies to better understand markers of Multiple Sclerosis, protein function prediction, and identification of NCR-sensitivity of genes in yeast.\n\nThe increase in available biological publications led to the issue of the increase in difficulty in searching through and compiling all the relevant available information on a given topic across all sources. This task is known as knowledge extraction. This is necessary for biological data collection which can then in turn be fed into machine learning algorithms to generate new biological knowledge. Machine learning can be used for this knowledge extraction task using techniques such as natural language processing to extract the useful information from human-generated reports in a database. Text Nailing, an alternative approach to machine learning, capable of extracting features from clinical narrative notes was introduced in 2017.\n\nThis technique has been applied to the search for novel drug targets, as this task requires the examination of information stored in biological databases and journals. Annotations of proteins in protein databases often do not reflect the complete known set of knowledge of each protein, so additional information must be extracted from biomedical literature. Machine learning has been applied to automatic annotation of the function of genes and proteins, determination of the subcellular localization of a protein, analysis of DNA-expression arrays, large-scale protein interaction analysis, and molecule interaction analysis.\n\nAnother application of text mining is the detection and visualization of distinct DNA regions given sufficient reference data.\n"}
{"id": "9392817", "url": "https://en.wikipedia.org/wiki?curid=9392817", "title": "Marianus IV of Arborea", "text": "Marianus IV of Arborea\n\nMarianus IV (in Catalan: Marià IV d'Arborea, 1329 – 1376), called the Great, was the Giudice of Arborea, island of Sardinia, from 1347 to his death. He was, as his nickname indicates, the greatest sovereign of Arborea. He was a legislator and a warrior whose reign saw the commencement of massive codification of the laws of his realm and incessant warfare with the Aragonese Empire. He was also a religious man, who had connections to Catherine of Siena. He was, in short, an \"wise legislator, able politician, and valiant warrior.\"\n\nBorn at Oristano, he was the son of Hugh II and successor of his brother Peter III. At the behest of his father he spent most of his youth in Barcelona, where he was educated at the court of Alfonso IV of Aragon. He participated actively in the coronation of Peter IV in 1336. In 1336 in Barcelona, he married Timbor, daughter of Dalmatius IV of Rocabertí and Beatrice of Serrallonga, Baroness of Cabrenys. In 1339, he was invested by Peter with the counties of Bas (Spain), Goceano (Sardinia), and Marmilla (Sardinia).\n\nIn 1347, the Doria rebelled and defeated the Catalans at Aidu de Turdu, occupying Bonorva. This sparked a long war between Aragon and Genoa, but at the outset the Doria could not take advantage of their victory. On 11 September 1349, he returned to Oristano, two years after succeeding his childless elder brother. One of his first acts was to repopulate the town of Goceano, rebuild the castle there, and plant a florid garden.\n\nThough an alliance had been in effect with Aragon for more than fifty years at the time of his accession, Marianus realised that the political aim of Peter IV was nothing less than the annexation of Sardinia and, following the conquest of Alghero (1353), he parted ways with the Catalans. He allied with the Genoese and the Doria, then at war with Aragon, and made himself an enemy of the Aragonese.\n\nMarianus' first directive was against Gherardo della Gherardescha, a loyal Pisan vassal of the Aragonese. He attacked Castel di Castro from the south but was rebuffed. He initiated a siege until his Doria allies could attack from the north. They did, and took Alghero. His armies proved successful in the field and he succeeded in expelling the Aragonese from every redoubt on the island save the stronghold of Castel di Castro. He even menaced Sassari in 1354. Later that year, Peter IV landed on the island at Nulauro. Marianus promptly began a guerilla war of ambushes against royal troops until, at the end of 1355, a brief peace was signed at Sanluri by which Marianus renounced Alghero. The peace lasted two years, during which Marianus reinforced his armies and the country progressed favourably economically.\n\nIn 1365, the war resumed with full force. Pope Urban V confirmed Arborean possession of the whole of the island save Sassari, Alghero, and Cagliari. Peter IV, however, sent a fleet commanded by Pere de Luna to lead an Aragonese army deep into Arborea, bypassing other fortifications to assault Oristano. The Aragonese troops were trapped between the Arborean armies commanded by Marianus and his son, the future Hugh III, and defeated. In 1368, Marianus finally occupied Sassari. He was preparing another campaign when he died in 1376.\n\nDuring the two-year peace, Marianus began the work of putting down in writing the oral laws and customs of Arborea. The chief legislative work of his lifetime was the \"Codice Rurale\", which his daughter Eleanor later incorporated into her massive \"Carta de Logu\". His great work, however, was cut short by his sudden death of the bubonic plague in 1376.\n\nBy his wife Tambor, Marianus left three children:\nTimbor was still living in 1361, but that is the last she is heard of.\n\n"}
{"id": "22690333", "url": "https://en.wikipedia.org/wiki?curid=22690333", "title": "Methuselah-like proteins", "text": "Methuselah-like proteins\n\nThe Methuselah-like proteins are a family of G protein-coupled receptors found in insects that play a role in aging and reproduction. Antagonizing these receptors can extend the life span of the animal and make it more resistant to free radicals and starvation, but also reduce reproduction and increase cold sensitivity. The age dependent decline in olfaction and motor function is unaffected.\n\nMethuselah-like proteins are related to G protein-coupled receptors of the secretin receptor family.\n"}
{"id": "42161726", "url": "https://en.wikipedia.org/wiki?curid=42161726", "title": "Mohammad Samir Hossain", "text": "Mohammad Samir Hossain\n\nMohammad Samir Hossain (born 28 November 1976) is a Bangladeshi theorist and one of the few Muslim scientists in the field of Death anxiety (psychology) research. He is the pioneering physician to introduce Scientific Thanatology and Spiritual Psychiatry in Bangladesh. He is also an author of multiple theory books on Death adjustment.\n\nHossain passed SSC and HSC exam from Ideal School and College and Dhaka College respectively. He completed his medical graduation from Sir Salimullah Medical College in Dhaka. After that, he studied Basic modern psychology, Abnormal psychology, Psychiatry, Psychotherapy and Philosophy (Death) for which he participated in different external educational activities offered from different institutions including some prestigious ones like Johns Hopkins University School of Medicine, Harvard Medical School, and Yale University. After his education in Psychotherapy, he was engaged in Death anxiety (psychology) researches.\nHossain is a physician. He taught Psychiatry at the Medical College for Women and Hospital at Uttara in Dhaka. He is an honorary external faculty of the Palliative Care Service at Bangabandhu Sheikh Mujib Medical University where he teaches death related psychiatry.\n\nHossain's educations in different modes of distance learning, after his medical graduation, created some sort of traumatic turmoil during his early career inside his own country. However, as per Hossain's autobiography, his researches and their conclusions helped him in healing that wound that ultimately gave birth to further development in his works.\n\nOf all the works of Hossain, Death and adjustment hypotheses is the most noteworthy one in the field of death anxiety research. Apart from his books written on it, in UK, the Royal College of Psychiatrists at London published the theory as the article \"Facing the finality: Death and Adjustment Hypotheses\". Later, once again it reached the international community of scientists through the Taylor & Francis publication \"Journal of Loss and Trauma\" as the article \"Introducing Death and Adjustment Hypotheses\". With the declaration of the hypotheses, two things were postulated. The first part of the hypotheses theorises that death should not be considered the end of existence. The next segment states the belief that the immortal pattern of human existence can only be adopted in a morally rich life with the attitude towards morality and materialism balanced mutually.\n\nTwo most practised theory books in English language authored by Hossain are \"Quest for a New Death\" and \"Human Immortality\"\n\nHossain’s personal life is contributory to his scientific activities in such a way that the \"Journal of Loss and Trauma\" published his partial autobiography \"Abstinence: A Memoir\" in 2011 to reflect how psychological traumas taught and helped him learning to cope. In that article, Hossain, mentioned of his elder son Mohammad Seeyam Samir's death as the strongest motivation for his researches on death anxiety. He also mentioned of his anticipatory fear about his parents – Md Manjur Hossain and Rezia Begum's death in unknown future as another motivational factor from his early childhood; both of them eventually died before 2015. It ultimately compelled him to learn and theorise how to adjust to the phenomenon of death of the loved ones. Hossain is married to Tahmina Rahman Chowdhury. They have another son, Mohammad Raiyan Samir.\n\n\n"}
{"id": "7123512", "url": "https://en.wikipedia.org/wiki?curid=7123512", "title": "Morris East", "text": "Morris East\n\nMorris East (born August 8, 1973 in Olongapo, Philippines) is a retired Filipino professional boxer and boxing trainer. East is the former GAB Light Middleweight, OPBF and WBA World Light Welterweight champion. Morris has trained world champions Zab Judah and Nonito Donaire.\n\nAs a teenager, East moved to Cebu City and was spotted by Lito Cortes who brought him to the Cebu Coliseum gym. Promoter Sammy Gello-ani then offered him amateur fights to keep him earning for his meals.\n\nEast turned professional in 1989 and won the WBA World Light Welterweight Championship by defeating Akinobu Hiranaka with an 11th round TKO victory in Tokyo on 9 September 1992. With the victory, East became the youngest ever Filipino to hold a world championship in boxing at the age of 19 years and 31 days old. He is also the second youngest boxer to win a world title at 140 lbs., second to Puerto Rico's Wilfred Benitez won the WBC jr. welterweight title when he was 17 years old. The victory over Hiranaka was named Ring Magazine Knockout of the Year for 1989. Morris lost the title in his first defense against Juan Martin Coggi.\n\nEast would retire after winning and defending the Philippines Games & Amusement Board Light Middleweight Championship in 1995.\n\nEast moved to San Diego, California in 1996 and later moved to Las Vegas, where he works as a fight trainer in the Johnny Tocco gym. In 2011, he worked with IBF light welterweight titleholder Zab Judah and WBC/WBO bantamweight champion Nonito Donaire. East also worked with Eddie Mustafa Muhammad.\n\nBorn of a Filipina and black American U.S. Navy sailor, East didn't meet his father until he became champion. He traveled from the Philippines to the United States a month after winning his WBA belt to locate his father, John East, Sr. With the help of a CNN news team, the father was located in Oakland, California and their first meeting was broadcast by CNN. Morris, Jr. improved his father's living condition but his father, suffering from bad health, died of a massive heart attack a few months later.\n"}
{"id": "41081887", "url": "https://en.wikipedia.org/wiki?curid=41081887", "title": "Odon device", "text": "Odon device\n\nOdón device is a medical device that assists during a difficult birth. The low-cost device consists of a plastic sleeve that is inflated around the baby's head and is used to gently pull and ease the head of the infant through the birth canal.\n\nWorldwide, more than 13 million births each year face serious complications, and every day about 800 women die from preventable causes related to pregnancy and childbirth (about 300,000 annually). The use of forceps and other mechanical devices in the extraction of a baby in a difficult delivery can cause internal bleeding in the mother or may result in injuries to the baby's head or spine.\n\nThe Odón Device has the potential to allow for vaginal delivery in complicated pregnancies in which common medical practice would have led to a cesarean section, the use of forceps to extract the newborn or the use of a ventouse vacuum device that attaches suction cups directly to the baby's scalp. By reducing contact between the baby's skull and the birth canal, the risk of infection is also reduced.\n\nThe device was developed by Jorge Odón, a car mechanic from Lanús, Argentina who had seen a video describing a method to extract a loose cork from inside an empty wine bottle by inserting a plastic bag into the bottle, inflating the bag once it has enveloped the cork and then pulling out the inflated bag together with the cork. Odón conceived of the use of this same technique that evening in bed and spoke with an obstetrician who encouraged him to move ahead with the idea. The first model of the device was created by sewing a sleeve onto a cloth bag and was tested using a doll inserted into a glass jar to simulate the use of the device in the delivery process.\n\nIn complicated deliveries, the device is positioned against the baby's scalp and the lubricated sleeve is gently inserted around the baby's head. Once a marker on the device indicates that it has been positioned properly, the inner compartment of the sleeve is inflated, providing a strong grip on the baby's head. The inserter is taken away and the sleeve can be pulled with up to of force to pull out the head and allow for delivery of the baby.\n\nAfter further testing, Odón was introduced to the chief of obstetrics at a hospital in Buenos Aires who saw the benefit of the method and arranged to have the device tested more thoroughly at an Iowa laboratory that has simulators designed to model delivery methods more realistically. Safety testing had been performed on 20 women in Argentina, all of whom had previously given birth and were experiencing uncomplicated pregnancies, including a woman who was able to deliver a baby weighing with only two pushes. Further testing will be conducted on more than 250 women in China, India and South Africa, with a mix of pregnant women experiencing normal and complicated labor.\n\nBecton Dickinson has agreed to manufacture and distribute the unit, and estimates that the Odón Device could be constructed for $50 per unit, and it is expected that it could be used by midwives as well as obstetricians who would need minimal training to use the device effectively. The World Health Organization (WHO) offered favorable notices regarding the device, which was recognized for its \"potential to save the lives of mothers and newborns at the time of birth\". The WHO's Dr. Mario Merialdi called the device \"exciting\", saying that childbirth is an area that has had little recent progress. Dr. Margaret Chan, Director-General of the WHO, described the device as \"a low-cost simplified way to deliver babies, and protect mothers [that] promises to transfer life-saving capacity to rural health posts, which almost never have the facilities and staff to perform a C-section [as] the first simple new tool for assisted delivery since forceps and vacuum extractors were introduced centuries ago.\"\n\n"}
{"id": "849508", "url": "https://en.wikipedia.org/wiki?curid=849508", "title": "Peak oil", "text": "Peak oil\n\nPeak oil is the theorized point in time when the maximum rate of extraction of petroleum is reached, after which it is expected to enter terminal decline. Peak oil theory is based on the observed rise, peak, fall, and depletion of aggregate production rate in oil fields over time. It is often confused with oil depletion; however, whereas \"depletion\" refers to a period of falling reserves and supply, \"peak oil\" refers to peak, before terminal depletion occurs. The concept of peak oil is often credited to geologist M. King Hubbert whose 1956 paper first presented a formal theory.\n\nSome observers, such as petroleum industry experts Kenneth S. Deffeyes and Matthew Simmons, predicted there would be negative global economy effects after a post-peak production decline and subsequent oil price increase because of the continued dependence of most modern industrial transport, agricultural, and industrial systems on the low cost and high availability of oil. Predictions vary greatly as to what exactly these negative effects would be. While the notion that petroleum production must peak at some point is not controversial, the assertion that this must coincide with a serious economic decline, or even that the decline in production will necessarily be caused by an exhaustion of available reserves, is not universally accepted.\n\nOil production forecasts on which predictions of peak oil are based are sometimes made within a range which includes optimistic (higher production) and pessimistic (lower production) scenarios. According to the International Energy Agency, conventional crude oil production peaked in 2006. A 2013 study concluded that peak oil \"appears probable before 2030\", and that there was a \"significant risk\" that it would occur before 2020, and assumed that major investments in alternatives will occur before a crisis, without requiring major changes in the lifestyle of heavily oil-consuming nations. Pessimistic predictions of future oil production made after 2007 state either that the peak has already occurred, that oil production is on the cusp of the peak, or that it will occur soon.\n\nHubbert's original prediction that US peak oil would occur in about 1970 appeared accurate for a time, as US average annual production peaked in 1970 at 9.6 million barrels per day and mostly declined for more than 3 decades after. However, the use of hydraulic fracturing caused US production to rebound during the 2000s, challenging the inevitability of post-peak decline for the US oil production. In addition, Hubbert's original predictions for world peak oil production proved premature. Nevertheless, the rate of discovery of new petroleum deposits peaked worldwide during the 1960s and has never approached these levels since.\n\nThe idea that the rate of oil production would peak and irreversibly decline is an old one. In 1919, David White, chief geologist of the United States Geological Survey, wrote of US petroleum: \"... the peak of production will soon be passed, possibly within 3 years.\" In 1953, Eugene Ayers, a researcher for Gulf Oil, projected that if US ultimate recoverable oil reserves were 100 billion barrels, then production in the US would peak no later than 1960. If ultimate recoverable were to be as high as 200 billion barrels, which he warned was wishful thinking, US peak production would come no later than 1970. Likewise for the world, he projected a peak somewhere between 1985 (one trillion barrels ultimate recoverable) and 2000 (two trillion barrels recoverable). Ayers made his projections without a mathematical model. He wrote: \"But if the curve is made to look reasonable, it is quite possible to adapt mathematical expressions to it and to determine, in this way, the peak dates corresponding to various ultimate recoverable reserve numbers\"\n\nBy observing past discoveries and production levels, and predicting future discovery trends, the geoscientist M. King Hubbert used statistical modelling in 1956 to predict that United States oil production would peak between 1965 and 1971. This prediction appeared accurate for a time however during 2018 daily production of oil in the United States was exceeding daily production in 1970, the year that was previously the peak. Hubbert used a semi-logistical curved model (sometimes incorrectly compared to a normal distribution). He assumed the production rate of a limited resource would follow a roughly symmetrical distribution. Depending on the limits of exploitability and market pressures, the rise or decline of resource production over time might be sharper or more stable, appear more linear or curved. That model and its variants are now called Hubbert peak theory; they have been used to describe and predict the peak and decline of production from regions, countries, and multinational areas. The same theory has also been applied to other limited-resource production.\n\nMore recently, the term \"peak oil\" was popularized by Colin Campbell and Kjell Aleklett in 2002 when they helped form the Association for the Study of Peak Oil and Gas (ASPO). In his publications, Hubbert used the term \"peak production rate\" and \"peak in the rate of discoveries\".\n\nIn a 2006 analysis of Hubbert theory, it was noted that uncertainty in real world oil production amounts and confusion in definitions increases the uncertainty in general of production predictions. By comparing the fit of various other models, it was found that Hubbert's methods yielded the closest fit over all, but that none of the models were very accurate. In 1956 Hubbert himself recommended using \"a family of possible production curves\" when predicting a production peak and decline curve.\n\nA comprehensive 2009 study of oil depletion by the UK Energy Research Centre noted:\n\nThe report noted that Hubbert had used the logistic curve because it was mathematically convenient, not because he firmly believed it to be correct. The study observed that in most cases, the asymmetric exponential model provided a better fit, and that peaks tended to occur well before half the oil had been produced, with the result that in nearly all cases, the post-peak decline was more gradual than the increase leading up to the peak.\n\nThe demand side of peak oil over time is concerned with the total quantity of oil that the global market would choose to consume at various possible market prices and how this entire listing of quantities at various prices would evolve over time. Global demand for crude oil grew an average of 1.76% per year from 1994 to 2006, with a high growth of 3.4% in 2003–2004. After reaching a high of per day in 2007, world consumption decreased in both 2008 and 2009 by a total of 1.8%, despite fuel costs plummeting in 2008. In spite of this lull, world's demanded for oil is projected to increase 21% over 2007 levels by 2030 ( from ), or about 0.8% average annual growth, largely due to increases in demand from the transportation sector. According to projections by the International Energy Agency (IEA) in 2013, growth in global oil demand will be significantly outpaced by growth in production capacity over the next 5 years. Developments in late 2014–2015 have seen an oversupply of global markets leading to a significant drop in the price of oil.\n\nEnergy demand is distributed amongst four broad sectors: transportation, residential, commercial, and industrial. In terms of oil use, transportation is the largest sector and the one that has seen the largest growth in demand in recent decades. This growth has largely come from new demand for personal-use vehicles powered by internal combustion engines. This sector also has the highest consumption rates, accounting for approximately 71% of the oil used in the United States in 2013. and 55% of oil use worldwide as documented in the Hirsch report. Transportation is therefore of particular interest to those seeking to mitigate the effects of peak oil.\nAlthough demand growth is highest in the developing world, the United States is the world's largest consumer of petroleum. Between 1995 and 2005, US consumption grew from to , a increase. China, by comparison, increased consumption from to , an increase of , in the same time frame. The Energy Information Administration (EIA) stated that gasoline usage in the United States may have peaked in 2007, in part because of increasing interest in and mandates for use of biofuels and energy efficiency.\n\nAs countries develop, industry and higher living standards drive up energy use, oil usage being a major component. Thriving economies, such as China and India, are quickly becoming large oil consumers. For example, China surpassed the United States as the world's largest crude oil importer in 2015. Oil consumption growth is expected to continue; however, not at previous rates, as China's economic growth is predicted to decrease from the high rates of the early part of the 21st century. India's oil imports are expected to more than triple from 2005 levels by 2020, rising to 5 million barrels per day (790×103 m/d).\n\nAnother significant factor affecting petroleum demand has been human population growth. The United States Census Bureau predicts that world population in 2030 will be almost double that of 1980. Oil production per capita peaked in 1979 at 5.5 barrels/year but then declined to fluctuate around 4.5 barrels/year since. In this regard, the decreasing population growth rate since the 1970s has somewhat ameliorated the per capita decline.\n\nSome analysts argue that the cost of oil has a profound effect on economic growth due to its pivotal role in the extraction of resources and the processing, manufacturing, and transportation of goods. As the industrial effort to extract new unconventional oil sources increases, this has a compounding negative effect on all sectors of the economy, leading to economic stagnation or even eventual contraction. Such a scenario would result in an inability for national economies to pay high oil prices, leading to declining demand and a price collapse.\n\nOil may come from conventional or unconventional sources. The terms are not strictly defined, and vary within the literature as definitions based on new technologies tend to change over time. As a result, different oil forecasting studies have included different classes of liquid fuels. Some use the terms \"conventional\" oil for what is included in the model, and \"unconventional\" oil for classes excluded.\n\nIn 1956, Hubbert confined his peak oil prediction to that crude oil \"producible by methods now in use.\" By 1962, however, his analyses included future improvements in exploration and production. All of Hubbert's analyses of peak oil specifically excluded oil manufactured from oil shale or mined from oil sands. A 2013 study predicting an early peak excluded deepwater oil, tight oil, oil with API gravity less than 17.5, and oil close to the poles, such as that on the North Slope of Alaska, all of which it defined as non-conventional. Some commonly used definitions for conventional and unconventional oil are detailed below.\n\nConventional oil is extracted on land and offshore using standard techniques, and can be categorized as light, medium, heavy, or extra heavy in grade. The exact definitions of these grades vary depending on the region from which the oil came.\nLight oil flows naturally to the surface or can be extracted by simply pumping it out of the ground. Heavy refers to oil that has higher density and therefore lower API gravity. It does not flow easily, and its consistency is similar to that of molasses. While some of it can be produced using conventional techniques, recovery rates are better using unconventional methods.\n\nAccording to the International Energy Agency, conventional crude oil production peaked in 2006, with al all-time maximum of 70 millions of barrels per day.\n\nOil currently considered unconventional is derived from multiple sources.\n\n\nThe peak of world oilfield discoveries occurred in the 1960s at around (Gb)/year. According to the Association for the Study of Peak Oil and Gas (ASPO), the rate of discovery has been falling steadily since. Less than 10 Gb/yr of oil were discovered each year between 2002 and 2007. According to a 2010 Reuters article, the annual rate of discovery of new fields has remained remarkably constant at 15–20 Gb/yr.\n\nBut despite the fall-off in new field discoveries, and record-high production rates, the reported proved reserves of crude oil remaining in the ground in 2014, which totaled 1,490 billion barrels, not counting Canadian heavy oil sands, were more than quadruple the 1965 proved reserves of 354 billion barrels. A researcher for the U.S. Energy Information Administration has pointed out that after the first wave of discoveries in an area, most oil and natural gas reserve growth comes not from discoveries of new fields, but from extensions and additional gas found within existing fields.\n\nA report by the UK Energy Research Centre noted that \"discovery\" is often used ambiguously, and explained the seeming contradiction between falling discovery rates since the 1960s and increasing reserves by the phenomenon of reserve growth. The report noted that increased reserves within a field may be discovered or developed by new technology years or decades after the original discovery. But because of the practice of \"backdating,\" any new reserves within a field, even those to be discovered decades after the field discovery, are attributed to the year of initial field discovery, creating an illusion that discovery is not keeping pace with production.\n\nTotal possible conventional crude oil reserves include crude oil with 90% certainty of being technically able to be produced from reservoirs (through a wellbore using primary, secondary, improved, enhanced, or tertiary methods); all crude with a 50% probability of being produced in the future (probable); and discovered reserves that have a 10% possibility of being produced in the future (possible). Reserve estimates based on these are referred to as 1P, proven (at least 90% probability); 2P, proven and probable (at least 50% probability); and 3P, proven, probable and possible (at least 10% probability), respectively. This does not include liquids extracted from mined solids or gasses (oil sands, oil shale, gas-to-liquid processes, or coal-to-liquid processes).\n\nHubbert's 1956 peak projection for the United States depended on geological estimates of ultimate recoverable oil resources, but starting in his 1962 publication, he concluded that ultimate oil recovery was an output of his mathematical analysis, rather than an assumption. He regarded his peak oil calculation as independent of reserve estimates.\n\nMany current 2P calculations predict reserves to be between 1150 and 1350 Gb, but some authors have written that because of misinformation, withheld information, and misleading reserve calculations, 2P reserves are likely nearer to 850–900 Gb. The Energy Watch Group wrote that actual reserves peaked in 1980, when production first surpassed new discoveries, that apparent increases in reserves since then are illusory, and concluded (in 2007): \"Probably the world oil production has peaked already, but we cannot be sure yet.\" \n\nSadad Al Husseini estimated that of the world's of proven reserves should be recategorized as speculative resources.\n\nOne difficulty in forecasting the date of peak oil is the opacity surrounding the oil reserves classified as \"proven\". In many major producing countries, the majority of reserves claims have not been subject to outside audit or examination. Many worrying signs concerning the depletion of proven reserves have emerged in recent years. This was best exemplified by the 2004 scandal surrounding the \"evaporation\" of 20% of Shell's reserves.\n\nFor the most part, proven reserves are stated by the oil companies, the producer states and the consumer states. All three have reasons to overstate their proven reserves: oil companies may look to increase their potential worth; producer countries gain a stronger international stature; and governments of consumer countries may seek a means to foster sentiments of security and stability within their economies and among consumers.\n\nMajor discrepancies arise from accuracy issues with the self-reported numbers from the Organization of the Petroleum Exporting Countries (OPEC). Besides the possibility that these nations have overstated their reserves for political reasons (during periods of no substantial discoveries), over 70 nations also follow a practice of not reducing their reserves to account for yearly production. Analysts have suggested that OPEC member nations have economic incentives to exaggerate their reserves, as the OPEC quota system allows greater output for countries with greater reserves.\n\nKuwait, for example, was reported in the January 2006 issue of \"Petroleum Intelligence Weekly\" to have only in reserve, of which only 24 were fully proven. This report was based on the leak of a confidential document from Kuwait and has not been formally denied by the Kuwaiti authorities. This leaked document is from 2001, but excludes revisions or discoveries made since then. Additionally, the reported of oil burned off by Iraqi soldiers in the First Persian Gulf War are conspicuously missing from Kuwait's figures.\n\nOn the other hand, investigative journalist Greg Palast argues that oil companies have an interest in making oil look more rare than it is, to justify higher prices. This view is contested by ecological journalist Richard Heinberg. Other analysts argue that oil producing countries understate the extent of their reserves to drive up the price.\n\nThe EUR reported by the 2000 USGS survey of has been criticized for assuming a discovery trend over the next twenty years that would reverse the observed trend of the past 40 years. Their 95% confidence EUR of assumed that discovery levels would stay steady, despite the fact that new-field discovery rates have declined since the 1960s. That trend of falling discoveries has continued in the ten years since the USGS made their assumption. The 2000 USGS is also criticized for other assumptions, as well as assuming 2030 production rates inconsistent with projected reserves.\n\nAs conventional oil becomes less available, it can be replaced with production of liquids from unconventional sources such as tight oil, oil sands, ultra-heavy oils, gas-to-liquid technologies, coal-to-liquid technologies, biofuel technologies, and shale oil. In the 2007 and subsequent International Energy Outlook editions, the word \"Oil\" was replaced with \"Liquids\" in the chart of world energy consumption. In 2009 biofuels was included in \"Liquids\" instead of in \"Renewables\". The inclusion of natural gas liquids, a bi-product of natural gas extraction, in \"Liquids\" has been criticized as it is mostly a chemical feedstock which is generally not used as transport fuel.\n\nReserve estimates are based on the oil price. Hence, unconventional sources such as heavy crude oil, oil sands, and oil shale may be included as new techniques reduce the cost of extraction. With rule changes by the SEC, oil companies can now book them as proven reserves after opening a strip mine or thermal facility for extraction. These unconventional sources are more labor and resource intensive to produce, however, requiring extra energy to refine, resulting in higher production costs and up to three times more greenhouse gas emissions per barrel (or barrel equivalent) on a \"well to tank\" basis or 10 to 45% more on a \"well to wheels\" basis, which includes the carbon emitted from combustion of the final product.\n\nWhile the energy used, resources needed, and environmental effects of extracting unconventional sources have traditionally been prohibitively high, major unconventional oil sources being considered for large-scale production are the extra heavy oil in the Orinoco Belt of Venezuela, the Athabasca Oil Sands in the Western Canadian Sedimentary Basin, and the oil shale of the Green River Formation in Colorado, Utah, and Wyoming in the United States. Energy companies such as Syncrude and Suncor have been extracting bitumen for decades but production has increased greatly in recent years with the development of Steam Assisted Gravity Drainage and other extraction technologies.\n\nChuck Masters of the USGS estimates that, \"Taken together, these resource occurrences, in the Western Hemisphere, are approximately equal to the Identified Reserves of conventional crude oil accredited to the Middle East.\" Authorities familiar with the resources believe that the world's ultimate reserves of unconventional oil are several times as large as those of conventional oil and will be highly profitable for companies as a result of higher prices in the 21st century. In October 2009, the USGS updated the Orinoco tar sands (Venezuela) recoverable \"mean value\" to , with a 90% chance of being within the range of 380-, making this area \"one of the world's largest recoverable oil accumulations\".\n\nDespite the large quantities of oil available in non-conventional sources, Matthew Simmons argued in 2005 that limitations on production prevent them from becoming an effective substitute for conventional crude oil. Simmons stated \"these are high energy intensity projects that can never reach high volumes\" to offset significant losses from other sources. Another study claims that even under highly optimistic assumptions, \"Canada's oil sands will not prevent peak oil,\" although production could reach by 2030 in a \"crash program\" development effort.\n\nMoreover, oil extracted from these sources typically contains contaminants such as sulfur and heavy metals that are energy-intensive to extract and can leave tailings, ponds containing hydrocarbon sludge, in some cases. The same applies to much of the Middle East's undeveloped conventional oil reserves, much of which is heavy, viscous, and contaminated with sulfur and metals to the point of being unusable. However, high oil prices make these sources more financially appealing. A study by Wood Mackenzie suggests that by the early 2020s all the world's extra oil supply is likely to come from unconventional sources.\n\nThe point in time when peak global oil production occurs defines peak oil. Some adherents of 'peak oil' believe that production capacity will remain the main limitation of supply, and that when production decreases, it will be the main bottleneck to the petroleum supply/demand equation. Others believe that the increasing industrial effort to extract oil will have a negative effect on global economic growth, leading to demand contraction and a price collapse, thereby causing production decline as some unconventional sources become uneconomical. Yet others believe that the peak may be to some extent led by declining demand as new technologies and improving efficiency shift energy usage away from oil.\n\nWorldwide oil discoveries have been less than annual production since 1980. World population has grown faster than oil production. Because of this, oil production \"per capita\" peaked in 1979 (preceded by a plateau during the period of 1973–1979).\n\nThe increasing investment in harder-to-reach oil as of 2005 was said to signal oil companies' belief in the end of easy oil. While it is widely believed that increased oil prices spur an increase in production, an increasing number of oil industry insiders were reportedly coming to believe in 2009 that even with higher prices, oil production was unlikely to increase significantly. Among the reasons cited were both geological factors as well as \"above ground\" factors that are likely to see oil production plateau.\n\nA 2008 \"Journal of Energy Security\" analysis of the energy return on drilling effort (energy returned on energy invested, also referred to as EROEI) in the United States concluded that there was extremely limited potential to increase production of both gas and (especially) oil. By looking at the historical response of production to variation in drilling effort, the analysis showed very little increase of production attributable to increased drilling. This was because of diminishing returns with increasing drilling effort: as drilling effort increased, the energy obtained per active drill rig was reduced according to a severely diminishing power law. The study concluded that even an enormous increase of drilling effort was unlikely to significantly increase oil and gas production in a mature petroleum region such as the United States. However, contrary to the study's conclusion, since the analysis was published in 2008, US production of crude oil has increased 86%, and production of dry natural gas has increased 34% (2015 compared to 2008).\n\nThe assumption of inevitable declining volumes of oil and gas produced per unit of effort is contrary to recent experience in the US. In the United States, as of 2017, there has been an ongoing decade-long increase in the productivity of oil and gas drilling in all the major tight oil and gas plays. The US Energy Information Administration reports, for instance, that in the Bakken Shale production area of North Dakota, the volume of oil produced per day of drilling rig time in January 2017 was 4 times the oil volume per day of drilling five years previous, in January 2012, and nearly 10 times the oil volume per day of ten years previous, in January 2007. In the Marcellus gas region of the northeast, The volume of gas produced per day of drilling time in January 2017 was 3 times the gas volume per day of drilling five years previous, in January 2012, and 28 times the gas volume per day of drilling ten years previous, in January 2007.\n\nAverage yearly gains in global supply from 1987 to 2005 were (1.7%). In 2005, the IEA predicted that 2030 production rates would reach , but this number was gradually reduced to . A 2008 analysis of IEA predictions questioned several underlying assumptions and claimed that a 2030 production level of (comprising of crude oil and of both non-conventional oil and natural gas liquids) was more realistic than the IEA numbers. More recently, the EIA's Annual Energy Outlook 2015 indicated no production peak out to 2040. However, this required a future Brent crude oil price of $US144/bbl (2013 dollars) \"as growing demand leads to the development of more costly resources\". Whether the world economy can grow and maintain demand for such a high oil price remains to be seen.\n\nIn a 2013 study of 733 giant oil fields, only 32% of the ultimately recoverable oil, condensate and gas remained. Ghawar, which is the largest oil field in the world and responsible for approximately half of Saudi Arabia's oil production over the last 50 years, was in decline before 2009. The world's second largest oil field, the Burgan Field in Kuwait, entered decline in November 2005.\n\nMexico announced that production from its giant Cantarell Field began to decline in March 2006, reportedly at a rate of 13% per year. Also in 2006, Saudi Aramco Senior Vice President Abdullah Saif estimated that its existing fields were declining at a rate of 5% to 12% per year. According to a study of the largest 811 oilfields conducted in early 2008 by Cambridge Energy Research Associates, the average rate of field decline is 4.5% per year. The Association for the Study of Peak Oil and Gas agreed with their decline rates, but considered the rate of new fields coming online overly optimistic. The IEA stated in November 2008 that an analysis of 800 oilfields showed the decline in oil production to be 6.7% a year for fields past their peak, and that this would grow to 8.6% in 2030. A more rapid annual rate of decline of 5.1% in 800 of the world's largest oil fields weighted for production over their whole lives was reported by the International Energy Agency in their World Energy Outlook 2008. The 2013 study of 733 giant fields mentioned previously had an average decline rate 3.83% which was described as \"conservative.\"\n\nEntities such as governments or cartels can reduce supply to the world market by limiting access to the supply through nationalizing oil, cutting back on production, limiting drilling rights, imposing taxes, etc. International sanctions, corruption, and military conflicts can also reduce supply.\n\nAnother factor affecting global oil supply is the nationalization of oil reserves by producing nations. The nationalization of oil occurs as countries begin to deprivatize oil production and withhold exports. Kate Dourian, Platts' Middle East editor, points out that while estimates of oil reserves may vary, politics have now entered the equation of oil supply. \"Some countries are becoming off limits. Major oil companies operating in Venezuela find themselves in a difficult position because of the growing nationalization of that resource. These countries are now reluctant to share their reserves.\"\n\nAccording to consulting firm PFC Energy, only 7% of the world's estimated oil and gas reserves are in countries that allow companies like ExxonMobil free rein. Fully 65% are in the hands of state-owned companies such as Saudi Aramco, with the rest in countries such as Russia and Venezuela, where access by Western European and North American companies is difficult. The PFC study implies political factors are limiting capacity increases in Mexico, Venezuela, Iran, Iraq, Kuwait, and Russia. Saudi Arabia is also limiting capacity expansion, but because of a self-imposed cap, unlike the other countries. As a result of not having access to countries amenable to oil exploration, ExxonMobil is not making nearly the investment in finding new oil that it did in 1981.\n\nOPEC is an alliance among 14 diverse oil-producing countries (as of May 2017: Algeria, Angola, Ecuador, Equatorial Guinea, Gabon, Iran, Iraq, Kuwait, Libya, Nigeria, Qatar, Saudi Arabia, United Arab Emirates, Venezuela) to manage the supply of oil. OPEC's power was consolidated in the 1960s and 1970s as various countries nationalized their oil holdings, and wrested decision-making away from the \"Seven Sisters\" (Anglo-Iranian, Socony, Royal Dutch Shell, Gulf, Esso, Texaco, Socal), and created their own oil companies to control the oil. OPEC often tries to influence prices by restricting production. It does this by allocating each member country a quota for production. Members agree to keep prices high by producing at lower levels than they otherwise would. There is no way to enforce adherence to the quota, so each member has an individual incentive to \"cheat\" the cartel.\n\nCommodities trader Raymond Learsy, author of \"Over a Barrel: Breaking the Middle East Oil Cartel\", contends that OPEC has trained consumers to believe that oil is a much more finite resource than it is. To back his argument, he points to past false alarms and apparent collaboration. He also believes that peak oil analysts have conspired with OPEC and the oil companies to create a \"fabricated drama of peak oil\" to drive up oil prices and profits; oil had risen to a little over $30/barrel at that time. A counter-argument was given in the Huffington Post after he and Steve Andrews, co-founder of ASPO, debated on CNBC in June 2007.\n\nIn 1962, Hubbert predicted that world oil production would peak at a rate of 12.5 billion barrels per year, around the year 2000. In 1974, Hubbert predicted that peak oil would occur in 1995 \"if current trends continue\". Those predictions proved incorrect. However, a number of industry leaders and analysts believe that world oil production will peak between 2015 and 2030, with a significant chance that the peak will occur before 2020. They consider dates after 2030 implausible. By comparison, a 2014 analysis of production and reserve data predicted a peak in oil production about 2035. Determining a more specific range is difficult due to the lack of certainty over the actual size of world oil reserves. Unconventional oil is not currently predicted to meet the expected shortfall even in a best-case scenario. For unconventional oil to fill the gap without \"potentially serious impacts on the global economy\", oil production would have to remain stable after its peak, until 2035 at the earliest.\n\nPapers published since 2010 have been relatively pessimistic. A 2010 Kuwait University study predicted production would peak in 2014. A 2010 Oxford University study predicted that production will peak before 2015, but its projection of a change soon \"... from a demand-led market to a supply constrained market ...\" was incorrect. A 2014 validation of a significant 2004 study in the journal \"Energy\" proposed that it is likely that conventional oil production peaked, according to various definitions, between 2005 and 2011. A set of models published in a 2014 Ph.D. thesis predicted that a 2012 peak would be followed by a drop in oil prices, which in some scenarios could turn into a rapid rise in prices thereafter. According to energy blogger Ron Patterson, the peak of world oil production was probably around 2010.\n\nMajor oil companies hit peak production in 2005. Several sources in 2006 and 2007 predicted that worldwide production was at or past its maximum. Fatih Birol, chief economist at the International Energy Agency, also stated that \"crude oil production for the world has already peaked in 2006.\" However, in 2013 OPEC's figures showed that world crude oil production and remaining proven reserves were at record highs. According to Matthew Simmons, former Chairman of Simmons & Company International and author of \"Twilight in the Desert: The Coming Saudi Oil Shock and the World Economy\", \"peaking is one of these fuzzy events that you only know clearly when you see it through a rear view mirror, and by then an alternate resolution is generally too late.\"\n\nThe wide use of fossil fuels has been one of the most important stimuli of economic growth and prosperity since the industrial revolution, allowing humans to participate in takedown, or the consumption of energy at a greater rate than it is being replaced. Some believe that when oil production decreases, human culture and modern technological society will be forced to change drastically. The impact of peak oil will depend heavily on the rate of decline and the development and adoption of effective alternatives.\n\nIn 2005, the United States Department of Energy published a report titled \"Peaking of World Oil Production: Impacts, Mitigation, & Risk Management\". Known as the Hirsch report, it stated, \"The peaking of world oil production presents the U.S. and the world with an unprecedented risk management problem. As peaking is approached, liquid fuel prices and price volatility will increase dramatically, and, without timely mitigation, the economic, social, and political costs will be unprecedented. Viable mitigation options exist on both the supply and demand sides, but to have substantial impact, they must be initiated more than a decade in advance of peaking.\" Some of the information was updated in 2007.\n\nThe oil price historically was comparatively low until the 1973 oil crisis and the 1979 energy crisis when it increased more than tenfold during that six-year timeframe. Even though the oil price dropped significantly in the following years, it has never come back to the previous levels. Oil price began to increase again during the 2000s until it hit historical heights of $143 per barrel (2007 inflation adjusted dollars) on 30 June 2008. As these prices were well above those that caused the 1973 and 1979 energy crises, they contributed to fears of an economic recession similar to that of the early 1980s.\n\nIt is generally agreed that the main reason for the price spike in 2005–2008 was strong demand pressure. For example, global consumption of oil rose from in 2004 to 31 billion in 2005. The consumption rates were far above new discoveries in the period, which had fallen to only eight billion barrels of new oil reserves in new accumulations in 2004.\n\nOil price increases were partially fueled by reports that petroleum production is at or near full capacity. In June 2005, OPEC stated that they would 'struggle' to pump enough oil to meet pricing pressures for the fourth quarter of that year. From 2007 to 2008, the decline in the U.S. dollar against other significant currencies was also considered as a significant reason for the oil price increases, as the dollar lost approximately 14% of its value against the Euro from May 2007 to May 2008.\n\nBesides supply and demand pressures, at times security related factors may have contributed to increases in prices, including the War on Terror, missile launches in North Korea, the Crisis between Israel and Lebanon, nuclear brinkmanship between the U.S. and Iran, and reports from the U.S. Department of Energy and others showing a decline in petroleum reserves.\n\nMore recently, between 2011 and 2014 the price of crude oil was relatively stable, fluctuating around $US100 per barrel. It dropped sharply in late 2014 to below $US70 where it remained for most of 2015. In early 2016 it traded at a low of $US27. The price drop has been attributed to both oversupply and reduced demand as a result of the slowing global economy, OPEC reluctance to concede market share, and a stronger US dollar. These factors may be exacerbated by a combination of monetary policy and the increased debt of oil producers, who may increase production to maintain liquidity.\n\nThis price drop has placed many US tight oil producers under considerable financial pressure. As a result, there has been a reduction by oil companies in capital expenditure of over $US400 billion. It is anticipated that this will have effects on global production in the longer term, leading to statements of concern by the International Energy Agency that governments should not be complacent about energy security. Energy Information Agency projections anticipate market oversupply and prices below $US50 until late 2017.\n\nIn the past, sudden increases in the price of oil have led to economic recessions, such as the 1973 and 1979 energy crises. The effect the increased price of oil has on an economy is known as a price shock. In many European countries, which have high taxes on fuels, such price shocks could potentially be mitigated somewhat by temporarily or permanently suspending the taxes as fuel costs rise. This method of softening price shocks is less useful in countries with much lower gas taxes, such as the United States. A baseline scenario for a recent IMF paper found oil production growing at 0.8% (as opposed to a historical average of 1.8%) would result in a small reduction in economic growth of 0.2–0.4%.\n\nResearchers at the Stanford Energy Modeling Forum found that the economy can adjust to steady, gradual increases in the price of crude better than wild lurches.\n\nSome economists predict that a substitution effect will spur demand for alternate energy sources, such as coal or liquefied natural gas. This substitution can be only temporary, as coal and natural gas are finite resources as well.\n\nPrior to the run-up in fuel prices, many motorists opted for larger, less fuel-efficient sport utility vehicles and full-sized pickups in the United States, Canada, and other countries. This trend has been reversing because of sustained high prices of fuel. The September 2005 sales data for all vehicle vendors indicated SUV sales dropped while small cars sales increased. Hybrid and diesel vehicles are also gaining in popularity.\n\nEIA published Household Vehicles Energy Use: Latest Data and Trends in Nov 2005 illustrating the steady increase in disposable income and $20–30 per barrel price of oil in 2004. The report notes \"The average household spent $1,520 on fuel purchases for transport.\" According to CNBC that expense climbed to $4,155 in 2011.\n\nIn 2008, a report by Cambridge Energy Research Associates stated that 2007 had been the year of peak gasoline usage in the United States, and that record energy prices would cause an \"enduring shift\" in energy consumption practices. The total miles driven in the U.S. peaked in 2006.\n\nThe Export Land Model states that after peak oil petroleum exporting countries will be forced to reduce their exports more quickly than their production decreases because of internal demand growth. Countries that rely on imported petroleum will therefore be affected earlier and more dramatically than exporting countries. Mexico is already in this situation. Internal consumption grew by 5.9% in 2006 in the five biggest exporting countries, and their exports declined by over 3%. It was estimated that by 2010 internal demand would decrease worldwide exports by .\n\nCanadian economist Jeff Rubin has stated that high oil prices are likely to result in increased consumption in developed countries through partial manufacturing de-globalisation of trade. Manufacturing production would move closer to the end consumer to minimise transportation network costs, and therefore a demand decoupling from gross domestic product would occur. Higher oil prices would lead to increased freighting costs and consequently, the manufacturing industry would move back to the developed countries since freight costs would outweigh the current economic wage advantage of developing countries. Economic research carried out by the International Monetary Fund puts overall price elasticity of demand for oil at −0.025 short-term and −0.093 long term.\n\nSince supplies of oil and gas are essential to modern agriculture techniques, a fall in global oil supplies could cause spiking food prices and unprecedented famine in the coming decades. Geologist Dale Allen Pfeiffer contends that current population levels are unsustainable, and that to achieve a sustainable economy and avert disaster the United States population would have to be reduced by at least one-third, and world population by two-thirds.\n\nThe largest consumer of fossil fuels in modern agriculture is ammonia production (for fertilizer) via the Haber process, which is essential to high-yielding intensive agriculture. The specific fossil fuel input to fertilizer production is primarily natural gas, to provide hydrogen via steam reforming. Given sufficient supplies of renewable electricity, hydrogen can be generated without fossil fuels using methods such as electrolysis. For example, the Vemork hydroelectric plant in Norway used its surplus electricity output to generate renewable ammonia from 1911 to 1971.\n\nIceland currently generates ammonia using the electrical output from its hydroelectric and geothermal power plants, because Iceland has those resources in abundance while having no domestic hydrocarbon resources, and a high cost for importing natural gas.\n\nA majority of Americans live in suburbs, a type of low-density settlement designed around universal personal automobile use. Commentators such as James Howard Kunstler argue that because over 90% of transportation in the U.S. relies on oil, the suburbs' reliance on the automobile is an unsustainable living arrangement. Peak oil would leave many Americans unable to afford petroleum based fuel for their cars, and force them to use bicycles or electric vehicles. Additional options include telecommuting, moving to rural areas, or moving to higher density areas, where walking and public transportation are more viable options. In the latter two cases, suburbs may become the \"slums of the future.\" The issue of petroleum supply and demand is also a concern for growing cities in developing countries (where urban areas are expected to absorb most of the world's projected 2.3 billion population increase by 2050). Stressing the energy component of future development plans is seen as an important goal.\n\nRising oil prices, if they occur, would also affect the cost of food, heating, and electricity. A high amount of stress would then be put on current middle to low income families as economies contract from the decline in excess funds, decreasing employment rates. The Hirsch/US DoE Report concludes that \"without timely mitigation, world supply/demand balance will be achieved through massive demand destruction (shortages), accompanied by huge oil price increases, both of which would create a long period of significant economic hardship worldwide.\"\n\nMethods that have been suggested for mitigating these urban and suburban issues include the use of non-petroleum vehicles such as electric cars, battery electric vehicles, transit-oriented development, carfree cities, bicycles, new trains, new pedestrianism, smart growth, shared space, urban consolidation, urban villages, and New Urbanism.\n\nAn extensive 2009 report on the effects of compact development by the United States National Research Council of the Academy of Sciences, commissioned by the United States Congress, stated six main findings. First, that compact development is likely to reduce \"Vehicle Miles Traveled\" (VMT) throughout the country. Second, that doubling residential density in a given area could reduce VMT by as much as 25% if coupled with measures such as increased employment density and improved public transportation. Third, that higher density, mixed-use developments would produce both direct reductions in emissions (from less driving), and indirect reductions (such as from lower amounts of materials used per housing unit, higher efficiency climate control, longer vehicle lifespans, and higher efficiency delivery of goods and services). Fourth, that although short term reductions in energy use and emissions would be modest, that these reductions would become more significant over time. Fifth, that a major obstacle to more compact development in the United States is political resistance from local zoning regulators, which would hamper efforts by state and regional governments to participate in land-use planning. Sixth, the committee agreed that changes in development that would alter driving patterns and building efficiency would have various secondary costs and benefits that are difficult to quantify. The report recommends that policies supporting compact development (and especially its ability to reduce driving, energy use, and emissions) should be encouraged.\n\nAn economic theory that has been proposed as a remedy is the introduction of a steady state economy. Such a system could include a tax shifting from income to depleting natural resources (and pollution), as well as the limitation of advertising that stimulates demand and population growth. It could also include the institution of policies that move away from globalization and toward localization to conserve energy resources, provide local jobs, and maintain local decision-making authority. Zoning policies could be adjusted to promote resource conservation and eliminate sprawl.\n\nSince aviation relies mainly on jet fuels derived from crude oil, commercial aviation has been predicted to go into decline with the global oil production.\n\nTo avoid the serious social and economic implications a global decline in oil production could entail, the Hirsch report emphasized the need to find alternatives, at least ten to twenty years before the peak, and to phase out the use of petroleum over that time. This was similar to a plan proposed for Sweden that same year. Such mitigation could include energy conservation, fuel substitution, and the use of unconventional oil. The timing of mitigation responses is critical. Premature initiation would be undesirable, but if initiated too late could be more costly and have more negative economic consequences.\n\nPermaculture sees peak oil as holding tremendous potential for positive change, assuming countries act with foresight. The rebuilding of local food networks, energy production, and the general implementation of \"energy descent culture\" are argued to be ethical responses to the acknowledgment of finite fossil resources. Majorca is an island currently diversifying its energy supply from fossil fuels to alternative sources and looking back at traditional construction and permaculture methods.\n\nThe Transition Towns movement, started in Totnes, Devon and spread internationally by \"The Transition Handbook\" (Rob Hopkins) and Transition Network, sees the restructuring of society for more local resilience and ecological stewardship as a natural response to the combination of peak oil and climate change.\n\nThe theory of peak oil is controversial and became an issue of political debate in the USA and Europe in the mid-2000s. Critics argued that newly found oil reserves forestalled a peak oil event. Some argued that oil production from new oil reserves and existing fields will continue to increase at a rate that outpaces demand, until alternate energy sources for current fossil fuel dependence are found. In 2015, analysts in the petroleum and financial industries claimed that the \"age of oil\" had already reached a new stage where the excess supply that appeared in late 2014 may continue. \nA consensus was emerging that parties to an international agreement would introduce measures to constrain the combustion of hydrocarbons in an effort to limit global temperature rise to the nominal 2 °C that scientists predicted would limit environmental harm to tolerable levels.\n\nAnother argument against the peak oil theory is reduced demand from various options and technologies substituting oil. US federal funding to develop algae fuels increased since 2000 due to rising fuel prices. Many other projects are being funded in\nAustralia, New Zealand, Europe, the Middle East, and elsewhere and private companies are entering the field.\n\nThe president of Royal Dutch Shell's U.S. operations John Hofmeister, while agreeing that conventional oil production would soon start to decline, criticized the analysis of peak oil theory by Matthew Simmons for being \"overly focused on a single country: Saudi Arabia, the world's largest exporter and OPEC swing producer.\" Hofmeister pointed to the large reserves at the US outer continental shelf, which held an estimated of oil and natural gas. However, only 15% of those reserves were currently exploitable, a good part of that off the coasts of Texas, Louisiana, Mississippi, and Alabama. \n\nHofmeister also pointed to unconventional sources of oil such as the oil sands of Canada, where Shell was active. The Canadian oil sands—a natural combination of sand, water, and oil found largely in Alberta and Saskatchewan—are believed to contain one trillion barrels of oil. Another trillion barrels are also said to be trapped in rocks in Colorado, Utah, and Wyoming, in the form of oil shale. Environmentalists argue that major environmental, social, and economic obstacles would make extracting oil from these areas excessively difficult. Hofmeister argued that if oil companies were allowed to drill more in the United States enough to produce another , oil and gas prices would not be as high as they were in the late 2000s. He thought in 2008 that high energy prices would cause social unrest similar to the 1992 Rodney King riots.\n\nIn 2009, Dr. Christoph Rühl, chief economist of BP, argued against the peak oil\nhypothesis:\nRühl argued that the main limitations for oil availability are \"above ground\" factors such as the availability of staff, expertise, technology, investment security, funds, and global warming, and that the oil question was about price and not the physical availability. \n\nIn 2008, Daniel Yergin of CERA suggest that a recent high price phase might add to a future demise of the oil industry, not of complete exhaustion of resources or an apocalyptic shock but the timely and smooth setup of alternatives. Yergin went on to say, \"This is the fifth time that the world is said to be running out of oil. Each time-whether it was the 'gasoline famine' at the end of WWI or the 'permanent shortage' of the 1970s-technology and the opening of new frontier areas have banished the spectre of decline. There's no reason to think that technology is finished this time.\"\n\nIn 2006, Clive Mather, CEO of Shell Canada, said the Earth's supply of bitumen hydrocarbons was \"almost infinite\", referring to hydrocarbons in oil sands.\n\nIn 2006 attorney and mechanical engineer Peter W. Huber asserted that the world was just running out of \"cheap oil,\" explaining that as oil prices rise, unconventional sources become economically viable. He predicted that, \"[t]he tar sands of Alberta alone contain enough hydrocarbon to fuel the entire planet for over 100 years.\"\n\nEnvironmental journalist George Monbiot responded to a 2012 report by Leonardo Maugeri by suggesting that there is more than enough oil (from unconventional sources) for capitalism to \"deep-fry\" the world with climate change. Stephen Sorrell, senior lecturer Science and Technology Policy Research, Sussex Energy Group, and lead author of the UKERC Global Oil Depletion report, and Christophe McGlade, doctoral researcher at the UCL Energy Institute have criticized Maugeri's assumptions about decline rates.\n\n\n\n\n\n"}
{"id": "44476972", "url": "https://en.wikipedia.org/wiki?curid=44476972", "title": "Peter Bell (actor)", "text": "Peter Bell (actor)\n\nPeter Bell was a British stage actor and producer. In 1951 he appeared opposite Jean Charlesworth and Ronald Radd in a Lionel Hamilton production of \"The Romantic Young Lady\" at the Kettering Savoy. He was employed by the Northampton Repertory Company in the early 1950s, but by 1953 had appeared to have moved on. His wife, Mary Honer, was involved with training young actors on stage in Northampton. In 1950, Bell and Jack Livesey produced youth productions of Stanley Houghton's comedy \"The Dear Departed\" and Ian Haly's farce \"The Crimson Coconut\" at Towcester Town Hall.\n"}
{"id": "6715322", "url": "https://en.wikipedia.org/wiki?curid=6715322", "title": "Phasm", "text": "Phasm\n\nIn the \"Dungeons & Dragons\" fantasy role-playing game, the phasm is an aberration.\n\nThe phasm appeared in the third edition \"Monster Manual\" (2000), and in the 3.5 revised \"Monster Manual\" (2003).\n\nA phasm resembles an amorphous blob of giant, brightly colored cells, organs, bacteria and ocean plants, roughly 5 feet in diameter and 2 feet tall, but can assume the form of almost any object or creature by using their innards as a cast. (The creature may be a bizarre form of life which has developed its organ system and cells, but has yet to obtain a permanent outer form.) It lives in underground caves. \n\nThe phasm, though classified as an aberration, has similar habits to an ooze when in normal form. When harassed or pursued, however, it morphs and develops the form of the deadliest creature it knows, and then attacks in whatever way it can in that form. When in natural form, it can attack with a pseudopod. The phasm is highly valued for various medical reasons and for study.\n\nPhasms can speak Common, but prefer to communicate telepathically.\n\nThey are chaotic neutral in alignment.\n\nThe phasm is a shapechanger, able to assume the form of any corporeal creature. It can also communicate telepathically.\n"}
{"id": "57559", "url": "https://en.wikipedia.org/wiki?curid=57559", "title": "Predation", "text": "Predation\n\nPredation is a biological interaction where one organism, the predator, kills and eats another organism, its prey. It is one of a family of common feeding behaviours that includes parasitism and micropredation (which usually do not kill the host) and parasitoidism (which always does, eventually). It is distinct from scavenging on dead prey, though many predators also scavenge; it overlaps with herbivory, as a seed predator is both a predator and a herbivore.\n\nPredators may actively search for prey or sit and wait for it. When prey is detected, the predator assesses whether to attack it. This may involve ambush or pursuit predation, sometimes after stalking the prey. If the attack is successful, the predator kills the prey, removes any inedible parts like the shell or spines, and eats it.\n\nPredators are adapted and often highly specialized for hunting, with acute senses such as vision, hearing, or smell. Many predatory animals, both vertebrate and invertebrate, have sharp claws or jaws to grip, kill, and cut up their prey. Other adaptations include stealth and aggressive mimicry that improve hunting efficiency. \n\nPredation has a powerful selective effect on prey, and the prey develop antipredator adaptations such as warning coloration, alarm calls and other signals, camouflage, mimicry of well-defended species, and defensive spines and chemicals. Sometimes predator and prey find themselves in an evolutionary arms race, a cycle of adaptations and counter-adaptations. Predation has been a major driver of evolution since at least the Cambrian period.\n\nAt the most basic level, predators kill and eat other organisms. However, the concept of predation is broad, defined differently in different contexts, and includes a wide variety of feeding methods; and some relationships that result in the prey's death are not generally called predation. A parasitoid, such as an ichneumon wasp, lays its eggs in or on its host; the eggs hatch into larvae, which eat the host, and it inevitably dies. Zoologists generally call this a form of parasitism, though conventionally parasites are thought not to kill their hosts. A predator can be defined to differ from a parasitoid in two ways: it kills its prey immediately; and it has many prey, captured over its lifetime, where a parasitoid's larva has just one, or at least has its food supply provisioned for it on just one occasion. \n\nThere are other difficult and borderline cases. Micropredators are small animals that, like predators, feed entirely on other organisms; they include fleas and mosquitoes that consume blood from living animals, and aphids that consume sap from living plants. However, since they typically do not kill their hosts, they are now often thought of as parasites. Animals that graze on phytoplankton or mats of microbes are predators, as they consume and kill their food organisms; but herbivores that browse leaves are not, as their food plants usually survive the assault. However, when animals eat seeds (\"seed predation\" or \"granivory\") or eggs (\"egg predation\"), they are consuming entire living organisms, which by definition makes them predators, albeit unconventional ones: for instance, a mouse that eats grass seeds has no adaptations for tracking, catching and subduing prey and its teeth are not adapted to slicing through flesh.\n\nScavengers, organisms that only eat organisms found already dead, are not predators, but many predators such as the jackal and the hyena scavenge when the opportunity arises. Among invertebrates, social wasps (yellowjackets) are both hunters and scavengers of other insects.\n\nWhile examples of predators among mammals and birds are well known, predators can be found in a broad range of taxa. They are common among insects, including mantids, dragonflies, lacewings and scorpionflies. In some species such as the alderfly, only the larvae are predatory (the adults do not eat). Spiders are predatory, as well as other terrestrial invertebrates such as scorpions; centipedes; some mites, snails and slugs; nematodes; and planarian worms. In marine environments, most cnidarians (e.g., jellyfish, hydroids), ctenophora (comb jellies), echinoderms (e.g., sea stars, sea urchins, sand dollars, and sea cucumbers) and flatworms are predatory. Among crustaceans, lobsters, crabs, shrimps and barnacles are predators, and in turn crustaceans are preyed on by nearly all cephalopods (including octopuses, squid and cuttlefish).\n\nSeed predation is restricted to mammals, birds, and insects and is found in almost all terrestrial ecosystems. Egg predation includes both specialist egg predators such as some colubrid snakes and generalists such as foxes and badgers that opportunistically take eggs when they find them.\n\nSome plants, like the pitcher plant, the Venus fly trap and the sundew, are carnivorous and consume insects. Some carnivorous fungi catch nematodes using either active traps in the form of constricting rings, or passive traps with adhesive structures. \n\nMany species of protozoa (eukaryotes) and bacteria (prokaryotes) prey on other microorganisms; the feeding mode is evidently ancient, and evolved many times in both groups. Among freshwater and marine zooplankton, whether single-celled or multi-cellular, predatory grazing on phytoplankton and smaller zooplankton is common, and found in many species of nanoflagellates, dinoflagellates, ciliates, rotifers, a diverse range of meroplankton animal larvae, and two groups of crustaceans, namely copepods and cladocerans.\n\nTo feed, a predator must search for, pursue and kill its prey. These actions form a foraging cycle. The predator must decide where to look for prey based on its geographical distribution; and once it has located prey, it must assess whether to pursue it or to wait for a better choice. If it chooses pursuit, its physical capabilities determine the mode of pursuit (e.g., ambush or chase). Having captured the prey, it may also need to expend energy \"handling\" it (e.g., killing it, removing any shell or spines, and ingesting it).\n\nPredators have a choice of search modes ranging from \"sit-and-wait\" to \"active\" or \"widely foraging\". The sit-and-wait method is most suitable if the prey are dense and mobile, and the predator has low energy requirements. Wide foraging expends more energy, and is used when prey is sedentary or sparsely distributed. There is a continuum of search modes with intervals between periods of movement ranging from seconds to months. Sharks, sunfish, Insectivorous birds and shrews are almost always moving while web-building spiders, aquatic invertebrates, praying mantises and kestrels rarely move. In between, plovers and other shorebirds, freshwater fish including crappies, and the larvae of coccinellid beetles (ladybirds), alternate between actively searching and scanning the environment.\n\nPrey distributions are often clumped, and predators respond by looking for \"patches\" where prey is dense and then searching within patches. Where food is found in patches, such as rare shoals of fish in a nearly empty ocean, the search stage requires the predator to travel for a substantial time, and to expend a significant amount of energy, to locate each food patch. For example, the black-browed albatross regularly makes foraging flights to a range of around , up to a maximum foraging range of for breeding birds gathering food for their young. With static prey, some predators can learn suitable patch locations and return to them at intervals to feed. The optimal foraging strategy for search has been modelled using the marginal value theorem.\n\nSearch patterns often appear random. One such is the Lévy walk, that tends to involve clusters of short steps with occasional long steps. It is a good fit to the behaviour of a wide variety of organisms including bacteria, honeybees, sharks and human hunter-gatherers.\n\nHaving found prey, a predator must decide whether to pursue it or keep searching. The decision depends on the costs and benefits involved. A bird foraging for insects spends a lot of time searching but capturing and eating them is quick and easy, so the efficient strategy for the bird is to eat every palatable insect it finds. By contrast, a predator such as a lion or falcon finds its prey easily but capturing it requires a lot of effort. In that case, the predator is more selective. \n\nOne of the factors to consider is size. Prey that is too small may not be worth the trouble for the amount of energy it provides. Too large, and it may be too difficult to capture. For example, a mantid captures prey with its forelegs and they are optimized for grabbing prey of a certain size. Mantids are reluctant to attack prey that is far from that size. There is a positive correlation between the size of a predator and its prey.\n\nA predator may also assess a patch and decide whether to spend time searching for prey in it. This may involve some knowledge of the preferences of the prey; for example, ladybirds can choose a patch of vegetation suitable for their aphid prey.\n\nTo capture prey, predators have a spectrum of pursuit modes that range from overt chase (also known as \"pursuit predation\") to a sudden strike on nearby prey (\"ambush predation\"). Another strategy in between ambush and pursuit is \"ballistic interception\", where a predator observes and predicts a prey's motion and then launches its attack accordingly.\n\nAmbush or sit-and-wait predators are carnivorous animals that capture prey by stealth or surprise. In animals, ambush predation is characterized by the predator's scanning the environment from a concealed position until a prey is spotted, and then rapidly executing a fixed surprise attack. Vertebrate ambush predators include frogs, fish such as the angel shark, the northern pike and the eastern frogfish. Among the many invertebrate ambush predators are trapdoor spiders on land and mantis shrimps in the sea. Ambush predators often construct a burrow in which to hide, improving concealment at the cost of reducing their field of vision. Some ambush predators also use lures to attract prey within striking range. The capturing movement has to be rapid to trap the prey, given that the attack is not modifiable once launched.\nBallistic interception is the strategy where a predator observes the movement of a prey, predicts its motion, works out an interception path, and then attacks the prey on that path. This differs from ambush predation in that the predator adjusts its attack according to how the prey is moving. Ballistic interception involves a brief period for planning, giving the prey an opportunity to escape. Some frogs wait until snakes have begun their strike before jumping, reducing the time available to the snake to recalibrate its attack, and maximising the angular adjustment that the snake would need to make to intercept the frog in real time. Ballistic predators include insects such as dragonflies, and vertebrates such as archerfish (attacking with a jet of water), chameleons (attacking with their tongues), and some colubrid snakes.\n\nIn pursuit predation, predators chase fleeing prey. If the prey flees in a straight line, capture depends only on the predator's being faster than the prey. If the prey manoeuvres by turning as it flees, the predator must react in real time to calculate and follow a new intercept path, such as by parallel navigation, as it closes on the prey. Many pursuit predators use camouflage to approach the prey as close as possible unobserved (\"stalking\") before starting the pursuit. Pursuit predators include terrestrial mammals such as lions, cheetahs, and wolves; marine predators such as dolphins and many predatory fishes, such as tuna; predatory birds (raptors) such as falcons; and insects such as dragonflies.\n\nAn extreme form of pursuit is endurance or persistence hunting, in which the predator tires out the prey by following it over a long distance, sometimes for hours at a time. The method is used by human hunter-gatherers and in canids such as African wild dogs and domestic hounds. The African wild dog is an extreme persistence predator, tiring out individual prey by following them for many miles at relatively low speed, compared for example to the cheetah's brief high-speed pursuit.\n\nA specialised form of pursuit predation is the lunge feeding of baleen whales. These very large marine predators feed on plankton, especially krill, diving and actively swimming into concentrations of plankton, and then taking a huge gulp of water and filtering it through their feathery baleen plates.\n\nPursuit predators may be social, like the lion and wolf that hunt in groups, or solitary, like the cheetah.\n\nOnce the predator has captured the prey, it has to handle it: very carefully if the prey is dangerous to eat, such as if it possesses sharp or poisonous spines, as in many prey fish. Some catfish such as the Ictaluridae have spines on the back (dorsal) and belly (pectoral) which lock in the erect position; as the catfish thrashes about when captured, these could pierce the predator's mouth, possibly fatally. Some fish-eating birds like the osprey avoid the danger of spines by tearing up their prey before eating it.\n\nIn social predation, a group of predators cooperates to kill prey. This makes it possible to kill creatures larger than those they could overpower singly; for example, hyenas, and wolves collaborate to catch and kill herbivores as large as buffalo, and lions even hunt elephants. It can also make prey more readily available through strategies like flushing of prey and herding it into a smaller area. For example, when mixed flocks of birds forage, the birds in front flush out insects that are caught by the birds behind. Spinner dolphins form a circle around a school of fish and move inwards, concentrating the fish by a factor of 200. By hunting socially chimpanzees can catch colobus monkeys that would readily escape an individual hunter, while cooperating Harris hawks can trap rabbits. \n\nPredators of different species sometimes cooperate to catch prey. In coral reefs, when fish such as the grouper and coral trout spot prey that is inaccessible to them, they signal to giant moray eels, Napoleon wrasses or octopuses. These predators are able to access small crevices and flush out the prey. Killer whales have been known to help whalers hunt baleen whales.\n\nSocial hunting allows predators to tackle a wider range of prey, but at the risk of competition for the captured food. Solitary predators have more chance of eating what they catch, at the price of increased expenditure of energy to catch it, and increased risk that the prey will escape. Ambush predators are often solitary to reduce the risk of becoming prey themselves. Of 245 terrestrial carnivores, 177 are solitary; and 35 of the 37 wild cats are solitary, including the cougar and cheetah. However, the solitary cougar does allow other cougars to share in a kill, and the coyote can be either solitary or social. Other solitary predators include the northern pike, wolf spiders and all the thousands of species of solitary wasps among arthropods, and many microorganisms and zooplankton.\n\nUnder the pressure of natural selection, predators have evolved a variety of physical adaptations for detecting, catching, killing, and digesting prey. These include speed, agility, stealth, sharp senses, claws, teeth, filters, and suitable digestive systems.\n\nFor detecting prey, predators have well-developed vision, smell, or hearing. Predators as diverse as owls and jumping spiders have forward-facing eyes, providing accurate binocular vision over a relatively narrow field of view, whereas prey animals often have less acute all-round vision. Animals such as foxes can smell their prey even when it is concealed under of snow or earth. Many predators have acute hearing, and some such as echolocating bats hunt exclusively by active or passive use of sound.\n\nPredators including big cats, birds of prey, and ants share powerful jaws, sharp teeth, or claws which they use to seize and kill their prey. Some predators such as snakes and fish-eating birds like herons and cormorants swallow their prey whole; some snakes can unhinge their jaws to allow them to swallow large prey, while fish-eating birds have long spear-like beaks that they use to stab and grip fast-moving and slippery prey. Fish and other predators have developed the ability to crush or open the armoured shells of molluscs.\n\nMany predators are powerfully built and can catch and kill animals larger than themselves; this applies as much to small predators such as ants and shrews as to big and visibly muscular carnivores like the cougar and lion. \n\nPredators are often highly specialized in their diet and hunting behaviour; for example, the Eurasian lynx only hunts small ungulates. Others such as leopards are more opportunistic generalists, preying on at least 100 species. The specialists may be highly adapted to capturing their preferred prey, whereas generalists may be better able to switch to other prey when a preferred target is scarce. When prey have a clumped (uneven) distribution, the optimal strategy for the predator is predicted to be more specialized as the prey are more conspicuous and can be found more quickly; this appears to be correct for predators of immobile prey, but is doubtful with mobile prey.\nIn size-selective predation, predators select prey of a certain size. Large prey may prove troublesome for a predator, while small prey might prove hard to find and in any case provide less of a reward. This has led to a correlation between the size of predators and their prey. Size may also act as a refuge for large prey. For example, adult elephants are relatively safe from predation by lions, but juveniles are vulnerable.\n\nMembers of the cat family such as the snow leopard (treeless highlands), tiger (grassy plains, reed swamps), ocelot (forest), fishing cat (waterside thickets), and lion (open plains) are camouflaged with coloration and disruptive patterns suiting their habitats.\n\nIn aggressive mimicry, certain predators, including insects and fishes, make use of coloration and behaviour to attract prey. Female \"Photuris\" fireflies, for example, copy the light signals of other species, thereby attracting male fireflies, which they capture and eat. Flower mantises are ambush predators; camouflaged as flowers, such as orchids, they attract prey and seize it when it is close enough. Frogfishes are extremely well camouflaged, and actively lure their prey to approach using an esca, a bait on the end of a rod-like appendage on the head, which they wave gently to mimic a small animal, gulping the prey in an extremely rapid movement when it is within range.\n\nMany smaller predators such as the box jellyfish use venom to subdue their prey, and venom can also aid in digestion (as is the case for rattlesnakes and some spiders). The marbled sea snake that has adapted to egg predation has atrophied venom glands, and the gene for its three finger toxin contains a mutation (the deletion of two nucleotides) that inactives it. These changes are explained by the fact that its prey does not need to be subdued.\n\nPhysiological adaptations to predation include the ability of predatory bacteria to digest the complex peptidoglycan polymer from the cell walls of the bacteria that they prey upon. Carnivorous vertebrates of all five major classes (fishes, amphibians, reptiles, birds, and mammals) have lower relative rates of sugar to amino acid transport than either herbivores or omnivores, presumably because they acquire plenty of amino acids from the animal proteins in their diet.\n\nTo counter predation, prey have a great variety of defences. They can try to avoid detection. They can detect predators and warn others of their presence. If detected, they can try to avoid being the target of an attack, for example, by signalling that a chase would be unprofitable or by forming groups. If they become a target, they can try to fend off the attack with defences such as armour, quills, unpalatability or mobbing; and they can escape an attack in progress by startling the predator, shedding body parts such as tails, or simply fleeing.\n\nPrey can avoid detection by predators with morphological traits and coloration that make them hard to detect. They can also adopt behaviour that avoids predators by, for example, avoiding the times and places where predators forage.\n\nPrey animals make use of a variety of mechanisms including camouflage and mimicry to misdirect the visual sensory mechanisms of predators, enabling the prey to remain unrecognized for long enough to give it an opportunity to escape. Camouflage delays recognition through coloration, shape, and pattern. Among the many mechanisms of camouflage are countershading and disruptive coloration. The resemblance can be to the biotic or non-living environment, such as a mantis resembling dead leaves, or to other organisms. In mimicry, an organism has a similar appearance to another species, as in the drone fly, which resembles a bee yet has no sting.\n\nAnimals avoid predators with behavioural mechanisms such as changing their habitats (particularly when raising young), reducing their activity, foraging less and forgoing reproduction when they sense that predators are about.\n\nEggs and nestlings are particularly vulnerable to predation, so birds take measures to protect their nests. Where birds locate their nests can have a large effect on the frequency of predation. It is lowest for those such as woodpeckers that excavate their own nests and progressively higher for those on the ground, in canopies and in shrubs. To compensate, shrub nesters must have more broods and shorter nesting times. Birds also choose appropriate habitat (e.g., thick foliage or islands) and avoid forest edges and small habitats. Similarly, some mammals raise their young in dens.\n\nBy forming groups, prey can often reduce the frequency of encounters with predators because the visibility of a group does not rise in proportion to its size. However, there are exceptions: for example, human fishermen can only detect large shoals of fish with sonar.\n\nPrey species use sight, sound and odor to detect predators, and they can be quite discriminating. For example, Belding's ground squirrel can distinguish several aerial and ground predators from each other and from harmless species. Prey also distinguish between the calls of predators and non-predators. Some species can even distinguish between dangerous and harmless predators of the same species. In the northeastern Pacific Ocean, transient killer whales prey on seals, but the local killer whales only eat fish. Seals rapidly exit the water if they hear calls between transients. Prey are also more vigilant if they smell predators.\n\nThe abilities of prey to detect predators do have limits. Belding's ground squirrel cannot distinguish between harriers flying at different heights, although only the low-flying birds are a threat. Wading birds sometimes take flight when there does not appear to be any predator present. Although such false alarms waste energy and lose feeding time, it can be fatal to make the opposite mistake of taking a predator for a harmless animal.\n\nPrey must remain \"vigilant\", scanning their surroundings for predators. This makes it more difficult to feed and sleep. Groups can provide more eyes, making detection of a predator more likely and reducing the level of vigilance needed by individuals. Many species, such as Eurasian jays, give alarm calls warning of the presence of a predator; these give other prey of the same or different species an opportunity to escape, and signal to the predator that it has been detected.\n\nIf predator and prey have spotted each other, the prey can signal to the predator to decrease the likelihood of an attack. These \"honest signals\" may benefit both the prey and predator, because they save the effort of a fruitless chase. Signals that appear to deter attacks include stotting, for example by Thomson's gazelle; push-up displays by lizards; and good singing by skylarks after a pursuit begins. Simply indicating that the predator has been spotted, as a hare does by standing on its hind legs and facing the predator, may sometimes be sufficient. \n\nMany prey animals are aposematically coloured or patterned as a warning to predators that they are distasteful or able to defend themselves. Such distastefulness or toxicity is brought about by chemical defences, found in a wide range of prey, especially insects, but the skunk is a dramatic mammalian example.\n\nBy forming groups, prey can reduce attacks by predators. There are several mechanisms that produce this effect. One is \"dilution\", where, in the simplest scenario, if a given predator attacks a group of prey, the chances of a given individual being the target is reduced in proportion to the size of the group. However, it is difficult to separate this effect from other group-related benefits such as increased vigilance and reduced encounter rate. Other advantages include confusing predators such as with motion dazzle, making it more difficult to single out a target.\n\nChemical defences include toxins, such as bitter compounds in leaves absorbed by leaf-eating insects, are used to dissuade potential predators. Mechanical defences include sharp spines, hard shells and tough leathery skin or exoskeletons, all making prey harder to kill.\n\nSome species mob predators cooperatively, reducing the likelihood of attack.\n\nWhen a predator is approaching an individual and attack seems imminent, the prey still has several options. One is to flee, whether by running, jumping, climbing, burrowing or swimming. The prey can gain some time by startling the predator. Many butterflies and moths have eyespots, wing markings that resemble eyes. When a predator disturbs the insect, it reveals its hind wings in a in a deimatic or bluffing display, startling the predator and giving the insect time to escape. Some other strategies include playing dead and uttering a distress call.\n\nPredators and prey are natural enemies, and many of their adaptations seem designed to counter each other. For example, bats have sophisticated echolocation systems to detect insects and other prey, and insects have developed a variety of defences including the ability to hear the echolocation calls. Many pursuit predators that run on land, such as wolves, have evolved long limbs in response to the increased speed of their prey. Their adaptations have been characterized as an evolutionary arms race, an example of the coevolution of two species. In a gene centered view of evolution, the genes of predator and prey can be thought of as competing for the prey's body. However, the \"life-dinner\" principle of Dawkins and Krebs predicts that this arms race is asymmetric: if a predator fails to catch its prey, it loses its dinner, while if it succeeds, the prey loses its life.\n\nThe metaphor of an arms race implies ever-escalating advances in attack and defence. However, these adaptations come with a cost; for instance, longer legs have an increased risk of breaking, while the specialized tongue of the chameleon, with its ability to act like a projectile, is useless for lapping water, so the chameleon must drink dew off vegetation.\n\nThe \"life-dinner\" principle has been criticized on multiple grounds. The extent of the asymmetry in natural selection depends in part on the heritability of the adaptive traits. Also, if a predator loses enough dinners, it too will lose its life. On the other hand, the fitness cost of a given lost dinner is unpredictable, as the predator may quickly find better prey. In addition, most predators are generalists, which reduces the impact of a given prey adaption on a predator. Since specialization is caused by predator-prey coevolution, the rarity of specialists may imply that predator-prey arms races are rare.\n\nIt is difficult to determine whether given adaptations are truly the result of coevolution, where a prey adaptation gives rise to a predator adaptation that is countered by further adaptation in the prey. An alternative explanation is \"escalation\", where predators are adapting to competitors, their own predators or dangerous prey. Apparent adaptations to predation may also have arisen for other reasons and then been co-opted for attack or defence. In some of the insects preyed on by bats, hearing evolved before bats appeared and was used to hear signals used for territorial defence and mating. Their hearing evolved in response to bat predation, but the only clear example of reciprocal adaptation in bats is stealth echolocation.\n\nA more symmetric arms race may occur when the prey are dangerous, having spines, quills, toxins or venom that can harm the predator. The predator can respond with avoidance, which in turn drives the evolution of mimicry. Avoidance is not necessarily an evolutionary response as it is generally learned from bad experiences with prey. However, when the prey is capable of killing the predator (as can a coral snake with its venom), there is no opportunity for learning and avoidance must be inherited. Predators can also respond to dangerous prey with counter-adaptations. In western North America, the common garter snake has developed a resistance to the toxin in the skin of the rough-skinned newt.\n\nOne way of classifying predators is by trophic level. Carnivores that feed on herbivores are secondary consumers; their predators are tertiary consumers, and so forth. At the top of this food chain are apex predators such as lions. Many predators however eat from multiple levels of the food chain; a carnivore may eat both secondary and tertiary consumers.\n\nPredators must also contend with intraguild predation, where other predators kill and eat them. For example, coyotes compete with and sometimes kill gray foxes and bobcats.\n\nPredators may increase the biodiversity of communities by preventing a single species from becoming dominant. Such predators are known as keystone species and may have a profound influence on the balance of organisms in a particular ecosystem. Introduction or removal of this predator, or changes in its population density, can have drastic cascading effects on the equilibrium of many other populations in the ecosystem. For example, grazers of a grassland may prevent a single dominant species from taking over.\n\nThe elimination of wolves from Yellowstone National Park had profound impacts on the trophic pyramid. In that area, wolves are both keystone species and apex predators. Without predation, herbivores began to over-graze many woody browse species, affecting the area's plant populations. In addition, wolves often kept animals from grazing near streams, protecting the beavers' food sources. The removal of wolves had a direct effect on the beaver population, as their habitat became territory for grazing. Increased browsing on willows and conifers along Blacktail Creek due to a lack of predation caused channel incision because the reduced beaver population was no longer able to slow the water down and keep the soil in place. The predators were thus demonstrated to be of vital importance in the ecosystem.\n\nIn the absence of predators, the population of a species can grow exponentially until it approaches the carrying capacity of the environment. Predators limit the growth of prey both by consuming them and by changing their behavior. Increases or decreases in the prey population can also lead to increases or decreases in the number of predators, for example, through an increase in the number of young they bear.\n\nCyclical fluctuations have been seen in populations of predator and prey, often with offsets between the predator and prey cycles. A well-known example is that of the snowshoe hare and lynx. Over a broad span of boreal forests in Alaska and Canada, the hare populations fluctuate in near synchrony with a 10-year period, and the lynx populations fluctuate in response. This was first seen in historical records of animals caught by fur hunters for the Hudson Bay Company over more than a century.\n\nA simple model of a system with one species each of predator and prey, the Lotka–Volterra equations, predicts population cycles. However, attempts to reproduce the predictions of this model in the laboratory have often failed; for example, when the protozoan \"Didinium nasutum\" is added to a culture containing its prey, \"Paramecium caudatum\", the latter is often driven to extinction.\n\nThe Lotka-Volterra equations rely on several simplifying assumptions, and they are structurally unstable, meaning that any change in the equations can stabilize or destabilize the dynamics. For example, one assumption is that predators have a linear functional response to prey: the rate of kills increases in proportion to the rate of encounters. If this rate is limited by time spent handling each catch, then prey populations can reach densities above which predators cannot control them. Another assumption is that all prey individuals are identical. In reality, predators tend to select young, weak, and ill individuals, leaving prey populations able to regrow.\n\nMany factors can stabilize predator and prey populations. One example is the presence of multiple predators, particularly generalists that are attracted to a given prey species if it is abundant and look elsewhere if it is not. As a result, population cycles are only found in northern temperate and subarctic ecosystems because the food webs are simpler. The snowshoe hare-lynx system is subarctic, but even this involves other predators, including coyotes, goshawks and great horned owls, and the cycle is reinforced by variations in the food available to the hares.\n\nMany mathematical models have been developed by relaxing the assumptions made in the Lotka-Volterra model; these variously allow animals to have geographic distributions, or to migrate; to have differences between individuals, such as sexes and an age structure, so that only some individuals reproduce; to live in a varying environment, such as with changing seasons; and analysing the interactions of more than just two species at once. Such models predict widely differing and often chaotic predator-prey population dynamics. The presence of refuge areas, where prey are safe from predators, may enable prey to maintain larger populations but may also destabilize the dynamics.\n\nPredation predates the rise of commonly recognized carnivores by hundreds of millions (perhaps billions) of years. Predation has evolved repeatedly in different groups of organisms. The rise of eukaryotic cells at around 2.7 Gya, the rise of multicellular organisms at about 2 Gya, and the rise of mobile predators (around 600 Mya - 2 Gya, probably around 1 Gya) have all been attributed to early predatory behavior,and many very early remains show evidence of boreholes or other markings attributed to small predator species. It likely triggered major evolutionary transitions including the arrival of cells, eukaryotes, sexual reproduction, multicellularity, increased size, mobility (including insect flight) and armoured shells and exoskeletons. \n\nThe earliest predators were microbial organisms, which engulfed or grazed on others. Because the fossil record is poor, these first predators could date back anywhere between 1 and over 2.7 Gya (billion years ago). Predation visibly became important shortly before the Cambrian period—around —as evidenced by the almost simultaneous development of calcification in animals and algae, and predation-avoiding burrowing. However, predators had been grazing on micro-organisms since at least , with evidence of selective (rather than random) predation from a similar time. \n\nThe fossil record demonstrates a long history of interactions between predators and their prey from the Cambrian period onwards, showing for example that some predators drilled through the shells of bivalve and gastropod molluscs, while others ate these organisms by breaking their shells.\nAmong the Cambrian predators were invertebrates like the anomalocaridids with appendages suitable for grabbing prey, large compound eyes and jaws made of a hard material like that in the exoskeleton of an insect.\nSome of the first fish to have jaws were the armoured and mainly predatory placoderms of the Silurian to Devonian periods, one of which, the \"Dunkleosteus\", is considered the world's first vertebrate \"superpredator\", preying upon other predators.\nInsects developed the ability to fly in the Early Carboniferous or Late Devonian, enabling them among other things to escape from predators.\nAmong the largest predators that have ever lived were the theropod dinosaurs such as \"Tyrannosaurus\" from the Cretaceous period. They preyed upon herbivorous dinosaurs such as hadrosaurs, ceratopsians and ankylosaurs.\n\nHumans are to some extent predatory, using weapons and tools to fish, hunt and trap animals. They also use other predatory species such as dogs, cormorants, and falcons to catch prey for food or for sport. \nTwo mid-sized predators, dogs and cats, are the animals most often kept as pets in western societies.\nNeolithic hunters, including the San of southern Africa, used persistence hunting, a form of pursuit predation where the pursuer may be slower than prey such as a kudu antelope over short distances, but follows it in the midday heat until it is exhausted, a pursuit that can take up to five hours. \nIn biological pest control, predators (and parasitoids) from a pest's natural range are introduced to control populations, at the risk of causing unforeseen problems. Natural predators, provided they do no harm to non-pest species, are an environmentally friendly and sustainable way of reducing damage to crops and an alternative to the use of chemical agents such as pesticides.\n\nIn film, the idea of the predator as a dangerous if humanoid enemy is used in the 1987 science fiction horror action film \"Predator\" and its three sequels. A terrifying predator, a gigantic man-eating great white shark, is central, too, to Steven Spielberg's 1974 thriller \"Jaws\".\n\nIn poetry, Ted Hughes's vigorous writings on animals, such as \"Pike\", imaginatively explore a predator's consciousness. \n\nIn mythology and folk fable, predators such as the fox and wolf have mixed reputations. The fox was a symbol of fertility in ancient Greece, but a weather demon in northern Europe, and a creature of the devil in early Christianity; the fox is sly, greedy, and cunning in fables from Aesop onwards. The big bad wolf is known to children in tales such as \"Little Red Riding Hood\", but is a demonic figure in the Icelandic Edda sagas, where the wolf Fenrir appears in the apocalyptic ending of the world. In the middle ages, belief spread in werewolves, men transformed into wolves. In ancient Rome, and in ancient Egypt, the wolf was worshipped, the she-wolf appearing in the founding myth of Rome, suckling Romulus and Remus. More recently, in Rudyard Kipling's 1894 \"The Jungle Book\", Mowgli is raised by the wolf pack. Attitudes to large predators in North America, such as wolf, grizzly bear and cougar, have shifted from hostility or ambivalence, accompanied by active persecution, towards positive and protective in the second half of the 20th century.\n\n\n"}
{"id": "25964", "url": "https://en.wikipedia.org/wiki?curid=25964", "title": "Revolution", "text": "Revolution\n\nIn political science, a revolution (Latin: \"revolutio\", \"a turn around\") is a fundamental and relatively sudden change in political power and political organization which occurs when the population revolt against the government, typically due to perceived oppression (political, social, economic). In book V of the \"Politics\", the Ancient Greek philosopher Aristotle (384–322 BC) described two types of political revolution:\n\nRevolutions have occurred through human history and vary widely in terms of methods, duration and motivating ideology. Their results include major changes in culture, economy and socio-political institutions, usually in response to perceived overwhelming autocracy or plutocracy.\n\nScholarly debates about what does and does not constitute a revolution center on several issues. Early studies of revolutions primarily analyzed events in European history from a psychological perspective, but more modern examinations include global events and incorporate perspectives from several social sciences, including sociology and political science. Several generations of scholarly thought on revolutions have generated many competing theories and contributed much to the current understanding of this complex phenomenon.\n\nNotable revolutions during later centuries include the creation of the United States through the American Revolutionary War (1775-1783), the French Revolution (1789-1799), the 1848 European Revolutions, The Russian Revolutions in March and November 1917. \nThe word \"revolucion\" is known in French from the 13th century, and \"revolution\" in English by the late fourteenth century, with regard to the revolving motion of celestial bodies. \"Revolution\" in the sense of representing abrupt change in a social order is attested by at least 1450. Political usage of the term had been well established by 1688 in the description of the replacement of James II with William III. This incident was termed the \"Glorious Revolution\".\nThere are many different typologies of revolutions in social science and literature.\n\nAlexis de Tocqueville differentiated between; \n\nOne of several different Marxist typologies divides revolutions into; \n\nCharles Tilly, a modern scholar of revolutions, differentiated between; \n\nMark Katz identified six forms of revolution;\n\nThese categories are not mutually exclusive; the Russian revolution of 1917 began with urban revolution to depose the Czar, followed by rural revolution, followed by the Bolshevik coup in November. Katz also cross-classified revolutions as follows;\n\nA further dimension to Katz's typology is that revolutions are either against (anti-monarchy, anti-dictatorial, anti-communist, anti-democratic) or for (pro-fascism, communism, nationalism etc.).In the latter cases, a transition period is often necessary to decide on the direction taken.\n\nOther types of revolution, created for other typologies, include the social revolutions; proletarian or communist revolutions (inspired by the ideas of Marxism that aims to replace capitalism with Communism); failed or abortive revolutions (revolutions that fail to secure power after temporary victories or large-scale mobilization); or violent vs. nonviolent revolutions.\n\nThe term \"revolution\" has also been used to denote great changes outside the political sphere. Such revolutions are usually recognized as having transformed in society, culture, philosophy, and technology much more than political systems; they are often known as social revolutions. Some can be global, while others are limited to single countries. One of the classic examples of the usage of the word \"revolution\" in such context is the Industrial Revolution, or the Commercial Revolution. Note that such revolutions also fit the \"slow revolution\" definition of Tocqueville.\nA similar example is the Digital Revolution.\n\nPerhaps most often, the word \"revolution\" is employed to denote a change in social and political institutions. Jeff Goodwin gives two definitions of a revolution. First, a broad one, including\nany and all instances in which a state or a political regime is overthrown and thereby transformed by a popular movement in an irregular, extraconstitutional and/or violent fashion.\nSecond, a narrow one, in which\n\nrevolutions entail not only mass mobilization and regime change, but also more or less rapid and fundamental social, economic and/or cultural change, during or soon after the struggle for state power.\n\nJack Goldstone defines a revolution as\nan effort to transform the political institutions and the justifications for political authority in society, accompanied by formal or informal mass mobilization and non-institutionalized actions that undermine authorities.\n\nPolitical and socioeconomic revolutions have been studied in many social sciences, particularly sociology, political sciences and history. Among the leading scholars in that area have been or are Crane Brinton, Charles Brockett, Farideh Farhi, John Foran, John Mason Hart, Samuel Huntington, Jack Goldstone, Jeff Goodwin, Ted Roberts Gurr, Fred Halliday, Chalmers Johnson, Tim McDaniel, Barrington Moore, Jeffery Paige, Vilfredo Pareto, Terence Ranger, Eugen Rosenstock-Huessy, Theda Skocpol, James Scott, Eric Selbin, Charles Tilly, Ellen Kay Trimberger, Carlos Vistas, John Walton, Timothy Wickham-Crowley, and Eric Wolf.\n\nScholars of revolutions, like Jack Goldstone, differentiate four current 'generations' of scholarly research dealing with revolutions. The scholars of the first generation such as Gustave Le Bon, Charles A. Ellwood, or Pitirim Sorokin, were mainly descriptive in their approach, and their explanations of the phenomena of revolutions was usually related to social psychology, such as Le Bon's crowd psychology theory.\n\nSecond generation theorists sought to develop detailed theories of why and when revolutions arise, grounded in more complex social behavior theories. They can be divided into three major approaches: psychological, sociological and political.\n\nThe works of Ted Robert Gurr, Ivo K. Feierbrand, Rosalind L. Feierbrand, James A. Geschwender, David C. Schwartz, and Denton E. Morrison fall into the first category. They followed theories of cognitive psychology and frustration-aggression theory and saw the cause of revolution in the state of mind of the masses, and while they varied in their approach as to what exactly caused the people to revolt (e.g., modernization, recession, or discrimination), they agreed that the primary cause for revolution was the widespread frustration with socio-political situation.\n\nThe second group, composed of academics such as Chalmers Johnson, Neil Smelser, Bob Jessop, Mark Hart, Edward A. Tiryakian, and Mark Hagopian, followed in the footsteps of Talcott Parsons and the structural-functionalist theory in sociology; they saw society as a system in equilibrium between various resources, demands and subsystems (political, cultural, etc.). As in the psychological school, they differed in their definitions of what causes disequilibrium, but agreed that it is a state of a severe disequilibrium that is responsible for revolutions.\n\nFinally, the third group, which included writers such as Charles Tilly, Samuel P. Huntington, Peter Ammann, and Arthur L. Stinchcombe followed the path of political sciences and looked at pluralist theory and interest group conflict theory. Those theories see events as outcomes of a power struggle between competing interest groups. In such a model, revolutions happen when two or more groups cannot come to terms within a normal decision making process traditional for a given political system, and simultaneously have enough resources to employ force in pursuing their goals.\n\nThe second generation theorists saw the development of the revolutions as a two-step process; first, some change results in the present situation being different from the past; second, the new situation creates an opportunity for a revolution to occur. In that situation, an event that in the past would not be sufficient to cause a revolution (e.g., a war, a riot, a bad harvest), now is sufficient; however, if authorities are aware of the danger, they can still prevent a revolution through reform or repression.\n\nMany such early studies of revolutions tended to concentrate on four classic cases: famous and uncontroversial examples that fit virtually all definitions of revolutions, such as the Glorious Revolution (1688), the French Revolution (1789–1799), the Russian Revolution of 1917, and the Chinese Revolution (also known as the Chinese Civil War) (1927–1949). In his \"The Anatomy of Revolution\", however, the Harvard historian Crane Brinton focused on the English Civil War, the American Revolution, the French Revolution, and the Russian Revolution.\n\nIn time, scholars began to analyze hundreds of other events as revolutions (see List of revolutions and rebellions), and differences in definitions and approaches gave rise to new definitions and explanations. The theories of the second generation have been criticized for their limited geographical scope, difficulty in empirical verification, as well as that while they may explain some particular revolutions, they did not explain why revolutions did not occur in other societies in very similar situations.\n\nThe criticism of the second generation led to the rise of a third generation of theories, with writers such as Theda Skocpol, Barrington Moore, Jeffrey Paige, and others expanding on the old Marxist class conflict approach, turning their attention to rural agrarian-state conflicts, state conflicts with autonomous elites, and the impact of interstate economic and military competition on domestic political change Particularly Skocpol's \"States and Social Revolutions\" became one of the most widely recognized works of the third generation; Skocpol defined revolution as \"rapid, basic transformations of society's state and class structures [...] accompanied and in part carried through by class-based revolts from below\", attributing revolutions to a conjunction of multiple conflicts involving state, elites and the lower classes.\nFrom the late 1980s a new body of scholarly work began questioning the dominance of the third generation's theories. The old theories were also dealt a significant blow by new revolutionary events that could not be easily explain by them. The Iranian and Nicaraguan Revolutions of 1979, the 1986 People Power Revolution in the Philippines and the 1989 Autumn of Nations in Europe saw multi-class coalitions topple seemingly powerful regimes amidst popular demonstrations and mass strikes in nonviolent revolutions.\n\nDefining revolutions as mostly European violent state versus people and class struggles conflicts was no longer sufficient. The study of revolutions thus evolved in three directions, firstly, some researchers were applying previous or updated structuralist theories of revolutions to events beyond the previously analyzed, mostly European conflicts. Secondly, scholars called for greater attention to conscious agency in the form of ideology and culture in shaping revolutionary mobilization and objectives. Third, analysts of both revolutions and social movements realized that those phenomena have much in common, and a new 'fourth generation' literature on contentious politics has developed that attempts to combine insights from the study of social movements and revolutions in hopes of understanding both phenomena.\n\nFurther, social science research on revolution, primarily work in political science, has begun to move beyond individual or comparative case studies towards large-N empirical studies assessing the causes and implications of revolution. Initial studies generally rely on the Polity Project’s data on democratization. Such analyses, like those by Enterline, Maoz, and Mansfield and Snyder, identify revolutions based on regime changes indicated by a change in the country’s score on Polity’s autocracy to democracy scale. More recently, scholars like Jeff Colgan have argued that Polity, which measures the degree of democratic or autocratic authority in a state's governing institutions based on the openness of executive recruitment, constraints on executive authority, and political competition, is inadequate because it measures democratization, not revolution, and fails to account for regimes which come to power by revolution but fail to change the structure of the state and society sufficiently to yield a notable difference in Polity score. Instead, Colgan offers a new data set on revolutionary leaders which identifies governments that \"transform the existing social, political, and economic relationships of the state by overthrowing or rejecting the principal existing institutions of society.\" This most recent data set has been employed to make empirically-based contributions to the literature on revolution by identifying links between revolution and the likelihood of international disputes.\n\nRevolutions have also been approached from anthropological perspectives. Drawing on Victor Turner’s writings on ritual and performance, Bjorn Thomassen has argued that revolutions can be understood as \"liminal\" moments: modern political revolutions very much resemble rituals and can therefore be studied within a process approach. This would imply not only a focus on political behavior \"from below\", but also to recognize moments where \"high and low\" are relativized, made irrelevant or subverted, and where the micro and macro levels fuse together in critical conjunctions.\n\nEconomist Douglass North argued that it is much easier for revolutionaries to alter formal political institutions such as laws and constitutions than to alter informal social conventions. According to North, inconsistencies between rapidly changing formal institutions and slow-changing informal ones can inhibit effective sociopolitical change. Because of this, the long-term effect of revolutionary political restructuring is often more moderate than the ostensible short-term effect.\n\nWhile revolutions encompass events ranging from the relatively peaceful revolutions that overthrew communist regimes to the violent Islamic revolution in Afghanistan, they exclude \"coups d'état\", civil wars, revolts, and rebellions that make no effort to transform institutions or the justification for authority (such as Józef Piłsudski's May Coup of 1926 or the American Civil War), as well as peaceful transitions to democracy through institutional arrangements such as plebiscites and free elections, as in Spain after the death of Francisco Franco.\n\n\n\n"}
{"id": "455201", "url": "https://en.wikipedia.org/wiki?curid=455201", "title": "Spaceplane", "text": "Spaceplane\n\nA spaceplane is an aerospace vehicle that operates as an aircraft in Earth's atmosphere, as well as a spacecraft when it is in space. It combines features of an aircraft and a spacecraft, which can be thought of as an aircraft that can endure and maneuver in the vacuum of space or likewise a spacecraft that can fly like an airplane. Typically, it takes the form of a spacecraft equipped with wings, although lifting bodies have been designed and tested as well. The propulsion to reach space may be purely rocket based or may use the assistance of airbreathing jet engines. The spaceflight is then followed by an unpowered glide return to landing.\n\nFive spaceplanes have successfully flown to date, having reentered Earth's atmosphere, returned to Earth, and safely landed — the North American X-15, Space Shuttle, Buran, SpaceShipOne, and Boeing X-37. All five are considered rocket gliders. , only these aircraft and rockets have succeeded in reaching space. Two of these five (X-15 and SpaceShipOne) are rocket-powered aircraft, having been carried up to an altitude of several tens of thousands of feet by an atmospheric mother ship before being released, and then flying beyond the Kármán line, the boundary of Earth's atmosphere, under their own power. Three (Space Shuttle, Buran, and X-37) are vertical takeoff horizontal landing (VTHL) vehicles relying upon rocket lift for the ascent phase in reaching space and atmospheric lift for reentry, descent and landing. The three VTHL spaceplanes flew much further than the aircraft launched ones, not merely leaving Earth's atmosphere but also entering orbit around it, which requires at least 50 times more energy on the way up and heavy heat shielding for the trip back. Of the five vehicles, three have been piloted by astronauts, with the Buran and X-37 flying unmanned missions.\n\nSignificant features distinguish spaceplanes from spacecraft.\n\nAll aircraft utilize aerodynamic surfaces in order to generate lift. For spaceplanes a variety of wing shapes can be used. Delta wings are common, but straight wings, lifting bodies and even rotorcraft have been proposed. Typically the force of lift generated by these surfaces is many times that of the drag that they induce.\n\nBecause suborbital spaceplanes are designed for trajectories that do not reach orbital speed, they do not need the kinds of thermal protection orbital spacecraft required during the hypersonic phase of atmospheric reentry. The Space Shuttle thermal protection system, for example, protects the orbiter from surface temperatures that could otherwise reach as high as , well above the melting point of steel.\n\nA spaceplane operates as an aircraft in Earth's atmosphere. Aircraft may land on firm runways, helicopter landing pads, or even water (amphibious aircraft), snow or ice. To land, the airspeed and the rate of descent are reduced such that the aircraft descends at a slow enough rate to allow for a gentle touch down. Landing is accomplished by slowing down and descending. This speed reduction is accomplished by reducing thrust and/or inducing a greater amount of drag using flaps, landing gear or speed brakes. Splashdown is an easier technical feat to accomplish, requiring only the deployment of a parachute (or parachutes), rather than successfully aviating the atmosphere. Project Gemini's original concept design was as a spaceplane, with paraglider and wheels (or skis) attached. However, this concept was abandoned in favor of parachute splashdowns, because of expensive technical failures during testing and development. Whereas Project Gemini's splashdown parachutes took only 5 months to develop in 1963, Gemini's spaceplane concept failed to materialize even after nearly 3 years of continued development.\n\nAll spaceplanes to date have used rocket engines with chemical fuels. As the orbital insertion burn has to be done in space, orbital spaceplanes require rocket engines for at least that portion of the flight.\n\nA difference between rocket based and air-breathing aerospace plane launch systems is that aerospace plane designs typically include minimal oxidizer storage for propulsion. Air-breathing aerospace plane designs include engine inlets so they can use atmospheric oxygen for combustion. Since the mass of the oxidizer is, at takeoff, the single largest mass of most rocket designs (the Space Shuttle's liquid oxygen tank weighs 629,340 kg, more than one of its solid rocket boosters), this provides a huge potential weight savings benefit. However, air breathing engines are usually very much heavier than rocket engines and the empty weight of the oxidizer tank, and since, unlike oxidizer, this extra weight (which is not expended to add kinetic energy to the vessel, as is propellant mass) must be carried into space it may offset the overall system performance.\n\nTypes of air breathing engines proposed for spaceplanes include scramjet, liquid air cycle engines, precooled jet engines, pulse detonation engine and ramjets. Some engine designs combine several types of engines features into a combined cycle. For instance, the Rocket-based combined cycle (RBCC) engine uses a rocket engine inside a ramscoop so that at low speed, the rockets thrust is boosted by ejector augmented thrust. It then transitions to ramjet propulsion at near-supersonic speeds, then to supersonic combustion or scramjet propulsion, above Mach 6, then back to pure rocket propulsion above Mach 10.\n\nThe flight trajectory required of air-breathing aerospace vehicles to reach orbit is to fly what is known as a 'depressed trajectory' which places the aerospace plane in the high-altitude hypersonic flight regime of the atmosphere. This environment induces high dynamic pressure, high temperature, and high heat flow loads particularly upon the leading edge surfaces of the aerospace plane. These loads typically require special advanced materials, active cooling, or both, for the structures to survive the environment.\n\nRocket-powered spaceplanes also face a significant thermal environment if they are burning for orbit, but this is nevertheless far less severe than air-breathing spaceplanes.\n\nSuborbital space planes designed to briefly reach space do not require significant thermal protection, as they experience peak heating for only a short time during re-entry. Intercontinental suborbital trajectories require much higher speeds and thermal protection more similar to orbital spacecraft reentry.\n\nA wingless launch vehicle has lower aerodynamic forces affecting the vehicle, and attitude control can be active perhaps with some fins to aid stability. For a winged vehicle the centre of lift moves during the atmospheric flight as well as the centre of mass; and the vehicle spends longer in the atmosphere as well. Historically, the X-33 and HOTOL spaceplanes were rear engined and had relatively heavy engines. This puts a heavy mass at the rear of the aircraft with wings that had to hold up the vehicle. As the wet mass reduces, the centre of mass tends to move rearward behind the centre of lift, which tends to be around the centre of the wings. This can cause severe instability that is usually solved by extra fins which add weight and decrease performance.\n\nAll three of the \"orbital\" spaceplanes successfully flown to date utilize a VTHL (vertical takeoff, horizontal landing) design. They include the piloted United States Space Shuttle and two unmanned spaceplanes: the late-1980s Soviet Buran and the early-2010s Boeing X-37.\n\nThe early-1980s BOR-4 (subscale test vehicle for the Spiral spaceplane that was subsequently cancelled) was a spacecraft that did successfully reenter the atmosphere and fly like an aircraft. But it was not designed to sustain atmospheric flight. It was designed to stop flying, open a parachute and then splash in the ocean.\n\nThese vehicles have used wings to provide aerobraking to return from orbit and to provide lift, allowing them to land on a runway like conventional aircraft. These vehicles are still designed to ascend to orbit vertically under rocket power like conventional expendable launch vehicles. One drawback of spaceplanes is that they have a significantly smaller payload fraction than a ballistic design with the same takeoff weight. This is in part due to the weight of the wings — around 9–12% of the weight of the atmospheric flight weight of the vehicle. This significantly reduces the payload size, but the reusability is intended to offset this disadvantage.\n\nWhile all spaceplanes have used atmospheric lift for the reentry phase, none to date have succeeded in a design that relies on aerodynamic lift for the ascent phase in reaching space (excluding a mother ship first stage). Efforts such as the Silbervogel and X-30/X-33 have all failed to materialize into a vehicle capable of successfully reaching space. The Pegasus winged booster has had many successful flights to deploy orbital payloads, but since its aerodynamic vehicle component operates only as a booster, and not operate in space as a spacecraft, it is not typically considered to be a spaceplane.\n\nOn the other hand, OREX is a test vehicle of HOPE-X and launched into 450 km LEO using H-II in 1994. OREX succeeded to reenter, but it was only hemispherical head of HOPE-X, that is, not plane-shaped.\n\nOther spaceplane designs are suborbital, requiring far less energy for propulsion, and can use the vehicle's wings to provide lift for the \"ascent to\" space in addition to the rocket. As of 2010, the only such craft to have successfully flown to and from space, back to Earth, have been the North American X-15 and SpaceShipOne. Neither of these craft was capable of entering orbit. The X-15 and SpaceShipOne both began their independent flight only after being lifted to high altitude by a carrier aircraft.\n\nScaled Composites and Virgin Galactic unveiled on 7 December 2009, the SpaceShipTwo space plane, the VSS Enterprise, and its WhiteKnightTwo mothership, \"Eve\". SpaceShipTwo is designed to carry two pilots and six passengers on suborbital flights. On 29 April 2013, after three years of unpowered testing, the spacecraft successfully performed its first powered test flight.\n\nHyflex was a miniaturized suborbital demonstrator of HOPE-X launched in 1996. Hyflex flew to 110 km altitude and succeeded in atmospheric reentry, subsequently achieving hypersonic flight. Though Hyflex achieved a controlled aircraft descent, it was not designed for a planned aircraft landing, the engineers opting instead for a splashdown without a parachute. Recovery of the Hyflex failed and it sank in the Pacific Ocean.\n\nVarious types of spaceplanes have been suggested since the early twentieth century. Notable early designs include Friedrich Zander's spaceplane equipped with wings made of combustible alloys that it would burn during its ascent, and Eugen Sänger's Silbervogel bomber design. Also in Germany and then in the US, winged versions of the V-2 rocket were considered during and after World War II, and when public interest in space exploration was high in the 1950s and '60s, winged rocket designs by Wernher von Braun and Willy Ley served to inspire science fiction artists and filmmakers.\n\nThe U.S. Air Force invested some effort in a paper study of a variety of spaceplane projects under their Aerospaceplane efforts of the late 1950s, but later ended these when they decided to use a modified version of Sänger's design. The result, Boeing X-20 Dyna-Soar, was to have been the first orbital spaceplane, but was canceled in the early 1960s in lieu of NASA's Project Gemini and the U.S. Air Force's Manned Orbiting Laboratory program.\n\nIn 1961, NASA originally planned to have the Gemini spacecraft land on a firm, solid ground runway with a Rogallo wing airfoil, rather than as a splashdown with parachute. The test vehicle became known as the Paraglider Research Vehicle. Development work on both Gemini's splashdown parachute and spaceplane paraglider began in 1963. By December 1963, the parachute was already to undergo full-scale deployment testing. On the other hand, by December 1963 the paraglider spaceplane concept was running into technical difficulties and subsequently became replaced by the parachute splashdown concept. Though attempts to revive Gemini's paraglider spaceplane concept persisted within NASA and North American Aviation as late as 1964, NASA Headquarters Gemini Chief William Schneider discontinued development as technical hurdles became too expensive.\n\nThe Rockwell X-30 National Aero-Space Plane (NASP), begun in the 1980s, was an attempt to build a scramjet vehicle capable of operating like an aircraft and achieving orbit like the shuttle. President Ronald Reagan described NASP in his 1986 State of the Union address as \"...a new Orient Express that could, by the end of the next decade, take off from Dulles Airport and accelerate up to twenty-five times the speed of sound, attaining low Earth orbit or flying to Tokyo within two hours...\"\nThere were six identifiable technologies which were considered critical to the success of the NASP project. Three of these \"enabling\" technologies were related to the propulsion system, which would consist of a hydrogen-fueled scramjet. \n\nThe NASP program became the Hypersonic Systems Technology Program (HySTP) in late 1994. HySTP was designed to transfer the accomplishments made in hypersonic technologies by the National Aero-Space Plane (NASP) program into a technology development program. On 27 January 1995 the Air Force terminated participation in (HySTP). It was canceled due to increasing technical challenges, and growing budgets.\n\nIn 1994 Mitchell Burnside Clapp proposed a single stage to orbit peroxide/kerosene spaceplane called \"Black Horse\". It was to take off almost empty and undergo mid-air refueling before launching to orbit.\n\nThe Lockheed Martin X-33 was a 1/3 scale prototype made as part of an attempt by NASA to build a SSTO hydrogen-fuelled spaceplane VentureStar that failed when the hydrogen tank design proved to be unconstructable in the planned way. \n\nThe edition of 5 March 2006 of Aviation Week & Space Technology published a story purporting to be \"outing\" a highly classified U.S. military two-stage-to-orbit spaceplane system with the code name Blackstar, SR-3/XOV among other nicknames.\n\nIn 1999 NASA started the Boeing X-37 project, an unmanned, remote controlled spaceplane. The project was transferred to the U.S. Department of Defense in 2004.\n\nBoeing has proposed that a larger variant of the X-37B, the X-37C could be built to carry up to six passengers up to LEO. The spaceplane would also be usable for carrying cargo, with both upmass and downmass (return to Earth) cargo capacity. The ideal size for the proposed derivative \"is approximately 165 to 180 percent of the current X-37B.\"\n\nIn December 2010, Orbital Sciences made a commercial proposal to NASA to develop the Prometheus, a lifting-body spaceplane vehicle about one-quarter the size of the Space Shuttle, in response to NASA's Commercial Crew Development (CCDev) solicitation. The vehicle would be launched on a human-rated (upgraded) Atlas V rocket but would land on a runway.\nFor the same solicitation, Sierra Nevada Corporation proposed extensions of its Dream Chaser spaceplane technology, partially developed under the first phase of NASA's CCDev program. Both the Orbital Sciences proposal and the Dream Chaser are lifting body designs. Sierra Nevada will utilize Virgin Galactic to market Dream Chaser commercial services and may use \"Virgin's WhiteKnightTwo carrier aircraft as a platform for drop trials of the Dream Chaser atmospheric test vehicle\"\nNASA expects to make approximately $200 million of phase 2 awards by March 2011, for technology development projects that could last up to 14 months.\n\nThe Soviet Union firstly considered a preliminary design of rocket-launch small spaceplane Lapotok in early 1960s. Then the Spiral airspace system with small orbital spaceplane and rocket as second stage was widely developed in the 1960s–1980s. Mikoyan-Gurevich MiG-105 was a manned test vehicle to explore low-speed handling and landing.\n\nIn recent times, an orbital spaceplane, called \"cosmoplane\" () capable of transporting passengers has been proposed by Russia's Institute of Applied Mechanics. According to researchers, it could take about 20 minutes to fly from Moscow to Paris, using hydrogen and oxygen-fueled engines.\n\nThe Multi-Unit Space Transport And Recovery Device (MUSTARD) was a concept explored by the British Aircraft Corporation (BAC) around 1968 for launching payloads weighing as much as into orbit. It was never constructed. The British Government also began development of a SSTO-spaceplane, called HOTOL, but the project was canceled due to technical and financial issues.\n\nThe lead engineer from the HOTOL project has since set up a private company dedicated to creating a similar plane called Skylon with a different combined cycle rocket/turbine precooled jet engine called SABRE. This vehicle is intended to be capable of a single stage to orbit launch carrying a payload into low Earth orbit. If successful it would be far in advance of anything currently in operation.\n\nThe British company Bristol Spaceplanes has undertaken design and prototyping of three potential spaceplanes since its founding by David Ashford in 1991. The European Space Agency has endorsed these designs on several occasions.\n\nFrance worked on the Hermes manned spaceplane launched by Ariane rocket in the late 20th century, and proposed in January 1985 to go through with Hermes development under the auspices of the ESA. Hopper was one of several proposals for a European reusable launch vehicle (RLV) planned to cheaply ferry satellites into orbit by 2015. One of those was 'Phoenix', a German project which is a one-seventh scale model of the Hopper concept vehicle. The suborbital Hopper was a FESTIP (Future European Space Transportation Investigations Programme) system study design A test project, the Intermediate eXperimental Vehicle (IXV), has demonstrated lifting reentry technologies and will be extended under the PRIDE programme.\nThe FAST20XX Future High-Altitude High Speed Transport 20XX aims to establish sound technological foundations for the introduction of advanced concepts in suborbital high-speed transportation with air-launch-to-orbit ALPHA vehicle.\n\nHOPE was a Japanese experimental spaceplane project designed by a partnership between NASDA and NAL (both now part of JAXA), started in the 1980s. It was positioned for most of its lifetime as one of the main Japanese contributions to the International Space Station, the other being the Japanese Experiment Module. The project was eventually cancelled in 2003, by which point test flights of a sub-scale testbed had flown successfully.\n\nAfter the German Sänger-Bredt RaBo and Silbervogel of the 1930s and 1940s, Eugen Sänger worked for time on various space plane projects, coming up with several designs for Messerschmitt-Bölkow-Blohm such as the MBB Raumtransporter-8. In the 1980s, West Germany funded design work on the MBB Sänger II with the Hypersonic Technology Program. Development continued on MBB/Deutsche Aerospace Sänger II/HORUS until the late 1980s when it was canceled. Germany went on to participate in the Ariane rocket, Columbus space station and Hermes spaceplane of ESA, Spacelab of ESA-NASA and \"Deutschland\" missions (non-U.S. funded Space Shuttle flights with Spacelab). The Sänger II had predicted cost savings of up to 30 percent over expendable rockets. The Daimler-Chrysler Aerospace RLV was a much later small reusable spaceplane prototype for ESA Future Launchers Preparatory Programme/FLTP program.\nMost recent project is SpaceLiner.\n\nAVATAR (Aerobic Vehicle for Hypersonic Aerospace Transportation; ) was a concept study for an unmanned single-stage reusable spaceplane capable of horizontal takeoff and landing, presented to India's Defence Research and Development Organisation. The mission concept was for low cost military and commercial satellite launches. No further studies or development have taken place since 2001.\n\n, the Indian Space Research Organisation is developing a launch system named the Reusable Launch Vehicle (RLV). It is India's first step towards realizing a two-stage-to-orbit reusable launch system. A space plane serves as the second stage. The plane is expected to have air-breathing scramjet engines as well as rocket engines. Tests with miniature spaceplanes and a working scramjet have been conducted by ISRO in 2016.\n\nShenlong () is a proposed Chinese robotic spaceplane that is similar to the American Boeing X-37. Only a few images have been released since late 2007.\n\n\n\n\n"}
{"id": "329915", "url": "https://en.wikipedia.org/wiki?curid=329915", "title": "Supercentenarian", "text": "Supercentenarian\n\nA supercentenarian (sometimes hyphenated as super-centenarian) is someone who has lived to or passed their 110th birthday. This age is achieved by about one in 1,000 centenarians. Anderson \"et al.\" concluded that supercentenarians live a life typically free of major age-related diseases until shortly before maximum human lifespan is reached.\n\nIn 2003, the Gerontology Research Group estimated that there were 300–450 living supercentenarians in the world (an estimate not updated ), while they had validated approximately 40 cases. Adding those mentioned in other sources results in over 100 cases. A study conducted in 2010 by the Max Planck Institute for Demographic Research found 663 validated cases of supercentenarians, living and dead, and showed that the countries with the highest total number (not frequency) of supercentenarians (in decreasing order) were the United States, Japan, England plus Wales, France, and Italy.\n\nThe first verified supercentenarian in human history, Geert Adriaans Boomgaard, died in the late nineteenth century, and it was not until the 1980s that the oldest verified age surpassed 115.\n\nThe term \"supercentenarian\" has been in existence since at least the nineteenth century. The term \"ultracentenarian\" has also been used to describe someone well over 100 – Norris McWhirter, editor of \"The Guinness Book of Records\", used the word in correspondence with age claims researcher A. Ross Eckler Jr. in 1976, and it was further popularised in 1991 by William Strauss and Neil Howe in their book \"Generations\". Meanwhile, \"semisupercentenarian\" has been used for the age range of 105–109 years. Early references to \"supercentenarian\" tend to mean simply \"someone well over 100\", but the 110-and-over cutoff is the accepted criterion of demographers.\n\nWhile claims of extreme age have persisted from the earliest times in history, the earliest supercentenarian accepted by Guinness World Records is Dutchman Thomas Peters (reportedly 1745–1857). Scholars such as French demographer Jean-Marie Robine, however, consider Geert Adriaans Boomgaard, also of the Netherlands, who turned 110 in 1898, to be the first verifiable case, as the alleged evidence for Peters has apparently been lost. The evidence for the 112 years of Englishman William Hiseland (reportedly 1620–1733) does not meet the standards required by Guinness World Records. Norwegian Church records, the accuracy of which is subject to dispute, also show what appear to be several supercentenarians who lived in the south-central part of present-day Norway during the 16th and 17th centuries, including Johannes Torpe (1549–1664), and Knud Erlandson Etun (1659–1770), both residents of Valdres, Oppland, Norway.\n\nIn 1902, Margaret Ann Neve, born in 1792, became the first verified female supercentenarian. Jeanne Calment of France, who died in 1997 aged 122 years, 164 days, had the longest human lifespan documented. The oldest man ever verified is Jiroemon Kimura of Japan, who died in 2013 aged 116 years and 54 days.\n\nOver 1,500 supercentenarians have been documented in history. It is likely that more have lived, but the majority of claims to have lived to this age do not have sufficient documentary support to be validated. This is slowly changing as those born after birth registration was standardized in more countries and localities attain supercentenarian age.\n\nResearch on the morbidity of supercentenarians has found that they remain free of major age-related diseases (e.g., stroke, cardiovascular disease, dementia, cancer, Parkinson's disease, and diabetes) until the very end of life when they die of exhaustion of organ reserve, which is the ability to return organ function to homeostasis. About 10% of supercentenarians survive until the last 3 months of life without major age-related diseases, as compared to only 4% of semisupercentenarians and 3% of centenarians.\n\nBy measuring the biological age of various tissues from supercentenarians, researchers may be able to identify the nature of those that are protected from aging effects. According to a study of 30 different body parts from a 112-year-old female supercentenarian, along with younger controls, the cerebellum is protected from aging according to an epigenetic biomarker of tissue age known as the epigenetic clock — the reading is about 15 years younger than expected in a centenarian. These findings could explain why the cerebellum exhibits fewer neuropathological hallmarks of age-related dementia as compared to other brain regions.\n\n\n"}
{"id": "26499712", "url": "https://en.wikipedia.org/wiki?curid=26499712", "title": "Teoh Beng Hock", "text": "Teoh Beng Hock\n\nTeoh Beng Hock (20 April 1979 – 1 July 2009) was a Malaysian journalist and political aide to Ean Yong Hian Wah, a member of the Selangor state legislative assembly and state executive council. On 1 July 2009, the Malaysian Anti-Corruption Commission (MACC) took Teoh into custody for questioning about allegations of corruption. Teoh was found dead the next morning on the rooftop of a building adjacent to the MACC offices. Pakatan Rakyat leaders and a number of federal government officials have called for a Royal Commission of inquiry into Teoh's death.\n\nTeoh was the third child of Teoh Leong Hwee (born 1953). He had an elder brother; Teoh Beng Kee (\"Chinese\": 赵铭基) (born 1976), elder sister; Teoh Lee Jun (\"Chinese:\" 赵丽君) and a younger sister; Teoh Lee Lan (1980，\"Chinese:\" 赵丽兰). His father was a taxi driver, and his mother, Teng Shuw Hor (1953) is a housewife.\n\nTeoh was engaged to 28-year-old teacher Soh Cher Wei after a two-year courtship, and had been planning to register his marriage with her the day following his death. A photo shoot on the same day, followed by a honeymoon and wedding reception in October had also been planned. At the time of Teoh's death, Soh was two months pregnant; she told the press she intended to keep the child. National Registration Department regulations only allow a father's name to be included on the birth certificate if he is physically present at the time of birth; after this was publicised through the press, Women, Family and Community Development Minister Shahrizat Abdul Jalil said she would ask the NRD to look into the matter. Shahrizat also stated her ministry would assist in transferring Soh from her current school to one closer to her hometown, and look into assisting Teoh's family since he was their sole breadwinner.\n\nPrime Minister Najib Tun Razak later met with Teoh's family to express his condolences. Najib told them he would direct the Attorney-General and relevant government departments to look into Teoh's family's wish that his child bear the Teoh surname. On 15 August 2009, Soh completed the traditional Chinese marriage ceremonies, together with Teoh Lee Lan, Beng Hock's younger sister, acting as a proxy. Kerk Kim Hock, a distant uncle of Teoh and the family's acting spokesperson, told the press that completion of the marriage rites now entitled Soh to have her name engraved on Teoh's gravestone as his wife.\n\nMACC officials claimed that the interrogation had lasted for about 9 hours. According to the MACC also, Teoh was freed at 3.45am—however without proper explanation, his possessions, including his mobile phone, remained in MACC custody. MACC officials also claimed that Teoh asked to stay the night at the MACC office, and was claimed to be last seen alive around 6am. Teoh was found dead at 1.30pm later in the day. The investigation had been into allegations that Ean Yong had paid RM2,400 for flags to be used in Merdeka Day celebrations, but not taken delivery of the flags. Teoh's colleagues who had also been questioned claimed that they were put under pressure from MACC officers, including being denied access to legal counsel and food or drink. Teoh's mobile phone was still in the possession of MACC officials when he was found dead at 1.30pm despite the MACC's claims that Teoh was released at 3.45am. The mobile phone was later handed to the police for investigation.\n\nOne of Teoh's colleagues, Tan Boon Wah, later filed suit against the MACC for false imprisonment. In the case, \"Tan Boon Wah v. Datuk Seri Ahmad Said Hamdan, Ketua Suruhanjaya, Suruhanjaya Pencegahan Rasuah Malaysia and Others\", the High Court ruled that because Tan was interrogated after normal working hours, he had been subject to false imprisonment, and ordered the MACC to pay him damages. Tan's lawyer, Karpal Singh, said that Teoh's family could also sue the MACC for damages, citing this ruling. Democratic Action Party Parliamentary leader and former Opposition Leader Lim Kit Siang lamented that \"Teoh Beng Hock would not have died if MACC had followed the law\" and interrogated him during the day.\n\nTeoh's family and others have called the MACC's version of events into question. When the investigating officer involved briefed the MACC's advisory board, several members of the board \"asked why he chose to stay back. Any detainee would have run for his life! The investigating officer replied that since Teoh wanted to stay back, he allowed it.\" During this briefing it also emerged that no official records of Teoh's detention or his release existed. In August, a video of men in uniform assaulting a man in their custody began circulating as a supposed video of Teoh's interrogation. The Bukit Aman federal police headquarters denied the men were police personnel. The video had previously been circulated in June as a supposed example of police interrogation techniques, ruling out the possibility of it involving Teoh. A political observer attributed the video's popularity to the government's failing to provide adequate clarification about the circumstances of Teoh's interrogation: \"The authorities are shedding little light about what happened to Teoh. This allows rumour mongers to spread ridiculous things.\"\n\nSome lawyers have argued that the law does not authorise the MACC to hold witnesses in custody, and that since Teoh was not a suspect, he should not have been held in custody. The Malaysian Human Rights Commission called the duration of Teoh's questioning \"inhumane and cruel.\" The president of the Bar Council, K. Ragunath, said the MACC had contravened the Federal Constitution by denying Teoh legal counsel during questioning, as well as the Lockup Rules 1953, which require all detainees to be locked up between 6.30pm and 6.30am to rest. Ragunath called Teoh's interrogation tantamount to torture. Ean Yong's lawyer insisted that as \"Teoh's movement was restricted, this amounts to an arrest,\" despite the MACC's insistence that Teoh was only a voluntary witness. Mohamad Ramli Manan, a senior official who retired from the Anti-Corruption Agency (before it became the MACC), agreed:\n\nAnother former ACA official, Abdul Razak Idris, disputed Ramli's view of the interrogation process, saying: \"The Act provides that we can interview anytime. Investigators are supposed to work 24 hours.\"\n\nIn response to calls from civil society and political leaders for a Royal Commission, Deputy Prime Minister Muhyiddin Yassin stated the Cabinet would consider setting up a Royal Commission of inquiry. Initial police findings suggested foul play was not involved in Teoh's death, but his friends and relatives insisted Teoh had not committed suicide, pointing to the fact that he was to be married, and that his fiance was pregnant. Teoh's family lawyers claimed that during questioning by the police, his family had been asked questions suggesting that they were primarily interested in \"[absolving] MACC officers from liability\" by pursuing the possibility of a suicide. A week after Teoh's death, his family vigorously insisted there was no possibility he had killed himself: \"Teoh was joyful and had no problems at work. Teoh had no financial issues, got along well with everyone and was preparing for his wedding.\" One detail which seemed to strike the police was Teoh's shoes; when his body was discovered, his shoes were apparently significantly damaged and worn. Teoh's fiancee \"said that Teoh had several pairs and seldom changed them unless they were damaged, but on that fateful day, he wore a new pair.\"\n\nThe United Malays National Organisation (UMNO)-owned newspaper Berita Harian published an op-ed by New Straits Times Press managing editor Zainul Ariffin Isa criticising the response to Teoh's death, stating that criticisms of the MACC were an attempt to undermine Malay institutions. Zainul was particularly critical of Selangor Menteri Besar Khalid Ibrahim for his criticism of the MACC officers' handling of the case, saying it was not right for Khalid to question \"those of the same race as him.\"\n\nAt the weekly Malaysian Cabinet meeting on 22 July following Teoh's death, the Cabinet transferred all MACC officers involved in the probe of graft allegations against the Selangor government to other assignments, pending the official police inquiry. The Cabinet also decided to establish a Royal Commission of inquiry into the MACC's interrogation procedures, but not Teoh's death, allowing the standard inquest to pursue the matter.\n\nIn a joint statement, Pakatan Rakyat leaders called the Royal Commission's terms of reference insufficient, saying that the \"question [of how Teoh died] cannot be separated from the more general issue of how the MACC conducts investigations.\" The Pakatan Rakyat statement insisted that the Commission probe the following questions:\nRobert Phang, a member of the MACC advisory board, suggested that \"Though the royal commission's scope is wide, it can be opened up further to dig deeper into issues not cleared by the inquest,\" such as establishing a reason for Teoh's death. Also being investigated is a mystery letter written on the letterhead of MACC, alleging MACC, Hishamuddin bin Hashim was involved with the torture of Teoh Beng Hock and former Menteri Besar Khir Toyo had prior knowledge of it.\n\nThe Royal Commission of Inquiry concluded that Teoh was not murdered but had committed suicide due to the aggressive interrogation tactics by three MACC officers. Teoh's family have rejected the RCI's findings and insisted that Teoh was murdered. The Bar Council of Malaysia have also questioned the RCI's findings. A coalition of 126 NGOs have called the Commission's findings a whitewash as it did not address the concerns regarding the interrogation methods by the MACC. Prime Minister Najib Tun Razak has defended the RCI's findings and called on all quarters not to question the findings. The Malay daily Utusan Malaysia, which is owned by UMNO, defended the MACC and blamed DAP for Teoh's death.\n\nThe three MACC officers involved in interrogating Teoh have been suspended pending an investigation by the police.\n\nOn 10 February 2012, Teoh Meng Kee, Beng Hock’s elder brother filed an application at the Court of Appeal to review the open verdict stating that Teoh Beng Hock committed suicide delivered by the Coroner’s Court in 2011, after his application was rejected by the High Court on 1 December 2011. The Court of Appeal on 5 September 2014 set aside the open verdict stating that \"a person or persons were responsible\" for Teoh's death.\n\n"}
{"id": "13119338", "url": "https://en.wikipedia.org/wiki?curid=13119338", "title": "Translation-quality standards", "text": "Translation-quality standards\n\nLike any supplier of goods or services, a translator potentially bears ethical and legal obligations toward his patron or employer. This has turned to be of enormous importance with the development of the language industry at global scale. For the protection of both parties, standards have been developed that seek to spell out their mutual duties.\n\nStandards of quality and documentation were originally developed for manufacturing businesses. Codes for all types of services are now maintained by standardization organizations such as the International Organization for Standardization. Standards of this type include those of the ISO 9000 series.\n\nAs interest in quality management has grown, specific quality standards have been developed for translation services. These have included the Italian UNI 10574, the German DIN 2345, the Austrian Önorm D 1200 and Önorm D 1201, and the Canadian CAN CGSB 131.10.\n\nIn 2015, EN 15038 was replaced by .\n\nThe European EN 15038 translation-services standard went into effect on August 1, 2006, replacing the previous standards of the 30 individual CEN member countries. It aims to unify the terminology used in the translation field, define basic requirements for language-service providers (human and technical resources, quality control, and project management) and create a framework for the interaction of customers and service providers in terms of their rights and obligations. It also defines certain services, in addition to translation, that may be offered by language-service providers.\n\nA strong focus is on administrative, documentation, review and revision processes, as well as on the functions of different specialists who guide the translation project over its duration. Appendices to the standard provide information and suggestions on how best to comply with the standard.\n\nOn May 12, 2009, the Language Industry Association of Canada, AILIA launched the latest standards certification program in the world. The certification is based on CAN/CGSB-131.10-2008, Translation Services, a national standard developed by the Canadian General Standards Board and approved by the Standards Council of Canada. It involved the participation of representatives from AILIA, professional associations, government, academia, purchasers of service, and other stakeholders.\nThe Canadian Standard for Translation Services CAN CGSB 131.10 - 2008 establishes and defines the requirements for the provision of translation services by translation service providers.\n\nThis National Standard of Canada is a modified adoption of the European Committee for Standardization (CEN) standard EN 15038 Translation Services. This document was prepared with the intent to harmonize where possible with the provisions of EN 15038 Translation Services. Variances in wording and content with EN 15038 reflect the Canadian perspective.\n\nConformity assessment and certification based on this standard are already in place. With the recent development of national and regional standards for translation services, many translation service providers, nationally and internationally, are now in the process of either considering or seeking certification of the services they provide in meeting the demands of the marketplace.\n\nThe standard specifies the requirements for the provision of translation services by the translation service provider (TSP).\n\nThere are three key points common to all standards:\n\nThe CGSB 131.10 discuss the following:\nThe standard does not apply to interpreting or terminology services.\n\nTSPs interested in getting certified can review the AILIA Certification Preparation Guide\n\nThe AILIA Translation Committee takes care of the promotion of the Canadian Translation Standard and its certification.\n\nThe American translation-services standard is the \"ASTM F2575-06 Standard Guide for Quality Assurance in Translation\". It provides a framework for customers and translation-service providers desirous of agreeing on the specific requirements of a translation project. It does \"not\" provide specific criteria for translation or project quality, as these requirements may be highly individual, but states parameters that should be considered before beginning a translation project. As the document's name suggests, it is a \"guideline\", informing stakeholders about what basic quality requirements are in need of compliance, rather than a prescriptive set of detail instructions for the translator.\n\nThere is, however, a view within the translation industry that, while not doing any actual harm, an over-reliance on such standards can give a false sense of security. Blindly following translation standards does not on its own provide real assurance regarding translation quality. The argument is that the path to quality in translation is by focusing more on providing on-going training and feedback to translators.\n\n\n"}
{"id": "9737756", "url": "https://en.wikipedia.org/wiki?curid=9737756", "title": "Triops longicaudatus", "text": "Triops longicaudatus\n\nTriops longicaudatus (commonly called longtail tadpole shrimp, American tadpole shrimp, or rice tadpole shrimp) is a freshwater crustacean of the order Notostraca, resembling a miniature horseshoe crab. It is characterized by an elongated, segmented body, a flattened shield-like brownish carapace covering two thirds of the thorax, and two long filaments on the abdomen. \"Triops\" refers to its three eyes, and \"longicaudatus\" refers to the elongated tail structures. \"Triops longicaudatus\" is found in freshwater ponds and pools, often in places where few higher forms of life can exist. Like its relative \"Triops cancriformis\", the longtail tadpole shrimp is considered a living fossil because its basic prehistoric morphology has changed little in the last 70 million years, exactly matching their ancient fossils. \"Triops longicaudatus\" is one of the oldest animal species still in existence.\n\n\"Triops longicaudatus\" is a member of the crustacean class Branchiopoda, which primarily contains freshwater animals with gills on their legs. The class Branchiopoda is divided into the subclasses Sarsostraca, containing fairy shrimp, and Phyllopoda, containing all other members (cladocerans, clam shrimps, and the tadpole shrimp). The subclass name, literally meaning \"leaf-footed\", is derived from their flat, leaflike appendages. Notostracans are placed in the infraclass Calamanostraca, which also contains the extinct Kazacharthra.\n\n\"Triops longicaudatus\" is usually greyish yellow or brown in color, and differs from many other species by the absence of the second maxilla. Apart from \"Triops cancriformis\", it is the only tadpole shrimp species whose individuals display as many as three reproductive strategies: bisexual, unisexual (parthenogenetic), and hermaphroditic; see below. \"Triops cancriformis\" is easily recognizable by its yellow carapace with dark spots, whilst \"T. longicaudatus\" individuals have a uniform carapace. The species also appeared about 50 million years later, and, as its name suggests, its elongated tail structures (cercopods) are often nearly as long as the rest of the body; including the cercopods, the body may reach in length.\nThe head of \"Triops longicaudatus\" is typical of crustaceans and consists of five segments, but there is a tendency toward reduction of cephalic appendages. The \"trunk\" is not distinctly divided into thorax and abdomen. Most trunk segments bear appendages. Zoologists find it difficult to decide where the thorax stops and the abdomen begins; the debate is seemingly endless. The first eleven trunk segments each bear a single pair of limbs. They are followed by segments that are fused similarly to those of millipedes, and as a result, each segment bears up to six pairs of limbs. The trunk ends with a region of limbless segments. Some zoologists consider the thorax to consist of the two regions with appendages, and the abdomen the region without appendages. Others believe the region of fused segments to be part of the abdomen. For clarity, this article uses \"thorax\" to mean the two body regions with limbs.\n\nThere is a carapace present, but no cephalothorax, since no thoracic segment is fused with the head. The terms \"carapace\" and \"cephalothorax\" often are confused, but should not be. The carapace of crustaceans is a fold of the body wall of the fifth head segment.\n\nThe head bears a pair of dorsal compound eyes that lie close to each other and are nearly fused together. The compound eyes are generally sessile (not stalked). In addition, there is a naupliar ocellus (the \"third eye\") between them. The compound eyes are on the surface of the head, but the ocellus is deep within the head. All the eyes, however, are easily visible through the shell covering of the head. A distinct horizontal groove, known as the mandibular groove, marks the division between the anterior three head segments and the posterior two. Posterior to it is the cervical groove, marking the division between the head and the thorax.\n\nOn the ventral side of the head is a lenslike window, admitting light to the naupliar eye, which is aimed both dorsally and ventrally. The first antennae (antennules) are small, slender filaments on the ventral surface of the head, at about the same level as the eyes. The second antennae are similar and lie laterally to the first. They are nonfunctioning. The large, well-developed mandibles oppose each other across the ventral midline. The opposing surfaces bear strong brownish teeth. As the crustacean periodically opens and closes the mandibles the teeth move apart and close together. Of the usual crustacean head appendages, only the mandibles are well developed. In \"Triops longicaudatus\", the larger second maxillae are absent, only maxillules being present.\n\n The anterior thorax consists of eleven segments, each bearing a pair of appendages, called thoracopods or pereiopods. None of the thoracopods are modified into maxillipeds for feeding. The posterior thorax consists of 16–25 segments, incompletely separated to form \"rings\". Each ring may consist of as many as six fused segments and bear up to six pairs of appendages (there are 54–66 limbs altogether).\n\nMost of the thoracic appendages resemble each other, but the first eleven pairs are best developed. There is a slight tendency to regional specialization and the first pair of legs is unlike the rest. It is an elongated, cheliped-like structure with a sensory function, while all the other appendages aid in feeding, respiration, and locomotion. In females, the eleventh pair of legs is modified into brood pouches. The many legs posterior to the eleventh pair move the spent feeding and respiratory currents away.\n\nThe posterior 5–14 rings at the end of the body do not bear appendages. At the end of the body is a pleotelson (fusion of the last abdominal segment and the telson) connected to long, multisegmented uropods.\n\nThe feeding method of tadpole shrimps is similar to that proposed for the ancestral crustacean. The anterior appendages (second pair to tenth pair) stir sediments and swirl muddy water into the wide, midventral food groove. The \"gnathobases\" (inward-facing lobes at the base of the leg) guide food anteriorly to the mouth. The large flat exopods (outward-facing lobes at the end of the leg) stir and lift the sediments. Fine silt particles and water escape easily, but large, coarse food particles are torn into smaller pieces by the blade-like, inward-facing lobes called \"endopods\" at the end of the leg.\n\nThe heart of tadpole shrimps is a long dorsal tube in the anterior eleven trunk segments. It has a pair of ostia in each of these segments. Sometimes hemoglobin is present in its blood and the crustacean may be pink as a result. The excretory organs are the paired maxillary glands, located on the segment of the second maxilla. The long looped ducts of these glands can be seen in the carapace. The role of the maxillary glands is primarily osmoregulatory. Nitrogen, in the form of ammonia, is lost by diffusion across the gill surfaces.\n\n\"Triops\" \"longicaudatus\" displays several reproductive strategies. Individuals may reproduce sexually, but this is rare, as most populations are highly male- or female-biased. Parthenogenesis (development from unfertilized eggs) is the most common reproductive strategy. Some populations, however, consist of hermaphrodites who fertilize each other. Different populations display different strategies or combinations of strategies, and may therefore, be considered separate species or subspecies in the future.\n\nIn females, the eleventh pair of legs is modified into egg sacs, where the eggs are carried for several hours. The eggs are released in batches, have a thick shell, and can stand freezing temperatures as well as drought, enabling the population to survive from one season to the next. The eggs have to dry out completely before being submerged in water again in order to hatch successfully; they may remain in a state of diapause for up to 20 years. These eggs may have helped \"Triops longicaudatus\", as well as other notostracans, to survive the various natural disasters and mass extinctions to date.\n\nTo complete their lives, tadpole shrimps depend on the changing nature of the temporary waters they inhabit. During the dry season (summer and fall), their offspring stay inside the eggs. As the pool fills with rainwater during the winter and spring, they hatch and feed on fairy shrimps and other invertebrates. The first larval stage (the metanauplius) is orange in color. It has a single eye, six legs, and develops through instars (growth stages). Each instar ends with shedding the exoskeleton. The number of segments and appendages increases as \"Triops \" grow, and they slowly change to greyish brown. In approximately eight days, they reach maturity and lay eggs. Adult \"Triops\" die as the pools dry up. \"Triops\" generally live for about 20–90 days if the pool does not dry up.\n\n\"T. longicaudatus\" is the most widespread notostracan species, and may be found in western North America, South America, Japan, South Korea, and several Pacific Islands. It is most active at a temperature of approximately , and is usually found scratching the mud at the bottom of pools, searching for benthic food. Triops collect food particles by straining the water with hairs on their limbs. Loose food particles are collected in a groove running down the underside of the body lengthwise, and held together by a sticky secretion until they are swallowed by their very small (2 mm wide) mouth. The tiny mouth is deep in its underbelly, and while the animal is capable of breaking up plant roots or dead fish, it is incapable of chasing down and eating prey larger than it is.\n\nTadpole shrimps are omnivorous and may eat algae, insects, and other organic debris; they are known to chase very small fry, tadpoles, and oligochaete worms. In general, they eat anything organic that is smaller than they are, which even may include their siblings (they are cannibalistic). In turn, \"Triops longicaudatus\" are eaten by frogs and birds.\n\nThe species is considered a human ally against the West Nile virus, as the individuals consume \"Culex\" mosquito larvae. They also are used as a biological agent in Japan, eating weeds in rice paddies. In Wyoming, the presence of \"Triops longicaudatus\" usually indicates a good chance of the hatching of spadefoot frogs.\n\nDried eggs of \"Triops\" \"longicaudatus\" are sold in kits to be raised as aquarium pets, sold under the name of \"aquasaurs\", \"trigons\" or \"triops\". These are most often of the greenish-brown variety, but the red variant is fairly common among enthusiasts. Captive \"Triops\" are frequently kept in aquariums and fed a diet consisting mainly of carrots, shrimp pellets and dried shrimp. Often they are also given living shrimp and daphnia as live prey. Because they can feed on just about anything, they are also fed lunch meat, potatoes, crackers, etc. They will often feed themselves by grazing on algae and other grunge from the bottom and sides of the tank and on small particles clinging to sponge filters or to Marimo moss balls, which are often cultured alongside Triops.\n\n\"Triops longicaudatus\" is widespread in North America. In Canada, it is found only in the provinces of Alberta, Saskatchewan and Manitoba. It is widespread throughout the contiguous United States, Mexico, and Hawaii, but not Alaska. Tadpole shrimps may be found in parts of South America, in (Argentina and Galápagos Islands), the West Indies, and the Pacific Islands, including Japan and New Caledonia.\n\n"}
{"id": "22984753", "url": "https://en.wikipedia.org/wiki?curid=22984753", "title": "Uncovering Our Earliest Ancestor: The Link", "text": "Uncovering Our Earliest Ancestor: The Link\n\nUncovering Our Earliest Ancestor: The Link is a one-hour television documentary made by Atlantic Productions for the BBC, first aired on 26 May 2009 on BBC One. It explores the story behind the discovery of an early primate fossil, \"Darwinius masillae\", nicknamed Ida, in a shale quarry in Germany. The fossil is believed to be around 47 million years old, and is extraordinarily well-preserved. Originally unearthed in 1983, Ida lay in the hands of a private collector for 20 years before it was shown to a Norwegian paleontologist, Dr Jørn Hurum. Realising that Ida could turn out to be a significant missing link between modern primates, lemurs and lower mammals, he persuaded the Natural History Museum in Oslo to purchase the fossil and assembled an international team of experts to study it. Their findings were announced in a press conference and the online publication of a scientific paper on 19 May 2009.\n\nThe BBC programme was narrated by David Attenborough. An alternative edit of the programme entitled \"The Link\" debuted on the History Channel on 25 May 2009 in a two-hour slot.\n\n"}
{"id": "27558974", "url": "https://en.wikipedia.org/wiki?curid=27558974", "title": "Underwater camouflage", "text": "Underwater camouflage\n\nUnderwater camouflage is the set of methods of achieving crypsis—avoidance of observation—that allows otherwise visible aquatic organisms to remain unnoticed by other organisms such as predators or prey.\n\nCamouflage in large bodies of water differs markedly from camouflage on land. The environment is essentially the same on all sides. Light always falls from above, and there is generally no variable background to compare with trees and bushes. Three main camouflage methods predominate in water: transparency, reflection, and counter-illumination. Transparency and reflectivity are most important in the top 100 metres of the ocean; counter-illumination is the main method from 100 metres down to 1000 metres; while camouflage becomes less important in the dark waters below 1000 metres.\n\nCamouflage in relatively shallow waters is more like terrestrial camouflage, where additional methods are used by many animals. For example, self-decoration is employed by decorator crabs; mimesis by animals such as the leafy sea dragon; countershading by many fish including sharks; distraction with eyespots by many fish; active camouflage through ability to change colour rapidly in fish such as the flounder, and cephalopods including octopus, cuttlefish, and squid.\n\nThe ability to camouflage oneself provides a survival advantage in the constant struggle between predators and prey. Natural selection has produced a wide variety of methods of survival in the oceans.\n\nIn Ancient Greece, Aristotle commented on the color-changing abilities, both for camouflage and for signalling, of cephalopods including the octopus, in his \"Historia animalium\":\n\nThree main camouflage methods predominate in the oceans: transparency, reflection, and counterillumination. Transparency and reflectivity are most important in the top 100 metres of the ocean; counterillumination is the main method from 100 metres down to 1000 metres; while camouflage becomes less important in the dark waters below 1000 metres. Most animals of the open sea use at least one of these methods to camouflage themselves. Camouflage in relatively shallow waters is more like terrestrial camouflage, where additional methods are used by animals in many different groups. These methods of camouflage are described in turn below.\n\nTransparency is common, even dominant, in animals of the open sea, especially those that live in relatively shallow waters. It is found in plankton of many species, as well as larger animals such as jellyfish, salps (floating tunicates), and comb jellies.\nMany marine animals that float near the surface are highly transparent, giving them almost perfect camouflage. However, transparency is difficult for bodies made of materials that have different refractive indices from seawater. Some marine animals such as jellyfish have gelatinous bodies, composed mainly of water; their thick mesogloea is acellular and highly transparent. This conveniently makes them buoyant, but it also makes them large for their muscle mass, so they cannot swim fast. Gelatinous planktonic animals are between 50 and 90 per cent transparent. A transparency of 50 per cent is enough to make an animal invisible to a predator such as cod at a depth of ; better transparency is required for invisibility in shallower water, where the light is brighter and predators can see better. For example, a cod can see prey that are 98 per cent transparent in optimal lighting in shallow water. Therefore, transparency is most effective in deeper waters.\n\nSome tissues such as muscles can be made transparent, provided either they are very thin or organised as regular layers or fibrils that are small compared to the wavelength of visible light. Familiar examples of transparent body parts are the lens and cornea of the vertebrate eye. The lens is made of the protein crystallin; the cornea is made of the protein collagen. Other structures cannot be made transparent, notably the retinas or equivalent light-absorbing structures of eyes — they must absorb light to be able to function. The camera-type eye of vertebrates and cephalopods must be completely opaque. Finally, some structures are visible for a reason, such as to lure prey. For example, the nematocysts (stinging cells) of the transparent siphonophore \"Agalma okenii\" resemble small copepods. Examples of transparent marine animals include a wide variety of larvae, including coelenterates, siphonophores, salps, gastropod molluscs, polychaete worms, many shrimplike crustaceans, and fish; whereas the adults of most of these are opaque and pigmented, resembling the seabed or shores where they live. Adult comb jellies and jellyfish are mainly transparent, like their watery background. The small Amazon river fish \"Microphilypnus amazonicus\" and the shrimps it associates with, \"Pseudopalaemon gouldingi\", are so transparent as to be \"almost invisible\"; further, these species appear to select whether to be transparent or more conventionally mottled (disruptively patterned) according to the local background in the environment.\n\nMany fish are covered with highly reflective scales, giving the appearance of silvered mirror glass. Reflection through silvering is widespread or dominant in fish of the open sea, especially those that live in the top 100 metres. Where transparency cannot be achieved, it can be imitated effectively by silvering to make an animal's body highly reflective. At medium depths at sea, light comes from above, so a mirror oriented vertically makes animals such as fish invisible from the side. Most fish in the upper ocean such as sardine and herring are camouflaged by silvering.\n\nThe marine hatchetfish is extremely flattened laterally (side to side), leaving the body just millimetres thick, and the body is so silvery as to resemble aluminium foil. The mirrors consist of microscopic structures similar to those used to provide structural coloration: stacks of between 5 and 10 crystals of guanine spaced about ¼ of a wavelength apart to interfere constructively and achieve nearly 100 per cent reflection. In the deep waters that the hatchetfish lives in, only blue light with a wavelength of 500 nanometres percolates down and needs to be reflected, so mirrors 125 nanometres apart provide good camouflage.\n\nIn fish such as the herring which live in shallower water, the mirrors must reflect a mixture of wavelengths, and the fish accordingly has crystal stacks with a range of different spacings. A further complication for fish with bodies that are rounded in cross-section is that the mirrors would be ineffective if laid flat on the skin, as they would fail to reflect horizontally. The overall mirror effect is achieved with many small reflectors, all oriented vertically. Silvering is found in other marine animals as well as fish. The cephalopods, including squid, octopus and cuttlefish, have multi-layer mirrors made of protein rather than guanine.\n\nCounter-illumination through bioluminescence on the underside (ventral region) of the body is found in many species that live in the open ocean down to about 1000 metres. The generated light increases an animal's brightness when seen from below to match the brightness of the ocean surface; it is an effective form of active camouflage. It is notably used by some species of squid, such as the midwater squid, \"Abralia veranyi\". These have light-producing organs (photophores) scattered all over their undersides, creating a sparkling glow that prevents the animal from appearing as a dark shape when seen from below. Counter-illumination camouflage is the likely function of the bioluminescence of many marine organisms, though light is also produced to attract or to detect prey and for signalling.\n\nTop/bottom countershading is common in fish including sharks, marlin, and mackerel, and animals in other groups such as dolphins, turtles and penguins. These animals have dark upper sides to match the ocean depths, and light undersides to avoid appearing dark against the bright sea surface.\n\nMimesis is practised by animals such as the leafy sea dragon, \"Phycodurus eques\", and the leaf scorpionfish, \"Taenianotus triacanthus\", which resemble parts of plants, and gently rock their bodies as if swayed by a current. In the fish species \"Novaculichthys taeniourus\", the rockmover or dragon wrasse, there is a striking difference in appearance between the adults and the juveniles. A juvenile Rockmover resembles a loose piece of sea weed. It swims in a vertical position with its head pointing downwards, and behaves in a way that perfectly resembles the movement of a piece of seaweed: moving back and forth in the surge, as if it was inanimate.\n\nSelf-decoration is employed by animals in different groups, including decorator crabs, which attach materials from their environment, as well as living organisms, to camouflage themselves. For example, the Japanese hermit crab, \"Eupagurus constans\", has the hydroid \"Hydractinia sodalis\" growing all over the shell that it lives in. Another hermit crab, \"Eupagurus cuanensis\", has the aposematic orange sponge \"Suberites domuncula\" which is bitter-tasting and not eaten by fish.\n\nSimilarly, sea urchins use their tube feet to pick up debris from the bottom and attach it to their upper surfaces. They use shells, rocks, algae and sometimes sea anemones.\n\nMany fish have eyespots near their tails, a form of automimicry, to distract attacks away from the vulnerable head and eye. For example, \"Chaetodon capistratus\" has both a (disruptive) eyestripe to conceal the eye, and a large eyespot near its tail, giving the impression that the head is at the tail end of the body.\n\nFish such as \"Dascyllus aruanus\" have bold disruptive patterns on their sides, breaking up their outlines with strong contrasts. Fish like \"Heniochus macrolepidotus\" have similar bands of colour that extend into fins projecting far from the body, distracting attention from the true shape of the fish.\n\nSome fish which mimic seaweeds such as the frogfishes \"Antennarius marmoratus\" and \"Pterophryne tumida\" have elaborate projections and spines which are combined with complex disruptive coloration. These have the effect of destroying the signature \"fish\" outline of these animals, as well as helping them to appear as pieces of algae.\n\nA variety of marine animals possess active camouflage through their ability to change colour rapidly. Several bottom-living fish such as the flounder can hide themselves effectively against a variety of backgrounds. Many cephalopods including octopus, cuttlefish, and squid similarly use colour change, in their case both for camouflage and signalling. For example, the big blue octopus, \"Octopus cyanea\", hunts during the day, and can match itself to the colours and textures of its surroundings, both to avoid predators and to enable it to approach prey. It can perfectly resemble a rock or a coral it is hiding beside. When necessary, in order to scare away a potential predator, it can display markings which resemble eyes.\n\nLike all flounders, Peacock flounders, \"Bothus mancus\", have excellent adaptive camouflage. They use cryptic coloration to avoid being detected by both prey and predators. Whenever possible rather than swim, they crawl on their fins along the bottom while constantly changing colours and patterns to match their background. In a study, some flounders demonstrated the ability to change pattern in eight seconds. They were able to match the pattern of checkerboards that they were placed on. Changing pattern is an extremely complex process involving the flounder's vision and hormones. If one of the fish's eyes is damaged, or covered by the sand, the flounder has difficulties in matching its pattern to its surroundings. Whenever the fish is hunting or hiding from predators, it buries itself into the sand, leaving only the eyes protruding.\n\n\n"}
{"id": "21919058", "url": "https://en.wikipedia.org/wiki?curid=21919058", "title": "Winnenden school shooting", "text": "Winnenden school shooting\n\nThe Winnenden school shooting occurred on the morning of 11 March 2009 at a secondary school in Winnenden, Baden-Württemberg, in southwestern Germany, followed by a shootout at a car dealership in nearby Wendlingen. The shooting spree resulted in 16 deaths, including the suicide of the perpetrator, 17-year-old Tim Kretschmer, who had graduated from the school one year earlier. He also injured nine people during the incident.\n\nIn the Albertville-Realschule at approximately 09:30 (CET), Kretschmer first began shooting with a 9 mm Beretta 92FS semi-automatic pistol, which he had taken from his parents' bedroom. Eyewitness reports state that Kretschmer started on the first upstairs floor, where he made a beeline for two top-floor classrooms and a chemistry laboratory. In the first classroom, Kretschmer fatally shot five students in the head at close range without warning. He then entered the next classroom, killed two more students, and wounded nine more, two of whom would die of their wounds en route to the hospital. As Kretschmer left the room to reload his weapon, the teacher reportedly closed the door and locked it. After unsuccessfully trying to shoot off the lock, Kretschmer then moved on to the chemistry laboratory, where he shot and killed the teacher. Students escaped Kretschmer by jumping out of windows. In the three targeted classrooms, he killed nine students (eight female and one male, 14–16 years old) and a female teacher. He shot most of his victims in the head. Kretschmer fired more than 60 rounds at the school. Because the majority of the victims were female, some speculated that Kretschmer specifically targeted females.\n\nThe school headmaster broadcast a coded announcement (\"Mrs \"Koma\" is coming\", which is amok spelled backwards) alerting the teachers of the situation; they locked classroom doors. This coded alert had been used by German educators after the Erfurt school massacre in April 2002.\n\nAfter receiving an emergency call from a student at 09:33 local time, three police officers reached the scene two minutes later and entered the school, interrupting the shooting spree. Kretschmer shot at them and fled the building, killing two female teachers in the hall as he departed. He killed a total of 12 people at the school. A quantity of unused ammunition was recovered from the school. \"It appears the perpetrator may have intended to do far more harm at the school than he managed,\" said chief criminal investigator Ralf Michelfelder.\n\nKretschmer fled the scene and killed the 56-year-old gardener (caretaker) of a nearby psychiatric hospital in the park. Large numbers of police officers secured the school building and searched for Kretschmer throughout Winnenden for hours without success. News reports confirmed that Kretschmer was on the run and warned motorists not to pick up any hitchhikers.\n\nAt approximately 10:00, the gunman carjacked a Volkswagen Sharan minivan at a car park in Winnenden. From his position on the rear seat, the gunman ordered the driver Igor Wolf to drive towards Wendlingen, from Winnenden. The gunman and the driver went westwards into the suburbs of Stuttgart, the Baden-Württemberg state capital, travelling through the towns and districts of Waiblingen, Fellbach and Bad Cannstatt, before driving on the B14 dual carriageway through the Heslach Tunnel onto the A81 autobahn motorway towards Böblingen and Tübingen. The two drove onto the B27 dual carriageway before leaving on the B313 to Nürtingen.\n\nIgor Wolf later reported that, when asked why he was doing this, the gunman replied, \"For fun, because it is fun.\" (\"\"Aus Spaß, weil es Spaß macht.“\") According to Wolf, the gunman revealed his intentions as he was loading his pistol magazines during the ride: \"Do you think we will find another school?\" (\"Meinst du, wir finden noch eine andere Schule?\"). Wolf says he quickly changed the conversation then.\n\nShortly after 12:00, just before the Wendlinger junction to the A8 \"autobahn\", the hostage steered the car onto the grass verge and jumped from the vehicle toward a police patrol car.\n\nThe gunman immediately left the car and ran towards the nearby industrial area, entering a Volkswagen car showroom through the main entrance. He threatened a salesperson and demanded a key for one of the vehicles. The salesperson escaped while the gunman was distracted. The gunman shot and killed another salesperson and a customer, firing 13 bullets into both people. As he reloaded, another salesperson and visitor fled through the rear exit. The gunman emerged at about 12:30 and shot at a passing car. The driver escaped without injury. The police started to arrive and a shootout began. An officer fired eight shots at the gunman, hitting him once in each leg.\n\nThe gunman returned to the car showroom, firing 12 shots from within the building at police from nearby Nürtingen, who were gradually surrounding the building. He left the rear of the building and ran across a yard to a neighbouring business complex, where he shot and injured two police officers in an unmarked police vehicle.\n\nAccording to police reports, at this point Kretschmer continued to shoot at random, shooting at nearby buildings and people, including an employee of a firm who was trying to lock the door. Witnesses described observing the gunman as he reloaded his pistol before shooting himself in the head. The final seconds of the shootout were captured with a cell phone video camera.\n\nAccording to forensic evidence, during the whole shooting spree, the gunman fired a total of 112 rounds.\n\nTim Kretschmer (26 July 1991 – 11 March 2009) lived with his parents in the neighbouring municipality of Leutenbach. He had graduated from Albertville Realschule in 2008 with relatively poor grades.\n\nKretschmer's failing grades had prevented him from entering an apprenticeship. He was attending a commercial high school (Donner + Kern) in Waiblingen to prepare for an apprenticeship for a commercial career. Several accounts described him as suffering from depression. Kretschmer was described by a friend as \"a lonely and frustrated person who felt rejected by society\". An anonymous friend described Kretschmer as a quiet student who began to withdraw from his peers.\n\nHe was an avid table tennis player and had hoped to become a professional player. Marko Habijanec, a Croatian table tennis player who coached Kretschmer at the Erdmannhausen sports club between 2000 and 2003, remembers him as being \"a bit spoiled\", with his mother fulfilling many of his demands. According to Habijanec, Kretschmer had great difficulties accepting defeat: he would have a temper tantrum, yelling and throwing his racket. The coach said that, having a high opinion of his own abilities, Kretschmer openly denigrated his teammates. When Habijanec discussed Tim's attitude with his mother, he discovered she sided fully with her son.\n\nMedia reports say he enjoyed playing the video game \"Counter-Strike\" and playing with airsoft guns. Commentators also noted that \"game addiction is a symptom of something wrong and not a cause\". He also shot his guns in the forest behind his home and also in the basement of his house. On his last day alive, Kretschmer played the video game \"Far Cry 2\" online as \"JawsPredator1\".\n\nHe had profiles at \"MyVideo.de\", Kwick.de and other websites. He often played poker with his classmates in the \"Cafe Tunix\" after school hours in Waiblingen.\n\nAfter inspection of his computer, officers found that Kretschmer was interested in sadomasochistic scenes where a man is bound and humiliated by women. He viewed such a movie the evening before his attacks.\n\nKretschmer did not have a criminal record. The press reported that in 2008, Kretschmer had received treatment as an in-patient at the Weissenhoff Psychiatric Clinic near the town of Heilbronn. After being discharged, Kretschmer was supposed to continue his treatment as an out-patient in Winnenden, but ended his treatment.\n\nAccording to police and clinic staff, he had been treated repeatedly for clinical depression on an out-patient basis in 2008. His family rejected these claims and maintained that he never received psychiatric treatment. According to a psychiatric report prepared for the prosecutor's office, Kretschmer met five times with a therapist and talked about his growing anger and violent urges; the therapist informed Kretschmer's parents.\n\nIn a press conference on 12 March, police reported that Kretschmer had announced his killing spree several hours ahead of time on Krautchan. The next day, police determined that this message had not been written on Kretschmer's computer and was a forgery.\n\nThree weeks before the shooting, Kretschmer had written a letter to his parents, saying that he was suffering and could not go on.\n\nIn an interview in 2014, Kretschmer's parents stated that they had always loved their son and that they did not have an explanation for his actions. They have since changed their name and moved to a different city.\n\nKretschmer killed 15 people, including nine students and three teachers at the school, before killing himself. They are:\n\nPolice raided the Kretschmer family house at about 11:00 a.m. on the day of the shooting. Tim Kretschmer's father legally owned 15 guns as a member of a local marksmen club (\"Schützenverein\" in German). One 9 mm Beretta handgun was found missing, along with several hundred rounds of ammunition. Fourteen of the guns were kept in a gun safe, while the Beretta had been kept unsecured in the bedroom.\n\nFive days after the event, prosecutors initiated preliminary proceedings against the father for negligent homicide, since the gun had not been properly locked away as required by law. The police confiscated the 14 remaining guns. The father announced that he would voluntarily relinquish his gun ownership authorization.\n\nIn November 2009, the Public Prosecutor's Department in Stuttgart announced that the father had been indicted on charges of negligent homicide, bodily injury caused by negligence, and violation of the weapons law. On 10 February 2011, the state court in Stuttgart found the father guilty of involuntary manslaughter in 15 cases, bodily harm caused by negligence and the negligent abandonment of a weapon. The father received a suspended sentence of one year and nine months, and appealed the verdict. The appeals court handed out a suspended sentence of one year and six months in 2013.\n\nGerman President Horst Köhler said he was \"appalled and saddened\" by the killings. Köhler and his wife expressed their condolences to the victims and their families and friends. Chancellor Angela Merkel described the shootings as \"incomprehensible\". \"It is unimaginable that in just seconds, pupils and teachers were killed – it is an appalling crime,\" she told reporters. \"This is a day of mourning for the whole of Germany,\" she continued. Baden-Württemberg Minister-President Günther Oettinger travelled to the scene of the crime by helicopter shortly after the news broke. Oettinger spoke of a \"horrible and in no way explainable crime\". He also expressed his condolences to the victims, students and families. \"This has touched all of Baden-Württemberg. The school, the town, the future, education, and raising children – to destroy these things like that is especially cruel.\" The European Parliament held a minute of silence to honour the dead.\nAn ecumenical Church service was held in Winnenden the evening of the shooting, with Protestant, Catholic, and Muslim clerics officiating. All German flags were flown on half-staff until 13 March, in memorial of the victims.\n\nIn the days following the event, some politicians called for legal consequences, including a total prohibition of all shooting video games, better monitoring of gun club members, a directive to have all ammunition deposited with police, and a provision to have gun club members store their weapons at the club house. Others dismissed such demands as \"placebos\".\n\nThe families of five victims wrote an open letter to Chancellor Angela Merkel, President Horst Köhler, and Baden-Württemberg Minister President Günther Oettinger with demands for consequences. They called for a prohibition on youths' access to guns in gun clubs, less violence on TV, and a prohibition of violent video games. They also called for reporting of these incidents without highlighting the perpetrator, so as to minimize the chance of copycats.\n\nThe German government passed legislation in June 2009 to improve handgun security with an electronic nationwide weapons registry, increased age limitations for large-calibre weapons, and unannounced, random inspections in gun-owner homes to ensure requirements for locked gun storage were met. Obligatory biometric security systems should be introduced once technically feasible.\n\nIn May 2009, Germany announced plans to ban games such as paintball on the grounds that they \"trivialise and encourage violence\". The legislature did not ban fighting games such as paintball, gotcha and laserdrome; did not limit the number of guns owned and did not pass a requirement to store guns with shooting clubs or restrictions regarding ammunition storage in private households.\n\n\n\n"}
