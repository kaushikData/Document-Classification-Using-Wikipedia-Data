{"id": "41263577", "url": "https://en.wikipedia.org/wiki?curid=41263577", "title": "3LAU", "text": "3LAU\n\nJustin David Blau (born January 9, 1991), better known by his stage name 3LAU (pronounced \"Blau\"), is an American DJ and electronic dance music producer. Based in Las Vegas, he has released three mixtapes since 2011 and several singles, and has performed at festivals such as Electric Zoo in New York and EDC Vegas. In 2014 he released the track \"Vikings\" with Botnek on Dim Mak Records.\n\nJustin David Blau was born in Syosset, New York, United States on . Blau grew up around an artistic family, and was soon playing piano, playing guitar and singing. At thirteen, he moved with his family to Las Vegas, Nevada, where he spent the rest of his youth. He attended The Meadows School for high school and Washington University in St. Louis for college.\n\nMuch of 3LAU's music incorporates electro house, dubstep, deep house, and progressive house. He is known for his melodic take on dance music and live sets that incorporate sampling. He is also known for using Ableton.\n\nIn 2011, at the age of 20, he vacationed in Sweden, where he discovered electronic dance music. After returning to college he started producing mashups, \"mainly because [he] didn't think anyone was doing a good job.\" He soon began mixing under the professional name 3LAU, and by June of that year was uploading mashups to YouTube.\n\n3LAU gained recognition in the electronic music world in 2011 with his two bootlegs, \"Girls Who Save the World\" and \"All Night Long\". He also won a remix competition for his remix of Tiesto's \"Work Hard, Play Hard\".\n\nBefore 2012, 3LAU spent his days studying at Washington University in St. Louis and DJing at night. He has had remixes charts in the top 10 on both Beatport and Hype Machine.\n\nIn 2012, he focused on DJing and launched his 3LAU Your Mind tour. That same year, he released his second bootleg album, \"Dance Floor Filth\". In 2012, the \"Las Vegas Review-Journal\" described him as \"one of America's fastest-rising DJ-producers.\" In 2012 he released \"Dance Floor Filth 2\", an album featuring his production.\n\nIn late 2013, he went on a short tour with Carnage called the Night Riot tour. Wrote Jonah Ollman of \"Sound of Boston\" about 3LAU's live performances in late 2013, \"A nice balance of mashups, electro-house, poppy vocal samples, and a 90’s throwback here and there keep the young 20-something crowd going and constantly entertained.\"\n\nIn the summer of 2015, 3LAU did his first European tour visiting Barcelona, Germany and Ibiza. Later that fall, he went on his first Asian tour which included Bangkok, Jakarta, Tokyo and more.\n\nOn 16 February 2018, 3LAU released his debut album \"Ultraviolet\" on label \"BLUME\" featuring singles \"Touch,\" \"On My Own\", \"Walk Away\" and \"Star Crossed\". The album features collaborations with Carly Paige, Nevve, Emma Hewitt, Max Schneider, Said the Sky, and Neonheart. The album hit #1 on iTunes' Electronic Chart, as did lead single \"Touch.\" On 21 June 2018, 3LAU announced the launch of OMF (Our Music Festival), the first blockchain-powered music festival. The festival will be held on 20 October 2018 in San Francisco and will be headlined by Zedd.\n\nIn 2016, 3LAU launched the record label Blume, the first not-for-profit dance music label. Profits from all songs released on Blume are dedicated to charity. 3LAU has raised over $200,000 for non-profit Pencils of Promise. In 2013, 3LAU announced he had built his first school with the non-profit and in 2018 another school was completed in Guatemala.\n\n\n"}
{"id": "26677990", "url": "https://en.wikipedia.org/wiki?curid=26677990", "title": "6₂ knot", "text": "6₂ knot\n\nIn knot theory, the 6 knot is one of three prime knots with crossing number six, the others being the stevedore knot and the 6 knot. This knot is sometimes referred to as the Miller Institute knot, because it appears in the logo of the Miller Institute for Basic Research in Science at the University of California, Berkeley.\n\nThe 6 knot is invertible but not amphichiral. Its Alexander polynomial is\n\nits Conway polynomial is\n\nand its Jones polynomial is\n\nThe 6 knot is a hyperbolic knot, with its complement having a volume of approximately 4.40083.\nWays to assemble of knot 6.2 \n"}
{"id": "1594003", "url": "https://en.wikipedia.org/wiki?curid=1594003", "title": "Abas I of Armenia", "text": "Abas I of Armenia\n\nAbas I of Armenia () was king of Armenia from 928 to 953. Abas was of the royal Bagratuni Dynasty. He was the son of Smbat I and the brother of Ashot \"Yerkat\" II. In contrast to the way his predecessors' ruled, Abas' reign was marked by years of peace, stability, and prosperity that Armenia had not enjoyed for decades.\n\nHis capital was based at the fortress-city of Kars and Abas achieved numerous successes on both the foreign and domestic fronts. In the same year that he became king, Abas traveled to Dvin, where he was able to convince the Arab governor there to release several Armenian hostages and turn over control of the pontifical palace back to Armenia. Conflict between the Arabs were minimal too, with the exception of a military defeat Abas suffered near the holy city of Vagharshapat. He was far less conciliatory towards the Byzantines, who had repeatedly demonstrated their unreliability as allies by attacking and annexing Armenian territories. However, the Byzantine emperor Romanus I Lecapenus was more focused on fighting the Arab Hamdanids, leaving Abas virtually free to conduct his policies without foreign hindrance. Also, Abas confronted an invasion by king Ber of Abkhazia in 943: a new church had been completed in Kars under Abas' orders and prior to its consecration, Ber had appeared with an army along the river of the Araxes, demanding that the new church be consecrated under Chalcedonian rite. Abas refused to make any concessions and ambushed Ber's forces in a dawn assault. Several more skirmishes took place, wherein Ber was finally captured by Abas' men. Abas took the king to his new church and told him that he would never see it again, blinding him and sending him back to Abkhazia. Abas died in 953, leaving his kingdom to his two sons, Ashot III and Mushegh.\n"}
{"id": "914098", "url": "https://en.wikipedia.org/wiki?curid=914098", "title": "Amplicon", "text": "Amplicon\n\nIn molecular biology, an amplicon is a piece of DNA or RNA that is the source and/or product of amplification or replication events. It can be formed artificially, using various methods including polymerase chain reactions (PCR) or ligase chain reactions (LCR), or naturally through gene duplication. In this context, \"amplification\" refers to the production of one or more copies of a genetic fragment or target sequence, specifically the amplicon. As it refers to the product of an amplification reaction, \"amplicon\" is used interchangeably with common laboratory terms, such as \"PCR product.\"\n\nArtificial amplification is used in research, forensics, and medicine for purposes that include detection and quantification of infectious agents, identification of human remains, and extracting genotypes from human hair.\n\nNatural gene duplication plays a major role in evolution. It is also implicated in several forms of human cancer including primary mediastinal B cell lymphoma and Hodgkin's lymphoma. In this context the term \"amplicon\" can refer both to a section of chromosomal DNA that has been excised, amplified, and reinserted elsewhere in the genome, and to a fragment of extrachromasomal DNA known as a double minute, each of which can be composed of one or more genes. Amplification of the genes encoded by these amplicons generally increases transcription of those genes and ultimately the volume of associated proteins.\n\nAmplicons in general are direct repeat (head-to-tail) or inverted repeat (head-to-head or tail-to-tail) genetic sequences, and can be either linear or circular in structure. Circular amplicons consist of imperfect inverted duplications annealed into a circle and are thought to arise from precursor linear amplicons.\n\nDuring artificial amplification, amplicon length is dictated by the experimental goals.\n\nAnalysis of amplicons has been made possible by the development of amplification methods such as PCR, and increasingly by cheaper and more high-throughput technologies for DNA sequencing or next-generation sequencing, such as ion semiconductor sequencing, popularly referred to as the brand of the developer, Ion Torrent.\n\nDNA sequencing technologies such as next-generation sequencing have made it possible to study amplicons in genome biology and genetics, including cancer genetics research, phylogenetic research, and human genetics. For example, using the 16S rRNA gene, which is part of every bacterial and archaeal genome and is highly conserved, bacteria can be taxonomically classified by comparison of the amplicon sequence to known sequences. This works similarly in the fungal domain with the 18S rRNA gene.\n\nIrrespective of the approach used to amplify the amplicons, some technique must be used to quantitate the amplified product. Generally, these techniques incorporate a capture step and a detection step, although how these steps are incorporated depends on the individual assay.\n\nExamples include the Amplicor HIV-1 Monitor Assay (RT-PCR), which has the capacity to recognize HIV in plasma; the HIV-1 QT (NASBA), which is used to measure plasma viral load by amplifying a segment of the HIV RNA; and Transcription Mediated Amplification, which employs a hybridization protection assay to distinguish \"Chlamydia trachomatis\" infections. Various detection and capture steps are involved in each approach to assess the amplification product, or amplicon. With amplicon sequencing the high number of different amplicons resulting from amplification of a usual sample are concatenated and sequenced. After quality control classification is done by different methods, the counts of identical taxa representing their relative abundance in the sample.\n\nPCR can be used to determine sex from a human DNA sample. The loci of Alu element insertion is selected, amplified and evaluated in terms of size of the fragment. The sex assay utilizes AluSTXa for the X chromosome, AluSTYa for the Y chromosome, or both AluSTXa and AluSTYa, to reduce the possibility of error to a negligible quantity. The inserted chromosome yields a large fragment when the homologous region is amplified. The males are distinguished as having two DNA amplicons present, while females have only a single amplicon. The kit adapted for carrying out the method includes a pair of primers to amplify the locus and optionally polymerase chain reaction reagents.\n\nLCR can be used to diagnose tuberculosis. The sequence containing protein antigen B is targeted by four oligonucleotide primers—two for the sense strand, and two for the antisense strand. The primers bind adjacent to one another, forming a segment of double stranded DNA that once separated, can serve as a target for future rounds of replication. In this instance, the product can be detected via the microparticle enzyme immunoassay (MEIA).\n\n"}
{"id": "98176", "url": "https://en.wikipedia.org/wiki?curid=98176", "title": "Ancient Egyptian concept of the soul", "text": "Ancient Egyptian concept of the soul\n\nThe ancient Egyptians believed that a soul (kꜣ/bꜣ; Egypt. pron. ka/ba) was made up of many parts. In addition to these components of the soul, there was the human body (called the \"ḥꜥ\", occasionally a plural \"ḥꜥw\", meaning approximately \"sum of bodily parts\"). \n\nAccording to ancient Egyptian creation myths, the god Atum created the world out of chaos, utilizing his own magic (ḥkꜣ). Because the earth was created with magic, Egyptians believed that the world was imbued with magic and so was every living thing upon it. When humans were created, that magic took the form of the soul, an eternal force which resided in and with every human being. The concept of the soul and the parts which encompass it has varied from the Old Kingdom to the New Kingdom, at times changing from one dynasty to another, from five parts to more. Most ancient Egyptian funerary texts reference numerous parts of the soul: the \"ẖt\" (physical body), the \"sꜥḥ\" (spiritual body), the \"rn\" (/ɾin/, Coptic ⲣⲁⲛ or ⲗⲉⲛ) \"name, identity\", the \"bꜣ\" \"personality\", the \"kꜣ\" (/kuʀ/) \"double\", the \"jb\" (/jib/, Coptic ⲉⲡ) \"heart\", the \"šwt\" \"shadow\", the \"sḫm\" \"power, form\", and the \"ꜣḫ\" (the combined spirits of a dead person that has successfully completed its transition to the afterlife). Rosalie David, an Egyptologist at the University of Manchester, explains the many facets of the soul as follows:\n\nThe \"ẖt\", or physical form, had to exist for the soul (kꜣ/bꜣ) to have intelligence or the chance to be judged by the guardians of the underworld. Therefore, it was necessary for the body to be preserved as efficiently and completely as possible and for the burial chamber to be as personalized as it could be, with paintings and statuary showing scenes and triumphs from the deceased's life. In the Old Kingdom, only the pharaoh was granted mummification and, thus, a chance at an eternal and fulfilling afterlife. However, by the Middle Kingdom, all dead were afforded the opportunity. Herodotus, an ancient Greek scholar, observed that grieving families were given a choice as to the type and or quality of the mummification they preferred: \"The best and most expensive kind is said to represent [Osiris], the next best is somewhat inferior and cheaper, while the third is cheapest of all.\"\n\nBecause the state of the body was tied so closely with the quality of the afterlife, by the time of the Middle Kingdom, not only were the burial chambers painted with depictions of favourite pastimes and great accomplishments of the dead, but there were also small figurines (\"ushabti\"s) of servants, slaves, and guards (and, in some cases beloved pets) included in the tombs, to serve the deceased in the afterlife. However, an eternal existence in the afterlife was, by no means, assured.\n\nBefore a person could be judged by the gods, they had to be \"awakened\" through a series of funerary rites designed to reanimate their mummified remains in the afterlife. The main ceremony, the opening of the mouth ceremony, is best depicted within Pharaoh Sety I's tomb. All along the walls and statuary inside the tomb are reliefs and paintings of priests performing the sacred rituals and, below the painted images, the text of the liturgy for opening of the mouth can be found. This ritual which, presumably, would have been performed during internment, was meant to reanimate each section of the body: brain, head, limbs, etc. so that the spiritual body would be able to move in the afterlife.\n\nIf all the rites, ceremonies, and preservation rituals for the \"ẖt\" were observed correctly, and the deceased was found worthy (by Osiris and the gods of the underworld) of passing through into the afterlife, the \"sꜥḥ\" (or spiritual representation of the physical body) forms. This spiritual body was then able to interact with the many entities extant in the afterlife. As a part of the larger construct, the ꜣḫ, the \"sꜥḥ\" was sometimes seen as an avenging spirit which would return from the underworld to seek revenge on those who had wronged the spirit in life. A well-known example was found in a tomb from the Middle Kingdom in which a man leaves a letter to his late wife who, it can be supposed, is haunting him:\nAn important part of the Egyptian soul was thought to be the \"jb\", or heart. The heart was believed to be formed from one drop of blood from the heart of the child's mother, taken at conception. To ancient Egyptians, the heart was the seat of emotion, thought, will and intention, evidenced by the many expressions in the Egyptian language which incorporate the word \"jb\". Unlike in English, when ancient Egyptians referenced the \"jb\" they generally meant the physical heart as opposed to a metaphorical heart. However, ancient Egyptians usually made no distinction between the mind and the heart with regard to emotion or thought. The two were synonymous. \n\nIn the Egyptian religion, the heart was the key to the afterlife. It was essential to surviving death in the nether world, where it gave evidence for, or against, its possessor. Like the physical body (\"ẖt\"), the heart was a necessary part of judgement in the afterlife and it was to be carefully preserved and stored within the mummified body with a heart scarab carefully secured to the body above it to prevent it from telling tales. According to the \"Text of the Book of Breathings\",It was thought that the heart was examined by Anubis and the deities during the Weighing of the Heart ceremony. If the heart weighed more than the feather of Maat, it was immediately consumed by the monster Ammit, and the soul became eternally restless.\n\nThe \"kꜣ\" was the Egyptian concept of vital essence, which distinguishes the difference between a living and a dead person, with death occurring when the kꜣ left the body. The Egyptians believed that Khnum created the bodies of children on a potter's wheel and inserted them into their mothers' bodies. Depending on the region, Egyptians believed that Heqet or Meskhenet was the creator of each person's kꜣ, breathing it into them at the instant of their birth as the part of their soul that made them be \"alive\". This resembles the concept of spirit in other religions.\n\nThe Egyptians also believed that the kꜣ was sustained through food and drink. For this reason food and drink offerings were presented to the dead, although it was the kꜣw within the offerings that was consumed, not the physical aspect. In the Middle kingdom a form of offering tray known as a Soul house was developed to facilitate this. The kꜣ was often represented in Egyptian iconography as a second image of the king, leading earlier works to attempt to translate kꜣ as \"double\".\n\nThe bꜣ (Egyptological pronunciation: \"ba\") was everything that makes an individual unique, similar to the notion of 'personality'. In this sense, inanimate objects could also have a bꜣ, a unique character, and indeed Old Kingdom pyramids often were called the bꜣ of their owner. The bꜣ is an aspect of a person that the Egyptians believed would live after the body died, and it is sometimes depicted as a human-headed bird flying out of the tomb to join with the kꜣ in the afterlife.\n\nIn the \"Coffin Texts,\" one form of the bꜣ that comes into existence after death is corporeal—eating, drinking and copulating. Egyptologist Louis Vico Žabkar argues that the bꜣ is not merely a part of the person \"but is the person himself\", unlike the soul in Greek, or late Judaic, Christian or Muslim thought. The idea of a purely immaterial existence was so foreign to Egyptian thought that when Christianity spread in Egypt, they borrowed the Greek word \"psychē\" to describe the concept of soul instead of the term bꜣ. Žabkar concludes that so particular was the concept of the bꜣ to ancient Egyptian thought that it ought not to be translated but instead the concept be footnoted or parenthetically explained as one of the modes of existence for a person.\n\nIn another mode of existence the bꜣ of the deceased is depicted in the \"Book of the Dead\" returning to the mummy and participating in life outside the tomb in non-corporeal form, echoing the solar theology of Ra uniting with Osiris each night.\n\nThe word \"bꜣw\" (\"baw\"), plural of the word bꜣ, meant something similar to \"impressiveness\", \"power\", and \"reputation\", particularly of a deity. When a deity intervened in human affairs, it was said that the bꜣw of the deity were at work.\n\nA person's shadow or silhouette, šwt, is always present. Because of this, Egyptians surmised that a shadow contains something of the person it represents. Through this association, statues of people and deities were sometimes referred to as shadows.\n\nThe shadow was also representative to Egyptians of a figure of death, or servant of Anubis, and was depicted graphically as a small human figure painted completely black. Sometimes people (usually pharaohs) had a shadow box in which part of their šwt was stored.\n\nLittle is known about the Egyptian interpretation of this portion of the soul. Many scholars define sḫm as the living force or life-force of the soul which exists in the afterlife after all judgement has been passed. However, sḫm is also defined in a \"Book of the Dead\" as the \"power\" and as a place within which Horus and Osiris dwell in the underworld. \n\nAs a part of the soul, a person's rn (\"rn\" 'name') was given to them at birth and the Egyptians believed that it would live for as long as that name was spoken, which explains why efforts were made to protect it and the practice of placing it in numerous writings. It is a person's identity, their experiences, and their entire life's worth of memories. For example, part of the \"Book of Breathings\", a derivative of the \"Book of the Dead\", was a means to ensure the survival of the name. A cartouche (magical rope) often was used to surround the name and protect it. Conversely, the names of deceased enemies of the state, such as Akhenaten, were hacked out of monuments in a form of \"damnatio memoriae\". Sometimes, however, they were removed in order to make room for the economical insertion of the name of a successor, without having to build another monument. The greater the number of places a name was used, the greater the possibility it would survive to be read and spoken.\n\nThe ꜣḫ \"(magically) effective one\" was a concept of the dead that varied over the long history of ancient Egyptian belief.\n\nIt was associated with thought, but not as an action of the mind; rather, it was intellect as a living entity. The ꜣḫ also played a role in the afterlife. Following the death of the ẖt (physical body), the bꜣ and kꜣ were reunited to reanimate the ꜣḫ. The reanimation of the ꜣḫ was only possible if the proper funeral rites were executed and followed by constant offerings. The ritual was termed s-ꜣḫ \"make (a dead person) into an (living) ꜣḫ\". In this sense, it even developed into a sort of ghost or roaming dead being (when the tomb was not in order any more) during the Twentieth Dynasty. An ꜣḫ could do either harm or good to persons still living, depending on the circumstances, causing e.g., nightmares, feelings of guilt, sickness, etc. It could be invoked by prayers or written letters left in the tomb's offering chapel also in order to help living family members, e.g., by intervening in disputes, by making an appeal to other dead persons or deities with any authority to influence things on earth for the better, but also to inflict punishments.\n\nThe separation of ꜣḫ and the unification of kꜣ and bꜣ were brought about after death by having the proper offerings made and knowing the proper, efficacious spell, but there was an attendant risk of dying again. Egyptian funerary literature (such as the \"Coffin Texts\" and the \"Book of the Dead\") were intended to aid the deceased in \"not dying a second time\" and to aid in becoming an ꜣḫ.\n\nAncient Egyptians believed that death occurs when a person's kꜣ leaves the body. Ceremonies conducted by priests after death, including the \"opening of the mouth (\"wp r\")\", aimed not only to restore a person's physical abilities in death, but also to release a \"Ba\"<nowiki>'</nowiki>s attachment to the body. This allowed the bꜣ to be united with the kꜣ in the afterlife, creating an entity known as an ꜣḫ.\n\nEgyptians conceived of an afterlife as quite similar to normal physical existence – but with a difference. The model for this new existence was the journey of the Sun. At night the Sun descended into the Duat or \"underworld\". Eventually the Sun meets the body of the mummified Osiris. Osiris and the Sun, re-energized by each other, rise to new life for another day. For the deceased, their body and their tomb were their personal Osiris and a personal Duat. For this reason they are often addressed as \"Osiris\". For this process to work, some sort of bodily preservation was required, to allow the bꜣ to return during the night, and to rise to new life in the morning. However, the complete \"ꜣḫ\"s were also thought to appear as stars. Until the Late Period, non-royal Egyptians did not expect to unite with the Sun deity, it being reserved for the royals.\n\nThe \"Book of the Dead\", the collection of spells which aided a person in the afterlife, had the Egyptian name of the \"Book of going forth by day\". They helped people avoid the perils of the afterlife and also aided their existence, containing spells to ensure \"not dying a second time in the underworld\", and to \"grant memory always\" to a person. In the Egyptian religion it was possible to die in the afterlife and this death was permanent.\n\nThe tomb of Paheri, an Eighteenth dynasty nomarch of Nekhen, has an eloquent description of this existence, and is translated by James Peter Allen as:\n\n"}
{"id": "20534126", "url": "https://en.wikipedia.org/wiki?curid=20534126", "title": "Ancient astronauts in popular culture", "text": "Ancient astronauts in popular culture\n\nAncient astronauts have been addressed frequently in science fiction and horror fiction. Occurrences in the genres include:\n\n\n\n\n\n"}
{"id": "44445380", "url": "https://en.wikipedia.org/wiki?curid=44445380", "title": "Ashide", "text": "Ashide\n\nAshide - one of the dominant general and empress clan of Turkic Khaganate.\n\nAshide led his descent from some ancient khagan.\n\nThe baga-tarkhan (military leader) of four Göktürk khagans - Tonyukuk was Ashide clan origin.\nAnd from Ashide was mother of Chinese warlord - An Lushan.\n\nHistorian S.G.Kjyashtorny said that originally Ashina and Ashide together were dual system, so well known among the Turkic and Mongolian peoples.\n\nChiefs Ashide bore the title Irkin (Erkin) common to tribal leaders in the Turkic Khaganate. However, their particular position is determined by kinship with the dynasty; no coincidence that one of Irkin Ashide tegin held the title - 'the prince of the royal family, prince'. Ashide clan was not is one root, so in the \"Tang Shu\" mentioned Da Ashide and Bayan Ashide; their tamgas differ from tamgas of Ashide.\n\nTo the end of the 7th-8th centuries, probably be more correct to speak about Ashide as one of the tribes of the khaganate, which together with Ashina was the main military and political support of Turkic dynasty. Ashide leaders initiated the liberation revolt of the Turkuts (679-682) against Tang dynasty.\n\nIn 2015-2016 yy. in Fudan University (Shanghai), headed by ethnogenomist Shao-Qing Wen (文少卿) in China were tested to determine the Y-DNA haplogroup the representatives from aristocratic Turkic clan Ashide.\n\nSubclade of clan Ashide: Q1a-L53.\n\nIn ancient Chinese texts \"Ashide\" is shaped 阿史德, for example: 阿史德 等 (translation: \"They [clan of] Ashide\"). Availability Form \"Ashina\" indicates the semantic aspect of the data forms. Character 阿 has semantics \"prefix names, clan of\"; Character semantics 史 has \"historical, ancient '; Character semantics 德 has \"a blessing and happiness.\" Therefore, the shape 阿史德 can translate as \"the dear old [race] happiness.\" As can be seen, the name or semantics of Göktürks (referred hieroglyph 德 [\"happiness\"]). On Proto-Altaic language semantics \"respected\" - * p \"ĕro; on the Proto-Turkic language - * er. On Proto-Altaic language semantics\" ancient \"- * bằǯá; on the Proto-Turkic language - * baja. On Proto-Altaic language semantics\" happiness \"- * kùt\" á ; on Proto-Turkic language - * kut. As can be seen, the shape 阿史德 praaltaysky translated into the language as follows: \"* p\" ĕro bằǯá * kùt \"á\"; on Turkic language - \"* er baja kut\". Therefore, the token on Proto-Turkic language \"kut\" was the name of one of the dominant clan - Turkuts.\n\nIn the body of Talas inscriptions has reported Qutlugh clan. \"Qutluγ (a) oγulan ïn ulatïq\" - \"to raise a son [of the clan] Qutlugh\" (written monument Talas IX); \"Qutluγ Özünčü esiz Özüge\" - \"Ozunchu [clan of] Qutlugh left without master Ozuge\" (written monument Talas IX).\n\nConsequently, the comparative analysis Old Chinese and Turkic languages, as well as analysis of the housing Talas written sources allows the following conclusions:\n\n1. Form 阿史德 [\"Ashide\"] is the semantic aspect.\n\n2. In the form 阿史德 [\"Ashide\"] hieroglyph 德 is the name of one of the dominant clan of Turkuts.\n\n3. Form 阿史德 [\"Ashide\"] translated into Turkic language follows \"* er baja kut\" - \"dear old [race] happiness\"\n\n4. Character 德 [kut] means the token \"Qutlugh\" - \"Happy\", where the affix \"meadow\" is the affix of belonging, referred to the original form, for example \"+ molecules meadow\" - \"having a slave\"\n\n5. One of the dominant clan of Turkuts (\"Ashide\") called \"Qutlugh|Kutlug\"\n\n"}
{"id": "2963", "url": "https://en.wikipedia.org/wiki?curid=2963", "title": "Assassination", "text": "Assassination\n\nAssassination is the killing of a prominent person, either for political or religious reasons or for payment.\n\nAn assassination may be prompted by religious, political, or military motives; it is an act that may be done for financial gain, to avenge a grievance, from a desire to acquire fame or notoriety, or because of a military, security or insurgent group's command to carry out the homicide.\n\nThe word \"assassin\" is often believed to derive from the word \"Hashshashin\" (Arabic: حشّاشين, ħashshāshīyīn, also Hashishin, Hashashiyyin, or Assassins), and shares its etymological roots with \"hashish\" ( or ; from Arabic: \"\"). It referred to a group of Nizari Shia Muslims who worked against various political targets.\n\nFounded by Hassan-i Sabbah, the Assassins were active in the fortress of Alamut in Persia from the 8th to the 14th centuries, and later expanded by capturing forts in Syria. The group killed members of the Abbasid, Seljuq, Fatimid, and Christian Crusader elite for political and religious reasons.\n\nAlthough it is commonly believed that Assassins were under the influence of hashish during their killings or during their indoctrination, there is debate as to whether these claims have merit, with many Eastern writers and an increasing number of Western academics coming to believe that drug-taking was not the key feature behind the name.\n\nThe earliest known use of the verb \"to assassinate\" in printed English was by Matthew Sutcliffe in \"A Briefe Replie to a Certaine Odious and Slanderous Libel, Lately Published by a Seditious Jesuite\", a pamphlet printed in 1600, five years before it was used in \"Macbeth\" by William Shakespeare (1605).\n\nAssassination is one of the oldest tools of power politics. It dates back at least as far as recorded history.\n\nIn the Old Testament, King Joash of Judah was recorded as being assassinated by his own servants; Joab assassinated Absalom, King David's son; and King Sennacherib of Assyria was assassinated by his own sons.\n\nChanakya (c. 350–283 BC) wrote about assassinations in detail in his political treatise \"Arthashastra\". His student Chandragupta Maurya, the founder of the Maurya Empire, later made use of assassinations against some of his enemies, including two of Alexander the Great's generals, Nicanor and Philip. Other famous victims are Philip II of Macedon (336 BC), the father of Alexander the Great, and Roman consul Julius Caesar (44 BC). Emperors of Rome often met their end in this way, as did many of the Muslim Shia Imams hundreds of years later. The practice was also well known in ancient China, as in Jing Ke's failed assassination of Qin king Ying Zheng in 227 BC. Whilst many assassinations were performed by individuals or small groups, there were also specialized units who used a collective group of people to perform more than one assassination. The earliest were the sicarii in 6 A.D., who predated the Middle Eastern assassins and Japanese ninjas by centuries.\n\nIn the Middle Ages, regicide was rare in Western Europe, but it was a recurring theme in the Eastern Roman Empire. Blinding and strangling in the bathtub were the most commonly used procedures. With the Renaissance, tyrannicide—or assassination for personal or political reasons—became more common again in Western Europe. High medieval sources mention the assassination of King Demetrius Zvonimir (1089), dying at the hands of his own people, who objected to a proposition by the Pope to go on a campaign to aid the Byzantines against the Seljuk Turks. This account is, however, contentious among historians, it being most commonly asserted that he died of natural causes. The myth of the \"Curse of King Zvonimir\" is based on the legend of his assassination. In 1192, Conrad of Montferrat, the \"de facto\" King of Jerusalem, was killed by an assassin.\n\nThe reigns of King Przemysł II of Poland (1296), William the Silent of the Netherlands (1584), and the French kings Henry III (1589) and Henry IV (1610) were all ended by assassins.\n\nIn the modern world, the killing of important people began to become more than a tool in power struggles between rulers themselves and was also used for political symbolism, such as in the propaganda of the deed. In Russia alone, two emperors, Paul I and his grandson Alexander II, were assassinated within 80 years. In the United Kingdom, only one Prime Minister has ever been assassinated—Spencer Perceval on May 11, 1812.\n\nIn Japan, a group of assassins called the Four Hitokiri of the Bakumatsu killed a number of people, including Ii Naosuke who was the head of administration for the Tokugawa shogunate, during the Boshin War. Most of the assassinations in Japan were committed with bladed weaponry, a trait that was carried on into modern history as seen during the assassination of Inejiro Asanuma on live television using a sword.\nIn the United States, within 100 years, four presidents—Abraham Lincoln, James A. Garfield, William McKinley and John F. Kennedy—died at the hands of assassins. There have been at least 20 known attempts on U.S. presidents' lives. Huey Long, a Senator, was assassinated on September 10, 1935. Robert F. Kennedy, a Senator and a presidential candidate, was also assassinated on June 6, 1968 in the United States.\n\nIn Austria, the assassination of Archduke Franz Ferdinand and his wife Sophie, Duchess of Hohenberg in Sarajevo on June 28, 1914, carried out by Gavrilo Princip, a Serbian national and a member of the Serbian nationalist insurgents (The Black Hand), is blamed for igniting World War I after a succession of minor conflicts, while belligerents on both sides in World War II used operatives specifically trained for assassination. Reinhard Heydrich died after an attack by British-trained Czechoslovak soldiers on behalf of the Czechoslovak government in exile in Operation Anthropoid, and knowledge from decoded transmissions allowed the United States to carry out a targeted attack, killing Japanese Admiral Isoroku Yamamoto while he was travelling by plane. The Polish Home Army conducted a regular campaign of assassinations against top Nazi German officials in occupied Poland. Adolf Hitler was almost killed by his own officers, and survived various attempts by other persons and organizations (such as Operation Foxley, though this plan was never put into practice).\n\nDuring the 1930s and 1940s, Joseph Stalin's NKVD carried out numerous assassinations outside of the Soviet Union, such as the killings of Organization of Ukrainian Nationalists leader Yevhen Konovalets, Ignace Poretsky, Fourth International secretary Rudolf Klement, Leon Trotsky, and the Workers' Party of Marxist Unification (POUM) leadership in Catalonia.\n\nIndia's \"Father of the Nation\", Mahatma Gandhi, was shot to death on January 30, 1948 by Nathuram Godse.\n\nThe African-American civil rights activist, Martin Luther King Jr., was assassinated on April 4, 1968, at the Lorraine Motel (now the National Civil Rights Museum) in Memphis, Tennessee. Three years prior, another African-American civil rights activist, Malcolm X, was assassinated at the Audubon Ballroom on February 21, 1965. Two years prior, another African-American civil rights activist, Medgar Evers, was assassinated on June 12, 1963. Fred Hampton of the Black Panther Party was assassinated on December 4, 1969.\n\nLiaquat Ali Khan, the first Prime Minister of Pakistan, was assassinated by Saad Akbar, a lone assassin, in 1951. Conspiracy theorists believe his conflict with certain members of the Pakistani military (Rawalpindi conspiracy) or suppression of Communists and antagonism towards the Soviet Union, were potential reasons for his assassination.\n\nIn 1960, Inejiro Asanuma, Chairman of the Japanese Socialist Party, was assassinated in a stabbing by an extreme rightist.\n\nThe U.S. Senate Select Committee chaired by Senator Frank Church (the Church Committee) reported in 1975 that it had found \"concrete evidence of at least eight plots involving the CIA to assassinate Fidel Castro from 1960 to 1965.\"\n\nMost major powers repudiated Cold War assassination tactics, though many allege that this was merely a smokescreen for political benefit and that covert and illegal training of assassins continues today, with Russia, Israel, the U.S., Argentina, Paraguay, Chile, and other nations accused of engaging in such operations. In 1986, U.S. President Ronald Reagan (who survived an assassination attempt himself) ordered the Operation El Dorado Canyon air raid on Libya in which one of the primary targets was the home residence of Libyan ruler Muammar Gaddafi. Gaddafi escaped unharmed; however, his adopted daughter Hanna was claimed to be one of the civilian casualties.\n\nIn the Philippines, the assassination of Benigno Aquino, Jr. triggered the eventual downfall of the 20-year autocratic rule of President Ferdinand Marcos. Aquino, a former Senator and a leading figure of the political opposition, was assassinated in 1983 at the Manila International Airport (now the Ninoy Aquino International Airport) upon returning home from exile. His death thrust his widow, Corazon Aquino, into the limelight and, ultimately, the presidency following the peaceful 1986 EDSA Revolution.\n\nAfter the Iranian Revolution of 1979, the new Islamic government of Iran began an international campaign of assassination that lasted into the 1990s. At least 162 killings in 19 countries have been linked to the senior leadership of the Islamic Republic of Iran. This campaign came to an end after the Mykonos restaurant assassinations, because a German court publicly implicated senior members of the government and issued arrest warrants for Ali Fallahian, the head of the Iranian Intelligence. Evidence indicates that Fallahian's personal involvement and individual responsibility for the murders were far more pervasive than his current indictment record represents.\nAnwar Sadat, President of the Arab Republic of Egypt (formerly President of the United Arab Republic), was assassinated October 6, 1981, during the annual parade celebrating Operation Badr, the opening maneuver of the Yom Kippur War.\n\nSwedish prime minister Olof Palme was murdered by a gun-wielding man close to midnight on February 28, 1986, after having visited a cinema with his wife. The couple were not accompanied by a body guard detail. The identity of the assassin and the reason for the murder are still unknown.\n\nOn August 17, 1988, President of Pakistan Gen. M. Zia ul Haq died alongside 31 others including the Chief of Staff of the Pakistani Armed Forces, the US Ambassador to Pakistan and the chief of the US Military Mission to Pakistan when his C-130 transport plane mysteriously crashed. The crash is widely considered – in Pakistan – to be an act of political assassination.\n\nIn post-Saddam Iraq, the Shiite-dominated government used death squads to perform extrajudicial executions of radical Sunni Iraqis, with some alleging that the death squads were trained by the U.S. Concrete allegations have since surfaced that the Iranian government has actively armed and funded Shia death-squads in post-Saddam Iraq.\nIn India, Prime Ministers Indira Gandhi and her son Rajiv Gandhi (neither of whom were related to Mahatma Gandhi, who was assassinated in 1948), were assassinated in 1984 and 1991 respectively. The assassinations were linked to separatist movements in Punjab and northern Sri Lanka, respectively.\n\nIn Israel, Prime Minister Yitzhak Rabin was assassinated on November 4, 1995. Yigal Amir confessed and was convicted of the crime.\n\nIsraeli tourist minister Rehavam Ze'evi was assassinated on October 17, 2001, by Hamdi Quran and three other members of the Popular Front for the Liberation of Palestine (PFLP). The PFLP stated that the assassination was in retaliation for the August 27, 2001, killing of Abu Ali Mustafa, the Secretary General of the PFLP, by the Israeli Air Force under its policy of targeted killings.\n\nIn Lebanon, the assassination of former Prime Minister Rafik Hariri on February 14, 2005, prompted an investigation by the United Nations. The suggestion in the resulting \"Mehlis report\" that there was Syrian involvement, prompted the Cedar Revolution, which drove Syrian troops out of Lebanon.\n\nIn Pakistan, former prime minister and opposition leader Benazir Bhutto was assassinated in 2007, while running for re-election. Bhutto's assassination drew unanimous condemnation from the international community.\n\nIn Guinea Bissau, President João Bernardo Vieira was assassinated in the early hours of March 2, 2009, in the capital, Bissau. Unlike typical assassinations his death was not swift; he first survived an explosion at the Presidential Villa, was then shot and wounded, and finally was butchered with machetes. His assassination was carried out by renegade soldiers who were apparently revenging the killing of General Tagme Na Waie, the Chief of Staff of the Armed Forces of Guinea Bissau, who had been killed in a bomb explosion the day before.\n\nAssassination for military purposes has long been espoused – Sun Tzu, writing around 500 BC, argued in favor of using assassination in his book \"The Art of War\". Nearly 2000 years later, in his book \"The Prince\", Machiavelli also argued assassination could be useful. An army and even a nation might be based upon and around a particularly strong, canny, or charismatic leader, whose loss could paralyze the ability of both to make war.\n\nFor similar and additional reasons, assassination has also sometimes been used in the conduct of foreign policy. The costs and benefits of such actions are difficult to compute, especially when they depend upon the policies of a successor, and one study has found that perceptual biases held by leaders often negatively affect decision making in this area, such that decisions made to go forward with assassinations often reflect the vague hope that any successor might be better.\n\nIn both military and foreign policy assassinations, there is the risk that the target could be replaced by an even more competent leader, or that such a killing (or a failed attempt) will \"martyr\" a leader and lead to greater support of his or her cause (by showing the moral ruthlessness of the assassins). Faced with particularly brilliant leaders, this possibility has in various instances been risked, such as in the attempts to kill the Athenian Alcibiades during the Peloponnesian War. A number of additional examples from World War II show how assassination was used as a tool:\n\n\nUse of assassination has continued in more recent conflicts:\n\nInsurgent groups have often employed assassination as a tool to further their causes. Assassinations provide several functions for such groups, namely the removal of specific enemies and as propaganda tools to focus the attention of media and politics on their cause.\n\nThe Irish Republican Army guerrillas of 1919–21 killed many RIC Police Intelligence officers during the Irish War of Independence. Michael Collins set up a special unit – the Squad – for this purpose, which had the effect of intimidating many policemen into resigning from the force. The Squad's activities peaked with the killing of 14 British agents in Dublin on Bloody Sunday in 1920.\n\nThis tactic was used again by the Provisional IRA during the Troubles in Northern Ireland (1969–1998). Killing of RUC officers and assassination of RUC politicians was one of a number of methods used in the Provisional IRA campaign 1969–1997. The IRA also attempted to assassinate British Prime Minister Margaret Thatcher by bombing the Conservative Party Conference in a Brighton hotel. Loyalist paramilitaries retaliated by killing Catholics at random and assassinating Irish nationalist politicians.\n\nBasque terrorists ETA in Spain have assassinated many security and political figures since the late 1960s, notably the President of the Government of Spain Luis Carrero Blanco, 1st Duke of Carrero-Blanco Grandee of Spain, in 1973. Since the early 1990s, they have also targeted academics, journalists and local politicians who publicly disagreed with them.\n\nThe Red Brigades in Italy carried out assassinations of political figures, as to a lesser extent, did the Red Army Faction in Germany in the 1970s and 1980s.\n\nIn the Vietnam War, Communist insurgents routinely assassinated government officials and individual civilians deemed to offend or rival the revolutionary movement. Such attacks, along with widespread military activity by insurgent bands, almost brought the Diem regime to collapse before the U.S. intervention.\n\nA major study about assassination attempts in the U.S. in the second half of the 20th century came to the conclusion that most prospective assassins spend copious amounts of time planning and preparing for their attempts. Assassinations are thus rarely a case of 'impulsive' action.\n\nHowever, about 25% of the actual attackers were found to be delusional, a figure that rose to 60% with 'near-lethal approachers' (people apprehended before reaching their target). This shows that while mental instability plays a role in many modern-age assassinations, the more delusional attackers are less likely to succeed in their attempt. The report also found that around two-thirds of attackers had previously been arrested (not necessarily for related offenses), that 44% had a history of serious depression, and that 39% had a history of substance abuse.\n\nWith the advent of effective ranged weaponry, and later firearms, the position of an assassination target was more precarious. Bodyguards were no longer enough to hold back determined killers, who no longer needed to directly engage or even subvert the guard to kill the leader in question. Moreover, the engagement of targets at greater distance dramatically increased the chances of an assassin's survival. The first heads of government to be assassinated with a firearm were the Regent Moray of Scotland in 1570, and William the Silent, the Prince of Orange of the Netherlands in 1584. Gunpowder and other explosives also allowed the use of bombs or even greater concentrations of explosives for deeds requiring a larger touch.\n\nExplosives, especially the car bomb, become far more common in modern history, with grenades and remote-triggered land mines also used, especially in the Middle East and Balkans (the initial attempt on Archduke Franz Ferdinand's life was with a grenade). With heavy weapons, the rocket-propelled grenade (RPG) has become a useful tool given the popularity of armored cars (discussed below), while Israeli forces have pioneered the use of aircraft-mounted missiles, as well as the innovative use of explosive devices.\n\nA sniper with a precision rifle is often used in fictional assassinations. However, certain difficulties attend long-range shooting, including finding a hidden shooting position with a clear line-of-sight, detailed advance knowledge of the intended victim's travel plans, the ability to identify the target at long range, and the ability to score a first-round lethal hit at long range, usually measured in hundreds of meters. A dedicated sniper rifle is also expensive, often costing thousands of dollars because of the high level of precision machining and hand-finishing required to achieve extreme accuracy.\n\nDespite their comparative disadvantages, handguns are more easily concealable, and consequentially much more commonly used than rifles. Of 74 principal incidents evaluated in a major study about assassination attempts in the U.S. in the second half of the 20th century, 51% were undertaken by a handgun, 30% with a rifle or shotgun, 15% used knives, and 8% explosives (usage of multiple weapons/methods was reported in 16% of all cases).\n\nIn the case of state-sponsored assassination, poisoning can be more easily denied. Georgi Markov, a Bulgarian dissident was assassinated by ricin poisoning. A tiny pellet containing the poison was injected into his leg through a specially designed umbrella. Widespread allegations involving the Bulgarian government and KGB have not led to any legal results. However, after the fall of the USSR, it was learned that the KGB had developed an umbrella that could inject ricin pellets into a victim, and two former KGB agents who defected said the agency assisted in the murder. The CIA made several attempts to assassinate Fidel Castro, many of the schemes involving poisoning his cigars. In the late 1950s, KGB assassin Bohdan Stashynsky killed Ukrainian nationalist leaders Lev Rebet and Stepan Bandera with a spray gun that fired a jet of poison gas from a crushed cyanide ampule, making their deaths look like heart attacks. A 2006 case in the UK concerned the assassination of Alexander Litvinenko who was given a lethal dose of radioactive polonium-210, possibly passed to him in aerosol form sprayed directly onto his food. Litvinenko, a former KGB agent, had been granted asylum in the UK in 2000 after citing persecution in Russia. Shortly before his death he issued a statement accusing President of Russia Vladimir Putin of involvement in his assassination. President Putin denies he had any part in Litvinenko's death.\n\nTargeted killing is the intentional killing–by a government or its agents–of a civilian or \"unlawful combatant\" who is not in the government's custody. The target is a person asserted to be taking part in an armed conflict or terrorism, whether by bearing arms or otherwise, who has thereby lost the immunity from being targeted that he would otherwise have under the Third Geneva Convention. Note that this is a different term and concept from that of \"targeted violence\" as used by specialists who study violence.\n\nOn the other hand, Georgetown Law Professor Gary Solis, in his 2010 book entitled \"The Law of Armed Conflict: International Humanitarian Law in War\", writes: \"Assassinations and targeted killings are very different acts\". The use of the term assassination is opposed, as it denotes murder, whereas the terrorists are targeted in self-defense, and thus it is viewed as a killing, but not a crime. Judge Abraham Sofaer, former federal judge for the U.S. District Court for the Southern District of New York, wrote on the subject:\n\nWhen people call a targeted killing an \"assassination,\" they are attempting to preclude debate on the merits of the action. Assassination is widely defined as murder, and is for that reason prohibited in the United States ... U.S. officials may not kill people merely because their policies are seen as detrimental to our interests ... But killings in self-defense are no more \"assassinations\" in international affairs than they are murders when undertaken by our police forces against domestic killers. Targeted killings in self-defense have been authoritatively determined by the federal government to fall outside the assassination prohibition.\n\nAuthor and former U.S. Army Captain Matthew J. Morgan has argued that \"there is a major difference between assassination and targeted killing ... targeted killing [is] not synonymous with assassination. Assassination ... constitutes an illegal killing.\" Similarly, Amos Guiora, professor of law at the University of Utah, writes: \"Targeted killing is ... not an assassination\", Steve David, Professor of International Relations at Johns Hopkins University, writes: \"There are strong reasons to believe that the Israeli policy of targeted killing is not the same as assassination\". Syracuse Law Professor William Banks and GW Law Professor Peter Raven-Hansen write: \"Targeted killing of terrorists is ... not unlawful and would not constitute assassination\", Rory Miller writes: \"Targeted killing ... is not 'assassination'\". Associate Professor Eric Patterson and Teresa Casale write: \"Perhaps most important is the legal distinction between targeted killing and assassination\".\n\nOn the other hand, the American Civil Liberties Union also states on its website, \"A program of targeted killing far from any battlefield, without charge or trial, violates the constitutional guarantee of due process. It also violates international law, under which lethal force may be used outside armed conflict zones only as a last resort to prevent imminent threats, when non-lethal means are not available. Targeting people who are suspected of terrorism for execution, far from any war zone, turns the whole world into a battlefield.\"\nYael Stein, the research director of B'Tselem, The Israeli Information Center for Human Rights in the Occupied Territories, also states in her article \"By Any Name Illegal and Immoral: Response to 'Israel's Policy of Targeted Killing'\":\nThe argument that this policy affords the public a sense of revenge and retribution could serve to justify acts both illegal and immoral. Clearly, lawbreakers ought to be punished. Yet, no matter how horrific their deeds, as the targeting of Israeli civilians indeed is, they should be punished according to the law. David's arguments could, in principle, justify the abolition of formal legal systems altogether.\n\nTargeted killing has become a frequent tactic of the United States and Israel in their fight against terrorism. The tactic can raise complex questions and lead to contentious disputes as to the legal basis for its application, who qualifies as an appropriate \"hit list\" target, and what circumstances must exist before the tactic may be employed. Opinions range from people considering it a legal form of self-defense that reduces terrorism, to people calling it an extra-judicial killing that lacks due process, and which leads to further violence. Methods used have included firing a five-foot-long Hellfire missile from a Predator or Reaper drone (an unmanned, remote-controlled plane), detonating a cell phone bomb, and long-range sniper shooting. Countries such as the U.S. (in Pakistan and Yemen) and Israel (in the West Bank and Gaza) have used targeted killing to eliminate members of groups such as Al-Qaeda and Hamas. In early 2010, with President Obama's approval, Anwar al-Awlaki became the first U.S. citizen to be publicly approved for targeted killing by the Central Intelligence Agency (CIA). Awlaki was killed in a drone strike in September 2011.\n\nUnited Nations (UN) investigator Ben Emmerson said that U.S. drone strikes may have violated international humanitarian law. \"The Intercept\" reported, \"Between January 2012 and February 2013, U.S. special operations airstrikes [in northeastern Afghanistan] killed more than 200 people. Of those, only 35 were the intended targets.\"\n\nOne of the earliest forms of defense against assassins was employing bodyguards. Bodyguards act as a shield for the potential target, keeping lookout for potential attackers (sometimes in advance, for example on a parade route), and putting themselves in harm's way—both by simple presence, showing that physical force is available to protect the target, and by shielding the target during any attack. To neutralize an attacker, bodyguards are typically armed as much as legal and practical concerns permit.\n\nNotable examples of bodyguards include the Roman Praetorian Guard or the Ottoman Janissaries—though, in both cases, the protectors sometimes became assassins themselves, exploiting their power to make the head of state a virtual hostage or killing the very leaders they were supposed to protect. The fidelity of individual bodyguards is an important question as well, especially for leaders who oversee states with strong ethnic or religious divisions. Failure to realize such divided loyalties led to the assassination of Indian Prime Minister Indira Gandhi, assassinated by two Sikh bodyguards in 1984.\n\nThis bodyguard function was often executed by the leader's most loyal warriors, and was extremely effective throughout most of early human history, leading assassins to attempt stealthy means, such as poison (which risk was answered by having another person taste the leader's food first).\n\nAnother notable measure is the use of a body double, a person who looks like the leader and who pretends to be the leader to draw attention away from the intended target.\n\nWith the advent of gunpowder, ranged assassination (via bombs or firearms) became possible. One of the first reactions was to simply increase the guard, creating what at times might seem a small army trailing every leader; another was to begin clearing large areas whenever a leader was present, to the point where entire sections of a city might be shut down.\n\nAs the 20th century dawned, the prevalence and capability of assassins grew quickly, as did measures to protect against them. For the first time, armored cars or limousines were put into service for safer transport, with modern versions virtually invulnerable to small arms fire, smaller bombs and mines. Bulletproof vests also began to be used, which were of limited utility, restricting movement and leaving the head unprotected – so they tended to be worn only during high-profile public events, if at all.\n\nAccess to famous persons, too, became more and more restricted; potential visitors would be forced through numerous different checks before being granted access to the official in question, and as communication became better and information technology more prevalent, it has become all but impossible for a would-be killer to get close enough to the personage at work or in private life to effect an attempt on his or her life, especially given the common use of metal and bomb detectors.\n\nMost modern assassinations have been committed either during a public performance or during transport, both because of weaker security and security lapses, such as with U.S. President John F. Kennedy and former Pakistani Prime Minister Benazir Bhutto, or as part of coups d'état where security is either overwhelmed or completely removed, such as with Patrice Lumumba.\nThe methods used for protection by famous people have sometimes evoked negative reactions by the public, with some resenting the separation from their officials or major figures. One example might be traveling in a car protected by a bubble of clear bulletproof glass, such as the Popemobile of Pope John Paul II, built following an attempt at his life. Politicians often resent this need for separation, sometimes sending their bodyguards away from them for personal or publicity reasons; U.S. President William McKinley did this at the public reception where he was assassinated.\n\nOther potential targets go into seclusion, and are rarely heard from or seen in public, such as writer Salman Rushdie. A related form of protection is the use of body doubles, people with similar builds to those they are expected to impersonate. These persons are then made up, and in some cases altered to look like the target, with the body double then taking the place of the person in high risk situations. According to Joe R. Reeder, Under Secretary of the Army from 1993 to 1997, Fidel Castro used body doubles.\n\nUnited States Secret Service protective agents receive training in the psychology of assassins.\n\n\n"}
{"id": "30775956", "url": "https://en.wikipedia.org/wiki?curid=30775956", "title": "Beot", "text": "Beot\n\nA bēot is Old English for a ritualized boast, vow, threat, or promise. The principle of a \"bēot\" is to proclaim one's acceptance of a seemingly impossible challenge in order to gain tremendous glory for actually accomplishing it.\n\nAnglo-Saxon warriors would usually deliver \"bēot\"s in the mead hall the night before a military engagement or during the battle itself. For example, a typical warrior may boast that he will be the first to strike a blow in a battle, that he would claim a renowned sword from an enemy warrior as spoil of battle, that he will slay a particular monster that has been wreaking havoc on a town or village, and so on. \"Bēot\"s were usually accompanied by grand stories of one's past glorious deeds. Although other cultures and times might disdain boasting as a sign of arrogance, or sinful pride, the pagan Anglo-Saxons highly regarded such behavior as a positive sign of one's determination, bravery, and character.\n\nExamples of the \"bēot\" can be seen throughout the epic poem \"Beowulf\", such as when Beowulf vows to fight Grendel without using any weapons or armor.\n\nThe Old English word \"bēot\" comes from earlier \"bíhát\" meaning ‘promise’. The original noun-form of \"bēot\" corresponds to the verb \"bi-\", \"be-ˈhátan\". A shifting of the stress from \"bíhát\" to \"bi-ˈhát\", on analogy of the verb, gave the late Old English \"beˈhát\", from which the Middle English word \"behote\" derives.\n\n\n"}
{"id": "23526104", "url": "https://en.wikipedia.org/wiki?curid=23526104", "title": "BioLegend", "text": "BioLegend\n\nBioLegend is a global developer and manufacturer of antibodies and reagents used in biomedical research located in San Diego, California. It was incorporated in June 2002 and has since expanded to include BioLegend Japan KK, where it is partnered with Tomy Digital Biology Co., Ltd. in Tokyo, BioLegend Europe in the United Kingdom, BioLegend GmbH in Germany, and BioLegend UK Ltd in the United Kingdom. BioLegend manufactures products in the areas of neuroscience, cell immunophenotyping, cytokines and chemokines, adhesion, cancer research, T regulatory cells, stem cells, innate immunity, cell-cycle analysis, apoptosis, and modification-specific antibodies. Reagents are created for use in flow cytometry, ELISA, immunoprecipitation, Western blotting, immunofluorescence microscopy, immunohistochemistry, and in vitro or in vivo functional assays.\n\nBioLegend was founded by CEO, Gene Lay, D.V.M., who was also the co-founder of PharMingen. In 2011, BioLegend co-developed and introduced Brilliant Violet(TM)-conjugated antibodies, using a novel fluorophore based on Nobel Prize-winning chemistry developed by Sirigen.\n"}
{"id": "36544849", "url": "https://en.wikipedia.org/wiki?curid=36544849", "title": "Celia Griffin", "text": "Celia Griffin\n\nCelia Griffin (1841 – March 1847), Irish famine victim. \n\nGriffin was born and raised on the Martin estate in Connemara, being a native of Corindulla, near Ross. The family were badly hit by the famine, and in February 1847 walked the thirty miles to Galway in search of relief. \"However, many city dwellers shunned these refugees arriving amongst them, a situation born more out of fear of contacting disease rather than selfishness or indifference.\" \n\nBrother Paul O'Connor ran the Orphans' Breakfast Institute in Lombard Street, where Celia and her siblings were fed. However, more and more people arrived each day, and the Institute could not feed them all. \n\nWhat became of Griffin's parents is unknown. She and her two sisters were taken into the Presentation Convent on Presentation Road in the second week of March when Celia collapsed on the street. Attempts by the nuns to save her failed and she died as a result of the effects of starvation. An inquest into her death was held on Thursday 11 March 1847. \n\n\" An inquest was held on Thursday last, before Michael Perrin, Esq., D.C., at the Presentation Convent, on view of the body of Celia Griffin, a girl about six years of age, from the village of Corindulla, near Ross, in this county. It appeared in evidence that the poor creature had been reduced to extreme poverty and that the family to whom she belonged, eight in number, were in the same pitiful condition. She had been recommended to the Ladies of the Presentation, by Rev. George Usher, as a fit object for relief, and accordingly she and her two sisters received a daily breakfast at that excellent Institute. They met Mr Usher on the Rahoon road about a fortnight ago, but famine had so preyed upon her feeble constitution, that, on the morning of Wednesday, she was unable to taste food of any description – so that on the post mortem examination made by Doctor Staunton, there was not a particle found in her stomach.\"\n\n\"She with her father, mother, brothers, and sisters, came to Galway about six weeks ago, in the hope of obtaining some charitable relief, and during that period have been begging in the streets, and about the country. The parents of the deceased formerly resided on the estate of Thomas Martin, Esq, MP. When Doctor Staunton was called on he found deceased is a state of inanition, except an occasional convulsive action of the muscles, and her body might be said to be literally skin and bone – with all the appearance of starvation. She was so exhausted, as not to be able to use the food supplied to her. The Jury found that her death was caused for want of the common necessaries of life, before she received relief at the Presentation Convent.\"\n\nOn the 4 July 2012 the Galway Famine Ship Memorial was dedicated at the Celia Griffin Memorial Park at Grattan Beach, Galway. According to the Galway Independent, \"The unveiling of the monument will be Galway’s tribute to Celia Griffin and the many thousands of children like her who perished in the Famine, and also to the ships and crews which carried so many of our people to safety ... [as] ... Celia Griffin has come to symbolise all the children who lost their lives in the Great Famine.\"\n\n\n"}
{"id": "37814244", "url": "https://en.wikipedia.org/wiki?curid=37814244", "title": "Centre for the Study of Existential Risk", "text": "Centre for the Study of Existential Risk\n\nThe Centre for the Study of Existential Risk (CSER) is a research centre at the University of Cambridge, intended to study possible extinction-level threats posed by present or future technology. The co-founders of the centre are Huw Price (a philosophy professor at Cambridge), Martin Rees (a cosmologist, astrophysicist, and former President of the Royal Society) and Jaan Tallinn (a computer programmer and co-founder of Skype). CSER's advisors include philosopher Peter Singer, computer scientist Stuart J. Russell, statistician David Spiegelhalter, and cosmologist Max Tegmark. Their \"goal is to steer a small fraction of Cambridge’s great intellectual resources, and of the reputation built on its past and present scientific pre-eminence, to the task of ensuring that our own species has a long-term future.\"\n\nThe centre's founding was announced in November, 2012. Its name stems from Oxford philosopher Nick Bostrom's concept of \"existential risk\", or risk \"where an adverse outcome would either annihilate Earth-originating intelligent life or permanently and drastically curtail its potential\". This includes technologies that might permanently deplete humanity's resources or block further scientific progress, in addition to ones that put the species itself at risk.\n\nAmong the global catastrophic risks to be studied by CSER are those stemming from possible future advances in artificial intelligence. The potential dangers of artificial general intelligence have been highlighted in early discussions of CSER, being likened in some press coverage to that of a robot uprising à la \"The Terminator\". Price explained to the AFP news agency, \"It seems a reasonable prediction that some time in this or the next century intelligence will escape from the constraints of biology\". He added that when this happens \"we're no longer the smartest things around,\" and will risk being at the mercy of \"machines that are not malicious, but machines whose interests don't include us\". Price has also mentioned synthetic biology as being dangerous because \"[as a result of] new innovations, the steps necessary to produce a weaponized virus or other bioterror agent have been dramatically simplified\" and that consequently “the number of individuals needed to wipe us all out is declining quite steeply.”\n\nOther technologies CSER seeks to evaluate include molecular nanotechnology, extreme climate change, and systemic risks from fragile networks.\n\nCSER has been covered in many different newspapers (particularly in the United Kingdom), mostly covering different topics of interest. University of Cambridge Research News' coverage of it focuses on risks from artificial general intelligence. The CSER was profiled in the special Frankenstein issue of \"Science\" in 2018. On existential risks to humanity, many scientists think the only serious one is the risk of global nuclear war. The Science article quoted Joyce Tait of the Imogen Institute in Edinburgh: \"There is nothing [else] on the horizon.\" \n\n"}
{"id": "1303452", "url": "https://en.wikipedia.org/wiki?curid=1303452", "title": "Chris Byrd", "text": "Chris Byrd\n\nChristopher Cornelius \"Chris\" Byrd (born August 15, 1970) is an American former professional boxer who competed from 1993 to 2009. He is a two-time world heavyweight champion, having first won the WBO title in 2000 after an upset corner stoppage over then-undefeated Vitali Klitschko. In his first title defense later that year, he lost to Vitali's brother Wladimir Klitschko. In 2002, Byrd defeated Evander Holyfield to win the IBF heavyweight title for his second reign as world champion. He made four successful defenses until losing his title again to Wladimir Klitschko in a 2006 rematch. Byrd's cousin, Lamon Brewster, has a shared history with Wladimir Klitschko: Brewster defeated him in 2004, but lost in a 2007 rematch.\n\nAs an amateur, Byrd represented the United States at the 1992 Summer Olympics and won a silver medal in the middleweight division. He is also a three-time national amateur champion, winning the light middleweight title in 1989, and the middleweight title in 1991 and 1992.\n\nChris Byrd was the youngest of eight children growing up in Flint, Michigan. He began boxing at age 5, training in his father (Joe Sr.)'s Joe Byrd Boxing Academy. His father continued to train and manage Byrd as a professional. Byrd attended Flint Northwestern High School.\n\nByrd began competing in the ring at age 10, and compiled an impressive 275 wins in the amateur ranks. He was a three-time U.S. amateur champion (1989, 1991, and 1992). He was on the 1991 U.S. National boxing Team that became the first (and only) U.S. team to score a tie against the heralded Cuban team.\nByrd won the silver medal in the 1992 Barcelona summer Olympics as a middleweight, losing to Cuba's Ariel Hernández in the final.\n\n\nByrd turned professional on January 28, 1993, knocking out 10 of his first 13 opponents. Byrd moved up to heavyweight three fights into his professional career.\n\nByrd remained undefeated for his first 26 fights, knocking off then-notable opponents like Phil Jackson, Lionel Butler, Uriah Grant, Bert Cooper, Craig Peterson, Frankie Swindell, Jimmy Thunder, undefeated Eliecer Castillo and Ross Puritty.\n\nHowever, in 1999, Byrd's undefeated record came to a dead end when he fought undefeated Ike Ibeabuchi. With 48 seconds left in the fifth round, a left-handed bolo punch followed with a right hook sent Byrd to the canvas, face first.\n\nDuring the last week of March 2000, Byrd was offered the chance to be the replacement (for Donovan Ruddock) against undefeated champion Vitali Klitschko in Berlin, Germany (Klitschko's adopted home country) for the WBO Heavyweight Title. He therefore had only seven days to prepare for the fight (not the customary 6–12 weeks). Byrd struggled greatly in the fight, trailing after nine rounds by scores of 88–83 (on two cards) and 89–82 (on one card), i.e. losing seven or eight of those rounds. However, Klitschko severely injured his shoulder and was unable to continue after the ninth round. The injury that Klitschko suffered was a torn shoulder rotator cuff, which required major surgery and a 7-month lay-off. Despite trailing on all three of the judges' scorecards, Byrd walked away the winner by a technical knockout due to the injury to Klitschko.\n\nSix months later, Byrd was back in Germany to defend the title against Wladimir Klitschko, Vitali's younger, more agile brother. Twelve rounds later, Byrd had lost a lopsided unanimous decision and the WBO belt after being knocked down twice.\n\nByrd returned to the U.S., signed with Don King and beat Maurice Harris to win the United States Boxing Association heavyweight belt in Madison Square Garden. He was now a top-five contender for the IBF title. After winning his next match (a title defense against New Zealand's top contender David Tua) Byrd eventually received his mandatory shot at the vacant IBF world Heavyweight Championship against Evander Holyfield in Atlantic City. On December 14, 2002, Byrd won a unanimous decision and the IBF title.\n\nByrd has successfully defended the IBF belt against: Fres Oquendo in 2003 (match ended in a controversial win for Byrd in which most people felt Oquendo won), a highly entertaining draw with \"Andrew\" Golota and a decision win over friend Jameel McCline in 2004, and DaVarryl Williamson in 2005. Byrd's fight with Golota did 75,000 buys on Pay per View\n\nOn April 22, 2006, Byrd faced Wladimir Klitschko for the second time. Byrd was making his fifth defense of his IBF title and the fight was also sanctioned by the International Boxing Organization for its title, which had been vacated upon the retirement of Lennox Lewis. The fight took place at SAP Arena in Mannheim, Germany. Byrd was heavily dominated throughout the fight, was down in the fifth, and again in the seventh. Referee Wayne Kelly stopped the fight after the second knockdown when Byrd had an open cut near his eyes. Klitschko won in a TKO\n\nAfter losing to Alexander Povetkin, Byrd would drop about 40 pounds to return to the light heavyweight division. He fought Shaun George on May 16, 2008, at the Thomas and Mack Center in Las Vegas. Byrd was dropped by George in round one and rocked again in round two. George then hammered Byrd with his right hand at will, finally flooring Byrd twice in the ninth round. Byrd beat the count after the first knockdown, but was then battered down again and the bout was waved off by the referee.\n\nIn 2010, Byrd officially announced his retirement from boxing.\n\nByrd hosts a weekly video podcast, entitled \"Byrd's Eye View\", which showcases former boxers as well as current professional and champion-level fighters.\n\n\n\n"}
{"id": "46944821", "url": "https://en.wikipedia.org/wiki?curid=46944821", "title": "Coccas (soldier)", "text": "Coccas (soldier)\n\nCoccas (, died 552) was an Eastern Roman soldier who deserted to the Ostrogoths during the final stages of the Gothic War. Procopius calls him \"a Roman soldier\" and \"a man of the Gothic army\". His name is not Germanic, and might be Thracian.\n\nCoccas was part of Ostrogothic king Totila's army at the Battle of Taginae in June/July 552. He is described as a cavalryman of great physical strength. In order to gain time for the arrival of 2,000 reinforcements led by Teia, Coccas rode towards the Romans and requested them to send forth a champion to engage him in single combat. Anzalas, an Armenian retainer of the Roman commander Narses, accepted the challenge. Coccas charged at Anzalas and aimed at his stomach, but at the last moment, Anzalas swerved his horse and stabbed Coccas in the side, mortally wounding him. Although Coccas' sacrifice gained him the necessary time for the arrival of Teia, Totila died in the ensuing battle, which was a disaster for the Goths.\n\n\n"}
{"id": "3112875", "url": "https://en.wikipedia.org/wiki?curid=3112875", "title": "Computational immunology", "text": "Computational immunology\n\nIn academia, computational immunology is a field of science that encompasses high-throughput genomic and bioinformatics approaches to immunology. The field's main aim is to convert immunological data into computational problems, solve these problems using mathematical and computational approaches and then convert these results into immunologically meaningful interpretations.\n\nThe immune system is a complex system of the human body and understanding it is one of the most challenging topics in biology. Immunology research is important for understanding the mechanisms underlying the defense of human body and to develop drugs for immunological diseases and maintain health. Recent findings in genomic and proteomic technologies have transformed the immunology research drastically. Sequencing of the human and other model organism genomes has produced increasingly large volumes of data relevant to immunology research and at the same time huge amounts of functional and clinical data are being reported in the scientific literature and stored in clinical records. Recent advances in bioinformatics or computational biology were helpful to understand and organize these large scale data and gave rise to new area that is called Computational immunology or immunoinformatics.\n\nComputational immunology is a branch of bioinformatics and it is based on similar concepts and tools, such as sequence alignment and protein structure prediction tools. Immunomics is a discipline like genomics and proteomics. It is a science, which specifically combines Immunology with computer science, mathematics, chemistry, and biochemistry for large-scale analysis of immune system functions. It aims to study the complex protein–protein interactions and networks and allows a better understanding of immune responses and their role during normal, diseased and reconstitution states. Computational immunology is a part of immunomics, which is focused on analyzing large scale experimental data.\n\nComputational immunology began over 90 years ago with the theoretic modeling of malaria epidemiology. At that time, the emphasis was on the use of mathematics to guide the study of disease transmission. Since then, the field has expanded to cover all other aspects of immune system processes and diseases.\n\nAfter the recent advances in sequencing and proteomics technology, there have been many fold increase in generation of molecular and immunological data. The data are so diverse that they can be categorized in different databases according to their use in the research. Until now there are total 31 different immunological databases noted in the Nucleic Acids Research (NAR) Database Collection, which are given in the following table, together with some more immune related databases. The information given in the table is taken from the database descriptions in NAR Database Collection.\n\nOnline resources for allergy information are also available on http://www.allergen.org. Such data is valuable for investigation of cross-reactivity between known allergens and analysis of potential allergenicity in proteins. The \"Structural Database of Allergen Proteins\" (SDAP) stores information of allergenic proteins. The \"Food Allergy Research and Resource Program (FARRP) Protein Allergen\"-Online Database contains sequences of known and putative allergens derived from scientific literature and public databases. \"Allergome\" emphasizes the annotation of allergens that result in an IgE-mediated disease.\n\nA variety of computational, mathematical and statistical methods are available and reported. These tools are helpful for collection, analysis, and interpretation of immunological data. They include text mining, information management, sequence analysis, analysis of molecular interactions, and mathematical models that enable advanced simulations of immune system and immunological processes. \nAttempts are being made for the extraction of interesting and complex patterns from non-structured text documents in the immunological domain. Such as categorization of allergen cross-reactivity information, identification of cancer-associated gene variants and the classification of immune epitopes.\n\nImmunoinformatics is using the basic bioinformatics tools such as ClustalW, BLAST, and TreeView, as well as specialized immunoinformatics tools, such as EpiMatrix, IMGT/V-QUEST for IG and TR sequence analysis, IMGT/ Collier-de-Perles and IMGT/StructuralQuery for IG variable domain structure analysis. Methods that rely on sequence comparison are diverse and have been applied to analyze HLA sequence conservation, help verify the origins of human immunodeficiency virus (HIV) sequences, and construct homology models for the analysis of hepatitis B virus polymerase resistance to lamivudine and emtricitabine.\n\nThere are also some computational models which focus on protein–protein interactions and networks. There are also tools which are used for T and B cell epitope mapping, proteasomal cleavage site prediction, and TAP– peptide prediction. The experimental data is very much important to design and justify the models to predict various molecular targets. Computational immunology tools is the game between experimental data and mathematically designed computational tools.\n\nAllergies, while a critical subject of immunology, also vary considerably among individuals and sometimes even among genetically similar individuals. The assessment of protein allergenic potential focuses on three main aspects: (i) immunogenicity; (ii) cross-reactivity; and (iii) clinical symptoms. Immunogenicity is due to responses of an IgE antibody-producing B cell and/or of a T cell to a particular allergen. Therefore, immunogenicity studies focus mainly on identifying recognition sites of B-cells and T-cells for allergens. The three-dimensional structural properties of allergens control their allergenicity.\n\nThe use of immunoinformatics tools can be useful to predict protein allergenicity and will become increasingly important in the screening of novel foods before their wide-scale release for human use. Thus, there are major efforts under way to make reliable broad based allergy databases and combine these with well validated prediction tools in order to enable the identification of potential allergens in genetically modified drugs and foods. Though the developments are on primary stage, the World Health organization and Food and Agriculture Organization have proposed guidelines for evaluating allergenicity of genetically modified foods. According to the Codex alimentarius, a protein is potentially allergenic if it possesses an identity of ≥6 contiguous amino acids or ≥35% sequence similarity over an 80 amino acid window with a known allergen. Though there are rules, their inherent limitations have started to become apparent and exceptions to the rules have been well reported \n\nIn the study of infectious diseases and host responses, the mathematical and computer models are a great help. These models were very useful in characterizing the behavior and spread of infectious disease, by understanding the dynamics of the pathogen in the host and the mechanisms of host factors which aid pathogen persistence. Examples include \"Plasmodium falciparum\" and nematode infection in ruminants.\n\nMuch has been done in understanding immune responses to various pathogens by integrating genomics\nand proteomics with bioinformatics strategies. Many exciting developments in large-scale screening of pathogens are currently taking place. National Institute of Allergy and Infectious Diseases (NIAID) has initiated an endeavor for systematic mapping of B and T cell epitopes of category A-C pathogens. These pathogens include \"Bacillus anthracis\" (anthrax), \"Clostridium botulinum\" toxin (botulism), \"Variola major\" (smallpox), \"Francisella tularensis\" (tularemia), viral hemorrhagic fevers, \"Burkholderia pseudomallei\", \"Staphylococcus enterotoxin\" B, yellow fever, influenza, rabies, Chikungunya virus etc. Rule-based systems have been reported for the automated extraction and curation of influenza A records.\n\nThis development would lead to the development of an algorithm which would help to identify the conserved regions of pathogen sequences and in turn would be useful for vaccine development. This would be helpful in limiting the spread of infectious disease. Examples include a method for identification of vaccine targets from protein regions of conserved HLA binding and computational assessment of cross-reactivity of broadly neutralizing antibodies against viral pathogens. These examples illustrate the power of immunoinformatics applications to help solve complex problems in public health. Immunoinformatics could accelerate the discovery process dramatically and potentially shorten the time required for vaccine development. Immunoinformatics tools have been used to design the vaccine against Dengue virus and Leishmania \n\nUsing this technology it is possible to know the model behind immune system. It has been used to model T-cell-mediated suppression, peripheral lymphocyte migration, T-cell memory, tolerance, thymic function, and antibody networks. Models are helpful to predicts dynamics of pathogen toxicity and T-cell memory in response to different stimuli. There are also several models which are helpful in understanding the nature of specificity in immune network and immunogenicity.\n\nFor example, it was useful to examine the functional relationship between TAP peptide transport and HLA class I antigen presentation. TAP is a transmembrane protein responsible for the transport of antigenic peptides into the endoplasmic reticulum, where MHC them class I molecules can bind them and presented to T cells. As TAP does not bind all peptides equally, TAP-binding affinity could influence the ability of a particular peptide to gain access to the MHC class I pathway. Artificial neural network (ANN), a computer model was used to study peptide binding to human TAP and its relationship with MHC class I binding. The affinity of HLA-binding peptides for TAP was found to differ according to the HLA supertype concerned using this method. This research could have important implications for the design of peptide based immuno-therapeutic drugs and vaccines. It shows the power of the modeling approach to understand complex immune interactions.\n\nThere exist also methods which integrate peptide prediction tools with computer simulations that can provide detailed information on the immune response dynamics specific to the given pathogen's peptides \n\nCancer is the result of somatic mutations which provide cancer cells with a selective growth advantage. Recently it has been very important to determine the novel mutations. Genomics and proteomics techniques are used worldwide to identify mutations related to each specific cancer and their treatments. Computational tools are used to predict growth and surface antigens on cancerous cells. There are publications explaining a targeted approach for assessing mutations and cancer risk. Algorithm CanPredict was used to indicate how closely a specific gene resembles known cancer-causing genes. Cancer immunology has been given so much importance that the data related to it is growing rapidly. Protein–protein interaction networks provide valuable information on tumorigenesis in humans. Cancer proteins exhibit a network topology that is different from normal proteins in the human interactome. Immunoinformatics have been useful in increasing success of tumour vaccination. Recently, pioneering works have been conducted to analyse the host immune system dynamics in response to artificial immunity induced by vaccination strategies.. Other simulation tools use predicted cancer peptides to forecast immune specific anticancer responses that is dependent on the specified HLA.\nThese resources are likely to grow significantly in the near future and immunoinformatics will be a major growth area in this domain.\n\n\n"}
{"id": "2178570", "url": "https://en.wikipedia.org/wiki?curid=2178570", "title": "Cosmic dust", "text": "Cosmic dust\n\nCosmic dust, also called extraterrestrial dust or space dust, is dust which exists in outer space, or has fallen on Earth. Most cosmic dust particles are between a few molecules to 0.1 µm in size. Cosmic dust can be further distinguished by its astronomical location: intergalactic dust, interstellar dust, interplanetary dust (such as in the zodiacal cloud) and circumplanetary dust (such as in a planetary ring). \n\nIn the Solar System, interplanetary dust causes the zodiacal light. Solar System dust includes comet dust, asteroidal dust, dust from the Kuiper belt, and interstellar dust passing through the Solar System. Thousands of tons of cosmic dust are estimated to reach the Earth's surface every year, with each grain having a mass between 10 kg (0.1 pg) and 10 kg (100 mg). The density of the dust cloud through which the Earth is traveling is approximately 10/m. \n\nCosmic dust contains some complex organic compounds (amorphous organic solids with a mixed aromatic–aliphatic structure) that could be created naturally, and rapidly, by stars. A smaller fraction of dust in space is \"stardust\" consisting of larger refractory minerals that condensed as matter left by stars.\n\nInterstellar dust particles were collected by the \"Stardust\" spacecraft and samples were returned to Earth in 2006.\n\nCosmic dust was once solely an annoyance to astronomers, as it obscures objects they wish to observe. When infrared astronomy began, the dust particles were observed to be significant and vital components of astrophysical processes. Their analysis can reveal information about phenomena like the formation of the Solar System. For example, cosmic dust can drive the mass loss when a star is nearing the end of its life, play a part in the early stages of star formation, and form planets. In the Solar System, dust plays a major role in the zodiacal light, Saturn's B Ring spokes, the outer diffuse planetary rings at Jupiter, Saturn, Uranus and Neptune, and comets.\n\nThe interdisciplinary study of dust brings together different scientific fields: physics (solid-state, electromagnetic theory, surface physics, statistical physics, thermal physics), fractal mathematics, surface chemistry on dust grains) meteoritics, as well as every branch of astronomy and astrophysics. These disparate research areas can be linked by the following theme: the cosmic dust particles evolve cyclically; chemically, physically and dynamically. The evolution of dust traces out paths in which the Universe recycles material, in processes analogous to the daily recycling steps with which many people are familiar: production, storage, processing, collection, consumption, and discarding. \n\nObservations and measurements of cosmic dust in different regions provide an important insight into the Universe's recycling processes; in the clouds of the diffuse interstellar medium, in molecular clouds, in the circumstellar dust of young stellar objects, and in planetary systems such as the Solar System, where astronomers consider dust as in its most recycled state. The astronomers accumulate observational ‘snapshots’ of dust at different stages of its life and, over time, form a more complete movie of the Universe's complicated recycling steps.\n\nParameters such as the particle's initial motion, material properties, intervening plasma and magnetic field determined the dust particle's arrival at the dust detector. Slightly changing any of these parameters can give significantly different dust dynamical behavior. Therefore, one can learn about where that object came from, and what is (in) the intervening medium.\n\nCosmic dust can be detected by indirect methods that utilize the radiative properties of the cosmic dust particles.\n\nCosmic dust can also be detected directly ('in-situ') using a variety of collection methods and from a variety of collection locations. Estimates of the daily influx of extraterrestrial material entering the Earth's atmosphere range between 5 and 300 tonnes.\n\nNASA collects samples of star dust particles in the Earth's atmosphere using plate collectors under the wings of stratospheric-flying airplanes. Dust samples are also collected from surface deposits on the large Earth ice-masses (Antarctica and Greenland/the Arctic) and in deep-sea sediments.\n\nDon Brownlee at the University of Washington in Seattle first reliably identified the extraterrestrial nature of collected dust particles in the latter 1970s. Another source is the meteorites, which contain stardust extracted from them. Stardust grains are solid refractory pieces of individual presolar stars. They are recognized by their extreme isotopic compositions, which can only be isotopic compositions within evolved stars, prior to any mixing with the interstellar medium. These grains condensed from the stellar matter as it cooled while leaving the star.\n\nIn interplanetary space, dust detectors on planetary spacecraft have been built and flown, some are presently flying, and more are presently being built to fly. The large orbital velocities of dust particles in interplanetary space (typically 10–40 km/s) make intact particle capture problematic. Instead, in-situ dust detectors are generally devised to measure parameters associated with the high-velocity impact of dust particles on the instrument, and then derive physical properties of the particles (usually mass and velocity) through laboratory calibration (i.e. impacting accelerated particles with known properties onto a laboratory replica of the dust detector). Over the years dust detectors have measured, among others, the impact light flash, acoustic signal and impact ionisation. Recently the dust instrument on Stardust captured particles intact in low-density aerogel.\n\nDust detectors in the past flew on the HEOS-2, \"Helios\", \"Pioneer 10\", \"Pioneer 11\", \"Giotto\", and \"Galileo\" space missions, on the Earth-orbiting LDEF, EURECA, and Gorid satellites, and some scientists have utilized the \"Voyager 1\" and \"2\" spacecraft as giant Langmuir probes to directly sample the cosmic dust. Presently dust detectors are flying on the \"Ulysses\", \"Cassini\", Proba, \"Rosetta\", \"Stardust\", and the \"New Horizons\" spacecraft. The collected dust at Earth or collected further in space and returned by sample-return space missions is then analyzed by dust scientists in their respective laboratories all over the world. One large storage facility for cosmic dust exists at the NASA Houston JSC.\n\nInfrared light can penetrate the cosmic dust clouds, allowing us to peer into regions of star formation and the centers of galaxies. NASA's Spitzer Space Telescope is the largest infrared telescope ever launched into space. The Spitzer Space Telescope (formerly SIRTF, the Space Infrared Telescope Facility) was launched into space by a Delta rocket from Cape Canaveral, Florida on 25 August 2003. During its mission, Spitzer will obtain images and spectra by detecting the infrared energy, or heat, radiated by objects in space between wavelengths of 3 and 180 micrometres. Most of this infrared radiation is blocked by the Earth's atmosphere and cannot be observed from the ground. The findings from the Spitzer already revitalized the studies of cosmic dust. A recent report from a Spitzer team shows some evidence that cosmic dust is formed near a supermassive black hole.\n\nAnother detection mechanism is polarimetry. Dust grains are not spherical and tend to align to interstellar magnetic fields, preferentially polarising starlight that passes through dust clouds. In nearby interstellar space, where cosmic reddening is not sensitive enough to be detected, high precision optical polarimetry has been used to glean the structure of dust within the Local Bubble.\n\nA dust particle interacts with electromagnetic radiation in a way that depends on its cross section, the wavelength of the electromagnetic radiation, and on the nature of the grain: its refractive index, size, etc. The radiation process for an individual grain is called its \"emissivity\", dependent on the grain's \"efficiency factor\". Furthermore, we have to specify whether the emissivity process is extinction, scattering, absorption, or polarisation. In the radiation emission curves, several important signatures identify the composition of the emitting or absorbing dust particles.\n\nDust particles can scatter light nonuniformly. Forward-scattered light means that light is redirected slightly by diffraction off its path from the star/sunlight, and back-scattered light is reflected light.\n\nThe scattering and extinction (\"dimming\") of the radiation gives useful information about the dust grain sizes. For example, if the object(s) in one's data is many times brighter in forward-scattered visible light than in back-scattered visible light, then we know that a significant fraction of the particles are about a micrometer in diameter.\n\nThe scattering of light from dust grains in long exposure visible photographs is quite noticeable in reflection nebulae, and gives clues about the individual particle's light-scattering properties. In X-ray wavelengths, many scientists are investigating the scattering of X-rays by interstellar dust, and some have suggested that astronomical X-ray sources would possess diffuse haloes, due to the dust.\n\nStardust grains (also called presolar grains by meteoriticists) are contained within meteorites, from which they are extracted in terrestrial laboratories. Stardust was a component of the dust in the interstellar medium before its incorporation into meteorites. The meteorites have stored those stardust grains ever since the meteorites first assembled within the planetary accretion disk more than four billion years ago. So-called carbonaceous chondrites are especially fertile reservoirs of stardust. Each stardust grain existed before the Earth was formed. \"Stardust\" is a scientific term referring to refractory dust grains that condensed from cooling ejected gases from individual presolar stars and incorporated into the cloud from which the Solar System condensed.\n\nMany different types of stardust have been identified by laboratory measurements of the highly unusual isotopic composition of the chemical elements that comprise each stardust grain. These refractory mineral grains may earlier have been coated with volatile compounds, but those are lost in the dissolving of meteorite matter in acids, leaving only insoluble refractory minerals. Finding the grain cores without dissolving most of the meteorite has been possible, but difficult and labor-intensive (see presolar grains).\n\nMany new aspects of nucleosynthesis have been discovered from the isotopic ratios within the stardust grains. An important property of stardust is the hard, refractory, high-temperature nature of the grains. Prominent are silicon carbide, graphite, aluminium oxide, aluminium spinel, and other such solids that would condense at high temperature from a cooling gas, such as in stellar winds or in the decompression of the inside of a supernova. They differ greatly from the solids formed at low temperature within the interstellar medium.\n\nAlso important are their extreme isotopic compositions, which are expected to exist nowhere in the interstellar medium. This also suggests that the stardust condensed from the gases of individual stars before the isotopes could be diluted by mixing with the interstellar medium. These allow the source stars to be identified. For example, the heavy elements within the silicon carbide (SiC) grains are almost pure S-process isotopes, fitting their condensation within AGB star red giant winds inasmuch as the AGB stars are the main source of S-process nucleosynthesis and have atmospheres observed by astronomers to be highly enriched in dredged-up s process elements.\n\nAnother dramatic example is given by the so-called supernova condensates, usually shortened by acronym to SUNOCON (from SUperNOva CONdensate) to distinguish them from other stardust condensed within stellar atmospheres. SUNOCONs contain in their calcium an excessively large abundance of Ca, demonstrating that they condensed containing abundant radioactive Ti, which has a 65-year half-life. The outflowing Ti nuclei were thus still \"alive\" (radioactive) when the SUNOCON condensed near one year within the expanding supernova interior, but would have become an extinct radionuclide (specifically Ca) after the time required for mixing with the interstellar gas. Its discovery proved the prediction from 1975 that it might be possible to identify SUNOCONs in this way. The SiC SUNOCONs (from supernovae) are only about 1% as numerous as are SiC stardust from AGB stars.\n\nStardust itself (SUNOCONs and AGB grains that come from specific stars) is but a modest fraction of the condensed cosmic dust, forming less than 0.1% of the mass of total interstellar solids. The high interest in stardust derives from new information that it has brought to the sciences of stellar evolution and nucleosynthesis.\n\nLaboratories have studied solids that existed before the Earth was formed. This was once thought impossible, especially in the 1970s when cosmochemists were confident that the Solar System began as a hot gas virtually devoid of any remaining solids, which would have been vaporized by high temperature. The existence of stardust proved this historic picture incorrect.\n\n Cosmic dust is made of dust grains and aggregates of dust grains. These particles are irregularly shaped, with porosity ranging from \"fluffy\" to \"compact\". The composition, size, and other properties depend on where the dust is found, and conversely, a compositional analysis of a dust particle can reveal much about the dust particle's origin. General diffuse interstellar medium dust, dust grains in dense clouds, planetary rings dust, and circumstellar dust, are each different in their characteristics. For example, grains in dense clouds have acquired a mantle of ice and on average are larger than dust particles in the diffuse interstellar medium. \"Interplanetary dust particles\" (IDPs) are generally larger still.\nMost of the influx of extraterrestrial matter that falls onto the Earth is dominated by meteoroids with diameters in the range 50 to 500 micrometers, of average density 2.0 g/cm³ (with porosity about 40%). The total influx rate of meteoritic sities of most IDPs captured in the Earth's stratosphere range between 1 and 3 g/cm³, with an average density at about 2.0 g/cm³.\n\nOther specific dust properties:\n\nThe large grains in interstellar space are probably complex, with refractory cores that condensed within stellar outflows topped by layers acquired subsequently during incursions into cold dense interstellar clouds. That cyclic process of growth and destruction outside of the clouds has been modeled to demonstrate that the cores live much longer than the average lifetime of dust mass. Those cores mostly start with silicate particles condensing in the atmospheres of cool oxygen rich red-giant stars and carbon grains condensing in the atmospheres of cool carbon stars. The red-giant stars have evolved off the main sequence and have entered the giant phase of their evolution and are the major source of refractory dust grain cores in galaxies. Those refractory cores are also called Stardust (section above), which is a scientific term for the small fraction of cosmic dust that condensed thermally within stellar gases as they were ejected from the stars. Several percent of refractory grain cores have condensed within expanding interiors of supernovae, a type of cosmic decompression chamber. And meteoriticists that study this refractory stardust extracted from meteorites often call it presolar grains, although the refractory stardust that they study is actually only a small fraction of all presolar dust. Stardust condenses within the stars via considerably different condensation chemistry than that of the bulk of cosmic dust, which accretes cold onto preexisting dust in dark molecular clouds of the galaxy. Those molecular clouds are very cold, typically less than 50K, so that ices of many kinds may accrete onto grains, perhaps to be destroyed later. Finally, when the Solar System formed, interstellar dust grains were further modified by chemical reactions within the planetary accretion disk. So the history of the complex grains in the early Solar System is complicated and only partially understood.\n\nAstronomers know that the dust is formed in the envelopes of late-evolved stars from specific observational signatures. In infrared light, emission at 9.7 micrometres is a signature of silicate dust in cool evolved oxygen-rich giant stars. Emission at 11.5 micrometres indicates the presence of silicon carbide dust in cool evolved carbon-rich giant stars. These help provide evidence that the small silicate particles in space came from the ejected outer envelopes of these stars.\n\nConditions in interstellar space are generally not suitable for the formation of silicate cores. This would take excessive time to accomplish, even if it might be possible. The arguments are that: given an observed typical grain diameter \"a\", the time for a grain to attain \"a\", and given the temperature of interstellar gas, it would take considerably longer than the age of the Universe for interstellar grains to form. On the other hand, grains are seen to have recently formed in the vicinity of nearby stars, in nova and supernova ejecta, and in R Coronae Borealis variable stars which seem to eject discrete clouds containing both gas and dust. So mass loss from stars is unquestionably where the refractory cores of grains formed.\n\nMost dust in the Solar System is highly processed dust, recycled from the material out of which the Solar System formed and subsequently collected in the planetesimals, and leftover solid material such as comets and asteroids, and reformed in each of those bodies' collisional lifetimes. During the Solar System's formation history, the most abundant element was (and still is) H. The metallic elements: magnesium, silicon, and iron, which are the principal ingredients of rocky planets, condensed into solids at the highest temperatures of the planetary disk. Some molecules such as CO, N, NH, and free oxygen, existed in a gas phase. Some molecules, for example, graphite (C) and SiC would condense into solid grains in the planetary disk; but carbon and SiC grains found in meteorites are presolar based on their isotopic compositions, rather than from the planetary disk formation. Some molecules also formed complex organic compounds and some molecules formed frozen ice mantles, of which either could coat the \"refractory\" (Mg, Si, Fe) grain cores. Stardust once more provides an exception to the general trend, as it appears to be totally unprocessed since its thermal condensation within stars as refractory crystalline minerals. The condensation of graphite occurs within supernova interiors as they expand and cool, and do so even in gas containing more oxygen than carbon, a surprising carbon chemistry made possible by the intense radioactive environment of supernovae. This special example of dust formation has merited specific review.\n\nPlanetary disk formation of precursor molecules was determined, in large part, by the temperature of the solar nebula. Since the temperature of the solar nebula decreased with heliocentric distance, scientists can infer a dust grain's origin(s) with knowledge of the grain's materials. Some materials could only have been formed at high temperatures, while other grain materials could only have been formed at much lower temperatures. The materials in a single interplanetary dust particle often show that the grain elements formed in different locations and at different times in the solar nebula. Most of the matter present in the original solar nebula has since disappeared; drawn into the Sun, expelled into interstellar space, or reprocessed, for example, as part of the planets, asteroids or comets.\n\nDue to their highly processed nature, IDPs (interplanetary dust particles) are fine-grained mixtures of thousands to millions of mineral grains and amorphous components. We can picture an IDP as a \"matrix\" of material with embedded elements which were formed at different times and places in the solar nebula and before the solar nebula's formation. Examples of embedded elements in cosmic dust are GEMS, chondrules, and CAIs.\n\nThe arrows in the adjacent diagram show one possible path from a collected interplanetary dust particle back to the early stages of the solar nebula.\n\nWe can follow the trail to the right in the diagram to the IDPs that contain the most volatile and primitive elements. The trail takes us first from interplanetary dust particles to chondritic interplanetary dust particles. Planetary scientists classify chondritic IDPs in terms of their diminishing degree of oxidation so that they fall into three major groups: the carbonaneous, the ordinary, and the enstatite chondrites. As the name implies, the carbonaceous chondrites are rich in carbon, and many have anomalies in the isotopic abundances of H, C, N, and O (Jessberger, 2000). From the carbonaceous chondrites, we follow the trail to the most primitive materials. They are almost completely oxidized and contain the lowest condensation temperature elements (\"volatile\" elements) and the largest amount of organic compounds. Therefore, dust particles with these elements are thought to be formed in the early life of the Solar System. The volatile elements have never seen temperatures above about 500 K, therefore, the IDP grain \"matrix\" consists of some very primitive Solar System material. Such a scenario is true in the case of comet dust. The provenance of the small fraction that is stardust (see above) is quite different; these refractory interstellar minerals thermally condense within stars, become a small component of interstellar matter, and therefore remain in the presolar planetary disk. Nuclear damage tracks are caused by the ion flux from solar flares. Solar wind ions impacting on the particle's surface produce amorphous radiation damaged rims on the particle's surface. And spallogenic nuclei are produced by galactic and solar cosmic rays. A dust particle that originates in the Kuiper Belt at 40 AU would have many more times the density of tracks, thicker amorphous rims and higher integrated doses than a dust particle originating in the main-asteroid belt.\n\nBased on 2012 computer model studies, the complex organic molecules necessary for life may have formed in the protoplanetary disk of dust grains surrounding the Sun before the formation of the Earth. According to the computer studies, this same process may also occur around other stars that acquire planets. (Also see Extraterrestrial organic molecules.)\n\nIn September 2012, NASA scientists reported that polycyclic aromatic hydrocarbons (PAHs), subjected to interstellar medium (ISM) conditions, are transformed, through hydrogenation, oxygenation and hydroxylation, to more complex organics - \"a step along the path toward amino acids and nucleotides, the raw materials of proteins and DNA, respectively\". Further, as a result of these transformations, the PAHs lose their spectroscopic signature which could be one of the reasons \"for the lack of PAH detection in interstellar ice grains, particularly the outer regions of cold, dense clouds or the upper molecular layers of protoplanetary disks.\"\n\nIn February 2014, NASA announced a greatly upgraded database for detecting and monitoring polycyclic aromatic hydrocarbons (PAHs) in the universe. According to NASA scientists, over 20% of the carbon in the Universe may be associated with PAHs, possible starting materials for the formation of life. PAHs seem to have been formed shortly after the Big Bang, are abundant in the Universe, and are associated with new stars and exoplanets.\n\nIn March 2015, NASA scientists reported that, for the first time, complex DNA and RNA organic compounds of life, including uracil, cytosine and thymine, have been formed in the laboratory under outer space conditions, using starting chemicals, such as pyrimidine, found in meteorites. Pyrimidine, like polycyclic aromatic hydrocarbons (PAHs), the most carbon-rich chemical found in the Universe, may have been formed in red giants or in interstellar dust and gas clouds, according to the scientists.\n\nThe Solar System has its own interplanetary dust cloud, as do extrasolar systems. There are different types of nebulae with different physical causes and processes: \n\nDistinctions between those types of nebula are that different radiation processes are at work. For example, H II regions, like the Orion Nebula, where a lot of star-formation is taking place, are characterized as thermal emission nebulae. Supernova remnants, on the other hand, like the Crab Nebula, are characterized as nonthermal emission (synchrotron radiation).\n\nSome of the better known dusty regions in the Universe are the diffuse nebulae in the Messier catalog, for example: M1, M8, M16, M17, M20, M42, M43.\n\nSome larger dust catalogs are:\n\nThe Discovery program's \"Stardust\" mission, was launched on 7 February 1999 to collect samples from the coma of comet Wild 2, as well as samples of cosmic dust. It returned samples to Earth on 15 January 2006. In the spring of 2014, the recovery of particles of interstellar dust from the samples was announced.\n\n"}
{"id": "38390", "url": "https://en.wikipedia.org/wiki?curid=38390", "title": "Dementia", "text": "Dementia\n\nDementia is a broad category of brain diseases that cause a long-term and often gradual decrease in the ability to think and remember that is great enough to affect a person's daily functioning. Other common symptoms include emotional problems, difficulties with language, and a decrease in motivation. A person's consciousness is usually not affected. A dementia diagnosis requires a change from a person's usual mental functioning and a greater decline than one would expect due to aging. These diseases also have a significant effect on a person's caregivers.\nThe most common type of dementia is Alzheimer's disease, which makes up 50% to 70% of cases. Other common types include vascular dementia (25%), Lewy body dementia (15%), and frontotemporal dementia. Less common causes include normal pressure hydrocephalus, Parkinson's disease dementia, syphilis, and Creutzfeldt–Jakob disease among others. More than one type of dementia may exist in the same person. A small proportion of cases run in families. In the DSM-5, dementia was reclassified as a neurocognitive disorder, with various degrees of severity. Diagnosis is usually based on history of the illness and cognitive testing with medical imaging and blood tests used to rule out other possible causes. The mini mental state examination is one commonly used cognitive test. Efforts to prevent dementia include trying to decrease risk factors such as high blood pressure, smoking, diabetes, and obesity. Screening the general population for the disorder is not recommended.\nThere is no known cure for dementia. Cholinesterase inhibitors such as donepezil are often used and may be beneficial in mild to moderate disorder. Overall benefit, however, may be minor. There are many measures that can improve the quality of life of people with dementia and their caregivers. Cognitive and behavioral interventions may be appropriate. Educating and providing emotional support to the caregiver is important. Exercise programs may be beneficial with respect to activities of daily living and potentially improve outcomes. Treatment of behavioral problems with antipsychotics is common but not usually recommended due to the little benefit and side effects, including an increased risk of death.\nGlobally, dementia affected about 46 million people in 2015. About 10% of people develop the disorder at some point in their lives. It becomes more common with age. About 3% of people between the ages of 65–74 have dementia, 19% between 75 and 84, and nearly half of those over 85 years of age. In 2013 dementia resulted in about 1.7 million deaths up from 0.8 million in 1990. As more people are living longer, dementia is becoming more common in the population as a whole. For people of a specific age, however, it may be becoming less frequent, at least in the developed world, due to a decrease in risk factors. It is one of the most common causes of disability among the old. It is believed to result in economic costs of 604 billion USD a year. People with dementia are often physically or chemically restrained to a greater degree than necessary, raising issues of human rights. Social stigma against those affected is common.\n\nThe symptoms of dementia vary across types and stages of the diagnosis. The most common affected areas include memory, visual-spatial, language, attention and problem solving. Most types of dementia are slow and progressive. By the time the person shows signs of the disorder, the process in the brain has been happening for a long time. It is possible for a patient to have two types of dementia at the same time. About 10% of people with dementia have what is known as \"mixed dementia\", which is usually a combination of Alzheimer's disease and another type of dementia such as frontotemporal dementia or vascular dementia.\n\nNeuropsychiatric symptoms that may be present are termed \"Behavioural and psychological symptoms of dementia\" (BPSD) and these can include: \n\nWhen people with dementia are put in circumstances beyond their abilities, there may be a sudden change to crying or anger (a \"catastrophic reaction\").\n\nPsychosis (often delusions of persecution) and agitation/aggression also often accompany dementia.\n\nIn the first stages of dementia, the signs and symptoms of the disorder may be subtle. Often, the early signs of dementia only become apparent when looking back in time. The earliest stage of dementia is called mild cognitive impairment (MCI). 70% of those diagnosed with MCI progress to dementia at some point. In MCI, changes in the person's brain have been happening for a long time, but the symptoms of the disorder are just beginning to show. These problems, however, are not yet severe enough to affect the person’s daily function. If they do, it is considered dementia. A person with MCI scores between 27 and 30 on the Mini-Mental State Examination (MMSE), which is a normal score. They may have some memory trouble and trouble finding words, but they solve everyday problems and handle their own life affairs well.\n\nIn the early stage of dementia, the person begins to show symptoms noticeable to the people around them. In addition, the symptoms begin to interfere with daily activities. The person usually scores between a 20 and 25 on the MMSE. The symptoms are dependent on the type of dementia a person has. The person may begin to have difficulty with more complicated chores and tasks around the house or at work. The person can usually still take care of him or herself but may forget things like taking pills or doing laundry and may need prompting or reminders.\n\nThe symptoms of early dementia usually include memory difficulty, but can also include some word-finding problems (anomia) and problems with planning and organizational skills (executive function). One very good way of assessing a person's impairment is by asking if they are still able to handle their finances independently. This is often one of the first things to become problematic. Other signs might be getting lost in new places, repeating things, personality changes, social withdrawal and difficulties at work.\n\nWhen evaluating a person for dementia, it is important to consider how the person was able to function five or ten years earlier. It is also important to consider a person's level of education when assessing for loss of function. For example, an accountant who can no longer balance a checkbook would be more concerning than a person who had not finished high school or had never taken care of his/her own finances.\n\nIn Alzheimer's dementia the most prominent early symptom is memory difficulty. Others include word-finding problems and getting lost. In other types of dementia, like dementia with Lewy bodies and fronto-temporal dementia, personality changes and difficulty with organization and planning may be the first signs.\n\nAs dementia progresses, the symptoms first experienced in the early stages of the dementia generally worsen. The rate of decline is different for each person. A person with moderate dementia scores between 6–17 on the MMSE. For example, people with Alzheimer's dementia in the moderate stages lose almost all new information very quickly. People with dementia may be severely impaired in solving problems, and their social judgment is usually also impaired. They cannot usually function outside their own home, and generally should not be left alone. They may be able to do simple chores around the house but not much else, and begin to require assistance for personal care and hygiene other than simple reminders.\n\nPeople with late-stage dementia typically turn increasingly inward and need assistance with most or all of their personal care. Persons with dementia in the late stages usually need 24-hour supervision to ensure personal safety, as well as to ensure that basic needs are being met. If left unsupervised, a person with late-stage dementia may wander or fall, may not recognize common dangers around them such as a hot stove, may not realize that they need to use the bathroom or become unable to control their bladder or bowels (incontinent).\n\nChanges in eating frequently occur. Caregivers of people with late-stage dementia often provide pureed diets, thickened liquids, and assistance in eating, to prolong their lives, to cause them to gain weight, to reduce the risk of choking, and to make feeding the person easier. The person's appetite may decline to the point that the person does not want to eat at all. They may not want to get out of bed, or may need complete assistance doing so. Commonly, the person no longer recognizes familiar people. They may have significant changes in sleeping habits or have trouble sleeping at all.\n\nThere are four main causes of easily reversible dementia: hypothyroidism, vitamin B12 deficiency, Lyme disease, and neurosyphillis. All people with memory difficulty should be checked for hypothyroidism and B12 deficiency. For Lyme disease and neurosyphilis, testing should be done if there are risk factors for those diseases in the person. Because risk factors are often difficult to determine, testing for neurosyphillis and Lyme disease, as well as other mentioned factors, may be undertaken as a matter of course in cases where dementia is suspected.\n\nAlzheimer's disease accounts for 50% to 70% of cases of dementia. The most common symptoms of Alzheimer's disease are short-term memory loss and word-finding difficulties. People with Alzheimer's disease also have trouble with visual-spatial areas (for example, they may begin to get lost often), reasoning, judgment, and insight. Insight refers to whether or not the person realizes he/she has memory problems.\n\nCommon early symptoms of Alzheimer's include repetition, getting lost, difficulties keeping track of bills, problems with cooking especially new or complicated meals, forgetting to take medication, and word-finding problems.\n\nThe part of the brain most affected by Alzheimer's is the hippocampus. Other parts of the brain that show shrinking (atrophy) include the temporal and parietal lobes. Although this pattern suggests Alzheimer's, the brain shrinkage in Alzheimer's disease is very variable, and a scan of the brain cannot actually make the diagnosis. The relationship between undergoing anesthesia and AD is unclear.\n\nVascular dementia is the cause of at least 20% of dementia cases, making it the second most common cause of dementia. It is caused by disease or injury affecting the blood supply to the brain, typically involving a series of minor strokes. The symptoms of this dementia depend on where in the brain the strokes have occurred and whether the vessels are large or small. Multiple injuries can cause progressive dementia over time, while a single injury located in an area critical for cognition (i.e. hippocampus, thalamus) can lead to sudden cognitive decline.\n\nOn scans of the brain, a person with vascular dementia may show evidence of multiple strokes of different sizes in various locations. People with vascular dementia tend to have risk factors for disease of the blood vessels, such as tobacco use, high blood pressure, atrial fibrillation, high cholesterol or diabetes, or other signs of vascular disease such as a previous heart attack or angina.\n\nDementia with Lewy bodies (DLB) is a dementia that has the primary symptoms of visual hallucinations and \"Parkinsonism\". Parkinsonism is the symptoms of Parkinson's disease, which includes tremor, rigid muscles, and a face without emotion. The visual hallucinations in DLB are generally very vivid hallucinations of people or animals and they often occur when someone is about to fall asleep or just waking up. Other prominent symptoms include problems with attention, organization, problem solving and planning (executive function), and difficulty with visual-spatial function.\n\nAgain, imaging studies cannot necessarily make the diagnosis of DLB, but some signs are particularly common. A person with DLB often shows occipital hypoperfusion on SPECT scan or occipital hypometabolism on a PET scan. Generally, a diagnosis of DLB is straightforward and unless it is complicated, a brain scan is not always necessary.\n\nFrontotemporal dementias (FTDs) are characterized by drastic personality changes and language difficulties. In all FTDs, the person has a relatively early social withdrawal and early lack of insight into the disorder. Memory problems are not a main feature of this disorder.\n\nThere are three main types of FTD. The first has major symptoms in the area of personality and behavior. This is called behavioral variant FTD (bv-FTD) and is the most common. In bv-FTD, the person shows a change in personal hygiene, becomes rigid in their thinking, and rarely recognize that there is a problem, they are socially withdrawn, and often have a drastic increase in appetite. They may also be socially inappropriate. For example, they may make inappropriate sexual comments, or may begin using pornography openly when they had not before. One of the most common signs is apathy, or not caring about anything. Apathy, however, is a common symptom in many different dementias.\n\nThe other two types of FTD feature language problems as the main symptom. The second type is called semantic dementia or temporal variant dementia (TV-FTD). The main feature of this is the loss of the meaning of words. It may begin with difficulty naming things. The person eventually may also lose the meaning of objects as well. For example, a drawing of a bird, dog, and an airplane in someone with FTD may all appear just about the same. In a classic test for this, a patient is shown a picture of a pyramid and below there is a picture of both a palm tree and a pine tree. The person is asked to say which one goes best with the pyramid. In TV-FTD the person would not be able to answer that question.\n\nThe last type of FTD is called progressive non-fluent aphasia (PNFA). This is mainly a problem with producing speech. They have trouble finding the right words, but mostly they have a difficulty coordinating the muscles they need to speak. Eventually, someone with PNFA only uses one-syllable words or may become totally mute.\n\nProgressive supranuclear palsy (PSP) is a form of dementia that is characterized by problems with eye movements. Generally the problems begin with difficulty moving the eyes up or down (vertical gaze palsy). Since difficulty moving the eyes upward can sometimes happen in normal aging, problems with downward eye movements are the key in PSP. Other key symptoms of PSP include falling backwards, balance problems, slow movements, rigid muscles, irritability, apathy, social withdrawal, and depression. The person may also have certain \"frontal lobe signs\" such as perseveration, a grasp reflex and utilization behavior (the need to use an object once you see it). People with PSP often have progressive difficulty eating and swallowing, and eventually with talking as well. Because of the rigidity and slow movements, PSP is sometimes misdiagnosed as Parkinson's disease.\n\nOn scans of the brain, the midbrain of people with PSP is generally shrunken (atrophied), but there are no other common brain abnormalities visible on images of the person's brain.\n\nCorticobasal degeneration is a rare form of dementia that is characterized by many different types of neurological problems that get progressively worse over time. This is because the disorder affects the brain in many different places, but at different rates. One common sign is difficulty with using only one limb. One symptom that is extremely rare in any condition other than corticobasal degeneration is the \"alien limb.\" The alien limb is a limb of the person that seems to have a mind of its own, it moves without control of the person's brain. Other common symptoms include jerky movements of one or more limbs (myoclonus), symptoms that are different in different limbs (asymmetric), difficulty with speech that is due to not being able to move the mouth muscles in a coordinated way, numbness and tingling of the limbs and neglecting one side of the person's vision or senses. In neglect, a person ignores the opposite side of the body from the one that has the problem. For example, a person may not feel pain on one side, or may only draw half of a picture when asked. In addition, the person's affected limbs may be rigid or have muscle contractions causing strange repetitive movements (dystonia).\n\nThe area of the brain most often affected in corticobasal degeneration is the posterior frontal lobe and parietal lobe. Still, many other part of the brain can be affected.\n\nCreutzfeldt–Jakob disease typically causes a dementia that worsens over weeks to months, and is caused by prions. The common causes of slowly progressive dementia also sometimes present with rapid progression: Alzheimer's disease, dementia with Lewy bodies, frontotemporal lobar degeneration (including corticobasal degeneration and progressive supranuclear palsy).\n\nOn the other hand, encephalopathy or delirium may develop relatively slowly and resemble dementia. Possible causes include brain infection (viral encephalitis, subacute sclerosing panencephalitis, Whipple's disease) or inflammation (limbic encephalitis, Hashimoto's encephalopathy, cerebral vasculitis); tumors such as lymphoma or glioma; drug toxicity (e.g., anticonvulsant drugs); metabolic causes such as liver failure or kidney failure; and chronic subdural hematoma.\n\nChronic inflammatory conditions that may affect the brain and cognition include Behçet's disease, multiple sclerosis, sarcoidosis, Sjögren's syndrome, systemic lupus erythematosus, celiac disease, and non-celiac gluten sensitivity. These types of dementias can rapidly progress, but usually have a good response to early treatment. This consists of immunomodulators or steroid administration, or in certain cases, the elimination of the causative agent.\n\nThere are many other medical and neurological conditions in which dementia only occurs late in the illness. For example, a proportion of patients with Parkinson's disease develop dementia, though widely varying figures are quoted for this proportion. When dementia occurs in Parkinson's disease, the underlying cause may be dementia with Lewy bodies or Alzheimer's disease, or both. Cognitive impairment also occurs in the Parkinson-plus syndromes of progressive supranuclear palsy and corticobasal degeneration (and the same underlying pathology may cause the clinical syndromes of frontotemporal lobar degeneration). Although the acute porphyrias may cause episodes of confusion and psychiatric disturbance, dementia is a rare feature of these rare diseases.\n\nAside from those mentioned above, inherited conditions that can cause dementia (alongside other symptoms) include:\n\nMild cognitive impairment means that the person exhibits memory or thinking difficulties, but those difficulties are not severe enough to meet criteria for a diagnosis of dementia. They should score between 25–30 on the MMSE. Around 70% of people with MCI go on to develop some form of dementia. MCI is generally divided into two categories. The first is one that is primarily memory loss (amnestic MCI). The second category is anything that is not primarily memory difficulties (non-amnestic MCI). People with primarily memory problems generally go on to develop Alzheimer's disease. People with the other type of MCI may go on to develop other types of dementia.\n\nDiagnosis of MCI is often difficult, as cognitive testing may be normal. Often, more in-depth neuropsychological testing is necessary to make the diagnosis. the most commonly used criteria are called the Peterson criteria and include:\n\nVarious types of brain injury may cause irreversible cognitive impairment that remains stable over time. Traumatic brain injury may cause generalized damage to the white matter of the brain (diffuse axonal injury), or more localized damage (as also may neurosurgery). A temporary reduction in the brain's supply of blood or oxygen may lead to hypoxic-ischemic injury. Strokes (ischemic stroke, or intracerebral, subarachnoid, subdural or extradural hemorrhage) or infections (meningitis or encephalitis) affecting the brain, prolonged epileptic seizures, and acute hydrocephalus may also have long-term effects on cognition. Excessive alcohol use may cause alcohol dementia, Wernicke's encephalopathy, or Korsakoff's psychosis.\n\nDementia that begins gradually and worsens progressively over several years is usually caused by neurodegenerative disease—that is, by conditions that affect only or primarily the neurons of the brain and cause gradual but irreversible loss of function of these cells. Less commonly, a non-degenerative condition may have secondary effects on brain cells, which may or may not be reversible if the condition is treated.\n\nCauses of dementia depend on the age when symptoms begin. In the elderly population (usually defined in this context as over 65 years of age), a large majority of dementia cases are caused by Alzheimer's disease, vascular dementia, or both. Dementia with Lewy bodies is another commonly exhibited form, which again may occur alongside either or both of the other causes. Hypothyroidism sometimes causes slowly progressive cognitive impairment as the main symptom, and this may be fully reversible with treatment. Normal pressure hydrocephalus, though relatively rare, is important to recognize since treatment may prevent progression and improve other symptoms of the condition. However, significant cognitive improvement is unusual.\n\nDementia is much less common under 65 years of age. Alzheimer's disease is still the most frequent cause, but inherited forms of the disorder account for a higher proportion of cases in this age group. Frontotemporal lobar degeneration and Huntington's disease account for most of the remaining cases. Vascular dementia also occurs, but this in turn may be due to underlying conditions (including antiphospholipid syndrome, CADASIL, MELAS, homocystinuria, moyamoya, and Binswanger's disease). People who receive frequent head trauma, such as boxers or football players, are at risk of chronic traumatic encephalopathy (also called dementia pugilistica in boxers).\n\nIn young adults (up to 40 years of age) who were previously of normal intelligence, it is very rare to develop dementia without other features of neurological disease, or without features of disease elsewhere in the body. Most cases of progressive cognitive disturbance in this age group are caused by psychiatric illness, alcohol or other drugs, or metabolic disturbance. However, certain genetic disorders can cause true neurodegenerative dementia at this age. These include familial Alzheimer's disease, SCA17 (dominant inheritance); adrenoleukodystrophy (X-linked); Gaucher's disease type 3, metachromatic leukodystrophy, Niemann-Pick disease type C, pantothenate kinase-associated neurodegeneration, Tay-Sachs disease, and Wilson's disease (all recessive). Wilson's disease is particularly important since cognition can improve with treatment.\n\nAt all ages, a substantial proportion of patients who complain of memory difficulty or other cognitive symptoms have depression rather than a neurodegenerative disease. Vitamin deficiencies and chronic infections may also occur at any age; they usually cause other symptoms before dementia occurs, but occasionally mimic degenerative dementia. These include deficiencies of vitamin B, folate, or niacin, and infective causes including cryptococcal meningitis, AIDS, Lyme disease, progressive multifocal leukoencephalopathy, subacute sclerosing panencephalitis, syphilis, and Whipple's disease.\n\nAs seen above, there are many specific types and causes of dementia, often showing slightly different symptoms. However, the symptoms are very similar and it is usually difficult to diagnose the type of dementia by symptoms alone. Diagnosis may be aided by brain scanning techniques. In many cases, the diagnosis cannot be absolutely sure except with a brain biopsy, but this is very rarely recommended (though it can be performed at autopsy). In those who are getting older, general screening for cognitive impairment using cognitive testing or early diagnosis of dementia has not been shown to improve outcomes. However, it has been shown that screening exams are useful in those people over the age of 65 with memory complaints.\n\nNormally, symptoms must be present for at least six months to support a diagnosis. Cognitive dysfunction of shorter duration is called \"delirium\". Delirium can be easily confused with dementia due to similar symptoms. Delirium is characterized by a sudden onset, fluctuating course, a short duration (often lasting from hours to weeks), and is primarily related to a somatic (or medical) disturbance. In comparison, dementia has typically a long, slow onset (except in the cases of a stroke or trauma), slow decline of mental functioning, as well as a longer duration (from months to years).\n\nSome mental illnesses, including depression and psychosis, may produce symptoms that must be differentiated from both delirium and dementia. Therefore, any dementia evaluation should include a depression screening such as the Neuropsychiatric Inventory or the Geriatric Depression Scale. Physicians used to think that anyone who came in with memory complaints had depression and not dementia (because they thought that those with dementia are generally unaware of their memory problems). This is called pseudodementia. However, in recent years researchers have realized that many older people with memory complaints in fact have MCI, the earliest stage of dementia. Depression should always remain high on the list of possibilities, however, for an elderly person with memory trouble.\n\nChanges in thinking, hearing and vision are associated with normal ageing and can cause problems when diagnosing dementia due to the similarities.\n\nThere are some brief tests (5–15 minutes) that have reasonable reliability to screen for dementia.\nWhile many tests have been studied, presently the mini mental state examination (MMSE) is the best studied and most commonly used. The MMSE is a useful tool for helping to diagnose dementia if the results are interpreted along with an assessment of a person's personality, their ability to perform activities of daily living, and their behaviour. Other cognitive tests include the abbreviated mental test score (AMTS), the, \"Modified Mini-Mental State Examination\" (3MS), the \"Cognitive Abilities Screening Instrument\" (CASI), the Trail-making test, and the clock drawing test. The MOCA (Montreal Cognitive Assessment) is a very reliable screening test and is available online for free in 35 different languages. The MOCA has also been shown somewhat better at detecting mild cognitive impairment than the MMSE.\n\nAnother approach to screening for dementia is to ask an informant (relative or other supporter) to fill out a questionnaire about the person's everyday cognitive functioning. Informant questionnaires provide complementary information to brief cognitive tests. Probably the best known questionnaire of this sort is the \"Informant Questionnaire on Cognitive Decline in the Elderly (IQCODE)\". There is not sufficient evidence to determine how accurate the IQCODE is for diagnosing or predicting dementia. The Alzheimer's Disease Caregiver Questionnaire is another tool. It is about 90% accurate for Alzheimer's and can be completed online or in the office by a caregiver. On the other hand, the \"General Practitioner Assessment Of Cognition\" combines both, a patient assessment and an informant interview. It was specifically designed for the use in the primary care setting.\n\nClinical neuropsychologists provide diagnostic consultation following administration of a full battery of cognitive testing, often lasting several hours, to determine functional patterns of decline associated with varying types of dementia. Tests of memory, executive function, processing speed, attention, and language skills are relevant, as well as tests of emotional and psychological adjustment. These tests assist with ruling out other etiologies and determining relative cognitive decline over time or from estimates of prior cognitive abilities.\n\nRoutine blood tests are also usually performed to rule out treatable causes. These tests include vitamin B, folic acid, thyroid-stimulating hormone (TSH), C-reactive protein, full blood count, electrolytes, calcium, renal function, and liver enzymes. Abnormalities may suggest vitamin deficiency, infection, or other problems that commonly cause confusion or disorientation in the elderly.\n\nA CT scan or magnetic resonance imaging (MRI scan) is commonly performed, although these tests do not pick up diffuse metabolic changes associated with dementia in a person that shows no gross neurological problems (such as paralysis or weakness) on neurological exam. CT or MRI may suggest normal pressure hydrocephalus, a potentially reversible cause of dementia, and can yield information relevant to other types of dementia, such as infarction (stroke) that would point at a vascular type of dementia.\n\nThe functional neuroimaging modalities of SPECT and PET are more useful in assessing long-standing cognitive dysfunction, since they have shown similar ability to diagnose dementia as a clinical exam and cognitive testing. The ability of SPECT to differentiate the vascular cause (i.e., multi-infarct dementia) from Alzheimer's disease dementias, appears superior to differentiation by clinical exam.\n\nRecent research has established the value of PET imaging using carbon-11 Pittsburgh Compound B as a radiotracer (PIB-PET) in predictive diagnosis of various kinds of dementia, in particular Alzheimer's disease. Studies from Australia have found PIB-PET 86% accurate in predicting which patients with mild cognitive impairment will develop Alzheimer's disease within two years. In another study, carried out using 66 patients seen at the University of Michigan, PET studies using either PIB or another radiotracer, carbon-11 dihydrotetrabenazine (DTBZ), led to more accurate diagnosis for more than one-fourth of patients with mild cognitive impairment or mild dementia.\n\nA number of factors can decrease the risk of dementia. A group of efforts is believed to be able to prevent a third of cases and include early education, treating high blood pressure, preventing obesity, preventing hearing loss, treating depression, being active, preventing diabetes, not smoking, and preventing social isolation. A 2018 review however concluded that no medications have good evidence of a preventative effect including blood pressure medications.\n\nAmong otherwise healthy older people, computerized cognitive training may improve memory. However it is not known if it prevents dementia. Short term exercise has limited evidence. In those with normal mental function evidence for medications is poor. The same applies to supplements.\n\nExcept for the treatable types listed above, there is no cure. Cholinesterase inhibitors are often used early in the disorder course; however, benefit is generally small. Cognitive and behavioral interventions may be appropriate. There is some evidence that educating and providing support for the person with dementia, as well as caregivers and family members, improves outcomes. Exercise programs are beneficial with respect to activities of daily living and potentially improve dementia.\n\nPsychological therapies for dementia include some limited evidence for reminiscence therapy (namely, some positive effects in the areas of quality of life, cognition, communication and mood - the first three particularly in care home settings), some benefit for cognitive reframing for caretakers, unclear evidence for validation therapy, and tentative evidence for mental exercises, such as cognitive stimulation programs for people with mild to moderate dementia. Reminiscence therapy can improve quality of life, cognition, communication, and possibly mood in people with dementia in some circumstances, although all of these benefits may be small.\n\nAdult daycare centers as well as special care units in nursing homes often provide specialized care for dementia patients. Adult daycare centers offer supervision, recreation, meals, and limited health care to participants, as well as providing respite for caregivers. In addition, home care can provide one-on-one support and care in the home allowing for more individualized attention that is needed as the disorder progresses. Psychiatric nurses can make a distinctive contribution to people's mental health.\n\nSince dementia impairs normal communication due to changes in receptive and expressive language, as well as the ability to plan and problem solve, agitated behaviour is often a form of communication for the person with dementia. Actively searching for a potential cause, such as pain, physical illness, or overstimulation can be helpful in reducing agitation. Additionally, using an \"ABC analysis of behaviour\" can be a useful tool for understanding behavior in people with dementia. It involves looking at the antecedents (A), behavior (B), and consequences (C) associated with an event to help define the problem and prevent further incidents that may arise if the person's needs are misunderstood. There is low quality evidence that regular (at least five sessions of) music therapy may be help resident in institutions. It may reduce depressive symptoms and improves overall behavioural problems. There may also be a beneficial effect on emotional well-being and quality of life, as well as reducing anxiety.\n\nNo medications have been shown to prevent or cure dementia. Medications may be used to treat the behavioural and cognitive symptoms but have no effect on the underlying disease process.\n\nAcetylcholinesterase inhibitors, such as donepezil, may be useful for Alzheimer disease and dementia in Parkinson's, DLB, or vascular dementia. The quality of the evidence however is poor and the benefit is small. No difference has been shown between the agents in this family. In a minority of people side effects include a slow heart rate and fainting.\n\nAs assessment for an underlying cause of the behavior is a needed before prescribing antipsychotic medication for symptoms of dementia. Antipsychotic drugs should be used to treat dementia only if non-drug therapies have not worked, and the person's actions threaten themselves or others. Aggressive behavior changes are sometimes the result of other solvable problems, that could make treatment with antipsychotics unnecessary. Because people with dementia can be aggressive, resistant to their treatment, and otherwise disruptive, sometimes antipsychotic drugs are considered as a therapy in response. These drugs have risky adverse effects, including increasing the patient's chance of stroke and death. Generally, stopping antipsychotics for people with dementia does not cause problems, even in those who have been on them a long time.\n\nN-methyl-D-aspartate (NMDA) receptor blockers such as memantine may be of benefit but the evidence is less conclusive than for AChEIs. Due to their differing mechanisms of action memantine and acetylcholinesterase inhibitors can be used in combination however the benefit is slight.\n\nWhile depression is frequently associated with dementia, selective serotonin reuptake inhibitors (SSRIs) do not appear to affect outcomes. The SSRIs sertraline and citalopram have been demonstrated to reduce symptoms of agitation, compared to placebo.\n\nThe use of medications to alleviate sleep disturbances that people with dementia often experience has not been well researched, even for medications that are commonly prescribed. In 2012 the American Geriatrics Society recommended that benzodiazepines such as diazepam, and non-benzodiazepine hypnotics, be avoided for people with dementia due to the risks of increased cognitive impairment and falls. Additionally, there is little evidence for the effectiveness of benzodiazepines in this population. There is no clear evidence that melatonin or ramelteon improves sleep for people with dementia due to Alzheimer's disease. There is limited evidence that a low dose of trazodone may improve sleep, however more research is needed.\n\nThere is no solid evidence that folate or vitamin B12 improves outcomes in those with cognitive problems. Statins also have no benefit in dementia. Medications for other health conditions may need to be managed differently for a person who also has a diagnosis of dementia. The MATCH-D criteria can help identify ways that a diagnosis of dementia changes medication management for other health conditions. It is unclear if there is a link between blood pressure medication and dementia. There is a possibility that people may experience an increase in cardiovascular-related events if these medications are withdrawn.\n\nAs people age, they experience more health problems, and most health problems associated with aging carry a substantial burden of pain; therefore, between 25% and 50% of older adults experience persistent pain. Seniors with dementia experience the same prevalence of conditions likely to cause pain as seniors without dementia. Pain is often overlooked in older adults and, when screened for, often poorly assessed, especially among those with dementia since they become incapable of informing others that they're in pain. Beyond the issue of humane care, unrelieved pain has functional implications. Persistent pain can lead to decreased ambulation, depressed mood, sleep disturbances, impaired appetite, and exacerbation of cognitive impairment, and pain-related interference with activity is a factor contributing to falls in the elderly.\n\nAlthough persistent pain in the person with dementia is difficult to communicate, diagnose, and treat, failure to address persistent pain has profound functional, psychosocial, and quality of life implications for this vulnerable population. Health professionals often lack the skills and usually lack the time needed to recognize, accurately assess, and adequately monitor pain in people with dementia. Family members and friends can make a valuable contribution to the care of a person with dementia by learning to recognize and assess their pain. Educational resources (such as the tutorial) and observational assessment tools are available.\n\nPersons with dementia may have difficulty eating. Whenever it is available as an option, the recommended response to eating problems is having a caretaker do assisted feeding for the person. A secondary option for people who cannot swallow effectively is to consider gastrostomy feeding tube placement as a way to give nutrition. However, in bringing person comfort and keeping functional status while lowering risk of aspiration pneumonia and death, assistance with oral feeding is at least as good as tube feeding. Tube-feeding is associated with agitation, increased use of physical and chemical restraints, and worsening pressure ulcers. Tube feedings may also cause fluid overload, diarrhea, abdominal pain, local complications, less human interaction, and may increase the risk of aspiration.\n\nBenefits of this procedure in those with advanced dementia has not been shown. The risks of using tube feeding include agitation, the person pulling out the tube or otherwise being physically or chemically immobilized to prevent them from doing this, or getting pressure ulcers. There is about a 1% fatality rate directly related to the procedure with a 3% major complication rate. The percentage of people at the end of their life with dementia using feeding tubes in the USA has dropped from 12% in 2000 to 6% as of 2014.\n\nAromatherapy and massage have unclear evidence. There have been studies on the efficacy and safety of cannabinoids in relieving behavioral and psychological symptoms of dementia.\n\nOmega-3 fatty acid supplements from plants or fish sources do not appear to benefit or harm people with mild to moderate Alzheimer's disease. It is unclear if taking omega-3 fatty acid supplements can improve other types of dementia.\n\nGiven the progressive and terminal nature of dementia, palliative care can be helpful to patients and their caregivers by helping both people with the disorder and their caregivers understand what to expect, deal with loss of physical and mental abilities, plan out a patient’s wishes and goals including surrogate decision making, and discuss wishes for or against CPR and life support. Because the decline can be rapid, and because most people prefer to allow the person with dementia to make their own decisions, palliative care involvement before the late stages of dementia is recommended. Further research is required to determine the appropriate palliative care interventions and how well they help people with advanced dementia.\n\nPerson-centered care helps maintain the dignity of people with dementia.\n\nThe number of cases of dementia worldwide in 2010 was estimated at 35.6 million. In 2015, 46.8 million people live with dementia, with 58 % living in low and middle income countries. The prevalence of dementia differs in different world regions, ranging from 4.7% in Central Europe to 8.7% in North Africa/Middle East; the prevalence in other regions is estimated to be between 5.6 and 7.6% . The number of people living with dementia is estimated to double every 20 years. In 2013 dementia resulted in about 1.7 million deaths, up from 0.8 million in 1990.Around two thirds of individuals with dementia live in low- and middle-income countries, where the sharpest increases in numbers are predicted. \n\nThe annual incidence of dementia is over 9.9 million worldwide. Almost half of the new cases of dementia occur in Asia, followed by Europe (25%), the Americas (18%) and Africa (8%). The incidence of dementia increases exponentially with increase in age, doubling with every 6.3 year increase in age. Dementia affecting 5% of the population older than 65 and 20–40% of those older than 85.Rates are slightly higher in women than men at ages 65 and greater.\n\nDementia impacts not only the individuals with dementia, but also their carers and the wider society. Among people aged 60 years and over, dementia is ranked the 9th most burdensome conditions according to the 2010 Global Burden of Disease (GBD) estimates.The global costs of dementia is around US$ 818 billion in 2015, a 35.4% increase from US$ 604 billion in 2010.\n\nUntil the end of the 19th century, dementia was a much broader clinical concept. It included mental illness and any type of psychosocial incapacity, including conditions that could be reversed. \"Dementia\" at this time simply referred to anyone who had lost the ability to reason, and was applied equally to psychosis of mental illness, \"organic\" diseases like syphilis that destroy the brain, and to the dementia associated with old age, which was attributed to \"hardening of the arteries\".\n\nDementia has been referred to in medical texts since antiquity. One of the earliest known allusions to dementia is attributed to the 7th-century BC Greek philosopher Pythagoras, who divided the human lifespan into six distinct phases, which were 0–6 (infancy), 7–21 (adolescence), 22–49 (young adulthood), 50–62 (middle age), 63–79 (old age), and 80–death (advanced age). The last two he described as the \"senium\", a period of mental and physical decay, and of the final phase being where \"the scene of mortal existence closes after a great length of time that very fortunately, few of the human species arrive at, where the mind is reduced to the imbecility of the first epoch of infancy\". In 550 BC, the Greek Athenian statesman and poet Solon argued that the terms of a man's will might be invalidated if he exhibited loss of judgement due to advanced age. Chinese medical texts made allusions to the condition as well, and the characters for \"dementia\" translate literally to \"foolish old person\".\n\nAristotle and Plato from Ancient Greece spoke of the mental decay of advanced age, but apparently simply viewed it as an inevitable process that affected all old men, and which nothing could prevent. Plato stated that the elderly were unsuited for any position of responsibility because, \"There is not much acumen of the mind that once carried them in their youth, those characteristics one would call judgement, imagination, power of reasoning, and memory. They see them gradually blunted by deterioration and can hardly fulfill their function.\"\n\nFor comparison, the Roman statesman Cicero held a view much more in line with modern-day medical wisdom that loss of mental function was not inevitable in the elderly and \"affected only those old men who were weak-willed\". He spoke of how those who remained mentally active and eager to learn new things could stave off dementia. However, Cicero's views on aging, although progressive, were largely ignored in a world that would be dominated by Aristotle's medical writings for centuries. Subsequent physicians during the time of Roman Empire such as Galen and Celsus simply repeated the beliefs of Aristotle while adding few new contributions to medical knowledge.\n\nByzantine physicians sometimes wrote of dementia, and it is recorded that at least seven emperors whose lifespans exceeded the age of 70 displayed signs of cognitive decline. In Constantinople, there existed special hospitals to house those diagnosed with dementia or insanity, but these naturally did not apply to the emperors who were above the law and whose health conditions could not be publicly acknowledged.\n\nOtherwise, little is recorded about dementia in Western medical texts for nearly 1700 years. One of the few references to it was the 13th-century friar Roger Bacon, who viewed old age as divine punishment for original sin. Although he repeated existing Aristotelian beliefs that dementia was inevitable after a long enough lifespan, he did make the extremely progressive assertion that the brain was the center of memory and thought rather than the heart.\n\nPoets, playwrights, and other writers however made frequent allusions to the loss of mental function in old age. Shakespeare notably mentions it in some of his plays including \"Hamlet\" and \"King Lear\".\n\nDuring the 19th century, doctors generally came to believe that dementia in the elderly was the result of cerebral atherosclerosis, although opinions fluctuated between the idea that it was due to blockage of the major arteries supplying the brain or small strokes within the vessels of the cerebral cortex. This viewpoint remained conventional medical wisdom through the first half of the 20th century, but by the 1960s was increasingly challenged as the link between neurodegenerative diseases and age-related cognitive decline was established. By the 1970s, the medical community maintained that vascular dementia was rarer than previously thought and Alzheimer's disease caused the vast majority of mental impairments in old age. More recently however, it is believed that dementia is often a mixture of both conditions.\n\nMuch like other diseases associated with aging, dementia was comparatively rare before the 20th century, due to the fact that it is most common in people over 80, and such lifespans were uncommon in preindustrial times. Conversely, syphilitic dementia was widespread in the developed world until largely being eradicated by the use of penicillin after WWII. With significant increases in life expectancy following WWII, the number of people in developed countries over 65 started rapidly climbing. While elderly persons constituted an average of 3–5% of the population prior to 1945, by 2010 it was common in many countries to have 10–14% of people over 65 and in Germany and Japan, this figure exceeded 20%. Public awareness of Alzheimer's Disease was greatly increased in 1994 when former US president Ronald Reagan announced that he had been diagnosed with the condition.\n\nIn the more recent history of dementia, some hospitals in London have found that using color, designs, pictures and lights have helped dementia patients adjust to being at the hospital. These adjustments to the layout of the dementia wings at these hospitals have helped patients by preventing confusion. \n\nDementia in the elderly has previously been called senile dementia or senility, and viewed as a normal and somewhat inevitable aspect of growing old, rather than as being caused by any specific diseases. The terminology, \"senile dementia\" or \"senility\", is no longer recommended. In 1907, a specific organic dementing process of early onset, called Alzheimer's disease, was described. This was associated with particular microscopic changes in the brain, but was seen as a rare disease of middle age because the first person diagnosed with it was a 50-year-old woman.\n\nBy the period of 1913–20, schizophrenia had been well-defined in a way similar to today, and also the term \"dementia praecox\" had been used to suggest the development of senile-type dementia at a younger age. Eventually the two terms fused, so that until 1952 physicians used the terms \"dementia praecox\" (precocious dementia) and \"schizophrenia\" interchangeably. The term \"precocious dementia\" for a mental illness suggested that a type of mental illness like schizophrenia (including paranoia and decreased cognitive capacity) could be expected to arrive normally in all persons with greater age (see paraphrenia). After about 1920, the beginning use of \"dementia\" for what is now understood as schizophrenia and senile dementia helped limit the word's meaning to \"permanent, irreversible mental deterioration\". This began the change to the more recognizable use of the term today.\n\nIn 1976, neurologist Robert Katzmann suggested a link between senile dementia and Alzheimer's disease. Katzmann suggested that much of the senile dementia occurring (by definition) after the age of 65, was pathologically identical with Alzheimer's disease occurring before age 65 and therefore should not be treated differently. He noted that \"senile dementia\" not being considered a disease, but rather part of aging, was keeping millions of aged patients experiencing what otherwise was identical with Alzheimer's disease from being diagnosed as having a disease process, rather than simply considered as aging normally. Katzmann thus suggested that Alzheimer's disease, if taken to occur over age 65, is actually common, not rare, and was the fourth- or 5th-leading cause of death, even though rarely reported on death certificates in 1976.\n\nThis suggestion opened the view that dementia is never normal, and must always be the result of a particular disease process, and is not part of the normal healthy aging process, \"per se\". The ensuing debate led for a time to the proposed disease diagnosis of \"senile dementia of the Alzheimer's type\" (SDAT) in persons over the age of 65, with \"Alzheimer's disease\" diagnosed in persons younger than 65 who had the same pathology. Eventually, however, it was agreed that the age limit was artificial, and that \"Alzheimer's disease\" was the appropriate term for persons with the particular brain pathology seen in this disorder, regardless of the age of the person with the diagnosis. A helpful finding was that although the incidence of Alzheimer's disease increased with age (from 5–10% of 75-year-olds to as many as 40–50% of 90-year-olds), there was no age at which all persons developed it, so it was not an inevitable consequence of aging, no matter how great an age a person attained. Evidence of this is shown by numerous documented supercentenarians (people living to 110 or more) that experienced no serious cognitive impairment. There is some evidence that dementia is most likely to develop between the ages of 80 and 84 and individuals who pass that point without being affected have a lower chance of developing it. Women account for a larger percentage of dementia cases than men, although this can be attributed to their longer overall lifespan and greater odds of attaining an age where the condition is likely to occur.\n\nAlso, after 1952, mental illnesses like schizophrenia were removed from the category of \"organic brain syndromes\", and thus (by definition) removed from possible causes of \"dementing illnesses\" (dementias). At the same, however, the traditional cause of senile dementia – \"hardening of the arteries\" – now returned as a set of dementias of vascular cause (small strokes). These were now termed \"multi-infarct dementias\" or \"vascular dementias\".\n\nIn the 21st century, a number of other types of dementia have been differentiated from Alzheimer's disease and vascular dementias (these two being the most common types). This differentiation is on the basis of pathological examination of brain tissues, by symptomatology, and by different patterns of brain metabolic activity in nuclear medical imaging tests such as SPECT and PETscans of the brain. The various forms of dementia have differing prognoses (expected outcome of illness), and also differing sets of epidemiologic risk factors. The causal etiology of many of them, including Alzheimer's disease, remains unclear, although many theories exist such as accumulation of protein plaques as part of normal aging, inflammation (either from bacterial pathogens or exposure to toxic chemicals), inadequate blood sugar, and traumatic brain injury.\n\nThe societal cost of dementia is high, especially for family caregivers.\n\nMany countries consider the care of people living with dementia a national priority and invest in resources and education to better inform health and social service workers, unpaid caregivers, relatives, and members of the wider community. Several countries have national plans or strategies. In these national plans, there is recognition that people can live well with dementia for a number of years, as long as there is the right support and timely access to a diagnosis. The former British Prime Minister David Cameron has described dementia as being a \"national crisis\", affecting 800,000 people in the United Kingdom.\n\nIn the United Kingdom, as with all mental disorders, where people with dementia could potentially be a danger to themselves or others, they can be detained under the Mental Health Act 1983 for the purposes of assessment, care and treatment. This is a last resort, and usually avoided if the person has family or friends who can ensure care.\n\nSome hospitals in Britain are working to provide enriched and friendlier care. To make the hospital wards calmer and less overwhelming to the residents, the staff is replacing the usual nurses' station with a collection of smaller desks, similar to a receptionist’s. The incorporation of bright lighting helps increase positive mood and allow for the residents to see more easily.\n\nDriving with dementia could lead to severe injury or even death to self and others. Doctors should advise appropriate testing on when to quit driving. The United Kingdom DVLA (Driver & Vehicle Licensing Agency) states that people with dementia who specifically have poor short term memory, disorientation, or lack of insight or judgment are not fit to drive, and in these instances the DVLA must be informed so that the driving licence can be revoked. They do, however, acknowledge low-severity cases and those with an early diagnosis, and those drivers may be permitted to drive pending medical reports.\nMany support networks are available to people with dementia and their families and caregivers. Several charitable organisations aim to raise awareness and campaign for the rights of people living with dementia. There is also support and guidance on assessing testamentary capacity in people who have dementia.\n\nIn 2015, Atlantic Philanthropies announced a $177 million gift aimed at understanding and reducing dementia. The recipient was Global Brain Health Institute, a program co-led by the University of California, San Francisco and Trinity College Dublin. This donation is the largest non-capital grant Atlantic has ever made, and the biggest philanthropic donation in Irish history.\n\nThere is limited evidence that links poor oral health to cognitive decline. However, failure to perform tooth brushing and gingival inflammation can be used as dementia risk predictors.\n\nThe link between Alzheimer's and gum disease is oral bacteria. In the oral cavity, a large number of bacterial species can be found including \"P. gingivalis\", \"F. nucleatum\", \"P. intermedia\", and \"T. forsythia\". Six oral trepomena spirochetes have also been examined in the brains of Alzheimer's patients. Spirochetes are neurotropic in nature, meaning they act to destroy nerve tissue and create inflammation. Inflammatory pathogens are an indicator of Alzheimer's disease and bacteria related to gum disease have been found in the brains of Alzheimer's disease individuals. The bacteria invade nerve tissue in the brain, increasing the permeability of the blood-brain barrier and promoting the onset of Alzheimer's among the elderly population. It has also been found that individuals with a plethora of tooth plaque have a risk of cognitive decline. Poor oral hygiene can also have an adverse effect on speech and nutrition causing general and cognitive health decline.\n\nHerpes simplex virus (HSV) has been found in over 70% of the 50 and older population. HSV persists in the peripheral nervous system and can be triggered by stress, illness or fatigue. High proportions of viral-associated proteins in amyloid-containing plaques or neurofibrillary tangles (NFTs) highly confirm the involvement of HSV-1 in Alzheimer's disease pathology. NFTs are known as the primary marker of Alzheimer's disease. HSV-1 produces the main components of NFTs.\n\n"}
{"id": "8449", "url": "https://en.wikipedia.org/wiki?curid=8449", "title": "Developmental biology", "text": "Developmental biology\n\nDevelopmental biology is the study of the process by which animals and plants grow and develop. Developmental biology also encompasses the biology of regeneration, asexual reproduction, metamorphosis, and the growth and differentiation of stem cells in the adult organism.\n\nIn the late 20th century, the discipline largely transformed into evolutionary developmental biology.\n\nThe main processes involved in the embryonic development of animals are: regional specification, morphogenesis, cell differentiation, growth, and the overall control of timing explored in evolutionary developmental biology:\n\n\nThe development of plants involves similar processes to that of animals. However plant cells are mostly immotile so morphogenesis is achieved by differential growth, without cell movements. Also, the inductive signals and the genes involved are different from those that control animal development.\n\nCell differentiation is the process whereby different functional cell types arise in development. For example, neurons, muscle fibers and hepatocytes (liver cells) are well known types of differentiated cell. Differentiated cells usually produce large amounts of a few proteins that are required for their specific function and this gives them the characteristic appearance that enables them to be recognized under the light microscope. The genes encoding these proteins are highly active. Typically their chromatin structure is very open, allowing access for the transcription enzymes, and specific transcription factors bind to regulatory sequences in the DNA in order to activate gene expression. For example, NeuroD is a key transcription factor for neuronal differentiation, myogenin for muscle differentiation, and HNF4 for hepatocyte differentiation.\nCell differentiation is usually the final stage of development, preceded by several states of commitment which are not visibly differentiated. A single tissue, formed from a single type of progenitor cell or stem cell, often consists of several differentiated cell types. Control of their formation involves a process of lateral inhibition, based on the properties of the Notch signaling pathway. For example, in the neural plate of the embryo this system operates to generate a population of neuronal precursor cells in which NeuroD is highly expressed.\n\nRegeneration indicates the ability to regrow a missing part. This is very prevalent amongst plants, which show continuous growth, and also among colonial animals such as hydroids and ascidians. But most interest by developmental biologists has been shown in the regeneration of parts in free living animals. In particular four models have been the subject of much investigation. Two of these have the ability to regenerate whole bodies: \"Hydra\", which can regenerate any part of the polyp from a small fragment, and planarian worms, which can usually regenerate both heads and tails. Both of these examples have continuous cell turnover fed by stem cells and, at least in planaria, at least some of the stem cells have been shown to be pluripotent. The other two models show only distal regeneration of appendages. These are the insect appendages, usually the legs of hemimetabolous insects such as the cricket, and the limbs of urodele amphibians. Considerable information is now available about amphibian limb regeneration and it is known that each cell type regenerates itself, except for connective tissues where there is considerable interconversion between cartilage, dermis and tendons. In terms of the pattern of structures, this is controlled by a re-activation of signals active in the embryo.\nThere is still debate about the old question of whether regeneration is a \"pristine\" or an \"adaptive\" property. If the former is the case, with improved knowledge, we might expect to be able to improve regenerative ability in humans. If the latter, then each instance of regeneration is presumed to have arisen by natural selection in circumstances particular to the species, so no general rules would be expected.\n\nThe sperm and egg fuse in the process of fertilization to form a fertilized egg, or zygote. This undergoes a period of divisions to form a ball or sheet of similar cells called a blastula or blastoderm. These cell divisions are usually rapid with no growth so the daughter cells are half the size of the mother cell and the whole embryo stays about the same size. They are called cleavage divisions. Morphogenetic movements convert the cell mass into a three layered structure consisting of multicellular sheets called ectoderm, mesoderm and endoderm, which are known as germ layers. This is the process of gastrulation. During cleavage and gastrulation the first regional specification events occur. In addition to the formation of the three germ layers themselves, these often generate extraembryonic structures, such as the mammalian placenta, needed for support and nutrition of the embryo, and also establish differences of commitment along the anteroposterior axis (head, trunk and tail).\n\nRegional specification is initiated by the presence of cytoplasmic determinants in one part of the zygote. The cells that contain the determinant become a signaling center and emit an inducing factor. Because the inducing factor is produced in one place, diffuses away, and decays, it forms a concentration gradient, high near the source cells and low further away. The remaining cells of the embryo, which do not contain the determinant, are competent to respond to different concentrations by upregulating specific developmental control genes. This results in a series of zones becoming set up, arranged at progressively greater distance from the signaling center. In each zone a different combination of developmental control genes is upregulated. These genes encode transcription factors which upregulate new combinations of gene activity in each region. Among other functions, these transcription factors control expression of genes conferring specific adhesive and motility properties on the cells in which they are active. Because of these different morphogenetic properties, the cells of each germ layer move to form sheets such that the ectoderm ends up on the outside, mesoderm in the middle, and endoderm on the inside. Morphogenetic movements not only change the shape and structure of the embryo, but by bringing cell sheets into new spatial relationships they also make possible new phases of signaling and response between them.\n\nGrowth in embryos is mostly autonomous. For each territory of cells the growth rate is controlled by the combination of genes that are active. Free-living embryos do not grow in mass as they have no external food supply. But embryos fed by a placenta or extraembryonic yolk supply can grow very fast, and changes to relative growth rate between parts in these organisms help to produce the final overall anatomy.\n\nThe whole process needs to be coordinated in time and how this is controlled is not understood. There may be a master clock able to communicate with all parts of the embryo that controls the course of events, or timing may depend simply on local causal sequences of events.\n\nDevelopmental processes are very evident during the process of metamorphosis. This occurs in various types of animal. Well-known are the examples of the frog, which usually hatches as a tadpole and metamorphoses to an adult frog, and certain insects which hatch as a larva and then become remodeled to the adult form during a pupal stage.\n\nAll the developmental processes listed above occur during metamorphosis. Examples that have been especially well studied include tail loss and other changes in the tadpole of the frog \"Xenopus\", and the biology of the imaginal discs, which generate the adult body parts of the fly \"Drosophila melanogaster\".\n\nPlant development is the process by which structures originate and mature as a plant grows. It is studied in plant anatomy and plant physiology as well as plant morphology.\n\nPlants constantly produce new tissues and structures throughout their life from meristems located at the tips of organs, or between mature tissues. Thus, a living plant always has embryonic tissues. By contrast, an animal embryo will very early produce all of the body parts that it will ever have in its life. When the animal is born (or hatches from its egg), it has all its body parts and from that point will only grow larger and more mature.\n\nThe properties of organization seen in a plant are emergent properties which are more than the sum of the individual parts. \"The assembly of these tissues and functions into an integrated multicellular organism yields not only the characteristics of the separate parts and processes but also quite a new set of characteristics which would not have been predictable on the basis of examination of the separate parts.\"\n\nA vascular plant begins from a single celled zygote, formed by fertilisation of an egg cell by a sperm cell. From that point, it begins to divide to form a plant embryo through the process of embryogenesis. As this happens, the resulting cells will organize so that one end becomes the first root, while the other end forms the tip of the shoot. In seed plants, the embryo will develop one or more \"seed leaves\" (cotyledons). By the end of embryogenesis, the young plant will have all the parts necessary to begin in its life.\n\nOnce the embryo germinates from its seed or parent plant, it begins to produce additional organs (leaves, stems, and roots) through the process of organogenesis. New roots grow from root meristems located at the tip of the root, and new stems and leaves grow from shoot meristems located at the tip of the shoot. Branching occurs when small clumps of cells left behind by the meristem, and which have not yet undergone cellular differentiation to form a specialized tissue, begin to grow as the tip of a new root or shoot. Growth from any such meristem at the tip of a root or shoot is termed primary growth and results in the lengthening of that root or shoot. Secondary growth results in widening of a root or shoot from divisions of cells in a cambium.\n\nIn addition to growth by cell division, a plant may grow through cell elongation. This occurs when individual cells or groups of cells grow longer. Not all plant cells will grow to the same length. When cells on one side of a stem grow longer and faster than cells on the other side, the stem will bend to the side of the slower growing cells as a result. This directional growth can occur via a plant's response to a particular stimulus, such as light (phototropism), gravity (gravitropism), water, (hydrotropism), and physical contact (thigmotropism).\n\nPlant growth and development are mediated by specific plant hormones and plant growth regulators (PGRs) (Ross et al. 1983). Endogenous hormone levels are influenced by plant age, cold hardiness, dormancy, and other metabolic conditions; photoperiod, drought, temperature, and other external environmental conditions; and exogenous sources of PGRs, e.g., externally applied and of rhizospheric origin.\n\nPlants exhibit natural variation in their form and structure. While all organisms vary from individual to individual, plants exhibit an additional type of variation. Within a single individual, parts are repeated which may differ in form and structure from other similar parts. This variation is most easily seen in the leaves of a plant, though other organs such as stems and flowers may show similar variation. There are three primary causes of this variation: positional effects, environmental effects, and juvenility.\n\nTranscription factors and transcriptional regulatory networks play key roles in plant morphogenesis and their evolution. During plant landing, many novel transcription factor families emerged and are preferentially wired into the networks of multicellular development, reproduction, and organ development, contributing to more complex morphogenesis of land plants.\n\nMuch of developmental biology research in recent decades has focused on the use of a small number of model organisms. It has turned out that there is much conservation of developmental mechanisms across the animal kingdom. In early development different vertebrate species all use essentially the same inductive signals and the same genes encoding regional identity. Even invertebrates use a similar repertoire of signals and genes although the body parts formed are significantly different. Model organisms each have some particular experimental advantages which have enabled them to become popular among researchers. In one sense they are \"models\" for the whole animal kingdom, and in another sense they are \"models\" for human development, which is difficult to study directly for both ethical and practical reasons. Model organisms have been most useful for elucidating the broad nature of developmental mechanisms. The more detail is sought, the more they differ from each other and from humans.\n\nPlants:\n\nVertebrates:\n\nInvertebrates:\n\n\nAlso popular for some purposes have been sea urchins and ascidians. For studies of regeneration urodele amphibians such as the axolotl \"Ambystoma mexicanum\" are used, and also planarian worms such as \"Schmidtea mediterranea\". Organoids have also been demonstrated as an efficient model for development. Plant development has focused on the thale cress \"Arabidopsis thaliana\" as a model organism.\n\n\n"}
{"id": "1809014", "url": "https://en.wikipedia.org/wiki?curid=1809014", "title": "Flo Hyman", "text": "Flo Hyman\n\nFlora Jean \"Flo\" Hyman (July 31, 1954 – January 24, 1986) was an American athlete who played volleyball. She was an Olympic silver medalist and played professional volleyball in Japan.\n\nHyman was the second of eight children. As a child, Hyman was self-conscious about her size, her rapid growth and the fact that she towered over her peers. In 1983 she recalled \"When they were three feet tall, I was four foot tall. When they reached four foot tall, I reached five\". Her nickname at school was \"Jolly green giant\", but her family and friends persuaded her to be proud of her height and to use it to her advantage. Hyman's parents were tall. Her father was 6'1\" (1.85 m) tall and her mother 5'11\" (1.80 m), but Flo outgrew both of them. Her sister recalled Flo being 5'9\" tall in the third grade.. Flo's final adult height was just over 6'5\" (1.96 m). Her shoe size was a size 12 USA men's or an EU 46. In January 1979, in an interview, Hyman said that she found the stares and questions about her height that she got from people irritating but she had learned to live with it.\n\nWhen she was 12, and standing 6ft 2in tall, she began playing two-on-two tournaments on the beach, usually with her sister Suzanne as partner. In 1970, at the age of 16, Hyman started playing volleyball professionally. By the time Flo was a senior in high school, she had developed a lethal spike.\n\nHyman graduated from Morningside High School in Inglewood, California and then attended El Camino College for one year before transferring to the University of Houston as that school's first female scholarship athlete. She spent three years there and led the Houston Cougars to two top-five national finishes, but did not complete her final year, instead focusing her attention on her volleyball career. Hyman said she would graduate once her volleyball career was over and that \"You can go to school when you're 60. You're only young once, and you can only do this once\".\n\n\"I had to learn to be honest with myself. I had to recognize my pain threshold. When I hit the floor, I have to realize it's not as if I broke a bone. Pushing yourself over the barrier is a habit. I know I can do it and try something else crazy. If you want to win the war, you've got to pay the price.\"\nHyman left Houston to play for the national team, based in Colorado. When Hyman joined, the squad was sorely in need of leadership. Operating without a coach, it had a host of talented players with no one at the helm to guide them.\n\nIn 1975 the U.S. team floundered through qualifying rounds for the 1976 Olympic games and failed to make it. In 1977 the team finished fifth at the World Championships. Hyman and her teammates looked forward to qualifying for and playing in the 1980 Olympics, but their dreams were curtailed when the United States boycotted the Moscow games.\n\nHyman played in the 1981 World Cup and the 1982 World Championship, when the US won the bronze medal. A speciality of Hyman was the \"Flying Clutchman\", a fast, hard-impacting volleyball spike that travels at 110 mph (180 km/h). It was perfected under Dr. Gideon Ariel, a former 1960 and 1964 Israeli Olympic shot putter in Coto de Caza, California. At the 1984 Olympics, Hyman, by now both the tallest and oldest member of the team, led the US to the silver medal, beaten by China in the final. The United States had defeated them earlier in the tournament.\n\nAfter the Olympics, Hyman moved to Japan to play volleyball professionally, joining the Daiei women's squad in the Japan Volleyball League. She was so popular in Japan that she began a modeling and acting career there and was constantly in demand. She intended to return to the United States permanently in the Summer of 1986, but never got the chance to do so. On January 24, 1986, Hyman collapsed while sitting on the sidelines after being substituted out in a game against Hitachi in Matsue City. She told her team to keep fighting, then moments later slid to the floor. She was pronounced dead at 9:36 that evening.\n\nAt first, the cause of Hyman's death was stated to be a heart attack. Not fully accepting this finding, her family requested that an autopsy be performed in Culver City, California. The autopsy, which was held on January 30, dismissed the possibility of a heart attack. It found that Hyman had a very healthy heart, and instead it was determined that she had suffered from undiagnosed Marfan syndrome, which had caused a fatal aortic dissection. Apart from her height, nearsightedness, very long arms and large hands, she showed few other physical symptoms. The pathologist who performed the autopsy, Dr. Victor Rosen, said that Hyman physically had been in superb condition except for a single fatal flaw—a dime-sized weak spot in her aorta. That small spot, less than an inch above her heart, had been there since her birth, and the artery had burst at that point as she sat on the sideline in Matsue. There was a three-week-old blood clot around the tear, indicating that an earlier rip in the same spot had already begun to heal when the fatal second rupture occurred.\n\nDoctors later discovered Hyman's brother had Marfan syndrome as well, and he underwent an open heart surgery afterwards. Experts believed Hyman was lucky to have survived as long as she did, playing a physically demanding sport such as volleyball.\n\nShe was buried at Inglewood Park Cemetery, Inglewood, California, on January 31, 1986. Over 500 people attended the funeral service.\n\n\n\nstory written by : Ta'ziyah moore // age 13"}
{"id": "13700452", "url": "https://en.wikipedia.org/wiki?curid=13700452", "title": "Gaius Aculeo", "text": "Gaius Aculeo\n\nGaius Aculeo was a Roman knight who married the sister of Helvia, the mother of Cicero. He was unsurpassed in his day in his knowledge of the Roman law, and possessed great acuteness of mind, but was not distinguished for other attainments. He was a friend of Lucius Licinius Crassus, renowned as the greatest Roman orator of his day, and was defended by him upon one occasion. The son of Aculeo was Gaius Visellius Varro; from which it would appear that Aculeo was only a surname given to the father from his acuteness, and that his full name was Gaius Visellius Varro Aculeo.\n\n"}
{"id": "44631723", "url": "https://en.wikipedia.org/wiki?curid=44631723", "title": "Genetics of posttraumatic stress disorder", "text": "Genetics of posttraumatic stress disorder\n\nGenetics play some role in the development of PTSD. Approximately 30% of the variance in PTSD is caused from genetics alone. For twin pairs exposed to combat in Vietnam, having a monozygotic (identical) twin with PTSD was associated with an increased risk of the co-twin's having PTSD compared to twins that were dizygotic (non-identical twins). There is also evidence that those with a genetically smaller hippocampus are more likely to develop PTSD following a traumatic event. Research has also found that PTSD shares many genetic influences common to other psychiatric disorders. Panic and generalized anxiety disorders and PTSD share 60% of the same genetic variance. Alcohol, nicotine, and drug dependence share greater than 40% genetic similarities.\n\nGamma-aminobutyric acid (GABA) is the major inhibitory neurotransmitter in the brain. A recent study reported significant interactions between three polymorphisms in the GABA alpha-2 receptor gene and the severity of childhood trauma in predicting PTSD in adults. A study found those with a specific genotype for G-protein signaling 2 (RGS2), a protein that decreases G protein-coupled receptor signaling, and high environmental stress exposure as adults and a diagnosis of lifetime PTSD. This was particularly prevalent in adults with prior trauma exposure and low social support.\n\nRecently, it has been found that several single-nucleotide polymorphisms (SNPs) in FK506 binding protein 5 (FKBP5) interact with childhood trauma to predict severity of adult PTSD. These findings suggest that individuals with these SNPs who are abused as children are more susceptible to PTSD as adults.\n\nThis is particularly interesting given that FKBP5 SNPs have previously been associated with peritraumatic dissociation in medically injured children (that is, dissociation at the time of the birth trauma), which has itself been shown to be predictive of PTSD. Furthermore, FKBP5 may be less expressed in those with current PTSD. Another recent study found a single SNP in a putative estrogen response element on ADCYAP1R1 (encodes pituitary adenylate cyclase-activating polypeptide type I receptor or PAC1) to predict PTSD diagnosis and symptoms in females. Incidentally, this SNP is also associated with fear discrimination. The study suggests that perturbations in the PACAP-PAC1 pathway are involved in abnormal stress responses underlying PTSD.\n\nPTSD is a psychiatric disorder that requires an environmental event that individuals may have varied responses to so gene-environment studies tend to be the most indicative of their effect on the probability of PTSD then studies of the main effect of the gene. Recent studies have demonstrated the interaction between PFBP5 and childhood environment to predict the severity of PTSD. Polymorphisms in FKBP5 have been associated with peritraumatic dissociation in mentally ill children. A study of highly traumatized African-American subjects from inner city primary-care clinics indicated 4 polymorphisms of the FKBP5 gene, each of these functional. The interaction between the polymorphisms and the severity of childhood abuse predicts the severity of the adult PTSD symptoms. A more recent study of the African-American population indicated that the TT genotype of the FKBP5 gene is associated with the highest risk of PTSD among those having experienced childhood adversity, however those with this genotype that experienced no childhood adversity had the lowest risk of PTSD. In addition alcohol dependence interacts with the FKBP5 polymorphisms and childhood adversity to increase the risk of PTSD in these populations. Emergency room expression of the FKPB5 mRNA following trauma was shown to indicate a later development of PTSD.\n\nCatechol-O-methyl transferase (COMT) is an enzyme that catalyzes the extraneuronal breakdown of catecholamines. The gene that codes for COMT has a functional polymorphism in which a valine has been replaced with a methionine at codon 158. This polymorphism has lower enzyme activity and has been tied to slower breakdown of the catecholamines. A study, of Rwandan Genocide survivors, indicated that carriers of the Val allel demonstrated the expected response relationship between the higher number of lifetime traumatic events and a lifetime diagnosis of PTSD. However, those with homozygotes for the Met/Met genotype demonstrated a high risk of lifetime PTSD independent of the number of traumatic experiences. Those with Met/Met genotype also demonstrated a reduced extinction of conditioned fear responses with may account for the high risk for PTSD experienced by this genotype.\n\nMany genes impact the limbic-frontal neurocircuitry as a result of its complexity. The main effect of the D2A1 allele of the dopamine receptor D2 (DRD2) has a strong association with the diagnosis of PTSD. The D2A1 allele has also shown a significant association to PTSD in those having engaged in harmful drinking. In addition a polymorphism in the dopamine transporter SLC6A3 gene has a significant association with chronic PTSD. A polymorphism of the serotonin receptor 2A gene has been associated with PTSD in Korean women. The short allele of the promoter region of the serotonin transporter (5-HTTLPR) has been shown to be less efficient then the long allele and is associated with the amygdala response for extinction of fear conditioning. However, the short allele is associated with a decreased risk of PTSD in a low risk environment but a high risk of PTSD in a high risk environment. The s/s genotype demonstrated a high risk for development of PTSD even in response to a small number of traumatic events, but those with the l allele demonstrate increasing rates of PTSD with increasing traumatic experiences.\n\nGenome-wide association study (GWAS) offer an opportunity to identify novel risk variants for PTSD that will in turn inform our understanding of the etiology of the disorder. Early results indicate the feasibility and potential power of GWAS to identify biomarkers for anxiety-related behaviors that suggest a future of PTSD. These studies will lead to the discovery of novel loci for the susceptibility and symptomatology of anxiety disorders including PTSD.\n\nGene and environment studies alone fail to explain the importance the developmental stressor timing exposure to the phenotypic changes associated with PTSD. Epigenetic modification is the environmentally induced change in DNA that alters the function rather than the structure of the gene. The biological mechanism of epigenetic modification typically involves the methylation of cytosine within a gene that produces decreased transcription of that segment of DNA. The neuroendocrine alteration seen in animal models parallel those of PTSD in which low basal cortisol and enhanced suppression of cortisol in response to synthetic glucocorticoid becomes hereditary. Lower levels of glucocorticoid receptor (GR) mRNA have been demonstrated in the hippocampus of suicide victims with histories of childhood abuse. It has not been possible to monitor the state of methylation over time, however the interpretation is early developmental methylation changes are long-lasting and enduring. It is hypothesized that epigenetic-mediated changes in the HPA axis could be associated with an increased vulnerability to PTSD following traumatic events. These findings support the mechanism in which early life trauma strongly validates as a risk factor for PTSD development in adulthood by recalibrating the set point and stress-responsivity of the HPA axis. Studies have reported an increased risk for PTSD and low cortisol levels in the offspring of female holocaust survivors with PTSD. Epigenetic mechanisms may also be relevant to the intrauterine environment. Mothers with PTSD produced infants with lower salivary cortisol levels only if the traumatic exposure occurred during the third trimester of gestation. These changes occur via transmission of hormonal responses to the fetus leading to a reprogramming of the glucocorticoid responsivity in the offspring.\n\nEvolutionary psychology views different types of fears and reactions caused by fears as adaptations that may have been useful in the ancestral environment in order to avoid or cope with various threats. In general, mammals display several defensive behaviors roughly dependent on how close the threat is: avoidance, vigilant immobility, withdrawal, aggressive defense, appeasement, and finally complete frozen immobility (the last possibly to confuse a predator's attack reflex or to simulate a dead and contaminated body). PTSD may correspond to and be caused by overactivation of such fear circuits. Thus, PTSD avoidance behaviors may correspond to mammal avoidance of and withdrawal from threats. Heightened memory of past threats may increase avoidance of similar situations in the future as well as be a prerequisite for analyzing the past threat and develop better defensive behaviors if the threat should recur. PTSD hyperarousal may correspond to vigilant immobility and aggressive defense. Complex posttraumatic stress disorder (and phenomena such as the Stockholm syndrome) may in part correspond to the appeasement stage and possibly the frozen immobility stage.\n\nThere may be evolutionary explanations for differences in resilience to traumatic events. Thus, PTSD is rare following traumatic fire that may be explained by events such as forest fires' long being part of the evolutionary history of mammals. On the other hand, PTSD is much more common following modern warfare, which may be explained by modern warfare's being a new development and very unlike the quick inter-group raids that are argued to have characterized the paleolithic.\n"}
{"id": "51204055", "url": "https://en.wikipedia.org/wiki?curid=51204055", "title": "Ginger Fitzgerald", "text": "Ginger Fitzgerald\n\nGinger Fitzgerald is a fictional character in the \"Ginger Snaps\" trilogy. She was portrayed by Katharine Isabelle and serves as the antihero of the films.\n\nIn \"Ginger Snaps\", Ginger and Brigitte Fitzgerald are teenage sisters who are both fascinated with death and, as children, formed a pact to die together. One night, while preparing to steal the dog of bully Trina Sinclair, Ginger starts her first period, which causes the girls to be attacked by a werewolf. Ginger is wounded, but is rescued by Brigitte. The creature is run over by a van belonging to Sam MacDonald, a local drug dealer. Ginger decides not to go the hospital as her wounds heal quickly. She slowly begins to change causing Brigitte to become worried and desperate to find a cure. After several failed cure attempts, Brigitte discovers that a dose of monkshood helps control the infection. Before Brigitte can inject Ginger with the plant, she completely transforms forcing Brigitte to kill her.\n\nIn \"\", while only appearing as a minor character, Ginger is seen in several of Brigitte's hallucinations telling her to give in to the transformation.\n\nIn \"\", Ginger returns as a lead character. Ginger and Brigitte are lost with their horse in the Canadian wilderness when they discover an \"abandoned camp\". An Indian woman gives them each a pendant. Brigitte's foot is caught in a trap. Ginger seeks help, but a hunter frees Brigitte before she returns. They are led to Fort Bailey. A group of werewolfs attack the camp and Ginger is bitten on the shoulder. After being held prisoner and fighting back, Ginger opens the gates and lets the werewolves enter. Ginger and Brigitte are the only ones left alive. Brigitte cuts her hand and presses it against a cut on Gingers hand mixing their blood and infecting Brigitte.\n\nCasting took place in Los Angeles, New York, Toronto, Montreal, and Vancouver. Isabelle auditioned on the same day as Perkins at their agency in Vancouver, reading to one another off-camera. Screenwriter Karen Walton said that they were exactly as she had pictured the characters when their taped auditions had arrived.\n\n\"What Culture\" said:\n\nJessica Roakes of \"The Toast\" also mentions the metaphorical nature of the character saying \"Ginger’s body has betrayed her by menstruating. This is a key tenant of the body-horror genre — the monstrous comes not just from the outside, but from within the human body, from infection or perversion or unwanted biological functions. In Ginger’s case, it is her metamorphosis from girl to woman that renders her monstrous.\"\n"}
{"id": "4435807", "url": "https://en.wikipedia.org/wiki?curid=4435807", "title": "Grauballe Man", "text": "Grauballe Man\n\nThe Grauballe Man is a bog body that was uncovered in 1952 from a peat bog near the village of Grauballe in Jutland, Denmark. The body is that of a man dating from the late 3rd century BC, during the early Germanic Iron Age. Based on the evidence of his wounds, he was most likely killed by having his throat slit. His corpse was then deposited in the bog, where his body was naturally preserved for over two millennia. He was not the only bog body to be found in the peat bogs of Jutland: with other notable examples Tollund Man and the Elling Woman, Grauballe Man represents an established tradition at the time; it is commonly thought that these killings, including that of Grauballe Man, were examples of human sacrifice, possibly an important rite in Iron Age Germanic paganism.\n\nGrauballe Man has been described as \"one of the most spectacular discoveries from Denmark's prehistory\" because it is one of the most exceptionally preserved bog bodies in the world. Upon excavation in 1952, it was moved to the Prehistoric Museum in Aarhus, where it underwent research and conservation. In 1955 the body went on display at the Moesgaard Museum near Aarhus, where it can still be seen today. Due to the preservation of the man's feet and hands, his fingerprints were successfully taken.\n\nGrauballe Man was initially dated to the late 3rd century BC by analysing the stratigraphic layer of peat that his body was found in. This date was subsequently confirmed by radiocarbon dating his liver, the results of which were published in 1955.\n\nInformation about the Grauballe Man's life have been ascertained from his remains. His hands were smooth and did not show evidence of hard work, indicating that Grauballe Man was not employed in hard labour such as farming. Study of his teeth and jaws indicated that he had suffered from \"periods of starvation or a poor state of health during his early childhood.\" The man's skeleton showed signs of significant calcium deficiency, and his spine also suffered the early stages of \"spondylosis deformans\", a generalized disease of aging that is secondary to the degeneration of intervertebral disks. Due to the shrinkage that the corpse suffered in the bog, the man's actual height is not known. It is known that he had dark hair, although this too was altered in the bog, and now appears reddish in colour.\n\nThe corpse was not found with any artifacts or any evidence of clothing, indicating that when he died he was entirely naked, or his clothing had deteriorated, something that had also happened with the Tollund Man. The actual manner of his death was by having his throat cut ear to ear, severing his trachea and oesophagus. Such a wound could not have been self-inflicted, indicating that this was not suicide. A damaged area to the skull that was initially thought to have been inflicted by a blow to the head, has since been determined by a CT scan to have been fractured by pressure from the bog long after his death. He was around thirty years old when he died.\n\nThe Grauballe Man's body was first discovered buried in the bog on 26 April 1952 by a team of peat diggers. One of the workmen, Tage Busk Sørensen, stuck his spade into something that he knew was not peat; upon revealing more they discovered the head protruding from the ground, and the local postman, who was passing, alerted the local doctor as well as an amateur archaeologist named Ulrik Balslev. With the body still in the peat, various locals came to visit it over the next day, one of whom accidentally stepped on its head. The following morning, Professor Peter Glob from the Prehistory Museum at Aarhus came to visit the body, and arranged for it to be removed to the museum, still encased in a block of surrounding peat.\n\nGlob and his team decided that they should not only research the body but that they should also attempt to preserve it so that it could be exhibited to the public. This concept was new at the time as most of the bog bodies previously discovered had been re-buried, sometimes in consecrated ground, with the Tollund Man which had been discovered two years earlier having only its head preserved. Despite the warnings of some scientists who believed that the corpse should immediately undergo preservation, it was exhibited straight away in order to capitalise on public interest. Indeed, the scientists' fears were proved right, as mould started to appear on certain areas of the body as a result of it having to be kept permanently moist.\n\nThe body then underwent research, including a post-mortem, and then preservation, which was organised by conservator C. Lange-Kornbak, who had to decide on the best way to do this. (No entire bog body had ever been preserved before.) He examined various methods for doing so, before deciding on a programme of tanning the body to turn it into leather and then stuffing it with oak bark. In 1955, the body went on display at the Moesgaard Museum near Aarhus, only to be removed for a time in 2001-2002 when it underwent more modern scientific study, including radiological study, CT scanning, 3D visualisation, stereolithography and analyses of the gut contents.\n\nThe Grauballe Man is the subject of a poem by Seamus Heaney, as is the Tollund Man.\n\n"}
{"id": "20744016", "url": "https://en.wikipedia.org/wiki?curid=20744016", "title": "Health management system", "text": "Health management system\n\nThe health management system (HMS) is an evolutionary medicine regulative process proposed by Nicholas Humphrey in which actuarial assessment of fitness and economic-type cost–benefit analysis determines the body’s regulation of its physiology and health. This incorporation of cost–benefit calculations into body regulation provides a science grounded approach to mind–body phenomena such as placebos that are otherwise not explainable by low level, noneconomic, and purely feedback based homeostatic or allostatic theories.\n\n\nPlacebos are explained as the result of false information about the availability of external treatment and support that mislead the health management system into not deploying evolved self-treatments. This results in the placebo suppression of medical symptoms.\n\nSince Hippocrates, it has been recognized that the body has self-healing powers (vis medicatrix naturae). Modern evolutionary medicine identifies them with physiologically based self-treatments that provide the body with prophylactic, healing, or restorative capabilities against injuries, infections and physiological disruption. Examples include:\n\n\nThese evolved self-treatments deployed by the body are experienced by humans as unpleasant and unwanted illness symptoms.\n\nSuch self-treatments according to evolutionary medicine are deployed to increase an individual’s biological fitness.\n\nTwo factors affect their deployment.\n\nFirst, it is usually advantageous to deploy them on a precautionary basis. As a result, it will often turn out that they have been deployed apparently unnecessarily, though this has in fact been advantageous since in probabilistic terms they have provided an insurance against a potentially costly outcome. As Nesse notes: \"Vomiting, for example, may cost only a few hundred calories and a few minutes, whereas not vomiting may result in a 5% chance of death\" page 77.\n\nSecond, self-treatments are costly both in using energy, and also in their risk of damaging the body.\n\n\nOne factor in deployment is low level physiological control by proinflammatory cytokines such as IL-1 triggered by bacterial lipopolysaccharides (LPS).\n\nAnother is higher level control in which the brain takes into account what it learns about circumstances and how that makes it well and ill. Conditioning shows the existence of such learnt control: give saccharin paired in a drink with a drug that creates immunosuppression, and later on, giving saccharin alone will produce immunosuppression. Such conditioning happens both in experimental rodents and humans.\n\nEvolution, according to Nicholas Humphrey, has selected an internal health management system that uses cost benefit analysis upon whether the deployment of a self-treatment aids biological fitness, and so should be activated. \na specially designed procedure for “economic resource management” that is, I believe, one of the key features of the “natural health-care service” which has evolved in ourselves and other animals to help us deal throughout our lives with repeated bouts of sickness, injury, and other threats to our well-being.\n\nAn analogy is explicitly made with the health economics consideration used in management decisions involving external medical treatment.\n\nNow, if you wonder about this choice of managerial terminology for talking about biological healing systems, I should say that it is quite deliberate (and so is the pun on NHS.) With the phrase “natural health-care service” I do intend to evoke, at a biological level, all the economic connotations that are so much a part of modern health-care in society.\n\nExternal medications will affect the cost benefits advantages of deploying an evolved self-treatment. Some animals use external ones. Wild animals, including apes, do so in the form of ingested detoxifying clays, rough leaves that clear gut parasites, and pharmacologically active plants Complementary to this, research finds that animals have the ability to select and prefer substances that aid their recuperation from illness.\n\nThe welfare of social animals (including humans) depends upon other individuals (social buffering). The actuarial assessments of the costs and benefits of deploying a self-treatment therefore will depend upon the presence, or not, of other individuals. The presence of helpful others will affect, for example, the risk of predators when incapacitated, and—in those case in which animals do this (such as humans)—the provision of food, and care during sickness.\n\nThe health management system factors in the presence of such external treatment and social support as one aspect of the circumstances needed to determine whether it is advantageous to deploy or not an evolved self-treatment.\n\nAll humans societies use external medications, and some individuals exist that are considered to have special healing knowledge about illnesses and their treatments. Humans are also usually supportive to those in their group. The availability of these things will affect the cost benefits of the body deploying its own biological ones. This could, in turn, lead to the health management system (given its beliefs (information) about treatments and support) to deploy or not, or doing so differently, the body’s own treatments.\n\nNicholas Humphrey describes how the health management system explains placebos – an external treatment without direct physiological effects – as follows:\nSuppose, for example, a doctor gives someone who is suffering an infection a pill that she rightly believes to contain an antibiotic: because her hopes will be raised she will no doubt make appropriate adjustments to her health-management strategy – lowering her precautionary defences in anticipation of the sickness not lasting long.\n\nThe health management system, in other words, when faced with an infection is tricked into making a mistaken cost benefit analysis using false information. The effect of that false information is that the benefits of the self-treatment cease to outweigh its costs. As a result, it is not deployed, and an individual does not experience unwanted medical symptoms.\n\nFailure to deploy an evolved self-treatment need not put an individual at risk since evolution has advantaged their deployment on a precautionary basis. As Nicholas Humphrey notes:\n\nTherefore, not deploying an evolved self-treatment, and so not having a medical symptom due to placebo false information might be without consequence.\n\nThe health management system’s idea of a top down neural control of the body is also found in the idea that a central governor regulates muscle fatigue to protect the body from the harmful effects (such as anoxia and hyperglycemia) of over prolonged exercise.\n\nThe idea of a fatigue governor was first proposed in 1924 by the 1922 Nobel Prize winner Archibald Hill, and more recently, on the basis of modern research, by Tim Noakes.\n\nLike with the health management system, the central governor shares the idea that much of what is attributed to low level feedback homeostatic regulation is, in fact, due to top down control by the brain. The advantage of this top down management is that the brain can enhance such regulation by allowing it to be modified by information. For example, in endurance running, a cost benefit trade exists off between the advantages of continuing to run, and the risk if this is too prolonged that it might harm the body. Being able to regulate fatigue in terms of information about the benefits and costs of continued exercise would enhance biological fitness.\n\nLow level theories exist that suggest that fatigue is due mechanical failure of the exercising muscles (\"peripheral fatigue\"). However, such low level theories do not explain why running muscle fatigue is affected by information relevant to cost benefit trade offs. For example, marathon runners can carry on running longer if told they are near the finishing line, than far away. The existence of a central governor can explain this effect.\n\n\n"}
{"id": "8613208", "url": "https://en.wikipedia.org/wiki?curid=8613208", "title": "Helen Smith (nurse)", "text": "Helen Smith (nurse)\n\nHelen Linda Smith (3 January 1956 – 20 May 1979) was a British nurse who died in allegedly suspicious circumstances in Jeddah, Saudi Arabia, after apparently falling from a balcony during a party. Her father refused to accept that her death was an accident, and alleged that there was a conspiracy to conceal the truth. \n\nMr Smith's campaign led to changes in the rules concerning inquests to violent deaths of UK citizens occurring outside the country. Helen Smith's body was stored for thirty years before being buried in 2009. The official version of her death was never disproved. \n\nFollowing a party at the house of Dr Richard Arnot and his wife Penny, the bodies of Helen (23) and Johannes Otten (35), a Dutch tugboat captain, were found in the street 70 feet below the Arnots' sixth floor balcony. Helen was found lying in the road clothed but with her underpants partly off and Johannes, whose underpants were around his thighs, was impaled upon the spiked railings surrounding the apartment block.\n\nAlso present at the Arnots' that night were Tim Hayter, a diver from New Zealand, a marine biologist named Jacques Texier, four German salvage operators and quite a number of other people who were never traced.\n\nThe presence of alcohol at the party in a \"dry\" country, and evidence given by Jacques Texier of a sexual encounter between Tim Hayter and Mrs Arnot at the time of the deaths, led to increased interest in the case, and the prosecution of Mrs Arnot by the Saudi authorities for \"unlawful intercourse\". Richard Arnot was sentenced to eighty strokes of the cane, to be administered in public, for illegally supplying alcohol – although the sentence was never carried out. Arnot also potentially faced a year in prison for \"allowing his wife to talk and dance with other men\".\n\nThe official Saudi investigation into the incident concluded that the couple had fallen from the balcony while drunk, possibly after or during a sexual encounter. This conclusion was endorsed by the British Foreign Office.\n\nHelen's father Ron Smith, a retired policeman, refused to accept the conclusion and embarked upon a lengthy campaign to expose what he considered to be a Saudi–British coverup. He accused Richard Arnot of murdering his daughter. Arnot later wrote the book \"Arabian Nightmare\" about his experience of being prosecuted in Saudi Arabia and being subsequently accused of rape and murder by Ron Smith.\n\nAt her father's insistence, Helen's body was left unburied for more than 30 years in storage at Leeds General Infirmary, thought to be the longest time a body has been stored without burial in the United Kingdom. Some six different post mortem examinations and forensic investigations were performed over this period, with varying conclusions.\n\nThe death of Helen Smith and her father's subsequent campaign led to a change in the rules regarding inquests into violent or unnatural deaths abroad. The West Yorkshire Coroner, Philip Gill had declined to hold an inquest, concluding that the case was outside his jurisdiction. Although his decision was upheld by the High Court, in July 1982 the Court of Appeal ordered the inquest to go ahead. The ruling imposed a duty on coroners to investigate deaths abroad where the body was returned to England and Wales. The hearing was billed as the inquest of the century but failed to give Ron Smith the closure he had wanted; the jury returned an open verdict. The 1982 ruling affected thousands of cases and was later cited as a reason for the decision to hold an inquest into the death of Diana, Princess of Wales. Human Rights barrister Geoffrey Robertson wrote about his time acting for Ron Smith, Helen's father, at the inquest into her death. Robertson theorises that Helen did not fall from the balcony while having sex, but was possibly sexually assaulted before her death. \"Usher (the pathologist) thought they (genital injuries) indicated rape, and suggested some violent sexual activity had taken place\". Robertson also states in his book that pathologists \"Dalgaard, Usher & Green...produced evidence which precluded a fall from a height\".\n\nAlthough her father had insisted that Helen would not receive a funeral until the full circumstances of her death had been uncovered, following discussions with Helen's mother he agreed to hold a funeral while both of them were still alive. Helen Smith's remains were cremated on 9 November 2009, over 30 years after her death. Her ashes were scattered on Ilkley Moor. Ron Smith died on 15 April 2011.\n\n\n"}
{"id": "35672322", "url": "https://en.wikipedia.org/wiki?curid=35672322", "title": "How to Start a Revolution", "text": "How to Start a Revolution\n\nHow to Start a Revolution is a BAFTA Award-winning British documentary film about Nobel Peace Prize nominee and political theorist Gene Sharp, described as the world's foremost scholar on nonviolent revolution. The 2011 film describes Sharp's ideas, and their influence on popular uprisings around the world. Screened in cinemas and television in more than 22 countries it became popular among the Occupy Wall Street Movement.\n\nDirected by British journalist Ruaridh Arrow the film follows the use of Gene Sharp's work across revolutionary groups throughout the world. There is particular focus on Sharp's key text \"From Dictatorship to Democracy\" which has been translated by democracy activists into more than 30 languages and used in revolutions from Serbia and Ukraine to Egypt and Syria. The film describes how Sharp's 198 methods of nonviolent action have inspired and informed uprisings across the globe.\n\nA primary character of the film is Gene Sharp, founder of the Albert Einstein Institution, and a 2009 and 2012 nominee for the Nobel Peace Prize. Sharp has been a scholar on nonviolent action for more than 50 years, and has been called both the \"Machiavelli of nonviolence\" and the \"Clausewitz of nonviolent warfare.\"\nOther main characters include Jamila Raqib, a former Afghan refugee and the Executive Director of the Albert Einstein Institution; Colonel Robert \"Bob\" Helvey; Srđa Popović, leader of Otpor! students group Serbia; Ahmed Maher, leader of April 6 democracy group Egypt; and Ausama Monajed, Syrian activist.\n\nScottish journalist Ruaridh Arrow, who wrote, directed, and co-produced the film, explained that he first learned about Gene Sharp's work as a student, and then heard that Sharp's booklets were turning up on the sites of many revolutions. But Sharp himself remained largely unknown. In explaining his motivation to make the film, Arrow stated that\n\nThe film was privately funded by Ruaridh Arrow and additional funding was raised through the US crowdfunding site Kickstarter. The film raised $57,342 in just under 4 weeks making it the most successful British crowdfunded film currently completed. Several high-profile figures are credited by the producers with supporting the crowdfunding project, including director Richard Linklater and actress Miriam Margolyes. Completion funding was donated by US art collector James Otis who sold the largest collection of Gandhi possessions including Gandhi's iconic glasses and sandals in 2009. Otis stated that he was selling the items to help fund nonviolent struggle projects and is described as the Executive Producer of the film.\n\nPrincipal photography began in May 2009 with Director of Photography Philip Bloom in Boston. Interview sequences were shot on Sony EX1 cameras with a Letus 35mm lens adapter and the Canon 5dmk2 DSLR camera. Ruaridh Arrow travelled to Egypt to film the Egyptian revolution in February 2011 but his camera equipment was seized by Egyptian secret police on landing and key sequences had to be filmed on the iphone4. Arrow reported live from Tahrir Square for BBC News during this period.\n\nThe premiere was held in Boston on 18 September 2011, the day after the Occupy Wall St protests officially began in New York. The film received a standing ovation and won Best Documentary and the Mass Impact award at Boston Film Festival, and went on to be screened by Occupy camps across the US and Europe including the Bank of Ideas in London.\n\nThe European premiere was held at Raindance Film Festival in London where the film received the award for Best Documentary. Subsequent awards have included Best Documentary Fort Lauderdale International Film Festival 2011, Special Jury Award One World Film Festival Ottawa, Jury Award Bellingham Human Rights Film Festival and Best Film, Barcelona Human Rights Film Festival. The film won the Scottish BAFTA for new talent in April 2012 and shortlisted for a Grierson Award in July 2012.\n\n\"How to Start a Revolution\" was picked up for distribution by TVF International in the UK and 7th Art Releasing in the US. The film has reportedly been translated into nine languages, including Japanese and Russian. The Albert Einstein Institute has reported that the film has been shown internationally on several television stations.\n\nThe film has received a positive critical reception. It received four stars in \"Time Out London\", \"a reminder of the importance of intellectual thought to the everyday\". The Huffington Post said it was a \"vital conversation starter and educational tool in a world awash with violence\" and the UK's Daily Telegraph described it as a \"World conquering Documentary\". \"The New York Times\" called it a \"noble documentary\" but criticised the absence of historical context of nonviolent struggles pre-dating Sharp. \"Variety\" described the film as \"straightforward\", \"informative\", and \"with potential to be updated as world events unfold\", stating it \"should have a long shelf life\". Negative references have been made to the use of dramatic music during certain sequences.\n\n\"How to Start a Revolution\" was released on 18 September 2011 the day after the first Occupy protests in Wall St, New York. The film was described as the unofficial film of the Occupy movement and shown in camps across the US and Europe. It was one of a number of high-profile events held in London's Bank of Ideas along with a concert by British band Radiohead.\n\nIn 2012 following the Mexican general election one of the country's largest newspapers reported that protestors were circulating a pirated Spanish translation of \"How to Start a Revolution\" which had gone viral in the country. The translation was viewed over half a million times in the space of three days. Reports have also been published citing the airing of the film on Spanish television concurrent with widespread discussion of Sharp's work in the Spanish anti-austerity 15-M Movement.\n\nThe academic premiere was hosted by the Program on Negotiation at Harvard Law School on 11 October 2011, and In February 2012, \"How to Start a Revolution\" was screened to an audience of MP's and Lords in the UK Houses of Parliament by the All Party Parliamentary Group on Conflict Issues (APPGCI) which was attended by Gene Sharp.\n\nA film about the making of \"How to Start a Revolution\", entitled \"Road to Revolution\", was screened in January 2012 by Current TV in the UK.\n\nOn January 22, 2017, after the inauguration of President Donald Trump, the PBS America channel screened \"How to Start a Revolution\" immediately after a \"Frontline\" investigation into his election.\n\nIn 2012 \"How to Start a Revolution\" was among the first \"Touch Documentaries\" to be released using the Apple iPad platform. The film was integrated into the platform along with four of Gene Sharp's books in several languages, including \"From Dictatorship to Democracy\", and several of Sharp's lectures. The app is supplemented by analysis and satellite mapping which is offered up to the viewer while watching the film. A \"Revolution Monitor\" is also included which fuses Google Earth maps with Twitter displaying tweets and YouTube links from revolutionary groups and individuals when countries of interest are touched by the viewer.\nA review by The Peace & Collaborative Network described the app as \"simply a must-have among peace studies scholars, those actively working to start or reorganize revolutions, or anyone who is interested in the logistics, history, and outcomes of nonviolent revolutions\" \nThe \"How to Start a Revolution\" touch documentary was shortlisted for the International Best Digital Media award in the One World Media Awards 2013 \n\n"}
{"id": "4915128", "url": "https://en.wikipedia.org/wiki?curid=4915128", "title": "IDRO Group", "text": "IDRO Group\n\nThe Industrial Development & Renovation Organization of Iran (IDRO) known as IDRO Group was established in 1967 in Iran. IDRO Group is one of the largest companies in Iran. It is also one of the largest conglomerates in Asia. IDRO's objective is to develop Iran's industry sector and to accelerate the industrialization process of the country and to export Iranian products worldwide. Today, IDRO owns 117 subsidiaries and affiliated companies both domestically as well as internationally.\n\nIn the course of its 40 years of activity, IDRO has gradually become a major shareholder of some key industries in Iran. In recent years and in accordance with the country's privatization policy, IDRO has made great efforts to privatize its affiliated companies. While carrying on its privatization policies and lessening its role as a holding company, IDRO intends to concentrate on its prime missions and to turn into an industrial development agency.\nIDRO has focused its activities on the following areas in order to materialize such strategy and to expedite the industrial development of Iran:\n\n\nIDRO had privatized 140 of its companies worth about 2,000 billion rials ($200 million) in the past. The organization will offer shares of 150 industrial units to private investors by March 2010. In 2009, 290 companies were under the control of the IDRO.\n\nThis is a list of IDRO's main subsidiaries (as of 2008):\n\n\n"}
{"id": "7969102", "url": "https://en.wikipedia.org/wiki?curid=7969102", "title": "Isaac ben Reuben Albargeloni", "text": "Isaac ben Reuben Albargeloni\n\nIsaac ben Reuben Albargeloni (born 1043) was a Spanish Talmudist and liturgical poet born in Barcelona. He was a judge in the Denia community, where he became connected with ibn Alḥatosh, likely due to becoming his son-in-law. Among his later descendants was Moses ben Naḥman (Naḥmanides); Judah Albargeloni is said to have been Isaac's pupil. He was one of five prominent contemporaneous scholars named Isaac and the regard in which he was held by his own and succeeding generations is indicated by the fact that he is simply designated \"Ha-Rab Albargeloni.\" He wrote commentaries on various sections of the tractate Ketubot, and at the age of thirty-five (1078) translated, from the Arabic into Hebrew, Hai Gaon's \"Ha-Miḳḳaḥ weha-Mimkar,\" on buying and selling (Venice, 1602, and frequently afterward with commentaries). Noteworthy among his liturgical poems are his \" \", included in the rituals of Constantine, Tlemçen, Tunis, Morocco, Algeria, and Oran. \n\nThere are 145 strophes in the poem, each consisting of three verses, and it ends with a Biblical quotation. Contemporaries said that Isaac's use of Biblical verses indicates great skill. Al-Ḥarizi remarked: \"He has put the religious laws into rime, and has fitted them so well to Biblical passages that it almost seems as if the work had been inspired by a higher power.\" Isaac faithfully copied the division of the laws and interdictions of the \"Halakot Gedolot\"; at times even following its wording, while he also took into account the regulations of traditional literature referring to Biblical prescriptions. The following poems of Isaac are also included in the \"Azharot\": \n\nIsaac also wrote \"Paḥadti mi-Yoẓeri\" and \"Yom Zeh Mekapper le-Shabim,\" both in three-line strophes, the latter with signature and alphabet. Rapoport also attributes to Isaac the works \"Ayumati Yonah,\" \"ahabah\" for the Sabbath before the Feast of Weeks, and \"Yaḳush be-'Onyo,\" \"ge'ullah\" for the fifth Sabbath after Pesaḥ, but other scholars dispute whether these works were created by Isaac.\n\n"}
{"id": "31805648", "url": "https://en.wikipedia.org/wiki?curid=31805648", "title": "Joseph Allison (Canadian politician)", "text": "Joseph Allison (Canadian politician)\n\nJoseph Allison (ca 1755 – November 23, 1806) was an Irish-born farmer and political figure in Nova Scotia. He represented Horton township in the Nova Scotia House of Assembly from 1799 to 1806.\n\nHe wasborn in Drumnaha in County Londonderry, the son of Joseph Allison and Alice Polk (or \"Pollock\"'), and came to Nova Scotia with his parents in 1769. Allison married Alice Harding, the daughter of a Loyalist. He died in Horton.\n\nHis brother John also served in the assembly.\n"}
{"id": "35998753", "url": "https://en.wikipedia.org/wiki?curid=35998753", "title": "L. Barrington Smith", "text": "L. Barrington Smith\n\nLloyd Barrington Smith (1917 – 8 April 1992) was a Jamaican stamp dealer based who is noted within Jamaican philately for his patriotic covers supporting the allies during World War II.\n\nHe married Kathleen Dickson on 18 November 1939.\n\nAs a stamp dealer, Barrington Smith dealt principally in used British Commonwealth stamps, but the home-made and naive patriotic covers that he sold in bulk during the war years have become collectable in their own right. Barrington Smith traded from 1935 until about 1962, initially as a printer, soon becoming a stamp dealer and later a stamp wholesaler and supplier of \"tropical novelties\". He is not believed to have been connected with the Leicester, England, firm of philatelic accessary wholesalers \"Barrington Smith\".\n\nHe traded under his own name and also as the \"Buccaneer Hobby Club\" from 127 Hagley Park Road, Half Way Tree Post Office, Jamaica, and was the publisher of \"The West Indian Philatelist\".\n\n\n"}
{"id": "5544888", "url": "https://en.wikipedia.org/wiki?curid=5544888", "title": "Machinesmith", "text": "Machinesmith\n\nMachinesmith (Samuel \"Starr\" Saxon) is a fictional supervillain appearing in American comic books published by Marvel Comics. He specializes in robotics, and is able to make convincing robotic doubles of other superhumans. His own mind was ultimately transferred to a robotic body.\n\nThe character first appeared in \"Daredevil\" #49 (February 1969), and briefly appeared as a character to have used the Mister Fear identity shortly thereafter in \"Daredevil\" #54 (July 1969). The character first appeared as Machinesmith in \"Marvel Two-in-One\" #47 (January 1979). \n\nHis robotic features looked nothing like his human ones, and it was not established until later in \"Captain America\" #249 (September 1980) that Machinesmith and Starr Saxon are the same character. Barry Windsor-Smith has stated that back the character was supposed to be presented as gay in \"Daredevil\" #50; however, the early art was not good enough to get the point across. Other issues have since revealed the character's sexuality more directly, such as \"Captain America\" #368 and \"Iron Man\" #320.\n\nStarr Saxon was born in Memphis, Tennessee, but his family was living in Queens, New York by his teenage years. At 14 years old, he discovered an abandoned Doombot in a NYC subway tunnel, and snuck the robot home piece by piece to deconstruct to learn robotics. His original efforts saw to his use of robotics and engineering abilities to be a professional criminal robot maker and construct a variety of androids to be used as assassins for hire/personal gain. At some point during this period, he constructs a facsimile of Magneto and a variety of robotic \"mutant\" drones who went on to battle the X-Men. Magneto's robot appears several times over the years, believing itself to be the true Magneto until being destroyed by a Sentinel. As of this writing, his employer(s) for the robot's creation is unrevealed.\n\nIn his first actual appearance, Saxon is hired by Biggie Benson to kill Daredevil. Saxon dispatches a powerful android to do so, and to commit a series of crimes in New York. He discovers Daredevil's secret identity, and kidnaps Karen Page (Daredevil's girlfriend). He blackmails Daredevil into allowing him to escape. Deciding to confront Daredevil directly, Saxon murders Zoltan Drago and steals the man's costume and weaponry. As \"Mister Fear\", he challenges Daredevil to a public duel in New York City. He rigs Daredevil's billy club to release fear-gas pellets, and begins a crime spree. However, Saxon battles Daredevil and loses, breaking his neck in a fall from a flying hovercraft platform.\n\nSaxon's robots find his dying body and transfer his brain patterns and consciousness into a computer, from which he could control a variety of android bodies. Now calling himself the \"Machinesmith\", he is hired by the Corporation agent known as the Carnation to defeat the Fantastic Four. He dispatches his robots to subdue the Thing to be brainwashed into destroying the Fantastic Four. The Thing encounters Jack of Hearts instead and is defeated and Machinesmith is then revealed to be a robot.\n\nWhen Captain America and Nick Fury have an encounter with Baron Strucker, Captain America defeats Strucker who is revealed to be a robot, controlled remotely by Machinesmith who plots to destroy Captain America. He comes to despise his artificial life, but his programming prevents him from committing suicide. Machinesmith sends the Dragon Man to kill Captain America but the attempt is unsuccessful.\n\nMachinesmith is hired into the Red Skull's exclusive employ, for whom he served as his primary scientist/machinist and member of the Skeleton Crew. On several occasions, Machinesmith undertakes various the Red Skull's field missions, usually confronting Captain America.\n\nAs per the Red Skull's orders, he sets mechanisms in S.H.I.E.L.D. Central against Captain America and Fury. As per the Red Skull's orders, he next activates the Sleeper robot, and attempts to liberate the other robots impounded on Avengers Island. He then assists Mother Night in an attempt to bug the Avengers headquarters, and he battles and overpowers the Vision. Alongside the Skeleton Crew, he later battles the Schutz-Heiliggruppe.\n\nA portion of the Machinesmith's consciousness is eventually captured (assumed to be Saxon's entire consciousness), and enslaved by Tony Stark under Kang the Conqueror's thrall. Machinesmith later claims to have easily recovered the lost fragment which he re-assimilated.\n\nWhen the Red Skull is blasted apart by Kubekult's Cosmic Cube destruction, Machinesmith enacts a series of protocols dictated by the Red Skull to kill Captain America and plunge the world into nuclear holocaust. Machinesmith is defeated.\n\nLater, Machinesmith is employed by the Crimson Cowl's Masters of Evil. After the team's defeat by the Thunderbolts, Machinesmith has since lain low.\n\nHe later resurfaces and appears battling the New Warriors.\n\nMachinesmith later appears in Madripoor, having captured Captain America and deactivated the Super Soldier Serum in his foe's body. It was Machinesmith's intent to reverse-engineer the Super Soldier Serum to sell to various military officials. Captain America is able to regain the enhanced physique and defeats Machinesmith, trapping his consciousness inside a robotic body that he could not wirelessly transfer himself out of. At the end of the issue, Machinesmith was revealed to be an expendable pawn of the mysterious Shadow Council. But Machinesmith is resurrected a year later by Sharon Carter to discover how Captain America was consistently reverting to a weaker form, bargaining for his freedom. Machinesmith confessed to having been in league with Captain America's old comrade Codename: Bravo and revealed that Captain America was infected with nanotechnology that mimicked red blood cells. Unbeknownst to Machinesmith, Carter had infected him with a virus that wiped away his memories eliminating him as a threat.\n\nMachinesmith is later paroled after helping the U.S. government hack into Latveria's computer network. He moves to Miami and gets a menial job performing at children's birthday parties, before he is asked by Grizzly to help Scott Lang with Cassie Lang's rescue from Cross Technological Enterprises. Machinesmith agrees on the condition that Lang gets him a job at Ant-Man Security Solutions as stable employment will help keep his parole officer off his back.\n\nAs part of the \"All-New, All-Different Marvel\" event, Machinesmith is temporarily hired by Augustine Cross to hack into the Power Broker Inc. database so that the Cross family could steal its algorithm to be used to create a Hench App knock-off called \"Lackey\" since Darren Cross refused to invest with Power Broker.\n\nDuring the \"Secret Empire\" storyline, Machinesmith and Grizzly join up with the Army of Evil during Hydra's rise to power.\n\nStarr Saxon originally had a genius intellect, but no superhuman powers. He is one of the most gifted robot designers in the world, and has vast experience in cybernetic and bionics.\n\nAfter breaking his neck, his consciousness began to occupy a robot duplicate of himself, programmed with his complete brain patterns, and capable of self-motivated, creative activity. His robotic materials, design and construction provided him with a number of superhuman capacities, including superhuman strength, speed, stamina, durability, agility and reflexes.\n\nMachinesmith exists as a living, cybernetic-system program (artificial consciousness), which is equipped to transmit its program along an infrared laser beam into virtually any electronics system at will; thus he can transfer from one robotic body to another within .25 seconds. Machinesmith can even place his personal program (personality) into multiple bodies at the same time, though the number of complex motions he can make his automata perform simultaneously is limited. His physical properties vary in accordance with the robot form he is inside. Certain robots possess superhuman capacities such as telescoping arms and legs, explosive launchers, special infrared or telescopic eyes. He has yet to inhabit a robot body greatly superior to a standard human-mimicking robot's capacity. If an electronics system shuts down before he has a chance to project out of it, he can be trapped inside of it. Machinesmith is a genius at creating complex behavioral programs and bionic systems.\n\nMachinesmith has also created a vast arsenal of weaponry, defense systems, and surveillance devices, whose specifications are constantly upgraded.\n\n"}
{"id": "20764758", "url": "https://en.wikipedia.org/wiki?curid=20764758", "title": "Maria Teresa Felicitas d'Este", "text": "Maria Teresa Felicitas d'Este\n\nMaria Teresa Felicitas d'Este (October 6, 1726 – April 30, 1754) was born a princess of Modena and was by marriage the Duchess of Penthièvre. She was the mother-in-law of Philippe Égalité and thus grandmother to the future Louis-Philippe of France.\n\nBorn in Emilia-Romagna in Modena, she was the daughter of Francesco III, Duke of Modena and his wife, the French princess Charlotte Aglaé d'Orléans.\n\nOne of nine children, she was the first daughter of the family but her mother eventually returned to France to live.\n\nPenthièvre was Maria Teresa's mother's first cousin. Charlotte Aglaé's mother was the sister of the duke's father, Louis-Alexandre Count of Toulouse. According to some contemporaries, the marriage between the duke and his wife was reportedly a happy one, to which numerous children were born.\n\n\nMarie Thérèse died at the Château de Rambouillet after giving birth to Louis Marie Félicité de Bourbon, who died soon afterwards. Only two of the Penthièvre children survived into adulthood:\n\n"}
{"id": "3077083", "url": "https://en.wikipedia.org/wiki?curid=3077083", "title": "Maurice Rollinat", "text": "Maurice Rollinat\n\nMaurice Rollinat (December 29, 1846 in Châteauroux, Indre – October 26, 1903 in Ivry-sur-Seine) was a French poet.\n\nHis father represented Indre in the National Assembly of 1848, and was a friend of George Sand, whose influence is very marked in young Rollinat's first volume, \"Dans les brandes\" (1877), and to whom it was dedicated.\n\nAfter its publication, he abandoned realism and worked in a very different manner. He joined a literary circle that called themselves \"Les Hydropathes\", founded by Émile Goudeau, an anti-clerical group with ties to the decadent literary movement. Under their influence wrote the poems that made his reputation. In \"Les Névroses\", with the sub-title \"Les Âmes, Les Luxures, Les Refuges, Les Spectres, Les Ténèbres\", he showed himself as a disciple of Charles Baudelaire. He constantly returns in these poems to the physical horrors of death, and is obsessed by unpleasant images. Less outre in sentiment are \"L'Abîme\" (1886), \"La Nature\", and a book of children's verse, \"Le Livre de la Nature\" (1893).\n\nHe was musician as well as poet, and set many of his songs to music. Several evenings a week, Rollinat would appear at the cabaret Le Chat Noir, and there he would perform his poems with piano accompaniment. His gaunt and pale appearance made his portrait a favourite subject for a number of painters, and the startling subjects of his verses brought him short lived fame; at the height of his popularity he drew a number of celebrities to the cabaret to see him perform; among them were Leconte de Lisle and Oscar Wilde. Rollinat's friend Jules Barbey d'Aurevilly wrote that \"Rollinat might be Baudelaire's superior in the sincerity and depth of his diabolism\".\n\nRollinat married the actress Cécile Pouettre. He lost his reason in consequence of his wife's death from rabies; after several suicide attempts, he died in an insane asylum at Ivry-sur-Seine. He is buried in the Saint-Denis cemetery in Châteauroux.\n\nOn Rollinat's death, Auguste Rodin offered the Fresselines commune a bas-relief entitled \"Poet and the Muse\". This sculpture is on display on the wall of the village church at Crozant.\n\n\n\n\n\n"}
{"id": "415446", "url": "https://en.wikipedia.org/wiki?curid=415446", "title": "McCaughey septuplets", "text": "McCaughey septuplets\n\nThe McCaughey septuplets (born November 19, 1997), are septuplets (seven offspring) born to Kenny and Bobbi McCaughey in Des Moines, Iowa, United States. They are the world's first set of surviving septuplets.\n\nBobbi and Kenny McCaughey (pronounced \"McCoy\") had one daughter, Mikayla Marie, born January 3, 1996.\nWhile under treatment with ovulation-stimulating Metrodin for infertility, Bobbi became pregnant with seven babies. The McCaugheys, residents of the town of Carlisle, declined selective reduction to reduce the number of infants, saying that they would \"put it in God's hands\". The obstetricians primarily responsible for the medical care of the babies were Karen Drake and Paula Mahone.\n\nThe septuplets, four boys and three girls, were born nine weeks prematurely in Des Moines on November 19, 1997. They were born by Caesarean section, all within six minutes. They were the world's first set of surviving septuplets.\n\nTwo of the septuplets, Alexis (the oldest girl of the set) and Nathan (the 2nd oldest boy of the set), have cerebral palsy. Both used walkers to get around, and in November 2005 Nathan had spinal surgery in order to improve his walking abilities.\n\nThe birth attracted significant media attention, both positive and negative, including a feature in \"Time\" magazine in December 1997. \"In the beginning, for every ten letters we would get that were happy for us, we'd get one letter accusing us of exploiting the kids and being selfish to waste the world's resources on a family this big,\" said Bobbi in a 2007 interview. \"Our neighbors never gawked. Here in Carlisle they gave us privacy. But we had complete strangers come around to the back door, knock, and ask if they could hold a baby.\"\n\nThe McCaugheys were the recipients of many donations, including a 5500 ft² (511 m²) house, a van and diapers for the first two years, as well as nanny services, clothes, and even the State of Iowa offering full college scholarships to any state university in Iowa upon their maturity and graduation from high school, also by the Hannibal–LaGrange University in Missouri. President Bill Clinton personally telephoned the McCaugheys to wish them his congratulations. The surviving Dionne quintuplets- Yvonne Dionne, Annette Allard, and Cecile Langlois- wrote a letter warning the parents to keep the septuplets out of the public eye and not allow them to fall into the same pitfalls as their parents did, but wished them the best of luck in raising them and their personal congratulations.\n\nBy the time of the septuplets' tenth birthday in 2007, the family was declining most requests for interviews, other than annual stories with KCCI (the Des Moines CBS affiliate) and Ladies' Home Journal. Bobbi McCaughey has noted that the level of media attention does not necessarily mean they have granted many interviews, saying, \"There was all kinds of stuff in the papers early on but they never actually interviewed us. Most of it is one paper quoting another.\"\n\nBobbi and Kenny McCaughey occasionally speak at pro-life events and continue to oppose selective reduction. Bobbi has been quoted as saying, \"Well, come to our house, and tell me which four I shouldn't have had!\" The family continues to attend a Baptist church in West Des Moines where Kenny serves as a deacon. In 2010, TLC made a documentary for the septuplets' thirteenth birthday that aired on December 28, 2010.\n\nThe septuplets graduated from high school in May 2016. Natalie, Kelsey, Nathan and Joel took up scholarships offered by private Hannibal-LaGrange University in Hannibal, Missouri, Kenny and Alexis chose to stay in the Des Moines area and attend Des Moines Area Community College, and Brandon enlisted in the United States Army. Brandon will be the first one to get married as he was engaged to Alana Hale and will wed in July 2018.\n"}
{"id": "365558", "url": "https://en.wikipedia.org/wiki?curid=365558", "title": "Nicotinamide adenine dinucleotide", "text": "Nicotinamide adenine dinucleotide\n\nNicotinamide adenine dinucleotide (NAD) is a cofactor found in all living cells. The compound is a dinucleotide, because it consists of two nucleotides joined through their phosphate groups. One nucleotide contains an adenine nucleobase and the other nicotinamide. Nicotinamide adenine dinucleotide exists in two forms: an oxidized and reduced form abbreviated as NAD and NADH respectively.\n\nIn metabolism, nicotinamide adenine dinucleotide is involved in redox reactions, carrying electrons from one reaction to another. The cofactor is, therefore, found in two forms in cells: NAD is an oxidizing agent – it accepts electrons from other molecules and becomes reduced. This reaction forms NADH, which can then be used as a reducing agent to donate electrons. These electron transfer reactions are the main function of NAD. However, it is also used in other cellular processes, most notably a substrate of enzymes that add or remove chemical groups from proteins, in posttranslational modifications. Because of the importance of these functions, the enzymes involved in NAD metabolism are targets for drug discovery.\n\nIn organisms, NAD can be synthesized from simple building-blocks (\"de novo\") from the amino acids tryptophan or aspartic acid. In an alternative fashion, more complex components of the coenzymes are taken up from food as niacin. Similar compounds are released by reactions that break down the structure of NAD. These preformed components then pass through a salvage pathway that recycles them back into the active form. Some NAD is converted into nicotinamide adenine dinucleotide phosphate (NADP); the chemistry of this related coenzyme is similar to that of NAD, but it has different roles in metabolism.\n\nAlthough NAD is written with a superscript plus sign because of the formal charge on a particular nitrogen atom, at physiological pH for the most part it is actually a singly charged anion (charge of minus 1), while NADH is a doubly charged anion.\n\nNicotinamide adenine dinucleotide, like all \"dinucleotide\"s, consists of two nucleosides joined by a pair of bridging phosphate groups. The nucleosides each contain a ribose ring, one with adenine attached to the first carbon atom (the 1' position) and the other with nicotinamide at this position. The nicotinamide moiety can be attached in two orientations to this anomeric carbon atom. Because of these two possible structures, the compound exists as two diastereomers. It is the β-nicotinamide diastereomer of NAD that is found in organisms. These nucleotides are joined together by a bridge of two phosphate groups through the 5' carbons.\nIn metabolism, the compound accepts or donates electrons in redox reactions. Such reactions (summarized in formula below) involve the removal of two hydrogen atoms from the reactant (R), in the form of a hydride ion (H), and a proton (H). The proton is released into solution, while the reductant RH is oxidized and NAD reduced to NADH by transfer of the hydride to the nicotinamide ring.\n\nFrom the hydride electron pair, one electron is transferred to the positively charged nitrogen of the nicotinamide ring of NAD, and the second hydrogen atom transferred to the C4 carbon atom opposite this nitrogen. The midpoint potential of the NAD/NADH redox pair is −0.32 volts, which makes NADH a strong \"reducing\" agent. The reaction is easily reversible, when NADH reduces another molecule and is re-oxidized to NAD. This means the coenzyme can continuously cycle between the NAD and NADH forms without being consumed.\n\nIn appearance, all forms of this coenzyme are white amorphous powders that are hygroscopic and highly water-soluble. The solids are stable if stored dry and in the dark. Solutions of NAD are colorless and stable for about a week at 4 °C and neutral pH, but decompose rapidly in acids or alkalis. Upon decomposition, they form products that are enzyme inhibitors.\n\nBoth NAD and NADH strongly absorb ultraviolet light because of the adenine. For example, peak absorption of NAD is at a wavelength of 259 nanometers (nm), with an extinction coefficient of 16,900 Mcm. NADH also absorbs at higher wavelengths, with a second peak in UV absorption at 339 nm with an extinction coefficient of 6,220 Mcm. This difference in the ultraviolet absorption spectra between the oxidized and reduced forms of the coenzymes at higher wavelengths makes it simple to measure the conversion of one to another in enzyme assays – by measuring the amount of UV absorption at 340 nm using a spectrophotometer.\n\nNAD and NADH also differ in their fluorescence. NADH in solution has an emission peak at 460 nm and a fluorescence lifetime of 0.4 nanoseconds, while the oxidized form of the coenzyme does not fluoresce. The properties of the fluorescence signal changes when NADH binds to proteins, so these changes can be used to measure dissociation constants, which are useful in the study of enzyme kinetics. These changes in fluorescence are also used to measure changes in the redox state of living cells, through fluorescence microscopy.\n\nIn rat liver, the total amount of NAD and NADH is approximately 1 μmole per gram of wet weight, about 10 times the concentration of NADP and NADPH in the same cells. The actual concentration of NAD in cell cytosol is harder to measure, with recent estimates in animal cells ranging around 0.3 mM, and approximately 1.0 to 2.0 mM in yeast. However, more than 80% of NADH fluorescence in mitochondria is from bound form, so the concentration in solution is much lower.\n\nData for other compartments in the cell are limited, although in the mitochondrion the concentration of NAD is similar to that in the cytosol. This NAD is carried into the mitochondrion by a specific membrane transport protein, since the coenzyme cannot diffuse across membranes.\n\nThe balance between the oxidized and reduced forms of nicotinamide adenine dinucleotide is called the NAD/NADH ratio. This ratio is an important component of what is called the \"redox state\" of a cell, a measurement that reflects both the metabolic activities and the health of cells. The effects of the NAD/NADH ratio are complex, controlling the activity of several key enzymes, including glyceraldehyde 3-phosphate dehydrogenase and pyruvate dehydrogenase. In healthy mammalian tissues, estimates of the ratio between free NAD and NADH in the cytoplasm typically lie around 700; the ratio is thus favourable for oxidative reactions. The ratio of total NAD/NADH is much lower, with estimates ranging from 3–10 in mammals. In contrast, the NADP/NADPH ratio is normally about 0.005, so NADPH is the dominant form of this coenzyme. These different ratios are key to the different metabolic roles of NADH and NADPH.\n\nNAD is synthesized through two metabolic pathways. It is produced either in a \"de novo\" pathway from amino acids or in salvage pathways by recycling preformed components such as nicotinamide back to NAD.\n\nMost organisms synthesize NAD from simple components. The specific set of reactions differs among organisms, but a common feature is the generation of quinolinic acid (QA) from an amino acid—either tryptophan (Trp) in animals and some bacteria, or aspartic acid (Asp) in some bacteria and plants. The quinolinic acid is converted to nicotinic acid mononucleotide (NaMN) by transfer of a phosphoribose moiety. An adenylate moiety is then transferred to form nicotinic acid adenine dinucleotide (NaAD). Finally, the nicotinic acid moiety in NaAD is amidated to a nicotinamide (Nam) moiety, forming nicotinamide adenine dinucleotide.\n\nIn a further step, some NAD is converted into NADP by NAD kinase, which phosphorylates NAD. In most organisms, this enzyme uses ATP as the source of the phosphate group, although several bacteria such as \"Mycobacterium tuberculosis\" and a hyperthermophilic archaeon \"Pyrococcus horikoshii\", use inorganic polyphosphate as an alternative phosphoryl donor.\n\nBesides assembling NAD \"de novo\" from simple amino acid precursors, cells also salvage preformed compounds containing a pyridine base. The three vitamin precursors used in these salvage metabolic pathways are nicotinic acid (NA), nicotinamide (Nam) and nicotinamide riboside (NR). These compounds can be taken up from the diet and are termed vitamin B or \"niacin\". However, these compounds are also produced within cells and by digestion of cellular NAD. Some of the enzymes involved in these salvage pathways appear to be concentrated in the cell nucleus, which may compensate for the high level of reactions that consume NAD in this organelle. There are some reports that mammalian cells can take up extracellular NAD from their surroundings, and both nicotinamide and nicotinamide riboside can be absorbed from the gut.\n\nDespite the presence of the \"de novo\" pathway, the salvage reactions are essential in humans; a lack of niacin in the diet causes the vitamin deficiency disease pellagra. This high requirement for NAD results from the constant consumption of the coenzyme in reactions such as posttranslational modifications, since the cycling of NAD between oxidized and reduced forms in redox reactions does not change the overall levels of the coenzyme.\n\nThe salvage pathways used in microorganisms differ from those of mammals. Some pathogens, such as the yeast \"Candida glabrata\" and the bacterium \"Haemophilus influenzae\" are NAD auxotrophs – they cannot synthesize NAD – but possess salvage pathways and thus are dependent on external sources of NAD or its precursors. Even more surprising is the intracellular pathogen \"Chlamydia trachomatis\", which lacks recognizable candidates for any genes involved in the biosynthesis or salvage of both NAD and NADP, and must acquire these coenzymes from its host.\n\nNicotinamide adenine dinucleotide has several essential roles in metabolism. It acts as a coenzyme in redox reactions, as a donor of ADP-ribose moieties in ADP-ribosylation reactions, as a precursor of the second messenger molecule cyclic ADP-ribose, as well as acting as a substrate for bacterial DNA ligases and a group of enzymes called sirtuins that use NAD to remove acetyl groups from proteins. In addition to these metabolic functions, NAD emerges as an adenine nucleotide that can be released from cells spontaneously and by regulated mechanisms, and can therefore have important extracellular roles.\n\nThe main role of NAD in metabolism is the transfer of electrons from one molecule to another. Reactions of this type are catalyzed by a large group of enzymes called oxidoreductases. The correct names for these enzymes contain the names of both their substrates: for example NADH-ubiquinone oxidoreductase catalyzes the oxidation of NADH by coenzyme Q. However, these enzymes are also referred to as \"dehydrogenases\" or \"reductases\", with NADH-ubiquinone oxidoreductase commonly being called \"NADH dehydrogenase\" or sometimes \"coenzyme Q reductase\".\n\nThere are many different superfamilies of enzymes that bind NAD / NADH. One of the most common superfamilies include a structural motif known as the Rossmann fold. The motif is named after Michael Rossmann who was the first scientist to notice how common this structure is within nucleotide-binding proteins.\n\nAn example of a NAD-binding bacterial enzyme involved in amino acid metabolism that does not have Rossmann fold: .\n\nWhen bound in the active site of an oxidoreductase, the nicotinamide ring of the coenzyme is positioned so that it can accept a hydride from the other substrate. Depending on the enzyme, the hydride donor is positioned either \"above\" or \"below\" the plane of the planar C4 carbon, as defined in the figure. Class A oxidoreductases transfer the atom from above; class B enzymes transfer it from below. Since the C4 carbon that accepts the hydrogen is prochiral, this can be exploited in enzyme kinetics to give information about the enzyme's mechanism. This is done by mixing an enzyme with a substrate that has deuterium atoms substituted for the hydrogens, so the enzyme will reduce NAD by transferring deuterium rather than hydrogen. In this case, an enzyme can produce one of two stereoisomers of NADH.\n\nDespite the similarity in how proteins bind the two coenzymes, enzymes almost always show a high level of specificity for either NAD or NADP. This specificity reflects the distinct metabolic roles of the respective coenzymes, and is the result of distinct sets of amino acid residues in the two types of coenzyme-binding pocket. For instance, in the active site of NADP-dependent enzymes, an ionic bond is formed between a basic amino acid side-chain and the acidic phosphate group of NADP. On the converse, in NAD-dependent enzymes the charge in this pocket is reversed, preventing NADP from binding. However, there are a few exceptions to this general rule, and enzymes such as aldose reductase, glucose-6-phosphate dehydrogenase, and methylenetetrahydrofolate reductase can use both coenzymes in some species.\n\nThe redox reactions catalyzed by oxidoreductases are vital in all parts of metabolism, but one particularly important area where these reactions occur is in the release of energy from nutrients. Here, reduced compounds such as glucose and fatty acids are oxidized, thereby releasing energy. This energy is transferred to NAD by reduction to NADH, as part of beta oxidation, glycolysis, and the citric acid cycle. In eukaryotes the electrons carried by the NADH that is produced in the cytoplasm are transferred into the mitochondrion (to reduce mitochondrial NAD) by mitochondrial shuttles, such as the malate-aspartate shuttle. The mitochondrial NADH is then oxidized in turn by the electron transport chain, which pumps protons across a membrane and generates ATP through oxidative phosphorylation. These shuttle systems also have the same transport function in chloroplasts.\n\nSince both the oxidized and reduced forms of nicotinamide adenine dinucleotide are used in these linked sets of reactions, the cell maintains significant concentrations of both NAD and NADH, with the high NAD/NADH ratio allowing this coenzyme to act as both an oxidizing and a reducing agent. In contrast, the main function of NADPH is as a reducing agent in anabolism, with this coenzyme being involved in pathways such as fatty acid synthesis and photosynthesis. Since NADPH is needed to drive redox reactions as a strong reducing agent, the NADP/NADPH ratio is kept very low.\n\nAlthough it is important in catabolism, NADH is also used in anabolic reactions, such as gluconeogenesis. This need for NADH in anabolism poses a problem for prokaryotes growing on nutrients that release only a small amount of energy. For example, nitrifying bacteria such as \"Nitrobacter\" oxidize nitrite to nitrate, which releases sufficient energy to pump protons and generate ATP, but not enough to produce NADH directly. As NADH is still needed for anabolic reactions, these bacteria use a nitrite oxidoreductase to produce enough proton-motive force to run part of the electron transport chain in reverse, generating NADH.\n\nThe coenzyme NAD is also consumed in ADP-ribose transfer reactions. For example, enzymes called ADP-ribosyltransferases add the ADP-ribose moiety of this molecule to proteins, in a posttranslational modification called ADP-ribosylation. ADP-ribosylation involves either the addition of a single ADP-ribose moiety, in \"mono-ADP-ribosylation\", or the transferral of ADP-ribose to proteins in long branched chains, which is called \"poly(ADP-ribosyl)ation\". Mono-ADP-ribosylation was first identified as the mechanism of a group of bacterial toxins, notably cholera toxin, but it is also involved in normal cell signaling. Poly(ADP-ribosyl)ation is carried out by the poly(ADP-ribose) polymerases. The poly(ADP-ribose) structure is involved in the regulation of several cellular events and is most important in the cell nucleus, in processes such as DNA repair and telomere maintenance. In addition to these functions within the cell, a group of extracellular ADP-ribosyltransferases has recently been discovered, but their functions remain obscure.\nNAD may also be added onto cellular RNA as a 5'-terminal modification.\n\nAnother function of this coenzyme in cell signaling is as a precursor of cyclic ADP-ribose, which is produced from NAD by ADP-ribosyl cyclases, as part of a second messenger system. This molecule acts in calcium signaling by releasing calcium from intracellular stores. It does this by binding to and opening a class of calcium channels called ryanodine receptors, which are located in the membranes of organelles, such as the endoplasmic reticulum.\n\nNAD is also consumed by sirtuins, which are NAD-dependent deacetylases, such as Sir2. These enzymes act by transferring an acetyl group from their substrate protein to the ADP-ribose moiety of NAD; this cleaves the coenzyme and releases nicotinamide and O-acetyl-ADP-ribose. The sirtuins mainly seem to be involved in regulating transcription through deacetylating histones and altering nucleosome structure. However, non-histone proteins can be deacetylated by sirtuins as well. These activities of sirtuins are particularly interesting because of their importance in the regulation of aging.\n\nOther NAD-dependent enzymes include bacterial DNA ligases, which join two DNA ends by using NAD as a substrate to donate an adenosine monophosphate (AMP) moiety to the 5' phosphate of one DNA end. This intermediate is then attacked by the 3' hydroxyl group of the other DNA end, forming a new phosphodiester bond. This contrasts with eukaryotic DNA ligases, which use ATP to form the DNA-AMP intermediate.\n\nIn recent years, NAD has also been recognized as an extracellular signaling molecule involved in cell-to-cell communication. NAD is released from neurons in blood vessels, urinary bladder, large intestine, from neurosecretory cells, and from brain synaptosomes, and is proposed to be a novel neurotransmitter that transmits information from nerves to effector cells in smooth muscle organs. Further studies are needed to determine the underlying mechanisms of its extracellular actions and their importance for human health and diseases.\n\nThe enzymes that make and use NAD and NADH are important in both pharmacology and the research into future treatments for disease. Drug design and drug development exploits NAD in three ways: as a direct target of drugs, by designing enzyme inhibitors or activators based on its structure that change the activity of NAD-dependent enzymes, and by trying to inhibit NAD biosynthesis.\n\nIt has been studied for its potential use in the therapy of neurodegenerative diseases such as Alzheimer's and Parkinson's disease. A placebo-controlled clinical trial in people with Parkinson's failed to show any effect.\n\nNAD is also a direct target of the drug isoniazid, which is used in the treatment of tuberculosis, an infection caused by \"Mycobacterium tuberculosis\". Isoniazid is a prodrug and once it has entered the bacteria, it is activated by a peroxidase enzyme, which oxidizes the compound into a free radical form. This radical then reacts with NADH, to produce adducts that are very potent inhibitors of the enzymes enoyl-acyl carrier protein reductase, and dihydrofolate reductase.\n\nSince a large number of oxidoreductases use NAD and NADH as substrates, and bind them using a highly conserved structural motif, the idea that inhibitors based on NAD could be specific to one enzyme is surprising. However, this can be possible: for example, inhibitors based on the compounds mycophenolic acid and tiazofurin inhibit IMP dehydrogenase at the NAD binding site. Because of the importance of this enzyme in purine metabolism, these compounds may be useful as anti-cancer, anti-viral, or immunosuppressive drugs. Other drugs are not enzyme inhibitors, but instead activate enzymes involved in NAD metabolism. Sirtuins are a particularly interesting target for such drugs, since activation of these NAD-dependent deacetylases extends lifespan in some animal models. Compounds such as resveratrol increase the activity of these enzymes, which may be important in their ability to delay aging in both vertebrate, and invertebrate model organisms. In one experiment, mice given NAD for one week had improved nuclear-mitochrondrial communication.\n\nBecause of the differences in the metabolic pathways of NAD biosynthesis between organisms, such as between bacteria and humans, this area of metabolism is a promising area for the development of new antibiotics. For example, the enzyme nicotinamidase, which converts nicotinamide to nicotinic acid, is a target for drug design, as this enzyme is absent in humans but present in yeast and bacteria.\n\nIn bacteriology, NAD, sometimes referred to factor V, is used a supplement to culture media for some fastidious bacteria.\n\nThe coenzyme NAD was first discovered by the British biochemists Arthur Harden and William John Young in 1906. They noticed that adding boiled and filtered yeast extract greatly accelerated alcoholic fermentation in unboiled yeast extracts. They called the unidentified factor responsible for this effect a \"coferment\". Through a long and difficult purification from yeast extracts, this heat-stable factor was identified as a nucleotide sugar phosphate by Hans von Euler-Chelpin. In 1936, the German scientist Otto Heinrich Warburg showed the function of the nucleotide coenzyme in hydride transfer and identified the nicotinamide portion as the site of redox reactions.\n\nVitamin precursors of NAD were first identified in 1938, when Conrad Elvehjem showed that liver has an \"anti-black tongue\" activity in the form of nicotinamide. Then, in 1939, he provided the first strong evidence that niacin is used to synthesize NAD. In the early 1940s, Arthur Kornberg was the first to detect an enzyme in the biosynthetic pathway. In 1949, the American biochemists Morris Friedkin and Albert L. Lehninger proved that NADH linked metabolic pathways such as the citric acid cycle with the synthesis of ATP in oxidative phosphorylation. In 1958, Jack Preiss and Philip Handler discovered the intermediates and enzymes involved in the biosynthesis of NAD; salvage synthesis from nicotinic acid is termed the Preiss-Handler pathway. In 2004, Charles Brenner and co-workers uncovered the nicotinamide riboside kinase pathway to NAD.\n\nThe non-redox roles of NAD(P) were discovered later. The first to be identified was the use of NAD as the ADP-ribose donor in ADP-ribosylation reactions, observed in the early 1960s. Studies in the 1980s and 1990s revealed the activities of NAD and NADP metabolites in cell signaling – such as the action of cyclic ADP-ribose, which was discovered in 1987. The metabolism of NAD remained an area of intense research into the 21st century, with interest heightened after the discovery of the NAD-dependent protein deacetylases called sirtuins in 2000, by Shin-ichiro Imai and coworkers.\n\n\n\n\n"}
{"id": "96628", "url": "https://en.wikipedia.org/wiki?curid=96628", "title": "Offspring", "text": "Offspring\n\nIn biology, offspring are the young born of living organisms, produced either by a single organism or, in the case of sexual reproduction, two organisms. Collective offspring may be known as a brood or progeny in a more general way. This can refer to a set of simultaneous offspring, such as the chicks hatched from one clutch of eggs, or to all the offspring, as with the honeybee.\n\nHuman offspring (descendants) are referred to as children (without reference to age, thus one can refer to a parent's \"minor children\" or \"adult children\" or \"infant children\" or \"teenage children\" depending on their age); male children are sons and female children are daughters (see kinship and descent). Offspring can occur after mating or after artificial insemination.\n\nOffspring contains many parts and properties that are precise and accurate in what they consist of, and what they define. As the offspring of a new species, also known as a child or f1 generation, consist of genes of the father and the mother, which is also known as the parent generation. Each of these offspring contains numerous genes which have coding for specific tasks and properties. Males and females both contribute equally to the genotypes of their offspring, in which gametes fuse and form. An important aspect of the formation of the parent offspring is the chromosome, which is a structure of DNA which contains many genes.\n\nTo focus more on the offspring and how it results in the formation of the f1 generation, is an inheritance called sex-linkage, which is a gene which is located on the sex chromosome and patterns of these inheritance differ in both male and female. The explanation that proves the theory of the offspring having genes from both parent generations, is proven through a process called crossing-over, which consists of taking genes from the male chromosomes and genes from the female chromosome, resulting in a process of meiosis occurring, and leading to the splitting of the chromosomes evenly. Depending on which genes are dominantly expressed in the gene will result in the sex of the offspring. The female will always give an X chromosome, whereas the male, depending on the situation, will either give an X chromosome or a Y chromosome. If a male offspring is produced, the gene will consist of an X and a Y chromosome. If two X chromosomes are expressed and produced, it produces a female offspring.\n\nCloning is the production of an offspring which represents the identical genes as its parent. Reproductive cloning begins with the removal of the nucleus from an egg, which holds the genetic material. In order to clone an organ, a stem cell is to be produced and then utilized to clone that specific organ. A common misconception of cloning is that it produces an exact copy of the parent being cloned. Cloning copies the DNA/genes of the parent and then creates a genetic duplicate. The clone will not be a similar copy as he or she will grow up in different surroundings from the parent and may encounter different opportunities and experiences. Although mostly positive, cloning also faces some setbacks in terms of ethics and human health. Though cell division and DNA replication is a vital part of survival, there are many steps involved and mutations can occur with permanent change in an organism's and their offspring's DNA. Some mutations can be good as they result in random evolution periods in which may be good for the species, but most mutations are bad as they can change the genotypes of offspring, which can result in changes that harm the species.\n\n"}
{"id": "7383487", "url": "https://en.wikipedia.org/wiki?curid=7383487", "title": "Oospore", "text": "Oospore\n\nAn oospore is a thick-walled sexual spore that develops from a fertilized oosphere in some algae, fungi, and Oomycetes. They are believed to have evolved either through the fusion of two species or the chemically-induced stimulation of mycelia, leading to oospore formation.\n\nIn Oomycetes, oospores can also result from asexual reproduction, by apomixis. These are found in Fungi as the sexual spores; these help in the sexual reproduction of Fungi.\n"}
{"id": "27160057", "url": "https://en.wikipedia.org/wiki?curid=27160057", "title": "Phinehas Abraham", "text": "Phinehas Abraham\n\nPhinehas Abraham was a West Indian merchant; born in the island of Jamaica about the beginning of the nineteenth century; and died 19 Feb. 1887. He was one of the last survivors of the body of West Indian merchants who contributed in a high degree to the prosperity of the West Indian colonial possessions. In former years Abraham was one of the largest landed proprietors in the island of Jamaica. He held various offices outside the Jewish community. He was senior justice of the peace for the parish of Trelawny in Jamaica, an agent of Lloyd's of London and the last surviving captain in the Trelawny militia. He was also one of the earliest members of the Berkeley Street Synagogue, London.\n"}
{"id": "41976229", "url": "https://en.wikipedia.org/wiki?curid=41976229", "title": "Pierre-Yves Oudeyer", "text": "Pierre-Yves Oudeyer\n\nDr. Pierre-Yves Oudeyer is Research Director at the French Institute for Research in Computer Science and Automation (Inria) and head of the Inria and Ensta-ParisTech FLOWERS team. Before, he has been a permanent researcher in Sony Computer Science Laboratory for 8 years (1999-2007). He studied theoretical computer science at Ecole Normale Supérieure in Lyon, and received his Ph.D. degree in artificial intelligence from the University Paris VI, France. After working on computational models of language evolution, he is now working on developmental and social robotics, focusing on sensorimotor development, language acquisition and lifelong learning in robots. Strongly inspired by infant development, the mechanisms he studies include artificial curiosity, intrinsic motivation, the role of morphology in learning motor control, human-robot interfaces, joint attention and joint intentional understanding, and imitation learning. He has published a book, more than 80 papers in international journals and conferences, holds 8 patents, gave several invited keynote lectures in international conferences, and received several prizes for his work in developmental robotics and on the origins of language. In particular, he is laureate of the ERC Starting Grant EXPLORERS. He is editor of the IEEE CIS Newsletter on Autonomous Mental Development, and associate editor of IEEE Transactions on Autonomous Mental Development, Frontiers in Neurorobotics, and of the International Journal of Social Robotics. He is also working actively for the diffusion of science towards the general public, through the writing of popular science articles and participation to radio and TV programs as well as science exhibitions.\n\nOudeyer is inventor or co-inventor of over 15 patents covering 5 different technological issues. He received a number of awards for his thesis and certain publications.\n\n"}
{"id": "13018947", "url": "https://en.wikipedia.org/wiki?curid=13018947", "title": "Presence (telepresence)", "text": "Presence (telepresence)\n\nPresence is a theoretical concept describing the extent to which media represent the world (in both physical and social environments). Presence is further described by Matthew Lombard and Theresa Ditton as “an illusion that a mediated experience is not mediated.\" Today, it often considers the effect that people experience when they interact with a computer-mediated or computer-generated environment. The conceptualization of presence borrows from multiple fields including communication, computer science, psychology, science, engineering, philosophy, and the arts. The concept of presence accounts for a variety of computer applications and Web-based entertainment today that are developed on the fundamentals of the phenomenon, in order to give people the sense of, as Sheridan called it, “being there.\" \n\nThe specialist use of the word “presence” derives from the term “telepresence”, coined by Massachusetts Institute of Technology professor Marvin Minsky in 1980. Minsky's research explained telepresence as the manipulation of objects in the real world through remote access technology. For example, a surgeon may use a computer to control robotic arms to perform minute procedures on a patient in another room. Or a NASA technician may use a computer to control a rover to collect rock samples on Mars. In either case, the operator is granted access to real, though remote, places via televisual tools.\n\nAs technologies progressed, the need for an expanded term arose. Sheridan extrapolated Minsky’s original definition. Using the shorter “presence,” Sheridan explained that the term refers to the effect felt when controlling real world objects remotely as well as the effect people feel when they interact with and immerse themselves in virtual reality or virtual environments.\n\nLombard and Ditton went a step further and enumerated six conceptualizations of presence:\n\n\nLombard's work discusses the extent to which 'presence' is felt, and how strong the perception of presence is regarded without the media involved. The article reviews the contextual characteristics that contribute to an individual's feeling presence. The most important variables that are important in the determinants of presence are those that involve sensory richness or vividness - and the number and consistency of sensory outputs. Researchers believe that the greater the number of human senses for which a medium provides stimulation, the greater the capability of the medium to produce a sense of presence. Additional important aspects of a medium are visual display characteristics (image quality, image size, viewing distance, motion and color, dimensionality, camera techniques) as well as aural presentation characteristics, stimuli for other senses (interactivity, obtrusiveness of medium, live versus recorded or constructed experience, number of people), content variables (social realism, use of media conventions, nature of task or activity), and media user variables (willingness to suspend disbelief, knowledge of and prior experience with the medium). Lombard also discusses the effects of presence, including both physiological and psychological consequences of \"the perceptual illusion of nonmediation.\" \nPhysiological effects of presence may include arousal, or vection and simulation sickness, while psychological effects may include enjoyment, involvement, task performance, skills training, desensitization, persuasion, memory and social judgement, or parasocial interaction and relationships.\n\nPresence has been delineated into subtypes, such as physical-, social-, and self-presence. Lombard's working definition was \"a psychological state in which virtual objects are experienced as actual objects in either sensory or nonsensory ways.\" Later extensions expanded the definition of \"virtual objects\" to specify that they may be either para-authentic or artificial. Further development of the concept of \"psychological state\" has led to study of the mental mechanism that permits humans to feel presence when using media or simulation technologies. One approach is to conceptualize presence as a cognitive feeling, that is, to take spatial presence as feedback from unconscious cognitive processes that inform conscious thought.\n\nSeveral studies provide insight into the concept of media influencing behavior.\n\n\n\n\n"}
{"id": "2310753", "url": "https://en.wikipedia.org/wiki?curid=2310753", "title": "Radial basis function", "text": "Radial basis function\n\nA radial basis function (RBF) is a real-valued function formula_1 whose value depends only on the distance from the origin, so that formula_2; or alternatively on the distance from some other point formula_3, called a \"center\", so that formula_4. Any function formula_1 that satisfies the property formula_2 is a radial function. The norm is usually Euclidean distance, although other distance functions are also possible.\n\nSums of radial basis functions are typically used to approximate given functions. This approximation process can also be interpreted as a simple kind of neural network; this was the context in which they originally surfaced, in work by David Broomhead and David Lowe in 1988, which stemmed from Michael J. D. Powell's seminal research from 1977.\nRBFs are also used as a kernel in support vector classification. The RBF implementation is ready enough to have the RBF exploited in different engineering applications.\n\nCommonly used types of radial basis functions include (writing formula_7 and using formula_8 to indicate the inverse of a \"critical radius\"):\n\n\n\n\n\n\nRadial basis functions are typically used to build up function approximations of the formformula_15\n\nwhere the approximating function formula_16 is represented as a sum of formula_17 radial basis functions, each associated with a different center formula_18, and weighted by an appropriate coefficient \"formula_19\". The weights \"formula_19\" can be estimated using the matrix methods of linear least squares, because the approximating function is \"linear\" in the weights \"formula_19\".\n\nApproximation schemes of this kind have been particularly used in time series prediction and control of nonlinear systems exhibiting sufficiently simple chaotic behaviour, 3D reconstruction in computer graphics (for example, hierarchical RBF and Pose Space Deformation).\n\nThe sumformula_15can also be interpreted as a rather simple single-layer type of artificial neural network called a radial basis function network, with the radial basis functions taking on the role of the activation functions of the network. It can be shown that any continuous function on a compact interval can in principle be interpolated with arbitrary accuracy by a sum of this form, if a sufficiently large number \"formula_23\" of radial basis functions is used. \n\nThe approximant formula_16 is differentiable with respect to the weights \"formula_19\". The weights could thus be learned using any of the standard iterative methods for neural networks.\n\nUsing radial basis functions in this manner yields a reasonable interpolation approach provided that the fitting set has been chosen such that it covers the entire range systematically (equidistant data points are ideal). However, without a polynomial term that is orthogonal to the radial basis functions, estimates outside the fitting set tend to perform poorly.\n\n\n"}
{"id": "28236076", "url": "https://en.wikipedia.org/wiki?curid=28236076", "title": "Sanctum (film)", "text": "Sanctum (film)\n\nSanctum is a 2011 Australian-American 3D disaster survival film directed by Alister Grierson and written by John Garvin and Andrew Wight. It stars Richard Roxburgh, Rhys Wakefield, Alice Parkinson, Dan Wyllie, and Ioan Gruffudd. Wight also produced the film, with James Cameron (writer/director of \"Avatar\" and \"Titanic\") as executive producer. The film was released in the United States on 4 February 2011 by Universal Pictures to predominantly negative reviews from critics but it was a box office success, earning $108.6 million on a $30 million budget. It also received an AACTA Award nomination for Best Visual Effects. Universal Studios Home Entertainment released \"Sanctum\" on DVD, Blu-ray Disc, and Blu-ray 3D on 7 June 2011.\n\nSeventeen-year-old Joshua \"Josh\" McGuire (Rhys Wakefield), expedition bank-roller Carl Hurley (Ioan Gruffudd) and his girlfriend, Victoria \"Vic\" Elaine (Alice Parkinson), travel to the Esa'ala Cave, an underwater cave exploration site in Papua New Guinea. Josh's father, Frank (Richard Roxburgh), a master diver, has already established a forward base camp at a lower level inside the cave, where the team has been exploring for weeks. As Josh voices his disdain for his father and his opinions about cave exploration, the team below prepares to dive into an unexplored area of the system.\n\nWhile exploring the entrance to the new system, Judes (Allison Cratchley) experiences a problem with her air tank hose. She loses use of her air mask forcing Frank to buddy breathe. After a few exchanges, Judes panics and tries to keep the mask on, but Frank forces the mask off of her knowing he will not have enough air otherwise to make it back to the team. As Judes drowns, Josh watches on a monitor at \"forward camp\" and presumes the worst of his father. In a struggle to determine who truly was responsible for her death, it is revealed by Frank that Judes had dived in an exhausted state since they previously had to retrieve the extra bailout tanks, a task Josh didn't do. Meanwhile, their above-ground crew realizes that a very big storm is preparing to hit their location sooner than anticipated. They attempt to warn the team below, but are unsuccessful. Josh expresses his desire to return to the surface and with communications down and an uncertain expedition in front of them, team leader Frank agrees to have his son return to the surface with a buddy climber ahead of them.\n\nWhile Josh climbs back towards the surface with Luko, Liz, and J.D, water begins rushing in from their exit. The storm they were trying to avoid had turned into a cyclone causing flash floods that begin to fill the cave. As J.D. and Liz make their way up through \"the elevator\" (an area leading up to the main entrance of the cave), Josh is unable to leave his father and the dive team behind to their doom and turns back, accompanied by Luko. As they are making their way back to \"forward camp\" they discover that Frank and the team have already evacuated their camp and are assisting Victoria as she climbs her way up and out of the cavern. Josh leaps in to help, strapping a rope around a nearby boulder and forming a belay for the team to escape. Unfortunately, the boulder begins to give way leaving Josh to hold Victoria's line and Luko to hold back the boulder. The water rushes through and forces Victoria and Josh to fall back down into the now flooded base camp. Luko is severely injured when the boulder breaks loose, sealing the shaft and throwing him back down into the cavern. He is swept into an underwater tunnel before the team can reach him. \n\nThe team decides to use the unexplored tunnel as an escape route from the rapidly flooding cavern. Victoria, panicked and anxious, refuses to wear Jude's old wetsuit and is given a quick tutorial on how to dive. Before the team can leave, a severely injured and mutilated Luko surfaces in the cavern. Seeing that his friend is in great pain and near death, Frank mercifully drowns him.\nThe team presses on and makes it through to the other side of the system. Without a wetsuit, Victoria suffers severely from the cold water. Meanwhile, George - an experienced, veteran diver - has become ill due to the dive and is dying unbeknownst to the rest of the team. After a short rest, the team continues through the system following the flow of water out towards the sea. George realizes that he cannot continue and after sharing a few words with Josh relieves himself of his pack and hides himself so as not to burden the rest of the team.\n\nAs the team marches on they arrive at a seemingly dead end. A great hole in the bottom of the cave separates them from the other side of their path. Josh uses his climbing expertise to fish a line across and begins to transfer their gear and each other. As Victoria begins to make her way across she catches her hair in her rope gear she is hung on and loses her grip, leaving her hair as the only thing holding her weight. Using her knife, she attempts to cut away the hair, but severs her rope and falls to her death. At the sight of Victoria's death, Carl becomes psychotic. In a fit of panic, he steals the last remaining rebreather and disappears into the tunnel. Josh and Frank, however, find another way out through a crevasse in the cavern. The tunnel leads them to a sunlit cavern where an unidentified WWII Japanese tank collapsed through the surface years ago. Unfortunately the hole in the middle of the roof where the tank fell is the only opening and they are unable to climb out. They spend the rest of the day there and proceed back into the cave by night.\n\nA little later they discover Carl who has gone completely insane and found Victoria's corpse. Carl attempts to murder Frank but Josh separates the two and temporarily disables Carl. Frank, however, has been gravely injured having fallen on a stalagmite which punctured his back. When Carl wakes, the gravity of his actions is realized and he solemnly disappears into the tunnel. In immense pain, Frank requests that Josh drown him. Josh reluctantly does so, and swims into the tunnel. \nA short distance in, he encounters Carl, who has already drowned looking for the exit. \nJust as he begins to lose hope, Josh discovers a way out through the cave to open ocean. He emerges from the water and crawls onto the beach, where he is discovered by fishermen.\n\n\n\"Sanctum\" was inspired by the film's co-writer Andrew Wight's experience with a 1988 cave diving expedition in Australia that resulted in 13 cavers becoming trapped in one of the world's largest underwater cave systems in Nullarbor Plain after a freak storm collapsed the entrance. That incident was documented in the film \"Nullarbor Dreaming\". James Cameron served as executive producer for \"Sanctum\". Even though the film's plot takes place in Esa'ala Cave, most of the film was shot in Australia (Gold Coast, Queensland). Sanctum employs 3D photography techniques Cameron developed to film \"Avatar\". All of the underwater sequences took place in a large water tank at the Village Roadshow Studios in Queensland. Real caves were also filmed in South Australia's cave-diving region around Mount Gambier. Stunt diver Agnes Milowka, who appears as a double in the already-released film, drowned in one of these caves on 27 February 2011 when she reportedly ran out of air. In striking similarities to the movie script, she also left her spare tank behind, to force the way through the tight restriction, and it is actually her playing Judes' drowning scene. Universal Studios and Relativity Media paid $12 million for rights to distribute the film in the United States and Canada, and in several foreign countries.\n\n\n\"Sanctum\" opened with $9.2 million in its first weekend, coming in second behind \"The Roommate.\" CinemaScore polls indicated a C+ rating from audiences.\nAs of March 2011, \"Sanctum\" was the tenth-highest-grossing Australian film at the international box office. The movie made $3,838,154 at the Australian box office.\n\nThe film received predominantly negative reviews in the US. Rotten Tomatoes reports that 30% of 161 critics rated the film positively with an average score of 4.5/10. The consensus states that \"\"Sanctum\" is beautifully photographed, and it makes better use of 3-D technology than most, but that doesn't make up for its ham-handed script and lifeless cast.\" It has a Metacritic rating of 42 out of 100. However, in the rest of the world, where the major part of the gross take occurred, some reviews were slightly better. In Australia, Jim Schembri gave it 3 1/2 stars from 5 whilst the UK's \"Daily Express\" gave it 3 from 5.\n\nUniversal Studios Home Entertainment released \"Sanctum\" on DVD, Blu-ray Disc, and Blu-ray 3D on 7 June 2011.\n\n\n"}
{"id": "33327117", "url": "https://en.wikipedia.org/wiki?curid=33327117", "title": "Super Bonk", "text": "Super Bonk\n\nSuper Bonk (released in Europe as Super B.C. Kid and in Japan as Chō Genjin) is a 1994 2D platform video game by Hudson Soft for the Super Nintendo Entertainment System. It is the fourth game in the \"Bonk\" series. The game was later re-released for the Wii Virtual Console in Japan and Europe in 2010 and in North America in 2011.\n\nIn their review, \"GamePro\" praised the clever usage of Bonk's various forms, the \"crisp, cutesy quality\" of the graphics, the easy controls, and the numerous bonus rounds, but nonetheless gave \"Super Bonk\" an overall negative assessment, concluding that platformer fans in general and Bonk fans in particular would find very little new about the game.\n\n"}
{"id": "6671528", "url": "https://en.wikipedia.org/wiki?curid=6671528", "title": "Ted Robledo", "text": "Ted Robledo\n\nEduardo Oliver \"Ted\" Robledo (26 July 1928 – 6 December 1970) was a Chilean professional football player. He played as a left-sided defender, and is most notable for his time spent with Newcastle United.\n\nRobledo was born in Iquique, Chile to a Chilean father and an English mother. He emigrated with his family to Wath-on-Dearne, Yorkshire in 1932, at the age of four, due to the political instability in Chile at the time. The family lived at Barnsley Rd, West Melton, in the same house where the Anglo-French biographer David Bret was later raised. \n\nRobledo started his footballing career at Barnsley with his brother George. First Division Newcastle United signed him on 27 January 1949. Newcastle were only interested in signing his brother, but neither of the Robledo brothers would move without the other. Their appearance together in the 1952 FA Cup Final was the first time more than one foreign player had appeared in a cup final eleven.\n\nThe majority of Robledo's appearances for the club came in the 1951–52 season. Robledo played for Newcastle until the end of the 1952–53 season, when he was sold to Colo-Colo. After retiring from football, Robledo served on an oil tanker where he died in mysterious circumstances in December 1970, at the age of 42. It was rumoured that Robledo was thrown off the tanker and drowned. His body has never been found. His brother George outlived him by nearly two decades, dying in April 1989 just before his 63rd birthday.\n\nNewcastle United\n\n"}
{"id": "2310993", "url": "https://en.wikipedia.org/wiki?curid=2310993", "title": "The Masque of Anarchy", "text": "The Masque of Anarchy\n\n\"The Masque of Anarchy\" (or \"The Mask of Anarchy\") is a British political poem written in 1819 (see 1819 in poetry) by Percy Bysshe Shelley following the Peterloo massacre of that year. In his call for freedom, it is perhaps the first modern statement of the principle of nonviolent resistance.\n\nThe poem was not published during Shelley's lifetime and did not appear in print until 1832 (see 1832 in poetry), when published by Edward Moxon in London with a preface by Leigh Hunt. Shelley had sent the manuscript in 1819 for publication in \"The Examiner\". Leigh Hunt withheld it from publication because he \"thought that the public at large had not become sufficiently discerning to do justice to the sincerity and kind-heartedness of the spirit that walked in this flaming robe of verse.\" The epigraph on the cover of the first edition is from \"The Revolt of Islam\" (1818): \"Hope is strong; Justice and Truth their winged child have found.\"\n\nUse of \"masque\" and \"mask\" is discussed by Morton Paley; Shelley used \"mask\" in the manuscript but the first edition uses \"masque\" in the title.\n\nShelley begins his poem, written on the occasion of the Peterloo massacre, Manchester 1819, with the powerful images of the unjust forms of authority of his time, \"God, and King, and Law\" – and then imagines the stirrings of a radically new form of social action: \"Let a great assembly be, of the fearless, of the free\". The crowd at this gathering is met by armed soldiers, but the protesters do not raise an arm against their assailants:\n\nShelley elaborates on the psychological consequences of violence met with pacifism. The guilty soldiers he says, will return shamefully to society, where \"blood thus shed will speak / In hot blushes on their cheek\". Women will point out the murderers on the streets, their former friends will shun them, and honourable soldiers will turn away from those responsible for the massacre, \"ashamed of such base company\". A version was taken up by Henry David Thoreau in his essay \"Civil Disobedience\", and later by Mohandas Karamchand Gandhi in his doctrine of \"Satyagraha\". Gandhi's passive resistance was influenced and inspired by Shelley's nonviolence in protest and political action. It is known that Gandhi would often quote Shelley's \"The Masque of Anarchy\" to vast audiences during the campaign for a free India.\n\nThe poem mentions several members of Lord Liverpool's government by name: the Foreign Secretary, Castlereagh, who appears as a mask worn by Murder, the Home Secretary, Lord Sidmouth, whose guise is taken by Hypocrisy, and the Lord Chancellor, Lord Eldon, whose ermine gown is worn by Fraud. Led by Anarchy, a skeleton with a crown, they try to take over England, but are slain by a mysterious armoured figure who arises from a mist. The maiden Hope, revived, then calls to the people of England:\n\nPolitical authors and campaigners such as Richard Holmes and Paul Foot, among others, describe it as \"the greatest political poem ever written in English\". In his book \"An Encyclopedia of Pacifism\", Aldous Huxley noted the poem's exhortation to the English to resist assault without fighting back, stating \"The Method of resistance inculcated in by Shelley in \"The Mask of Anarchy\" (sic) is the method of non-violence\".\n\nAuthor, educator, and activist Howard Zinn refers to the poem in \"A People's History of the United States\". In a subsequent interview, he underscored the power of the poem, suggesting: \"What a remarkable affirmation of the power of people who seem to have no power. Ye are many, they are few. It has always seemed to me that poetry, music, literature, contribute very special power.\" In particular, Zinn uses \"The Mask of Anarchy\" as an example of literature that members of the American labour movement would read to other workers to inform and educate them.\n\nThe rallying language of the poem had led to elements of it being used by political movements. It was recited by students at the Tiananmen Square protests of 1989 and by protesters in Tahrir Square during the Egyptian revolution of 2011. The phrase \"like lions after slumber, in unvanquishable number\" from the poem is used as a motto/slogan by the International Socialist Organization in their organ. The line \"Ye are many-they are few\" inspired the campaign slogan \"We are many, they are few\" used by protesters during the Poll tax riots of 1989–90 in the United Kingdom, and also inspired the title of the 2014 documentary film \"We Are Many\", which focused on the global 15 February 2003 anti-war protests.\n\nJeremy Corbyn, the leader of the British Labour Party, quoted the final stanza from the poem at his rally in Islington, on the final day of campaigning for the 2017 UK general election. Corbyn subsequently quoted the final stanza again during his speech at the Pyramid stage at the 2017 Glastonbury Festival. Academic and writer John Sutherland has suggested that the title of the party's 2017 manifesto, \"For the Many, Not the Few\", was derived from the poem. The phrase 'a community in which power, wealth and opportunity are in the hands of the many, not the few' also appears in the revised version of Clause IV of the Labour Party Constitution.\n\nThe same variation, \"For The Many, Not The Few\", was the sub-title to Robert Reich's 2016 book, \"Saving Capitalism\".\n\nLines from the poem are quoted on the sleeve of The Jam's 1980 album \"Sound Affects\".\n\nLines from the poem inspired and are featured in the John Vanderslice song \"Pale Horse\".\n\nThe poem's lines \"like lions after slumber, in unvanquishable number\" are quoted by the title and closing lines of the Scritti Politti song \"Lions After Slumber\". They are also referenced in the opening lines of the song \"Blaze\" by Strike Anywhere.\n\nThe chorus of the song \"Robin Hood\" by The Mekons contains the lines \"Rise like lions, shake your chains, babe. Ye are many, they are few\".\n\nIn the episode of \"Heartbeat\" titled \"The Heart of a Man\", the character played by Jeff Hordley, Stan Keaton, quotes the last verse in its entirety whilst standing outside the Aidensfield Arms with his fellow sacked workers.\n\n\n"}
{"id": "5442150", "url": "https://en.wikipedia.org/wiki?curid=5442150", "title": "The Twa Sisters", "text": "The Twa Sisters\n\n\"The Two Sisters\" is a Northumbrian murder ballad that recounts the tale of a girl drowned by her sister. It is first known to have appeared on a broadside in 1656 as \"The Miller and the King's Daughter.\" At least 21 English variants exist under several names, including \"Minnorie\" or \"Binnorie\", \"The Cruel Sister\", \"The Wind and Rain\", \"Dreadful Wind and Rain\", \"Two Sisters\", \"The Bonny Swans\" and the \"Bonnie Bows of London\". The ballad was collected by Francis J. Child (Child 10) and is also listed in the Roud Folk Song Index.\n\nTwo sisters go down by a body of water, sometimes a river and sometimes the sea. The older one pushes the younger in and refuses to pull her out again; generally the lyrics explicitly state her intent to drown her younger sister. Her motive, when included in the lyrics, is sexual jealousy – in some variants, the sisters are being two-timed by a suitor; in others, the elder sister's affections are not encouraged by the young man. In a few versions, a third sister is mentioned, but plays no significant role in events. In most versions, the older sister is described as dark, while the younger sister is fair.\n\nWhen the murdered girl's body floats ashore, someone makes a musical instrument out of it, generally a harp or a fiddle, with a frame of bone and the girl's \"long yellow hair\" (or \"golden hair\") for strings. The instrument then plays itself and sings about the murder. In some versions, this occurs after the musician has taken it to the family's household, so that the elder sister is publicly revealed (sometimes at her wedding to the murdered girl's suitor) as the murderess.\n\nIt should be noted that the variant titled \"The Two Sisters\" typically omits the haunted instrument entirely, ending instead with an unrelated person (often a miller) executed for robbing the murdered girl's corpse and the elder sister sometimes going unpunished, or sometimes boiled in lead.\n\nThe theme of this ballad was common in many northern European languages. There are 125 different variants known in Swedish alone. Its general Scandinavian classification is TSB A 38; and it is (among others) known as \"Den talende strængelek\" or \"De to søstre\" (DgF 95) in Danish, \"Hørpu ríma\" (CCF 136) in Faroese, \"Hörpu kvæði\" (IFkv 13) in Icelandic, \"Dei tvo systar\" in Norwegian, and \"De två systrarna\" (SMB 13) in Swedish. It has also spread further south; for example, as \"Gosli iz človeškega telesa izdajo umor\" (A Fiddle Made from a Human Body Reveals a Murder) in Slovenian.\n\nIn the Norse variants, the older sister is depicted as dark and the younger as fair, often with great contrast, comparing the one to soot or the other to the sun or milk. This can inspire taunts from the younger about the older's looks.\n\nIn most of the Norwegian and some of the Swedish variants, the story ends by the instrument being broken and the younger sister coming alive again. In a few, she was not actually drowned, but saved and nursed back to health; she tells the story herself.\n\nThis tale is also found in prose form, in fairy tales such as \"The Singing Bone\", where the siblings are brothers instead of sisters. This is widespread throughout Europe; often the motive is not jealousy because of a lover, but the younger child's success in winning the object that will cure the king, or that will win the father's inheritance.\n\nIn Polish literature from the romanticism period, a similar theme is found in \"Balladyna\" (1838) by Juliusz Słowacki. Two sisters engage in a raspberry-gathering contest to decide which of them gets to marry Prince Kirkor. When the younger Alina wins, the older Balladyna kills her. Finally, she is killed by a bolt of lightning in an act of divine retribution.\n\nA Hungarian version exists, where a king has three daughters. The older two are bad and ugly and envy the younger child sister because of her beauty. One day, they murder her in the forest and place her corpse inside a fiddle. The fiddle plays music on its own and eventually is given to the royal family. The fiddle does not play for the evil sisters, but the princess is restored to life once her father tries to play it. The sisters are imprisoned, but the good princess pardons them once she becomes queen.\n\nThe ballad also appears in a number of guises in Scottish Gaelic, under the name 'A' Bhean Eudach' or 'The Jealous Woman'. In many of the Scottish Gaelic variants the cruel sister murders her sibling while she is sleeping by knotting her hair into the seaweed on a rock at low tide. When she wakes the tide is coming in fast and as she is drowning she sings the song 'A' Bhean Eudach' detailing her tragic end.\n\nAs is frequently found with traditional folksongs, versions of The Twa Sisters are associated with tunes that are used in common with several other ballads. For example, at least one variant of this ballad (\"Cruel Sister\") uses the tune and refrain from \"Lay the bent to the bonny broom\", a widely used song (whose original lyrics are lost) which is also used, for example, by some versions of \"Riddles Wisely Expounded\" (Child 1).\n\nCanadian singer and harpist Loreena McKennitt's song \"The Bonny Swans\" is a pastiche of several traditional variants of the ballad. The first stanza mentions the third sister, but she subsequently disappears from the narrative. The song recounts a tale in which a young woman is drowned by her jealous older sister in an effort to gain the younger sister's beloved. The girl's body washes up near a mill, where the miller's daughter mistakes her corpse for that of a swan. Later, after she is pulled from the water, a passing harper fashions a harp from the bones and hair of the dead girl; the harp plays alone, powered by the girl's soul. The harp is brought to her father's hall and plays before the entire court, telling of her sister's crime. The song also mentions her brother named Hugh, and her beloved William, and gives a name to the older sister, Anne.\n\nAn early Alfred Lord Tennyson poem, \"The Sisters\", also bears a resemblance to the ballad: a sister scorned in love who murders the lover of her sister, and possibly the sister too, out of jealousy.\n\n\n\n"}
{"id": "52240704", "url": "https://en.wikipedia.org/wiki?curid=52240704", "title": "Transboundary breed", "text": "Transboundary breed\n\nA transboundary breed is a breed which is present in several countries. Transboundary species of the five significant livestock types (cattle, sheep, goats, pigs and chickens), have been developed for a hundred years or more in intensive manufacturing systems, which has led to global availability. A relatively small number of worldwide transboundary breeds compose the ever-increasing share of total global animal products. However, only in North America and the Southwest Pacific do the number of transboundary breeds surpass that of local breeds. \n\nThere can be both regional and international types of transboundary breeds. Regional breeds are breeds that are reported to only be found in one \"region\", which may include several countries, and an international transboundary breed is one that is reported to be found in multiple regions. For example, the Holstein Fresian cattle is an international transboundary breed, because it is found in several different continents and regions.\n"}
{"id": "1851887", "url": "https://en.wikipedia.org/wiki?curid=1851887", "title": "Weasel program", "text": "Weasel program\n\nThe weasel program or Dawkins' weasel is a thought experiment and a variety of computer simulations illustrating it. Their aim is to demonstrate that the process that drives evolutionary systems—random variation combined with non-random cumulative selection—is different from pure chance.\n\nThe thought experiment was formulated by Richard Dawkins, and the first simulation written by him; various other implementations of the program have been written by others.\n\nIn chapter 3 of his book \"The Blind Watchmaker\", Dawkins gave the following introduction to the program, referencing the well-known infinite monkey theorem:\n\nThe scenario is staged to produce a string of gibberish letters, assuming that the selection of each letter in a sequence of 28 characters will be random. The number of possible combinations in this random sequence is 27, or about 10, so the probability that the monkey will produce a given sequence is extremely low. Any particular sequence of 28 characters could be selected as a \"target\" phrase, all equally as improbable as Dawkins's chosen target, \"METHINKS IT IS LIKE A WEASEL\".\n\nA computer program could be written to carry out the actions of Dawkins's hypothetical monkey, continuously generating combinations of 26 letters and spaces at high speed. Even at the rate of millions of combinations per second, it is unlikely, even given the entire lifetime of the universe to run, that the program would ever produce the phrase \"METHINKS IT IS LIKE A WEASEL\".\n\nDawkins intends this example to illustrate a common misunderstanding of evolutionary change, i.e. that DNA sequences or organic compounds such as proteins are the result of atoms randomly combining to form more complex structures. In these types of computations, any sequence of amino acids in a protein will be extraordinarily improbable (this is known as Hoyle's fallacy). Rather, evolution proceeds by hill climbing, as in adaptive landscapes.\n\nDawkins then goes on to show that a process of \"cumulative\" selection can take far fewer steps to reach any given target. In Dawkins's words:\n\nBy repeating the procedure, a randomly generated sequence of 28 letters and spaces will be gradually changed each generation. The sequences progress through each generation:\n\nDawkins continues:\n\nThe program aims to demonstrate that the preservation of small changes in an evolving string of characters (or genes) can produce meaningful combinations in a relatively short time as long as there is some mechanism to select cumulative changes, whether it is a person identifying which traits are desirable (in the case of artificial selection) or a criterion of survival (\"fitness\") imposed by the environment (in the case of natural selection). Reproducing systems tend to preserve traits across generations, because the offspring inherit a copy of the parent's traits. It is the differences between offspring, the variations in copying, which become the basis for selection, allowing phrases closer to the target to survive, and the remaining variants to \"die.\"\n\nDawkins discusses the issue of the mechanism of selection with respect to his \"biomorphs\" program:\n\nRegarding the example's applicability to biological evolution, he is careful to point out that it has its limitations:\n\nIn \"The Blind Watchmaker,\" Dawkins goes on to provide a graphical model of gene selection involving entities he calls biomorphs. These are two-dimensional sets of line segments which bear relationships to each other, drawn under the control of \"genes\" that determine the appearance of the biomorph. By selecting entities from sequential generations of biomorphs, an experimenter can guide the evolution of the figures toward given shapes, such as \"airplane\" or \"octopus\" biomorphs.\n\nAs a simulation, the biomorphs are not much closer to the actual genetic behavior of biological organisms. Like the Weasel program, their development is shaped by an external factor, in this case the decisions of the experimenter who chooses which of many possible shapes will go forward into the following generation. They do however serve to illustrate the concept of \"genetic space,\" where each possible gene is treated as a dimension, and the actual genomes of living organisms make up a tiny fraction of all possible gene combinations, most of which will not produce a viable organism. As Dawkins puts it, \"however many ways there may be of being alive, it is certain that there are vastly more ways of being dead\". \n\nIn \"Climbing Mount Improbable\", Dawkins responded to the limitations of the Weasel program by describing programs, written by other parties, that modeled the evolution of the spider web. He suggested that these programs were more realistic models of the evolutionary process, since they had no predetermined goal other than coming up with a web that caught more flies through a \"trial and error\" process. Spiderwebs were seen as good topics for evolutionary modeling because they were simple examples of biosystems that were easily visualized; the modeling programs successfully generated a range of spider webs similar to those found in nature.\n\nAlthough Dawkins did not provide the source code for his program, a \"Weasel\" style algorithm could run as follows.\n\n\nFor these purposes, a \"character\" is any uppercase letter, or a space. The number of copies per generation, and the chance of mutation per letter are not specified in Dawkins's book; 100 copies and a 5% mutation rate are examples. Correct letters are not \"locked\". Each correct letter may become incorrect in subsequent generations. The terms of the program and the existence of the target phrase do however mean that such 'negative mutations' will quickly be 'corrected'.\n\n\n"}
