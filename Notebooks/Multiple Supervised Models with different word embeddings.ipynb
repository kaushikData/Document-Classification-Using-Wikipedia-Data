{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T06:25:44.409405Z",
     "start_time": "2019-09-09T06:25:42.623170Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import gensim\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "%matplotlib inline\n",
    "import time\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T06:25:54.035655Z",
     "start_time": "2019-09-09T06:25:46.161479Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('base_data_preprocessed.json') as obj:\n",
    "    df = json.load(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T06:25:54.594838Z",
     "start_time": "2019-09-09T06:25:54.037241Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text     category  category_id\n",
      "0  !Wowow!\\n\\n!Wowow! is a collective in Peckham,...         Arts            0\n",
      "1                         1376 in literature\\n\\n\\n\\n         Arts            0\n",
      "2  C. F. Møller Architects\\n\\nArkitektfirmaet C. ...         Arts            0\n",
      "3  Anne Beatts\\n\\nAnne Beatts (born February 25, ...         Arts            0\n",
      "4  Norton &amp; Wallis\\n\\nNorton & Wallis was an ...         Arts            0\n",
      "5  All the Young Men\\n\\nAll the Young Men is a 19...         Arts            0\n",
      "6  Global cascades model\\n\\nGlobal cascades model...  Mathematics            9\n",
      "7  Graded-symmetric algebra\\n\\nIn algebra, given ...  Mathematics            9\n",
      "8  Hans Eberstark\\n\\nHans Eberstark (27 January 1...  Mathematics            9\n",
      "9  Hexacoordinate\\n\\nHexacoordinate in chemistry ...  Mathematics            9\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(df).reset_index()\n",
    "df.drop(['index'], axis = 1,inplace = True)\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T06:27:00.635155Z",
     "start_time": "2019-09-09T06:25:54.596908Z"
    }
   },
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    text = text.lower()\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) \n",
    "    text = BAD_SYMBOLS_RE.sub('', text) \n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) \n",
    "    return text\n",
    "    \n",
    "df['text'] = df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T06:27:07.570043Z",
     "start_time": "2019-09-09T06:27:00.636682Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132911725"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].apply(lambda x: len(x.split(' '))).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T06:27:07.638612Z",
     "start_time": "2019-09-09T06:27:07.572366Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df.text\n",
    "y = df.category\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T06:29:16.019094Z",
     "start_time": "2019-09-09T06:27:07.640166Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39 µs, sys: 0 ns, total: 39 µs\n",
      "Wall time: 4.05 µs\n",
      "accuracy 0.4152521046852123\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         Arts       0.32      0.38      0.35      3249\n",
      "     Business       0.48      0.47      0.47      3266\n",
      "     Concepts       0.22      0.32      0.26      3257\n",
      "      Culture       0.21      0.17      0.19      3263\n",
      "    Education       0.38      0.55      0.45      3271\n",
      "Entertainment       0.35      0.57      0.43      3281\n",
      "       Events       0.39      0.35      0.37      3255\n",
      "    Geography       0.63      0.39      0.48      3281\n",
      "       Health       0.54      0.51      0.52      3269\n",
      "      History       0.36      0.44      0.39      3346\n",
      "   Humanities       0.27      0.16      0.20      3135\n",
      "     Language       0.58      0.34      0.42      3235\n",
      "          Law       0.42      0.59      0.49      3280\n",
      "         Life       0.40      0.18      0.25      3124\n",
      "  Mathematics       0.70      0.76      0.72      3207\n",
      "       Nature       0.46      0.43      0.45      3272\n",
      "       People       0.28      0.32      0.30      3142\n",
      "   Philosophy       0.43      0.52      0.47      3257\n",
      "     Politics       0.25      0.55      0.34      3227\n",
      "    Reference       0.64      0.17      0.27      3005\n",
      "     Religion       0.45      0.58      0.51      3206\n",
      "      Science       0.57      0.17      0.26      3322\n",
      "      Society       0.50      0.27      0.36      3334\n",
      "       Sports       0.78      0.80      0.79      3294\n",
      "   Technology       0.34      0.52      0.42      3184\n",
      "     Universe       0.51      0.51      0.51      3205\n",
      "        World       0.80      0.16      0.27      3257\n",
      "\n",
      "     accuracy                           0.42     87424\n",
      "    macro avg       0.45      0.41      0.41     87424\n",
      " weighted avg       0.45      0.42      0.41     87424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "%time\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T06:35:16.092939Z",
     "start_time": "2019-09-09T06:32:16.057449Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16 µs, sys: 1e+03 ns, total: 17 µs\n",
      "Wall time: 3.81 µs\n",
      "accuracy 0.4548178989751098\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         Arts       0.34      0.26      0.30      3249\n",
      "     Business       0.49      0.58      0.53      3266\n",
      "     Concepts       0.25      0.12      0.16      3257\n",
      "      Culture       0.21      0.10      0.13      3263\n",
      "    Education       0.47      0.57      0.52      3271\n",
      "Entertainment       0.43      0.44      0.44      3281\n",
      "       Events       0.40      0.42      0.41      3255\n",
      "    Geography       0.53      0.57      0.55      3281\n",
      "       Health       0.48      0.67      0.56      3269\n",
      "      History       0.46      0.48      0.47      3346\n",
      "   Humanities       0.26      0.13      0.17      3135\n",
      "     Language       0.48      0.51      0.49      3235\n",
      "          Law       0.45      0.59      0.51      3280\n",
      "         Life       0.40      0.30      0.35      3124\n",
      "  Mathematics       0.52      0.89      0.66      3207\n",
      "       Nature       0.46      0.45      0.46      3272\n",
      "       People       0.39      0.29      0.33      3142\n",
      "   Philosophy       0.46      0.57      0.51      3257\n",
      "     Politics       0.42      0.31      0.35      3227\n",
      "    Reference       0.45      0.44      0.45      3005\n",
      "     Religion       0.48      0.60      0.54      3206\n",
      "      Science       0.47      0.29      0.36      3322\n",
      "      Society       0.44      0.30      0.36      3334\n",
      "       Sports       0.54      0.93      0.68      3294\n",
      "   Technology       0.46      0.40      0.43      3184\n",
      "     Universe       0.48      0.59      0.53      3205\n",
      "        World       0.51      0.45      0.48      3257\n",
      "\n",
      "     accuracy                           0.45     87424\n",
      "    macro avg       0.43      0.45      0.43     87424\n",
      " weighted avg       0.44      0.45      0.43     87424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "               ])\n",
    "sgd.fit(X_train, y_train)\n",
    "\n",
    "%time\n",
    "\n",
    "y_pred = sgd.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T06:35:19.678791Z",
     "start_time": "2019-09-09T06:35:16.094706Z"
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "transform_feature_names not available for TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f21e9e308e89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0meli5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0meli5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msgd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/eli5/ipython.py\u001b[0m in \u001b[0;36mshow_weights\u001b[0;34m(estimator, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \"\"\"\n\u001b[1;32m    129\u001b[0m     \u001b[0mformat_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplain_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_split_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mexpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplain_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mexplain_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m     \u001b[0m_set_html_kwargs_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_as_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mformat_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    825\u001b[0m                             '1 positional argument')\n\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 827\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    828\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m     \u001b[0mfuncname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__name__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'singledispatch function'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/eli5/sklearn/explain_weights.py\u001b[0m in \u001b[0;36mexplain_weights_pipeline\u001b[0;34m(estimator, feature_names, **kwargs)\u001b[0m\n\u001b[1;32m    474\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'vec'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0mfeature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m     \u001b[0mfeature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m     out = explain_weights(last_estimator,\n\u001b[1;32m    478\u001b[0m                           \u001b[0mfeature_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    825\u001b[0m                             '1 positional argument')\n\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 827\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    828\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m     \u001b[0mfuncname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__name__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'singledispatch function'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/eli5/sklearn/transform.py\u001b[0m in \u001b[0;36m_pipeline_names\u001b[0;34m(est, in_names)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrans\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    825\u001b[0m                             '1 positional argument')\n\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 827\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    828\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m     \u001b[0mfuncname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__name__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'singledispatch function'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/eli5/transform.py\u001b[0m in \u001b[0;36mtransform_feature_names\u001b[0;34m(transformer, in_names)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     raise NotImplementedError('transform_feature_names not available for '\n\u001b[0;32m---> 34\u001b[0;31m                               '{}'.format(transformer))\n\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m: transform_feature_names not available for TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
     ]
    }
   ],
   "source": [
    "import eli5\n",
    "eli5.show_weights(sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-09T05:48:17.623Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaushik-shakkari/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/kaushik-shakkari/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "               ])\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "%time\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word to Vector with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "wv = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "wv.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        # FIXME: remove these examples in pre-processing\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, post) for post in text_list ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:cannot compute similarity with no input ['kobori', 'nanrei', 'sohaku']\n",
      "WARNING:root:cannot compute similarity with no input ['sexfriend']\n",
      "WARNING:root:cannot compute similarity with no input ['n2m']\n",
      "WARNING:root:cannot compute similarity with no input ['hitoshi', 'nagai']\n",
      "WARNING:root:cannot compute similarity with no input ['kovvatnet', 'finnmark']\n",
      "WARNING:root:cannot compute similarity with no input ['hiroyuki', 'nishimoto']\n",
      "WARNING:root:cannot compute similarity with no input ['ighost']\n",
      "WARNING:root:cannot compute similarity with no input ['katei']\n",
      "WARNING:root:cannot compute similarity with no input ['kisaku', 'mayekawa']\n",
      "WARNING:root:cannot compute similarity with no input ['1970s', 'andorra1970s', 'andorra']\n",
      "WARNING:root:cannot compute similarity with no input ['genichi', 'koidzumi']\n",
      "WARNING:root:cannot compute similarity with no input ['iydmma']\n",
      "WARNING:root:cannot compute similarity with no input ['kank']\n",
      "WARNING:root:cannot compute similarity with no input ['tsuneo', 'hasegawa']\n",
      "WARNING:root:cannot compute similarity with no input ['ibtl']\n",
      "WARNING:root:cannot compute similarity with no input ['mitsui', 'takatoshi']\n",
      "WARNING:root:cannot compute similarity with no input ['ryz', 'hiranuma']\n",
      "WARNING:root:cannot compute similarity with no input ['kyushu', 'asahi', 'broadcastingetc']\n",
      "WARNING:root:cannot compute similarity with no input ['genbun']\n",
      "WARNING:root:cannot compute similarity with no input ['afaicr']\n"
     ]
    }
   ],
   "source": [
    "warnings.simplefilter('ignore')\n",
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens\n",
    "    \n",
    "train, test = train_test_split(df, test_size=0.3, random_state = 42)\n",
    "\n",
    "test_tokenized = test.apply(lambda r: w2v_tokenize_text(r['text']), axis=1).values\n",
    "train_tokenized = train.apply(lambda r: w2v_tokenize_text(r['text']), axis=1).values\n",
    "\n",
    "X_train_word_average = word_averaging_list(wv,train_tokenized)\n",
    "X_test_word_average = word_averaging_list(wv,test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 11.2 µs\n",
      "accuracy 0.3900120336943442\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         Arts       0.31      0.30      0.30       300\n",
      "     Business       0.48      0.50      0.49       323\n",
      "     Concepts       0.27      0.21      0.24       316\n",
      "      Culture       0.20      0.12      0.15       319\n",
      "    Education       0.47      0.53      0.50       301\n",
      "Entertainment       0.34      0.46      0.39       324\n",
      "       Events       0.32      0.30      0.31       328\n",
      "    Geography       0.36      0.37      0.37       297\n",
      "       Health       0.49      0.54      0.52       300\n",
      "      History       0.37      0.39      0.38       285\n",
      "   Humanities       0.23      0.15      0.18       313\n",
      "     Language       0.44      0.40      0.42       332\n",
      "          Law       0.43      0.52      0.47       320\n",
      "         Life       0.30      0.24      0.27       294\n",
      "  Mathematics       0.61      0.77      0.68       297\n",
      "       Nature       0.36      0.39      0.38       291\n",
      "       People       0.27      0.32      0.29       317\n",
      "   Philosophy       0.41      0.46      0.43       316\n",
      "     Politics       0.32      0.34      0.33       324\n",
      "    Reference       0.31      0.28      0.29       275\n",
      "     Religion       0.41      0.47      0.44       307\n",
      "      Science       0.30      0.22      0.25       305\n",
      "      Society       0.35      0.30      0.32       288\n",
      "       Sports       0.70      0.84      0.76       305\n",
      "   Technology       0.35      0.39      0.37       319\n",
      "     Universe       0.45      0.46      0.46       295\n",
      "        World       0.38      0.29      0.33       319\n",
      "\n",
      "     accuracy                           0.39      8310\n",
      "    macro avg       0.38      0.39      0.38      8310\n",
      " weighted avg       0.38      0.39      0.38      8310\n",
      "\n"
     ]
    }
   ],
   "source": [
    "warnings.simplefilter('ignore')\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg = logreg.fit(X_train_word_average, train['category'])\n",
    "%time\n",
    "y_pred = logreg.predict(X_test_word_average)\n",
    "print('accuracy %s' % accuracy_score(y_pred, test.category))\n",
    "print(classification_report(test.category, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document to Vector with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import doc2vec\n",
    "from sklearn import utils\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import re\n",
    "\n",
    "def label_sentences(corpus, label_type):\n",
    "    \"\"\"\n",
    "    Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n",
    "    We do this by using the TaggedDocument method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n",
    "    a dummy index of the post.\n",
    "    \"\"\"\n",
    "    labeled = []\n",
    "    for i, v in enumerate(corpus):\n",
    "        label = label_type + '_' + str(i)\n",
    "        labeled.append(doc2vec.TaggedDocument(v.split(), [label]))\n",
    "    return labeled\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.category, df.category, random_state=0, test_size=0.3)\n",
    "X_train = label_sentences(X_train, 'Train')\n",
    "X_test = label_sentences(X_test, 'Test')\n",
    "all_data = X_train + X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27699/27699 [00:00<00:00, 2750361.65it/s]\n",
      "100%|██████████| 27699/27699 [00:00<00:00, 3830592.06it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|██████████| 27699/27699 [00:00<00:00, 3154354.39it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|██████████| 27699/27699 [00:00<00:00, 3460769.33it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|██████████| 27699/27699 [00:00<00:00, 3260313.93it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|██████████| 27699/27699 [00:00<00:00, 3393347.15it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|██████████| 27699/27699 [00:00<00:00, 3375501.96it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|██████████| 27699/27699 [00:00<00:00, 3395330.58it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|██████████| 27699/27699 [00:00<00:00, 3385436.56it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|██████████| 27699/27699 [00:00<00:00, 3195566.80it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|██████████| 27699/27699 [00:00<00:00, 3361049.20it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|██████████| 27699/27699 [00:00<00:00, 3339696.63it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|██████████| 27699/27699 [00:00<00:00, 3476718.53it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|██████████| 27699/27699 [00:00<00:00, 3397415.68it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|██████████| 27699/27699 [00:00<00:00, 3382085.72it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|██████████| 27699/27699 [00:00<00:00, 3412885.24it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|██████████| 27699/27699 [00:00<00:00, 3283813.18it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|██████████| 27699/27699 [00:00<00:00, 3350483.82it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|██████████| 27699/27699 [00:00<00:00, 3166304.00it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|██████████| 27699/27699 [00:00<00:00, 3240760.59it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|██████████| 27699/27699 [00:00<00:00, 3350193.97it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|██████████| 27699/27699 [00:00<00:00, 3509274.04it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|██████████| 27699/27699 [00:00<00:00, 3379232.88it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|██████████| 27699/27699 [00:00<00:00, 3261412.23it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|██████████| 27699/27699 [00:00<00:00, 3474431.08it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|██████████| 27699/27699 [00:00<00:00, 3241936.22it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|██████████| 27699/27699 [00:00<00:00, 3484225.84it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|██████████| 27699/27699 [00:00<00:00, 3548504.17it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|██████████| 27699/27699 [00:00<00:00, 3377170.04it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|██████████| 27699/27699 [00:00<00:00, 3528994.46it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|██████████| 27699/27699 [00:00<00:00, 3484121.35it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, min_count=1, alpha=0.065, min_alpha=0.065)\n",
    "model_dbow.build_vocab([x for x in tqdm(all_data)])\n",
    "\n",
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
    "    \"\"\"\n",
    "    Get vectors from trained doc2vec model\n",
    "    :param doc2vec_model: Trained Doc2Vec model\n",
    "    :param corpus_size: Size of the data\n",
    "    :param vectors_size: Size of the embedding vectors\n",
    "    :param vectors_type: Training or Testing vectors\n",
    "    :return: list of vectors\n",
    "    \"\"\"\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = vectors_type + '_' + str(i)\n",
    "        vectors[i] = model.docvecs[prefix]\n",
    "    return vectors\n",
    "    \n",
    "train_vectors_dbow = get_vectors(model_dbow, len(X_train), 300, 'Train')\n",
    "test_vectors_dbow = get_vectors(model_dbow, len(X_test), 300, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 11.9 µs\n",
      "accuracy 0.9914560770156438\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         Arts       0.99      0.98      0.99       284\n",
      "     Business       1.00      1.00      1.00       298\n",
      "     Concepts       0.99      0.98      0.99       329\n",
      "      Culture       0.99      0.99      0.99       306\n",
      "    Education       0.99      0.99      0.99       310\n",
      "Entertainment       1.00      0.99      0.99       298\n",
      "       Events       0.98      0.99      0.98       323\n",
      "    Geography       0.99      0.99      0.99       315\n",
      "       Health       1.00      0.99      1.00       315\n",
      "      History       1.00      1.00      1.00       320\n",
      "   Humanities       0.99      0.99      0.99       311\n",
      "     Language       0.99      0.99      0.99       283\n",
      "          Law       1.00      1.00      1.00       325\n",
      "         Life       0.99      1.00      0.99       285\n",
      "  Mathematics       0.99      0.99      0.99       287\n",
      "       Nature       1.00      0.99      1.00       317\n",
      "       People       0.99      0.99      0.99       312\n",
      "   Philosophy       0.99      0.99      0.99       311\n",
      "     Politics       1.00      0.98      0.99       319\n",
      "    Reference       0.99      1.00      0.99       280\n",
      "     Religion       0.99      0.98      0.99       293\n",
      "      Science       0.99      0.99      0.99       338\n",
      "      Society       1.00      0.99      1.00       305\n",
      "       Sports       0.99      0.99      0.99       317\n",
      "   Technology       0.99      1.00      0.99       304\n",
      "     Universe       0.99      0.99      0.99       297\n",
      "        World       0.98      0.99      0.99       328\n",
      "\n",
      "     accuracy                           0.99      8310\n",
      "    macro avg       0.99      0.99      0.99      8310\n",
      " weighted avg       0.99      0.99      0.99      8310\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg.fit(train_vectors_dbow, y_train)\n",
    "logreg = logreg.fit(train_vectors_dbow, y_train)\n",
    "%time\n",
    "y_pred = logreg.predict(test_vectors_dbow)\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words Model with Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import utils\n",
    "\n",
    "train_posts, train_tags, test_posts, test_tags = train_test_split(X, y, test_size=0.25, random_state=1)\n",
    "\n",
    "\n",
    "max_words = 1000\n",
    "tokenize = text.Tokenizer(num_words=max_words, char_level=False)\n",
    "tokenize.fit_on_texts(train_posts) # only fit on train\n",
    "\n",
    "x_train = tokenize.texts_to_matrix(train_posts)\n",
    "x_test = tokenize.texts_to_matrix(test_posts)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)\n",
    "\n",
    "num_classes = np.max(y_train) + 1\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
